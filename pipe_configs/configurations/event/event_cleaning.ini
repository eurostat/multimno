[Logging]
level = DEBUG

[Spark]
session_name = EventCleaning

[EventCleaning]
# TODO: reading in chunks by now reading parquet files from one location - default path of landing/mno_events 
timestamp_format = yyyy-MM-dd'T'HH:mm:ss
input_timezone = America/Los_Angeles
data_period_start = 2023-01-01 # '2023-01-01 00:00:00'
data_period_end = 2023-01-05
data_folder_date_format = %Y%m%d
do_bounding_box_filtering = True
bounding_box = {
    'min_lon': -180,
    'max_lon': 180,
    'min_lat': -90,
    'max_lat': 90
    }
mandatory_columns_casting_dict = {
    "user_id": "binary",
    "timestamp": "timestamp",
    "mcc": "integer",
    "cell_id": "string",
    "latitude": "float",
    "longitude": "float"
    }   
optional_columns_casting_dict = {
    "loc_error": "float"
    }