[Logging]
level = DEBUG

[Spark]
session_name = EventCleaning

[EventCleaning]
# TODO: reading in chunks by now reading parquet files from one location - default path of landing/mno_events 
timestamp_format = yyyy-MM-dd'T'HH:mm:ss
input_timezone = America/Los_Angeles
data_period_start = 2023-01-01 # '2023-01-01 00:00:00'
data_period_end = 2023-01-06
data_folder_date_format = %Y%m%d
do_bounding_box_filtering = True
bounding_box = {
    'min_lon': -180,
    'max_lon': 180,
    'min_lat': -90,
    'max_lat': 90
    }
