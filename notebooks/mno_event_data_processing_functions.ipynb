{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db74e4b-3c0c-4930-89c6-27f98637ca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from hashlib import sha256\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import ShortType, ByteType\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b334f8c6-7d4c-434b-8c2a-0cc7b6958fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"MNO_Event_Cleansing\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3af8cf1-51f3-429c-9ab5-a5e1c8749e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'user_id':['00', '1', '1','1','2', '2', '2', '3'],\n",
    "    'timestamp': ['2023-01-01T00:12:00', None, '2023-01-01T00:00:00', '2023-01-01T00:03:00', \n",
    "                  '2023-05-01T00:00:00', '2023-05-01T01:00:00', '2023-05-01T01:00:00', '2023-01-01T00:03:00'],\n",
    "    'mcc': [254, 254, 254, 254, 254, 254, 254, 254], \n",
    "    'cell_id': ['214030412038931', '214030412038931', '214030412038931', '214030412038931', '214030412038935', None, None, None], \n",
    "    'latitude': [-3.62958, -3.62954, -3.62958, -3.62954, None, -3.62959, -3.62950, None],\n",
    "    'longitude': [40.51873, 40.51870, 40.51873, 40.51870, None, None, 40.51874, None],\n",
    "    'loc_error': [100, 100, 100, 100, None, None, 100, None]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d6d600-b129-4cb1-a106-32d3e7eca8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mno_data = pd.DataFrame(data)\n",
    "mno_data['user_id'] = mno_data['user_id'].apply(lambda x: sha256(x.encode('utf-8')).digest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f4a552-62ae-4584-9c0a-904714e7600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mno_data.loc[0, 'user_id'] = None\n",
    "mno_data = mno_data.replace({float('nan'): None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e53ee63-f845-452f-8c7b-da51ade9df1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(mno_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f79018-1f17-4cea-aa4e-7f49f90626c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_format = \"yyyy-MM-dd'T'HH:mm:ss\"\n",
    "input_timezone = 'America/Los_Angeles'\n",
    "data_period_start = '2023-01-01' # '2023-01-01 00:00:00'\n",
    "data_period_end = '2023-05-01'\n",
    "do_bounding_box_filtering = True\n",
    "bounding_box = {'min_lon': -180,\n",
    "                'max_lon': 180,\n",
    "                'min_lat': -90,\n",
    "                'max_lat': 90\n",
    "                }\n",
    "clean_mno_event_data_write_path = '../sample_data/output/clean_mno_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cba463-c49c-4c87-a4dc-5f419485ecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mandatory_columns_casting_dict = {\n",
    "    \"user_id\": \"binary\",\n",
    "    \"timestamp\": \"timestamp\",\n",
    "    \"mcc\": \"integer\",\n",
    "    \"cell_id\": \"string\",\n",
    "    \"latitude\": \"float\",\n",
    "    \"longitude\": \"float\"\n",
    "}\n",
    "# if you know for sure that this column is not present and thus will be always null\n",
    "# make dtype '' - empty string\n",
    "optional_columns_casting_dict = {\n",
    "    \"loc_error\": \"float\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d58ee79-dd8a-4def-b9bd-83c1b6ae5499",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(mno_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34500184-c397-4f47-921b-b07d12010ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.printSchema()\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f639ff1-fd5a-4f1d-a139-f712f880249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# better naming ? convert_to_schema?\n",
    "def check_existance_of_columns(df: pyspark.sql.dataframe.DataFrame, \n",
    "                               mandatory_columns: list[str], \n",
    "                               optional_columns: list[str]\n",
    "                              ) -> pyspark.sql.dataframe.DataFrame:\n",
    "    df_columns = df.columns\n",
    "    mandatory_common_columns = set(mandatory_columns).intersection(set(df_columns))\n",
    "    optional_common_columns = set(optional_columns).intersection(set(df_columns))\n",
    "\n",
    "    if len(mandatory_common_columns) != len(mandatory_columns):\n",
    "        raise KeyError(\"Not all mandatory columns in df are present\")\n",
    "\n",
    "    missing_optional_columns = set(optional_columns) - set(optional_common_columns)\n",
    "    for missing_optional_column in missing_optional_columns:\n",
    "        df = df.withColumn(missing_optional_column, F.lit(None).cast('string'))\n",
    "        \n",
    "    df = df.select(mandatory_columns + optional_columns)\n",
    "    \n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f4a184-16fb-4442-b75a-86c633fcdad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_nulls(df: pyspark.sql.dataframe.DataFrame, \n",
    "                 filter_columns: list[str]= None\n",
    "                ) -> pyspark.sql.dataframe.DataFrame:\n",
    "    \n",
    "    df = df.na.drop(how='any', subset = filter_columns)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e37abd-6e40-4874-a705-76f032793f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_null_locations(df: pyspark.sql.dataframe.DataFrame) -> pyspark.sql.dataframe.DataFrame:\n",
    "    \n",
    "    df = df.filter((F.col('cell_id').isNotNull()) | (F.col('longitude').isNotNull()&F.col('latitude').isNotNull()))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf563f4-7cb9-4dc0-b432-4ebfaff376de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time_column_to_timestamp(df: pyspark.sql.dataframe.DataFrame, \n",
    "                                     timestampt_format: str, \n",
    "                                     input_timezone: str\n",
    "                                    ) -> pyspark.sql.dataframe.DataFrame:\n",
    "\n",
    "    df = df.withColumn('timestamp',  F.to_utc_timestamp(F.to_timestamp('timestamp', timestampt_format), input_timezone))\\\n",
    "           .filter(F.col('timestamp').isNotNull())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc96717-851a-411f-832d-fced68217a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_period_filtering(df: pyspark.sql.dataframe.DataFrame, \n",
    "                          data_period_start: str, \n",
    "                          data_period_end: str\n",
    "                         ) -> pyspark.sql.dataframe.DataFrame:\n",
    "    \n",
    "    data_period_start = F.to_date(F.lit(data_period_start))\n",
    "    data_period_end = F.to_date(F.lit(data_period_end))\n",
    "    # inclusive on both sides\n",
    "    df = df.filter(F.col('timestamp').between(data_period_start, data_period_end))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba23051c-48a0-450a-9bc4-c88c6261ed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bounding_box_filtering(df: pyspark.sql.dataframe.DataFrame,\n",
    "                           bounding_box: dict\n",
    "                          ) -> pyspark.sql.dataframe.DataFrame:\n",
    "    # coordinates of bounding box should be of the same crs of mno data\n",
    "    lat_condition = (F.col('latitude').between(bounding_box['min_lat'], bounding_box['max_lat']))\n",
    "    lon_condition = (F.col('longitude').between(bounding_box['min_lon'], bounding_box['max_lon']))\n",
    "\n",
    "    df = df.filter(lat_condition & lon_condition)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb265d49-595c-4511-bde4-7681b9a2e3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_columns(df: pyspark.sql.dataframe.DataFrame, \n",
    "                 mandatory_columns_casting_dict: dict, \n",
    "                 optional_columns_casting_dict: dict\n",
    "                ) -> pyspark.sql.dataframe.DataFrame:\n",
    "    # casting timestamp was done in convert_time_column_to_timestamp function\n",
    "    # not del or pop to not change global mandatory_columns_casting_dict\n",
    "    mandatory_columns_casting_dict = {col:dtype for col, dtype in mandatory_columns_casting_dict.items() if col!='timestamp'}\n",
    "    # perform casting only for those optional columns that are not entirely nulls\n",
    "    optional_columns_casting_dict = {col:dtype for col, dtype in optional_columns_casting_dict.items() if dtype.strip()}\n",
    "    # for python 3.9 and greater \n",
    "    columns_casting_dict = mandatory_columns_casting_dict | optional_columns_casting_dict\n",
    "    \n",
    "    for col, dtype in columns_casting_dict.items():\n",
    "        df = df.withColumn(col, F.col(col).cast(dtype))\n",
    "        \n",
    "    # nulls in location columns are treated differently\n",
    "    # optional columns can have null values\n",
    "    filter_columns = list(set(mandatory_columns_casting_dict.keys())\\\n",
    "                          - set(['cell_id', 'latitude', 'longitude'] + list(optional_columns_casting_dict.keys())))\n",
    "\n",
    "    df = filter_nulls(df, filter_columns)\n",
    "    df = filter_null_locations(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5424c17f-88f9-4ddc-9fe8-8768a5ed34bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = check_existance_of_columns(spark_df, \n",
    "                                list(mandatory_columns_casting_dict.keys()), \n",
    "                                list(optional_columns_casting_dict.keys()))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979e6be7-7103-4fd7-863e-0f3ee415848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = filter_nulls(df, ['user_id', 'timestamp'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19774ce8-e157-467c-b0ef-aa58bc5d4420",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = filter_null_locations(df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f879b8-360e-4774-9fd7-171ecc08e591",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = convert_time_column_to_timestamp(df, timestamp_format, input_timezone)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb99f043-28f4-46a3-82dc-28ad10f18a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_period_filtering(df, data_period_start, data_period_end)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d436c22f-1886-4759-8a9a-0c3db096c487",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_bounding_box_filtering:\n",
    "    df = bounding_box_filtering(df, bounding_box)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54908c09-eec5-4c8d-bbbb-601977c532d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cast_columns(df, mandatory_columns_casting_dict, optional_columns_casting_dict)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baba9258-c5cd-4312-a0b9-091ee912cac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"year\", F.year(\"timestamp\").cast(ShortType())) \\\n",
    "       .withColumn(\"month\", F.month(\"timestamp\").cast(ByteType())) \\\n",
    "       .withColumn(\"day\", F.dayofmonth(\"timestamp\").cast(ByteType()))\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056a3e2c-d158-486e-a7ba-58634dbb7c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort(['user_id', 'timestamp'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ffc597-88f1-43d0-bb7c-11b335f2d79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(clean_mno_event_data_write_path, mode='append', partitionBy = ['year', 'month', 'day'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
