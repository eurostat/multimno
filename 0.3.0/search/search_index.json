{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MultiMNO","text":"<p>This repository contains code that processes MNO Data to generate population and mobility insights.</p>"},{"location":"#sections","title":"Sections","text":"<ul> <li>User Manual: Guide on how to setup, configure and execute the software.</li> <li>Dev Guide: Guidelines and best practices for contributing to the repository and setting up a development environment.</li> <li>Pipeline: View of the data processing pipeline.</li> <li>Reference: Code documentation.</li> <li>System Requirements: Mandatory requirements to execute the software.</li> <li>License: Software license - EUROPEAN UNION PUBLIC LICENCE v. 1.2.</li> </ul>"},{"location":"LICENSE/","title":"License","text":"<p>EUROPEAN UNION PUBLIC LICENCE v. 1.2  EUPL \u00a9 the European Union 2007, 2016 </p> <p>This European Union Public Licence (the \u2018EUPL\u2019) applies to the Work (as defined below) which is provided under the  terms of this Licence. Any use of the Work, other than as authorised under this Licence is prohibited (to the extent such  use is covered by a right of the copyright holder of the Work).  The Work is provided under the terms of this Licence when the Licensor (as defined below) has placed the following  notice immediately following the copyright notice for the Work:                            Licensed under the EUPL  or has expressed by any other means his willingness to license under the EUPL. </p> <p>1.Definitions  In this Licence, the following terms have the following meaning:  \u2014 \u2018The Licence\u2019:this Licence.  \u2014 \u2018The Original Work\u2019:the work or software distributed or communicated by the Licensor under this Licence, available  as Source Code and also as Executable Code as the case may be.  \u2014 \u2018Derivative Works\u2019:the works or software that could be created by the Licensee, based upon the Original Work or  modifications thereof. This Licence does not define the extent of modification or dependence on the Original Work  required in order to classify a work as a Derivative Work; this extent is determined by copyright law applicable in  the country mentioned in Article 15.  \u2014 \u2018The Work\u2019:the Original Work or its Derivative Works.  \u2014 \u2018The Source Code\u2019:the human-readable form of the Work which is the most convenient for people to study and  modify.  \u2014 \u2018The Executable Code\u2019:any code which has generally been compiled and which is meant to be interpreted by  a computer as a program.  \u2014 \u2018The Licensor\u2019:the natural or legal person that distributes or communicates the Work under the Licence.  \u2014 \u2018Contributor(s)\u2019:any natural or legal person who modifies the Work under the Licence, or otherwise contributes to  the creation of a Derivative Work.  \u2014 \u2018The Licensee\u2019 or \u2018You\u2019:any natural or legal person who makes any usage of the Work under the terms of the  Licence.  \u2014 \u2018Distribution\u2019 or \u2018Communication\u2019:any act of selling, giving, lending, renting, distributing, communicating,  transmitting, or otherwise making available, online or offline, copies of the Work or providing access to its essential  functionalities at the disposal of any other natural or legal person. </p> <p>2.Scope of the rights granted by the Licence  The Licensor hereby grants You a worldwide, royalty-free, non-exclusive, sublicensable licence to do the following, for  the duration of copyright vested in the Original Work:  \u2014 use the Work in any circumstance and for all usage,  \u2014 reproduce the Work,  \u2014 modify the Work, and make Derivative Works based upon the Work,  \u2014 communicate to the public, including the right to make available or display the Work or copies thereof to the public  and perform publicly, as the case may be, the Work,  \u2014 distribute the Work or copies thereof,  \u2014 lend and rent the Work or copies thereof,  \u2014 sublicense rights in the Work or copies thereof.  Those rights can be exercised on any media, supports and formats, whether now known or later invented, as far as the  applicable law permits so.  In the countries where moral rights apply, the Licensor waives his right to exercise his moral right to the extent allowed  by law in order to make effective the licence of the economic rights here above listed.  The Licensor grants to the Licensee royalty-free, non-exclusive usage rights to any patents held by the Licensor, to the  extent necessary to make use of the rights granted on the Work under this Licence. </p> <p>3.Communication of the Source Code  The Licensor may provide the Work either in its Source Code form, or as Executable Code. If the Work is provided as  Executable Code, the Licensor provides in addition a machine-readable copy of the Source Code of the Work along with  each copy of the Work that the Licensor distributes or indicates, in a notice following the copyright notice attached to  the Work, a repository where the Source Code is easily and freely accessible for as long as the Licensor continues to  distribute or communicate the Work. </p> <p>4.Limitations on copyright  Nothing in this Licence is intended to deprive the Licensee of the benefits from any exception or limitation to the  exclusive rights of the rights owners in the Work, of the exhaustion of those rights or of other applicable limitations  thereto. </p> <p>5.Obligations of the Licensee  The grant of the rights mentioned above is subject to some restrictions and obligations imposed on the Licensee. Those  obligations are the following: </p> <p>Attribution right: The Licensee shall keep intact all copyright, patent or trademarks notices and all notices that refer to  the Licence and to the disclaimer of warranties. The Licensee must include a copy of such notices and a copy of the  Licence with every copy of the Work he/she distributes or communicates. The Licensee must cause any Derivative Work  to carry prominent notices stating that the Work has been modified and the date of modification. </p> <p>Copyleft clause: If the Licensee distributes or communicates copies of the Original Works or Derivative Works, this  Distribution or Communication will be done under the terms of this Licence or of a later version of this Licence unless  the Original Work is expressly distributed only under this version of the Licence \u2014 for example by communicating  \u2018EUPL v. 1.2 only\u2019. The Licensee (becoming Licensor) cannot offer or impose any additional terms or conditions on the  Work or Derivative Work that alter or restrict the terms of the Licence. </p> <p>Compatibility clause: If the Licensee Distributes or Communicates Derivative Works or copies thereof based upon both  the Work and another work licensed under a Compatible Licence, this Distribution or Communication can be done  under the terms of this Compatible Licence. For the sake of this clause, \u2018Compatible Licence\u2019 refers to the licences listed  in the appendix attached to this Licence. Should the Licensee's obligations under the Compatible Licence conflict with  his/her obligations under this Licence, the obligations of the Compatible Licence shall prevail. </p> <p>Provision of Source Code: When distributing or communicating copies of the Work, the Licensee will provide  a machine-readable copy of the Source Code or indicate a repository where this Source will be easily and freely available  for as long as the Licensee continues to distribute or communicate the Work.  Legal Protection: This Licence does not grant permission to use the trade names, trademarks, service marks, or names  of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and  reproducing the content of the copyright notice. </p> <p>6.Chain of Authorship  The original Licensor warrants that the copyright in the Original Work granted hereunder is owned by him/her or  licensed to him/her and that he/she has the power and authority to grant the Licence.  Each Contributor warrants that the copyright in the modifications he/she brings to the Work are owned by him/her or  licensed to him/her and that he/she has the power and authority to grant the Licence.  Each time You accept the Licence, the original Licensor and subsequent Contributors grant You a licence to their contributions  to the Work, under the terms of this Licence. </p> <p>7.Disclaimer of Warranty  The Work is a work in progress, which is continuously improved by numerous Contributors. It is not a finished work  and may therefore contain defects or \u2018bugs\u2019 inherent to this type of development.  For the above reason, the Work is provided under the Licence on an \u2018as is\u2019 basis and without warranties of any kind  concerning the Work, including without limitation merchantability, fitness for a particular purpose, absence of defects or  errors, accuracy, non-infringement of intellectual property rights other than copyright as stated in Article 6 of this  Licence.  This disclaimer of warranty is an essential part of the Licence and a condition for the grant of any rights to the Work. </p> <p>8.Disclaimer of Liability  Except in the cases of wilful misconduct or damages directly caused to natural persons, the Licensor will in no event be  liable for any direct or indirect, material or moral, damages of any kind, arising out of the Licence or of the use of the  Work, including without limitation, damages for loss of goodwill, work stoppage, computer failure or malfunction, loss  of data or any commercial damage, even if the Licensor has been advised of the possibility of such damage. However,  the Licensor will be liable under statutory product liability laws as far such laws apply to the Work. </p> <p>9.Additional agreements  While distributing the Work, You may choose to conclude an additional agreement, defining obligations or services  consistent with this Licence. However, if accepting obligations, You may act only on your own behalf and on your sole  responsibility, not on behalf of the original Licensor or any other Contributor, and only if You agree to indemnify,  defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against such Contributor by  the fact You have accepted any warranty or additional liability. </p> <p>10.Acceptance of the Licence  The provisions of this Licence can be accepted by clicking on an icon \u2018I agree\u2019 placed under the bottom of a window  displaying the text of this Licence or by affirming consent in any other similar way, in accordance with the rules of  applicable law. Clicking on that icon indicates your clear and irrevocable acceptance of this Licence and all of its terms  and conditions.  Similarly, you irrevocably accept this Licence and all of its terms and conditions by exercising any rights granted to You  by Article 2 of this Licence, such as the use of the Work, the creation by You of a Derivative Work or the Distribution  or Communication by You of the Work or copies thereof. </p> <p>11.Information to the public  In case of any Distribution or Communication of the Work by means of electronic communication by You (for example,  by offering to download the Work from a remote location) the distribution channel or media (for example, a website)  must at least provide to the public the information requested by the applicable law regarding the Licensor, the Licence  and the way it may be accessible, concluded, stored and reproduced by the Licensee. </p> <p>12.Termination of the Licence  The Licence and the rights granted hereunder will terminate automatically upon any breach by the Licensee of the terms  of the Licence.  Such a termination will not terminate the licences of any person who has received the Work from the Licensee under  the Licence, provided such persons remain in full compliance with the Licence. </p> <p>13.Miscellaneous  Without prejudice of Article 9 above, the Licence represents the complete agreement between the Parties as to the  Work.  If any provision of the Licence is invalid or unenforceable under applicable law, this will not affect the validity or  enforceability of the Licence as a whole. Such provision will be construed or reformed so as necessary to make it valid  and enforceable.  The European Commission may publish other linguistic versions or new versions of this Licence or updated versions of  the Appendix, so far this is required and reasonable, without reducing the scope of the rights granted by the Licence.  New versions of the Licence will be published with a unique version number.  All linguistic versions of this Licence, approved by the European Commission, have identical value. Parties can take  advantage of the linguistic version of their choice. </p> <p>14.Jurisdiction  Without prejudice to specific agreement between parties,  \u2014 any litigation resulting from the interpretation of this License, arising between the European Union institutions,  bodies, offices or agencies, as a Licensor, and any Licensee, will be subject to the jurisdiction of the Court of Justice  of the European Union, as laid down in article 272 of the Treaty on the Functioning of the European Union,  \u2014 any litigation arising between other parties and resulting from the interpretation of this License, will be subject to  the exclusive jurisdiction of the competent court where the Licensor resides or conducts its primary business. </p> <p>15.Applicable Law  Without prejudice to specific agreement between parties,  \u2014 this Licence shall be governed by the law of the European Union Member State where the Licensor has his seat,  resides or has his registered office,  \u2014 this licence shall be governed by Belgian law if the Licensor has no seat, residence or registered office inside  a European Union Member State. </p> <pre><code>                                                     Appendix\n</code></pre> <p>\u2018Compatible Licences\u2019 according to Article 5 EUPL are:  \u2014 GNU General Public License (GPL) v. 2, v. 3  \u2014 GNU Affero General Public License (AGPL) v. 3  \u2014 Open Software License (OSL) v. 2.1, v. 3.0  \u2014 Eclipse Public License (EPL) v. 1.0  \u2014 CeCILL v. 2.0, v. 2.1  \u2014 Mozilla Public Licence (MPL) v. 2  \u2014 GNU Lesser General Public Licence (LGPL) v. 2.1, v. 3  \u2014 Creative Commons Attribution-ShareAlike v. 3.0 Unported (CC BY-SA 3.0) for works other than software  \u2014 European Union Public Licence (EUPL) v. 1.1, v. 1.2  \u2014 Qu\u00e9bec Free and Open-Source Licence \u2014 Reciprocity (LiLiQ-R) or Strong Reciprocity (LiLiQ-R+).</p> <p>The European Commission may update this Appendix to later versions of the above licences without producing  a new version of the EUPL, as long as they provide the rights granted in Article 2 of this Licence and protect the  covered Source Code from exclusive appropriation.  All other changes or additions to this Appendix require the production of a new EUPL version. </p>"},{"location":"pipeline/","title":"Pipeline","text":"<p>The software can perform the following pipeline:</p> <pre><code>%%{\n  init: {\n    'theme': 'base',\n    'themeVariables': {\n      'primaryColor': '#BB2528',\n      'primaryTextColor': '#000000',\n      'primaryBorderColor': '#7C0000',\n      'lineColor': '#F8B229',\n      'secondaryColor': '#006100',\n      'tertiaryColor': '#fff'\n    }\n  }\n}%%\nflowchart TD;\n    %% --- Reference Data ---\n    %% Inspire grid generation\n    subgraph Ingestion\n    InspireGridGeneration--&gt;InspireGridData[(Inspire Grid\\n)];\n    end\n    %% --- MNO Data ---\n    %% -- NETWORK --\n    subgraph Cleaning\n    %% RAW Network cleaning\n    PhysicalNetworkRAWData[(MNO-Network\\nPhysical\\nRAW)]--&gt;NetworkCleaning--&gt;PhysicalNetworkData[(MNO-Network\\nPhysical)];\n    NetworkCleaning--&gt;NetworkQAData[(MNO-Network\\nQuality Checks)];\n    %% RAW Network QA\n    NetworkQAData--&gt;NetworkQualityWarnings--&gt;NetworkWarnings[(Network\\nQuality Warnings)];\n    NetworkQualityWarnings--&gt;NetworkReports{{Network\\nQA \\ngraph data\\ncsv}};\n    %% -- EVENTS --\n    %% RAW Events cleaning\n    EventsRAWData[(MNO-Event\\nRAW)]--&gt;EventCleaning--&gt;EventsData[(MNO-Event)];\n    EventCleaning--&gt;EventsQA[(MNO-Event\\nQuality Checks)]--&gt;EventQualityWarnings;\n    EventCleaning--&gt;EventsQAfreq[(MNO-Event\\nQuality Checks\\nfrequency)];\n    %% RAW Events Warnings\n    EventsQAfreq--&gt;EventQualityWarnings;\n    EventQualityWarnings--&gt;EventsWarnings[(Events\\nQuality Warnings)];\n    EventQualityWarnings--&gt;EventsReports{{Event QA \\ngraph data\\ncsv}};\n    %% Event Semantic Checks\n    EventsData--&gt;SemanticCleaning--&gt;EventsSemanticCleaned[(Events\\nSemantic\\nCleaned)];\n    PhysicalNetworkData--&gt;SemanticCleaning;\n    SemanticCleaning--&gt;DeviceSemanticQualityMetrics[(Device\\nSemantic\\nQuality\\nMetrics)];\n    %% Event Semantic Warnings\n    EventsSemanticCleaned--&gt;SemanticQualityWarnings--&gt;EventSemanticWarnings[(Event\\nSemantic\\nQuality\\nWarnings)];\n    DeviceSemanticQualityMetrics--&gt;SemanticQualityWarnings--&gt;EventSemanticReports{{Event Semantic QA \\ngraph data\\ncsv}};\n    %% Device activity Statistics\n    EventsData--&gt;DeviceActivityStatistics--&gt;DeviceActivityStatisticsData[(Device\\nActivity\\nStatistics)];\n    PhysicalNetworkData--&gt;DeviceActivityStatistics;\n    end\n\n\n    subgraph Network Processing\n    %% Signal Strength\n    InspireGridData--&gt;SignalStrengthModeling;\n    PhysicalNetworkData--&gt;SignalStrengthModeling--&gt;SignalStrengthData[(Signal Strength)];\n    %% Cell Footprint\n    SignalStrengthData--&gt;CellFootprintEstimation--&gt;CellFootprintData[(Cell Footprint\\nValues)];\n    CellFootprintEstimation--&gt;CellIntersectionGroupsData[(Cell Intersection Groups)];\n    %% Cell Connection Probability\n    CellFootprintData--&gt;CellConnectionProbabilityEstimation;\n    InspireGridData--&gt;CellConnectionProbabilityEstimation--&gt;CellConnectionProbabilityData[(Cell Connection\\nProbability)];\n    end\n\n    subgraph Daily Products\n    %% --- Daily Processing module ---\n    %% Daily Permanence Score\n    EventsSemanticCleaned--&gt;DailyPermanenceScore--&gt;DPSdata[(Daily\\nPermanence\\nScore\\nData)];\n    CellFootprintData--&gt;DailyPermanenceScore;\n    %% Continuous Time segmentation\n    EventsSemanticCleaned--&gt;ContinuousTimeSegmentation--&gt;DailyCTSdata[(Daily\\nContinuous\\nTime\\nSegmentation)];\n    CellFootprintData--&gt;ContinuousTimeSegmentation;\n    CellIntersectionGroupsData--&gt;ContinuousTimeSegmentation;\n    %% Present Population Estimation\n    EventsSemanticCleaned--&gt;PresentPopulation--&gt;PresentPopulationData[(Present\\nPopulation)];\n    CellConnectionProbabilityData--&gt;PresentPopulation--&gt;PresentPopulationZoneData[(Present\\nPopulation\\nZone)];\n    InspireGridData--&gt;PresentPopulation;\n    end\n\n\n    %% --- Longitudinal module ---\n    subgraph MidTerm Products\n    %% Midterm Permanence Score\n    HolidayData[(Holiday\\nData)]\n    DPSdata--&gt;MidTermPermanenceScore--&gt;MPSdata[(MidTerm\\nPermanence\\nScore\\nData)];\n    HolidayData--&gt;MidTermPermanenceScore;\n    end\n\n    %% [LONGTERM]\n    subgraph LongTerm Products\n    %% Longterm Permanence Score\n    MPSdata--&gt;LongTermPermanenceScore--&gt;LPSdata[(LongTerm\\nPermanence\\nScore\\nData)];\n    LPSdata--&gt;UsualEnvironmentLabelling--&gt;UELdata[(UsualEnvironment\\nLabelling\\nData)];\n    UELdata--&gt;UsualEnvironmentAggregation--&gt;UEAdata[(UsualEnvironment\\nAggregation\\nData)];\n    InspireGridData--&gt;UsualEnvironmentAggregation;\n    end\n\n    classDef green fill:#229954,stroke:#333,stroke-width:2px;\n    classDef light_green fill:#AFE1AF,stroke:#333,stroke-width:1px;\n    classDef bronze fill:#CD7F32,stroke:#333,stroke-width:2px;\n    classDef silver fill:#adadad,stroke:#333,stroke-width:2px;\n    classDef light_silver fill:#dcdcdc,stroke:#333,stroke-width:2px;\n    classDef gold fill:#FFD700,stroke:#333,stroke-width:2px;\n\n    %% --- Reference Data ---\n    class InspireGridData,HolidayData light_silver\n    %% --- MNO Data ---\n    %% -- NETWORK --\n    class PhysicalNetworkRAWData bronze\n    class PhysicalNetworkData light_silver\n    class NetworkQAData,NetworkWarnings silver\n    class NetworkReports gold\n    class SignalStrengthData,CellFootprintData,CellConnectionProbabilityData,CellIntersectionGroupsData light_silver\n    %% -- EVENTS --\n    %% event cleaning\n    class EventsRAWData bronze\n    class EventsData light_silver\n    class EventsQA,EventsQAfreq,EventsWarnings silver\n    class EventsReports gold\n    %% event deduplicated\n    class EventsDeduplicated light_silver\n    class EventsDeduplicatedQA,EventsDeduplicatedQAfreq,EventsDeduplicatedWarnings silver\n    class EventsDeduplicatedReports gold\n    %% device activity statistics\n    class DeviceActivityStatisticsData light_silver\n    %% events semantic clean\n    class EventsSemanticCleaned light_silver\n    class DeviceSemanticQualityMetrics,EventSemanticWarnings silver\n    class EventSemanticReports gold\n    %% Present population\n    class PresentPopulationData,PresentPopulationZoneData light_silver\n    %% --- Daily Processing module ---\n    %% Daily Permanence Score\n    class DPSdata light_silver\n    %% Continuous Time segmentation\n    class DailyCTSdata light_silver\n    %% Longitudinal data\n    class MPSdata,LPSdata light_silver\n    %% UE data\n    class UELdata,UEAdata light_silver\n\n    %% ---- Components ----\n    class InspireGridGeneration light_green\n    %% Net\n    class NetworkCleaning,SignalStrengthModeling,CellFootprintEstimation,CellConnectionProbabilityEstimation light_green\n    class NetworkQualityWarnings green\n    %% Events\n    class EventCleaning,EventDeduplication,SemanticCleaning light_green\n    class EventQualityWarnings,EventQualityWarnings2,SemanticQualityWarnings green\n    %% -&gt; Device Activity Statistics should start from semantic cleaned events\n    class DeviceActivityStatistics light_green\n    %% Daily\n    class DailyPermanenceScore,ContinuousTimeSegmentation,PresentPopulation light_green\n    %% Longitudinal - Midterm\n    class MidTermPermanenceScore light_green\n    class LongTermPermanenceScore,UsualEnvironmentLabelling,UsualEnvironmentAggregation light_green</code></pre>"},{"location":"system_requirements/","title":"System Requirements","text":"<p>Multimno is a python library which requires the installation of additional system &amp; python libraries.</p> <p>In this section the requirements for executing this software are defined. In the case of using docker for execution, the system only needs to comply with the Host and Docker requirements as the docker image will have all the software requirements.</p>"},{"location":"system_requirements/#host-requirements","title":"Host requirements","text":"<ul> <li>Cores: 4</li> <li>RAM: 16 Gb</li> <li>Disk: 32 Gb of free space</li> <li>OS: <ul> <li>Ubuntu 22.04 (Recommended)</li> <li>Mac 12.6</li> <li>Windows 11 + WSL2 with Ubuntu 22.04 </li> </ul> </li> </ul>"},{"location":"system_requirements/#docker-requirements","title":"Docker requirements","text":"<p>In the case of using the docker image provided the following requirements must be fulfilled:</p> <ul> <li>Docker-engine: 25.0.X</li> <li>Docker-compose: 2.24.X</li> <li>Internet connection to Ubuntu/Spark/Docker official repositories for building the docker image</li> </ul>"},{"location":"system_requirements/#software-requirements","title":"Software Requirements","text":"<p>In the case of setting up a system for launching the software, the following dependencies have to be installed.</p>"},{"location":"system_requirements/#os-libraries","title":"OS Libraries","text":"Library Version Python &gt; 3.10.8 Java JDK 17.0.9 Apache Spark 3.5.1 GDAL 3.6.2"},{"location":"system_requirements/#spark-libraries","title":"Spark Libraries","text":"Library Version Apache Sedona 1.5.1 Geotools wrapper 28.2"},{"location":"system_requirements/#python-libraries","title":"Python Libraries","text":"Library Version numpy 1.26.2 pandas 2.1.4 geopandas 0.11.1 shapely 1.8.4 pyarrow 14.0.1 requests 2.31.0 py4j 0.10.9.7 pydeck 0.8.0"},{"location":"DevGuide/","title":"Developer Guide","text":"<p>The developer guide contains two sections: - Contribute: Guidelines on how to contribute to the software. - Developer Guidelines: Guidelines and tips on how to develop and test the software.  </p>"},{"location":"DevGuide/1_contribute/","title":"Contribute","text":"<p>In this document the general rules and guidelines for contributing to the multimno repository are detailed.</p>"},{"location":"DevGuide/1_contribute/#source-control-strategy","title":"Source control strategy","text":"<p>This repository uses three principal branches for source control:</p> <ul> <li>main: Branch where the official releases are tagged. The HEAD of the branch corresponds to the    latest release of the software.  </li> <li>integration: Branch used for preproduction testing and validation from which a release to the main branch will be generated.</li> <li>development: Branch that centralizes the latest features developed in the repository. After enough features/bugs have been delivered to this branch, a snapshot will be created in the integration branch for testing before  generating a release.</li> </ul> <p>These three branches shall only accept changes by the repository administrator. Commits shall not be performed directly in these branches except for small hotfixes in the integration branch.</p> <p>All features and bug fixes will be developed in branches that origin from the development branch.</p>"},{"location":"DevGuide/1_contribute/#forking-the-repository","title":"Forking the repository","text":"<p>Developers that want to contribute to the multimno repository shall fork the repository with all its branches. This can  be done through the github website. After creating a fork of the repository, developers can clone the forked repo in  their computers.</p>"},{"location":"DevGuide/1_contribute/#create-an-issue-with-the-development-that-will-be-performed","title":"Create an Issue with the development that will be performed","text":"<p>First of all, Check if the issue you will develop already exists.  Then, create an issue in the multimno repository stating the objective of the development that will be performed. Templates for creating issues for features or fixes are provided in the repository.</p>"},{"location":"DevGuide/1_contribute/#creating-a-featurefix-branch","title":"Creating a feature/fix branch","text":"<p>Within the forked repository developers shall create a branch that originates from the development branch.  This branch shall have the following naming convention:</p> <ul> <li>feat_\\&lt;name&gt;: If it is a new feature.</li> <li>fix_\\&lt;name&gt;: If it is a bug solution.</li> </ul> <p>Please remember to keep the forked development branch up-to-date with the latest changes.</p> <p>Don't forget to look up the developer guidelines to check the code style, testing and development practices that shall be followed to develop new code for the multimno repository.</p>"},{"location":"DevGuide/1_contribute/#opening-a-pull-request","title":"Opening a pull request","text":"<p>Before opening a pull request in the multimno repository verify: * Latest development changes are merged into the branch that performs the pull request.  * All tests pass successfully. * All the new code is documented following the Google style docstrings. * New tests for the code developed are included and pass successfully.</p> <p>Use the github web to create a pull request. The pull request must deliver your developed branch to the development branch of the multimno repository. Associate the PR(pull request) to the previously created issue.</p>"},{"location":"DevGuide/1_contribute/#the-review-process-pull-request-closure","title":"The review process &amp; pull request closure","text":"<p>The repository administrators will review the pull request performed to the development branch. </p> <ul> <li>If the changes are accepted, they will be incorporated in the development branch and the pull request will be closed. </li> <li>If the changes are not accepted, the repository administrators may indicate as a comment in the pull request feedback  and modifications needed to accept the pull request. However, the pull request may be desestimated to which it  will be closed and changes will not be incorporated.</li> </ul>"},{"location":"DevGuide/2_dev_guidelines/","title":"Developer Guidelines","text":"<p>The repository contains a devcontainer configuration compatible with VsCode. This configuration will create a docker container with all the necessary libraries and configurations to develop and execute the source code. </p>"},{"location":"DevGuide/2_dev_guidelines/#development-environment-setup","title":"\ud83d\udee0\ufe0f Development Environment Setup","text":"<p>This repository provides a docker dev-container with a system completely configured to execute the source code as well as useful libraries and tools for the development of the multimno software.  Thanks to the dev-container, it is guaranteed that all developers are developing/executing code in the same environment.</p> <p>The dev-container specification is all stored inside the <code>.devcontainer</code> directory.</p>"},{"location":"DevGuide/2_dev_guidelines/#configuring-the-docker-container","title":"Configuring the docker container","text":"<p>Configuration parameters for building the docker image and for creating the docker container are specified in the configuration file <code>.devcontainer/.env</code> file. This file contains user specific container configuration (like the path in the host machine to the data). As this file will change  for each developer it is ignored for the git version control and must be created after cloning the repository.</p> <p>A <code>template file</code> is stored in this repostiory. You can use this file as a baseline copying it to the <code>.devcontainer</code> directory.</p> <pre><code>cp resources/templates/dev_container_template.env .devcontainer/.env\n</code></pre> <p>Please edit the <code>.devcontainer/.env</code> file <code>Docker run parameters</code> section  with your preferences:</p> <pre><code># ------------------- Docker Build parameters -------------------\nPYTHON_VERSION=3.11 # Python version.\nJDK_VERSION=17 # Java version.\nSPARK_VERSION=3.4.1 # Spark/Pyspark version.\nSCALA_VERSION=2.12 # Spark dependency.\nSEDONA_VERSION=1.5.1 # Sedona\nGEOTOOLS_WRAPPER_VERSION=28.2 # Sedona dependency\n\n# ------------------- Docker run parameters -------------------\nCONTAINER_NAME=multimno_dev_container # Container name.\nDATA_DIR=../sample_data # Path of the host machine to the data to be used within the container.\nSPARK_LOGS_DIR=../sample_data/logs # Path of the host machine to where the spark logs will be stored.\nJL_PORT=8888 # Port of the host machine to deploy a jupyterlab.\nJL_CPU=4 # CPU cores of the container.\nJL_MEM=16g # RAM of the container.\n</code></pre>"},{"location":"DevGuide/2_dev_guidelines/#starting-the-dev-environment","title":"Starting the dev environment","text":""},{"location":"DevGuide/2_dev_guidelines/#vscode","title":"VsCode","text":"<p>Prerequisite: Dev-Container/Docker extension</p> <p>In VsCode: F1 -&gt; Dev Containers: Rebuild and Reopen in container</p>"},{"location":"DevGuide/2_dev_guidelines/#manual","title":"Manual","text":""},{"location":"DevGuide/2_dev_guidelines/#docker-image-creation","title":"Docker image creation","text":"<p>Execute the following command: <pre><code>docker compose -f .devcontainer/docker-compose.yml --env-file=.devcontainer/.env build\n</code></pre></p>"},{"location":"DevGuide/2_dev_guidelines/#docker-container-creation","title":"Docker container creation","text":"<p>Create a container and start a shell session in it with the commands: <pre><code>docker compose -f .devcontainer/docker-compose.yml --env-file=.devcontainer/.env up -d\ndocker exec -it multimno_dev_container bash\n</code></pre></p>"},{"location":"DevGuide/2_dev_guidelines/#stopping-the-dev-environment","title":"Stopping the dev environment","text":""},{"location":"DevGuide/2_dev_guidelines/#vscode_1","title":"VsCode","text":"<p>Closing VsCode will automatically stop the devcontainer.</p>"},{"location":"DevGuide/2_dev_guidelines/#manual_1","title":"Manual","text":"<p>Exit the terminal with:</p> <p>Ctrl+D or writing <code>exit</code></p>"},{"location":"DevGuide/2_dev_guidelines/#deleting-the-dev-environment","title":"Deleting the dev environment","text":"<p>Delete the container created with: <pre><code>docker compose -f .devcontainer/docker-compose.yml --env-file=.devcontainer/.env down\n</code></pre></p>"},{"location":"DevGuide/2_dev_guidelines/#execution","title":"\ud83d\udc0e Execution","text":""},{"location":"DevGuide/2_dev_guidelines/#launching-a-single-component","title":"Launching a single component","text":"<p>In a terminal execute the command:  </p> <pre><code>spark-submit multimno/main.py &lt;component_id&gt; &lt;path_to_general_config&gt; &lt;path_to_component_config&gt; \n</code></pre> <p>Example:  </p> <pre><code>spark-submit multimno/main.py SyntheticEvents pipe_configs/configurations/general_config.ini pipe_configs/configurations/synthetic_events/synth_config.ini \n</code></pre>"},{"location":"DevGuide/2_dev_guidelines/#launching-a-pipeline","title":"Launching a pipeline","text":"<p>In a terminal execute the command:  </p> <pre><code>python multimno/orchestrator.py &lt;pipeline_json_path&gt;\n</code></pre> <p>Example:  </p> <pre><code>python multimno/orchestrator.py pipe_configs/pipelines/pipeline.json \n</code></pre>"},{"location":"DevGuide/2_dev_guidelines/#monitoringdebug","title":"\ud83d\udd0d Monitoring/Debug","text":""},{"location":"DevGuide/2_dev_guidelines/#launching-a-spark-history-server","title":"Launching a spark history server","text":"<p>The history server will access SparkUI logs stored at the path ${SPARK_LOGS_DIR} defined in the <code>.devcontainer/.env</code> file.</p> <p>Starting the history server <pre><code>start-history-server.sh \n</code></pre> Accesing the history server * Go to the address http://localhost:18080</p>"},{"location":"DevGuide/2_dev_guidelines/#style","title":"\ud83e\udeb6 Style","text":""},{"location":"DevGuide/2_dev_guidelines/#coding-style","title":"Coding style","text":"<p>The code shall follow the standard PEP 8 which is the coding style proposed for writing clean, readable, and maintainable Python code.  It was created to promote consistency in Python code and make it easier for developers to collaborate on projects. </p> <p>PEP 8 official guide</p>"},{"location":"DevGuide/2_dev_guidelines/#docstring-style","title":"Docstring style","text":"<p>The docstrings written in the code shall follow the Google Docstrings style. Adhering  to a unique docstring style  guarantees consistency within software development in a project. Google Docstrings are the most popular convention for  docstrings which facilitates readability and collaboration in open-source projects. </p> <p>Google Docstrings official guide</p>"},{"location":"DevGuide/2_dev_guidelines/#code-cleaning","title":"\ud83e\uddfc Code cleaning","text":"<p>Developed code shall be formatted and jupyter notebooks shall be cleaned of outputs to guarantee consistency and reduce  unnecessary differences between commits.</p>"},{"location":"DevGuide/2_dev_guidelines/#code-linting","title":"Code Linting","text":"<p>The python code generated shall be formatted with black. For formatting all source code execute the following command:</p> <pre><code>black -l 120 multimno tests/test_code/\n</code></pre>"},{"location":"DevGuide/2_dev_guidelines/#clean-jupyter-notebooks","title":"Clean jupyter notebooks","text":"<pre><code>find ./notebooks/ -type f -name \\*.ipynb | xargs jupyter nbconvert --clear-output --inplace\n</code></pre>"},{"location":"DevGuide/2_dev_guidelines/#code-security-scan","title":"Code Security Scan","text":"<pre><code>bandit -r multimno\n</code></pre>"},{"location":"DevGuide/2_dev_guidelines/#testing","title":"\ud83e\uddea Testing","text":""},{"location":"DevGuide/2_dev_guidelines/#launch-tests","title":"Launch Tests","text":""},{"location":"DevGuide/2_dev_guidelines/#manual_2","title":"Manual","text":"<pre><code>pytest tests/test_code/multimno\n</code></pre>"},{"location":"DevGuide/2_dev_guidelines/#vscode_2","title":"VsCode","text":"<p>1) Open the test view at the left panel.  2) Launch tests.</p>"},{"location":"DevGuide/2_dev_guidelines/#generate-test-coverage-reports","title":"Generate test &amp; coverage reports","text":"<p>Launch the command</p> <pre><code>pytest --cov-config=tests/.coveragerc \\\n    --cov-report=\"html:docs/autodoc/coverage\" \\\n    --cov=multimno --html=docs/autodoc/test_report.md \\\n    --self-contained-html tests/test_code/multimno\n</code></pre> <p>Test reports will be stored in the dir: <code>docs/autodoc</code></p>"},{"location":"DevGuide/2_dev_guidelines/#see-coverage-in-ide-vscode-extension","title":"See coverage in IDE (VsCode extension)","text":"<p>1) Launch tests with coverage to generate the coverage report (xml) <pre><code>pytest --cov-report=\"xml\" --cov=multimno tests/test_code/multimno\n</code></pre> 1) Install the extension: Coverage Gutters 2) Right click and select Coverage Gutters: Watch</p> <p>Note: You can see the coverage percentage at the bottom bar</p>"},{"location":"DevGuide/2_dev_guidelines/#code-documentation","title":"\ud83d\udcc4 Code Documentation","text":""},{"location":"DevGuide/2_dev_guidelines/#documentation-server-debugmkdocs","title":"Documentation server Debug(Mkdocs)","text":"<p>A code documentation can be deployed using mkdocs backend. </p> <p>1) Create documentation (This will launch all tests) <pre><code>./resources/scripts/generate_docs.sh\n</code></pre> 2) Launch doc server</p> <p><pre><code>mkdocs serve\n</code></pre> and navigate to the address: http://127.0.0.1:8000</p>"},{"location":"DevGuide/2_dev_guidelines/#documentation-deploy-mike","title":"Documentation deploy (mike)","text":"<p>Set <code>latest</code> as default version <pre><code>mike set-default --push latest\n</code></pre></p> <p>Deploy a version of the documentation with:</p> <pre><code>mike deploy --push --update-aliases &lt;version&gt; latest\n</code></pre> <p>Example:</p> <pre><code>mike deploy --push --update-aliases 0.2 latest\n</code></pre>"},{"location":"UserManual/","title":"User Manual","text":"<p>This document presents the user manual of the multimno software. Three sections are included: - Configuration: Section explaining the configuration values for all the components of the pipeline. - Setup: Section explaining how to install and deploy the software. - Execution: Section explaining how to execute the software.  </p>"},{"location":"UserManual/execution/","title":"Execution","text":"<p>The multimno software is a python application that launches a single component with a given configuration.  This atomic design allows the application to be integrated with multiple orchestration software. At the moment a python script called <code>orchestrator_multimno.py</code> is provided which will execute a pipeline of components sequentially using <code>spark-submit</code> commands.</p>"},{"location":"UserManual/execution/#single-component-execution","title":"Single component execution","text":"<p>The entrypoint of the application is a main.py which receives the following positional parameters: - component_id: Id of the component that will be launched. - general_config_path: Path to the general configuration file of the application. - component_config_path: Path to the component configuration file.</p> <pre><code>multimno/main.py &lt;component_id&gt; &lt;general_config_path&gt; &lt;component_config_path&gt;\n</code></pre> <p>Example:</p> <pre><code>multimno/main.py InspireGridGeneration pipe_configs/configurations/general_config.ini pipe_configs/configurations/grid/grid_generation.ini\n</code></pre>"},{"location":"UserManual/execution/#pipeline-execution","title":"Pipeline execution","text":"<p>For executing a pipeline the <code>orchestrator_multimno.py</code> script shall be used which takes a path to a json file as its only parameter. This json file, shall be defined as a file that glues all the configuration files  and defines the execution order of the components. The structure is as follows:</p> <ul> <li>general_config_path: Path to the general configuration file</li> <li>spark_submit_args: List containing arguments that will be passed to the spark-submit command. It can be empty.</li> <li>pipeline: List containing the order in which the components will be executed. Each item is composed of the values:<ul> <li>component_id: Id of the component to be executed.</li> <li>component_config_path: Path to the component configuration file.</li> </ul> </li> </ul> <p>Example: <pre><code>{\n    \"general_config_path\": \"/opt/dev/pipe_configs/configurations/general_config.ini\",\n    \"spark_submit_args\": [\n        \"--master=spark://spark-master:7077\",\n        \"--packages=org.apache.sedona:sedona-spark-3.5_2.12:1.5.1,org.datasyslab:geotools-wrapper:1.5.1-28.2\"\n    ],\n    \"pipeline\": [\n        {\n            \"component_id\": \"SyntheticEvents\",\n            \"component_config_path\": \"/opt/dev/pipe_configs/configurations/synthetic_events/synth_config.ini\"\n        },\n        {\n            \"component_id\": \"EventCleaning\",\n            \"component_config_path\": \"/opt/dev/pipe_configs/configurations/event/event_cleaning.ini\"\n        }\n    ]\n}\n</code></pre></p> <p>Configuration for executing a demo pipeline is given in the file: <code>pipe_configs/pipelines/pipeline.json</code> This file contains the order of the execution of the pipeline components and references to its configuration files.</p> <pre><code>./orchestrator_multimno.py pipe_configs/pipelines/pipeline.json\n</code></pre> <p>This demo will process MNO Event &amp; Network data cleaning it. At the same time it will generate quality metrics in both of these processes. Then, it will process the cleaned data until the generation of the continuous time  segmentation and daily permanence score indicators.</p> <p>If using the docker setup, all data will be stored under the path <code>/opt/data/lakehouse</code>. </p>"},{"location":"UserManual/setup_guide/","title":"Setup Guide","text":"<p>The multimno software is a python application using the PySpark library to harness the power of Apache Spark, a fast and general-purpose cluster-computing system. PySpark provides an interface for Apache Spark in Python, enabling developers to utilize Spark's high-level APIs and distributed computing capabilities while working in the Python programming language. The Spark framework is critical to this application as it handles the distribution of data and computation across the cluster, ensures fault tolerance, and optimizes execution for performance gains. Deployment of a PySpark application can be done on a single node (local mode), where Spark runs on a single machine for simplicity and ease of development or testing. Alternatively, for production and scaling, it can be deployed on a cluster (cluster mode) comprising multiple machines, either on-premises or in the cloud, where Spark distributes tasks across the nodes, allowing for parallel processing and efficient handling of large-scale data workloads. There are two ways of setting up a system for executing the source code:   1) Building the docker image provided. (Recommended for local executions)   2) Installing and setting up all required system libraries.  </p>"},{"location":"UserManual/setup_guide/#docker-setup","title":"Docker setup","text":"<p>A Dockerfile is provided to build a docker image with all necessary dependencies for the code execution.</p>"},{"location":"UserManual/setup_guide/#installing-docker-software","title":"Installing docker software","text":"<p>To use the docker image it is necessary to have the docker engine installed. Please follow the official docker  guide to set it up in your system: -  Official guide: Click here</p>"},{"location":"UserManual/setup_guide/#docker-image-creation","title":"Docker image creation","text":"<p>Execute the following command: <pre><code>docker build -t multimno:1.0-prod --target=multimno-prod .\n</code></pre></p>"},{"location":"UserManual/setup_guide/#docker-container-creation","title":"Docker container creation","text":""},{"location":"UserManual/setup_guide/#run-an-example-pipeline-within-a-container","title":"Run an example pipeline within a container","text":"<pre><code>docker run --rm --name=multimno-container -v \"${PWD}/sample_data:/opt/data\" -v \"${PWD}/pipe_configs:/opt/app/pipe_configs\" multimno:1.0-prod pipe_configs/pipelines/pipeline.json\n</code></pre>"},{"location":"UserManual/setup_guide/#run-a-container-in-interactive-mode","title":"Run a container in interactive mode","text":"<pre><code>docker run -it --name=multimno-container -v \"${PWD}/sample_data:/opt/data\" -v \"${PWD}/pipe_configs:/opt/app/pipe_configs\" --entrypoint=bash multimno:1.0-prod \n</code></pre> <p>After performing this command your shell(command-line) will be inside the container and you can perform  the execution steps to try out the code.</p>"},{"location":"UserManual/setup_guide/#clean-up","title":"Clean up","text":"<p>Exit the terminal with:</p> <p>Ctrl+D or writing <code>exit</code></p> <p>Delete the container created with: <pre><code>docker rm multimno-container\n</code></pre></p> <p>Delete the docker image with: <pre><code>docker rmi multimno:1.0-prod\n</code></pre></p>"},{"location":"UserManual/setup_guide/#docker-lite-version","title":"Docker Lite version","text":"<p>As the multimno software is a python application designed to be executed in a Spark cluster, a lightweight Dockerfile called <code>Dockerfile-lite</code> is given for execution of the software in existing Spark clusters.</p> <p>This docker image contains only minimum requirements to launch the application against an existing spark cluster. The image is a ubuntu:22.04 with python 3.10, jdk 17 and the required python dependencies.</p>"},{"location":"UserManual/setup_guide/#build-lite-image","title":"Build lite image","text":"<p>Execute the following command: <pre><code>docker build -t multimno_lite:1.0 -f ./Dockerfile-lite .\n</code></pre></p>"},{"location":"UserManual/setup_guide/#create-lite-container","title":"Create lite container","text":"<pre><code>docker run -it --name=multimno-lite-container -v \"${PWD}/pipe_configs:/opt/app/pipe_configs\" multimno_lite:1.0 bash\n</code></pre>"},{"location":"UserManual/setup_guide/#configuration","title":"Configuration","text":"<p>spark-submit args</p> <p>As explained in the execution guide, the entrypoint for the pipeline execution: <code>orchestrator_multimno.py</code>, performs <code>spark-submit</code> commands. To define <code>spark-submit</code> arguments edit the <code>spark_submit_args</code> variable in the pipeline.json.</p> <ul> <li>Spark submit documentation: https://spark.apache.org/docs/latest/submitting-applications.html</li> </ul> <p>Spark Configuration</p> <p>Edit the <code>[Spark]</code> section in the general_configuration file to define Spark session configuration parameters.</p> <ul> <li>Spark configuration documentation: https://spark.apache.org/docs/latest/configuration.html</li> </ul> <p>Python Version A requirement of a pyspark application is that the python version must be alligned for all the cluster. As the Dockerfile-Lite uses python3.10 the Spark cluster must have this python version alligned.</p> <ul> <li>Python package management: https://spark.apache.org/docs/latest/api/python/user_guide/python_packaging.html</li> </ul> <p>Python dependencies The application uses Apache Sedona and so it will need the Sedona jars as well as python dependencies installed in the Spark cluster. Please refer to the Apache Sedona official documentation: </p> <ul> <li> <p>Python setup: https://sedona.apache.org/1.5.1/setup/install-python/</p> </li> <li> <p>Spark Cluster: https://sedona.apache.org/1.5.1/setup/cluster/</p> </li> </ul> <p>Lite Configuration example An example of configuration of an execution with the lite image is given in the files: <code>pipe_configs/pipelines/test_production.json</code> and <code>pipe_configs/configurations/general_config_production.ini</code></p>"},{"location":"UserManual/setup_guide/#execution-lite","title":"Execution lite","text":"<p>Execute as defined in the execution guide.</p>"},{"location":"UserManual/setup_guide/#software-setup","title":"Software setup","text":"<p>The software is aimed to be executed in a Linux OS. It is recommended to use Ubuntu 22.04 LTS but these steps should also work in MAC OS 12.6(or superior) and in Windows 11 with WSL2 and setting up as the distro of WSL Ubuntu 22.04.</p>"},{"location":"UserManual/setup_guide/#install-system-libs","title":"Install system libs","text":"<pre><code>sudo apt-get update &amp;&amp; \\\n    sudo apt-get install -y --no-install-recommends \\\n      sudo \\\n      openjdk-17-jdk \\\n      build-essential \\\n      software-properties-common \\\n      openssh-client openssh-server \\\n      gdal-bin \\\n      libgdal-dev \\\n      ssh\n</code></pre>"},{"location":"UserManual/setup_guide/#download-spark-source-code","title":"Download Spark source code","text":"<pre><code>SPARK_VERSION=3.5.1\nexport SPARK_HOME=${SPARK_HOME:-\"/opt/spark\"}\nmkdir -p ${SPARK_HOME}\ncd ${SPARK_HOME}\nwget https://dlcdn.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.tgz \\\n  &amp;&amp; tar -xvzf spark-${SPARK_VERSION}-bin-hadoop3.tgz --directory /opt/spark --strip-components 1 \\\n  &amp;&amp; rm -rf spark-${SPARK_VERSION}-bin-hadoop3.tgz\nexport PATH=\"${PATH}:$SPARK_HOME/bin:$SPARK_HOME/sbin\"\n</code></pre>"},{"location":"UserManual/setup_guide/#install-python-requirements","title":"Install python requirements","text":"<pre><code>pip install --upgrade pip\npip install -r resources/requirements/requirements.in\npip install -r resources/requirements/dev_requirements.in\n</code></pre> <p>You can use a virtualenv for avoiding conflicts with other python libraries.</p>"},{"location":"UserManual/setup_guide/#install-spark-dependencies","title":"Install Spark dependencies","text":"<pre><code>SCALA_VERSION=2.12\nSEDONA_VERSION=1.5.1\nGEOTOOLS_WRAPPER_VERSION=28.2\nchmod +x ./resources/scripts/install_sedona_jars.sh\n./resources/scripts/install_sedona_jars.sh ${SPARK_VERSION} ${SCALA_VERSION} ${SEDONA_VERSION} ${GEOTOOLS_WRAPPER_VERSION} \n</code></pre>"},{"location":"UserManual/setup_guide/#setup-spark-configuration","title":"Setup spark configuration","text":"<pre><code>cp conf/spark-defaults.conf \"$SPARK_HOME/conf/spark-defaults.conf\"\ncp conf/log4j2.properties \"$SPARK_HOME/conf/log4j2.properties\"\n</code></pre>"},{"location":"UserManual/configuration/","title":"Configuration","text":""},{"location":"UserManual/configuration/#configuration-section-structure","title":"Configuration Section Structure","text":"<p>The configuration section is divided in four sections:  </p> <ul> <li>Pipeline: Main pipeline components to generate indicators from Mno Data.  </li> <li>Optional: Components that are optional to the pipeline execution. They enrich some data objects which may lead to quality improvements of the final results but are not essential to the pipeline.  </li> <li>QualityWarnings: Components that analyze quality metrics of data objects.  </li> <li>SyntheticMnoData: Components that are used to create synthetic Mno Data. Mainly used for testing purposes.  </li> </ul>"},{"location":"UserManual/configuration/#configuration-files-used","title":"Configuration files used","text":"<p>The multimno application requires from multiple configuration files.  </p> <ul> <li>One general configuration file describing general parameters like file paths, logging, spark and  common values for all components in the pipeline.  </li> <li>A configuration file per each component of the pipeline with configuration parameters exclusive to the component. Values defined in these files can override values defined in the general configuration file.</li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/0_general_configuration/","title":"General Configuration","text":"<p>The general configuration file contains transversal settings for all the pipeline. It is an INI file composed of four main sections:</p> <p>Logging: Section which contains the logger settings.</p> <p>Paths: Section containing the definition of all paths to be used.</p> <p>Spark: Apache Spark configuration values. Parameters defined in this section will be used to create the spark session. Values supported are based in: Configuration - Spark 3.5.1 Documentation (apache.org). As an exception the parameter session_name has been included which identifies the name of the spark session.</p> <p>General: Parameters that may be useful for all components in the pipeline. </p> <p>Example:</p> <pre><code>[Logging]\nlevel = INFO\nformat= %(asctime)-20s %(message)s\ndatefmt = %y-%m-%d %H:%M:%S\n\n[Paths]\n# Main paths\nhome_dir = /opt/data\nlakehouse_dir = ${Paths:home_dir}/lakehouse\n# Lakehouse\nlanding_dir = ${Paths:lakehouse_dir}/landing\nbronze_dir = ${Paths:lakehouse_dir}/bronze\nsilver_dir = ${Paths:lakehouse_dir}/silver\ngold_dir = ${Paths:lakehouse_dir}/gold\n\n[Spark]\n# parameters in this section are passed to Spark except session_name \nsession_name = MultiMNO\nspark.master = local[*]\nspark.driver.host = localhost\n\n[General]\nstart_date = 2023-01-01\nend_date = 2023-01-30\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/10_continuous_time_segmentation/","title":"ContinuousTimeSegmentation Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>time_segments.ini</code>. In <code>general_config.ini</code> to execute the component specify all paths to its four corresponding data objects (input + output). Example: </p> <pre><code>[Paths.Silver]\nevent_data_silver_flagged = ${Paths:silver_dir}/mno_events_flagged\ncell_intersection_groups_data_silver = ${Paths:silver_dir}/cell_intersection_groups\ntime_segments_silver = ${Paths:silver_dir}/time_segments\n</code></pre> <p>In time_segments.ini parameters are as follows: </p> <ul> <li> <p>data_period_start - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-01), the date from which start Event Cleaning</p> </li> <li> <p>data_period_end - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-05), the date till which perform Event Cleaning</p> </li> <li> <p>is_first_run - boolean, if True, the component won't use previously calculated time segments. If False, the component will use last calculated time segment per device.</p> </li> <li> <p>event_error_flags_to_include - list of integers, the list of error flags that should be included in the time segments processing. Default value is [0], so only events with no errors are included.</p> </li> <li> <p>min_time_stay_s - integer, the minimum dwell time in seconds for a time segments to be considered as a \"stay\". Default value is 15 minutes.</p> </li> <li> <p>max_time_missing_stay_s - integer, maximum time difference between events to be considered a \u201cstay\u201d. If larger, the time segment will be marked \u201cunknown\u201d. Default value is 12 hours to support devices being offline at home or work addresses.</p> </li> <li> <p>max_time_missing_move_s - integer, maximum time difference between events to be considered a \u201cmove\u201d. If larger, the time segment will be marked \u201cunknown\u201d. Default value is 2 hours.</p> </li> <li> <p>pad_time_s - integer, half the size of an isolated time segment: between two \u201cunknowns\u201d time segments. It expands the isolated event in time, by \u201cpadding\u201d from the \u201cunknown\u201d time segments on both sides. Default value is 5 minutes.</p> </li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/10_continuous_time_segmentation/#configuration-example","title":"Configuration example","text":"<pre><code>[ContinuousTimeSegmentation]\ndata_period_start = 2023-01-01\ndata_period_end = 2023-01-05\n\nis_first_run = true\nevent_error_flags_to_include = [0]\n\nmin_time_stay_s = 900\nmax_time_missing_stay_s = 43200\nmax_time_missing_move_s = 7200\npad_time_s = 300\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/11_present_population_estimation/","title":"DailyPermanenceScore Configuration","text":"<p>To initialise and run the component two configs are used -  <code>general_config.ini</code> and component\u2019s config <code>present_population_estimation.ini</code>. In  <code>general_config.ini</code> all paths to the data objects used by DailyPermanenceScore component shall be specified. An example with the specified paths is shown below:</p> <pre><code>[Paths.Silver]\n...\ngrid_data_silver = ${Paths:silver_dir}/grid\ncell_connection_probabilities_data_silver = ${Paths:silver_dir}/cell_connection_probabilities_data_silver\nevent_data_silver_flagged = ${Paths:silver_dir}/mno_events_flagged\npresent_population_silver = ${Paths:silver_dir}/present_population\npresent_population_zone_silver = ${Paths:silver_dir}/present_population_zone\nzone_to_grid_map_silver = ${Paths:silver_dir}/zone_to_grid_map\n...\n</code></pre> <p>Below there is a description of the sub component\u2019s config  - <code>present_population_estimation.ini</code>. </p> <p>Parameters are as follows:</p> <p>Under  <code>[PresentPopulationEstimation]</code> config section: </p> <ul> <li>data_period_start - string, format should be \u201cyyyy-MM-dd HH:mm:ss\u201c (e.g. 2023-01-08 12:30:00). Determines when the first time point is generated.</li> <li>data_period_end - string, format should be \u201cyyyy-MM-dd HH:mm:ss\u201c (e.g. 2023-01-08 12:30:00). No time points can be generated after this time. A time point can be generated at this exact time.</li> <li>time_point_gap_s - integer, in seconds. Determines the interval between two time points. Starting from <code>data_period_start</code>, one time point is generated after each <code>time_point_gap_s</code> seconds until <code>data_period_end</code> is reached.</li> <li>nr_of_user_id_partitions - integer. Total number of user_id_modulo partitions. This should be equal to the number of partitions that user event data has been split into-  </li> <li>nr_of_user_id_partitions_per_slice - integer. Number of user_id_modulo partitions to process at one time. Should be adjusted to optimize between processing speed and memory usage limitations.</li> <li>tolerance_period_s - integer, in seconds. Determines the size of the temporal window of each time point. Only events within this distance from the time point are included in the results calculation of that point. </li> <li>max_iterations - integer. Maximum number of iteration allowed for the Bayesian process for each time point.</li> <li>min_difference_threshold - float. Minumum difference between Bayesian process prior and posterior population estimates needed to continue iterating the process.</li> <li>output_aggregation_level - string, value is either \"grid\" or \"zone\". Determines whether the final results are aggregated per grid tile or per zoning area. </li> <li>zoning_dataset_id - string. Name of the zoning data to use, has to match the <code>dataset_id</code> column in the grid to zone mapping dataset. Only needed when output_aggregation_level is \"zone\".</li> <li>zoning_hierarchical_level - integer. Level of hierarchial zoning to aggregate results to. The corresponding level from the <code>hierarchical_id</code> column in the grid to zone mapping dataset is used. Only needed when output_aggregation_level is \"zone\".</li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/11_present_population_estimation/#configuration-example-grid-level-aggregation","title":"Configuration example: grid-level aggregation","text":"<pre><code>[Logging]\nlevel = DEBUG\n\n[Spark]\nsession_name = PresentPopulationEstimationGrid\n\n[PresentPopulationEstimation]\ndata_period_start = 2023-01-01 00:00:00 # Starting bound when to create time points. The first time point is created at this timestamp. \ndata_period_end = 2023-01-02 00:00:00 # Ending bound when to create time points. No time points are generated later than this timestamp. A time point can happen to be generated on this timestamp, but this is not always the case.\ntime_point_gap_s = 43200 # space between consecutive time points\ntolerance_period_s = 3600 # Maximum allowed time difference for an event to be included in a time point\nnr_of_user_id_partitions = 128 # Total number of user_id_modulo partitions. TODO should be a global conf value\nnr_of_user_id_partitions_per_slice = 32 # Number of user_id_modulo partitions to process at one time\nmax_iterations = 20 # Number of iterations allowed for the Bayesian process\nmin_difference_threshold = 10000 # Minimum total difference between Bayesian process prior and posterior needed to continue processing \noutput_aggregation_level = grid # Supported values: \"grid\", \"zone\". Determines which level the results are aggregated to.\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/11_present_population_estimation/#configuration-example-zone-level-aggregation","title":"Configuration example: zone-level aggregation","text":"<pre><code>[Logging]\nlevel = DEBUG\n\n[Spark]\nsession_name = PresentPopulationEstimationZone\n\n[PresentPopulationEstimation]\ndata_period_start = 2023-01-01 00:00:00 # Starting bound when to create time points. The first time point is created at this timestamp. \ndata_period_end = 2023-01-02 00:00:00 # Ending bound when to create time points. No time points are generated later than this timestamp. A time point can happen to be generated on this timestamp, but this is not always the case.\ntime_point_gap_s = 43200 # space between consecutive time points\ntolerance_period_s = 3600 # Maximum allowed time difference for an event to be included in a time point\nnr_of_user_id_partitions = 128 # Total number of user_id_modulo partitions. TODO should be a global conf value\nnr_of_user_id_partitions_per_slice = 32 # Number of user_id_modulo partitions to process at one time\nmax_iterations = 20 # Number of iterations allowed for the Bayesian process\nmin_difference_threshold = 10000 # Minimum total difference between Bayesian process prior and posterior needed to continue processing \noutput_aggregation_level = zone # Supported values: \"grid\", \"zone\". Determines which level the results are aggregated to.\nzoning_dataset_id = zoning_01 # Name of zoning dataset. Only needed when output_aggregation_level is \"zone\".\nzoning_hierarchical_level = 3 # Level of hierarchial zoning to aggregate results to. Only needed when output_aggregation_level is \"zone\".\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/12_daily_permanence_score/","title":"DailyPermanenceScore Configuration","text":"<p>To initialise and run the component two configs are used -  <code>general_config.ini</code> and component\u2019s config <code>daily_permanence_score.ini</code>. In  <code>general_config.ini</code> all paths to the data objects used by DailyPermanenceScore component shall be specified. An example with the specified paths is shown below:</p> <pre><code>[Paths.Silver]\n...\nevent_data_silver_flagged = ${Paths:silver_dir}/mno_events_flagged\ncell_footprint_data_silver = ${Paths:silver_dir}/cell_footprint\ndaily_permanence_score_data_silver = ${Paths:silver_dir}/daily_permanence_score\n...\n</code></pre> <p>Below there is a description of the sub component\u2019s config  - <code>daily_permanence_score.ini</code>. </p> <p>Parameters are as follows:</p> <p>Under  <code>[DailyPermanenceScore]</code> config section: </p> <ul> <li> <p>data_period_start - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-08), the date from which start DailyPermanenceScore processing. Make sure events and cell footprint data are available for all the dates between data_period_start and data_period_end, as well as for the previous day to data_period_start and the posterior day to data_period_end. </p> </li> <li> <p>data_period_end - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-15), the date till which we will perform DailyPermanenceScore processing. Make sure events and cell footprint data are available for all the dates between data_period_start and data_period_end, as well as for the previous day to data_period_start and the posterior day to data_period_end. </p> </li> <li> <p>time_slot_number - integer, one of 24, 48, or 96. Number of equal-length time slots in which to divide each date for the daily permanence score calculation. The recommended value is 24, which results in 1-hour time slots. The values 48 and 96 result, respectively, in 30- and 15-minute time slots.</p> </li> <li> <p>max_time_thresh - integer, in seconds. In case of 2 consecutive events taking place in different cells, if the time difference between the events is lower than this threshold, then the both the first event's assigned end time and the second event's assigned start time will be equal to the average between the first and second event's timestamps. If the time difference between the 2 consecutive events is higher than this threshold, then the assigned end time for the first event will be equal to the first event's timestamp plus half the value of this threshold; and the assigned start time for the second event will be equal to the second event's timestamp minus half the value of this threshold. For the case of 2 consecutive events taking place in different cells, if the time difference between the events is higher than the corresponding threshold (either <code>max_time_thresh_day</code> or <code>max_time_thresh_night</code>), then the event timestamps are also extended half this value of <code>max_time_thresh</code>. Recommended value: 900 seconds (15 minutes).</p> </li> <li> <p>max_time_thresh_day - integer, in seconds. In case of 2 consecutive events in the same cell, if the time of at least one of the events is included in the \"day time\", i.e. from 9:00 to 22:59, then <code>max_time_thresh_day</code> is the threshold to be applied. If the time difference between the events is lower than this threshold, then the both the first event's assigned end time and the second event's assigned start time will be equal to the average between the first and second event's timestamps. Otherwise, the assigned end time for the first event will be equal to the first event's timestamp plus half the value of <code>max_time_thresh</code>; and the assigned start time for the second event will be equal to the second event's timestamp minus half the value of <code>max_time_thresh</code>. Recommended value: 7200 seconds (2 hours).</p> </li> <li> <p>max_time_thresh_night - integer, in seconds. In case of 2 consecutive events in the same cell, if the time of both events is included in the \"night time\", i.e. from 23:00 to 8:59, then <code>max_time_thresh_night</code> is the threshold to be applied. If the time difference between the events is lower than this threshold, then the both the first event's assigned end time and the second event's assigned start time will be equal to the average between the first and second event's timestamps. Otherwise, the assigned end time for the first event will be equal to the first event's timestamp plus half the value of <code>max_time_thresh</code>; and the assigned start time for the second event will be equal to the second event's timestamp minus half the value of <code>max_time_thresh</code>. Recommended value: 28800 seconds (8 hours).</p> </li> <li> <p>max_vel_thresh - float, in metres per second (m/s). In order to evaluate if an event corresponds to a \"move\", the speed between the previous event and the next event is calculated. If the speed exceeds <code>max_vel_thresh</code>, then the event is tagged as a move and will be discarded for daily permanence score calculation. Recommended value: 13.889 m/s (50 km/h).</p> </li> <li> <p>score_interval - integer, currently the only accepted value is 2. It indicates the number of integer values in which to discretise the stay duration in each tile and time slot for the daily permanence score calculation. Thus, if a user has stayed in a tile less than half of the duration of the time slot, it will be assigned the daily permanence score (DPS) value of 0, and if has stayed at least half of the duration, it is assigned the value 1.</p> </li> <li> <p>event_error_flags_to_include - set of integers, e.g. \"{0}\". It indicates the values of the \"error_flag\" column of the input event data that will be kept. Rows with \"error_flag\" values not included in this set will be discarded and will not be considered for any step of the daily permanence score component. Recommended value: {0}.</p> </li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/12_daily_permanence_score/#configuration-example","title":"Configuration example","text":"<pre><code>[Logging]\nlevel = DEBUG\n\n[Spark]\nsession_name = DailyPermanenceScore\n\n[DailyPermanenceScore]\ndata_period_start = 2023-01-02\ndata_period_end = 2023-01-02\n\ntime_slot_number = 24\nscore_interval = 2\n\nmax_time_thresh = 900  # 15 min\nmax_time_thresh_day = 7_200  # 2 h\nmax_time_thresh_night = 28_800  # 8 h\n\nmax_speed_thresh = 13.88888889  # 50 km/h\n\nevent_error_flags_to_include = {0}\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/13_midterm_permanence_score/","title":"MidtermPermanenceScore Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>midterm_permanence_score.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>...\n[Paths.Bronze]\nholiday_calendar_data_bronze = ${Paths:bronze_dir}/holiday_calendar\n\n...\n\n[Paths.Silver]\ncell_footprint_data_silver = ${Paths:silver_dir}/cell_footprint\ndaily_permanence_score_data_silver = ${Paths:silver_dir}/daily_permanence_score\nmidterm_permanence_score_data_siilver = ${Paths:silver_dir}/midterm_permanence_score\n...\n</code></pre> <p>The expected parameters in <code>midterm_permanence_score.ini</code> are as follows:  - start_month: string, with <code>YYYY-MM</code> format, indicating the first month for which mid-term permanence score and metrics are to be computed. All months between start_month and end_month, both inclusive, will be processed individually. Example: <code>2023-01</code>.  - end_month: string, with <code>YYYY-MM</code> format, indicating the last month for which mid-term permanence score and metrics are to be computed. All months between start_month and end_month, both inclusive, will be processed individually. Must be a month equal or later than start_month. Example: <code>2023-08</code>.  - before_regularity_days: positive integer, it represents the number of days previous to a month that will be loaded and used in the calculation of the mid-term regularity metrics by searching for the latest date in which a given user performed a stay in a given grid tile. Example: <code>7</code>.  - after_regularity_days: positive integer, it represents the number of days following a month that will be loaded and used in the calculation of the mid-term regularity metrics by searching for the earliest date in which a given user performed a stay in a given grid tile. Example: <code>7</code>.  - day_start_hour: integer between 0 and 23 (both inclusive) that marks the starting time of a given day in the mid-term analysis to be performed. Example: <code>4</code>. A value of <code>4</code> means that the time slots of the Daily Permanence Score contained between the 04:00 of day $D$ (inclusive) and the 04:00 of day $D+1$ (not inclusive) are considered to belong to day $D$.  - country_of_study: two-letter, upper-case string, marking the ISO Alpha 2 code of the country being studied, used to load the holiday dates of that country that are used to define the workdays and holidays day types. Example: <code>ES</code> (Spain).  - weekend_start: integer between 1 and 7, marks the first day of the week (inclusive) that belongs to the weekends day type. Monday is 1, Tuesday is 2, ..., and Sunday is 7. Example: <code>6</code>.  - weekend_end: integer between 1 and 7, marks the last day of the week (inclusive) that belongs to the weekends day type. Monday is 1, Tuesday is 2, ..., and Sunday is 7. Example: <code>7</code>.  - night_time_start: string, <code>HH:MM</code> format, marking the starting hour and minutes (inclusive) of the night_time time interval. Only allowed values for the minutes <code>MM</code> are <code>00</code>, <code>15</code>, <code>30</code>, or <code>45</code>. The time interval limits must be compatible with the time slot limits and durations of the Daily Permanence Score data used. See the Time interval additional information for additional restrictions of this parameter. Example: <code>18:00</code>.  - night_time_end: string, <code>HH:MM</code> format, marking the ending hour and minutes (exclusive) of the night_time time interval. Only allowed values for the minutes <code>MM</code> are <code>00</code>, <code>15</code>, <code>30</code>, or <code>45</code>. The time interval limits must be compatible with the time slot limits and durations of the Daily Permanence Score data used. See the Time interval additional information for additional restrictions of this parameter. Example: <code>08:00</code>.  - working_hours_start: string, <code>HH:MM</code> format, marking the starting hour and minutes (inclusive) of the working_hours time interval. Only allowed values for the minutes <code>MM</code> are <code>00</code>, <code>15</code>, <code>30</code>, or <code>45</code>. The time interval limits must be compatible with the time slot limits and durations of the Daily Permanence Score data used. See the Time interval additional information for additional restrictions of this parameter. Example: <code>07:30</code>.  - working_hours_end: string, <code>HH:MM</code> format, marking the ending hour and minutes (exclusive) of the working_hours time interval. Only allowed values for the minutes <code>MM</code> are <code>00</code>, <code>15</code>, <code>30</code>, or <code>45</code>. The time interval limits must be compatible with the time slot limits and durations of the Daily Permanence Score data used. See the Time interval additional information for additional restrictions of this parameter. Example: <code>17:30</code>.  - evening_time_start: string, <code>HH:MM</code> format, marking the starting hour and minutes (inclusive) of the evening_time time interval. Only allowed values for the minutes <code>MM</code> are <code>00</code>, <code>15</code>, <code>30</code>, or <code>45</code>. The time interval limits must be compatible with the time slot limits and durations of the Daily Permanence Score data used. See the Time interval additional information for additional restrictions of this parameter. Example: <code>18:00</code>.  - evening_time_end: string, <code>HH:MM</code> format, marking the ending hour and minutes (exclusive) of the evening_time time interval. Only allowed values for the minutes <code>MM</code> are <code>00</code>, <code>15</code>, <code>30</code>, or <code>45</code>. The time interval limits must be compatible with the time slot limits and durations of the Daily Permanence Score data used. See the Time interval additional information for additional restrictions of this parameter. Example: <code>21:00</code>.  - period_combinations: dictionary indicating the combinations of sub-monthly and sub-daily periods (i.e., day types and time intervals) that are to be considered for the mid-term permanence score and metrics computation, for each month betwen start_month and end_month. The structure is as follows (a full example can be in Configuration example):    - The keys of the dictionary must be one of the possible day type values surrounded by quotes:      - <code>\"all\"</code>: every day of the month.      - <code>\"workdays\"</code>: every day of the month that does not belong to the weekend and is not a holiday in the country of study.      - <code>\"holidays\"</code>: every day of the month that is a holiday in the country of study.      - <code>\"weekends\"</code>: every day of the month that is part of the weekend.      - <code>\"mondays\"</code>: every Monday of the month.      - <code>\"tuesdays\"</code>: every Tuesday of the month.      - <code>\"wednesdays\"</code>: every Wednesday of the month.      - <code>\"thursdays\"</code>: every Thursday of the month.      - <code>\"fridays\"</code>: every Friday of the month.      - <code>\"saturdays\"</code>: every Saturday of the month.      - <code>\"sundays\"</code>: every Sunday of the month.    - The value assigned to a key must be a non-empty list surrounded by square brackets containing some of the possible time interval values:      - <code>\"all\"</code>: all time slots of the day.      - <code>\"night_time\"</code>: time slots of the day contained in the hour interval defined by night_time_start and night_time_end.      - <code>\"working_hours\"</code>: time slots of the day contained in the hour interval defined by working_hours_start and working_hours_end.      - <code>\"evening_time\"</code>: time slots of the day contained in the hour interval defined by evening_time_start and evening_time_end.</p>"},{"location":"UserManual/configuration/1_Pipeline/13_midterm_permanence_score/#time-interval-additional-information","title":"Time interval additional information","text":"<p>There are some nuances and restrictions related to the definition of the different time intervals and the day_start_hour parameter:  - By definition, a time interval will belong to the date that contains its start hour. See the following example:    - Suppose that day_start_hour has been set to <code>4</code>, so that the day \"starts\" at 04:00.    - Suppose that night_time_start has been set to <code>20:15</code> and night_time_end has been set to <code>06:30</code>. Then, the night time interval starts at 20:15 in the evening of some day $D$, crosses midnight, and ends at 06:30 in the morning of the following day $D+1$.    - In this case, the start of the time interval, 20:15, is between the 04:00 of day $D$ and the 04:00 of day $D+1$. Then, this time interval will be assigned to the date $D$.</p> <ul> <li>The following configuration is not allowed for the time intervals night_time, working_hours, and evening_time:</li> <li>day_start_hour is different from <code>0</code>, and</li> <li>the start of the interval is between 00:00 (exclusive) and day_start_hour (exclusive), and</li> <li> <p>the end of the interval is between 00:00 (exclusive) and the start of the interval (exclusive).</p> <p>Example of a not-allowed configuration under this restriction:    - day_start_hour is <code>4</code>, that is, 04:00.    - night_time_start is <code>03:00</code>.    - night_time_end is <code>01:15</code>.  - The following configuration is not allowed for the time intervals working_hours and evening_time:    - the start of the interval is between 00:00 (inclusive) and day_start_hour (exclusive)    - the end of the interval is later than day_start_hour (exclusive).</p> <p>Example of a not-allowed configuration under this restriction: - day_start_hour is <code>4</code>, that is, 04:00. - working_hours_start is <code>03:00</code>. - working_hours_end is <code>18:00</code>.</p> </li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/13_midterm_permanence_score/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = MidtermPermanenceScore\n\n[MidtermPermanenceScore]\n\nstart_month = 2023-02\nend_month = 2023-03\n\nbefore_regularity_days = 7\nafter_regularity_days = 7\nday_start_hour = 4  # at what time the day starts, e.g. day gos from 4AM Mon to 4AM Tue\n\ncountry_of_study = ES\n\nnight_time_start = 18:00\nnight_time_end = 08:00\n\nworking_hours_start = 08:00\nworking_hours_end = 17:00\n\nevening_time_start = 16:00\nevening_time_end = 22:00\n\nweekend_start = 6\nweekend_end = 7\n\nperiod_combinations = {\n    \"all\": [\"all\", \"night_time\", \"evening_time\", \"working_hours\"],\n    \"workdays\": [\"night_time\", \"working_hours\"],\n    \"holidays\": [\"all\", \"night_time\"],\n    \"weekends\": [\"all\", \"night_time\"],\n    \"mondays\": [\"all\"]\n    }\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/14_longterm_permanence_score/","title":"LongtermPermanenceScore Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>longterm_permanence_score.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Silver]\n...\nmidterm_permanence_score_data_siilver = ${Paths:silver_dir}/midterm_permanence_score\nlongterm_permanence_score_data_siilver = ${Paths:silver_dir}/longterm_permanence_score\n...\n</code></pre> <p>The expected parameters in <code>longterm_permanence_score.ini</code> are as follows:  - start_month: string, in <code>YYYY-MM</code> format, it indicates the first month to be included in the long-term permanence score computations and analyses. All months between start_month and end_month, both inclusive, will be considered in the long-term analyses. Example: <code>2023-01</code>.  - end_month: string, in <code>YYYY-MM</code> format, it indicates the last month to be included in the long-term permanence score computations and analyses. All months between start_month and end_month, both inclusive, will be considered in the long-term analyses. Example: <code>2023-06</code>.  - winter_months: comma-separated sequence of integers between 1 and 12, they indicate which months of the year belong to the winter season. 1 represents January, 2 represents February, ..., and 12 represents December. Example: <code>12, 1, 2</code>.  - spring_months: comma-separated sequence of integers between 1 and 12, they indicate which months of the year belong to the spring season. 1 represents January, 2 represents February, ..., and 12 represents December. Example: <code>3, 4, 5</code>.  - summer_months: comma-separated sequence of integers between 1 and 12, they indicate which months of the year belong to the summer season. 1 represents January, 2 represents February, ..., and 12 represents December. Example: <code>6, 7, 8</code>.  - autumn_months: comma-separated sequence of integers between 1 and 12, they indicate which months of the year belong to the autumn season. 1 represents January, 2 represents February, ..., and 12 represents December. Example: <code>9, 10, 11</code>.  - period_combinations: dictionary indicating the combinations of sub-yearly, sub-monthly and sub-daily periods (i.e., seasons, day types and time intervals) to consider in the long-term analyses. Each combination will result in the computation of indicators resulting from aggregating the data of months belonging to the particular season, day type and time interval defined by the combination. The structure is as follows (a full example can be found in Configuration example):     - The keys of the dictionary must be one of the possible season values:         - <code>\"all\"</code>: every month between start_month and end_month, both inclusive, are considered in the aggregation.         - <code>\"winter\"</code>: every month included in the winter_months list between start_month and end_month, both inclusive, are considered in the aggregation. If this key appears then the winter_months list must contain at least one month.         - <code>\"spring\"</code>: every month included in the spring_months list between start_month and end_month, both inclusive, are considered in the aggregation. If this key appears then the spring_months list must contain at least one month.         - <code>\"summer\"</code>: every month included in the summer_months list between start_month and end_month, both inclusive, are considered in the aggregation. If this key appears then the summer_months list must contain at least one month.         - <code>\"autumn\"</code>: every month included in the autumn_months list between start_month and end_month, both inclusive, are considered in the aggregation. If this key appears then the autumn_months list must contain at least one month.     - The values assigned to a key is, in turn, another dictionary with the following structure:         - The keys of this dictionary must be one of the possible day type values surrounded by quotes:             - <code>\"all\"</code>             - <code>\"workdays\"</code>             - <code>\"holidays\"</code>             - <code>\"weekends\"</code>             - <code>\"mondays\"</code>             - <code>\"tuesdays\"</code>             - <code>\"wednesdays\"</code>             - <code>\"thursdays\"</code>             - <code>\"fridays\"</code>             - <code>\"saturdays\"</code>             - <code>\"sundays\"</code>         - The value assigned to a key must be a non-empty list surrounded by square brackets containing some of the possible time interval values:             - <code>\"all\"</code>             - <code>\"night_time\"</code>             - <code>\"working_hours\"</code>             - <code>\"evening_time\"</code></p> <pre><code>The **period_combinations** example that appears in the [Configuration example](#configuration-example) would result in the computation of the long-term permanence metrics for the following combinations:\n\n| season | day type | time interval   |\n|--------|----------|-----------------|\n| all    | all      | all             |\n| all    | all      | night_time      |\n| winter | all      | all             |\n| spring | all      | all             |\n| spring | workdays | working_hours   |\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/14_longterm_permanence_score/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = LongtermPermanenceScore\n\n[LongtermPermanenceScore]\nstart_month = 2023-01\nend_month = 2023-06\n\nwinter_months = 12, 1, 2\nspring_months = 3, 4, 5\nsummer_months = 6, 7, 8\nautumn_months = 9, 10, 11\n\nperiod_combinations = {\n    \"all\": {\n        \"all\": [\"all\", \"night_time\"]\n    },\n    \"winter\": {\n        \"all\": [\"all\"],\n    },\n    \"spring\": {\n        \"all\": [\"all\"],\n        \"workdays\": [\"working_hours\"]\n    }\n    }\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/15_usual_environment_labeling/","title":"UsualEnvironmentLabeling Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>usual_environment_labeling.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Silver]\n...\nlongterm_permanence_score_data_siilver = ${Paths:silver_dir}/longterm_permanence_score\nusual_environment_labels_data_silver = ${Paths:silver_dir}/usual_environment_labels\n...\n</code></pre> <p>The expected parameters in <code>usual_environment_labeling.ini</code> are as follows:</p> <ul> <li> <p>start_month: string, in <code>YYYY-MM</code> format, it indicates the first month to be included in the Usual Environment Labeling process. Long-term permanence metrics data shall be available for the corresponding start_month, end_month and season. Example: <code>2023-01</code>.</p> </li> <li> <p>end_month: string, in <code>YYYY-MM</code> format, it indicates the last month to be included in the Usual Environment Labeling process. Long-term permanence metrics data shall be available for the corresponding start_month, end_month and season. Example: <code>2023-06</code>.</p> </li> <li> <p>season: string, it indicates the season to be included in the Usual Environment Labeling process. Long-term permanence metrics data shall be available for the corresponding start_month, end_month and season. The months that correspond to each season are defined in the configuration of the Long-term permanence score method. Example: <code>all</code>. Options:</p> <ul> <li><code>\"all\"</code>: every month between start_month and end_month, both inclusive, are considered in the aggregation.</li> <li><code>\"winter\"</code>: every month included in the winter_months list between start_month and end_month, both inclusive, are considered in the aggregation.</li> <li><code>\"spring\"</code>: every month included in the spring_months list between start_month and end_month, both inclusive, are considered in the aggregation.</li> <li><code>\"summer\"</code>: every month included in the summer_months list between start_month and end_month, both inclusive, are considered in the aggregation.</li> <li><code>\"autumn\"</code>: every month included in the autumn_months list between start_month and end_month, both inclusive, are considered in the aggregation.</li> </ul> </li> <li> <p>total_ps_threshold: int, permanence score. It represents a long-term permanence score (ps) threshold to apply to discard rarely observed devices. Rarely observed devices are defined as those devices for which the total observed device's ps is lower than this threshold. Example: <code>1500</code>.</p> </li> <li> <p>total_ndays_threshold: int, number of days. It represents a frequency (number of days) threshold to apply to discard discontinuously observed devices. Discontinuously observed devices are defined as those devices for which the total observed device's frequency is lower than this threshold. Example <code>50</code>: </p> </li> <li> <p>ue_gap_ps_threshold: float, percentage. It represents a long-term permanence score (ps) threshold to apply in the preselection of tiles that may be tagged as usual environment (ue). This threshold is relative (a percentage) to the total ps device observation in the corresponding day type and time interval. The tiles associated to each device are sorted by ps in descending order, and then those that are below a ps gap that is bigger than <code>device_observation_ps * ue_gap_ps_threshold / 100</code> are filtered out. Example: <code>20</code> (%).</p> </li> <li> <p>gap_ps_threshold: int, permanence score. It represents a long-term permanence score (ps) threshold to apply in the preselection of tiles that may be tagged as home or work locations. This threshold is an absolute ps value, and its recommended value is 1. The tiles associated to each device are sorted by ps in descending order, and then those that are below a ps gap that is bigger than <code>gap_ps_threshold</code> are filtered out. Example: <code>1</code>.</p> </li> <li> <p>ue_ps_threshold: float, percentage. It represents a long-term permanence score (ps) threshold for the selection of tiles that may be tagged as usual environment (ue). This threshold is relative (a percentage) to the total ps device observation in the corresponding day type and time interval. The tiles associated to one device that have passed the ue gap cut and which have a ps higher than <code>device_observation_ps * ue_ps_threshold / 100</code> are tagged as ue. Example: <code>70</code> (%).</p> </li> <li> <p>home_ps_threshold: float, percentage. It represents a long-term permanence score (ps) threshold for the selection of tiles that may be tagged as home location. This threshold is relative (a percentage) to the total ps device observation in the corresponding day type and time interval. The tiles associated to one device that have passed the home gap cut and which have a ps higher than <code>device_observation_ps * home_ps_threshold / 100</code> are tagged as home. Example: <code>80</code> (%).</p> </li> <li> <p>work_ps_threshold: float, percentage. It represents a long-term permanence score (ps) threshold for the selection of tiles that may be tagged as work location. This threshold is relative (a percentage) to the total ps device observation in the corresponding day type and time interval. The tiles associated to one device that have passed the work gap cut and which have a ps higher than <code>device_observation_ps * work_ps_threshold / 100</code> are tagged as work. Example: <code>80</code> (%).</p> </li> <li> <p>ue_ndays_threshold: float, percentage. It represents a frequency (number of days) threshold for the selection of tiles that may be tagged as usual environment (ue). This threshold is relative (a percentage) to the total observed frequency for the device in the corresponding day type and time interval. The tiles associated to one device that have passed the ue gap cut and which have a frequency higher than device_observation_frequency * ue_ndays_threshold / 100 are tagged as ue. Example: <code>70</code> (%).</p> </li> <li> <p>home_ndays_threshold: float, percentage. It represents a frequency (number of days) threshold for the selection of tiles that may be tagged as home location. This threshold is relative (a percentage) to the total observed frequency for the device in the corresponding day type and time interval. The tiles associated to one device that have passed the home gap cut and which have a frequency higher than device_observation_frequency * home_ndays_threshold / 100 are tagged as home. Example: <code>70</code> (%).</p> </li> <li> <p>ue_ndays_threshold: float, percentage. It represents a frequency (number of days) threshold for the selection of tiles that may be tagged as work location. This threshold is relative (a percentage) to the total observed frequency for the device in the corresponding day type and time interval. The tiles associated to one device that have passed the work gap cut and which have a frequency higher than device_observation_frequency * work_ndays_threshold / 100 are tagged as work. Example: <code>70</code> (%).</p> </li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/15_usual_environment_labeling/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = UsualEnvironmentLabeling\n\n[UsualEnvironmentLabeling]\nstart_month = 2023-01\nend_month = 2023-04\nseason = all\n\n# filtering rarely/discontinuously observed devices\ntotal_ps_threshold = 1500\ntotal_ndays_threshold = 50\n\nue_gap_ps_threshold = 20\ngap_ps_threshold = 1\n\nue_ps_threshold = 70\nhome_ps_threshold = 80\nwork_ps_threshold = 80\n\nue_ndays_threshold = 70\nhome_ndays_threshold = 80\nwork_ndays_threshold = 80\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/16_usual_environment_aggregation/","title":"UsualEnvironmentAggregation Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>ue_aggregation.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Silver]\n...\nusual_environment_labels_data_silver = ${Paths:silver_dir}/usual_environment_labels\ngrid_data_silver = ${Paths:silver_dir}/grid\naggregated_usual_environments_silver = ${Paths:silver_dir}/aggregated_usual_environment\n...\n</code></pre> <p>The expected parameters in <code>ue_aggregation.ini</code> are as follows:</p> <ul> <li> <p>start_month: string, in <code>YYYY-MM</code> format, it indicates the first month to be included in the Usual Environment Aggregation process. Long-term permanence metrics data shall be available for the corresponding start_month, end_month and season. Example: <code>2023-01</code>.</p> </li> <li> <p>end_month: string, in <code>YYYY-MM</code> format, it indicates the last month to be included in the Usual Environment Aggregation process. Long-term permanence metrics data shall be available for the corresponding start_month, end_month and season. Example: <code>2023-06</code>.</p> </li> <li> <p>season: string, it indicates the season to be included in the Usual Environment Aggregation process. Long-term permanence metrics data shall be available for the corresponding start_month, end_month and season. The months that correspond to each season are defined in the configuration of the Long-term permanence score method. Example: <code>all</code>. Options:</p> <ul> <li><code>\"all\"</code>: every month between start_month and end_month, both inclusive, are considered in the aggregation.</li> <li><code>\"winter\"</code>: every month included in the winter_months list between start_month and end_month, both inclusive, are considered in the aggregation.</li> <li><code>\"spring\"</code>: every month included in the spring_months list between start_month and end_month, both inclusive, are considered in the aggregation.</li> <li><code>\"summer\"</code>: every month included in the summer_months list between start_month and end_month, both inclusive, are considered in the aggregation.</li> <li><code>\"autumn\"</code>: every month included in the autumn_months list between start_month and end_month, both inclusive, are considered in the aggregation.</li> </ul> </li> <li> <p>clear_destination_directory: bool, if to delete all previous outputs before running the component.</p> </li> <li> <p>uniform_tile_weights: int, if to use uniform tile weights in the aggregation process. If <code>True</code>, the weights of the tiles are equal. If <code>False</code>, the weights of the tiles are calculated based on landuse information.</p> </li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/16_usual_environment_aggregation/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = UsualEnvironmentLabeling\n\n[UsualEnvironmentLabeling]\nclear_destination_directory = True\n\nstart_month = 2023-01\nend_month = 2023-03\n\nseason = all\nuniform_tile_weights = True\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/1_inspire_grid_generation/","title":"InspireGridGeneration Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>grid_generation.ini</code>. In <code>general_config.ini</code> to execute the component specify all paths to its four corresponding data objects (input + output). Example: </p> <pre><code>[Paths.Silver]\ngrid_data_silver = ${Paths:silver_dir}/grid\n</code></pre> <p>In grid_generation.ini parameters are as follows: </p> <ul> <li> <p>clear_destination_directory - boolean, if True, the component will clear all the data in output paths.</p> </li> <li> <p>grid_mask - string, the mask to be used for grid generation. It can be either 'extent' or 'polygon'. If 'extent' is chosen, the extent parameter should be provided. If 'polygon' is chosen, reference country iso2 code should be provided.</p> </li> <li> <p>extent - list, the extent of the grid to be generated if 'extent' is chosen spatial mask type. The format is [min_lon, min_lat, max_lon, max_lat].</p> </li> <li> <p>reference_country - string, iso2 country code to use as a spatial mask for grid generation.</p> </li> <li> <p>country_buffer - integer, buffer distance to extend country polygon for grid generation.</p> </li> <li> <p>grid_generation_partition_size - integer, the size of the partition to be used for grid generation as a size of a side of grid subsquare. Default value is 500 grid tiles, so generation will be done with 500 by 500 grid subsquares.</p> </li> <li> <p>grid_processing_partition_quadkey_level - integer, the level of quadkey for resulted grid partitioning. Default value is level 8.  </p> </li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/1_inspire_grid_generation/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = InspireGridGeneration\n\n[InspireGridGeneration]\nclear_destination_directory = True\ngrid_mask = polygon # extent or polygon\nextent = [-4.5699,39.9101,-2.8544,40.9416] # [min_lon, min_lat, max_lon, max_lat]\nreference_country = ES # ISO A2 code\ncountry_buffer = 10000 # meters\ngrid_generation_partition_size = 500\ngrid_processing_partition_quadkey_level = 8\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/2_event_data_cleaning/","title":"EventCleaning Configuration","text":"<p>To initialise and run the component two configs are used - general_config.ini and event_cleaning.ini.  In general_config.ini to execute Event Cleaning component specify all paths to its four corresponding data objects (input + output). Example: </p> <pre><code>[Paths.Bronze]\nevent_data_bronze = ${Paths:bronze_dir}/mno_events\n\n[Paths.Silver]\nevent_data_silver = ${Paths:silver_dir}/mno_events\nevent_syntactic_quality_metrics_by_column = ${Paths:silver_dir}/event_syntactic_quality_metrics_by_column\nevent_syntactic_quality_metrics_frequency_distribution = ${Paths:silver_dir}/event_syntactic_quality_metrics_frequency_distribution\n</code></pre> <p>In event_cleaning.ini parameters are as follows: </p> <ul> <li> <p>data_period_start - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-01), the date from which start Event Cleaning</p> </li> <li> <p>data_period_end - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-05), the date till which perform Event Cleaning</p> </li> <li> <p>data_folder_date_format - string, to what string format convert dates so they match the naming of input data folders (it is expected that input data is divided into separate folders for each date of research period). Example: if you know that data for 2023-01-01 is stored in f\"{bronze_event_path}/20230101\", then the format to convert 2023-01-01 date to 20230101 string using strftimewill be %Y%m%d</p> </li> <li> <p>spark_data_folder_date_format - string, as for data_folder_date_format it depends on folder\u2019s naming pattern of input data but since datetime patterns in pyspark and strftime differ, it is a separate config param. Used to convert string to datetype when creating date column in frequency distribution table </p> </li> <li> <p>timestamp_format - str, expected string format of timestamp column when converting it to timestamp type</p> </li> <li> <p>input_timezone - str, timezone of data to use when converting to UTC, if you are sure that data was already changed to UTC or geographically in UTC, leave as \u201cUTC\u201c</p> </li> <li> <p>local_mcc- int, MCC of the country where the data is from</p> </li> <li> <p>do_bounding_box_filtering- boolean, True/False, decides whether to apply bounding box filtering</p> </li> <li> <p>bounding_box - dictionary, with following keys 'min_lon', 'max_lon', 'min_lat', and 'max_lat' and integer/float values, to specify coordinates of bounding box, within which records should fall, make sure that records and bounding box are in the same src </p> </li> <li> <p>number_of_partitions - an integer, that determines the value of the modulo operator. This value will determine the number expected partitions as to the last partitioning column user_id_modulo. This value does not affect the number of folders in terms of other partitioning columns (day, month, year).</p> </li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/2_event_data_cleaning/#configuration-example","title":"Configuration example","text":"<pre><code>[EventCleaning]\ndata_period_start = 2023-01-01\ndata_period_end = 2023-01-05\ndata_folder_date_format = %Y%m%d\nspark_data_folder_date_format = yyyyMMdd\ntimestamp_format = yyyy-MM-dd'T'HH:mm:ss\ninput_timezone = America/Los_Angeles\nlocal_mcc = 214  \ndo_bounding_box_filtering = True\ndo_same_location_deduplication = True\nbounding_box = {\n    'min_lon': -180,\n    'max_lon': 180,\n    'min_lat': -90,\n    'max_lat': 90\n    }\nnumber_of_partitions = 256\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/3_network_cleaning/","title":"NetworkCleaning Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>network_cleaning.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Bronze]\n...\nnetwork_data_bronze = ${Paths:bronze_dir}/mno_network\n...\n\n[Paths.Silver]\n...\nnetwork_data_silver = ${Paths:silver_dir}/mno_network\nnetwork_syntactic_quality_metrics_by_column = ${Paths:silver_quality_metrics_dir}/network_syntactic_quality_metrics_by_column\n...\n</code></pre> <p>The expected parameters in <code>network_cleaning.ini</code> are as follows:</p> <ul> <li>latitude_min: float, minimum accepted latitude (WGS84) for the latitude of cells in the input data. Values lower than this will be treated as out of bounds/range.</li> <li>latitude_max: float, maximum accepted latitude (WGS84) for the latitude of cells in the input data. Values higher than this will be treated as out of bounds/range.</li> <li>longitude_min: float, minimum accepted longitude (WGS84) for the longitude of cells in the input data. Values lower than this will be treated as out of bounds/range.</li> <li>longitude_max: float, minimum accepted longitude (WGS84) for the longitude of cells in the input data. Values higher than this will be treated as out of bounds/range.</li> <li>cell_type_options: comma-separated list of strings, this parameter indicates the accepted values in the <code>cell_type</code> field. Other values will be treated as out of bounds/range. Example: <code>macrocell, microcell, picocell</code>.</li> <li>technology_options: comma-separated list of strings, this parameter indicates the accepted values in the <code>technology</code> field. Other values will be treated as out of bounds/range. Example: <code>5G, LTE, UMTS, GSM</code>.</li> <li>data_period_start: string, format should be the \"yyyy-MM-dd\" (e.g., <code>2023-01-01</code>), the first date for which data will be processed by the component. All dates between this one and the specified in <code>data_period_end</code> will be processed (both inclusive).</li> <li>data_period_end: string, format should be \"yyyy-MM-dd\" (e.g., <code>2023-01-09</code>), the last date for which data will be processed by the component. All dates between the specified in <code>data_period_start</code> and this one will be processed (both inclusive).</li> <li>valid_date_timestamp_format: string, the timestamp format that is expected to be in the input network data and that will be parsed with PySpark using thiis format. Example: <code>yyyy-MM-dd'T'HH:mm:ss</code></li> <li>frequent_error_criterion: string, criterion to use when computing the most frequent errors encountered. It can take two values: <code>absolute</code> if one wants to find the top k most frequent errors (e.g., <code>k=10</code>); or <code>percentage</code> if one wants to find the most frequent errors that represent <code>k</code> percentage of all errors found. Example: <code>percentage</code>.</li> <li>top_k_errors: integer if <code>frequent_error_criterion=absolute</code> or float if <code>top_k_errors</code> if <code>frequent_error_criterion=percentage</code>, represents what portion of the most frequent errors to save. Example: <code>10</code>.</li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/3_network_cleaning/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = NetworkCleaning\n\n[NetworkCleaning]\n# Bounding box\nlatitude_min = 40.352\nlatitude_max = 40.486\nlongitude_min = -3.751\nlongitude_max = -3.579\n\ncell_type_options = macrocell, microcell, picocell\ntechnology_options = 5G, LTE, UMTS, GSM\n# Left- and right-inclusive date range for the data to be read\ndata_period_start = 2023-01-01\ndata_period_end = 2023-01-09\n\nvalid_date_timestamp_format = yyyy-MM-dd'T'HH:mm:ss\n\nfrequent_error_criterion = percentage  # allowed values: `absolute`, `percentage`\ntop_k_errors = 40.5\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/4_signal_strength_modeling/","title":"SignalStrengthModeling Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>signal_strength_modeling.ini</code>. In <code>general_config.ini</code> to execute the component specify all paths to its corresponding data objects (input + output). Example:</p> <pre><code>[Paths.Silver]\nnetwork_data_silver = ${Paths:silver_dir}/mno_network\ngrid_data_silver = ${Paths:silver_dir}/grid\nsignal_strength_data_silver = ${Paths:silver_dir}/signal_strength\n</code></pre> <p>In signal_strength_modeling.ini parameters are as follows: </p> <ul> <li> <p>data_period_start - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-01), the date from which start Event Cleaning</p> </li> <li> <p>data_period_end - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-05), the date till which perform Event Cleaning</p> </li> <li> <p>use_elevation - boolean, if True, the elevation data will be used for signal strength modeling. If False, the elevation will be set to 0</p> </li> <li> <p>do_azimuth_angle_adjustments - boolean, if True, signal strengths values will be adjusted based on azimuth and antenna horizontal beam widths for directional cells. If False, no adjustments will be made.</p> </li> <li> <p>do_elevation_angle_adjustments - boolean, if True, signal strengths values will be adjusted based on tilt and antenna vertical beam widths for directional cells. If False, no adjustments will be made.</p> </li> <li> <p>cartesian_crs - integer, the coordinate reference system (CRS) to use for the cartesian coordinates. The default value is 3035.</p> </li> <li> <p>default_cell_physical_properties - dictionary, the default physical properties of the cell types. These properties will be assigned to cells of corresponding type if the properties are not found in the network topology data. If cell types are not peresent in network topology data, the default type properties will be assigned to all cells.</p> </li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/4_signal_strength_modeling/#configuration-example","title":"Configuration example","text":"<pre><code>[SignalStrengthModeling]\ndata_period_start = 2023-01-01\ndata_period_end = 2023-01-15\nuse_elevation = False\ndo_azimuth_angle_adjustments = True\ndo_elevation_angle_adjustments = True\ncartesian_crs = 3035\n\ndefault_cell_physical_properties = {\n    'macrocell': {\n        'power': 10,\n        'range': 10000,\n        'path_loss_exponent': 3.75,\n        'antenna_height': 30,\n        'elevation_angle': 5,\n        'vertical_beam_width': 9,\n        'horizontal_beam_width': 65,\n        'azimuth_signal_strength_back_loss': -30,\n        'elevation_signal_strength_back_loss': -30,\n    },\n    'microcell': {\n        'power': 5,\n        'range': 1000,\n        'path_loss_exponent': 6.0,\n        'antenna_height': 8,\n        'elevation_angle': 5,\n        'vertical_beam_width': 9,\n        'horizontal_beam_width': 65,\n        'azimuth_signal_strength_back_loss': -30,\n        'elevation_signal_strength_back_loss': -30,\n    },\n    'default': {\n        'power': 5,\n        'range': 5000,\n        'path_loss_exponent': 3.75,\n        'antenna_height': 8,\n        'elevation_angle': 5,\n        'vertical_beam_width': 9,\n        'horizontal_beam_width': 65,\n        'azimuth_signal_strength_back_loss': -30,\n        'elevation_signal_strength_back_loss': -30,\n        }\n    }\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/5_cell_footprint_estimation/","title":"CellFootprintEstimation Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>cell_footprint_estimation.ini</code>. In <code>general_config.ini</code> to execute the component specify all paths to its corresponding data objects (input + output). Example: </p> <pre><code>[Paths.Silver]\nsignal_strength_data_silver = ${Paths:silver_dir}/signal_strength\ncell_footprint_data_silver = ${Paths:silver_dir}/cell_footprint\ncell_intersection_groups_data_silver = ${Paths:silver_dir}/cell_intersection_groups\n</code></pre> <p>In cell_footprint_estimation.ini parameters are as follows: </p> <ul> <li> <p>data_period_start - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-01), the date from which start Event Cleaning</p> </li> <li> <p>data_period_end - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-05), the date till which perform Event Cleaning</p> </li> <li> <p>logistic_function_steepness - float, the steepness of the logistic function used to estimate the signal dominance (cell footprint) value from signal strength. The default value is 0.2.</p> </li> <li> <p>logistic_function_midpoint - float, the midpoint of the logistic function used to estimate the signal dominance (cell footprint) value from signal strength. The default value is -92.5.</p> </li> <li> <p>do_difference_from_best_sd_prunning - boolean, if True, the cells per grid tile with signal dominance values that are lower than the threshold percentage from the best signal dominance will be pruned. If False, no pruning of this method will be performed.</p> </li> <li> <p>difference_from_best_sd_treshold - float, the threshold percentage from the best signal dominance value. The default value is 90.</p> </li> <li> <p>do_max_cells_per_tile_prunning - boolean, if True, the maximum number of cells per grid tile will be kept, other cells will be pruned. If False, no pruning of this method will be performed.</p> </li> <li> <p>max_cells_per_grid_tile - integer, the maximum number of cells per grid tile. The default value is 10.</p> </li> <li> <p>do_sd_treshold_prunning - boolean, if True, the cells with signal dominance values that are lower than the threshold will be pruned. If False, no pruning of this method will be performed.</p> </li> <li> <p>signal_dominance_treshold - float, the threshold value for signal dominance. The default value is 0.01.</p> </li> <li> <p>do_cell_intersection_groups_calculation - boolean, if True, the cell intersection groups will be calculated and corresponding data object created. If False, no cell intersection groups will be calculated.</p> </li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/5_cell_footprint_estimation/#configuration-example","title":"Configuration example","text":"<pre><code>[CellFootprintEstimation]\ndata_period_start = 2023-01-01\ndata_period_end = 2023-01-05\nlogistic_function_steepness = 0.2\nlogistic_function_midpoint = -92.5\n\ndo_difference_from_best_sd_prunning = True\ndifference_from_best_sd_treshold = 90 # percentage\n\ndo_max_cells_per_tile_prunning = False\nmax_cells_per_grid_tile = 10\n\ndo_sd_treshold_prunning = True\nsignal_dominance_treshold = 0.01\n\ndo_cell_intersection_groups_calculation = True\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/6_cell_connection_probability/","title":"CellConnectionProbabilityEstimation Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>cell_connection_probability_estimation</code>.ini. In <code>general_config.ini</code> to execute the component specify all paths to its four corresponding data objects (input + output). Example: </p> <pre><code>[Paths.Bronze]\nevent_data_bronze = ${Paths:bronze_dir}/mno_events\n\n[Paths.Silver]\ncell_footprint_data_silver = ${Paths:silver_dir}/cell_footprint\ngrid_data_silver = ${Paths:silver_dir}/grid\ncell_connection_probabilities_data_silver = ${Paths:silver_dir}/cell_conn_probs\n</code></pre> <p>In cell_connection_probability_estimation.ini parameters are as follows: </p> <ul> <li> <p>data_period_start - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-01), the date from which start Event Cleaning</p> </li> <li> <p>data_period_end - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-05), the date till which perform Event Cleaning</p> </li> <li> <p>use_land_use_prior - boolean, if True, the land use prior will be used for cell connection posterior probability estimation. If False, the land use prior will not be used, only connection probability based on cell footprint will be estimated.</p> </li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/6_cell_connection_probability/#configuration-example","title":"Configuration example","text":"<pre><code>[CellConnectionProbabilityEstimation]\ndata_period_start = 2023-01-01\ndata_period_end = 2023-01-15\nuse_land_use_prior = False\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/8_event_semantic_cleaning/","title":"SemanticCleaning Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>event_semantic_cleaning.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Silver]\n...\nnetwork_data_silver = ${Paths:silver_dir}/mno_network\nevent_data_silver_flagged = ${Paths:silver_dir}/mno_events_flagged\nevent_device_semantic_quality_metrics = ${Paths:silver_quality_metrics_dir}/semantic_quality_metrics\n...\n</code></pre> <p>The expected parameters in <code>event_semantic_cleaning.ini</code> are as follows: - data_period_start: string, format should be the one specified <code>data_period_format</code> (e.g., <code>2023-01-01</code> for <code>%Y-%m-%d</code>), the first date for which data will be processed by the component. All dates between this one and the specified in <code>data_period_end</code> will be processed (both inclusive). - data_period_end: string, format should be \"yyyy-MM-dd\" (e.g., <code>2023-01-09</code> for <code>%Y-%m-%d</code>), the last date for which data will be processed by the component. All dates between the specified in <code>data_period_start</code> and this one will be processed (both inclusive). - data_period_format: string, it indicates the format expected in <code>data_period_start</code> and <code>data_period_end</code>. For example, use <code>%Y-%m-%d</code> for the usual \"2023-01-09\" format separated by <code>-</code>. - semantic_min_distance_m: float, minimum distance (in metres) between two consecutive events above which they will be considered for flagging as suspicious or incorrect location. Example: <code>10000</code>. - semantic_min_speed_m_s: float, minimum mean speed (in metres per second) between two consecutive events above whihc they will be considered for flagging as suspicious or incorrect location. Example: <code>55</code>. - do_different_location_deduplication: boolean, True/False. Determines whether to flag duplicates with different location information (cases where a single user has one or more rows with identical timestamp values, but non-identical values in any other columns).</p>"},{"location":"UserManual/configuration/1_Pipeline/8_event_semantic_cleaning/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = SemanticCleaning\n\n[SemanticCleaning]\ndata_period_start = 2023-01-01\ndata_period_end = 2023-01-09\ndate_format = %Y-%m-%d\n\nsemantic_min_distance_m = 10000\nsemantic_min_speed_m_s = 55\n\ndo_different_location_deduplication = True\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/9_device_activity_statistics/","title":"DeviceActivityStatistics Configuration","text":"<p>To initialise and run the component two configs are used - general_config.ini and device_activity_statistics.ini.  In general_config.ini to execute Device Activity Statistics component specify all paths to its three corresponding data objects (input + output). The local timezone must also be specified in the general config. Example: </p> <pre><code>[Timezone]\nlocal_timezone = UTC\n\n[Paths.Silver]\n# Data\nnetwork_data_silver = ${Paths:silver_dir}/mno_network\n\ndevice_activity_statistics = ${Paths:silver_dir}/device_activity_statistics\n</code></pre> <p>In device_activity_statistics.ini parameters are as follows: </p> <ul> <li> <p>data_period_start - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-01), the date from which start Event Cleaning</p> </li> <li> <p>data_period_end - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-05), the date till which perform Event Cleaning</p> </li> <li> <p>clear_destination_directory - boolean, whether to empty the destination directory before running or not</p> </li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/9_device_activity_statistics/#configuration-example","title":"Configuration example","text":"<pre><code>[DeviceActivityStatistics]\nclear_destination_directory = True\ndata_period_start = 2023-01-01\ndata_period_end = 2023-01-04\n</code></pre>"},{"location":"UserManual/configuration/2_Optional/1_grid_enrichment/","title":"GridEnrichment Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>grid_enrichment.ini</code>. In <code>general_config.ini</code> to execute the component specify all paths to its corresponding data objects (input + output). </p> <pre><code>[Paths.Bronze]\ntransportation_data_bronze = ${Paths:bronze_dir}/spatial/transportation\nlanduse_data_bronze = ${Paths:bronze_dir}/spatial/landuse\n[Paths.Silver]\ngrid_data_silver = ${Paths:silver_dir}/grid\nenriched_grid_data_silver = ${Paths:silver_dir}/grid_enriched\n</code></pre> <p>In grid_enrichment.ini parameters are as follows: </p> <ul> <li> <p>clear_destination_directory - boolean, if True, the component will clear all the data in output paths.</p> </li> <li> <p>do_landcover_enrichment - boolean, if True, the component will enrich the grid with landuse prior probabilities and Path Loss Exponent environment coefficient.</p> </li> <li> <p>transportation_category_buffer_m - dictionary, buffer distance for each transportation category in meters. Used to convert transportation lines to polygons.</p> </li> <li> <p>prior_weights - dictionary, weights for each landuse category. Used to calculate prior probabilities.</p> </li> <li> <p>ple_coefficient_weights - dictionary, weights for each landuse category. Used to calculate Path Loss Exponent coefficient.</p> </li> <li> <p>do_elevation_enrichment - boolean, if True, the component will enrich the grid with elevation data. Not implemented yet.</p> </li> </ul>"},{"location":"UserManual/configuration/2_Optional/1_grid_enrichment/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = InspireGridGeneration\n\n[InspireGridGeneration]\n[Spark]\nsession_name = GridEnrichment\n\n[GridEnrichment]\nclear_destination_directory = True\ndo_landcover_enrichment = True\nprior_calculation_repartition_size = 12\ntransportation_category_buffer_m = {\n    \"primary\": 30,\n    \"secondary\": 15,\n    \"tertiary\": 5,\n    \"pedestrian\": 5,\n    \"railroad\": 15,\n    \"unknown\": 2\n    }\nprior_weights = {\n    \"residential_builtup\": 1.0,\n    \"other_builtup\": 1.0,\n    \"roads\": 0.5,\n    \"open_area\": 0.0,\n    \"forest\": 0.1,\n    \"water\": 0.0\n    }\nple_coefficient_weights = {\n    \"residential_builtup\": 1.0,\n    \"other_builtup\": 1.0,\n    \"roads\": 0.0,\n    \"open_area\": 0.0,\n    \"forest\": 1.0,\n    \"water\": 0.0\n    }\ndo_elevation_enrichment = False\n</code></pre>"},{"location":"UserManual/configuration/2_Optional/2_geozones_to_grid_mapping/","title":"GeozonesGridMapping Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>geozones_grid_mapping.ini</code>. In <code>general_config.ini</code> to execute the component specify all paths to its corresponding data objects (input + output). </p> <pre><code>[Paths.Bronze]\ngeographic_zones_data_bronze = ${Paths:bronze_dir}/spatial/geographic_zones \nadmin_units_data_bronze = ${Paths:bronze_dir}/spatial/admin_units \n[Paths.Silver]\ngrid_data_silver = ${Paths:silver_dir}/grid\ngeozones_grid_map_data_silver = ${Paths:silver_dir}/geozones_grid_map\n</code></pre> <p>In geozones_grid_mapping.ini parameters are as follows: </p> <ul> <li> <p>clear_destination_directory - boolean, if True, the component will clear all the data in output paths.</p> </li> <li> <p>zoning_type - string, type of zoning data object to be used for mapping. Possible values are \"admin\" and \"other\".</p> </li> <li> <p>dataset_ids - list, ids of zonning datasets to use for mapping. Grid mapping will be done for each dataset separately.</p> </li> </ul>"},{"location":"UserManual/configuration/2_Optional/2_geozones_to_grid_mapping/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = GeozonesGridMapping\n\n[GeozonesGridMapping]\nclear_destination_directory = True\nzoning_type = other # admin or other\ndataset_ids = ['nuts'] # list of dataset ids to map grid to\n</code></pre>"},{"location":"UserManual/configuration/3_QualityWarnings/1_network_quality_warnings/","title":"NetworkQualityWarnings Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>network_quality_warnings.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Silver]\n...\nnetwork_syntactic_quality_metrics_by_column = ${Paths:silver_quality_metrics_dir}/network_syntactic_quality_metrics_by_column\nnetwork_syntactic_quality_warnings_log_table = ${Paths:silver_quality_warnings_dir}/network_syntactic_quality_warnings_log_table\nnetwork_syntactic_quality_warnings_line_plot_data = ${Paths:silver_quality_warnings_dir}/network_syntactic_quality_warnings_line_plot_data\nnetwork_syntactic_quality_warnings_pie_plot_data = ${Paths:silver_quality_warnings_dir}/network_syntactic_quality_warnings_pie_plot_data\n...\n</code></pre> <p>The expected parameters in <code>network_quality_warnings.ini</code> are as follows: - date_of_study: string, format should be the one specified <code>date_format</code> (e.g., <code>2023-01-01</code> for <code>%Y-%m-%d</code>), the first date for which data will be processed by the component. All dates between this one and the specified in <code>data_period_end</code> will be processed (both inclusive). - date_format: string, it indicates the format expected in <code>date_of_study</code>. For example, use <code>%Y-%m-%d</code> for the usual \"2023-01-01\" format separated by <code>-</code>. - lookback_period: string, it indicates the length of the lookback period used to compare the metrics of the date of study with past data volume and error rates. Three possible values are accepted: <code>week</code>, <code>month</code>, and <code>quarter</code>. - thresholds: dictionary, indicating the different thresholds to be used for raising warnings.</p>"},{"location":"UserManual/configuration/3_QualityWarnings/1_network_quality_warnings/#thresholds","title":"Thresholds","text":"<p>The thresholds parameter is a dictionary of dictionaries, used to indicate the different thresholds to be used for each type of warning. In the case that one of the thresholds is missing, a default value will be used instead. The default values are contained in <code>multimno.core.constants.network_default_thresholds.NETWORK_DEFAULT_THRESHOLDS</code>.</p> <p>Ihe dictionary structure is as follows: - <code>\"SIZE_RAW_DATA\"</code>: refers to the size of the input data.   - <code>\"OVER_AVERAGE\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the input raw file is greater than this threshold, in percentage different, when compared to the average over the lookback period. By default, the value is <code>30</code>.   - <code>\"UNDER_AVERAGE\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the input raw file is lower than this threshold, in percentage different, when compared to the average over the lookback period. By default, the value is <code>30</code>.   - <code>\"VARIABILITY\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the input raw file differs from its average over the lookback period by more than its standard deviation multiplied by this threshold. This is, this value is a threshold of the number of standard deviations than a value can differ from its average over the lookback period, either over or under the average. By default, the value is <code>2</code>.   - <code>\"ABS_VALUE_UPPER_LIMIT\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the input raw file is greater than this value. By default, it is equal to the threshold calculated from the <code>VARIABILITY</code> parameter over the average.   - <code>\"ABS_VALUE_LOWER_LIMIT\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the input raw file is greater than this value. By default, it is equal to the threshold calculated from the <code>VARIABILITY</code> parameter under the average.</p> <ul> <li><code>\"SIZE_CLEAN_DATA\"</code>: refers to the size of the output data.</li> <li><code>\"OVER_AVERAGE\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the output processed file is greater than this threshold, in percentage different, when compared to the average over the lookback period. By default, the value is <code>30</code>.</li> <li><code>\"UNDER_AVERAGE\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the output processed file is lower than this threshold, in percentage different, when compared to the average over the lookback period. By default, the value is <code>30</code>.</li> <li><code>\"VARIABILITY\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the output processed file differs from its average over the lookback period by more than its standard deviation multiplied by this threshold. This is, this value is a threshold of the number of standard deviations than a value can differ from its average over the lookback period, either over or under the average. By default, the value is <code>2</code>.</li> <li><code>\"ABS_VALUE_UPPER_LIMIT\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the output processed file is greater than this value. by default, it is equal to the threshold calculated from the <code>VARIABILITY</code> parameter over the average.</li> <li> <p><code>\"ABS_VALUE_LOWER_LIMIT\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the output processed file is greater than this value. By default, it is equal to the threshold calculated from the <code>VARIABILITY</code> parameter under the average.</p> </li> <li> <p><code>\"TOTAL_ERROR_RATE\"</code>: refers to the percentage of rows preserved from the input file, i.e., the rows that passed the cleaning/check procedure.</p> </li> <li><code>\"OVER_AVERAGE\"</code>: integer or float, indicates the threshold for a warning to be raised when the error rate is greater than this threshold, in percentage different, when compared to the average over the lookback period. By default, the value is <code>30</code>.</li> <li><code>\"VARIABILITY\"</code>: integer or float, indicates the threshold for a warning to be raised when the error rate differs from its average over the lookback period by more than its standard deviation multiplied by this threshold. This is, this value is a threshold of the number of standard deviations than a value can differ from its average over the lookback period, obly over the average. By default, the value is <code>2</code>.</li> <li> <p><code>\"ABS_VALUE_UPPER_LIMIT\"</code>: integer or float, indicates the threshold for a warning to be raised when the error rate is greater than this value. By default, the value is <code>20</code>.</p> </li> <li> <p><code>\"Missing_value_RATE\"</code>: refers to the percentage of missing/null values of a given field in the input data. The value of this dictionary key is a dictionary where the keys are field names, and the values are a dictionary containing the thresholds form this error type and field.</p> </li> <li>Admitted keys (i.e., fields) are: <code>cell_id</code>, <code>valid_date_start</code>, <code>latitude</code>, <code>longitude</code>, <code>altitude</code>, <code>antenna_height</code>, <code>directionality</code>, <code>azimuth_angle</code>, <code>elevation_angle</code>, <code>horizontal_beam_width</code>, <code>vertical_beam_width</code>, <code>power</code>, <code>frequency</code>, <code>technology</code>, and <code>cell_type</code>.</li> <li> <p>Each key (i.e., field) has the following thresholds:</p> <ul> <li><code>\"AVERAGE\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate is greater than this threshold, in percentage different, when compared to the average over the lookback period. By default, the value is <code>30</code> for <code>cell_id</code>, <code>latitude</code>, and <code>longitude</code>, and <code>60</code> otherwise.</li> <li><code>\"VARIABILITY\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate differs from its average over the lookback period by more than its standard deviation multiplied by this threshold. This is, this value is a threshold of the number of standard deviations than a value can differ from its average over the lookback period, obly over the average. By default, the value is <code>2</code> for <code>cell_id</code>, <code>latitude</code>, and <code>longitude</code>, and <code>3</code> otherwise.</li> <li><code>\"ABS_VALUE_UPPER_LIMIT\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate is greater than this value. By default, the value is <code>20</code> for <code>cell_id</code>, <code>latitude</code>, and <code>longitude</code>, and <code>50</code> otherwise.</li> </ul> </li> <li> <p><code>\"Out_of_range_RATE\"</code>: refers to the percentage of out of bounds, out of range or invalid values of a given field in the input data. The value of this dictionary key is a dictionary where the keys are field names, and the values are a dictionary containing the thresholds form this error type and field.</p> </li> <li>Admitted keys (i.e., fields) are: <code>\"cell_id\"</code>, <code>\"latitude\"</code>, <code>\"longitude\"</code>, <code>\"antenna_height\"</code>, <code>\"directionality\"</code>, <code>\"azimuth_angle\"</code>, <code>\"elevation_angle\"</code>, <code>\"horizontal_beam_width\"</code>, <code>\"vertical_beam_width\"</code>, <code>\"power\"</code>, <code>\"frequency\"</code>, <code>\"technology\"</code>, and <code>\"cell_type\"</code>. Exceptionally, the <code>None</code> value is also accepted, referring to the specific error where <code>valid_date_end</code> is a point int time earlier than <code>valid_date_start</code>.</li> <li> <p>Each key has the following thresholds:</p> <ul> <li><code>\"AVERAGE\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate is greater than this threshold, in percentage different, when compared to the average over the lookback period. By default, the value is <code>20</code> for <code>cell_id</code>, <code>latitude</code>, and <code>longitude</code>, and <code>60</code> otherwise.</li> <li><code>\"VARIABILITY\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate differs from its average over the lookback period by more than its standard deviation multiplied by this threshold. This is, this value is a threshold of the number of standard deviations than a value can differ from its average over the lookback period, obly over the average. By default, the value is <code>2</code> for <code>cell_id</code>, <code>latitude</code>, and <code>longitude</code>, and <code>3</code> otherwise.</li> <li><code>\"ABS_VALUE_UPPER_LIMIT\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate is greater than this value. By default, the value is <code>20</code> for <code>cell_id</code>, <code>latitude</code>, and <code>longitude</code>, and <code>50</code> otherwise.</li> </ul> </li> <li> <p><code>\"Parsing_error_RATE\"</code>: refers to values that could not be parsed.</p> </li> <li><code>\"valid_date_start\"</code>:<ul> <li><code>\"AVERAGE\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate is greater than this threshold, in percentage different, when compared to the average over the lookback period. By default, the value is <code>60</code>.</li> <li><code>\"VARIABILITY\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate differs from its average over the lookback period by more than its standard deviation multiplied by this threshold. This is, this value is a threshold of the number of standard deviations than a value can differ from its average over the lookback period, obly over the average. By default, the value is <code>3</code>.</li> <li><code>\"ABS_VALUE_UPPER_LIMIT\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate is greater than this value. By default, the value is <code>50</code>.</li> </ul> </li> </ul>"},{"location":"UserManual/configuration/3_QualityWarnings/1_network_quality_warnings/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = NetworkQualityWarnings\n\n[NetworkQualityWarnings]\ndate_format = %Y-%m-%d\ndate_of_study = 2023-01-08\nlookback_period = week\n\n# All values must be numeric\n# Missing parameter will take the default value\n# Incorrect value will throw an error\nthresholds = {\n    \"SIZE_RAW_DATA\": {\n        \"OVER_AVERAGE\": 30,\n        \"UNDER_AVERAGE\": 30,\n        \"VARIABILITY\": 2,\n        \"ABS_VALUE_UPPER_LIMIT\": None,\n        \"ABS_VALUE_LOWER_LIMIT\": None,\n    },\n    \"SIZE_CLEAN_DATA\": {\n        \"OVER_AVERAGE\": 30,\n        \"UNDER_AVERAGE\": 30,\n        \"VARIABILITY\": 2,\n        \"ABS_VALUE_UPPER_LIMIT\": None,\n        \"ABS_VALUE_LOWER_LIMIT\": None,\n    },\n    \"TOTAL_ERROR_RATE\": {\n        \"OVER_AVERAGE\": 30,\n        \"VARIABILITY\": 2,\n        \"ABS_VALUE_UPPER_LIMIT\": 20,\n    },\n    \"Missing_value_RATE\": {\n        \"cell_id\": {\n            \"AVERAGE\": 30,\n            \"VARIABILITY\": 2,\n            \"ABS_VALUE_UPPER_LIMIT\": 20,\n        },\n        \"valid_date_start\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"latitude\": {\n            \"AVERAGE\": 30,\n            \"VARIABILITY\": 2,\n            \"ABS_VALUE_UPPER_LIMIT\": 20,\n        },\n        \"longitude\": {\n            \"AVERAGE\": 30,\n            \"VARIABILITY\": 2,\n            \"ABS_VALUE_UPPER_LIMIT\": 20,\n        },\n        \"altitude\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"antenna_height\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"directionality\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"azimuth_angle\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"elevation_angle\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"horizontal_beam_width\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"vertical_beam_width\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"power\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"frequency\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"technology\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"cell_type\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n    },\n    \"Out_of_range_RATE\": {\n        \"cell_id\": {\n            \"AVERAGE\": 30,\n            \"VARIABILITY\": 2,\n            \"ABS_VALUE_UPPER_LIMIT\": 20,\n        },\n        \"latitude\": {\n            \"AVERAGE\": 30,\n            \"VARIABILITY\": 2,\n            \"ABS_VALUE_UPPER_LIMIT\": 20,\n        },\n        \"longitude\": {\n            \"AVERAGE\": 30,\n            \"VARIABILITY\": 2,\n            \"ABS_VALUE_UPPER_LIMIT\": 20,\n        },\n        \"antenna_height\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"directionality\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"azimuth_angle\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"elevation_angle\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"horizontal_beam_width\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"vertical_beam_width\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"power\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"frequency\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"technology\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"cell_type\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        None: {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n    },\n    \"Parsing_error_RATE\": {\n        \"valid_date_start\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"valid_date_end\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        }\n    }\n    }\n</code></pre>"},{"location":"UserManual/configuration/3_QualityWarnings/2_event_quality_warnings/","title":"EventQualityWarnings Configuration","text":"<p>To initialise and run the component two configs are used -  <code>general_config.ini</code> and component\u2019s config (either  <code>event_cleaning_quality_warnings.ini</code> or  <code>event_deduplication_quality_warnings.ini</code>). In  <code>general_config.ini</code> to execute Event Quality Warnings component specify all paths to its corresponding data objects. Example with specified paths for both cases:</p> <pre><code>[Paths.Silver]\n...\n# for Event Cleaning Quality Warnings\nevent_syntactic_quality_metrics_by_column = ${Paths:silver_dir}/event_syntactic_quality_metrics_by_column\nevent_syntactic_quality_metrics_frequency_distribution = ${Paths:silver_dir}/event_syntactic_quality_metrics_frequency_distribution\nevent_syntactic_quality_warnings_log_table = ${Paths:silver_dir}/event_syntactic_quality_warnings_log_table\nevent_syntactic_quality_warnings_for_plots = ${Paths:silver_dir}/event_syntactic_quality_warnings_for_plots\n</code></pre> <p>Below there is a description of one of sub component\u2019s config  - <code>event_cleaning_quality_warnings.ini</code>. </p> <p>Parameters are as follows:</p> <p>Under <code>[EventQualityWarnings]</code> config section: </p> <ul> <li> <p>input_qm_by_column_path_key - string, key in Paths.Silver section in general config for a path to corresponding Quality Metrics By Column data</p> </li> <li> <p>input_qm_freq_distr_path_key - string, key in Paths.Silver section in general config for a path to corresponding Quality Metrics Frequency Distribution data</p> </li> <li> <p>output_qw_log_table_path_key - string, key in Paths.Silver section in general config for a path to write output Quality Warnings Log Table</p> </li> <li> <p>output_qw_for_plots_path_key - string, key in Paths.Silver section in general config for a path to write output Quality Warnings ForPLots</p> </li> <li> <p>data_period_start - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-08), the date from which start Event Quality Warnings, by now make sure the first day(s) of research period has enough previous data in in Quality Metrics Frequency Fistribution and Quality Metrics By Column </p> </li> <li> <p>data_period_end - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-15), the date till which perform Event Quality Warnings</p> </li> <li> <p>lookback_period - the length of lookback period, represented as string (could be either \u2018week' or 'month') which than will get its numeric representation in number of days</p> </li> <li> <p>do_size_raw_data_qw - boolean, whether perform QW checks on <code>initial_frequency</code> column in Quality Metrics Frequency Fistribution</p> </li> <li> <p>do_size_clean_data_qw - boolean, whether perform QW checks on <code>final_frequency</code> column in Quality Metrics Frequency Distribution</p> </li> <li> <p>data_size_tresholds - dictionary, with keys as thresholds names and values as corresponding thresholds values for  <code>do_size_raw_data_qw</code> and <code>do_size_clean_data_qw</code></p> </li> <li> <p>do_error_rate_by_date_qw - boolean, whether to perform QW checks on total error rate by <code>date</code></p> </li> <li> <p>do_error_rate_by_date_and_cell_qw -  boolean, whether to perform QW checks on total error rate by <code>date</code> and <code>cell_id</code></p> </li> <li> <p>do_error_rate_by_date_and_user_qw -  boolean, whether to perform QW checks on total error rate by <code>date</code> and <code>user_id</code></p> </li> <li> <p>do_error_rate_by_date_and_cell_user_qw - boolean, whether to perform QW checks on total error rate by <code>date</code>, <code>cell_id</code> and <code>user_id</code></p> </li> <li> <p>error_rate_tresholds - dictionary, with keys as thresholds names and values as corresponding thresholds values for  <code>do_error_rate_by_date_qw</code>, <code>do_error_rate_by_date_and_cell_qw</code>, <code>do_error_rate_by_date_and_user_qw</code>, and <code>do_error_rate_by_date_and_cell_user_qw</code></p> </li> <li> <p>error_type_qw_checks - dictionary, where the keys are names of error types (please see <code>multimno/core/constants/error_types.py</code> file) and values list of column names on which you want to perform QWs of the this error type. Example: during Event Cleaning three columns are checked for null values, if you want to check error rate of <code>missing_value</code> type for all mentioned columns specify them in the list. Some error types might have None for column names, which means that technically this kind or error do not belong to just one column but several (e.g. for <code>no_location</code> error three columns are used - cell_id, lat, lon): </p> </li> </ul> <p><pre><code>error_type_qw_checks = {\n    'missing_value': ['user_id', 'mcc', 'timestamp'],\n    'out_of_bounding_box':[None]\n    }\n</code></pre> If you do not intend to run QWs on some error type leave its corresponding list of columns empty.</p> <ul> <li>thresholds for each error_type &amp; column combination you want to compute QWs  - thresholds are combined in groups: each set of thresholds relevant to some error type is a separate config param of type dictionary, where keys are column names, values is another dictionary of structure: <code>threshold_name:threshold_value</code>. Example: </li> </ul> <p><pre><code>missing_value_thresholds = {\n    'user_id': {\"Missing_value_RATE_BYDATE_USER_AVERAGE\": 30,\n                \"Missing_value_RATE_BYDATE_USER_VARIABILITY\": 2,\n                \"Missing_value_RATE_BYDATE_USER_ABS_VALUE_UPPER_LIMIT\": 20\n               }, \n    'mcc': {\"Missing_value_RATE_BYDATE_MCC_AVERAGE\": 30,\n            \"Missing_value_RATE_BYDATE_MCC_VARIABILITY\": 2,\n            \"Missing_value_RATE_BYDATE_MCC_ABS_VALUE_UPPER_LIMIT\": 20\n           }, \n    'timestamp': {\"Missing_value_RATE_BYDATE_TIMESTAMP_AVERAGE\": 30,\n                  \"Missing_value_RATE_BYDATE_TIMESTAMP_VARIABILITY\": 2,\n                  \"Missing_value_RATE_BYDATE_TIMESTAMP_ABS_VALUE_UPPER_LIMIT\": 20\n                 }\n    }\nout_of_bounding_box_thresholds = {\n    None: {\"Out_of_bbox_RATE_BYDATE_AVERAGE\": 30,\n           \"Out_of_bbox_RATE_BYDATE_VARIABILITY\": 2,\n           \"Out_of_bbox_RATE_BYDATE_ABS_VALUE_UPPER_LIMIT\": 20\n          }\n    }\n</code></pre> Make sure, that for each column of interest specified in <code>error_type_qw_checks</code> there are corresponding thresholds. The order of thresholds is important and should be: <code>AVERAGE</code>, <code>VARIABILITY</code>, and <code>ABS_VALUE_UPPER_LIMIT</code> (at least by now all error type QWs follow the same logic and thus their computation is done within one function with ordered threshold arguments). Currently the code supports running QWs on following thresholds: </p> <p><pre><code># possible thresholds in event_cleaning_quality_warnings.ini\nmissing_value_thresholds\n\nout_of_admissible_values_thresholds\n\nnot_right_syntactic_format_thresholds\n\nno_location_thresholds\n\nno_domain_thresholds\n\nout_of_bounding_box_thresholds\n\ndeduplication_same_location_thresholds\n</code></pre> - clear_destination_directory - boolean, if True deletes all output of the Component in init stage</p>"},{"location":"UserManual/configuration/3_QualityWarnings/2_event_quality_warnings/#configuration-example","title":"Configuration example","text":"<pre><code>[EventQualityWarnings]\n# keys in Paths.Silver section in general config\ninput_qm_by_column_path_key = event_syntactic_quality_metrics_by_column\ninput_qm_freq_distr_path_key = event_syntactic_quality_metrics_frequency_distribution\noutput_qw_log_table_path_key = event_syntactic_quality_warnings_log_table\noutput_qw_for_plots_path_key = event_syntactic_quality_warnings_for_plots\n# BY NOW make sure that the first day(s) of research period has enough previous data\n# of df_qa_by_column and df_qa_freq_distribution \n# (e.g. staring from 2023-01-01, if period is a week and start period is 2023-01-08)\ndata_period_start = 2023-01-01\n# you can exceed max(df_qa_by_column.date) \n# although you will still get QWs for dates till max(df_qa_by_column.date), including\ndata_period_end = 2023-01-09\n# should be either week or month\nlookback_period = week\n# SIZE QA\ndo_size_raw_data_qw = True\ndo_size_clean_data_qw = True\ndata_size_tresholds = {\n    \"SIZE_RAW_DATA_BYDATE_VARIABILITY\": 3,\n    \"SIZE_RAW_DATA_BYDATE_ABS_VALUE_UPPER_LIMIT\": 10000000,\n    \"SIZE_RAW_DATA_BYDATE_ABS_VALUE_LOWER_LIMIT\": 0,\n    \"SIZE_CLEAN_DATA_BYDATE_VARIABILITY\": 3,\n    \"SIZE_CLEAN_DATA_BYDATE_ABS_VALUE_UPPER_LIMIT\": 10000000,\n    \"SIZE_CLEAN_DATA_BYDATE_ABS_VALUE_LOWER_LIMIT\": 0,\n    }\n# ERROR RATE QW\ndo_error_rate_by_date_qw = True\ndo_error_rate_by_date_and_cell_qw = False\ndo_error_rate_by_date_and_user_qw = True\ndo_error_rate_by_date_and_cell_user_qw = True\nerror_rate_tresholds = {\n    \"TOTAL_ERROR_RATE_BYDATE_OVER_AVERAGE\": 30,\n    \"TOTAL_ERROR_RATE_BYDATE_VARIABILITY\": 2,\n    \"TOTAL_ERROR_RATE_BYDATE_ABS_VALUE_UPPER_LIMIT\": 20,\n    \"ERROR_RATE_BYDATE_BYCELL_OVER_AVERAGE\": 30,\n    \"ERROR_RATE_BYDATE_BYCELL_VARIABILITY\": 2,\n    \"ERROR_RATE_BYDATE_BYCELL_ABS_VALUE_UPPER_LIMIT\": 20,\n    \"ERROR_RATE_BYDATE_BYUSER_OVER_AVERAGE\": 30,\n    \"ERROR_RATE_BYDATE_BYUSER_VARIABILITY\": 2,\n    \"ERROR_RATE_BYDATE_BYUSER_ABS_VALUE_UPPER_LIMIT\": 20,\n    \"ERROR_RATE_BYDATE_BYCELL_USER_OVER_AVERAGE\": 30,\n    \"ERROR_RATE_BYDATE_BYCELL_USER_VARIABILITY\": 2,\n    \"ERROR_RATE_BYDATE_BYCELL_USER_ABS_VALUE_UPPER_LIMIT\": 20,\n    }\n# ERROR TYPE QW\n# for each type of error (key), specified the colums you want to check, naming of columns must be oidentical to ColNames\n# if you do not want to run qw on some error_type leave the list empty\n# None - for no_location and out_of_bounding_box because they do not have more than one column used for this error_type\n# for more clarity please check event_cleaning.py\nerror_type_qw_checks = {\n    'missing_value': ['user_id', 'mcc', 'timestamp'],\n    'out_of_admissible_values': ['cell_id', 'mcc', 'mnc', 'plmn', 'timestamp'],\n    'not_right_syntactic_format': ['timestamp'], \n    'no_domain': [None],\n    'no_location':[None], \n    'out_of_bounding_box':[None],\n    'same_location_duplicate':[None]\n    }\n# for each dict_error_type_thresholds make sure you specified all relevant columns\n# the order of thresholds is important, should be: AVERAGE, VARIABILITY, and ABS_VALUE_UPPER_LIMIT\nmissing_value_thresholds = {\n    'user_id': {\"Missing_value_RATE_BYDATE_USER_AVERAGE\": 30,\n                \"Missing_value_RATE_BYDATE_USER_VARIABILITY\": 2,\n                \"Missing_value_RATE_BYDATE_USER_ABS_VALUE_UPPER_LIMIT\": 20\n               }, \n    'mcc': {\"Missing_value_RATE_BYDATE_MCC_AVERAGE\": 30,\n            \"Missing_value_RATE_BYDATE_MCC_VARIABILITY\": 2,\n            \"Missing_value_RATE_BYDATE_MCC_ABS_VALUE_UPPER_LIMIT\": 20\n           },\n    'mnc': {\"Missing_value_RATE_BYDATE_MNC_AVERAGE\": 30,\n            \"Missing_value_RATE_BYDATE_MNC_VARIABILITY\": 2,\n            \"Missing_value_RATE_BYDATE_MNC_ABS_VALUE_UPPER_LIMIT\": 20\n           }, \n    'plmn': {\"Missing_value_RATE_BYDATE_PLMN_AVERAGE\": 30,\n            \"Missing_value_RATE_BYDATE_PLMN_VARIABILITY\": 2,\n            \"Missing_value_RATE_BYDATE_PLMN_ABS_VALUE_UPPER_LIMIT\": 20\n           }, \n    'timestamp': {\"Missing_value_RATE_BYDATE_TIMESTAMP_AVERAGE\": 30,\n                  \"Missing_value_RATE_BYDATE_TIMESTAMP_VARIABILITY\": 2,\n                  \"Missing_value_RATE_BYDATE_TIMESTAMP_ABS_VALUE_UPPER_LIMIT\": 20\n                 }\n    }\nout_of_admissible_values_thresholds = {\n    'cell_id': {\"Out_of_range_RATE_BYDATE_CELL_AVERAGE\": 30,\n                \"Out_of_range_RATE_BYDATE_CELL_VARIABILITY\": 2,\n                \"Out_of_range_RATE_BYDATE_CELL_ABS_VALUE_UPPER_LIMIT\": 20\n               }, \n    'mcc': {\"Out_of_range_RATE_BYDATE_MCC_AVERAGE\": 30,\n            \"Out_of_range_RATE_BYDATE_MCC_VARIABILITY\": 2,\n            \"Out_of_range_RATE_BYDATE_MCC_ABS_VALUE_UPPER_LIMIT\": 20\n           }, \n    'mnc': {\"Out_of_range_RATE_BYDATE_MNC_AVERAGE\": 30,\n            \"Out_of_range_RATE_BYDATE_MNC_VARIABILITY\": 2,\n            \"Out_of_range_RATE_BYDATE_MNC_ABS_VALUE_UPPER_LIMIT\": 20\n           },\n    'plmn': {\"Out_of_range_RATE_BYDATE_PLMN_AVERAGE\": 30,\n            \"Out_of_range_RATE_BYDATE_PLMN_VARIABILITY\": 2,\n            \"Out_of_range_RATE_BYDATE_PLMN_ABS_VALUE_UPPER_LIMIT\": 20\n           },\n    'timestamp': {\"Out_of_range_RATE_BYDATE_TIMESTAMP_AVERAGE\": 30,\n                  \"Out_of_range_RATE_BYDATE_TIMESTAMP_VARIABILITY\": 2,\n                  \"Out_of_range_RATE_BYDATE_TIMESTAMP_ABS_VALUE_UPPER_LIMIT\": 20\n                 }\n    }\nnot_right_syntactic_format_thresholds = {\n    'timestamp': {\"Wrong_type_RATE_BYDATE_TIMESTAMP_AVERAGE\": 30,\n                  \"Wrong_type_RATE_BYDATE_TIMESTAMP_VARIABILITY\": 2,\n                  \"Wrong_type_RATE_BYDATE_TIMESTAMP_ABS_VALUE_UPPER_LIMIT\": 20\n                 }\n    }\nno_location_thresholds = {\n    None: {\"No_location_RATE_BYDATE_AVERAGE\": 30,\n           \"No_location_RATE_BYDATE_VARIABILITY\": 2,\n           \"No_location_RATE_BYDATE_ABS_VALUE_UPPER_LIMIT\": 20\n          }\n    }\nno_domain_thresholds = {\n    None: {\"No_domain_RATE_BYDATE_AVERAGE\": 30,\n           \"No_domain_RATE_BYDATE_VARIABILITY\": 2,\n           \"No_domain_RATE_BYDATE_ABS_VALUE_UPPER_LIMIT\": 20\n          }\n    }\nout_of_bounding_box_thresholds = {\n    None: {\"Out_of_bbox_RATE_BYDATE_AVERAGE\": 30,\n           \"Out_of_bbox_RATE_BYDATE_VARIABILITY\": 2,\n           \"Out_of_bbox_RATE_BYDATE_ABS_VALUE_UPPER_LIMIT\": 20\n          }\n    }\ndeduplication_same_location_thresholds = {\n    None: {\"Deduplication_same_location_RATE_BYDATE_AVERAGE\": 30,\n           \"Deduplication_same_location_RATE_BYDATE_VARIABILITY\": 2,\n           \"Deduplication_same_location_RATE_BYDATE_ABS_VALUE_UPPER_LIMIT\": 20\n          }\n    }\n\nclear_destination_directory = True\n</code></pre>"},{"location":"UserManual/configuration/3_QualityWarnings/3_semantic_quality_warnings/","title":"SemanticQualityWarnings Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>event_semantic_quality_warnings.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Silver]\n...\nevent_device_semantic_quality_metrics = ${Paths:silver_quality_metrics_dir}/semantic_quality_metrics\nevent_device_semantic_quality_warnings_log_table = ${Paths:silver_quality_warnings_dir}/semantic_quality_warnings_log_table\nevent_device_semantic_quality_warnings_bar_plot_data = ${Paths:silver_quality_warnings_dir}/semantic_quality_warnings_bar_plot_data\n...\n</code></pre> <p>The expected parameters in <code>event_semantic_quality_warnings.ini</code> are as follows: - date_of_study: string, format should be the one specified <code>date_format</code> (e.g., <code>2023-01-01</code> for <code>%Y-%m-%d</code>), the first date for which data will be processed by the component. All dates between this one and the specified in <code>data_period_end</code> will be processed (both inclusive). - date_format: string, it indicates the format expected in <code>date_of_study</code>. For example, use <code>%Y-%m-%d</code> for the usual \"2023-01-01\" format separated by <code>-</code>. - thresholds: dictionary, indicating the different thresholds to be used for raising warnings.</p>"},{"location":"UserManual/configuration/3_QualityWarnings/3_semantic_quality_warnings/#thresholds","title":"Thresholds","text":"<p>The thresholds parameter is a dictionary of dictionaries, used to indicate the different thresholds and lookback periods to be used for each type of warning. In the case than one of the thresholds is missing, a default value will be used instead. The default values are contained in <code>multimno.core.constants.semantic_qw_default_thresholds.SEMANTIC_DEFAULT_THRESHOLDS</code>.</p> <p>The dictionary structure is as follows:  - <code>\"CELL_ID_NON_EXISTENT\"</code>: refers to events that make reference to a non-existent cell ID.   - <code>\"sd_lookback_days\"</code>: integer, indicates the days to use as a lookback period to calculate the average and standard deviation over. By default, the value is 7.   - <code>\"min_sd\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate differs from the average over the lookback period by more than its standard deviation multiplied by this threshold. By default, the value is 2.   - <code>\"min_percentage\"</code>: integer or float, indicates the threshold for a warning to be raised for this error rate when the lookback date specified is strictly lower than 3. By default, the value is 0.  - <code>\"CELL_ID_NOT_VALID\"</code>: refers to events that make reference to an existent cell ID, but the cell is not operative.   - <code>\"sd_lookback_days\"</code>: integer, indicates the days to use as a lookback period to calculate the average and standard deviation over. By default, the value is 7.   - <code>\"min_sd\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate differs from the average over the lookback period by more than its standard deviation multiplied by this threshold. By default, the value is 2.   - <code>\"min_percentage\"</code>: integer or float, indicates the threshold for a warning to be raised for this error rate when the lookback date specified is strictly lower than 3. By default, the value is 0.  - <code>\"INCORRECT_EVENT_LOCATION\"</code>: refers to events that have been flagged as having an incorrect location.   - <code>\"sd_lookback_days\"</code>: integer, indicates the days to use as a lookback period to calculate the average and standard deviation over. By default, the value is 7.   - <code>\"min_sd\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate differs from the average over the lookback period by more than its standard deviation multiplied by this threshold. By default, the value is 2.   - <code>\"min_percentage\"</code>: integer or float, indicates the threshold for a warning to be raised for this error rate when the lookback date specified is strictly lower than 3. By default, the value is 0.  - <code>\"SUSPICIOUS_EVENT_LOCATION\"</code>: refers to events that have been flagged as having a suspicious location.   - <code>\"sd_lookback_days\"</code>: integer, indicates the days to use as a lookback period to calculate the average and standard deviation over. By default, the value is 7.   - <code>\"min_sd\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate differs from the average over the lookback period by more than its standard deviation multiplied by this threshold. By default, the value is 2.   - <code>\"min_percentage\"</code>: integer or float, indicates the threshold for a warning to be raised for this error rate when the lookback date specified is strictly lower than 3. By default, the value is 0.</p>"},{"location":"UserManual/configuration/3_QualityWarnings/3_semantic_quality_warnings/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = SemanticQualityWarnings\n\n[SemanticQualityWarnings]\n\ndate_format = %Y-%m-%d\ndate_of_study = 2023-01-08\n\nthresholds = {\n    \"CELL_ID_NON_EXISTENT\": {\n        \"sd_lookback_days\": 8,\n        \"min_sd\": 1.5,\n        \"min_percentage\": 5\n    },\n    \"CELL_ID_NOT_VALID\": {\n        \"sd_lookback_days\": 8,\n        \"min_sd\": 1.5,\n        \"min_percentage\": 5\n    },\n    \"INCORRECT_EVENT_LOCATION\": {\n        \"sd_lookback_days\": 8,\n        \"min_sd\": 1.5,\n        \"min_percentage\": 5\n    },\n    \"SUSPICIOUS_EVENT_LOCATION\": {\n        \"sd_lookback_days\": 8,\n        \"min_sd\": 1.5,\n        \"min_percentage\": 5\n    },\n    }\n</code></pre>"},{"location":"UserManual/configuration/4_SyntheticMnoData/1_synthetic_diaries/","title":"SyntheticDiaries Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>synthetic_diaries.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Bronze]\n...\ndiaries_data_bronze = ${Paths:bronze_dir}/diaries\n...\n</code></pre> <p>The expected parameters in <code>synthetic_diaries.ini</code> are as follows:  - number_of_users: integer, number of devices for which to generate synthetic diaries in each target date. Example: <code>100</code>.  - date_format: string, format for date parsing from initial_date field (1989 C standard). Example: <code>%Y-%m-%d</code>.  - initial_date: string, initial date for which to generate synthetic diaries, matching the date format specified in date_format. Example: <code>2023-01-01</code>.  - number_of_dates: integer, number of dates for which synthetic diaries will be generated. Synthetic diaries for initial_date and for the following <code>number_of_dates - 1</code> dates will be generated. If diaries are to be generated for just one date, set this parameter equal to 1. Example: <code>9</code>.</p> <ul> <li>latitude_min: float, degrees. Lower limit of the bounding box within which all locations will be generated. Example: <code>40.352</code>.</li> <li>latitude_max: float, degrees. Upper limit of the bounding box within which all locations will be generated. Must be higher than latitude_min. Example: <code>40.486</code>.</li> <li>longitude_min: float, degrees. Left limit of the bounding box within which all locations will be generated. Example: <code>-3.751</code>.</li> <li> <p>longitude_max: float, degrees. Right limit of the bounding box within which all locations will be generated. Must be higher than longitude_min. Example: <code>-3.579</code>.</p> </li> <li> <p>home_work_distance_min: float, meters. Work location of a user will be generated at a distance higher than this minimum from the user's home location. Example: <code>2000</code> (m).</p> </li> <li>home_work_distance_max: float, meters. Work location of a user will be generated at a distance lower than this maximum from the user's home location. Must be higher than home_work_distance_min. Example: <code>10000</code> (m).</li> <li>other_distance_min: float, meters. For activities of type 'other', the location of the activity will be generated at a distance higher than this minimum from the user's previous activity. Example: <code>1000</code> (m).</li> <li> <p>other_distance_max: float, meters. For activities of type 'other', the location of the activity will be generated at a distance lower than this maximum from the user's previous activity. Must be higher than other_distance_min. Example: <code>3000</code> (m).</p> </li> <li> <p>home_duration_min: float, hours. This is the minimum duration that will be considered for an activity with stay type 'home'. Example: <code>5</code> (hours).</p> </li> <li>home_duration_max: float, hours. This is the maximum duration that will be considered for an activity with stay type 'home'. Must be higher than home_duration_min. Example: <code>12</code> (hours).</li> <li>work_duration_min: float, hours. This is the minimum duration that will be considered for an activity with stay type 'work'. Example: <code>4</code> (hours).</li> <li>work_duration_max: float, hours. This is the maximum duration that will be considered for an activity with stay type 'work'. Must be higher than work_duration_min. Example: <code>8</code> (hours).</li> <li>other_duration_min: float, hours. This is the minimum duration that will be considered for an activity with stay type 'other'. Example: <code>1</code> (hours).</li> <li> <p>other_duration_max: float, hours. This is the maximum duration that will be considered for an activity with stay type 'other'. Must be higher than other_duration_min. Example: <code>3</code> (hours).</p> </li> <li> <p>displacement_speed: float, m/s. Given the location of 2 consecutive generated activities for a user, this speed helps us define the second activity's start time once we know the first activity's end time. The distance between both activities is calculated and then divided by this speed in order to calculate the trip time, which is then added to the first activity's end time in order to obtain the second activity's initial time. Example: <code>3</code> (m/s).</p> </li> </ul>"},{"location":"UserManual/configuration/4_SyntheticMnoData/1_synthetic_diaries/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\n# parameters in this section are passed to Spark except session_name \nsession_name = SyntheticDiarySession\n\n[SyntheticDiaries]\nnumber_of_users = 100\ndate_format = %Y-%m-%d\ninitial_date = 2023-01-01\nnumber_of_dates = 9\n\nlatitude_min = 40.352\nlatitude_max = 40.486\nlongitude_min = -3.751\nlongitude_max = -3.579\n\nhome_work_distance_min = 2_000  # in meters\nhome_work_distance_max = 10_000  # in meters\nother_distance_min = 1_000  # in meters\nother_distance_max = 3_000  # in meters\nhome_duration_min = 5  # in hours\nhome_duration_max = 12  # in hours\nwork_duration_min = 4  # in hours\nwork_duration_max = 8  # in hours\nother_duration_min = 1  # in hours\nother_duration_max = 3  # in hours\ndisplacement_speed = 3  # 3 m/s = 10 km/h\n\nstay_sequence_superset = home,other,work,other,other,other,home\nstay_sequence_probabilities = 1,0.1,0.6,0.4,0.2,0.1,1\n</code></pre>"},{"location":"UserManual/configuration/4_SyntheticMnoData/2_synthetic_network/","title":"LongtermPermanenceScore Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>synthetic_network.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Bronze]\n...\nnetwork_data_bronze = ${Paths:bronze_dir}/mno_network\n...\n</code></pre> <p>The expected parameters in <code>synthetic_network.ini</code> are as follows:  - seed: integer, seed for random number generation used to generate the synthetic network topology data.  - n_cells: positive integer, number of synthetic cells that will be generated. Example: <code>500</code>.  - cell_id_generation_type: string, identifier of the generator of cell IDs to be used. Currently the only option available is <code>random_cell_id</code>, which generates a random 14- or 15- digit string. Example: <code>random_cell_id</code>.  - cell_type_options: comma-separated list of strings, it contains the values that the <code>cell_type</code> field can take for the generated synthetic data. Each option has the same probability of being assigned. Example: <code>macrocell, microcell, picocell, femtocell</code>.  - latitude_min: float, minimum latitude that defines the bounding box in which the coordinates of the synthetic cells will be generated.  - latitude_max: float, maximum latitude that defines the bounding box in which the coordinates of the synthetic cells will be generated.  - longitude_min: float, minimum longitude that defines the bounding box in which the coordinates of the synthetic cells will be generated.  - longitude_max: float, maximum longitude that defines the bounding box in which the coordinates of the synthetic cells will be generated.  - altitude_min: float, minimum value that the altitude field can take in the generated cells. Example: <code>-40</code>.  - altitude_max: float, maximum value that the altitude field can take in the generated cells. Example: <code>5000</code>.  - antenna_height_max: float, maximum value that the <code>antenna_height</code> field might take in the generated cells. Example: <code>120</code>.  - altitude_min: float, minimum value that the altitude field can take in the generated cells. Example: <code>-40</code>.  - altitude_max: float, maximum value that the altitude field can take in the generated cells. Example: <code>5000</code>.  - power_min: float, minimum value that the power field can take in the generated cells. Units are watts (W). Example: <code>0.1</code>.  - power_max: float, maximum value that the power field can take in the generated cells. Units are watts (W). Example: <code>500</code>.  - frequency_min: float, minimum value that the frequency field can take in the generated cells. Units are megahertz (MHz). Example: <code>1</code>.  - frequency_max: float, maximum value that the frequency field can take in the generated cells. Units are megahertz (MHz). Example: <code>4000</code>.  - data_period_format: string, it indicates the format expected in <code>earliest_valid_date_start</code> and <code>latest_valid_date_end</code>. For example, use <code>%Y-%m-%dT%H:%M:%S</code> for the usual \"2023-01-09T00:00:00\" format.  - earliest_valid_date_start: string, it indicates the timestamp value that the <code>valid_date_start</code> will take in all generated cells. Example: <code>2022-12-15T00:00:00</code>.  - latest_valid_date_end: string, it indicates the timestamp value that the <code>valid_date_end</code> will take in all generated cells. Example: <code>2023-01-15T00:00:00</code>.  - date_format: string, it indicates the format expected in <code>starting_date</code> and <code>ending_date</code>. For example, use <code>%Y-%m-%d</code> for the usual \"2023-01-09\" format separated by <code>-</code>. Example: <code>%Y-%m-%d</code>.  - starting_date: string, format should be the one specified <code>date_format</code> (e.g., <code>2023-01-01</code> for <code>%Y-%m-%d</code>), the first date for which data will be generated. All dates between this one and the specified in <code>ending_date</code> will be have data generated (both inclusive). The cell properties will be equal across all dates.   - ending_date: string, format should be the one specified <code>date_format</code> (e.g., <code>2023-01-09</code> for <code>%Y-%m-%d</code>), the last date for which data will be generated. All dates between the specified in <code>starting_date</code> and this one will be have data generated (both inclusive). The cell properties will be equal across all dates.   - no_optional_fields_probability: float, probability that all of the optional fields of a record take the null value. Example: <code>0.05</code>.  - mandatory_null_probability: float, probability that one of the mandatory fields of a record will take a null value. Example: <code>0.05</code>.  - out_of_bounds_values_probability: float, probability that a field of a record will take a value outside its valid values. This could be, for example, a negative power or a latitude outside the $[-90, 90]$ interval. Example: <code>0.05</code>.  - erroneous_values_probability: float, probability that one of the following erroneous values might occur:    - The <code>cell_id</code> takes a non-valid value (not a 14- or 15-digit string).    - The <code>valid_date_start</code> and <code>valid_date_end</code> fields has an invalid timestamp format.    - The <code>valid_date_end</code> is a point in time earlier than <code>valid_date_start</code>.</p> <pre><code>Example: `0.05`.\n</code></pre>"},{"location":"UserManual/configuration/4_SyntheticMnoData/2_synthetic_network/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = SyntheticNetworkSession\n\n[SyntheticNetwork]\nseed = 33\nn_cells = 500\ncell_id_generation_type = random_cell_id  # options: random_cell_id\ncell_type_options = macrocell, microcell, picocell, femtocell\nlatitude_min = 40.352\nlatitude_max = 40.486\nlongitude_min = -3.751\nlongitude_max = -3.579\n\naltitude_min = -40\naltitude_max = 5000\n# antenna_height_min is always 0\nantenna_height_max = 120\npower_min = 0.1\npower_max = 500\nfrequency_min = 1\nfrequency_max = 4000\ntimestamp_format = %Y-%m-%dT%H:%M:%S\nearliest_valid_date_start = 2022-12-15T00:00:00\nlatest_valid_date_end = 2023-01-15T00:00:00\ndate_format = %Y-%m-%d\nstarting_date = 2023-01-01\nending_date = 2023-01-09\n\nno_optional_fields_probability = 0.0\nmandatory_null_probability = 0.0\nout_of_bounds_values_probability = 0.0\nerroneous_values_probability = 0.0\n</code></pre>"},{"location":"UserManual/configuration/4_SyntheticMnoData/3_synthetic_events/","title":"GeozonesGridMapping Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>synthetic_events.ini</code>. In <code>general_config.ini</code> to execute the component specify all paths to its corresponding data objects (input + output). </p> <pre><code>[Paths.Bronze]\ndiaries_data_bronze = ${Paths:bronze_dir}/diaries\nevent_data_bronze = ${Paths:bronze_dir}/mno_events\nnetwork_data_bronze = ${Paths:bronze_dir}/mno_network\n</code></pre> <p>In synthetic_events.ini parameters are as follows: </p> <ul> <li> <p>seed -integer, the random seed value for all subprocesses that involve randomness, such as the random generation of timestamps, latitude, longitude,  random selection of cell to be linked to a point, random selection of rows and columns for null generation, random selection of rows for duplicates generations, etc.</p> </li> <li> <p>event_freq_stays - integer, the frequency in seconds for events to be generated for stays (higher means that less events will be generated for a given stay in a given time interval in synthetic diaries).</p> </li> <li> <p>event_freq_moves - integer, the frequency in seconds for events to be generated for moves (higher means that less events will be generated for a given move in a given time interval in synthetic diaries).</p> </li> <li> <p>error_location_probability - float (between 0.0 and 1.0), ratio of rows from all generated events, to be selected for generating errors in x and y coordinates.</p> </li> <li> <p>error_location_distance_min - integer, the minimum distance in meters to which generated points may reach from the original generated point, when offseting these points to be erroneous</p> </li> <li> <p>error_location_distance_max - integer, the maximum distance in meters to which generated points may reach from the original generated point, when offseting these points to be erroneous.  </p> </li> <li> <p>error_cell_id_probability - float (between 0.0 and 1.0), the ratio of rows that are to be selected from all generated events, for generating events with nonexistent cell ids. These are events that have a syntactically valid cell_id that is not present in the input network data.</p> </li> <li> <p>mcc - integer, the value for the mcc column to be applied to the data of all generated users.</p> </li> <li> <p>maximum_number_of_cells_for_event - integer, the maximimum number of cells to consider, when linking a generated point to a cell in the input network data. If several cells are within the distance provided by closest_cell_distance_max for a given generated point (erroneous or not), a single cell is selected from as many closest cells, randomly, as given by maximum_number_of_cells_for_event.</p> </li> <li> <p>closest_cell_distance_max - integer, the distance in meters to a cell from a generated point, for that cell to be included as one of the possible cells to be linked to the given point.</p> </li> <li> <p>closest_cell_distance_max_for_errors - integer, the distance in meters to a cell from an erroneously generated point, for the cell to be included as one of the possible cells to be linked to that point.</p> </li> <li> <p>closest_cell_distance_max_for_errors - integer, the distance in meters to a cell from an erroneously generated point, for the cell to be included as one of the possible cells to be linked to that point.</p> </li> <li> <p>do_event_error_generation - boolean, whether to generate syntactic errors for clean rows that have been generated.</p> </li> <li> <p>null_row_probability - float, probability to use for sampling rows for which one or more columns will be set to null. If set to 0, no null rows are generated. Which columns on the sampeld rows are selected as null is affected by the parameter column_is_null_probability.</p> </li> <li> <p>out_of_bounds_probability - float, probability to use for sampling rows for which timestamp will be replaced with a timestamp that is out of the temporal bounds set in the input synthetic diaries. These rows will not include any other errors.</p> </li> <li> <p>data_type_error_probability - float, probability to use for sampling rows for which a random selection of columns will be modified as erroneous. Modifications are column specific, for instance the cell_id column will be replaced by random string. </p> </li> <li> <p>column_is_null_probability - float, probability that a given column will be set as null for the rows that have been sampled for null selection. If column_is_null_probability=1, all columns in the rows selected for null generation are replaced with nulls. If null_row_probability=0, this parameter is not used.</p> </li> <li> <p>same_location_duplicates_probability - float, probability for selecting rows that will be transformed into same location duplicates. The value corresponds to the approximate ratio of rows in the final output that will include pair-wise full duplicates. This process generates at a maximum two duplicate records for a given user, not more.</p> </li> <li> <p>different_location_duplicates_probability - float, probability for selecting rows that will be transformed into different location duplicates. The value corresponds to the approximate ratio of rows in the final output that will include pair-wise duplicates in all columns, except for longitude and latitude. This process generates at a maximum two duplicate records for a given user, not more.</p> </li> <li> <p>order_output_by_timestamp - boolean, whether to order the final output by the timestamp column.</p> </li> <li> <p>cartesian_crs - integer, the coordinate reference system (CRS) to use for the cartesian coordinates. The default value is 3035.</p> </li> </ul> <p>Note on probability parameters: These parameters do not necessarily translate to the exact ratios for each probability type in the output data object, as the exact number of rows selected is affected by the random seed, in addition to the probability value itself.</p>"},{"location":"UserManual/configuration/4_SyntheticMnoData/3_synthetic_events/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\n\nsession_name = SyntheticEventsSession\n\n\n[SyntheticEvents]\nseed = 999\nevent_freq_stays = 2400 # s\nevent_freq_moves = 1200  # s\nerror_location_probability = 0.0\nerror_location_distance_min = 1000 # m\nerror_location_distance_max = 10000 # m\nerror_cell_id_probability  = 0.0\nmcc = 214\nmaximum_number_of_cells_for_event = 3\nclosest_cell_distance_max = 5000 # m\nclosest_cell_distance_max_for_errors = 10000 # m\ncartesian_crs = 3035\n\ndo_event_error_generation = True \nnull_row_probability = 0.3 \nout_of_bounds_probability = 0.0 \ndata_type_error_probability = 0.3 \ncolumn_is_null_probability = 0.5 \ndifferent_location_duplicates_probability = 0.3\nsame_location_duplicates_probability = 0.25\n\norder_output_by_timestamp = True\n</code></pre>"},{"location":"autodoc/code_quality_report/","title":"Code quality report","text":"Pylint report Pylint report from report.jinja2 Score         8.54 / 10            Messages Module <code>components.execution.cell_connection_probability.cell_connection_probability</code> (<code>multimno/components/execution/cell_connection_probability/cell_connection_probability.py</code>) Line Col. Type Symbol ID Obj Message 92 9 warning <code>fixme</code> W0511 <pre>TODO: We might need to iterate over dates and process the data for each date separately</pre> 135 13 warning <code>fixme</code> W0511 <pre>TODO should default value be configurable? 1 would keep cell connection probability value</pre> Module <code>components.execution.cell_footprint.cell_footprint_estimation</code> (<code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code>) Line Col. Type Symbol ID Obj Message 29 0 refactor <code>too-many-instance-attributes</code> R0902 CellFootprintEstimation <pre>Too many instance attributes (16/7)</pre> 111 9 warning <code>fixme</code> W0511 <pre>TODO: We might need to iterate over dates and process the data for each date separately</pre> 350 4 convention <code>missing-function-docstring</code> C0116 CellFootprintEstimation.generate_combinations <pre>Missing function or method docstring</pre> Module <code>components.execution.daily_permanence_score.daily_permanence_score</code> (<code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code>) Line Col. Type Symbol ID Obj Message 26 0 refactor <code>too-many-instance-attributes</code> R0902 DailyPermanenceScore <pre>Too many instance attributes (21/7)</pre> 56 44 error <code>no-member</code> E1101 DailyPermanenceScore.__init__ <pre>Instance of 'ConfigParser' has no 'geteval' member</pre> 438 16 refactor <code>consider-using-min-builtin</code> R1730 DailyPermanenceScore.calculate_min_distance_between_point_lists <pre>Consider using 'min_distance = min(min_distance, distance)' instead of unnecessary if block</pre> 456 9 warning <code>fixme</code> W0511 <pre>TODO: optimise</pre> 534 20 convention <code>singleton-comparison</code> C0121 DailyPermanenceScore.determine_stay_durations <pre>Comparison 'F.col('is_move') == False' should be 'F.col('is_move') is False' if checking for the singleton value False, or 'not F.col('is_move')' if testing for falsiness</pre> Module <code>components.execution.device_activity_statistics.device_activity_statistics</code> (<code>multimno/components/execution/device_activity_statistics/device_activity_statistics.py</code>) Line Col. Type Symbol ID Obj Message 12 0 convention <code>ungrouped-imports</code> C0412 <pre>Imports from package pyspark are not grouped</pre> 34 0 refactor <code>too-many-instance-attributes</code> R0902 DeviceActivityStatistics <pre>Too many instance attributes (16/7)</pre> 50 9 warning <code>fixme</code> W0511 <pre>TODO: update this to semantically cleaned files after merge</pre> 52 9 warning <code>fixme</code> W0511 <pre>TODO: Figure out how this would work with coverage areas. We don't have location of cells in those cases</pre> 113 13 warning <code>fixme</code> W0511 <pre>TODO: should this selection also be based on user_id_modulo?</pre> 273 9 warning <code>fixme</code> W0511 <pre>TODO: check if this is the correct way to calculate, got varying results</pre> Module <code>components.execution.event_cleaning.event_cleaning</code> (<code>multimno/components/execution/event_cleaning/event_cleaning.py</code>) Line Col. Type Symbol ID Obj Message 5 0 warning <code>unused-import</code> W0611 <pre>Unused defaultdict imported from collections</pre> 44 0 refactor <code>too-many-instance-attributes</code> R0902 EventCleaning <pre>Too many instance attributes (23/7)</pre> 70 28 error <code>no-member</code> E1101 EventCleaning.__init__ <pre>Instance of 'ConfigParser' has no 'geteval' member</pre> 164 12 warning <code>attribute-defined-outside-init</code> W0201 EventCleaning.execute <pre>Attribute 'current_date' defined outside __init__</pre> 166 12 warning <code>attribute-defined-outside-init</code> W0201 EventCleaning.execute <pre>Attribute 'current_input_do' defined outside __init__</pre> 178 8 warning <code>attribute-defined-outside-init</code> W0201 EventCleaning.transform <pre>Attribute 'quality_metrics_distribution_before' defined outside __init__</pre> 215 9 warning <code>fixme</code> W0511 <pre>TODO: discuss is this step even needed (did since it was in Method description)</pre> 238 8 warning <code>attribute-defined-outside-init</code> W0201 EventCleaning.transform <pre>Attribute 'quality_metrics_distribution_after' defined outside __init__</pre> 242 9 warning <code>fixme</code> W0511 <pre>TODO: discuss</pre> 260 9 warning <code>fixme</code> W0511 <pre>TODO: remove this, if we decide to save domain column</pre> 603 9 warning <code>fixme</code> W0511 <pre>TODO: Check timestamp validation</pre> 652 0 convention <code>line-too-long</code> C0301 <pre>Line too long (122/120)</pre> 652 9 warning <code>fixme</code> W0511 <pre>TODO: if we decide not to use data_period_filtering don't forget to put this in convert_time_column_to_timestamp</pre> 753 9 warning <code>fixme</code> W0511 <pre>TODO make hex truncation (substring parameters) as configurable by user?</pre> Module <code>components.execution.event_semantic_cleaning.event_semantic_cleaning</code> (<code>multimno/components/execution/event_semantic_cleaning/event_semantic_cleaning.py</code>) Line Col. Type Symbol ID Obj Message 8 0 warning <code>unused-import</code> W0611 <pre>Unused StructField imported from pyspark.sql.types</pre> 31 0 refactor <code>too-many-instance-attributes</code> R0902 SemanticCleaning <pre>Too many instance attributes (13/7)</pre> 146 0 convention <code>line-too-long</code> C0301 <pre>Line too long (123/120)</pre> 269 9 warning <code>fixme</code> W0511 <pre>TODO: find best ordering</pre> 364 0 convention <code>line-too-long</code> C0301 <pre>Line too long (127/120)</pre> 367 0 convention <code>line-too-long</code> C0301 <pre>Line too long (130/120)</pre> 374 0 convention <code>line-too-long</code> C0301 <pre>Line too long (124/120)</pre> 455 12 warning <code>attribute-defined-outside-init</code> W0201 SemanticCleaning.execute <pre>Attribute 'events_df' defined outside __init__</pre> 459 12 warning <code>attribute-defined-outside-init</code> W0201 SemanticCleaning.execute <pre>Attribute 'cells_df' defined outside __init__</pre> Module <code>components.execution.geozones_grid_mapping.geozones_grid_mapping</code> (<code>multimno/components/execution/geozones_grid_mapping/geozones_grid_mapping.py</code>) Line Col. Type Symbol ID Obj Message 5 0 warning <code>unused-import</code> W0611 <pre>Unused Any imported from typing</pre> 31 0 refactor <code>consider-using-from-import</code> R0402 <pre>Use 'from multimno.core import utils' instead</pre> 44 34 error <code>no-member</code> E1101 GeozonesGridMapping.__init__ <pre>Instance of 'ConfigParser' has no 'geteval' member</pre> 62 12 error <code>possibly-used-before-assignment</code> E0606 GeozonesGridMapping.initalize_data_objects <pre>Possibly using variable 'zoning_data' before assignment</pre> 101 12 warning <code>attribute-defined-outside-init</code> W0201 GeozonesGridMapping.execute <pre>Attribute 'current_dataset_id' defined outside __init__</pre> 168 0 convention <code>line-too-long</code> C0301 <pre>Line too long (130/120)</pre> 171 0 convention <code>line-too-long</code> C0301 <pre>Line too long (127/120)</pre> 214 0 convention <code>line-too-long</code> C0301 <pre>Line too long (130/120)</pre> 241 26 warning <code>f-string-without-interpolation</code> W1309 GeozonesGridMapping.extract_hierarchy_ids <pre>Using an f-string that does not have any interpolated variables</pre> Module <code>components.execution.grid_enrichment.grid_enrichment</code> (<code>multimno/components/execution/grid_enrichment/grid_enrichment.py</code>) Line Col. Type Symbol ID Obj Message 7 0 warning <code>unused-import</code> W0611 <pre>Unused import math</pre> 8 0 warning <code>unused-import</code> W0611 <pre>Unused st_constructors imported from sedona.sql as STC</pre> 16 0 warning <code>unused-import</code> W0611 <pre>Unused BooleanType imported from pyspark.sql.types</pre> 41 0 refactor <code>consider-using-from-import</code> R0402 <pre>Use 'from multimno.core import utils' instead</pre> 44 0 refactor <code>too-many-instance-attributes</code> R0902 GridEnrichment <pre>Too many instance attributes (13/7)</pre> 55 48 error <code>no-member</code> E1101 GridEnrichment.__init__ <pre>Instance of 'ConfigParser' has no 'geteval' member</pre> 58 29 error <code>no-member</code> E1101 GridEnrichment.__init__ <pre>Instance of 'ConfigParser' has no 'geteval' member</pre> 59 39 error <code>no-member</code> E1101 GridEnrichment.__init__ <pre>Instance of 'ConfigParser' has no 'geteval' member</pre> 74 9 warning <code>fixme</code> W0511 <pre>TODO: For now set to default 100, but can be dynamic from config in future</pre> 128 12 error <code>assignment-from-no-return</code> E1111 GridEnrichment.transform <pre>Assigning result of a function call, where the function has no return</pre> 136 29 warning <code>f-string-without-interpolation</code> W1309 GridEnrichment.transform <pre>Using an f-string that does not have any interpolated variables</pre> 140 16 warning <code>attribute-defined-outside-init</code> W0201 GridEnrichment.transform <pre>Attribute 'current_quadkey' defined outside __init__</pre> 158 17 warning <code>fixme</code> W0511 <pre>TODO: asses if it would be possible to use persist instead of checkpoint</pre> 191 4 convention <code>missing-function-docstring</code> C0116 GridEnrichment.add_elevation_to_grid <pre>Missing function or method docstring</pre> 192 9 warning <code>fixme</code> W0511 <pre>TODO: implement elevation enrichment</pre> 204 0 convention <code>line-too-long</code> C0301 <pre>Line too long (126/120)</pre> 231 0 convention <code>line-too-long</code> C0301 <pre>Line too long (124/120)</pre> 285 9 warning <code>fixme</code> W0511 <pre>TODO: make vertices number parameter</pre> 361 18 refactor <code>unnecessary-comprehension</code> R1721 GridEnrichment.calculate_landuse_ratios_per_tile <pre>Unnecessary use of a comprehension, use list(self.prior_weights.keys()) instead.</pre> 378 9 warning <code>fixme</code> W0511 <pre>TODO: remove hardcoded values</pre> 407 18 refactor <code>unnecessary-comprehension</code> R1721 GridEnrichment.populate_grid_tiles_with_empty_ratios <pre>Unnecessary use of a comprehension, use list(self.prior_weights.keys()) instead.</pre> 418 0 convention <code>line-too-long</code> C0301 <pre>Line too long (138/120)</pre> 445 0 convention <code>line-too-long</code> C0301 <pre>Line too long (123/120)</pre> 446 0 convention <code>line-too-long</code> C0301 <pre>Line too long (129/120)</pre> 533 0 convention <code>line-too-long</code> C0301 <pre>Line too long (121/120)</pre> 586 0 convention <code>line-too-long</code> C0301 <pre>Line too long (143/120)</pre> Module <code>components.execution.longterm_permanence_score.longterm_permanence_score</code> (<code>multimno/components/execution/longterm_permanence_score/longterm_permanence_score.py</code>) Line Col. Type Symbol ID Obj Message 25 0 refactor <code>too-many-instance-attributes</code> R0902 LongtermPermanenceScore <pre>Too many instance attributes (13/7)</pre> 33 4 refactor <code>too-many-branches</code> R0912 LongtermPermanenceScore.__init__ <pre>Too many branches (21/12)</pre> 33 4 refactor <code>too-many-statements</code> R0915 LongtermPermanenceScore.__init__ <pre>Too many statements (59/50)</pre> 67 30 error <code>no-member</code> E1101 LongtermPermanenceScore.__init__ <pre>Instance of 'ConfigParser' has no 'geteval' member</pre> Module <code>components.execution.midterm_permanence_score.midterm_permanence_score</code> (<code>multimno/components/execution/midterm_permanence_score/midterm_permanence_score.py</code>) Line Col. Type Symbol ID Obj Message 62 4 refactor <code>no-else-return</code> R1705 frequency_and_regularity <pre>Unnecessary \"elif\" after \"return\", remove the leading \"el\" from \"elif\"</pre> 99 0 refactor <code>too-many-instance-attributes</code> R0902 MidtermPermanenceScore <pre>Too many instance attributes (19/7)</pre> 111 4 refactor <code>too-many-locals</code> R0914 MidtermPermanenceScore.__init__ <pre>Too many local variables (16/15)</pre> 111 4 refactor <code>too-many-branches</code> R0912 MidtermPermanenceScore.__init__ <pre>Too many branches (18/12)</pre> 111 4 refactor <code>too-many-statements</code> R0915 MidtermPermanenceScore.__init__ <pre>Too many statements (70/50)</pre> 187 0 convention <code>line-too-long</code> C0301 <pre>Line too long (123/120)</pre> 195 0 convention <code>line-too-long</code> C0301 <pre>Line too long (147/120)</pre> 222 30 error <code>no-member</code> E1101 MidtermPermanenceScore.__init__ <pre>Instance of 'ConfigParser' has no 'geteval' member</pre> 423 4 refactor <code>too-many-branches</code> R0912 MidtermPermanenceScore.filter_dps_by_time_interval <pre>Too many branches (13/12)</pre> 517 0 convention <code>line-too-long</code> C0301 <pre>Line too long (126/120)</pre> 541 4 refactor <code>too-many-return-statements</code> R0911 MidtermPermanenceScore.filter_dps_by_day_type <pre>Too many return statements (10/6)</pre> 751 0 convention <code>line-too-long</code> C0301 <pre>Line too long (126/120)</pre> Module <code>components.execution.network_cleaning.network_cleaning</code> (<code>multimno/components/execution/network_cleaning/network_cleaning.py</code>) Line Col. Type Symbol ID Obj Message 38 0 refactor <code>too-many-instance-attributes</code> R0902 NetworkCleaning <pre>Too many instance attributes (18/7)</pre> 139 4 refactor <code>too-many-locals</code> R0914 NetworkCleaning.transform <pre>Too many local variables (40/15)</pre> 139 4 refactor <code>too-many-branches</code> R0912 NetworkCleaning.transform <pre>Too many branches (20/12)</pre> 139 4 refactor <code>too-many-statements</code> R0915 NetworkCleaning.transform <pre>Too many statements (107/50)</pre> 152 0 convention <code>line-too-long</code> C0301 <pre>Line too long (127/120)</pre> 237 9 warning <code>fixme</code> W0511 <pre>TODO: correct check for CGI in cell ids</pre> 243 9 warning <code>fixme</code> W0511 <pre>TODO: cover case where bounding box crosses the -180/180 longitude</pre> 350 24 refactor <code>use-dict-literal</code> R1735 NetworkCleaning.transform <pre>Consider using '{}' instead of a call to 'dict'.</pre> 366 0 convention <code>consider-using-dict-items</code> C0206 NetworkCleaning.transform <pre>Consider iterating with .items()</pre> 401 34 error <code>invalid-unary-operand-type</code> E1130 NetworkCleaning.transform <pre>bad operand type for unary ~: object</pre> 445 9 warning <code>fixme</code> W0511 <pre>TODO: possible improvement if pyspark.sql.GroupedData.pivot can be used instead.</pre> 463 23 refactor <code>use-dict-literal</code> R1735 NetworkCleaning.transform <pre>Consider using '{}' instead of a call to 'dict'.</pre> 617 27 warning <code>unnecessary-lambda</code> W0108 NetworkCleaning.transform.&lt;lambda&gt; <pre>Lambda may not be necessary</pre> Module <code>components.execution.present_population.present_population_estimation</code> (<code>multimno/components/execution/present_population/present_population_estimation.py</code>) Line Col. Type Symbol ID Obj Message 38 0 refactor <code>too-many-instance-attributes</code> R0902 PresentPopulationEstimation <pre>Too many instance attributes (15/7)</pre> 66 12 warning <code>fixme</code> W0511 <pre>TODO this should come from global config?</pre> 86 0 convention <code>line-too-long</code> C0301 <pre>Line too long (153/120)</pre> 116 0 convention <code>line-too-long</code> C0301 <pre>Line too long (153/120)</pre> 163 9 warning <code>fixme</code> W0511 <pre>TODO: optimizing when to write.</pre> 302 13 warning <code>fixme</code> W0511 <pre>TODO: do not enter iterations, as everything will be zero!</pre> 306 9 warning <code>fixme</code> W0511 <pre>TODO: total number of grid tiles, or total number of grid tiles covered by the cells our events refer to?</pre> 414 9 warning <code>fixme</code> W0511 <pre>TODO Casting type to float. Should this happen earlier, or not at all (have result col type be Double)?</pre> 433 5 warning <code>fixme</code> W0511 <pre>TODO this might be reusable across components.</pre> 484 5 warning <code>fixme</code> W0511 <pre>TODO this might be reusable across components.</pre> Module <code>components.execution.signal_strength.signal_stength_modeling</code> (<code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code>) Line Col. Type Symbol ID Obj Message 32 0 refactor <code>too-many-instance-attributes</code> R0902 SignalStrengthModeling <pre>Too many instance attributes (11/7)</pre> 32 0 refactor <code>too-many-public-methods</code> R0904 SignalStrengthModeling <pre>Too many public methods (23/20)</pre> 62 39 error <code>no-member</code> E1101 SignalStrengthModeling.__init__ <pre>Instance of 'ConfigParser' has no 'geteval' member</pre> 110 9 warning <code>fixme</code> W0511 <pre>TODO: We might need to iterate over dates and process the data for each date separately</pre> 113 9 warning <code>fixme</code> W0511 <pre>TODO: Potentially</pre> 122 9 warning <code>fixme</code> W0511 <pre>TODO: Add Path Loss Exponent calculation based on grid landuse data</pre> 558 4 refactor <code>inconsistent-return-statements</code> R1710 SignalStrengthModeling.normal_distribution <pre>Either all return statements in a function should return an expression, or none of them should.</pre> 575 8 refactor <code>no-else-return</code> R1705 SignalStrengthModeling.normal_distribution <pre>Unnecessary \"elif\" after \"return\", remove the leading \"el\" from \"elif\"</pre> 582 4 convention <code>invalid-name</code> C0103 SignalStrengthModeling.norm_dBloss <pre>Method name \"norm_dBloss\" doesn't conform to snake_case naming style</pre> 607 4 convention <code>missing-function-docstring</code> C0116 SignalStrengthModeling.norm_dBloss_udf <pre>Missing function or method docstring</pre> 607 4 convention <code>invalid-name</code> C0103 SignalStrengthModeling.norm_dBloss_udf <pre>Method name \"norm_dBloss_udf\" doesn't conform to snake_case naming style</pre> 689 9 warning <code>fixme</code> W0511 <pre>TODO: simplify math in this function by using Sedona built in spatial methods</pre> Module <code>components.execution.time_segments.continuous_time_segmentation</code> (<code>multimno/components/execution/time_segments/continuous_time_segmentation.py</code>) Line Col. Type Symbol ID Obj Message 40 0 refactor <code>too-many-instance-attributes</code> R0902 ContinuousTimeSegmentation <pre>Too many instance attributes (18/7)</pre> 65 44 error <code>no-member</code> E1101 ContinuousTimeSegmentation.__init__ <pre>Instance of 'ConfigParser' has no 'geteval' member</pre> 129 12 warning <code>attribute-defined-outside-init</code> W0201 ContinuousTimeSegmentation.execute <pre>Attribute 'intital_time_segment' defined outside __init__</pre> 132 12 warning <code>attribute-defined-outside-init</code> W0201 ContinuousTimeSegmentation.execute <pre>Attribute 'intital_time_segment' defined outside __init__</pre> 134 19 convention <code>singleton-comparison</code> C0121 ContinuousTimeSegmentation.execute <pre>Comparison 'F.col(ColNames.is_last) == True' should be 'F.col(ColNames.is_last) is True' if checking for the singleton value True, or 'bool(F.col(ColNames.is_last))' if testing for truthiness</pre> 138 12 warning <code>attribute-defined-outside-init</code> W0201 ContinuousTimeSegmentation.execute <pre>Attribute 'intital_time_segment' defined outside __init__</pre> 148 12 warning <code>attribute-defined-outside-init</code> W0201 ContinuousTimeSegmentation.execute <pre>Attribute 'intital_time_segment' defined outside __init__</pre> 158 12 warning <code>attribute-defined-outside-init</code> W0201 ContinuousTimeSegmentation.execute <pre>Attribute 'current_date' defined outside __init__</pre> 159 12 warning <code>attribute-defined-outside-init</code> W0201 ContinuousTimeSegmentation.execute <pre>Attribute 'current_input_events_sdf' defined outside __init__</pre> 164 12 warning <code>attribute-defined-outside-init</code> W0201 ContinuousTimeSegmentation.execute <pre>Attribute 'current_interesection_groups_sdf' defined outside __init__</pre> 191 9 warning <code>fixme</code> W0511 <pre>TODO: This conversion is needed for Pandas serialisation/deserialisation,</pre> 208 9 warning <code>fixme</code> W0511 <pre>TODO: To test this approach with large datasets, might not be feasible</pre> 227 9 warning <code>fixme</code> W0511 <pre>TODO: To test this approach with large datasets, might not be feasible</pre> 242 8 warning <code>attribute-defined-outside-init</code> W0201 ContinuousTimeSegmentation.transform <pre>Attribute 'current_segments_sdf' defined outside __init__</pre> 247 52 convention <code>singleton-comparison</code> C0121 ContinuousTimeSegmentation.transform <pre>Comparison 'F.col(ColNames.is_last) == True' should be 'F.col(ColNames.is_last) is True' if checking for the singleton value True, or 'bool(F.col(ColNames.is_last))' if testing for truthiness</pre> 260 8 warning <code>attribute-defined-outside-init</code> W0201 ContinuousTimeSegmentation.transform <pre>Attribute 'intital_time_segment' defined outside __init__</pre> 263 9 warning <code>fixme</code> W0511 <pre>TODO: This conversion is needed to get back to binary after Pandas serialisation/deserialisation,</pre> 282 4 refactor <code>too-many-arguments</code> R0913 ContinuousTimeSegmentation.aggregate_stays <pre>Too many arguments (8/5)</pre> 282 4 refactor <code>too-many-locals</code> R0914 ContinuousTimeSegmentation.aggregate_stays <pre>Too many local variables (29/15)</pre> 282 4 refactor <code>too-many-statements</code> R0915 ContinuousTimeSegmentation.aggregate_stays <pre>Too many statements (55/50)</pre> 460 9 warning <code>fixme</code> W0511 <pre>TODO: NOT IMPLEMENTED.</pre> 461 0 convention <code>line-too-long</code> C0301 <pre>Line too long (127/120)</pre> 462 0 convention <code>line-too-long</code> C0301 <pre>Line too long (126/120)</pre> 490 4 refactor <code>too-many-arguments</code> R0913 ContinuousTimeSegmentation.handle_first_segment <pre>Too many arguments (6/5)</pre> 600 4 refactor <code>too-many-arguments</code> R0913 ContinuousTimeSegmentation.initialize_user_and_ts <pre>Too many arguments (6/5)</pre> Module <code>components.execution.usual_environment_aggregation.usual_environment_aggregation</code> (<code>multimno/components/execution/usual_environment_aggregation/usual_environment_aggregation.py</code>) Line Col. Type Symbol ID Obj Message 6 0 warning <code>unused-import</code> W0611 <pre>Unused Any imported from typing</pre> 6 0 warning <code>unused-import</code> W0611 <pre>Unused Dict imported from typing</pre> 11 0 convention <code>wrong-import-order</code> C0411 <pre>third party import \"pyspark.sql.functions\" should be placed before first party import \"multimno.core.data_objects.silver.silver_grid_data_object.SilverGridDataObject\" </pre> 12 0 convention <code>wrong-import-order</code> C0411 <pre>third party import \"pyspark.sql.window.Window\" should be placed before first party import \"multimno.core.data_objects.silver.silver_grid_data_object.SilverGridDataObject\" </pre> 13 0 convention <code>wrong-import-order</code> C0411 <pre>third party import \"pyspark.sql.DataFrame\" should be placed before first party import \"multimno.core.data_objects.silver.silver_grid_data_object.SilverGridDataObject\" </pre> 15 0 convention <code>ungrouped-imports</code> C0412 <pre>Imports from package multimno are not grouped</pre> 32 0 refactor <code>consider-using-from-import</code> R0402 <pre>Use 'from multimno.core import utils' instead</pre> 175 0 convention <code>line-too-long</code> C0301 <pre>Line too long (127/120)</pre> 198 0 convention <code>line-too-long</code> C0301 <pre>Line too long (122/120)</pre> 201 0 convention <code>line-too-long</code> C0301 <pre>Line too long (126/120)</pre> 218 0 convention <code>line-too-long</code> C0301 <pre>Line too long (142/120)</pre> 241 0 convention <code>line-too-long</code> C0301 <pre>Line too long (121/120)</pre> 265 0 convention <code>line-too-long</code> C0301 <pre>Line too long (124/120)</pre> 266 0 convention <code>line-too-long</code> C0301 <pre>Line too long (133/120)</pre> Module <code>components.execution.usual_environment_labeling.usual_environment_labeling</code> (<code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code>) Line Col. Type Symbol ID Obj Message 29 0 refactor <code>too-many-instance-attributes</code> R0902 UsualEnvironmentLabeling <pre>Too many instance attributes (15/7)</pre> 29 0 refactor <code>too-many-public-methods</code> R0904 UsualEnvironmentLabeling <pre>Too many public methods (24/20)</pre> 51 12 warning <code>fixme</code> W0511 <pre>TODO:</pre> 563 4 refactor <code>too-many-locals</code> R0914 UsualEnvironmentLabeling.compute_generic_labeling <pre>Too many local variables (23/15)</pre> 755 4 refactor <code>too-many-locals</code> R0914 UsualEnvironmentLabeling.generate_quality_metrics <pre>Too many local variables (19/15)</pre> Module <code>components.ingestion.grid_generation.inspire_grid_generation</code> (<code>multimno/components/ingestion/grid_generation/inspire_grid_generation.py</code>) Line Col. Type Symbol ID Obj Message 7 0 warning <code>unused-import</code> W0611 <pre>Unused DataFrame imported from pyspark.sql</pre> 23 0 refactor <code>consider-using-from-import</code> R0402 <pre>Use 'from multimno.core import utils' instead</pre> 26 0 refactor <code>too-many-instance-attributes</code> R0902 InspireGridGeneration <pre>Too many instance attributes (13/7)</pre> 37 27 error <code>no-member</code> E1101 InspireGridGeneration.__init__ <pre>Instance of 'ConfigParser' has no 'geteval' member</pre> 49 9 warning <code>fixme</code> W0511 <pre>TODO: For now set to default 100, but can be dynamic from config in future</pre> 102 16 warning <code>redefined-builtin</code> W0622 InspireGridGeneration.execute <pre>Redefining built-in 'id'</pre> 103 16 warning <code>attribute-defined-outside-init</code> W0201 InspireGridGeneration.execute <pre>Attribute 'current_country_part' defined outside __init__</pre> 129 4 convention <code>missing-function-docstring</code> C0116 InspireGridGeneration.get_country_mask <pre>Missing function or method docstring</pre> Module <code>components.ingestion.spatial_data_ingestion.gisco_data_ingestion</code> (<code>multimno/components/ingestion/spatial_data_ingestion/gisco_data_ingestion.py</code>) Line Col. Type Symbol ID Obj Message 1 0 convention <code>empty-docstring</code> C0112 <pre>Empty module docstring</pre> 5 0 warning <code>unused-import</code> W0611 <pre>Unused import math</pre> 6 0 warning <code>unused-import</code> W0611 <pre>Unused Dict imported from typing</pre> 6 0 warning <code>unused-import</code> W0611 <pre>Unused List imported from typing</pre> 6 0 warning <code>unused-import</code> W0611 <pre>Unused Optional imported from typing</pre> 6 0 warning <code>unused-import</code> W0611 <pre>Unused Tuple imported from typing</pre> 27 0 refactor <code>consider-using-from-import</code> R0402 <pre>Use 'from multimno.core import utils' instead</pre> 30 0 convention <code>empty-docstring</code> C0112 GiscoDataIngestion <pre>Empty class docstring</pre> 30 0 refactor <code>too-many-instance-attributes</code> R0902 GiscoDataIngestion <pre>Too many instance attributes (9/7)</pre> 35 4 warning <code>useless-parent-delegation</code> W0246 GiscoDataIngestion.__init__ <pre>Useless parent or super() delegation in method '__init__'</pre> 143 25 warning <code>f-string-without-interpolation</code> W1309 GiscoDataIngestion.get_countries_data <pre>Using an f-string that does not have any interpolated variables</pre> Module <code>components.ingestion.spatial_data_ingestion.overture_data_ingestion</code> (<code>multimno/components/ingestion/spatial_data_ingestion/overture_data_ingestion.py</code>) Line Col. Type Symbol ID Obj Message 1 0 convention <code>empty-docstring</code> C0112 <pre>Empty module docstring</pre> 5 0 warning <code>unused-import</code> W0611 <pre>Unused import math</pre> 30 0 refactor <code>consider-using-from-import</code> R0402 <pre>Use 'from multimno.core import utils' instead</pre> 33 0 convention <code>empty-docstring</code> C0112 OvertureDataIngestion <pre>Empty class docstring</pre> 33 0 refactor <code>too-many-instance-attributes</code> R0902 OvertureDataIngestion <pre>Too many instance attributes (18/7)</pre> 41 42 error <code>no-member</code> E1101 OvertureDataIngestion.__init__ <pre>Instance of 'ConfigParser' has no 'geteval' member</pre> 44 35 error <code>no-member</code> E1101 OvertureDataIngestion.__init__ <pre>Instance of 'ConfigParser' has no 'geteval' member</pre> 47 35 error <code>no-member</code> E1101 OvertureDataIngestion.__init__ <pre>Instance of 'ConfigParser' has no 'geteval' member</pre> 49 39 error <code>no-member</code> E1101 OvertureDataIngestion.__init__ <pre>Instance of 'ConfigParser' has no 'geteval' member</pre> 52 41 error <code>no-member</code> E1101 OvertureDataIngestion.__init__ <pre>Instance of 'ConfigParser' has no 'geteval' member</pre> 56 46 error <code>no-member</code> E1101 OvertureDataIngestion.__init__ <pre>Instance of 'ConfigParser' has no 'geteval' member</pre> 60 22 error <code>no-member</code> E1101 OvertureDataIngestion.__init__ <pre>Instance of 'ConfigParser' has no 'geteval' member</pre> 136 12 warning <code>attribute-defined-outside-init</code> W0201 OvertureDataIngestion.execute <pre>Attribute 'current_extent' defined outside __init__</pre> 137 12 warning <code>attribute-defined-outside-init</code> W0201 OvertureDataIngestion.execute <pre>Attribute 'current_quadkey' defined outside __init__</pre> 195 9 warning <code>fixme</code> W0511 <pre>TODO: asses feasibility of this</pre> 196 9 warning <code>fixme</code> W0511 <pre>TODO: make vertices number parameter</pre> 197 9 warning <code>fixme</code> W0511 <pre>TODO: introduce cut by qaudkeys</pre> 198 0 convention <code>line-too-long</code> C0301 <pre>Line too long (135/120)</pre> 202 25 warning <code>f-string-without-interpolation</code> W1309 OvertureDataIngestion.transform <pre>Using an f-string that does not have any interpolated variables</pre> 226 25 warning <code>f-string-without-interpolation</code> W1309 OvertureDataIngestion.transform <pre>Using an f-string that does not have any interpolated variables</pre> 245 29 warning <code>f-string-without-interpolation</code> W1309 OvertureDataIngestion.transform <pre>Using an f-string that does not have any interpolated variables</pre> 260 9 warning <code>fixme</code> W0511 <pre>TODO: Figure out if this optimization is ever needed and how to implement it properly</pre> 284 0 convention <code>line-too-long</code> C0301 <pre>Line too long (124/120)</pre> 315 9 warning <code>fixme</code> W0511 <pre>TODO: Figure out how to implement this</pre> 316 0 convention <code>line-too-long</code> C0301 <pre>Line too long (160/120)</pre> 378 9 warning <code>fixme</code> W0511 <pre>TODO: test more if this would make any difference</pre> 403 0 convention <code>line-too-long</code> C0301 <pre>Line too long (127/120)</pre> 407 0 convention <code>line-too-long</code> C0301 <pre>Line too long (135/120)</pre> 409 0 convention <code>line-too-long</code> C0301 <pre>Line too long (143/120)</pre> 410 0 convention <code>line-too-long</code> C0301 <pre>Line too long (146/120)</pre> 439 0 convention <code>line-too-long</code> C0301 <pre>Line too long (133/120)</pre> 455 0 convention <code>line-too-long</code> C0301 <pre>Line too long (133/120)</pre> 463 0 convention <code>line-too-long</code> C0301 <pre>Line too long (130/120)</pre> 488 0 convention <code>line-too-long</code> C0301 <pre>Line too long (126/120)</pre> 491 0 convention <code>line-too-long</code> C0301 <pre>Line too long (121/120)</pre> 511 0 convention <code>line-too-long</code> C0301 <pre>Line too long (127/120)</pre> 512 0 convention <code>line-too-long</code> C0301 <pre>Line too long (131/120)</pre> 513 0 convention <code>line-too-long</code> C0301 <pre>Line too long (124/120)</pre> 517 0 convention <code>line-too-long</code> C0301 <pre>Line too long (142/120)</pre> 520 0 convention <code>line-too-long</code> C0301 <pre>Line too long (142/120)</pre> 523 0 convention <code>line-too-long</code> C0301 <pre>Line too long (150/120)</pre> Module <code>components.ingestion.synthetic.synthetic_diaries</code> (<code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code>) Line Col. Type Symbol ID Obj Message 19 0 refactor <code>too-many-instance-attributes</code> R0902 SyntheticDiaries <pre>Too many instance attributes (23/7)</pre> 19 0 refactor <code>too-many-public-methods</code> R0904 SyntheticDiaries <pre>Too many public methods (22/20)</pre> 67 16 warning <code>fixme</code> W0511 <pre>TODO: cambiar por stay_sequence</pre> 187 4 refactor <code>too-many-arguments</code> R0913 SyntheticDiaries.generate_stay_location <pre>Too many arguments (8/5)</pre> 222 4 refactor <code>too-many-arguments</code> R0913 SyntheticDiaries.create_agent_activities_min_duration <pre>Too many arguments (8/5)</pre> 222 4 refactor <code>too-many-locals</code> R0914 SyntheticDiaries.create_agent_activities_min_duration <pre>Too many local variables (19/15)</pre> 311 8 refactor <code>no-else-return</code> R1705 SyntheticDiaries.create_agent_activities_min_duration <pre>Unnecessary \"else\" after \"return\", remove the \"else\" and de-indent the code inside it</pre> 343 4 refactor <code>too-many-arguments</code> R0913 SyntheticDiaries.adjust_activity_times <pre>Too many arguments (7/5)</pre> 343 4 refactor <code>too-many-locals</code> R0914 SyntheticDiaries.adjust_activity_times <pre>Too many local variables (17/15)</pre> 395 4 refactor <code>too-many-arguments</code> R0913 SyntheticDiaries.add_agent_date_activities <pre>Too many arguments (9/5)</pre> 579 4 refactor <code>too-many-arguments</code> R0913 SyntheticDiaries.generate_other_location <pre>Too many arguments (7/5)</pre> 624 4 refactor <code>too-many-arguments</code> R0913 SyntheticDiaries.generate_stay_duration <pre>Too many arguments (6/5)</pre> 666 8 refactor <code>no-else-return</code> R1705 SyntheticDiaries.generate_min_stay_duration <pre>Unnecessary \"elif\" after \"return\", remove the leading \"el\" from \"elif\"</pre> Module <code>components.ingestion.synthetic.synthetic_events</code> (<code>multimno/components/ingestion/synthetic/synthetic_events.py</code>) Line Col. Type Symbol ID Obj Message 32 0 refactor <code>too-many-instance-attributes</code> R0902 SyntheticEvents <pre>Too many instance attributes (25/7)</pre> 217 9 warning <code>fixme</code> W0511 <pre>TODO: add rows with PLMN</pre> 258 0 convention <code>line-too-long</code> C0301 <pre>Line too long (123/120)</pre> 527 4 refactor <code>too-many-arguments</code> R0913 SyntheticEvents.generate_location_errors <pre>Too many arguments (6/5)</pre> 608 9 warning <code>fixme</code> W0511 <pre>TODO check how to make this more optimal</pre> 670 13 warning <code>fixme</code> W0511 <pre>TODO logging</pre> 711 13 warning <code>fixme</code> W0511 <pre>TODO logging</pre> 715 0 convention <code>line-too-long</code> C0301 <pre>Line too long (144/120)</pre> 717 9 warning <code>fixme</code> W0511 <pre>TODO</pre> 752 0 convention <code>line-too-long</code> C0301 <pre>Line too long (121/120)</pre> 763 13 warning <code>fixme</code> W0511 <pre>TODO logging</pre> 780 0 convention <code>line-too-long</code> C0301 <pre>Line too long (125/120)</pre> 807 21 warning <code>fixme</code> W0511 <pre>TODO: Temporary remove of timezone addition as cleaning</pre> 817 61 error <code>possibly-used-before-assignment</code> E0606 SyntheticEvents.generate_erroneous_type_values <pre>Possibly using variable 'to_value' before assignment</pre> 823 0 convention <code>line-too-long</code> C0301 <pre>Line too long (122/120)</pre> 853 0 convention <code>line-too-long</code> C0301 <pre>Line too long (125/120)</pre> 871 0 convention <code>line-too-long</code> C0301 <pre>Line too long (127/120)</pre> 900 0 convention <code>line-too-long</code> C0301 <pre>Line too long (125/120)</pre> 912 0 convention <code>line-too-long</code> C0301 <pre>Line too long (129/120)</pre> 925 0 convention <code>line-too-long</code> C0301 <pre>Line too long (129/120)</pre> 933 0 convention <code>line-too-long</code> C0301 <pre>Line too long (126/120)</pre> Module <code>components.ingestion.synthetic.synthetic_network</code> (<code>multimno/components/ingestion/synthetic/synthetic_network.py</code>) Line Col. Type Symbol ID Obj Message 18 0 refactor <code>too-few-public-methods</code> R0903 CellIDGenerator <pre>Too few public methods (1/2)</pre> 47 0 refactor <code>too-few-public-methods</code> R0903 RandomCellIDGenerator <pre>Too few public methods (1/2)</pre> 65 0 refactor <code>too-few-public-methods</code> R0903 CellIDGeneratorBuilder <pre>Too few public methods (1/2)</pre> 97 0 refactor <code>too-many-instance-attributes</code> R0902 SyntheticNetwork <pre>Too many instance attributes (31/7)</pre> 193 4 refactor <code>too-many-locals</code> R0914 SyntheticNetwork.clean_cells_generator <pre>Too many local variables (22/15)</pre> 246 12 refactor <code>no-else-return</code> R1705 SyntheticNetwork.clean_cells_generator.random_azimuth_angle <pre>Unnecessary \"else\" after \"return\", remove the \"else\" and de-indent the code inside it</pre> 298 12 refactor <code>no-else-return</code> R1705 SyntheticNetwork.clean_cells_generator.check_operational <pre>Unnecessary \"else\" after \"return\", remove the \"else\" and de-indent the code inside it</pre> Module <code>components.quality.event_quality_warnings.event_quality_warnings</code> (<code>multimno/components/quality/event_quality_warnings/event_quality_warnings.py</code>) Line Col. Type Symbol ID Obj Message 1 0 convention <code>missing-module-docstring</code> C0114 <pre>Missing module docstring</pre> 34 0 refactor <code>too-many-instance-attributes</code> R0902 EventQualityWarnings <pre>Too many instance attributes (33/7)</pre> 88 35 error <code>no-member</code> E1101 EventQualityWarnings.__init__ <pre>Instance of 'ConfigParser' has no 'geteval' member</pre> 116 36 error <code>no-member</code> E1101 EventQualityWarnings.__init__ <pre>Instance of 'ConfigParser' has no 'geteval' member</pre> 120 36 error <code>no-member</code> E1101 EventQualityWarnings.__init__ <pre>Instance of 'ConfigParser' has no 'geteval' member</pre> 124 40 error <code>no-member</code> E1101 EventQualityWarnings.__init__ <pre>Instance of 'ConfigParser' has no 'geteval' member</pre> 128 51 error <code>no-member</code> E1101 EventQualityWarnings.__init__ <pre>Instance of 'ConfigParser' has no 'geteval' member</pre> 134 53 error <code>no-member</code> E1101 EventQualityWarnings.__init__ <pre>Instance of 'ConfigParser' has no 'geteval' member</pre> 140 38 error <code>no-member</code> E1101 EventQualityWarnings.__init__ <pre>Instance of 'ConfigParser' has no 'geteval' member</pre> 144 36 error <code>no-member</code> E1101 EventQualityWarnings.__init__ <pre>Instance of 'ConfigParser' has no 'geteval' member</pre> 148 46 error <code>no-member</code> E1101 EventQualityWarnings.__init__ <pre>Instance of 'ConfigParser' has no 'geteval' member</pre> 154 54 error <code>no-member</code> E1101 EventQualityWarnings.__init__ <pre>Instance of 'ConfigParser' has no 'geteval' member</pre> 235 4 refactor <code>too-many-locals</code> R0914 EventQualityWarnings.transform <pre>Too many local variables (19/15)</pre> 235 4 refactor <code>too-many-branches</code> R0912 EventQualityWarnings.transform <pre>Too many branches (19/12)</pre> 235 4 refactor <code>too-many-statements</code> R0915 EventQualityWarnings.transform <pre>Too many statements (54/50)</pre> 240 9 warning <code>fixme</code> W0511 <pre>TODO: deal with cases when df_qa_by_column, df_qa_freq_distribution do not have data for</pre> 242 9 warning <code>fixme</code> W0511 <pre>TODO: dynamically define/check the possible research period of QW  based on data period</pre> 244 9 warning <code>fixme</code> W0511 <pre>TODO: implement min_period conf param which is minimal amount of days with previous data to</pre> 258 9 warning <code>fixme</code> W0511 <pre>TODO: maybe makes sense to first sum init and final freq, cache and parse this aggregation</pre> 305 9 warning <code>fixme</code> W0511 <pre>TODO: should we consider error rate of null user_id or/and null cell_id in QW computation</pre> 374 33 error <code>possibly-used-before-assignment</code> E0606 EventQualityWarnings.transform <pre>Possibly using variable 'error_type_thresholds' before assignment</pre> 391 4 refactor <code>too-many-arguments</code> R0913 EventQualityWarnings.data_size_qw <pre>Too many arguments (10/5)</pre> 391 4 refactor <code>too-many-locals</code> R0914 EventQualityWarnings.data_size_qw <pre>Too many local variables (18/15)</pre> 554 4 refactor <code>too-many-arguments</code> R0913 EventQualityWarnings.error_rate_qw <pre>Too many arguments (12/5)</pre> 554 4 refactor <code>too-many-locals</code> R0914 EventQualityWarnings.error_rate_qw <pre>Too many local variables (19/15)</pre> 563 0 convention <code>line-too-long</code> C0301 <pre>Line too long (130/120)</pre> 564 0 convention <code>line-too-long</code> C0301 <pre>Line too long (145/120)</pre> 565 0 convention <code>line-too-long</code> C0301 <pre>Line too long (127/120)</pre> 647 4 refactor <code>too-many-arguments</code> R0913 EventQualityWarnings.error_type_rate_qw <pre>Too many arguments (13/5)</pre> 647 4 refactor <code>too-many-locals</code> R0914 EventQualityWarnings.error_type_rate_qw <pre>Too many local variables (23/15)</pre> 658 0 convention <code>line-too-long</code> C0301 <pre>Line too long (145/120)</pre> 659 0 convention <code>line-too-long</code> C0301 <pre>Line too long (160/120)</pre> 660 0 convention <code>line-too-long</code> C0301 <pre>Line too long (142/120)</pre> 762 4 refactor <code>too-many-arguments</code> R0913 EventQualityWarnings.rate_common_qw <pre>Too many arguments (11/5)</pre> 924 4 convention <code>missing-function-docstring</code> C0116 EventQualityWarnings.save_quality_warnings_log_table <pre>Missing function or method docstring</pre> 931 4 convention <code>missing-function-docstring</code> C0116 EventQualityWarnings.save_quality_warnings_for_plots <pre>Missing function or method docstring</pre> Module <code>components.quality.network_quality_warnings.network_quality_warnings</code> (<code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code>) Line Col. Type Symbol ID Obj Message 2 0 convention <code>line-too-long</code> C0301 <pre>Line too long (124/120)</pre> 7 0 convention <code>wrong-import-order</code> C0411 <pre>standard import \"datetime\" should be placed before third party imports \"pyspark.sql.functions\", \"pyspark.sql.Row\"</pre> 8 0 convention <code>wrong-import-order</code> C0411 <pre>standard import \"math\" should be placed before third party imports \"pyspark.sql.functions\", \"pyspark.sql.Row\"</pre> 29 0 refactor <code>too-many-instance-attributes</code> R0902 NetworkQualityWarnings <pre>Too many instance attributes (12/7)</pre> 58 0 convention <code>line-too-long</code> C0301 <pre>Line too long (126/120)</pre> 60 0 convention <code>line-too-long</code> C0301 <pre>Line too long (143/120)</pre> 61 0 convention <code>line-too-long</code> C0301 <pre>Line too long (160/120)</pre> 66 0 convention <code>line-too-long</code> C0301 <pre>Line too long (125/120)</pre> 68 0 convention <code>line-too-long</code> C0301 <pre>Line too long (142/120)</pre> 69 0 convention <code>line-too-long</code> C0301 <pre>Line too long (159/120)</pre> 74 0 convention <code>line-too-long</code> C0301 <pre>Line too long (126/120)</pre> 76 0 convention <code>line-too-long</code> C0301 <pre>Line too long (143/120)</pre> 77 0 convention <code>line-too-long</code> C0301 <pre>Line too long (160/120)</pre> 85 0 convention <code>line-too-long</code> C0301 <pre>Line too long (165/120)</pre> 93 0 convention <code>line-too-long</code> C0301 <pre>Line too long (164/120)</pre> 101 0 convention <code>line-too-long</code> C0301 <pre>Line too long (165/120)</pre> 150 26 refactor <code>use-dict-literal</code> R1735 NetworkQualityWarnings.__init__ <pre>Consider using '{}' instead of a call to 'dict'.</pre> 152 4 refactor <code>too-many-branches</code> R0912 NetworkQualityWarnings.get_thresholds <pre>Too many branches (14/12)</pre> 167 28 error <code>no-member</code> E1101 NetworkQualityWarnings.get_thresholds <pre>Instance of 'ConfigParser' has no 'geteval' member</pre> 213 0 convention <code>line-too-long</code> C0301 <pre>Line too long (146/120)</pre> 218 0 convention <code>line-too-long</code> C0301 <pre>Line too long (131/120)</pre> 276 21 convention <code>invalid-name</code> C0103 NetworkQualityWarnings.transform <pre>Variable name \"raw_UCL\" doesn't conform to snake_case naming style</pre> 276 30 convention <code>invalid-name</code> C0103 NetworkQualityWarnings.transform <pre>Variable name \"raw_LCL\" doesn't conform to snake_case naming style</pre> 278 23 convention <code>invalid-name</code> C0103 NetworkQualityWarnings.transform <pre>Variable name \"clean_UCL\" doesn't conform to snake_case naming style</pre> 278 34 convention <code>invalid-name</code> C0103 NetworkQualityWarnings.transform <pre>Variable name \"clean_LCL\" doesn't conform to snake_case naming style</pre> 280 36 convention <code>invalid-name</code> C0103 NetworkQualityWarnings.transform <pre>Variable name \"error_rate_UCL\" doesn't conform to snake_case naming style</pre> 374 0 convention <code>line-too-long</code> C0301 <pre>Line too long (146/120)</pre> 375 0 convention <code>line-too-long</code> C0301 <pre>Line too long (143/120)</pre> 406 21 refactor <code>use-dict-literal</code> R1735 NetworkQualityWarnings.get_lookback_period_statistics <pre>Consider using '{}' instead of a call to 'dict'.</pre> 415 41 refactor <code>use-dict-literal</code> R1735 NetworkQualityWarnings.get_lookback_period_statistics <pre>Consider using '{}' instead of a call to 'dict'.</pre> 457 4 refactor <code>too-many-arguments</code> R0913 NetworkQualityWarnings.register_warning <pre>Too many arguments (6/5)</pre> 533 0 convention <code>line-too-long</code> C0301 <pre>Line too long (154/120)</pre> 542 0 convention <code>line-too-long</code> C0301 <pre>Line too long (154/120)</pre> 552 0 convention <code>line-too-long</code> C0301 <pre>Line too long (135/120)</pre> 555 0 convention <code>line-too-long</code> C0301 <pre>Line too long (126/120)</pre> 562 0 convention <code>line-too-long</code> C0301 <pre>Line too long (135/120)</pre> 565 0 convention <code>line-too-long</code> C0301 <pre>Line too long (125/120)</pre> 639 0 convention <code>line-too-long</code> C0301 <pre>Line too long (137/120)</pre> 648 0 convention <code>line-too-long</code> C0301 <pre>Line too long (136/120)</pre> 658 0 convention <code>line-too-long</code> C0301 <pre>Line too long (135/120)</pre> 661 0 convention <code>line-too-long</code> C0301 <pre>Line too long (126/120)</pre> 668 0 convention <code>line-too-long</code> C0301 <pre>Line too long (135/120)</pre> 671 0 convention <code>line-too-long</code> C0301 <pre>Line too long (125/120)</pre> 753 0 convention <code>line-too-long</code> C0301 <pre>Line too long (132/120)</pre> 761 0 convention <code>line-too-long</code> C0301 <pre>Line too long (139/120)</pre> 764 0 convention <code>line-too-long</code> C0301 <pre>Line too long (158/120)</pre> 796 0 convention <code>line-too-long</code> C0301 <pre>Line too long (139/120)</pre> 798 0 convention <code>line-too-long</code> C0301 <pre>Line too long (139/120)</pre> 820 0 convention <code>line-too-long</code> C0301 <pre>Line too long (132/120)</pre> 856 4 refactor <code>too-many-arguments</code> R0913 NetworkQualityWarnings.create_plots_data <pre>Too many arguments (13/5)</pre> 856 4 refactor <code>too-many-locals</code> R0914 NetworkQualityWarnings.create_plots_data <pre>Too many local variables (20/15)</pre> 865 8 convention <code>invalid-name</code> C0103 NetworkQualityWarnings.create_plots_data <pre>Argument name \"raw_UCL\" doesn't conform to snake_case naming style</pre> 866 8 convention <code>invalid-name</code> C0103 NetworkQualityWarnings.create_plots_data <pre>Argument name \"clean_UCL\" doesn't conform to snake_case naming style</pre> 867 8 convention <code>invalid-name</code> C0103 NetworkQualityWarnings.create_plots_data <pre>Argument name \"error_rate_UCL\" doesn't conform to snake_case naming style</pre> 868 8 convention <code>invalid-name</code> C0103 NetworkQualityWarnings.create_plots_data <pre>Argument name \"raw_LCL\" doesn't conform to snake_case naming style</pre> 869 8 convention <code>invalid-name</code> C0103 NetworkQualityWarnings.create_plots_data <pre>Argument name \"clean_LCL\" doesn't conform to snake_case naming style</pre> 875 0 convention <code>line-too-long</code> C0301 <pre>Line too long (124/120)</pre> 876 0 convention <code>line-too-long</code> C0301 <pre>Line too long (121/120)</pre> 877 0 convention <code>line-too-long</code> C0301 <pre>Line too long (127/120)</pre> Module <code>components.quality.semantic_quality_warnings.semantic_quality_warnings</code> (<code>multimno/components/quality/semantic_quality_warnings/semantic_quality_warnings.py</code>) Line Col. Type Symbol ID Obj Message 1 0 convention <code>empty-docstring</code> C0112 <pre>Empty module docstring</pre> 7 0 convention <code>wrong-import-order</code> C0411 <pre>standard import \"datetime\" should be placed before third party imports \"pyspark.sql.functions\", \"pyspark.sql.Row\", \"pandas\"</pre> 8 0 convention <code>wrong-import-order</code> C0411 <pre>standard import \"math\" should be placed before third party imports \"pyspark.sql.functions\", \"pyspark.sql.Row\", \"pandas\"</pre> 25 0 convention <code>empty-docstring</code> C0112 SemanticQualityWarnings <pre>Empty class docstring</pre> 67 28 error <code>no-member</code> E1101 SemanticQualityWarnings.get_thresholds <pre>Instance of 'ConfigParser' has no 'geteval' member</pre> 146 0 convention <code>consider-using-dict-items</code> C0206 SemanticQualityWarnings.transform <pre>Consider iterating with .items()</pre> 146 84 convention <code>consider-iterating-dictionary</code> C0201 SemanticQualityWarnings.transform <pre>Consider iterating the dictionary directly instead of calling .keys()</pre> 159 22 refactor <code>use-dict-literal</code> R1735 SemanticQualityWarnings.transform <pre>Consider using '{}' instead of a call to 'dict'.</pre> 163 36 refactor <code>use-dict-literal</code> R1735 SemanticQualityWarnings.transform <pre>Consider using '{}' instead of a call to 'dict'.</pre> 168 37 convention <code>consider-iterating-dictionary</code> C0201 SemanticQualityWarnings.transform <pre>Consider iterating the dictionary directly instead of calling .keys()</pre> 173 19 convention <code>consider-iterating-dictionary</code> C0201 SemanticQualityWarnings.transform <pre>Consider iterating the dictionary directly instead of calling .keys()</pre> 181 26 convention <code>consider-iterating-dictionary</code> C0201 SemanticQualityWarnings.transform <pre>Consider iterating the dictionary directly instead of calling .keys()</pre> 232 12 refactor <code>simplifiable-if-statement</code> R1703 SemanticQualityWarnings.quality_warnings_by_error <pre>The if statement can be replaced with 'var = bool(test)'</pre> 245 4 refactor <code>too-many-arguments</code> R0913 SemanticQualityWarnings.register_warning <pre>Too many arguments (6/5)</pre> Module <code>core.constants.columns</code> (<code>multimno/core/constants/columns.py</code>) Line Col. Type Symbol ID Obj Message 6 0 refactor <code>too-few-public-methods</code> R0903 ColNames <pre>Too few public methods (0/2)</pre> Module <code>core.constants.conditions</code> (<code>multimno/core/constants/conditions.py</code>) Line Col. Type Symbol ID Obj Message 1 0 convention <code>missing-module-docstring</code> C0114 <pre>Missing module docstring</pre> 1 0 convention <code>missing-class-docstring</code> C0115 Conditions <pre>Missing class docstring</pre> 1 0 refactor <code>too-few-public-methods</code> R0903 Conditions <pre>Too few public methods (0/2)</pre> 2 0 convention <code>line-too-long</code> C0301 <pre>Line too long (206/120)</pre> 6 0 convention <code>line-too-long</code> C0301 <pre>Line too long (252/120)</pre> 10 0 convention <code>line-too-long</code> C0301 <pre>Line too long (262/120)</pre> Module <code>core.constants.error_types</code> (<code>multimno/core/constants/error_types.py</code>) Line Col. Type Symbol ID Obj Message 6 0 refactor <code>too-few-public-methods</code> R0903 ErrorTypes <pre>Too few public methods (0/2)</pre> 26 0 refactor <code>too-few-public-methods</code> R0903 NetworkErrorType <pre>Too few public methods (0/2)</pre> 40 0 refactor <code>too-few-public-methods</code> R0903 SemanticErrorType <pre>Too few public methods (0/2)</pre> Module <code>core.constants.measure_definitions</code> (<code>multimno/core/constants/measure_definitions.py</code>) Line Col. Type Symbol ID Obj Message 1 0 convention <code>missing-module-docstring</code> C0114 <pre>Missing module docstring</pre> 1 0 convention <code>missing-class-docstring</code> C0115 MeasureDefinitions <pre>Missing class docstring</pre> 1 0 refactor <code>too-few-public-methods</code> R0903 MeasureDefinitions <pre>Too few public methods (0/2)</pre> Module <code>core.constants.transformations</code> (<code>multimno/core/constants/transformations.py</code>) Line Col. Type Symbol ID Obj Message 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.bronze.bronze_admin_units_data_object:[27:66]\n==core.data_objects.bronze.bronze_geographic_zones_data_object:[27:66]\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.zone_id, StringType(), nullable=False),\n            StructField(ColNames.name, StringType(), nullable=True),\n            StructField(ColNames.level, ShortType(), nullable=True),\n            StructField(ColNames.parent_id, StringType(), nullable=True),\n            StructField(ColNames.iso2, StringType(), nullable=False),\n            StructField(ColNames.geometry, GeometryType(), nullable=False),\n            # partition columns\n            StructField(ColNames.dataset_id, StringType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    def __init__(\n        self, spark: SparkSession, default_path: str, partition_columns: \"list[str]\" = None, default_crs: int = 3035\n    ) -&gt; None:\n\n        super().__init__(spark, default_path)\n        self.interface: GeoParquetInterface = GeoParquetInterface()\n        self.partition_columns = partition_columns\n\n        self.default_crs = default_crs\n\n    def read(self):\n\n        self.df = self.interface.read_from_interface(self.spark, self.default_path, self.SCHEMA)\n        self.df = self.df.withColumn(ColNames.geometry, STF.ST_SetSRID((ColNames.geometry), F.lit(self.default_crs)))\n\n    def write(self, path: str = None, partition_columns: \"list[str]\" = None):\n\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.bronze.bronze_landuse_data_object:[28:63]\n==core.data_objects.bronze.bronze_transportation_data_object:[27:62]\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.category, StringType(), nullable=False),\n            StructField(ColNames.geometry, GeometryType(), nullable=False),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.quadkey, StringType(), nullable=False),\n        ]\n    )\n\n    def __init__(\n        self, spark: SparkSession, default_path: str, partition_columns: list[str] = None, default_crs: int = 3035\n    ) -&gt; None:\n\n        super().__init__(spark, default_path)\n        self.interface: GeoParquetInterface = GeoParquetInterface()\n        self.partition_columns = partition_columns\n\n        self.default_crs = default_crs\n\n    def read(self):\n\n        self.df = self.interface.read_from_interface(self.spark, self.default_path, self.SCHEMA)\n        self.df = self.df.withColumn(ColNames.geometry, STF.ST_SetSRID((ColNames.geometry), F.lit(self.default_crs)))\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_network_syntactic_quality_warnings_log_table:[41:64]\n==core.data_objects.silver.silver_semantic_quality_warnings_log_table:[48:76]\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        # Always append\n        self.df.write.format(\n            self.interface.FILE_FORMAT,\n        ).partitionBy(partition_columns).mode(\n            \"append\"\n        ).save(path)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==components.execution.longterm_permanence_score.longterm_permanence_score:[33:55]\n==components.execution.usual_environment_aggregation.usual_environment_aggregation:[42:63]\n        super().__init__(general_config_path, component_config_path)\n\n        # Months that compose the long-term period, at least one\n        start_month = self.config.get(self.COMPONENT_ID, \"start_month\")\n        try:\n            self.start_date = dt.datetime.strptime(start_month, \"%Y-%m\").date()\n        except ValueError as e:\n            self.logger.error(f\"Could not parse start_month = `{start_month}`. Expected format: YYYY-MM\")\n            raise e\n\n        end_month = self.config.get(self.COMPONENT_ID, \"end_month\")\n        try:\n            self.end_date = dt.datetime.strptime(end_month, \"%Y-%m\").date()\n        except ValueError as e:\n            self.logger.error(f\"Could not parse end_month = `{end_month}`. Expected format: YYYY-MM\")\n            raise e\n        self.end_date = self.end_date.replace(day=cal.monthrange(self.end_date.year, self.end_date.month)[1])\n\n        if self.end_date &lt; self.start_date:\n            raise ValueError(f\"End month `{end_month}` should not be an earlier than start month `{start_month}`\")\n</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_network_syntactic_quality_warnings_plot_data:[37:59]\n==core.data_objects.silver.silver_semantic_quality_warnings_plot_data:[34:54]\n            StructField(ColNames.variable, StringType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.timestamp, TimestampType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = [ColNames.variable, ColNames.year, ColNames.month, ColNames.day, ColNames.timestamp]\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n\n</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_semantic_quality_metrics:[31:52]\n==core.data_objects.silver.silver_semantic_quality_warnings_log_table:[48:74]\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        default_path: str,\n        partition_columns: list[str] = None,\n    ) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        # Always append\n        self.df.write.format(\n            self.interface.FILE_FORMAT,\n        ).partitionBy(partition_columns).mode(</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_present_population_data_object:[29:50]\n==core.data_objects.silver.silver_present_population_zone_data_object:[29:50]\n            StructField(ColNames.population, FloatType(), nullable=False),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.timestamp, TimestampType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_network_data_syntactic_quality_metrics_by_column:[35:54]\n==core.data_objects.silver.silver_network_row_error_metrics:[31:50]\n            StructField(ColNames.value, IntegerType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_event_data_syntactic_quality_warnings_for_plots:[34:53]\n==core.data_objects.silver.silver_event_data_syntactic_quality_warnings_log_table:[33:52]\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface: ParquetInterface = ParquetInterface()\n        self.partition_columns = [ColNames.date]\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.df.write.format(\n            self.interface.FILE_FORMAT,\n        ).partitionBy(partition_columns).mode(\n            \"append\"\n        ).save(path)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==components.execution.usual_environment_aggregation.usual_environment_aggregation:[45:61]\n==components.execution.usual_environment_labeling.usual_environment_labeling:[80:96]\n        start_month = self.config.get(self.COMPONENT_ID, \"start_month\")\n        try:\n            self.start_date = dt.datetime.strptime(start_month, \"%Y-%m\").date()\n        except ValueError as e:\n            self.logger.error(f\"Could not parse start_month = `{start_month}`. Expected format: YYYY-MM\")\n            raise e\n\n        end_month = self.config.get(self.COMPONENT_ID, \"end_month\")\n        try:\n            self.end_date = dt.datetime.strptime(end_month, \"%Y-%m\").date()\n        except ValueError as e:\n            self.logger.error(f\"Could not parse end_month = `{end_month}`. Expected format: YYYY-MM\")\n            raise e\n        self.end_date = self.end_date.replace(day=cal.monthrange(self.end_date.year, self.end_date.month)[1])\n\n        if self.end_date &lt; self.start_date:</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==components.execution.midterm_permanence_score.midterm_permanence_score:[111:129]\n==components.execution.usual_environment_aggregation.usual_environment_aggregation:[42:60]\n        super().__init__(general_config_path, component_config_path)\n\n        # Months that compose the long-term period, at least one\n        start_month = self.config.get(self.COMPONENT_ID, \"start_month\")\n        try:\n            self.start_date = dt.datetime.strptime(start_month, \"%Y-%m\").date()\n        except ValueError as e:\n            self.logger.error(f\"Could not parse start_month = `{start_month}`. Expected format: YYYY-MM\")\n            raise e\n\n        end_month = self.config.get(self.COMPONENT_ID, \"end_month\")\n        try:\n            self.end_date = dt.datetime.strptime(end_month, \"%Y-%m\").date()\n        except ValueError as e:\n            self.logger.error(f\"Could not parse end_month = `{end_month}`. Expected format: YYYY-MM\")\n            raise e\n        self.end_date = self.end_date.replace(day=cal.monthrange(self.end_date.year, self.end_date.month)[1])\n</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_network_data_object:[48:66]\n==core.data_objects.silver.silver_network_row_error_metrics:[32:50]\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.bronze.bronze_synthetic_diaries_data_object:[40:58]\n==core.data_objects.silver.silver_network_data_top_frequent_errors_data_object:[36:54]\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.bronze.bronze_admin_units_data_object:[40:66]\n==core.data_objects.bronze.bronze_landuse_data_object:[37:63]\n        ]\n    )\n\n    def __init__(\n        self, spark: SparkSession, default_path: str, partition_columns: \"list[str]\" = None, default_crs: int = 3035\n    ) -&gt; None:\n\n        super().__init__(spark, default_path)\n        self.interface: GeoParquetInterface = GeoParquetInterface()\n        self.partition_columns = partition_columns\n\n        self.default_crs = default_crs\n\n    def read(self):\n\n        self.df = self.interface.read_from_interface(self.spark, self.default_path, self.SCHEMA)\n        self.df = self.df.withColumn(ColNames.geometry, STF.ST_SetSRID((ColNames.geometry), F.lit(self.default_crs)))\n\n    def write(self, path: str = None, partition_columns: \"list[str]\" = None):\n\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.bronze.bronze_geographic_zones_data_object:[40:66]\n==core.data_objects.bronze.bronze_transportation_data_object:[36:62]\n        ]\n    )\n\n    def __init__(\n        self, spark: SparkSession, default_path: str, partition_columns: \"list[str]\" = None, default_crs: int = 3035\n    ) -&gt; None:\n\n        super().__init__(spark, default_path)\n        self.interface: GeoParquetInterface = GeoParquetInterface()\n        self.partition_columns = partition_columns\n\n        self.default_crs = default_crs\n\n    def read(self):\n\n        self.df = self.interface.read_from_interface(self.spark, self.default_path, self.SCHEMA)\n        self.df = self.df.withColumn(ColNames.geometry, STF.ST_SetSRID((ColNames.geometry), F.lit(self.default_crs)))\n\n    def write(self, path: str = None, partition_columns: \"list[str]\" = None):\n\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==components.execution.midterm_permanence_score.midterm_permanence_score:[114:129]\n==components.execution.usual_environment_labeling.usual_environment_labeling:[80:95]\n        start_month = self.config.get(self.COMPONENT_ID, \"start_month\")\n        try:\n            self.start_date = dt.datetime.strptime(start_month, \"%Y-%m\").date()\n        except ValueError as e:\n            self.logger.error(f\"Could not parse start_month = `{start_month}`. Expected format: YYYY-MM\")\n            raise e\n\n        end_month = self.config.get(self.COMPONENT_ID, \"end_month\")\n        try:\n            self.end_date = dt.datetime.strptime(end_month, \"%Y-%m\").date()\n        except ValueError as e:\n            self.logger.error(f\"Could not parse end_month = `{end_month}`. Expected format: YYYY-MM\")\n            raise e\n        self.end_date = self.end_date.replace(day=cal.monthrange(self.end_date.year, self.end_date.month)[1])\n</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==components.quality.network_quality_warnings.network_quality_warnings:[180:196]\n==components.quality.semantic_quality_warnings.semantic_quality_warnings:[89:105]\n                    try:\n                        val = float(val)\n                    except ValueError as e:\n                        error_msg = (\n                            f\"Expected numeric value for parameter {param_key} under {error_key} - got {val} instead.\"\n                        )\n                        self.logger.error(error_msg)\n                        raise e\n\n                if val &lt; 0:\n                    error_msg = f\"Parameter {param_key} under {error_key} must be non-negative - got {val}\"\n                    self.logger.error(error_msg)\n                    raise ValueError(error_msg)\n\n                thresholds[error_key][param_key] = val\n</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_network_data_object:[48:65]\n==core.data_objects.silver.silver_network_syntactic_quality_warnings_log_table:[41:59]\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_network_data_top_frequent_errors_data_object:[36:53]\n==core.data_objects.silver.silver_semantic_quality_metrics:[31:49]\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        # Always append</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_enriched_grid_data_object:[39:60]\n==core.data_objects.silver.silver_grid_data_object:[35:56]\n        super().__init__(spark, default_path)\n        self.interface: GeoParquetInterface = GeoParquetInterface()\n        self.partition_columns = partition_columns\n\n        # Clear path\n        self.first_write = True\n        self.default_crs = default_crs\n\n    def read(self):\n\n        self.df = self.interface.read_from_interface(self.spark, self.default_path, self.SCHEMA)\n        self.df = self.df.withColumn(ColNames.geometry, STF.ST_SetSRID((ColNames.geometry), F.lit(self.default_crs)))\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_cell_connection_probabilities_data_object:[38:55]\n==core.data_objects.silver.silver_cell_intersection_groups_data_object:[32:49]\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    ]\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.bronze.bronze_synthetic_diaries_data_object:[40:57]\n==core.data_objects.silver.silver_semantic_quality_warnings_log_table:[48:71]\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_event_data_object:[28:39]\n==core.data_objects.silver.silver_event_flagged_data_object:[30:41]\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.mcc, IntegerType(), nullable=True),\n            StructField(ColNames.mnc, StringType(), nullable=True),\n            StructField(ColNames.plmn, IntegerType(), nullable=True),\n            StructField(ColNames.cell_id, StringType(), nullable=True),\n            StructField(ColNames.latitude, FloatType(), nullable=True),\n            StructField(ColNames.longitude, FloatType(), nullable=True),\n            StructField(ColNames.loc_error, FloatType(), nullable=True),</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==components.execution.cell_footprint.cell_footprint_estimation:[54:71]\n==components.execution.network_cleaning.network_cleaning:[57:75]\n        )\n\n        self.data_period_start = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n\n        self.data_period_end = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n\n        self.data_period_dates = [\n            self.data_period_start + datetime.timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n    def initalize_data_objects(self):\n</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_longterm_permanence_score_data_object:[54:67]\n==core.data_objects.silver.silver_midterm_permanence_score_data_object:[51:64]\n            ColNames.day_type,\n            ColNames.time_interval,\n            ColNames.id_type,\n            ColNames.user_id_modulo,\n        ]\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_network_data_object:[51:66]\n==core.data_objects.silver.silver_present_population_data_object:[35:50]\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_network_data_top_frequent_errors_data_object:[39:54]\n==core.data_objects.silver.silver_present_population_zone_data_object:[35:50]\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_cell_connection_probabilities_data_object:[25:36]\n==core.data_objects.silver.silver_signal_strength_data_object:[25:36]\n            StructField(ColNames.year, ShortType(), nullable=True),\n            StructField(ColNames.month, ByteType(), nullable=True),\n            StructField(ColNames.day, ByteType(), nullable=True),\n        ]\n    )\n\n    MANDATORY_COLUMNS = [\n        ColNames.cell_id,\n        ColNames.grid_id,\n        ColNames.valid_date_start,\n        ColNames.valid_date_end,</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.bronze.bronze_event_data_object:[42:57]\n==core.data_objects.bronze.bronze_synthetic_diaries_data_object:[43:58]\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==components.execution.geozones_grid_mapping.geozones_grid_mapping:[63:76]\n==components.execution.grid_enrichment.grid_enrichment:[98:111]\n        for key, value in inputs.items():\n            if self.config.has_option(CONFIG_BRONZE_PATHS_KEY, key):\n                path = self.config.get(CONFIG_BRONZE_PATHS_KEY, key)\n            else:\n                path = self.config.get(CONFIG_SILVER_PATHS_KEY, key)\n            if check_if_data_path_exists(self.spark, path):\n                self.input_data_objects[value.ID] = value(self.spark, path)\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")\n                raise ValueError(f\"Invalid path for {value.ID}: {path}\")\n\n        # outputs\n</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==components.execution.cell_connection_probability.cell_connection_probability:[41:57]\n==components.execution.network_cleaning.network_cleaning:[59:75]\n        self.data_period_start = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n        self.data_period_end = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n\n        # List of datetime.dates for which to perform cleaning of the raw network input data.\n        # Notice that date_period_end is included\n        self.data_period_dates = [\n            self.data_period_start + datetime.timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n        # Timestamp format that the function pyspark.sql.functions.to_timestamp expects.\n        # Format must follow guidelines in https://spark.apache.org/docs/3.4.2/sql-ref-datetime-pattern.html</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_device_activity_statistics:[49:59]\n==core.data_objects.silver.silver_event_data_object:[61:71]\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n\n        self.df.write.format(\n            self.interface.FILE_FORMAT,  # File format\n        ).partitionBy(self.partition_columns).mode(\n            \"append\"\n        ).save(path)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_event_data_syntactic_quality_metrics_by_column:[46:56]\n==core.data_objects.silver.silver_event_data_syntactic_quality_warnings_for_plots:[43:53]\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.df.write.format(\n            self.interface.FILE_FORMAT,\n        ).partitionBy(partition_columns).mode(\n            \"append\"\n        ).save(path)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_longterm_permanence_score_data_object:[43:54]\n==core.data_objects.silver.silver_usual_environment_labels_data_object:[37:48]\n            StructField(ColNames.user_id_modulo, IntegerType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface: ParquetInterface = ParquetInterface()\n        self.partition_columns = [\n            ColNames.season,\n            ColNames.start_date,\n            ColNames.end_date,</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_longterm_permanence_score_data_object:[40:51]\n==core.data_objects.silver.silver_midterm_permanence_score_data_object:[38:49]\n            StructField(ColNames.day_type, StringType(), nullable=False),\n            StructField(ColNames.time_interval, StringType(), nullable=False),\n            StructField(ColNames.id_type, StringType(), nullable=False),\n            StructField(ColNames.user_id_modulo, IntegerType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface: ParquetInterface = ParquetInterface()\n        self.partition_columns = [</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_event_data_syntactic_quality_warnings_log_table:[42:52]\n==core.data_objects.silver.silver_network_syntactic_quality_warnings_log_table:[53:64]\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        # Always append\n        self.df.write.format(\n            self.interface.FILE_FORMAT,\n        ).partitionBy(partition_columns).mode(\n            \"append\"\n        ).save(path)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_event_data_syntactic_quality_metrics_frequency_distribution:[41:51]\n==core.data_objects.silver.silver_semantic_quality_warnings_log_table:[65:76]\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        # Always append\n        self.df.write.format(\n            self.interface.FILE_FORMAT,\n        ).partitionBy(partition_columns).mode(\n            \"append\"\n        ).save(path)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_network_syntactic_quality_warnings_log_table:[44:59]\n==core.data_objects.silver.silver_present_population_data_object:[35:49]\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        # Always append</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_present_population_zone_data_object:[35:49]\n==core.data_objects.silver.silver_semantic_quality_metrics:[34:49]\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_daily_permanence_score_data_object:[39:50]\n==core.data_objects.silver.silver_event_data_object:[43:54]\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface: ParquetInterface = ParquetInterface()\n        self.partition_columns = [\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            ColNames.user_id_modulo,</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.bronze.bronze_event_data_object:[42:56]\n==core.data_objects.silver.silver_semantic_quality_warnings_log_table:[51:71]\n        ]\n    )\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        default_path: str,\n        partition_columns: list[str] = None,\n    ) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        # Always append</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.bronze.bronze_network_physical_data_object:[63:77]\n==core.data_objects.silver.silver_cell_connection_probabilities_data_object:[41:55]\n    ]\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==components.quality.network_quality_warnings.network_quality_warnings:[110:125]\n==components.quality.semantic_quality_warnings.semantic_quality_warnings:[32:46]\n        super().__init__(general_config_path, component_config_path)\n\n        self.timestamp = datetime.datetime.now()\n\n        # Read date of study\n        self.date_format = self.config.get(self.COMPONENT_ID, \"date_format\")\n        date_of_study = self.config.get(self.COMPONENT_ID, \"date_of_study\")\n\n        try:\n            self.date_of_study = datetime.datetime.strptime(date_of_study, self.date_format).date()\n        except ValueError as e:\n            self.logger.error(str(e), exc_info=True)\n            raise e\n\n        # Read lookback period</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_cell_footprint_data_object:[42:53]\n==core.data_objects.silver.silver_network_data_top_frequent_errors_data_object:[43:54]\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_event_data_object:[42:52]\n==core.data_objects.silver.silver_midterm_permanence_score_data_object:[41:51]\n            StructField(ColNames.user_id_modulo, IntegerType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface: ParquetInterface = ParquetInterface()\n        self.partition_columns = [\n            ColNames.year,\n            ColNames.month,</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_longterm_permanence_score_data_object:[44:54]\n==core.data_objects.silver.silver_usual_environment_labeling_quality_metrics_data_object:[32:42]\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface: ParquetInterface = ParquetInterface()\n        self.partition_columns = [\n            ColNames.season,\n            ColNames.start_date,\n            ColNames.end_date,</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_event_data_object:[39:49]\n==core.data_objects.silver.silver_time_segments_data_object:[39:49]\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.user_id_modulo, IntegerType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface: ParquetInterface = ParquetInterface()</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_network_syntactic_quality_warnings_plot_data:[72:82]\n==core.data_objects.silver.silver_present_population_data_object:[31:41]\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.timestamp, TimestampType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_present_population_data_object:[39:50]\n==core.data_objects.silver.silver_signal_strength_data_object:[45:56]\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_network_syntactic_quality_warnings_plot_data:[38:48]\n==core.data_objects.silver.silver_present_population_zone_data_object:[31:41]\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.timestamp, TimestampType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_cell_intersection_groups_data_object:[38:49]\n==core.data_objects.silver.silver_present_population_zone_data_object:[39:50]\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.bronze.bronze_event_data_object:[46:57]\n==core.data_objects.silver.silver_cell_connection_probabilities_data_object:[44:55]\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.bronze.bronze_network_physical_data_object:[29:37]\n==core.data_objects.silver.silver_network_data_object:[36:44]\n            StructField(ColNames.azimuth_angle, FloatType(), nullable=True),\n            StructField(ColNames.elevation_angle, FloatType(), nullable=True),\n            StructField(ColNames.horizontal_beam_width, FloatType(), nullable=True),\n            StructField(ColNames.vertical_beam_width, FloatType(), nullable=True),\n            StructField(ColNames.power, FloatType(), nullable=True),\n            StructField(ColNames.range, FloatType(), nullable=True),\n            StructField(ColNames.frequency, IntegerType(), nullable=True),\n            StructField(ColNames.technology, StringType(), nullable=True),</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.bronze.bronze_network_physical_data_object:[66:77]\n==core.data_objects.bronze.bronze_synthetic_diaries_data_object:[47:58]\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.bronze.bronze_transportation_data_object:[47:62]\n==core.data_objects.silver.silver_enriched_grid_data_object:[45:60]\n        self.default_crs = default_crs\n\n    def read(self):\n\n        self.df = self.interface.read_from_interface(self.spark, self.default_path, self.SCHEMA)\n        self.df = self.df.withColumn(ColNames.geometry, STF.ST_SetSRID((ColNames.geometry), F.lit(self.default_crs)))\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.bronze.bronze_geographic_zones_data_object:[51:66]\n==core.data_objects.silver.silver_grid_data_object:[41:56]\n        self.default_crs = default_crs\n\n    def read(self):\n\n        self.df = self.interface.read_from_interface(self.spark, self.default_path, self.SCHEMA)\n        self.df = self.df.withColumn(ColNames.geometry, STF.ST_SetSRID((ColNames.geometry), F.lit(self.default_crs)))\n\n    def write(self, path: str = None, partition_columns: \"list[str]\" = None):\n\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_midterm_permanence_score_data_object:[54:64]\n==core.data_objects.silver.silver_usual_environment_labels_data_object:[48:58]\n            ColNames.user_id_modulo,\n        ]\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_event_flagged_data_object:[56:66]\n==core.data_objects.silver.silver_longterm_permanence_score_data_object:[57:67]\n            ColNames.user_id_modulo,\n        ]\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_event_flagged_data_object:[42:51]\n==core.data_objects.silver.silver_time_segments_data_object:[39:48]\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.user_id_modulo, IntegerType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_aggregated_usual_environments_data_object:[30:39]\n==core.data_objects.silver.silver_usual_environment_labeling_quality_metrics_data_object:[29:38]\n            StructField(ColNames.start_date, DateType(), nullable=False),\n            StructField(ColNames.end_date, DateType(), nullable=False),\n            StructField(ColNames.season, StringType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface: ParquetInterface = ParquetInterface()</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_event_data_syntactic_quality_metrics_frequency_distribution:[41:49]\n==core.data_objects.silver.silver_semantic_quality_metrics:[43:52]\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        # Always append\n        self.df.write.format(\n            self.interface.FILE_FORMAT,\n        ).partitionBy(partition_columns).mode(</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_cell_intersection_groups_data_object:[38:48]\n==core.data_objects.silver.silver_network_syntactic_quality_warnings_log_table:[48:59]\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_daily_permanence_score_data_object:[39:48]\n==core.data_objects.silver.silver_midterm_permanence_score_data_object:[42:51]\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface: ParquetInterface = ParquetInterface()\n        self.partition_columns = [\n            ColNames.year,\n            ColNames.month,</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_cell_connection_probabilities_data_object:[44:54]\n==core.data_objects.silver.silver_semantic_quality_metrics:[38:49]\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.bronze.bronze_event_data_object:[31:39]\n==core.data_objects.silver.silver_event_flagged_data_object:[34:41]\n            StructField(ColNames.mcc, IntegerType(), nullable=True),\n            StructField(ColNames.mnc, StringType(), nullable=True),\n            StructField(ColNames.plmn, IntegerType(), nullable=True),\n            StructField(ColNames.cell_id, StringType(), nullable=True),\n            StructField(ColNames.latitude, FloatType(), nullable=True),\n            StructField(ColNames.longitude, FloatType(), nullable=True),\n            StructField(ColNames.loc_error, FloatType(), nullable=True),</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.bronze.bronze_network_physical_data_object:[66:76]\n==core.data_objects.silver.silver_semantic_quality_warnings_log_table:[60:71]\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        # Always append</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.bronze.bronze_geographic_zones_data_object:[36:48]\n==core.data_objects.silver.silver_geozones_grid_map_data_object:[24:40]\n            StructField(ColNames.dataset_id, StringType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            # StructField(ColNames.quadkey, StringType(), nullable=False),\n        ]\n    )\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        default_path: str,\n        partition_columns: list[str] = None,\n    ) -&gt; None:\n\n        super().__init__(spark, default_path)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==components.execution.network_cleaning.network_cleaning:[162:169]\n==core.data_objects.bronze.bronze_network_physical_data_object:[53:60]\n            ColNames.elevation_angle,\n            ColNames.horizontal_beam_width,\n            ColNames.vertical_beam_width,\n            ColNames.power,\n            ColNames.range,\n            ColNames.frequency,\n            ColNames.technology,</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==components.execution.time_segments.continuous_time_segmentation:[98:107]\n==components.execution.usual_environment_aggregation.usual_environment_aggregation:[95:105]\n        for key, value in inputs.items():\n            path = self.config.get(CONFIG_SILVER_PATHS_KEY, key)\n            if check_if_data_path_exists(self.spark, path):\n                self.input_data_objects[value.ID] = value(self.spark, path)\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")\n                raise ValueError(f\"Invalid path for {value.ID}: {path}\")\n\n        # Output</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==components.execution.daily_permanence_score.daily_permanence_score:[33:42]\n==components.execution.time_segments.continuous_time_segmentation:[47:56]\n        super().__init__(general_config_path, component_config_path)\n\n        self.data_period_start = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n        self.data_period_end = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==components.execution.cell_connection_probability.cell_connection_probability:[39:49]\n==components.execution.signal_strength.signal_stength_modeling:[46:54]\n        super().__init__(general_config_path, component_config_path)\n\n        self.data_period_start = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n        self.data_period_end = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_event_data_object:[49:57]\n==core.data_objects.silver.silver_event_flagged_data_object:[52:60]\n        self.partition_columns = [\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            ColNames.user_id_modulo,\n        ]\n\n        # Clear path</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_longterm_permanence_score_data_object:[58:67]\n==core.data_objects.silver.silver_usual_environment_labeling_quality_metrics_data_object:[42:51]\n        ]\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.landing.landing_geoparquet_data_object:[27:37]\n==core.data_objects.silver.silver_geozones_grid_map_data_object:[48:58]\n        self.df = self.interface.read_from_interface(self.spark, self.default_path, self.SCHEMA)\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_daily_permanence_score_data_object:[51:63]\n==core.data_objects.silver.silver_event_data_object:[54:66]\n        ]\n\n        # Clear path\n        self.first_write = True\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        # If it is the first writing of this data object, clear the input directory, otherwise add\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_daily_permanence_score_data_object:[54:64]\n==core.data_objects.silver.silver_time_segments_data_object:[52:62]\n        self.first_write = True\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        # If it is the first writing of this data object, clear the input directory, otherwise add\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_cell_connection_probabilities_data_object:[16:22]\n==core.data_objects.silver.silver_signal_strength_data_object:[16:22]\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.cell_id, StringType(), nullable=True),\n            StructField(ColNames.grid_id, StringType(), nullable=True),\n            StructField(ColNames.valid_date_start, DateType(), nullable=True),\n            StructField(ColNames.valid_date_end, DateType(), nullable=True),</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.bronze.bronze_network_physical_data_object:[68:77]\n==core.data_objects.silver.silver_aggregated_usual_environments_data_object:[39:48]\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==components.execution.grid_enrichment.grid_enrichment:[74:81]\n==components.ingestion.grid_generation.inspire_grid_generation:[49:56]\n        self.grid_resolution = 100\n\n        self.grid_generator = InspireGridGenerator(\n            self.spark,\n            self.grid_resolution,\n            ColNames.geometry,\n            ColNames.grid_id,</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==components.execution.grid_enrichment.grid_enrichment:[102:111]\n==components.execution.time_segments.continuous_time_segmentation:[99:107]\n                path = self.config.get(CONFIG_SILVER_PATHS_KEY, key)\n            if check_if_data_path_exists(self.spark, path):\n                self.input_data_objects[value.ID] = value(self.spark, path)\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")\n                raise ValueError(f\"Invalid path for {value.ID}: {path}\")\n\n        # outputs\n</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==components.execution.geozones_grid_mapping.geozones_grid_mapping:[67:76]\n==components.execution.usual_environment_aggregation.usual_environment_aggregation:[96:105]\n                path = self.config.get(CONFIG_SILVER_PATHS_KEY, key)\n            if check_if_data_path_exists(self.spark, path):\n                self.input_data_objects[value.ID] = value(self.spark, path)\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")\n                raise ValueError(f\"Invalid path for {value.ID}: {path}\")\n\n        # outputs\n</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==components.execution.cell_connection_probability.cell_connection_probability:[67:73]\n==components.execution.usual_environment_aggregation.usual_environment_aggregation:[95:101]\n        for key, value in inputs.items():\n            path = self.config.get(CONFIG_SILVER_PATHS_KEY, key)\n            if check_if_data_path_exists(self.spark, path):\n                self.input_data_objects[value.ID] = value(self.spark, path)\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==components.execution.network_cleaning.network_cleaning:[59:68]\n==components.execution.signal_strength.signal_stength_modeling:[48:54]\n        self.data_period_start = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n        self.data_period_end = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_event_data_object:[57:66]\n==core.data_objects.silver.silver_time_segments_data_object:[52:61]\n        self.first_write = True\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        # If it is the first writing of this data object, clear the input directory, otherwise add\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_aggregated_usual_environments_data_object:[39:47]\n==core.data_objects.silver.silver_semantic_quality_warnings_log_table:[62:71]\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        # Always append</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_signal_strength_data_object:[50:56]\n==core.data_objects.silver.silver_usual_environment_labels_data_object:[52:58]\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_present_population_data_object:[44:50]\n==core.data_objects.silver.silver_semantic_quality_warnings_plot_data:[48:54]\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_cell_intersection_groups_data_object:[43:49]\n==core.data_objects.silver.silver_event_flagged_data_object:[60:66]\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_midterm_permanence_score_data_object:[58:64]\n==core.data_objects.silver.silver_present_population_zone_data_object:[44:50]\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_daily_permanence_score_data_object:[45:50]\n==core.data_objects.silver.silver_event_flagged_data_object:[52:57]\n        self.partition_columns = [\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            ColNames.user_id_modulo,</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.silver.silver_cell_connection_probabilities_data_object:[49:55]\n==core.data_objects.silver.silver_network_syntactic_quality_warnings_plot_data:[85:91]\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.bronze.bronze_admin_units_data_object:[60:66]\n==core.data_objects.landing.landing_geoparquet_data_object:[31:37]\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.bronze.bronze_event_data_object:[51:57]\n==core.data_objects.silver.silver_network_syntactic_quality_warnings_plot_data:[51:59]\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n\n</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.bronze.bronze_synthetic_diaries_data_object:[52:58]\n==core.data_objects.silver.silver_longterm_permanence_score_data_object:[61:67]\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.bronze.bronze_landuse_data_object:[57:63]\n==core.data_objects.silver.silver_time_segments_data_object:[56:62]\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.bronze.bronze_network_physical_data_object:[71:77]\n==core.data_objects.silver.silver_usual_environment_labeling_quality_metrics_data_object:[45:51]\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.bronze.bronze_transportation_data_object:[56:62]\n==core.data_objects.silver.silver_geozones_grid_map_data_object:[52:58]\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==core.data_objects.bronze.bronze_geographic_zones_data_object:[60:66]\n==core.data_objects.silver.silver_daily_permanence_score_data_object:[58:64]\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n\n        self.interface.write_from_interface(self.df, path, partition_columns)</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==components.execution.cell_connection_probability.cell_connection_probability:[68:73]\n==components.execution.geozones_grid_mapping.geozones_grid_mapping:[67:72]\n                path = self.config.get(CONFIG_SILVER_PATHS_KEY, key)\n            if check_if_data_path_exists(self.spark, path):\n                self.input_data_objects[value.ID] = value(self.spark, path)\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")</pre> 1 0 refactor <code>duplicate-code</code> R0801 <pre>Similar lines in 2 files\n==components.execution.usual_environment_aggregation.usual_environment_aggregation:[67:76]\n==components.execution.usual_environment_labeling.usual_environment_labeling:[73:80]\n        self.season = self.config.get(self.COMPONENT_ID, \"season\")\n        if self.season not in SEASONS:\n            error_msg = f\"season: expected one of: {', '.join(SEASONS)} - found: {self.season}\"\n            self.logger.error(error_msg)\n            raise ValueError(error_msg)\n\n        # Months that compose the long-term period, at least one</pre> 6 0 refactor <code>too-few-public-methods</code> R0903 Transformations <pre>Too few public methods (0/2)</pre> Module <code>core.constants.warnings</code> (<code>multimno/core/constants/warnings.py</code>) Line Col. Type Symbol ID Obj Message 1 0 convention <code>missing-module-docstring</code> C0114 <pre>Missing module docstring</pre> 1 0 convention <code>missing-class-docstring</code> C0115 Warnings <pre>Missing class docstring</pre> 1 0 refactor <code>too-few-public-methods</code> R0903 Warnings <pre>Too few public methods (0/2)</pre> 2 0 convention <code>line-too-long</code> C0301 <pre>Line too long (185/120)</pre> 3 0 convention <code>line-too-long</code> C0301 <pre>Line too long (157/120)</pre> 5 0 convention <code>line-too-long</code> C0301 <pre>Line too long (146/120)</pre> 6 0 convention <code>line-too-long</code> C0301 <pre>Line too long (194/120)</pre> 9 0 convention <code>line-too-long</code> C0301 <pre>Line too long (156/120)</pre> 10 0 convention <code>line-too-long</code> C0301 <pre>Line too long (204/120)</pre> Module <code>core.data_objects.bronze.bronze_admin_units_data_object</code> (<code>multimno/core/data_objects/bronze/bronze_admin_units_data_object.py</code>) Line Col. Type Symbol ID Obj Message 1 0 convention <code>empty-docstring</code> C0112 <pre>Empty module docstring</pre> 54 4 warning <code>arguments-differ</code> W0221 BronzeAdminUnitsDataObject.read <pre>Number of parameters was 4 in 'PathDataObject.read' and is now 1 in overriding 'BronzeAdminUnitsDataObject.read' method</pre> 54 4 warning <code>arguments-differ</code> W0221 BronzeAdminUnitsDataObject.read <pre>Variadics removed in overriding 'BronzeAdminUnitsDataObject.read' method</pre> 59 4 warning <code>arguments-differ</code> W0221 BronzeAdminUnitsDataObject.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'BronzeAdminUnitsDataObject.write' method</pre> 59 4 warning <code>arguments-differ</code> W0221 BronzeAdminUnitsDataObject.write <pre>Variadics removed in overriding 'BronzeAdminUnitsDataObject.write' method</pre> Module <code>core.data_objects.bronze.bronze_countries_data_object</code> (<code>multimno/core/data_objects/bronze/bronze_countries_data_object.py</code>) Line Col. Type Symbol ID Obj Message 1 0 convention <code>empty-docstring</code> C0112 <pre>Empty module docstring</pre> 44 4 warning <code>arguments-differ</code> W0221 BronzeCountriesDataObject.read <pre>Number of parameters was 4 in 'PathDataObject.read' and is now 1 in overriding 'BronzeCountriesDataObject.read' method</pre> 44 4 warning <code>arguments-differ</code> W0221 BronzeCountriesDataObject.read <pre>Variadics removed in overriding 'BronzeCountriesDataObject.read' method</pre> 49 4 warning <code>arguments-differ</code> W0221 BronzeCountriesDataObject.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'BronzeCountriesDataObject.write' method</pre> 49 4 warning <code>arguments-differ</code> W0221 BronzeCountriesDataObject.write <pre>Variadics removed in overriding 'BronzeCountriesDataObject.write' method</pre> Module <code>core.data_objects.bronze.bronze_event_data_object</code> (<code>multimno/core/data_objects/bronze/bronze_event_data_object.py</code>) Line Col. Type Symbol ID Obj Message 51 4 warning <code>arguments-differ</code> W0221 BronzeEventDataObject.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'BronzeEventDataObject.write' method</pre> 51 4 warning <code>arguments-differ</code> W0221 BronzeEventDataObject.write <pre>Variadics removed in overriding 'BronzeEventDataObject.write' method</pre> Module <code>core.data_objects.bronze.bronze_geographic_zones_data_object</code> (<code>multimno/core/data_objects/bronze/bronze_geographic_zones_data_object.py</code>) Line Col. Type Symbol ID Obj Message 1 0 convention <code>empty-docstring</code> C0112 <pre>Empty module docstring</pre> 54 4 warning <code>arguments-differ</code> W0221 BronzeGeographicZonesDataObject.read <pre>Number of parameters was 4 in 'PathDataObject.read' and is now 1 in overriding 'BronzeGeographicZonesDataObject.read' method</pre> 54 4 warning <code>arguments-differ</code> W0221 BronzeGeographicZonesDataObject.read <pre>Variadics removed in overriding 'BronzeGeographicZonesDataObject.read' method</pre> 59 4 warning <code>arguments-differ</code> W0221 BronzeGeographicZonesDataObject.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'BronzeGeographicZonesDataObject.write' method</pre> 59 4 warning <code>arguments-differ</code> W0221 BronzeGeographicZonesDataObject.write <pre>Variadics removed in overriding 'BronzeGeographicZonesDataObject.write' method</pre> Module <code>core.data_objects.bronze.bronze_holiday_calendar_data_object</code> (<code>multimno/core/data_objects/bronze/bronze_holiday_calendar_data_object.py</code>) Line Col. Type Symbol ID Obj Message 33 4 warning <code>arguments-differ</code> W0221 BronzeHolidayCalendarDataObject.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'BronzeHolidayCalendarDataObject.write' method</pre> 33 4 warning <code>arguments-differ</code> W0221 BronzeHolidayCalendarDataObject.write <pre>Variadics removed in overriding 'BronzeHolidayCalendarDataObject.write' method</pre> Module <code>core.data_objects.bronze.bronze_landuse_data_object</code> (<code>multimno/core/data_objects/bronze/bronze_landuse_data_object.py</code>) Line Col. Type Symbol ID Obj Message 1 0 convention <code>empty-docstring</code> C0112 <pre>Empty module docstring</pre> 9 0 warning <code>unused-import</code> W0611 <pre>Unused DateType imported from pyspark.sql.types</pre> 51 4 warning <code>arguments-differ</code> W0221 BronzeLanduseDataObject.read <pre>Number of parameters was 4 in 'PathDataObject.read' and is now 1 in overriding 'BronzeLanduseDataObject.read' method</pre> 51 4 warning <code>arguments-differ</code> W0221 BronzeLanduseDataObject.read <pre>Variadics removed in overriding 'BronzeLanduseDataObject.read' method</pre> 56 4 warning <code>arguments-differ</code> W0221 BronzeLanduseDataObject.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'BronzeLanduseDataObject.write' method</pre> 56 4 warning <code>arguments-differ</code> W0221 BronzeLanduseDataObject.write <pre>Variadics removed in overriding 'BronzeLanduseDataObject.write' method</pre> Module <code>core.data_objects.bronze.bronze_network_physical_data_object</code> (<code>multimno/core/data_objects/bronze/bronze_network_physical_data_object.py</code>) Line Col. Type Symbol ID Obj Message 71 4 warning <code>arguments-differ</code> W0221 BronzeNetworkDataObject.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'BronzeNetworkDataObject.write' method</pre> 71 4 warning <code>arguments-differ</code> W0221 BronzeNetworkDataObject.write <pre>Variadics removed in overriding 'BronzeNetworkDataObject.write' method</pre> Module <code>core.data_objects.bronze.bronze_synthetic_diaries_data_object</code> (<code>multimno/core/data_objects/bronze/bronze_synthetic_diaries_data_object.py</code>) Line Col. Type Symbol ID Obj Message 52 4 warning <code>arguments-differ</code> W0221 BronzeSyntheticDiariesDataObject.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'BronzeSyntheticDiariesDataObject.write' method</pre> 52 4 warning <code>arguments-differ</code> W0221 BronzeSyntheticDiariesDataObject.write <pre>Variadics removed in overriding 'BronzeSyntheticDiariesDataObject.write' method</pre> Module <code>core.data_objects.bronze.bronze_transportation_data_object</code> (<code>multimno/core/data_objects/bronze/bronze_transportation_data_object.py</code>) Line Col. Type Symbol ID Obj Message 1 0 convention <code>empty-docstring</code> C0112 <pre>Empty module docstring</pre> 50 4 warning <code>arguments-differ</code> W0221 BronzeTransportationDataObject.read <pre>Number of parameters was 4 in 'PathDataObject.read' and is now 1 in overriding 'BronzeTransportationDataObject.read' method</pre> 50 4 warning <code>arguments-differ</code> W0221 BronzeTransportationDataObject.read <pre>Variadics removed in overriding 'BronzeTransportationDataObject.read' method</pre> 55 4 warning <code>arguments-differ</code> W0221 BronzeTransportationDataObject.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'BronzeTransportationDataObject.write' method</pre> 55 4 warning <code>arguments-differ</code> W0221 BronzeTransportationDataObject.write <pre>Variadics removed in overriding 'BronzeTransportationDataObject.write' method</pre> Module <code>core.data_objects.landing.landing_geoparquet_data_object</code> (<code>multimno/core/data_objects/landing/landing_geoparquet_data_object.py</code>) Line Col. Type Symbol ID Obj Message 1 0 convention <code>empty-docstring</code> C0112 <pre>Empty module docstring</pre> 6 0 warning <code>unused-import</code> W0611 <pre>Unused StructType imported from pyspark.sql.types</pre> 10 0 warning <code>unused-import</code> W0611 <pre>Unused ColNames imported from multimno.core.constants.columns</pre> 26 4 warning <code>arguments-differ</code> W0221 LandingGeoParquetDataObject.read <pre>Number of parameters was 4 in 'PathDataObject.read' and is now 1 in overriding 'LandingGeoParquetDataObject.read' method</pre> 26 4 warning <code>arguments-differ</code> W0221 LandingGeoParquetDataObject.read <pre>Variadics removed in overriding 'LandingGeoParquetDataObject.read' method</pre> 30 4 warning <code>arguments-differ</code> W0221 LandingGeoParquetDataObject.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'LandingGeoParquetDataObject.write' method</pre> 30 4 warning <code>arguments-differ</code> W0221 LandingGeoParquetDataObject.write <pre>Variadics removed in overriding 'LandingGeoParquetDataObject.write' method</pre> Module <code>core.data_objects.landing.landing_http_geojson_data_object</code> (<code>multimno/core/data_objects/landing/landing_http_geojson_data_object.py</code>) Line Col. Type Symbol ID Obj Message 1 0 convention <code>empty-docstring</code> C0112 <pre>Empty module docstring</pre> 26 4 warning <code>arguments-differ</code> W0221 LandingHttpGeoJsonDataObject.read <pre>Number of parameters was 4 in 'PathDataObject.read' and is now 1 in overriding 'LandingHttpGeoJsonDataObject.read' method</pre> 26 4 warning <code>arguments-differ</code> W0221 LandingHttpGeoJsonDataObject.read <pre>Variadics removed in overriding 'LandingHttpGeoJsonDataObject.read' method</pre> Module <code>core.data_objects.silver.silver_aggregated_usual_environments_data_object</code> (<code>multimno/core/data_objects/silver/silver_aggregated_usual_environments_data_object.py</code>) Line Col. Type Symbol ID Obj Message 42 4 warning <code>arguments-differ</code> W0221 SilverAggregatedUsualEnvironmentsDataObject.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'SilverAggregatedUsualEnvironmentsDataObject.write' method</pre> 42 4 warning <code>arguments-differ</code> W0221 SilverAggregatedUsualEnvironmentsDataObject.write <pre>Variadics removed in overriding 'SilverAggregatedUsualEnvironmentsDataObject.write' method</pre> Module <code>core.data_objects.silver.silver_cell_connection_probabilities_data_object</code> (<code>multimno/core/data_objects/silver/silver_cell_connection_probabilities_data_object.py</code>) Line Col. Type Symbol ID Obj Message 13 0 convention <code>empty-docstring</code> C0112 SilverCellConnectionProbabilitiesDataObject <pre>Empty class docstring</pre> 49 4 warning <code>arguments-differ</code> W0221 SilverCellConnectionProbabilitiesDataObject.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'SilverCellConnectionProbabilitiesDataObject.write' method</pre> 49 4 warning <code>arguments-differ</code> W0221 SilverCellConnectionProbabilitiesDataObject.write <pre>Variadics removed in overriding 'SilverCellConnectionProbabilitiesDataObject.write' method</pre> Module <code>core.data_objects.silver.silver_cell_footprint_data_object</code> (<code>multimno/core/data_objects/silver/silver_cell_footprint_data_object.py</code>) Line Col. Type Symbol ID Obj Message 1 0 convention <code>empty-docstring</code> C0112 <pre>Empty module docstring</pre> 6 0 warning <code>unused-import</code> W0611 <pre>Unused IntegerType imported from pyspark.sql.types</pre> 13 0 convention <code>empty-docstring</code> C0112 SilverCellFootprintDataObject <pre>Empty class docstring</pre> 47 4 warning <code>arguments-differ</code> W0221 SilverCellFootprintDataObject.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'SilverCellFootprintDataObject.write' method</pre> 47 4 warning <code>arguments-differ</code> W0221 SilverCellFootprintDataObject.write <pre>Variadics removed in overriding 'SilverCellFootprintDataObject.write' method</pre> Module <code>core.data_objects.silver.silver_cell_intersection_groups_data_object</code> (<code>multimno/core/data_objects/silver/silver_cell_intersection_groups_data_object.py</code>) Line Col. Type Symbol ID Obj Message 1 0 convention <code>empty-docstring</code> C0112 <pre>Empty module docstring</pre> 13 0 convention <code>empty-docstring</code> C0112 SilverCellIntersectionGroupsDataObject <pre>Empty class docstring</pre> 43 4 warning <code>arguments-differ</code> W0221 SilverCellIntersectionGroupsDataObject.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'SilverCellIntersectionGroupsDataObject.write' method</pre> 43 4 warning <code>arguments-differ</code> W0221 SilverCellIntersectionGroupsDataObject.write <pre>Variadics removed in overriding 'SilverCellIntersectionGroupsDataObject.write' method</pre> Module <code>core.data_objects.silver.silver_daily_permanence_score_data_object</code> (<code>multimno/core/data_objects/silver/silver_daily_permanence_score_data_object.py</code>) Line Col. Type Symbol ID Obj Message 57 4 warning <code>arguments-differ</code> W0221 SilverDailyPermanenceScoreDataObject.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'SilverDailyPermanenceScoreDataObject.write' method</pre> 57 4 warning <code>arguments-differ</code> W0221 SilverDailyPermanenceScoreDataObject.write <pre>Variadics removed in overriding 'SilverDailyPermanenceScoreDataObject.write' method</pre> Module <code>core.data_objects.silver.silver_device_activity_statistics</code> (<code>multimno/core/data_objects/silver/silver_device_activity_statistics.py</code>) Line Col. Type Symbol ID Obj Message 48 4 warning <code>arguments-differ</code> W0221 SilverDeviceActivityStatistics.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'SilverDeviceActivityStatistics.write' method</pre> 48 4 warning <code>arguments-differ</code> W0221 SilverDeviceActivityStatistics.write <pre>Variadics removed in overriding 'SilverDeviceActivityStatistics.write' method</pre> Module <code>core.data_objects.silver.silver_enriched_grid_data_object</code> (<code>multimno/core/data_objects/silver/silver_enriched_grid_data_object.py</code>) Line Col. Type Symbol ID Obj Message 1 0 convention <code>empty-docstring</code> C0112 <pre>Empty module docstring</pre> 9 0 warning <code>unused-import</code> W0611 <pre>Unused IntegerType imported from pyspark.sql.types</pre> 48 4 warning <code>arguments-differ</code> W0221 SilverEnrichedGridDataObject.read <pre>Number of parameters was 4 in 'PathDataObject.read' and is now 1 in overriding 'SilverEnrichedGridDataObject.read' method</pre> 48 4 warning <code>arguments-differ</code> W0221 SilverEnrichedGridDataObject.read <pre>Variadics removed in overriding 'SilverEnrichedGridDataObject.read' method</pre> 53 4 warning <code>arguments-differ</code> W0221 SilverEnrichedGridDataObject.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'SilverEnrichedGridDataObject.write' method</pre> 53 4 warning <code>arguments-differ</code> W0221 SilverEnrichedGridDataObject.write <pre>Variadics removed in overriding 'SilverEnrichedGridDataObject.write' method</pre> Module <code>core.data_objects.silver.silver_event_data_object</code> (<code>multimno/core/data_objects/silver/silver_event_data_object.py</code>) Line Col. Type Symbol ID Obj Message 60 4 warning <code>arguments-differ</code> W0221 SilverEventDataObject.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'SilverEventDataObject.write' method</pre> 60 4 warning <code>arguments-differ</code> W0221 SilverEventDataObject.write <pre>Variadics removed in overriding 'SilverEventDataObject.write' method</pre> Module <code>core.data_objects.silver.silver_event_data_syntactic_quality_metrics_by_column</code> (<code>multimno/core/data_objects/silver/silver_event_data_syntactic_quality_metrics_by_column.py</code>) Line Col. Type Symbol ID Obj Message 46 4 warning <code>arguments-differ</code> W0221 SilverEventDataSyntacticQualityMetricsByColumn.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'SilverEventDataSyntacticQualityMetricsByColumn.write' method</pre> 46 4 warning <code>arguments-differ</code> W0221 SilverEventDataSyntacticQualityMetricsByColumn.write <pre>Variadics removed in overriding 'SilverEventDataSyntacticQualityMetricsByColumn.write' method</pre> Module <code>core.data_objects.silver.silver_event_data_syntactic_quality_metrics_frequency_distribution</code> (<code>multimno/core/data_objects/silver/silver_event_data_syntactic_quality_metrics_frequency_distribution.py</code>) Line Col. Type Symbol ID Obj Message 41 4 warning <code>arguments-differ</code> W0221 SilverEventDataSyntacticQualityMetricsFrequencyDistribution.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'SilverEventDataSyntacticQualityMetricsFrequencyDistribution.write' method</pre> 41 4 warning <code>arguments-differ</code> W0221 SilverEventDataSyntacticQualityMetricsFrequencyDistribution.write <pre>Variadics removed in overriding 'SilverEventDataSyntacticQualityMetricsFrequencyDistribution.write' method</pre> Module <code>core.data_objects.silver.silver_event_data_syntactic_quality_warnings_for_plots</code> (<code>multimno/core/data_objects/silver/silver_event_data_syntactic_quality_warnings_for_plots.py</code>) Line Col. Type Symbol ID Obj Message 5 0 warning <code>unused-import</code> W0611 <pre>Unused defaultdict imported from collections</pre> 43 4 warning <code>arguments-differ</code> W0221 SilverEventDataSyntacticQualityWarningsForPlots.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'SilverEventDataSyntacticQualityWarningsForPlots.write' method</pre> 43 4 warning <code>arguments-differ</code> W0221 SilverEventDataSyntacticQualityWarningsForPlots.write <pre>Variadics removed in overriding 'SilverEventDataSyntacticQualityWarningsForPlots.write' method</pre> Module <code>core.data_objects.silver.silver_event_data_syntactic_quality_warnings_log_table</code> (<code>multimno/core/data_objects/silver/silver_event_data_syntactic_quality_warnings_log_table.py</code>) Line Col. Type Symbol ID Obj Message 42 4 warning <code>arguments-differ</code> W0221 SilverEventDataSyntacticQualityWarningsLogTable.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'SilverEventDataSyntacticQualityWarningsLogTable.write' method</pre> 42 4 warning <code>arguments-differ</code> W0221 SilverEventDataSyntacticQualityWarningsLogTable.write <pre>Variadics removed in overriding 'SilverEventDataSyntacticQualityWarningsLogTable.write' method</pre> Module <code>core.data_objects.silver.silver_event_flagged_data_object</code> (<code>multimno/core/data_objects/silver/silver_event_flagged_data_object.py</code>) Line Col. Type Symbol ID Obj Message 60 4 warning <code>arguments-differ</code> W0221 SilverEventFlaggedDataObject.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'SilverEventFlaggedDataObject.write' method</pre> 60 4 warning <code>arguments-differ</code> W0221 SilverEventFlaggedDataObject.write <pre>Variadics removed in overriding 'SilverEventFlaggedDataObject.write' method</pre> Module <code>core.data_objects.silver.silver_geozones_grid_map_data_object</code> (<code>multimno/core/data_objects/silver/silver_geozones_grid_map_data_object.py</code>) Line Col. Type Symbol ID Obj Message 1 0 convention <code>empty-docstring</code> C0112 <pre>Empty module docstring</pre> 47 4 warning <code>arguments-differ</code> W0221 SilverGeozonesGridMapDataObject.read <pre>Number of parameters was 4 in 'PathDataObject.read' and is now 1 in overriding 'SilverGeozonesGridMapDataObject.read' method</pre> 47 4 warning <code>arguments-differ</code> W0221 SilverGeozonesGridMapDataObject.read <pre>Variadics removed in overriding 'SilverGeozonesGridMapDataObject.read' method</pre> 51 4 warning <code>arguments-differ</code> W0221 SilverGeozonesGridMapDataObject.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'SilverGeozonesGridMapDataObject.write' method</pre> 51 4 warning <code>arguments-differ</code> W0221 SilverGeozonesGridMapDataObject.write <pre>Variadics removed in overriding 'SilverGeozonesGridMapDataObject.write' method</pre> Module <code>core.data_objects.silver.silver_grid_data_object</code> (<code>multimno/core/data_objects/silver/silver_grid_data_object.py</code>) Line Col. Type Symbol ID Obj Message 1 0 convention <code>empty-docstring</code> C0112 <pre>Empty module docstring</pre> 9 0 warning <code>unused-import</code> W0611 <pre>Unused FloatType imported from pyspark.sql.types</pre> 9 0 warning <code>unused-import</code> W0611 <pre>Unused IntegerType imported from pyspark.sql.types</pre> 44 4 warning <code>arguments-differ</code> W0221 SilverGridDataObject.read <pre>Number of parameters was 4 in 'PathDataObject.read' and is now 1 in overriding 'SilverGridDataObject.read' method</pre> 44 4 warning <code>arguments-differ</code> W0221 SilverGridDataObject.read <pre>Variadics removed in overriding 'SilverGridDataObject.read' method</pre> 49 4 warning <code>arguments-differ</code> W0221 SilverGridDataObject.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'SilverGridDataObject.write' method</pre> 49 4 warning <code>arguments-differ</code> W0221 SilverGridDataObject.write <pre>Variadics removed in overriding 'SilverGridDataObject.write' method</pre> Module <code>core.data_objects.silver.silver_longterm_permanence_score_data_object</code> (<code>multimno/core/data_objects/silver/silver_longterm_permanence_score_data_object.py</code>) Line Col. Type Symbol ID Obj Message 61 4 warning <code>arguments-differ</code> W0221 SilverLongtermPermanenceScoreDataObject.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'SilverLongtermPermanenceScoreDataObject.write' method</pre> 61 4 warning <code>arguments-differ</code> W0221 SilverLongtermPermanenceScoreDataObject.write <pre>Variadics removed in overriding 'SilverLongtermPermanenceScoreDataObject.write' method</pre> Module <code>core.data_objects.silver.silver_midterm_permanence_score_data_object</code> (<code>multimno/core/data_objects/silver/silver_midterm_permanence_score_data_object.py</code>) Line Col. Type Symbol ID Obj Message 58 4 warning <code>arguments-differ</code> W0221 SilverMidtermPermanenceScoreDataObject.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'SilverMidtermPermanenceScoreDataObject.write' method</pre> 58 4 warning <code>arguments-differ</code> W0221 SilverMidtermPermanenceScoreDataObject.write <pre>Variadics removed in overriding 'SilverMidtermPermanenceScoreDataObject.write' method</pre> Module <code>core.data_objects.silver.silver_network_data_object</code> (<code>multimno/core/data_objects/silver/silver_network_data_object.py</code>) Line Col. Type Symbol ID Obj Message 60 4 warning <code>arguments-differ</code> W0221 SilverNetworkDataObject.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'SilverNetworkDataObject.write' method</pre> 60 4 warning <code>arguments-differ</code> W0221 SilverNetworkDataObject.write <pre>Variadics removed in overriding 'SilverNetworkDataObject.write' method</pre> Module <code>core.data_objects.silver.silver_network_data_syntactic_quality_metrics_by_column</code> (<code>multimno/core/data_objects/silver/silver_network_data_syntactic_quality_metrics_by_column.py</code>) Line Col. Type Symbol ID Obj Message 48 4 warning <code>arguments-differ</code> W0221 SilverNetworkDataQualityMetricsByColumn.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'SilverNetworkDataQualityMetricsByColumn.write' method</pre> 48 4 warning <code>arguments-differ</code> W0221 SilverNetworkDataQualityMetricsByColumn.write <pre>Variadics removed in overriding 'SilverNetworkDataQualityMetricsByColumn.write' method</pre> Module <code>core.data_objects.silver.silver_network_data_top_frequent_errors_data_object</code> (<code>multimno/core/data_objects/silver/silver_network_data_top_frequent_errors_data_object.py</code>) Line Col. Type Symbol ID Obj Message 48 4 warning <code>arguments-differ</code> W0221 SilverNetworkDataTopFrequentErrors.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'SilverNetworkDataTopFrequentErrors.write' method</pre> 48 4 warning <code>arguments-differ</code> W0221 SilverNetworkDataTopFrequentErrors.write <pre>Variadics removed in overriding 'SilverNetworkDataTopFrequentErrors.write' method</pre> Module <code>core.data_objects.silver.silver_network_row_error_metrics</code> (<code>multimno/core/data_objects/silver/silver_network_row_error_metrics.py</code>) Line Col. Type Symbol ID Obj Message 44 4 warning <code>arguments-differ</code> W0221 SilverNetworkRowErrorMetrics.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'SilverNetworkRowErrorMetrics.write' method</pre> 44 4 warning <code>arguments-differ</code> W0221 SilverNetworkRowErrorMetrics.write <pre>Variadics removed in overriding 'SilverNetworkRowErrorMetrics.write' method</pre> Module <code>core.data_objects.silver.silver_network_syntactic_quality_warnings_log_table</code> (<code>multimno/core/data_objects/silver/silver_network_syntactic_quality_warnings_log_table.py</code>) Line Col. Type Symbol ID Obj Message 6 0 warning <code>unused-import</code> W0611 <pre>Unused IntegerType imported from pyspark.sql.types</pre> 53 4 warning <code>arguments-differ</code> W0221 SilverNetworkDataSyntacticQualityWarningsLogTable.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'SilverNetworkDataSyntacticQualityWarningsLogTable.write' method</pre> 53 4 warning <code>arguments-differ</code> W0221 SilverNetworkDataSyntacticQualityWarningsLogTable.write <pre>Variadics removed in overriding 'SilverNetworkDataSyntacticQualityWarningsLogTable.write' method</pre> Module <code>core.data_objects.silver.silver_network_syntactic_quality_warnings_plot_data</code> (<code>multimno/core/data_objects/silver/silver_network_syntactic_quality_warnings_plot_data.py</code>) Line Col. Type Symbol ID Obj Message 51 4 warning <code>arguments-differ</code> W0221 SilverNetworkSyntacticQualityWarningsLinePlotData.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'SilverNetworkSyntacticQualityWarningsLinePlotData.write' method</pre> 51 4 warning <code>arguments-differ</code> W0221 SilverNetworkSyntacticQualityWarningsLinePlotData.write <pre>Variadics removed in overriding 'SilverNetworkSyntacticQualityWarningsLinePlotData.write' method</pre> 85 4 warning <code>arguments-differ</code> W0221 SilverNetworkSyntacticQualityWarningsPiePlotData.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'SilverNetworkSyntacticQualityWarningsPiePlotData.write' method</pre> 85 4 warning <code>arguments-differ</code> W0221 SilverNetworkSyntacticQualityWarningsPiePlotData.write <pre>Variadics removed in overriding 'SilverNetworkSyntacticQualityWarningsPiePlotData.write' method</pre> Module <code>core.data_objects.silver.silver_present_population_data_object</code> (<code>multimno/core/data_objects/silver/silver_present_population_data_object.py</code>) Line Col. Type Symbol ID Obj Message 44 4 warning <code>arguments-differ</code> W0221 SilverPresentPopulationDataObject.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'SilverPresentPopulationDataObject.write' method</pre> 44 4 warning <code>arguments-differ</code> W0221 SilverPresentPopulationDataObject.write <pre>Variadics removed in overriding 'SilverPresentPopulationDataObject.write' method</pre> Module <code>core.data_objects.silver.silver_present_population_zone_data_object</code> (<code>multimno/core/data_objects/silver/silver_present_population_zone_data_object.py</code>) Line Col. Type Symbol ID Obj Message 44 4 warning <code>arguments-differ</code> W0221 SilverPresentPopulationZoneDataObject.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'SilverPresentPopulationZoneDataObject.write' method</pre> 44 4 warning <code>arguments-differ</code> W0221 SilverPresentPopulationZoneDataObject.write <pre>Variadics removed in overriding 'SilverPresentPopulationZoneDataObject.write' method</pre> Module <code>core.data_objects.silver.silver_semantic_quality_metrics</code> (<code>multimno/core/data_objects/silver/silver_semantic_quality_metrics.py</code>) Line Col. Type Symbol ID Obj Message 1 0 convention <code>empty-docstring</code> C0112 <pre>Empty module docstring</pre> 21 0 convention <code>empty-docstring</code> C0112 SilverEventSemanticQualityMetrics <pre>Empty class docstring</pre> 43 4 warning <code>arguments-differ</code> W0221 SilverEventSemanticQualityMetrics.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'SilverEventSemanticQualityMetrics.write' method</pre> 43 4 warning <code>arguments-differ</code> W0221 SilverEventSemanticQualityMetrics.write <pre>Variadics removed in overriding 'SilverEventSemanticQualityMetrics.write' method</pre> Module <code>core.data_objects.silver.silver_semantic_quality_warnings_log_table</code> (<code>multimno/core/data_objects/silver/silver_semantic_quality_warnings_log_table.py</code>) Line Col. Type Symbol ID Obj Message 65 4 warning <code>arguments-differ</code> W0221 SilverEventSemanticQualityWarningsLogTable.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'SilverEventSemanticQualityWarningsLogTable.write' method</pre> 65 4 warning <code>arguments-differ</code> W0221 SilverEventSemanticQualityWarningsLogTable.write <pre>Variadics removed in overriding 'SilverEventSemanticQualityWarningsLogTable.write' method</pre> Module <code>core.data_objects.silver.silver_semantic_quality_warnings_plot_data</code> (<code>multimno/core/data_objects/silver/silver_semantic_quality_warnings_plot_data.py</code>) Line Col. Type Symbol ID Obj Message 48 4 warning <code>arguments-differ</code> W0221 SilverEventSemanticQualityWarningsBarPlotData.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'SilverEventSemanticQualityWarningsBarPlotData.write' method</pre> 48 4 warning <code>arguments-differ</code> W0221 SilverEventSemanticQualityWarningsBarPlotData.write <pre>Variadics removed in overriding 'SilverEventSemanticQualityWarningsBarPlotData.write' method</pre> Module <code>core.data_objects.silver.silver_signal_strength_data_object</code> (<code>multimno/core/data_objects/silver/silver_signal_strength_data_object.py</code>) Line Col. Type Symbol ID Obj Message 1 0 convention <code>empty-docstring</code> C0112 <pre>Empty module docstring</pre> 6 0 warning <code>unused-import</code> W0611 <pre>Unused IntegerType imported from pyspark.sql.types</pre> 13 0 convention <code>empty-docstring</code> C0112 SilverSignalStrengthDataObject <pre>Empty class docstring</pre> 50 4 warning <code>arguments-differ</code> W0221 SilverSignalStrengthDataObject.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'SilverSignalStrengthDataObject.write' method</pre> 50 4 warning <code>arguments-differ</code> W0221 SilverSignalStrengthDataObject.write <pre>Variadics removed in overriding 'SilverSignalStrengthDataObject.write' method</pre> Module <code>core.data_objects.silver.silver_time_segments_data_object</code> (<code>multimno/core/data_objects/silver/silver_time_segments_data_object.py</code>) Line Col. Type Symbol ID Obj Message 1 0 convention <code>empty-docstring</code> C0112 <pre>Empty module docstring</pre> 55 4 warning <code>arguments-differ</code> W0221 SilverTimeSegmentsDataObject.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'SilverTimeSegmentsDataObject.write' method</pre> 55 4 warning <code>arguments-differ</code> W0221 SilverTimeSegmentsDataObject.write <pre>Variadics removed in overriding 'SilverTimeSegmentsDataObject.write' method</pre> Module <code>core.data_objects.silver.silver_usual_environment_labeling_quality_metrics_data_object</code> (<code>multimno/core/data_objects/silver/silver_usual_environment_labeling_quality_metrics_data_object.py</code>) Line Col. Type Symbol ID Obj Message 45 4 warning <code>arguments-differ</code> W0221 SilverUsualEnvironmentLabelingQualityMetricsDataObject.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'SilverUsualEnvironmentLabelingQualityMetricsDataObject.write' method</pre> 45 4 warning <code>arguments-differ</code> W0221 SilverUsualEnvironmentLabelingQualityMetricsDataObject.write <pre>Variadics removed in overriding 'SilverUsualEnvironmentLabelingQualityMetricsDataObject.write' method</pre> Module <code>core.data_objects.silver.silver_usual_environment_labels_data_object</code> (<code>multimno/core/data_objects/silver/silver_usual_environment_labels_data_object.py</code>) Line Col. Type Symbol ID Obj Message 6 0 warning <code>unused-import</code> W0611 <pre>Unused FloatType imported from pyspark.sql.types</pre> 52 4 warning <code>arguments-differ</code> W0221 SilverUsualEnvironmentLabelsDataObject.write <pre>Number of parameters was 5 in 'PathDataObject.write' and is now 3 in overriding 'SilverUsualEnvironmentLabelsDataObject.write' method</pre> 52 4 warning <code>arguments-differ</code> W0221 SilverUsualEnvironmentLabelsDataObject.write <pre>Variadics removed in overriding 'SilverUsualEnvironmentLabelsDataObject.write' method</pre> Module <code>core.grid</code> (<code>multimno/core/grid.py</code>) Line Col. Type Symbol ID Obj Message 58 4 refactor <code>too-many-arguments</code> R0913 InspireGridGenerator.__init__ <pre>Too many arguments (6/5)</pre> 99 8 refactor <code>no-else-return</code> R1705 InspireGridGenerator._format_distance <pre>Unnecessary \"else\" after \"return\", remove the \"else\" and de-indent the code inside it</pre> 513 4 warning <code>arguments-renamed</code> W0237 InspireGridGenerator.get_children_grid_ids <pre>Parameter 'sdf' has been renamed to 'grid_id' in overriding 'InspireGridGenerator.get_children_grid_ids' method</pre> 514 9 warning <code>fixme</code> W0511 <pre>TODO: Implement this method</pre> 517 4 warning <code>arguments-renamed</code> W0237 InspireGridGenerator.get_parent_grid_id <pre>Parameter 'sdf' has been renamed to 'grid_id' in overriding 'InspireGridGenerator.get_parent_grid_id' method</pre> 518 9 warning <code>fixme</code> W0511 <pre>TODO: Implement this method</pre> Module <code>core.io_interface</code> (<code>multimno/core/io_interface.py</code>) Line Col. Type Symbol ID Obj Message 8 0 error <code>import-error</code> E0401 <pre>Unable to import 'requests.packages.urllib3.util.retry'</pre> 9 0 convention <code>wrong-import-order</code> C0411 <pre>standard import \"abc.ABCMeta\" should be placed before third party imports \"requests\", \"requests.adapters.HTTPAdapter\", \"requests.packages.urllib3.util.retry.Retry\"</pre> 21 8 refactor <code>no-else-return</code> R1705 IOInterface.__subclasshook__ <pre>Unnecessary \"else\" after \"return\", remove the \"else\" and de-indent the code inside it</pre> 34 4 convention <code>missing-function-docstring</code> C0116 IOInterface.read_from_interface <pre>Missing function or method docstring</pre> 38 4 convention <code>missing-function-docstring</code> C0116 IOInterface.write_from_interface <pre>Missing function or method docstring</pre> 47 4 warning <code>arguments-differ</code> W0221 PathInterface.read_from_interface <pre>Number of parameters was 3 in 'IOInterface.read_from_interface' and is now 4 in overriding 'PathInterface.read_from_interface' method</pre> 47 4 warning <code>arguments-differ</code> W0221 PathInterface.read_from_interface <pre>Variadics removed in overriding 'PathInterface.read_from_interface' method</pre> 58 8 refactor <code>no-else-return</code> R1705 PathInterface.read_from_interface <pre>Unnecessary \"else\" after \"return\", remove the \"else\" and de-indent the code inside it</pre> 65 4 warning <code>arguments-differ</code> W0221 PathInterface.write_from_interface <pre>Number of parameters was 4 in 'IOInterface.write_from_interface' and is now 4 in overriding 'PathInterface.write_from_interface' method</pre> 65 4 warning <code>arguments-differ</code> W0221 PathInterface.write_from_interface <pre>Variadics removed in overriding 'PathInterface.write_from_interface' method</pre> 131 4 refactor <code>too-many-arguments</code> R0913 CsvInterface.read_from_interface <pre>Too many arguments (6/5)</pre> 151 4 refactor <code>too-many-arguments</code> R0913 CsvInterface.write_from_interface <pre>Too many arguments (6/5)</pre> 182 4 warning <code>arguments-differ</code> W0221 HttpGeoJsonInterface.read_from_interface <pre>Number of parameters was 3 in 'IOInterface.read_from_interface' and is now 5 in overriding 'HttpGeoJsonInterface.read_from_interface' method</pre> 182 4 warning <code>arguments-differ</code> W0221 HttpGeoJsonInterface.read_from_interface <pre>Variadics removed in overriding 'HttpGeoJsonInterface.read_from_interface' method</pre> 203 12 warning <code>raise-missing-from</code> W0707 HttpGeoJsonInterface.read_from_interface <pre>Consider explicitly re-raising using 'raise Exception('Maximum number of retries exceeded.') from e'</pre> 203 12 warning <code>broad-exception-raised</code> W0719 HttpGeoJsonInterface.read_from_interface <pre>Raising too general exception: Exception</pre> 206 12 warning <code>broad-exception-raised</code> W0719 HttpGeoJsonInterface.read_from_interface <pre>Raising too general exception: Exception</pre> 216 4 warning <code>arguments-differ</code> W0221 HttpGeoJsonInterface.write_from_interface <pre>Number of parameters was 4 in 'IOInterface.write_from_interface' and is now 5 in overriding 'HttpGeoJsonInterface.write_from_interface' method</pre> 216 4 warning <code>arguments-differ</code> W0221 HttpGeoJsonInterface.write_from_interface <pre>Variadics removed in overriding 'HttpGeoJsonInterface.write_from_interface' method</pre> Module <code>core.log</code> (<code>multimno/core/log.py</code>) Line Col. Type Symbol ID Obj Message 12 0 convention <code>missing-class-docstring</code> C0115 LoggerKeys <pre>Missing class docstring</pre> 12 0 refactor <code>too-few-public-methods</code> R0903 LoggerKeys <pre>Too few public methods (0/2)</pre> 22 0 refactor <code>too-many-locals</code> R0914 generate_logger <pre>Too many local variables (16/15)</pre> Module <code>core.utils</code> (<code>multimno/core/utils.py</code>) Line Col. Type Symbol ID Obj Message 13 0 warning <code>reimported</code> W0404 <pre>Reimport 'st_functions' (imported line 10)</pre> 14 0 warning <code>unused-import</code> W0611 <pre>Unused GeometryType imported from sedona.sql.types</pre> 19 0 warning <code>unused-import</code> W0611 <pre>Unused BooleanType imported from pyspark.sql.types</pre> 108 4 convention <code>invalid-name</code> C0103 latlon_to_tilexy <pre>Variable name \"sinLatitude\" doesn't conform to snake_case naming style</pre> 109 4 convention <code>invalid-name</code> C0103 latlon_to_tilexy <pre>Variable name \"pixelX\" doesn't conform to snake_case naming style</pre> 110 4 convention <code>invalid-name</code> C0103 latlon_to_tilexy <pre>Variable name \"pixelY\" doesn't conform to snake_case naming style</pre> 111 4 convention <code>invalid-name</code> C0103 latlon_to_tilexy <pre>Variable name \"tileX\" doesn't conform to snake_case naming style</pre> 112 4 convention <code>invalid-name</code> C0103 latlon_to_tilexy <pre>Variable name \"tileY\" doesn't conform to snake_case naming style</pre> 223 1 warning <code>fixme</code> W0511 <pre>TODO: make this function work properly</pre> 224 0 convention <code>missing-function-docstring</code> C0116 coarsen_quadkey_to_partition_size <pre>Missing function or method docstring</pre> 231 8 convention <code>invalid-name</code> C0103 coarsen_quadkey_to_partition_size <pre>Variable name \"windowSpec\" doesn't conform to snake_case naming style</pre> 309 0 convention <code>line-too-long</code> C0301 <pre>Line too long (121/120)</pre> 350 0 convention <code>line-too-long</code> C0301 <pre>Line too long (122/120)</pre> 355 0 convention <code>line-too-long</code> C0301 <pre>Line too long (122/120)</pre> 379 0 convention <code>line-too-long</code> C0301 <pre>Line too long (122/120)</pre> 384 0 convention <code>line-too-long</code> C0301 <pre>Line too long (122/120)</pre> 477 0 convention <code>line-too-long</code> C0301 <pre>Line too long (126/120)</pre> Module <code>main</code> (<code>multimno/main.py</code>) Line Col. Type Symbol ID Obj Message 75 0 warning <code>reimported</code> W0404 <pre>Reimport 'UsualEnvironmentLabeling' (imported line 73)</pre> 99 15 warning <code>duplicate-key</code> W0109 <pre>Duplicate key 'PresentPopulationEstimation.COMPONENT_ID' in dictionary</pre> 138 10 warning <code>redefined-outer-name</code> W0621 build <pre>Redefining name 'component_id' from outer scope (line 162)</pre> 138 29 warning <code>redefined-outer-name</code> W0621 build <pre>Redefining name 'general_config_path' from outer scope (line 163)</pre> 138 55 warning <code>redefined-outer-name</code> W0621 build <pre>Redefining name 'component_config_path' from outer scope (line 164)</pre> Metrics Count per types Name Count warning 245 convention 199 error 36 refactor 209 Count per messages Name Count reimported 2 duplicate-key 1 redefined-outer-name 3 empty-docstring 26 no-member 30 consider-using-dict-items 2 consider-iterating-dictionary 4 use-dict-literal 7 simplifiable-if-statement 1 too-many-arguments 20 wrong-import-order 8 line-too-long 119 fixme 63 missing-module-docstring 4 too-many-instance-attributes 21 too-many-locals 14 possibly-used-before-assignment 3 too-many-branches 6 too-many-statements 5 missing-function-docstring 9 invalid-name 18 no-else-return 9 inconsistent-return-statements 1 too-many-public-methods 3 attribute-defined-outside-init 20 unused-import 28 consider-using-min-builtin 1 singleton-comparison 3 consider-using-from-import 6 ungrouped-imports 2 invalid-unary-operand-type 1 unnecessary-lambda 1 f-string-without-interpolation 6 assignment-from-no-return 1 unnecessary-comprehension 2 too-many-return-statements 1 useless-parent-delegation 1 too-few-public-methods 12 redefined-builtin 1 missing-class-docstring 4 arguments-renamed 2 import-error 1 arguments-differ 114 raise-missing-from 1 broad-exception-raised 2 duplicate-code 100 Count per modules Name Count main 5 components.quality.semantic_quality_warnings.semantic_quality_warnings 14 components.quality.event_quality_warnings.event_quality_warnings 36 components.quality.network_quality_warnings.network_quality_warnings 59 components.execution.signal_strength.signal_stength_modeling 12 components.execution.event_semantic_cleaning.event_semantic_cleaning 9 components.execution.daily_permanence_score.daily_permanence_score 5 components.execution.usual_environment_aggregation.usual_environment_aggregation 14 components.execution.network_cleaning.network_cleaning 13 components.execution.event_cleaning.event_cleaning 14 components.execution.cell_connection_probability.cell_connection_probability 2 components.execution.cell_footprint.cell_footprint_estimation 3 components.execution.longterm_permanence_score.longterm_permanence_score 4 components.execution.geozones_grid_mapping.geozones_grid_mapping 9 components.execution.present_population.present_population_estimation 10 components.execution.grid_enrichment.grid_enrichment 26 components.execution.midterm_permanence_score.midterm_permanence_score 12 components.execution.time_segments.continuous_time_segmentation 25 components.execution.usual_environment_labeling.usual_environment_labeling 5 components.execution.device_activity_statistics.device_activity_statistics 6 components.ingestion.spatial_data_ingestion.gisco_data_ingestion 11 components.ingestion.spatial_data_ingestion.overture_data_ingestion 41 components.ingestion.synthetic.synthetic_events 21 components.ingestion.synthetic.synthetic_diaries 13 components.ingestion.synthetic.synthetic_network 7 components.ingestion.grid_generation.inspire_grid_generation 8 core.log 3 core.grid 6 core.utils 17 core.io_interface 19 core.data_objects.bronze.bronze_holiday_calendar_data_object 2 core.data_objects.bronze.bronze_geographic_zones_data_object 5 core.data_objects.bronze.bronze_transportation_data_object 5 core.data_objects.bronze.bronze_network_physical_data_object 2 core.data_objects.bronze.bronze_landuse_data_object 6 core.data_objects.bronze.bronze_synthetic_diaries_data_object 2 core.data_objects.bronze.bronze_event_data_object 2 core.data_objects.bronze.bronze_admin_units_data_object 5 core.data_objects.bronze.bronze_countries_data_object 5 core.data_objects.silver.silver_cell_connection_probabilities_data_object 3 core.data_objects.silver.silver_daily_permanence_score_data_object 2 core.data_objects.silver.silver_present_population_zone_data_object 2 core.data_objects.silver.silver_geozones_grid_map_data_object 5 core.data_objects.silver.silver_cell_intersection_groups_data_object 4 core.data_objects.silver.silver_present_population_data_object 2 core.data_objects.silver.silver_signal_strength_data_object 5 core.data_objects.silver.silver_event_data_syntactic_quality_metrics_frequency_distribution 2 core.data_objects.silver.silver_semantic_quality_warnings_log_table 2 core.data_objects.silver.silver_aggregated_usual_environments_data_object 2 core.data_objects.silver.silver_time_segments_data_object 3 core.data_objects.silver.silver_grid_data_object 7 core.data_objects.silver.silver_usual_environment_labeling_quality_metrics_data_object 2 core.data_objects.silver.silver_semantic_quality_metrics 4 core.data_objects.silver.silver_network_syntactic_quality_warnings_log_table 3 core.data_objects.silver.silver_longterm_permanence_score_data_object 2 core.data_objects.silver.silver_network_syntactic_quality_warnings_plot_data 4 core.data_objects.silver.silver_event_data_syntactic_quality_warnings_log_table 2 core.data_objects.silver.silver_midterm_permanence_score_data_object 2 core.data_objects.silver.silver_event_data_syntactic_quality_metrics_by_column 2 core.data_objects.silver.silver_network_data_top_frequent_errors_data_object 2 core.data_objects.silver.silver_network_data_object 2 core.data_objects.silver.silver_event_flagged_data_object 2 core.data_objects.silver.silver_network_row_error_metrics 2 core.data_objects.silver.silver_semantic_quality_warnings_plot_data 2 core.data_objects.silver.silver_event_data_object 2 core.data_objects.silver.silver_cell_footprint_data_object 5 core.data_objects.silver.silver_event_data_syntactic_quality_warnings_for_plots 3 core.data_objects.silver.silver_device_activity_statistics 2 core.data_objects.silver.silver_usual_environment_labels_data_object 3 core.data_objects.silver.silver_enriched_grid_data_object 6 core.data_objects.silver.silver_network_data_syntactic_quality_metrics_by_column 2 core.data_objects.landing.landing_http_geojson_data_object 3 core.data_objects.landing.landing_geoparquet_data_object 7 core.constants.columns 1 core.constants.warnings 9 core.constants.conditions 6 core.constants.measure_definitions 3 core.constants.error_types 3 core.constants.transformations 101 Count per path Name Count multimno/main.py 5 multimno/components/quality/semantic_quality_warnings/semantic_quality_warnings.py 14 multimno/components/quality/event_quality_warnings/event_quality_warnings.py 36 multimno/components/quality/network_quality_warnings/network_quality_warnings.py 59 multimno/components/execution/signal_strength/signal_stength_modeling.py 12 multimno/components/execution/event_semantic_cleaning/event_semantic_cleaning.py 9 multimno/components/execution/daily_permanence_score/daily_permanence_score.py 5 multimno/components/execution/usual_environment_aggregation/usual_environment_aggregation.py 14 multimno/components/execution/network_cleaning/network_cleaning.py 13 multimno/components/execution/event_cleaning/event_cleaning.py 14 multimno/components/execution/cell_connection_probability/cell_connection_probability.py 2 multimno/components/execution/cell_footprint/cell_footprint_estimation.py 3 multimno/components/execution/longterm_permanence_score/longterm_permanence_score.py 4 multimno/components/execution/geozones_grid_mapping/geozones_grid_mapping.py 9 multimno/components/execution/present_population/present_population_estimation.py 10 multimno/components/execution/grid_enrichment/grid_enrichment.py 26 multimno/components/execution/midterm_permanence_score/midterm_permanence_score.py 12 multimno/components/execution/time_segments/continuous_time_segmentation.py 25 multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py 5 multimno/components/execution/device_activity_statistics/device_activity_statistics.py 6 multimno/components/ingestion/spatial_data_ingestion/gisco_data_ingestion.py 11 multimno/components/ingestion/spatial_data_ingestion/overture_data_ingestion.py 41 multimno/components/ingestion/synthetic/synthetic_events.py 21 multimno/components/ingestion/synthetic/synthetic_diaries.py 13 multimno/components/ingestion/synthetic/synthetic_network.py 7 multimno/components/ingestion/grid_generation/inspire_grid_generation.py 8 multimno/core/log.py 3 multimno/core/grid.py 6 multimno/core/utils.py 17 multimno/core/io_interface.py 19 multimno/core/data_objects/bronze/bronze_holiday_calendar_data_object.py 2 multimno/core/data_objects/bronze/bronze_geographic_zones_data_object.py 5 multimno/core/data_objects/bronze/bronze_transportation_data_object.py 5 multimno/core/data_objects/bronze/bronze_network_physical_data_object.py 2 multimno/core/data_objects/bronze/bronze_landuse_data_object.py 6 multimno/core/data_objects/bronze/bronze_synthetic_diaries_data_object.py 2 multimno/core/data_objects/bronze/bronze_event_data_object.py 2 multimno/core/data_objects/bronze/bronze_admin_units_data_object.py 5 multimno/core/data_objects/bronze/bronze_countries_data_object.py 5 multimno/core/data_objects/silver/silver_cell_connection_probabilities_data_object.py 3 multimno/core/data_objects/silver/silver_daily_permanence_score_data_object.py 2 multimno/core/data_objects/silver/silver_present_population_zone_data_object.py 2 multimno/core/data_objects/silver/silver_geozones_grid_map_data_object.py 5 multimno/core/data_objects/silver/silver_cell_intersection_groups_data_object.py 4 multimno/core/data_objects/silver/silver_present_population_data_object.py 2 multimno/core/data_objects/silver/silver_signal_strength_data_object.py 5 multimno/core/data_objects/silver/silver_event_data_syntactic_quality_metrics_frequency_distribution.py 2 multimno/core/data_objects/silver/silver_semantic_quality_warnings_log_table.py 2 multimno/core/data_objects/silver/silver_aggregated_usual_environments_data_object.py 2 multimno/core/data_objects/silver/silver_time_segments_data_object.py 3 multimno/core/data_objects/silver/silver_grid_data_object.py 7 multimno/core/data_objects/silver/silver_usual_environment_labeling_quality_metrics_data_object.py 2 multimno/core/data_objects/silver/silver_semantic_quality_metrics.py 4 multimno/core/data_objects/silver/silver_network_syntactic_quality_warnings_log_table.py 3 multimno/core/data_objects/silver/silver_longterm_permanence_score_data_object.py 2 multimno/core/data_objects/silver/silver_network_syntactic_quality_warnings_plot_data.py 4 multimno/core/data_objects/silver/silver_event_data_syntactic_quality_warnings_log_table.py 2 multimno/core/data_objects/silver/silver_midterm_permanence_score_data_object.py 2 multimno/core/data_objects/silver/silver_event_data_syntactic_quality_metrics_by_column.py 2 multimno/core/data_objects/silver/silver_network_data_top_frequent_errors_data_object.py 2 multimno/core/data_objects/silver/silver_network_data_object.py 2 multimno/core/data_objects/silver/silver_event_flagged_data_object.py 2 multimno/core/data_objects/silver/silver_network_row_error_metrics.py 2 multimno/core/data_objects/silver/silver_semantic_quality_warnings_plot_data.py 2 multimno/core/data_objects/silver/silver_event_data_object.py 2 multimno/core/data_objects/silver/silver_cell_footprint_data_object.py 5 multimno/core/data_objects/silver/silver_event_data_syntactic_quality_warnings_for_plots.py 3 multimno/core/data_objects/silver/silver_device_activity_statistics.py 2 multimno/core/data_objects/silver/silver_usual_environment_labels_data_object.py 3 multimno/core/data_objects/silver/silver_enriched_grid_data_object.py 6 multimno/core/data_objects/silver/silver_network_data_syntactic_quality_metrics_by_column.py 2 multimno/core/data_objects/landing/landing_http_geojson_data_object.py 3 multimno/core/data_objects/landing/landing_geoparquet_data_object.py 7 multimno/core/constants/columns.py 1 multimno/core/constants/warnings.py 9 multimno/core/constants/conditions.py 6 multimno/core/constants/measure_definitions.py 3 multimno/core/constants/error_types.py 3 multimno/core/constants/transformations.py 101"},{"location":"autodoc/test_report/","title":"Test Report","text":"test_report.md"},{"location":"autodoc/test_report/#title","title":"test_report.md","text":"<p>Report generated on 02-Aug-2024 at 16:31:05 by pytest-html         v4.1.1</p> Environment No results found. Check the filters. &lt; &gt; Summary <p>37 tests took 00:05:60.</p> <p>(Un)check the boxes to filter the results.</p> There are still tests running. Reload this page to get the latest results! 0 Failed, 37 Passed, 3 Skipped, 0 Expected failures, 0 Unexpected passes, 0 Errors, 0 Reruns Show all details\u00a0/\u00a0Hide all details Result Test Duration Links"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>components<ul> <li>execution<ul> <li>cell_connection_probability<ul> <li>cell_connection_probability</li> </ul> </li> <li>cell_footprint<ul> <li>cell_footprint_estimation</li> </ul> </li> <li>daily_permanence_score<ul> <li>daily_permanence_score</li> </ul> </li> <li>device_activity_statistics<ul> <li>device_activity_statistics</li> </ul> </li> <li>event_cleaning<ul> <li>event_cleaning</li> </ul> </li> <li>event_semantic_cleaning<ul> <li>event_semantic_cleaning</li> </ul> </li> <li>geozones_grid_mapping<ul> <li>geozones_grid_mapping</li> </ul> </li> <li>grid_enrichment<ul> <li>grid_enrichment</li> </ul> </li> <li>longterm_permanence_score<ul> <li>longterm_permanence_score</li> </ul> </li> <li>midterm_permanence_score<ul> <li>midterm_permanence_score</li> </ul> </li> <li>network_cleaning<ul> <li>network_cleaning</li> </ul> </li> <li>present_population<ul> <li>present_population_estimation</li> </ul> </li> <li>signal_strength<ul> <li>signal_stength_modeling</li> </ul> </li> <li>time_segments<ul> <li>continuous_time_segmentation</li> </ul> </li> <li>usual_environment_aggregation<ul> <li>usual_environment_aggregation</li> </ul> </li> <li>usual_environment_labeling<ul> <li>usual_environment_labeling</li> </ul> </li> </ul> </li> <li>ingestion<ul> <li>grid_generation<ul> <li>inspire_grid_generation</li> </ul> </li> <li>spatial_data_ingestion<ul> <li>gisco_data_ingestion</li> <li>overture_data_ingestion</li> </ul> </li> <li>synthetic<ul> <li>synthetic_diaries</li> <li>synthetic_events</li> <li>synthetic_network</li> </ul> </li> </ul> </li> <li>quality<ul> <li>event_quality_warnings<ul> <li>event_quality_warnings</li> </ul> </li> <li>network_quality_warnings<ul> <li>network_quality_warnings</li> </ul> </li> <li>semantic_quality_warnings<ul> <li>semantic_quality_warnings</li> </ul> </li> </ul> </li> </ul> </li> <li>core<ul> <li>component</li> <li>configuration</li> <li>constants<ul> <li>columns</li> <li>conditions</li> <li>error_types</li> <li>measure_definitions</li> <li>network_default_thresholds</li> <li>period_names</li> <li>semantic_qw_default_thresholds</li> <li>transformations</li> <li>warnings</li> </ul> </li> <li>data_objects<ul> <li>bronze<ul> <li>bronze_admin_units_data_object</li> <li>bronze_countries_data_object</li> <li>bronze_event_data_object</li> <li>bronze_geographic_zones_data_object</li> <li>bronze_holiday_calendar_data_object</li> <li>bronze_landuse_data_object</li> <li>bronze_network_physical_data_object</li> <li>bronze_synthetic_diaries_data_object</li> <li>bronze_transportation_data_object</li> </ul> </li> <li>data_object</li> <li>landing<ul> <li>landing_geoparquet_data_object</li> <li>landing_http_geojson_data_object</li> </ul> </li> <li>silver<ul> <li>silver_aggregated_usual_environments_data_object</li> <li>silver_cell_connection_probabilities_data_object</li> <li>silver_cell_footprint_data_object</li> <li>silver_cell_intersection_groups_data_object</li> <li>silver_daily_permanence_score_data_object</li> <li>silver_device_activity_statistics</li> <li>silver_enriched_grid_data_object</li> <li>silver_event_data_object</li> <li>silver_event_data_syntactic_quality_metrics_by_column</li> <li>silver_event_data_syntactic_quality_metrics_frequency_distribution</li> <li>silver_event_data_syntactic_quality_warnings_for_plots</li> <li>silver_event_data_syntactic_quality_warnings_log_table</li> <li>silver_event_flagged_data_object</li> <li>silver_geozones_grid_map_data_object</li> <li>silver_grid_data_object</li> <li>silver_longterm_permanence_score_data_object</li> <li>silver_midterm_permanence_score_data_object</li> <li>silver_network_data_object</li> <li>silver_network_data_syntactic_quality_metrics_by_column</li> <li>silver_network_data_top_frequent_errors_data_object</li> <li>silver_network_row_error_metrics</li> <li>silver_network_syntactic_quality_warnings_log_table</li> <li>silver_network_syntactic_quality_warnings_plot_data</li> <li>silver_present_population_data_object</li> <li>silver_present_population_zone_data_object</li> <li>silver_semantic_quality_metrics</li> <li>silver_semantic_quality_warnings_log_table</li> <li>silver_semantic_quality_warnings_plot_data</li> <li>silver_signal_strength_data_object</li> <li>silver_time_segments_data_object</li> <li>silver_usual_environment_labeling_quality_metrics_data_object</li> <li>silver_usual_environment_labels_data_object</li> </ul> </li> </ul> </li> <li>grid</li> <li>io_interface</li> <li>log</li> <li>settings</li> <li>spark_session</li> <li>utils</li> </ul> </li> <li>main</li> </ul>"},{"location":"reference/main/","title":"main","text":"<p>Application entrypoint for launching a single component.</p> <p>Usage:</p> <pre><code>python multimno/main.py &lt;component_id&gt; &lt;general_config_path&gt; &lt;component_config_path&gt;\n</code></pre> <ul> <li>component_id: ID of the component to be executed.</li> <li>general_config_path: Path to a INI file with the general configuration of the execution.</li> <li>component_config_path: Path to a INI file with the specific configuration of the component.</li> </ul>"},{"location":"reference/main/#main.build","title":"<code>build(component_id, general_config_path, component_config_path)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>component_id</code> <code>str</code> <p>id of the component</p> required <code>general_config_path</code> <code>str</code> <p>general config path</p> required <code>component_config_path</code> <code>str</code> <p>component config path</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the component_id is not supported.</p> <p>Returns:</p> Type Description <code>Component</code> <p>Component constructor.</p> Source code in <code>multimno/main.py</code> <pre><code>def build(component_id: str, general_config_path: str, component_config_path: str):\n    \"\"\"\n\n\n    Args:\n        component_id (str): id of the component\n        general_config_path (str): general config path\n        component_config_path (str): component config path\n\n    Raises:\n        ValueError: If the component_id is not supported.\n\n    Returns:\n        (multimno.core.component.Component): Component constructor.\n    \"\"\"\n    try:\n        constructor = CONSTRUCTORS[component_id]\n    except KeyError as e:\n        raise ValueError(f\"Component {component_id} is not supported.\") from e\n\n    return constructor(general_config_path, component_config_path)\n</code></pre>"},{"location":"reference/components/","title":"components","text":""},{"location":"reference/components/execution/","title":"execution","text":""},{"location":"reference/components/execution/cell_connection_probability/","title":"cell_connection_probability","text":""},{"location":"reference/components/execution/cell_connection_probability/cell_connection_probability/","title":"cell_connection_probability","text":"<p>Module that calculates cell connection probabilities and posterior probabilities.</p>"},{"location":"reference/components/execution/cell_connection_probability/cell_connection_probability/#components.execution.cell_connection_probability.cell_connection_probability.CellConnectionProbabilityEstimation","title":"<code>CellConnectionProbabilityEstimation</code>","text":"<p>               Bases: <code>Component</code></p> <p>Estimates the cell connection probabilities and posterior probabilities for each grid tile. Cell connection probabilities are calculated based on footprint per grid. Posterior probabilities are calculated based on the cell connection probabilities and grid prior probabilities.</p> <p>This class reads in cell footprint estimation and the grid model wit prior probabilities. The output is a DataFrame that represents cell connection probabilities and  posterior probabilities for each cell and grid id combination for a given date.</p> Source code in <code>multimno/components/execution/cell_connection_probability/cell_connection_probability.py</code> <pre><code>class CellConnectionProbabilityEstimation(Component):\n    \"\"\"\n    Estimates the cell connection probabilities and posterior probabilities for each grid tile.\n    Cell connection probabilities are calculated based on footprint per grid.\n    Posterior probabilities are calculated based on the cell connection probabilities\n    and grid prior probabilities.\n\n    This class reads in cell footprint estimation and the grid model wit prior probabilities.\n    The output is a DataFrame that represents cell connection probabilities and\n     posterior probabilities for each cell and grid id combination for a given date.\n    \"\"\"\n\n    COMPONENT_ID = \"CellConnectionProbabilityEstimation\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.data_period_start = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n\n        self.data_period_end = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n\n        self.data_period_dates = [\n            self.data_period_start + datetime.timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n    def initalize_data_objects(self):\n\n        # Input\n        self.input_data_objects = {}\n        self.use_land_use_prior = self.config.getboolean(self.COMPONENT_ID, \"use_land_use_prior\")\n\n        inputs = {\n            \"cell_footprint_data_silver\": SilverCellFootprintDataObject,\n        }\n\n        if self.use_land_use_prior:\n            inputs[\"enriched_grid_data_silver\"] = SilverEnrichedGridDataObject\n\n        for key, value in inputs.items():\n            path = self.config.get(CONFIG_SILVER_PATHS_KEY, key)\n            if check_if_data_path_exists(self.spark, path):\n                self.input_data_objects[value.ID] = value(self.spark, path)\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")\n                raise ValueError(f\"Invalid path for {value.ID} in component {self.COMPONENT_ID} initialization\")\n\n        # Output\n        self.output_data_objects = {}\n        self.silver_cell_footprint_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"cell_connection_probabilities_data_silver\"\n        )\n        self.output_data_objects[SilverCellConnectionProbabilitiesDataObject.ID] = (\n            SilverCellConnectionProbabilitiesDataObject(\n                self.spark,\n                self.silver_cell_footprint_path,\n                partition_columns=[ColNames.year, ColNames.month, ColNames.day],\n            )\n        )\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n\n        # TODO: We might need to iterate over dates and process the data for each date separately\n        # have to do tests with larger datasets.\n\n        cell_footprint_df = self.input_data_objects[SilverCellFootprintDataObject.ID].df.filter(\n            F.make_date(\"year\", \"month\", \"day\").isin(self.data_period_dates)\n        )\n\n        # Calculate the cell connection probabilities\n\n        grid_footprint_sums = cell_footprint_df.groupBy(\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            ColNames.valid_date_start,\n            ColNames.valid_date_end,\n            ColNames.grid_id,\n        ).agg(F.sum(ColNames.signal_dominance).alias(\"total_grid_footprint\"))\n\n        cell_footprint_df = cell_footprint_df.join(\n            grid_footprint_sums,\n            on=[\n                ColNames.year,\n                ColNames.month,\n                ColNames.day,\n                ColNames.valid_date_start,\n                ColNames.valid_date_end,\n                ColNames.grid_id,\n            ],\n            how=\"left\",\n        )\n\n        cell_conn_probs_df = cell_footprint_df.withColumn(\n            ColNames.cell_connection_probability,\n            F.col(ColNames.signal_dominance) / F.col(\"total_grid_footprint\"),\n        ).drop(\"total_grid_footprint\")\n\n        # Calculate the posterior probabilities\n\n        if self.use_land_use_prior:\n            grid_model_df = self.input_data_objects[SilverEnrichedGridDataObject.ID].df.select(\n                ColNames.grid_id, ColNames.prior_probability\n            )\n\n            # TODO should default value be configurable? 1 would keep cell connection probability value\n            # Assign 0 to any missing prior values\n            grid_model_df = grid_model_df.fillna(0, subset=[ColNames.prior_probability])\n            cell_conn_probs_df = cell_conn_probs_df.join(grid_model_df, on=ColNames.grid_id)\n            cell_conn_probs_df = cell_conn_probs_df.withColumn(\n                ColNames.posterior_probability,\n                F.col(ColNames.cell_connection_probability) * F.col(ColNames.prior_probability),\n            )\n\n        elif not self.use_land_use_prior:\n            cell_conn_probs_df = cell_conn_probs_df.withColumn(\n                ColNames.posterior_probability,\n                F.col(ColNames.cell_connection_probability),\n            )\n\n        # Normalize the posterior probabilities\n\n        total_posterior_df = cell_conn_probs_df.groupBy(\n            ColNames.year, ColNames.month, ColNames.day, ColNames.cell_id\n        ).agg(F.sum(ColNames.posterior_probability).alias(\"total_posterior_probability\"))\n\n        cell_conn_probs_df = (\n            cell_conn_probs_df.join(\n                total_posterior_df,\n                on=[ColNames.year, ColNames.month, ColNames.day, ColNames.cell_id],\n            )\n            .withColumn(\n                ColNames.posterior_probability,\n                F.col(ColNames.posterior_probability) / F.col(\"total_posterior_probability\"),\n            )\n            .drop(\"total_posterior_probability\")\n        )\n\n        cell_conn_probs_df = cell_conn_probs_df.select(\n            *[field.name for field in SilverCellConnectionProbabilitiesDataObject.SCHEMA.fields]\n        )\n        columns = {\n            field.name: F.col(field.name).cast(field.dataType)\n            for field in SilverCellConnectionProbabilitiesDataObject.SCHEMA.fields\n        }\n        cell_conn_probs_df = cell_conn_probs_df.withColumns(columns)\n\n        self.output_data_objects[SilverCellConnectionProbabilitiesDataObject.ID].df = cell_conn_probs_df\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/","title":"cell_footprint","text":""},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/","title":"cell_footprint_estimation","text":"<p>Module that cleans RAW MNO Event data.</p>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation","title":"<code>CellFootprintEstimation</code>","text":"<p>               Bases: <code>Component</code></p> <p>Estimates the footprint of each cell per grid tile in its coverage area from its signal strength.</p> <p>This class reads in signal strength data, calculates the signal dominance of each cell, and prunes the data based on configurable thresholds. The output is a DataFrame that represents the footprint of each cell.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>class CellFootprintEstimation(Component):\n    \"\"\"\n    Estimates the footprint of each cell per grid tile in its coverage area from its signal strength.\n\n    This class reads in signal strength data, calculates the signal dominance of each cell, and prunes the data based on\n    configurable thresholds. The output is a DataFrame that represents the footprint of each cell.\n    \"\"\"\n\n    COMPONENT_ID = \"CellFootprintEstimation\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.logistic_function_steepness = self.config.getfloat(self.COMPONENT_ID, \"logistic_function_steepness\")\n        self.logistic_function_midpoint = self.config.getfloat(self.COMPONENT_ID, \"logistic_function_midpoint\")\n        self.signal_dominance_treshold = self.config.getfloat(self.COMPONENT_ID, \"signal_dominance_treshold\")\n        self.max_cells_per_grid_tile = self.config.getfloat(self.COMPONENT_ID, \"max_cells_per_grid_tile\")\n        self.difference_from_best_sd_treshold = self.config.getfloat(\n            self.COMPONENT_ID, \"difference_from_best_sd_treshold\"\n        )\n        self.do_sd_treshold_prunning = self.config.getboolean(self.COMPONENT_ID, \"do_sd_treshold_prunning\")\n        self.do_max_cells_per_tile_prunning = self.config.getboolean(\n            self.COMPONENT_ID, \"do_max_cells_per_tile_prunning\"\n        )\n        self.do_difference_from_best_sd_prunning = self.config.getboolean(\n            self.COMPONENT_ID, \"do_difference_from_best_sd_prunning\"\n        )\n\n        self.data_period_start = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n\n        self.data_period_end = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n\n        self.data_period_dates = [\n            self.data_period_start + datetime.timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n    def initalize_data_objects(self):\n\n        self.do_do_cell_intersection_groups_calculation = self.config.getboolean(\n            self.COMPONENT_ID, \"do_cell_intersection_groups_calculation\"\n        )\n        # Input\n        self.input_data_objects = {}\n\n        signal_strength = self.config.get(CONFIG_SILVER_PATHS_KEY, \"signal_strength_data_silver\")\n        if check_if_data_path_exists(self.spark, signal_strength):\n            self.input_data_objects[SilverSignalStrengthDataObject.ID] = SilverSignalStrengthDataObject(\n                self.spark, signal_strength\n            )\n        else:\n            self.logger.warning(f\"Expected path {signal_strength} to exist but it does not\")\n            raise ValueError(f\"Invalid path for {signal_strength} {CellFootprintEstimation.COMPONENT_ID}\")\n\n        # Output\n        self.output_data_objects = {}\n        self.silver_cell_footprint_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"cell_footprint_data_silver\")\n        self.output_data_objects[SilverCellFootprintDataObject.ID] = SilverCellFootprintDataObject(\n            self.spark,\n            self.silver_cell_footprint_path,\n            partition_columns=[ColNames.year, ColNames.month, ColNames.day],\n        )\n\n        if self.do_do_cell_intersection_groups_calculation:\n            self.silver_cell_intersection_groups_path = self.config.get(\n                CONFIG_SILVER_PATHS_KEY, \"cell_intersection_groups_data_silver\"\n            )\n            self.output_data_objects[SilverCellIntersectionGroupsDataObject.ID] = (\n                SilverCellIntersectionGroupsDataObject(\n                    self.spark,\n                    self.silver_cell_intersection_groups_path,\n                    partition_columns=[ColNames.year, ColNames.month, ColNames.day],\n                )\n            )\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n\n        # TODO: We might need to iterate over dates and process the data for each date separately\n        # have to do tests with larger datasets.\n\n        current_signal_strength_sdf = self.input_data_objects[SilverSignalStrengthDataObject.ID].df.filter(\n            F.make_date(\"year\", \"month\", \"day\").isin(self.data_period_dates)\n        )\n\n        # Calculate the cell signal dominance\n        cell_footprint_sdf = self.signal_strength_to_signal_dominance(\n            current_signal_strength_sdf,\n            self.logistic_function_steepness,\n            self.logistic_function_midpoint,\n        )\n\n        # Prune max cells per grid tile\n        if self.do_max_cells_per_tile_prunning:\n            cell_footprint_sdf = self.prune_max_cells_per_grid_tile(cell_footprint_sdf, self.max_cells_per_grid_tile)\n\n        # Prune signal dominance difference from best\n        if self.do_difference_from_best_sd_prunning:\n            cell_footprint_sdf = self.prune_signal_difference_from_best(\n                cell_footprint_sdf, self.difference_from_best_sd_treshold\n            )\n\n        # Prune small signal dominance values\n        if self.do_sd_treshold_prunning:\n            cell_footprint_sdf = self.prune_small_signal_dominance(cell_footprint_sdf, self.signal_dominance_treshold)\n\n        cell_footprint_sdf = cell_footprint_sdf.select(SilverCellFootprintDataObject.MANDATORY_COLUMNS)\n        columns = {\n            field.name: F.col(field.name).cast(field.dataType) for field in SilverCellFootprintDataObject.SCHEMA.fields\n        }\n\n        cell_footprint_sdf = cell_footprint_sdf.withColumns(columns)\n        self.output_data_objects[SilverCellFootprintDataObject.ID].df = cell_footprint_sdf\n\n        if self.do_do_cell_intersection_groups_calculation:\n\n            cell_intersection_groups_sdf = self.calculate_intersection_groups(cell_footprint_sdf)\n\n            cell_intersection_groups_sdf = cell_intersection_groups_sdf.persist(StorageLevel.MEMORY_AND_DISK)\n            cell_intersection_groups_sdf.count()\n\n            cell_intersection_groups_sdf = self.calculate_all_intersection_combinations(cell_intersection_groups_sdf)\n\n            cell_intersection_groups_sdf = cell_intersection_groups_sdf.select(\n                SilverCellIntersectionGroupsDataObject.MANDATORY_COLUMNS\n            )\n            columns = {\n                field.name: F.col(field.name).cast(field.dataType)\n                for field in SilverCellIntersectionGroupsDataObject.SCHEMA.fields\n            }\n            cell_intersection_groups_sdf = cell_intersection_groups_sdf.withColumns(columns)\n            self.output_data_objects[SilverCellIntersectionGroupsDataObject.ID].df = cell_intersection_groups_sdf\n\n    @staticmethod\n    def signal_strength_to_signal_dominance(\n        sdf: DataFrame,\n        logistic_function_steepness: float,\n        logistic_function_midpoint: float,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Converts signal strength to signal dominance using a logistic function.\n        Methodology from A Bayesian approach to location estimation of mobile devices\n        from mobile network operator data. Tennekes and Gootzen (2022).\n\n        The logistic function is defined as 1 / (1 + exp(-scale)),\n        where scale is (signal_strength - logistic_function_midpoint) * logistic_function_steepness.\n\n        Parameters:\n        sdf (DataFrame): SignalStrenghtDataObject Spark DataFrame.\n        logistic_function_steepness (float): The steepness parameter for the logistic function.\n        logistic_function_midpoint (float): The midpoint parameter for the logistic function.\n\n        Returns:\n        DataFrame: A Spark DataFrame with the signal dominance added as a new column.\n        \"\"\"\n        sdf = sdf.withColumn(\n            \"scale\",\n            (F.col(ColNames.signal_strength) - logistic_function_midpoint) * logistic_function_steepness,\n        )\n        sdf = sdf.withColumn(ColNames.signal_dominance, 1 / (1 + F.exp(-F.col(\"scale\"))))\n        sdf = sdf.drop(\"scale\")\n\n        return sdf\n\n    @staticmethod\n    def prune_small_signal_dominance(sdf: DataFrame, signal_dominance_threshold: float) -&gt; DataFrame:\n        \"\"\"\n        Prunes rows from the DataFrame where the signal dominance is less than or equal to the provided threshold.\n\n        Parameters:\n        sdf (DataFrame): A Spark DataFrame containing the signal dominance data.\n        signal_dominance_threshold (float): The threshold for pruning small signal dominance values.\n\n        Returns:\n        DataFrame: A DataFrame with rows having signal dominance less than or equal to the threshold removed.\n        \"\"\"\n        sdf = sdf.filter(F.col(ColNames.signal_dominance) &gt; signal_dominance_threshold)\n        return sdf\n\n    @staticmethod\n    def prune_max_cells_per_grid_tile(sdf: DataFrame, max_cells_per_grid_tile: int) -&gt; DataFrame:\n        \"\"\"\n        Prunes rows from the DataFrame exceeding the maximum number of cells allowed per grid tile.\n\n        The rows are ordered by signal dominance in descending order,\n        and only the top 'max_cells_per_grid_tile' rows are kept for each grid tile.\n\n        Parameters:\n        sdf (DataFrame): A Spark DataFrame containing the signal dominance data.\n        max_cells_per_grid_tile (int): The maximum number of cells allowed per grid tile.\n\n        Returns:\n        DataFrame: A DataFrame with rows exceeding the maximum number of cells per grid tile removed.\n        \"\"\"\n        window = Window.partitionBy(ColNames.year, ColNames.month, ColNames.day, ColNames.grid_id).orderBy(\n            F.desc(ColNames.signal_dominance)\n        )\n\n        sdf = sdf.withColumn(\"row_number\", F.row_number().over(window))\n        sdf = sdf.filter(F.col(\"row_number\") &lt;= max_cells_per_grid_tile)\n        sdf = sdf.drop(\"row_number\")\n\n        return sdf\n\n    @staticmethod\n    def prune_signal_difference_from_best(sdf: DataFrame, difference_threshold: float) -&gt; DataFrame:\n        \"\"\"\n        Prunes rows from the DataFrame based on a threshold of signal dominance difference.\n\n        The rows are ordered by signal dominance in descending order, and only the rows where the difference\n        in signal dominance from the maximum is less than the threshold are kept for each grid tile.\n\n        Parameters:\n        sdf (DataFrame): A Spark DataFrame containing the signal dominance data.\n        threshold (float): The threshold for signal dominance difference in percentage.\n\n        Returns:\n        DataFrame: A DataFrame with rows pruned based on the signal dominance difference threshold.\n        \"\"\"\n        window = Window.partitionBy(ColNames.year, ColNames.month, ColNames.day, ColNames.grid_id).orderBy(\n            F.desc(ColNames.signal_dominance)\n        )\n\n        sdf = sdf.withColumn(\"row_number\", F.row_number().over(window))\n\n        sdf = sdf.withColumn(\"max_signal_dominance\", F.first(ColNames.signal_dominance).over(window))\n        sdf = sdf.withColumn(\n            \"signal_dominance_diff_percentage\",\n            (sdf[\"max_signal_dominance\"] - sdf[ColNames.signal_dominance]) / sdf[\"max_signal_dominance\"] * 100,\n        )\n\n        sdf = sdf.filter(\n            (F.col(\"row_number\") == 1) | (F.col(\"signal_dominance_diff_percentage\") &lt;= difference_threshold)\n        )\n\n        sdf = sdf.drop(\"row_number\", \"max_signal_dominance\", \"signal_dominance_diff_percentage\")\n\n        return sdf\n\n    @staticmethod\n    def calculate_intersection_groups(sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Calculates the cell intersection groups based on cell footprints overlaps\n        over grid tiles.\n\n        Parameters:\n        sdf (DataFrame): A Spark DataFrame containing the signal dominance data.\n\n        Returns:\n        DataFrame: A DataFrame with the intersection groups.\n        \"\"\"\n        intersections_sdf = (\n            sdf.groupBy(\n                F.col(ColNames.year),\n                F.col(ColNames.month),\n                F.col(ColNames.day),\n                F.col(ColNames.grid_id),\n            )\n            .agg(F.array_sort(F.collect_set(ColNames.cell_id)).alias(ColNames.cells))\n            .select(ColNames.cells, ColNames.year, ColNames.month, ColNames.day)\n        )\n\n        intersections_sdf = intersections_sdf.drop_duplicates(\n            [ColNames.year, ColNames.month, ColNames.day, ColNames.cells]\n        )\n        intersections_sdf = intersections_sdf.withColumn(ColNames.group_size, F.size(F.col(ColNames.cells)))\n        intersections_sdf = intersections_sdf.filter(F.col(ColNames.group_size) &gt; 1)\n\n        return intersections_sdf\n\n    @staticmethod\n    def calculate_all_intersection_combinations(sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Calculates all possible combinations of intersection groups.\n        It is necessary to extract all overlap combinations from every\n        intersection group. If there is an intersection group ABC intersection\n        groups AB, AC, BC also has to be present.\n\n        Parameters:\n        sdf (DataFrame): A Spark DataFrame containing the intersection groups data.\n\n        Returns:\n        DataFrame: A DataFrame with all possible combinations of intersection groups for each grid tile.\n        \"\"\"\n        combinations_sdf = []\n        max_level = sdf.agg(F.max(ColNames.group_size).alias(\"max\")).collect()[0][\"max\"]\n\n        for level in range(2, max_level + 1):\n            combinations_udf = CellFootprintEstimation.generate_combinations(level)\n\n            combinations_sdf_level = sdf.filter(F.col(ColNames.group_size) &gt;= level).withColumn(\n                ColNames.cells, F.explode(combinations_udf(F.col(ColNames.cells)))\n            )\n\n            combinations_sdf_level = combinations_sdf_level.withColumn(\n                ColNames.cells, F.array_sort(F.col(ColNames.cells))\n            )\n            combinations_sdf_level = combinations_sdf_level.drop_duplicates(\n                [ColNames.year, ColNames.month, ColNames.day, ColNames.cells]\n            )\n\n            combinations_sdf_level = combinations_sdf_level.withColumn(ColNames.group_size, F.lit(level))\n\n            window = Window.partitionBy(ColNames.year, ColNames.month, ColNames.day, ColNames.group_size).orderBy(\n                F.col(ColNames.group_size)\n            )\n\n            combinations_sdf_level = combinations_sdf_level.withColumn(\n                ColNames.group_id,\n                F.concat(F.col(ColNames.group_size), F.lit(\"_\"), F.row_number().over(window)),\n            )\n\n            combinations_sdf.append(combinations_sdf_level)\n\n        return reduce(DataFrame.unionAll, combinations_sdf)\n\n    @staticmethod\n    def generate_combinations(level):\n        def combinations_udf(arr):\n            return list(combinations(arr, level))\n\n        return F.udf(combinations_udf, ArrayType(ArrayType(StringType())))\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.calculate_all_intersection_combinations","title":"<code>calculate_all_intersection_combinations(sdf)</code>  <code>staticmethod</code>","text":"<p>Calculates all possible combinations of intersection groups. It is necessary to extract all overlap combinations from every intersection group. If there is an intersection group ABC intersection groups AB, AC, BC also has to be present.</p> <p>Parameters: sdf (DataFrame): A Spark DataFrame containing the intersection groups data.</p> <p>Returns: DataFrame: A DataFrame with all possible combinations of intersection groups for each grid tile.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef calculate_all_intersection_combinations(sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Calculates all possible combinations of intersection groups.\n    It is necessary to extract all overlap combinations from every\n    intersection group. If there is an intersection group ABC intersection\n    groups AB, AC, BC also has to be present.\n\n    Parameters:\n    sdf (DataFrame): A Spark DataFrame containing the intersection groups data.\n\n    Returns:\n    DataFrame: A DataFrame with all possible combinations of intersection groups for each grid tile.\n    \"\"\"\n    combinations_sdf = []\n    max_level = sdf.agg(F.max(ColNames.group_size).alias(\"max\")).collect()[0][\"max\"]\n\n    for level in range(2, max_level + 1):\n        combinations_udf = CellFootprintEstimation.generate_combinations(level)\n\n        combinations_sdf_level = sdf.filter(F.col(ColNames.group_size) &gt;= level).withColumn(\n            ColNames.cells, F.explode(combinations_udf(F.col(ColNames.cells)))\n        )\n\n        combinations_sdf_level = combinations_sdf_level.withColumn(\n            ColNames.cells, F.array_sort(F.col(ColNames.cells))\n        )\n        combinations_sdf_level = combinations_sdf_level.drop_duplicates(\n            [ColNames.year, ColNames.month, ColNames.day, ColNames.cells]\n        )\n\n        combinations_sdf_level = combinations_sdf_level.withColumn(ColNames.group_size, F.lit(level))\n\n        window = Window.partitionBy(ColNames.year, ColNames.month, ColNames.day, ColNames.group_size).orderBy(\n            F.col(ColNames.group_size)\n        )\n\n        combinations_sdf_level = combinations_sdf_level.withColumn(\n            ColNames.group_id,\n            F.concat(F.col(ColNames.group_size), F.lit(\"_\"), F.row_number().over(window)),\n        )\n\n        combinations_sdf.append(combinations_sdf_level)\n\n    return reduce(DataFrame.unionAll, combinations_sdf)\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.calculate_intersection_groups","title":"<code>calculate_intersection_groups(sdf)</code>  <code>staticmethod</code>","text":"<p>Calculates the cell intersection groups based on cell footprints overlaps over grid tiles.</p> <p>Parameters: sdf (DataFrame): A Spark DataFrame containing the signal dominance data.</p> <p>Returns: DataFrame: A DataFrame with the intersection groups.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef calculate_intersection_groups(sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Calculates the cell intersection groups based on cell footprints overlaps\n    over grid tiles.\n\n    Parameters:\n    sdf (DataFrame): A Spark DataFrame containing the signal dominance data.\n\n    Returns:\n    DataFrame: A DataFrame with the intersection groups.\n    \"\"\"\n    intersections_sdf = (\n        sdf.groupBy(\n            F.col(ColNames.year),\n            F.col(ColNames.month),\n            F.col(ColNames.day),\n            F.col(ColNames.grid_id),\n        )\n        .agg(F.array_sort(F.collect_set(ColNames.cell_id)).alias(ColNames.cells))\n        .select(ColNames.cells, ColNames.year, ColNames.month, ColNames.day)\n    )\n\n    intersections_sdf = intersections_sdf.drop_duplicates(\n        [ColNames.year, ColNames.month, ColNames.day, ColNames.cells]\n    )\n    intersections_sdf = intersections_sdf.withColumn(ColNames.group_size, F.size(F.col(ColNames.cells)))\n    intersections_sdf = intersections_sdf.filter(F.col(ColNames.group_size) &gt; 1)\n\n    return intersections_sdf\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.prune_max_cells_per_grid_tile","title":"<code>prune_max_cells_per_grid_tile(sdf, max_cells_per_grid_tile)</code>  <code>staticmethod</code>","text":"<p>Prunes rows from the DataFrame exceeding the maximum number of cells allowed per grid tile.</p> <p>The rows are ordered by signal dominance in descending order, and only the top 'max_cells_per_grid_tile' rows are kept for each grid tile.</p> <p>Parameters: sdf (DataFrame): A Spark DataFrame containing the signal dominance data. max_cells_per_grid_tile (int): The maximum number of cells allowed per grid tile.</p> <p>Returns: DataFrame: A DataFrame with rows exceeding the maximum number of cells per grid tile removed.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef prune_max_cells_per_grid_tile(sdf: DataFrame, max_cells_per_grid_tile: int) -&gt; DataFrame:\n    \"\"\"\n    Prunes rows from the DataFrame exceeding the maximum number of cells allowed per grid tile.\n\n    The rows are ordered by signal dominance in descending order,\n    and only the top 'max_cells_per_grid_tile' rows are kept for each grid tile.\n\n    Parameters:\n    sdf (DataFrame): A Spark DataFrame containing the signal dominance data.\n    max_cells_per_grid_tile (int): The maximum number of cells allowed per grid tile.\n\n    Returns:\n    DataFrame: A DataFrame with rows exceeding the maximum number of cells per grid tile removed.\n    \"\"\"\n    window = Window.partitionBy(ColNames.year, ColNames.month, ColNames.day, ColNames.grid_id).orderBy(\n        F.desc(ColNames.signal_dominance)\n    )\n\n    sdf = sdf.withColumn(\"row_number\", F.row_number().over(window))\n    sdf = sdf.filter(F.col(\"row_number\") &lt;= max_cells_per_grid_tile)\n    sdf = sdf.drop(\"row_number\")\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.prune_signal_difference_from_best","title":"<code>prune_signal_difference_from_best(sdf, difference_threshold)</code>  <code>staticmethod</code>","text":"<p>Prunes rows from the DataFrame based on a threshold of signal dominance difference.</p> <p>The rows are ordered by signal dominance in descending order, and only the rows where the difference in signal dominance from the maximum is less than the threshold are kept for each grid tile.</p> <p>Parameters: sdf (DataFrame): A Spark DataFrame containing the signal dominance data. threshold (float): The threshold for signal dominance difference in percentage.</p> <p>Returns: DataFrame: A DataFrame with rows pruned based on the signal dominance difference threshold.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef prune_signal_difference_from_best(sdf: DataFrame, difference_threshold: float) -&gt; DataFrame:\n    \"\"\"\n    Prunes rows from the DataFrame based on a threshold of signal dominance difference.\n\n    The rows are ordered by signal dominance in descending order, and only the rows where the difference\n    in signal dominance from the maximum is less than the threshold are kept for each grid tile.\n\n    Parameters:\n    sdf (DataFrame): A Spark DataFrame containing the signal dominance data.\n    threshold (float): The threshold for signal dominance difference in percentage.\n\n    Returns:\n    DataFrame: A DataFrame with rows pruned based on the signal dominance difference threshold.\n    \"\"\"\n    window = Window.partitionBy(ColNames.year, ColNames.month, ColNames.day, ColNames.grid_id).orderBy(\n        F.desc(ColNames.signal_dominance)\n    )\n\n    sdf = sdf.withColumn(\"row_number\", F.row_number().over(window))\n\n    sdf = sdf.withColumn(\"max_signal_dominance\", F.first(ColNames.signal_dominance).over(window))\n    sdf = sdf.withColumn(\n        \"signal_dominance_diff_percentage\",\n        (sdf[\"max_signal_dominance\"] - sdf[ColNames.signal_dominance]) / sdf[\"max_signal_dominance\"] * 100,\n    )\n\n    sdf = sdf.filter(\n        (F.col(\"row_number\") == 1) | (F.col(\"signal_dominance_diff_percentage\") &lt;= difference_threshold)\n    )\n\n    sdf = sdf.drop(\"row_number\", \"max_signal_dominance\", \"signal_dominance_diff_percentage\")\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.prune_small_signal_dominance","title":"<code>prune_small_signal_dominance(sdf, signal_dominance_threshold)</code>  <code>staticmethod</code>","text":"<p>Prunes rows from the DataFrame where the signal dominance is less than or equal to the provided threshold.</p> <p>Parameters: sdf (DataFrame): A Spark DataFrame containing the signal dominance data. signal_dominance_threshold (float): The threshold for pruning small signal dominance values.</p> <p>Returns: DataFrame: A DataFrame with rows having signal dominance less than or equal to the threshold removed.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef prune_small_signal_dominance(sdf: DataFrame, signal_dominance_threshold: float) -&gt; DataFrame:\n    \"\"\"\n    Prunes rows from the DataFrame where the signal dominance is less than or equal to the provided threshold.\n\n    Parameters:\n    sdf (DataFrame): A Spark DataFrame containing the signal dominance data.\n    signal_dominance_threshold (float): The threshold for pruning small signal dominance values.\n\n    Returns:\n    DataFrame: A DataFrame with rows having signal dominance less than or equal to the threshold removed.\n    \"\"\"\n    sdf = sdf.filter(F.col(ColNames.signal_dominance) &gt; signal_dominance_threshold)\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.signal_strength_to_signal_dominance","title":"<code>signal_strength_to_signal_dominance(sdf, logistic_function_steepness, logistic_function_midpoint)</code>  <code>staticmethod</code>","text":"<p>Converts signal strength to signal dominance using a logistic function. Methodology from A Bayesian approach to location estimation of mobile devices from mobile network operator data. Tennekes and Gootzen (2022).</p> <p>The logistic function is defined as 1 / (1 + exp(-scale)), where scale is (signal_strength - logistic_function_midpoint) * logistic_function_steepness.</p> <p>Parameters: sdf (DataFrame): SignalStrenghtDataObject Spark DataFrame. logistic_function_steepness (float): The steepness parameter for the logistic function. logistic_function_midpoint (float): The midpoint parameter for the logistic function.</p> <p>Returns: DataFrame: A Spark DataFrame with the signal dominance added as a new column.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef signal_strength_to_signal_dominance(\n    sdf: DataFrame,\n    logistic_function_steepness: float,\n    logistic_function_midpoint: float,\n) -&gt; DataFrame:\n    \"\"\"\n    Converts signal strength to signal dominance using a logistic function.\n    Methodology from A Bayesian approach to location estimation of mobile devices\n    from mobile network operator data. Tennekes and Gootzen (2022).\n\n    The logistic function is defined as 1 / (1 + exp(-scale)),\n    where scale is (signal_strength - logistic_function_midpoint) * logistic_function_steepness.\n\n    Parameters:\n    sdf (DataFrame): SignalStrenghtDataObject Spark DataFrame.\n    logistic_function_steepness (float): The steepness parameter for the logistic function.\n    logistic_function_midpoint (float): The midpoint parameter for the logistic function.\n\n    Returns:\n    DataFrame: A Spark DataFrame with the signal dominance added as a new column.\n    \"\"\"\n    sdf = sdf.withColumn(\n        \"scale\",\n        (F.col(ColNames.signal_strength) - logistic_function_midpoint) * logistic_function_steepness,\n    )\n    sdf = sdf.withColumn(ColNames.signal_dominance, 1 / (1 + F.exp(-F.col(\"scale\"))))\n    sdf = sdf.drop(\"scale\")\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/","title":"daily_permanence_score","text":""},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/","title":"daily_permanence_score","text":"<p>Module that implements the Daily Permanence Score functionality</p>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore","title":"<code>DailyPermanenceScore</code>","text":"<p>               Bases: <code>Component</code></p> <p>A class to calculate the daily permanence score of each user per interval and grid tile.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>class DailyPermanenceScore(Component):\n    \"\"\"\n    A class to calculate the daily permanence score of each user per interval and grid tile.\n    \"\"\"\n\n    COMPONENT_ID = \"DailyPermanenceScore\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.data_period_start = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n        self.data_period_end = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n\n        self.time_slot_number = self.config.getint(self.COMPONENT_ID, \"time_slot_number\")\n        if self.time_slot_number not in [24, 48, 96]:\n            raise ValueError(\"Only allowed values of time_slot_number are 24, 48, 98 -- found\", self.time_slot_number)\n\n        self.max_time_thresh = timedelta(seconds=self.config.getint(self.COMPONENT_ID, \"max_time_thresh\"))\n        self.max_time_thresh_day = timedelta(seconds=self.config.getint(self.COMPONENT_ID, \"max_time_thresh_day\"))\n        self.max_time_thresh_night = timedelta(seconds=self.config.getint(self.COMPONENT_ID, \"max_time_thresh_night\"))\n        self.max_speed_thresh = self.config.getfloat(self.COMPONENT_ID, \"max_speed_thresh\")\n\n        self.score_interval = self.config.getint(self.COMPONENT_ID, \"score_interval\")\n        if self.score_interval != 2:\n            raise NotImplementedError(\"Only allowed value of score_interval in current version is `2`\")\n\n        self.event_error_flags_to_include = self.config.geteval(self.COMPONENT_ID, \"event_error_flags_to_include\")\n\n        self.data_period_dates = [\n            self.data_period_start + timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n        self.current_date = None\n        self.previous_date = None\n        self.next_date = None\n\n        self.current_events = None\n        self.previous_events = None\n        self.next_events = None\n\n        self.current_cell_footprint = None\n        self.previous_cell_footprint = None\n        self.next_cell_footprint = None\n\n    def initalize_data_objects(self):\n        input_events_silver_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"event_data_silver_flagged\")\n        input_cell_footprint_silver_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"cell_footprint_data_silver\")\n        output_dps_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"daily_permanence_score_data_silver\")\n\n        silver_events = SilverEventFlaggedDataObject(self.spark, input_events_silver_path)\n\n        silver_cell_footprint = SilverCellFootprintDataObject(\n            self.spark,\n            input_cell_footprint_silver_path,\n            partition_columns=[ColNames.year, ColNames.month, ColNames.day],\n        )\n\n        silver_dps = SilverDailyPermanenceScoreDataObject(\n            self.spark,\n            output_dps_path,\n        )\n\n        self.input_data_objects = {silver_events.ID: silver_events, silver_cell_footprint.ID: silver_cell_footprint}\n\n        self.output_data_objects = {silver_dps.ID: silver_dps}\n\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n        self.check_needed_dates()\n\n        for current_date in self.data_period_dates:\n\n            self.logger.info(f\"Processing events for {current_date.strftime('%Y-%m-%d')}\")\n\n            self.current_date = current_date\n            self.previous_date = current_date - timedelta(days=1)\n            self.next_date = current_date + timedelta(days=1)\n\n            self.current_events = self.filter_events(self.current_date)\n            self.previous_events = self.filter_events(self.previous_date)\n            self.next_events = self.filter_events(self.next_date)\n\n            self.current_cell_footprint = self.filter_cell_footprint(self.current_date)\n            self.previous_cell_footprint = self.filter_cell_footprint(self.previous_date)\n            self.next_cell_footprint = self.filter_cell_footprint(self.next_date)\n\n            self.transform()\n\n            self.write()\n\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def check_needed_dates(self):\n        \"\"\"\n        Method that checks if both the dates of study and the dates necessary to generate\n        the daily permanence scores are present in the input data (events + cell footprint).\n\n        Raises:\n            ValueError: If there is no data for one or more of the needed dates.\n        \"\"\"\n        # needed dates: for each date D, we also need D-1 and D+1\n        # this is built this way so it would also support definition of study\n        # dates that are not consecutive\n        needed_dates = (\n            {d + timedelta(days=1) for d in self.data_period_dates}\n            | set(self.data_period_dates)\n            | {d - timedelta(days=1) for d in self.data_period_dates}\n        )\n\n        # Assert needed dates in event data:\n        self.assert_needed_dates_data_object(SilverEventFlaggedDataObject.ID, needed_dates)\n\n        # Assert needed dates in cell footprint data:\n        self.assert_needed_dates_data_object(SilverCellFootprintDataObject.ID, needed_dates)\n\n    def assert_needed_dates_data_object(self, data_object_id: str, needed_dates: list[datetime]):\n        \"\"\"\n        Method that checks if data for a set of dates exists for a data object.\n\n        Args:\n            data_object_id (str): name of the data object to check.\n            needed_dates (list[datetime]): list of the dates for which data shall be available.\n\n        Raises:\n            ValueError: If there is no data for one or more of the needed dates.\n        \"\"\"\n        # Load data\n        df = self.input_data_objects[data_object_id].df\n\n        # Find dates that match the needed dates:\n        dates = (\n            df.withColumn(ColNames.date, F.make_date(ColNames.year, ColNames.month, ColNames.day))\n            .select(F.col(ColNames.date))\n            .filter(F.col(ColNames.date).isin(needed_dates))\n            .distinct()\n            .collect()\n        )\n        available_dates = {row[ColNames.date] for row in dates}\n\n        # If missing needed dates, raise error:\n        missing_dates = needed_dates.difference(available_dates)\n        if missing_dates:\n            error_msg = f\"Missing {data_object_id} data for dates {sorted(list(missing_dates))}\"\n            self.logger.error(error_msg)\n            raise ValueError(error_msg)\n\n    def filter_events(self, current_date: date) -&gt; DataFrame:\n        \"\"\"\n        Load events with no errors for a specific date.\n\n        Args:\n            current_date (date): current date.\n\n        Returns:\n            DataFrame: filtered events dataframe.\n        \"\"\"\n        return self.input_data_objects[SilverEventFlaggedDataObject.ID].df.filter(\n            (F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(current_date))\n            &amp; (F.col(ColNames.error_flag).isin(self.event_error_flags_to_include))\n        )\n\n    def filter_cell_footprint(self, current_date: date) -&gt; DataFrame:\n        \"\"\"\n        Load cell footprints for a specific date.\n\n        Args:\n            current_date (date): current date.\n\n        Returns:\n            DataFrame: filtered cell footprint dataframe.\n        \"\"\"\n        return self.input_data_objects[SilverCellFootprintDataObject.ID].df.filter(\n            (F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(current_date))\n        )\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n\n        # load users events (dates D-1, D, D+1):\n        events = self.build_events_table()\n\n        # load cell footprint (dates D-1, D, D+1):\n        cell_footprint = self.build_cell_footprint_table()\n\n        # build time slots dataframe (date D):\n        time_slots = self.build_time_slots_table()\n\n        # differentiate 'move' events:\n        events = self.detect_move_events(events, cell_footprint)\n\n        # Determine stay durations:\n        stays = self.determine_stay_durations(events)\n\n        # Assign stay time slot, assign duration to time slots and map to calculate DPS:\n        dps = self.calculate_dps(stays, time_slots)\n\n        self.output_data_objects[SilverDailyPermanenceScoreDataObject.ID].df = dps\n\n    def build_events_table(self) -&gt; DataFrame:\n        \"\"\"\n        Load events data for date D, also adding last event of each\n        user from date D-1 and first event of each user from D+1.\n\n        Returns:\n            DataFrame: events dataframe.\n        \"\"\"\n        # reach last event from previous day:\n        window = Window.partitionBy(ColNames.user_id).orderBy(F.desc(ColNames.timestamp))\n        self.previous_events = (\n            self.previous_events.withColumn(\"row_number\", F.row_number().over(window))\n            .filter(F.col(\"row_number\") == 1)\n            .drop(\"row_number\")\n        )\n\n        # reach first event from next day:\n        window = Window.partitionBy(ColNames.user_id).orderBy(ColNames.timestamp)\n        self.next_events = (\n            self.next_events.withColumn(\"row_number\", F.row_number().over(window))\n            .filter(F.col(\"row_number\") == 1)\n            .drop(\"row_number\")\n        )\n\n        # concat all events together (last of D-1 + all D + first of D+1):\n        events = (\n            self.previous_events.union(self.current_events)\n            .union(self.next_events)\n            .drop(ColNames.longitude, ColNames.latitude, ColNames.loc_error, ColNames.error_flag)\n        )\n\n        return events\n\n    def build_cell_footprint_table(self) -&gt; DataFrame:\n        \"\"\"\n        Load cell footprint data for dates D-1, D and D+1.\n\n        Returns:\n            DataFrame: cell footprint dataframe.\n        \"\"\"\n        cell_footprint = (\n            self.previous_cell_footprint.union(self.current_cell_footprint)\n            .union(self.next_cell_footprint)\n            .groupBy([ColNames.cell_id, ColNames.year, ColNames.month, ColNames.day])\n            .agg(F.collect_list(ColNames.grid_id).alias(\"grid_ids\"))\n        )\n\n        return cell_footprint\n\n    def build_time_slots_table(self) -&gt; DataFrame:\n        \"\"\"\n        Build a dataframe with the specified time slots for the current date.\n\n        Returns:\n            DataFrame: time slots dataframe.\n        \"\"\"\n        time_slot_length = timedelta(days=1) / self.time_slot_number\n\n        time_slots_list = []\n        previous_end_time = datetime(\n            year=self.current_date.year,\n            month=self.current_date.month,\n            day=self.current_date.day,\n            hour=0,\n            minute=0,\n            second=0,\n        )\n\n        while previous_end_time.date() == self.current_date:\n            init_time = previous_end_time\n            end_time = init_time + time_slot_length\n            time_slot = (init_time, end_time)\n            time_slots_list.append(time_slot)\n            previous_end_time = end_time\n\n        schema = StructType(\n            [\n                StructField(ColNames.time_slot_initial_time, TimestampType(), True),\n                StructField(\"time_slot_end_time\", TimestampType(), True),\n            ]\n        )\n\n        return self.spark.createDataFrame(time_slots_list, schema=schema)\n\n    def detect_move_events(self, events: DataFrame, cell_footprint: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Detect which of the events are associated to moves according to the\n        distances/times from previous to posterior event and a speed threshold.\n\n        Args:\n            events (DataFrame): events dataframe.\n            cell_footprint (DataFrame): cells footprint dataframe.\n\n        Returns:\n            DataFrame: events dataframe, with an additional 'is_move' boolean column.\n        \"\"\"\n        # left join -&gt; bring cell footprints to events data:\n        events = events.join(\n            cell_footprint,\n            (events[ColNames.cell_id] == cell_footprint[ColNames.cell_id])\n            &amp; (events[ColNames.year] == cell_footprint[ColNames.year])\n            &amp; (events[ColNames.month] == cell_footprint[ColNames.month])\n            &amp; (events[ColNames.day] == cell_footprint[ColNames.day]),\n            \"left\",\n        ).drop(\n            cell_footprint[ColNames.cell_id],\n            cell_footprint[ColNames.year],\n            cell_footprint[ColNames.month],\n            cell_footprint[ColNames.day],\n        )\n\n        # Add lags of timestamp, cell_id and grid_ids:\n        window = Window.partitionBy(ColNames.user_id).orderBy(ColNames.timestamp)\n        lag_fields = [ColNames.timestamp, ColNames.cell_id, \"grid_ids\"]\n        for lf in lag_fields:\n            events = events.withColumn(f\"{lf}_+1\", F.lag(lf, -1).over(window)).withColumn(\n                f\"{lf}_-1\", F.lag(lf, 1).over(window)\n            )\n\n        # Calculate distance between grid tiles associated to events -1, 0 and +1:\n        # Calculate speeds and determine which rows are moves:\n        events = (\n            events.withColumn(\"dist_0_+1\", self.grid_footprint_distance(F.col(\"grid_ids\"), F.col(\"grid_ids_+1\")))\n            .withColumn(\"dist_-1_0\", self.grid_footprint_distance(F.col(\"grid_ids_-1\"), F.col(\"grid_ids\")))\n            .withColumn(\"dist_-1_+1\", self.grid_footprint_distance(F.col(\"grid_ids_-1\"), F.col(\"grid_ids_+1\")))\n            .withColumn(\n                \"time_difference\",\n                F.unix_timestamp(events[f\"{ColNames.timestamp}_+1\"])\n                - F.unix_timestamp(events[f\"{ColNames.timestamp}_-1\"]),\n            )\n            .withColumn(\"max_dist\", F.greatest(F.col(\"dist_-1_0\") + F.col(\"dist_0_+1\"), F.col(\"dist_-1_+1\")))\n            .withColumn(\"speed\", F.col(\"max_dist\") / F.col(\"time_difference\"))\n            .withColumn(\"is_move\", F.when(F.col(\"speed\") &gt; self.max_speed_thresh, True).otherwise(False))\n            .drop(\n                \"dist_0_+1\",\n                \"dist_-1_0\",\n                \"dist_-1_+1\",\n                \"grid_ids_+1\",\n                \"grid_ids_-1\",\n                \"time_difference\",\n                \"max_dist\",\n                \"speed\",\n            )\n        )\n\n        return events\n\n    @staticmethod\n    def get_grid_id_size_meters(grid_id: str) -&gt; float:\n        \"\"\"\n        Reach the size of a grid tile from its identifier.\n\n        Args:\n            grid_id (str): ID of the corresponding grid tile.\n\n        Returns:\n            float: grid tile size, in meters.\n        \"\"\"\n        size_txt = re.search(r\"^.*m\", grid_id)[0]\n        if size_txt[-2:] == \"km\":\n            size = float(size_txt[:-2]) * 1000\n        else:\n            size = float(size_txt[:-1])\n        return size\n\n    @staticmethod\n    def get_grid_id_vertices(grid_id: str) -&gt; list[tuple[float]]:\n        \"\"\"\n        Obtain the coordinates of the vertices of a given grid tile.\n\n        Args:\n            grid_id (str): ID of the corresponding grid tile.\n\n        Returns:\n            list[tuple[float]]: list of tuples. Each tuple contains the x, y\n                coordinates of a vertex of the corresponding grid tile.\n        \"\"\"\n        size = DailyPermanenceScore.get_grid_id_size_meters(grid_id)\n        xleft = float(re.search(r\"E\\s*(\\d+)\", grid_id)[0][1:])\n        ybottom = float(re.search(r\"N\\s*(\\d+)\", grid_id)[0][1:])\n        xright = xleft + size\n        ytop = ybottom + size\n        xs = (xleft, xright)\n        ys = (ybottom, ytop)\n        vertices = list(itertools.product(xs, ys))\n        return vertices\n\n    @staticmethod\n    def calculate_min_distance_between_point_lists(points_i: set[tuple[float]], points_j: set[tuple[float]]) -&gt; float:\n        \"\"\"\n        Calculate minimum distance between the points in one list and the points\n        in another list.\n\n        Args:\n            points_i (set[tuple[float]]): set of tuples. Each tuple contains\n                the x, y coordinates of a point.\n            points_j (set[tuple[float]]): set of tuples. Each tuple contains\n                the x, y coordinates of a point.\n\n        Returns:\n            float: minimum distance between points in both lists.\n        \"\"\"\n        if points_i &amp; points_j:  # same point is included in both sets\n            return 0.0\n        min_distance = float(\"inf\")\n        for pi in points_i:\n            for pj in points_j:\n                distance = (pi[0] - pj[0]) ** 2 + (pi[1] - pj[1]) ** 2\n                if distance &lt; min_distance:\n                    min_distance = distance\n        return min_distance**0.5\n\n    @staticmethod\n    @F.udf(returnType=FloatType())\n    def grid_footprint_distance(grid_ids_i: list[str], grid_ids_j: list[str]) -&gt; float:\n        \"\"\"\n        Calculate minimum distance between the grid tiles in one list and the\n        grid tiles in another list, provided as grid tile IDs.\n\n        Args:\n            grid_ids_i (list[str]): IDs of the corresponding grid tiles.\n            grid_ids_j (list[str]): IDs of the corresponding grid tiles.\n\n        Returns:\n            float: minimum distance.\n        \"\"\"\n        # TODO: optimise\n        if not grid_ids_i or not grid_ids_j:\n            return None\n\n        if set(grid_ids_i) == set(grid_ids_j):\n            return 0.0\n\n        gi_vertices = set()\n        for gi in grid_ids_i:\n            gi_vertices |= set(DailyPermanenceScore.get_grid_id_vertices(gi))\n        gj_vertices = set()\n        for gj in grid_ids_j:\n            gj_vertices |= set(DailyPermanenceScore.get_grid_id_vertices(gj))\n\n        min_distance = DailyPermanenceScore.calculate_min_distance_between_point_lists(gi_vertices, gj_vertices)\n\n        return min_distance\n\n    def determine_stay_durations(self, events: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Determine the start time and end time for each stay event.\n\n        Args:\n            events (DataFrame): events dataframe.\n\n        Returns:\n            DataFrame: stays dataframe (filtering out moves).\n        \"\"\"\n        current_datetime = datetime(self.current_date.year, self.current_date.month, self.current_date.day)\n        night_start_time = current_datetime - timedelta(hours=1)\n        night_end_time = current_datetime + timedelta(hours=9)\n\n        stays = (\n            events\n            # Set applicable time thresholds:\n            .withColumn(\n                \"threshold_-1\",\n                F.when(\n                    (F.col(\"cell_id\") == F.col(\"cell_id_-1\"))\n                    &amp; (F.col(\"timestamp_-1\") &gt;= night_start_time)\n                    &amp; (F.col(\"timestamp\") &lt;= night_end_time),\n                    self.max_time_thresh_night,\n                ).otherwise(\n                    F.when(F.col(\"cell_id\") == F.col(\"cell_id_-1\"), self.max_time_thresh_day).otherwise(\n                        self.max_time_thresh\n                    )\n                ),\n            )\n            .withColumn(\n                \"threshold_+1\",\n                F.when(\n                    (F.col(\"cell_id\") == F.col(\"cell_id_+1\"))\n                    &amp; (F.col(\"timestamp\") &gt;= night_start_time)\n                    &amp; (F.col(\"timestamp_+1\") &lt;= night_end_time),\n                    self.max_time_thresh_night,\n                ).otherwise(\n                    F.when(F.col(\"cell_id\") == F.col(\"cell_id_+1\"), self.max_time_thresh_day).otherwise(\n                        self.max_time_thresh\n                    )\n                ),\n            )\n            # Calculate init_time and end_time according to thresholds and time differences between events:\n            .withColumn(\n                \"init_time\",\n                F.when(\n                    F.col(ColNames.timestamp) - F.col(f\"{ColNames.timestamp}_-1\") &lt;= F.col(\"threshold_-1\"),\n                    F.col(ColNames.timestamp) - (F.col(ColNames.timestamp) - F.col(f\"{ColNames.timestamp}_-1\")) / 2,\n                ).otherwise(F.col(ColNames.timestamp) - self.max_time_thresh / 2),\n            )\n            .withColumn(\n                \"end_time\",\n                F.when(\n                    F.col(f\"{ColNames.timestamp}_+1\") - F.col(ColNames.timestamp) &lt;= F.col(\"threshold_+1\"),\n                    F.col(f\"{ColNames.timestamp}_+1\")\n                    - (F.col(f\"{ColNames.timestamp}_+1\") - F.col(ColNames.timestamp)) / 2,\n                ).otherwise(F.col(ColNames.timestamp) + self.max_time_thresh / 2),\n            )\n            # Filter out 'move' events (keep only stays), and also drop unneeded columns:\n            .filter(F.col(\"is_move\") == False)\n            .drop(\n                ColNames.cell_id,\n                f\"{ColNames.cell_id}_-1\",\n                f\"{ColNames.cell_id}_+1\",\n                ColNames.timestamp,\n                f\"{ColNames.timestamp}_-1\",\n                f\"{ColNames.timestamp}_+1\",\n                ColNames.mcc,\n                \"is_move\",\n                \"threshold_-1\",\n                \"threshold_+1\",\n            )\n        )\n\n        return stays\n\n    def calculate_dps(self, stays: DataFrame, time_slots: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Temporally intersect each stay interval with the specified time slots. Then\n        calculate the number of seconds that each user stays at each grid tile within\n        each of these time slots according to the stay intervals and the grid tiles\n        associated to each stay.\n\n        Args:\n            stays (DataFrame): stays dataframe.\n            time_slots (DataFrame): time slots dataframe.\n\n        Returns:\n            DataFrame: daily permanence score dataframe.\n        \"\"\"\n        dps = (\n            stays.crossJoin(time_slots)\n            .withColumn(\"int_init_time\", F.greatest(F.col(\"init_time\"), F.col(ColNames.time_slot_initial_time)))\n            .withColumn(\"int_end_time\", F.least(F.col(\"end_time\"), F.col(\"time_slot_end_time\")))\n            .withColumn(\n                \"int_duration\",\n                F.when(\n                    F.col(\"int_init_time\") &lt; F.col(\"int_end_time\"),\n                    F.unix_timestamp(F.col(\"int_end_time\")) - F.unix_timestamp(F.col(\"int_init_time\")),\n                ).otherwise(0),\n            )\n            .drop(\"int_init_time\", \"int_end_time\", \"init_time\", \"end_time\")\n        )\n\n        unknown_dps = (\n            dps.groupby(\n                ColNames.user_id, ColNames.user_id_modulo, ColNames.time_slot_initial_time, ColNames.time_slot_end_time\n            )\n            .agg(\n                (\n                    F.lit((timedelta(days=1) / self.time_slot_number).total_seconds()).cast(IntegerType())\n                    - F.sum(\"int_duration\")\n                ).alias(\"int_duration\")\n            )\n            .filter(F.col(\"int_duration\") &gt; 0.0)\n            .select(\n                ColNames.user_id,\n                ColNames.user_id_modulo,\n                F.lit(\"unknown\").alias(\"grid_id\"),\n                ColNames.time_slot_initial_time,\n                ColNames.time_slot_end_time,\n                \"int_duration\",\n                F.lit(\"unknown\").alias(ColNames.id_type),\n            )\n        )\n\n        known_dps = (\n            dps.filter(F.col(\"int_duration\") &gt; 0.0)\n            .withColumn(ColNames.grid_id, F.explode(\"grid_ids\"))\n            .drop(\"grid_ids\")\n            .groupby(\n                ColNames.user_id,\n                ColNames.user_id_modulo,\n                ColNames.grid_id,\n                ColNames.time_slot_initial_time,\n                ColNames.time_slot_end_time,\n            )\n            .agg(F.sum(\"int_duration\").alias(\"int_duration\"))\n            .withColumn(ColNames.id_type, F.lit(\"grid\"))\n        )\n\n        dps = (\n            known_dps.union(unknown_dps)\n            .withColumn(\n                \"time_slot_duration\",\n                F.lit((timedelta(days=1) / self.time_slot_number).total_seconds()).cast(IntegerType()),\n            )\n            .withColumn(\n                \"dps\",\n                F.lit(-1)\n                + F.ceil(F.lit(self.score_interval) * F.col(\"int_duration\") / F.col(\"time_slot_duration\")).cast(\n                    ByteType()\n                ),\n            )\n            .drop(\"int_duration\", \"time_slot_duration\")\n            # since some stays may be from events in previous date, fix and always set current date:\n            .withColumn(ColNames.year, F.lit(self.current_date.year).cast(ShortType()))\n            .withColumn(ColNames.month, F.lit(self.current_date.month).cast(ByteType()))\n            .withColumn(ColNames.day, F.lit(self.current_date.day).cast(ByteType()))\n        )\n\n        return dps\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.assert_needed_dates_data_object","title":"<code>assert_needed_dates_data_object(data_object_id, needed_dates)</code>","text":"<p>Method that checks if data for a set of dates exists for a data object.</p> <p>Parameters:</p> Name Type Description Default <code>data_object_id</code> <code>str</code> <p>name of the data object to check.</p> required <code>needed_dates</code> <code>list[datetime]</code> <p>list of the dates for which data shall be available.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If there is no data for one or more of the needed dates.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def assert_needed_dates_data_object(self, data_object_id: str, needed_dates: list[datetime]):\n    \"\"\"\n    Method that checks if data for a set of dates exists for a data object.\n\n    Args:\n        data_object_id (str): name of the data object to check.\n        needed_dates (list[datetime]): list of the dates for which data shall be available.\n\n    Raises:\n        ValueError: If there is no data for one or more of the needed dates.\n    \"\"\"\n    # Load data\n    df = self.input_data_objects[data_object_id].df\n\n    # Find dates that match the needed dates:\n    dates = (\n        df.withColumn(ColNames.date, F.make_date(ColNames.year, ColNames.month, ColNames.day))\n        .select(F.col(ColNames.date))\n        .filter(F.col(ColNames.date).isin(needed_dates))\n        .distinct()\n        .collect()\n    )\n    available_dates = {row[ColNames.date] for row in dates}\n\n    # If missing needed dates, raise error:\n    missing_dates = needed_dates.difference(available_dates)\n    if missing_dates:\n        error_msg = f\"Missing {data_object_id} data for dates {sorted(list(missing_dates))}\"\n        self.logger.error(error_msg)\n        raise ValueError(error_msg)\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.build_cell_footprint_table","title":"<code>build_cell_footprint_table()</code>","text":"<p>Load cell footprint data for dates D-1, D and D+1.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>cell footprint dataframe.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def build_cell_footprint_table(self) -&gt; DataFrame:\n    \"\"\"\n    Load cell footprint data for dates D-1, D and D+1.\n\n    Returns:\n        DataFrame: cell footprint dataframe.\n    \"\"\"\n    cell_footprint = (\n        self.previous_cell_footprint.union(self.current_cell_footprint)\n        .union(self.next_cell_footprint)\n        .groupBy([ColNames.cell_id, ColNames.year, ColNames.month, ColNames.day])\n        .agg(F.collect_list(ColNames.grid_id).alias(\"grid_ids\"))\n    )\n\n    return cell_footprint\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.build_events_table","title":"<code>build_events_table()</code>","text":"<p>Load events data for date D, also adding last event of each user from date D-1 and first event of each user from D+1.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>events dataframe.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def build_events_table(self) -&gt; DataFrame:\n    \"\"\"\n    Load events data for date D, also adding last event of each\n    user from date D-1 and first event of each user from D+1.\n\n    Returns:\n        DataFrame: events dataframe.\n    \"\"\"\n    # reach last event from previous day:\n    window = Window.partitionBy(ColNames.user_id).orderBy(F.desc(ColNames.timestamp))\n    self.previous_events = (\n        self.previous_events.withColumn(\"row_number\", F.row_number().over(window))\n        .filter(F.col(\"row_number\") == 1)\n        .drop(\"row_number\")\n    )\n\n    # reach first event from next day:\n    window = Window.partitionBy(ColNames.user_id).orderBy(ColNames.timestamp)\n    self.next_events = (\n        self.next_events.withColumn(\"row_number\", F.row_number().over(window))\n        .filter(F.col(\"row_number\") == 1)\n        .drop(\"row_number\")\n    )\n\n    # concat all events together (last of D-1 + all D + first of D+1):\n    events = (\n        self.previous_events.union(self.current_events)\n        .union(self.next_events)\n        .drop(ColNames.longitude, ColNames.latitude, ColNames.loc_error, ColNames.error_flag)\n    )\n\n    return events\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.build_time_slots_table","title":"<code>build_time_slots_table()</code>","text":"<p>Build a dataframe with the specified time slots for the current date.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>time slots dataframe.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def build_time_slots_table(self) -&gt; DataFrame:\n    \"\"\"\n    Build a dataframe with the specified time slots for the current date.\n\n    Returns:\n        DataFrame: time slots dataframe.\n    \"\"\"\n    time_slot_length = timedelta(days=1) / self.time_slot_number\n\n    time_slots_list = []\n    previous_end_time = datetime(\n        year=self.current_date.year,\n        month=self.current_date.month,\n        day=self.current_date.day,\n        hour=0,\n        minute=0,\n        second=0,\n    )\n\n    while previous_end_time.date() == self.current_date:\n        init_time = previous_end_time\n        end_time = init_time + time_slot_length\n        time_slot = (init_time, end_time)\n        time_slots_list.append(time_slot)\n        previous_end_time = end_time\n\n    schema = StructType(\n        [\n            StructField(ColNames.time_slot_initial_time, TimestampType(), True),\n            StructField(\"time_slot_end_time\", TimestampType(), True),\n        ]\n    )\n\n    return self.spark.createDataFrame(time_slots_list, schema=schema)\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.calculate_dps","title":"<code>calculate_dps(stays, time_slots)</code>","text":"<p>Temporally intersect each stay interval with the specified time slots. Then calculate the number of seconds that each user stays at each grid tile within each of these time slots according to the stay intervals and the grid tiles associated to each stay.</p> <p>Parameters:</p> Name Type Description Default <code>stays</code> <code>DataFrame</code> <p>stays dataframe.</p> required <code>time_slots</code> <code>DataFrame</code> <p>time slots dataframe.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>daily permanence score dataframe.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def calculate_dps(self, stays: DataFrame, time_slots: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Temporally intersect each stay interval with the specified time slots. Then\n    calculate the number of seconds that each user stays at each grid tile within\n    each of these time slots according to the stay intervals and the grid tiles\n    associated to each stay.\n\n    Args:\n        stays (DataFrame): stays dataframe.\n        time_slots (DataFrame): time slots dataframe.\n\n    Returns:\n        DataFrame: daily permanence score dataframe.\n    \"\"\"\n    dps = (\n        stays.crossJoin(time_slots)\n        .withColumn(\"int_init_time\", F.greatest(F.col(\"init_time\"), F.col(ColNames.time_slot_initial_time)))\n        .withColumn(\"int_end_time\", F.least(F.col(\"end_time\"), F.col(\"time_slot_end_time\")))\n        .withColumn(\n            \"int_duration\",\n            F.when(\n                F.col(\"int_init_time\") &lt; F.col(\"int_end_time\"),\n                F.unix_timestamp(F.col(\"int_end_time\")) - F.unix_timestamp(F.col(\"int_init_time\")),\n            ).otherwise(0),\n        )\n        .drop(\"int_init_time\", \"int_end_time\", \"init_time\", \"end_time\")\n    )\n\n    unknown_dps = (\n        dps.groupby(\n            ColNames.user_id, ColNames.user_id_modulo, ColNames.time_slot_initial_time, ColNames.time_slot_end_time\n        )\n        .agg(\n            (\n                F.lit((timedelta(days=1) / self.time_slot_number).total_seconds()).cast(IntegerType())\n                - F.sum(\"int_duration\")\n            ).alias(\"int_duration\")\n        )\n        .filter(F.col(\"int_duration\") &gt; 0.0)\n        .select(\n            ColNames.user_id,\n            ColNames.user_id_modulo,\n            F.lit(\"unknown\").alias(\"grid_id\"),\n            ColNames.time_slot_initial_time,\n            ColNames.time_slot_end_time,\n            \"int_duration\",\n            F.lit(\"unknown\").alias(ColNames.id_type),\n        )\n    )\n\n    known_dps = (\n        dps.filter(F.col(\"int_duration\") &gt; 0.0)\n        .withColumn(ColNames.grid_id, F.explode(\"grid_ids\"))\n        .drop(\"grid_ids\")\n        .groupby(\n            ColNames.user_id,\n            ColNames.user_id_modulo,\n            ColNames.grid_id,\n            ColNames.time_slot_initial_time,\n            ColNames.time_slot_end_time,\n        )\n        .agg(F.sum(\"int_duration\").alias(\"int_duration\"))\n        .withColumn(ColNames.id_type, F.lit(\"grid\"))\n    )\n\n    dps = (\n        known_dps.union(unknown_dps)\n        .withColumn(\n            \"time_slot_duration\",\n            F.lit((timedelta(days=1) / self.time_slot_number).total_seconds()).cast(IntegerType()),\n        )\n        .withColumn(\n            \"dps\",\n            F.lit(-1)\n            + F.ceil(F.lit(self.score_interval) * F.col(\"int_duration\") / F.col(\"time_slot_duration\")).cast(\n                ByteType()\n            ),\n        )\n        .drop(\"int_duration\", \"time_slot_duration\")\n        # since some stays may be from events in previous date, fix and always set current date:\n        .withColumn(ColNames.year, F.lit(self.current_date.year).cast(ShortType()))\n        .withColumn(ColNames.month, F.lit(self.current_date.month).cast(ByteType()))\n        .withColumn(ColNames.day, F.lit(self.current_date.day).cast(ByteType()))\n    )\n\n    return dps\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.calculate_min_distance_between_point_lists","title":"<code>calculate_min_distance_between_point_lists(points_i, points_j)</code>  <code>staticmethod</code>","text":"<p>Calculate minimum distance between the points in one list and the points in another list.</p> <p>Parameters:</p> Name Type Description Default <code>points_i</code> <code>set[tuple[float]]</code> <p>set of tuples. Each tuple contains the x, y coordinates of a point.</p> required <code>points_j</code> <code>set[tuple[float]]</code> <p>set of tuples. Each tuple contains the x, y coordinates of a point.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>minimum distance between points in both lists.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>@staticmethod\ndef calculate_min_distance_between_point_lists(points_i: set[tuple[float]], points_j: set[tuple[float]]) -&gt; float:\n    \"\"\"\n    Calculate minimum distance between the points in one list and the points\n    in another list.\n\n    Args:\n        points_i (set[tuple[float]]): set of tuples. Each tuple contains\n            the x, y coordinates of a point.\n        points_j (set[tuple[float]]): set of tuples. Each tuple contains\n            the x, y coordinates of a point.\n\n    Returns:\n        float: minimum distance between points in both lists.\n    \"\"\"\n    if points_i &amp; points_j:  # same point is included in both sets\n        return 0.0\n    min_distance = float(\"inf\")\n    for pi in points_i:\n        for pj in points_j:\n            distance = (pi[0] - pj[0]) ** 2 + (pi[1] - pj[1]) ** 2\n            if distance &lt; min_distance:\n                min_distance = distance\n    return min_distance**0.5\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.check_needed_dates","title":"<code>check_needed_dates()</code>","text":"<p>Method that checks if both the dates of study and the dates necessary to generate the daily permanence scores are present in the input data (events + cell footprint).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there is no data for one or more of the needed dates.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def check_needed_dates(self):\n    \"\"\"\n    Method that checks if both the dates of study and the dates necessary to generate\n    the daily permanence scores are present in the input data (events + cell footprint).\n\n    Raises:\n        ValueError: If there is no data for one or more of the needed dates.\n    \"\"\"\n    # needed dates: for each date D, we also need D-1 and D+1\n    # this is built this way so it would also support definition of study\n    # dates that are not consecutive\n    needed_dates = (\n        {d + timedelta(days=1) for d in self.data_period_dates}\n        | set(self.data_period_dates)\n        | {d - timedelta(days=1) for d in self.data_period_dates}\n    )\n\n    # Assert needed dates in event data:\n    self.assert_needed_dates_data_object(SilverEventFlaggedDataObject.ID, needed_dates)\n\n    # Assert needed dates in cell footprint data:\n    self.assert_needed_dates_data_object(SilverCellFootprintDataObject.ID, needed_dates)\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.detect_move_events","title":"<code>detect_move_events(events, cell_footprint)</code>","text":"<p>Detect which of the events are associated to moves according to the distances/times from previous to posterior event and a speed threshold.</p> <p>Parameters:</p> Name Type Description Default <code>events</code> <code>DataFrame</code> <p>events dataframe.</p> required <code>cell_footprint</code> <code>DataFrame</code> <p>cells footprint dataframe.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>events dataframe, with an additional 'is_move' boolean column.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def detect_move_events(self, events: DataFrame, cell_footprint: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Detect which of the events are associated to moves according to the\n    distances/times from previous to posterior event and a speed threshold.\n\n    Args:\n        events (DataFrame): events dataframe.\n        cell_footprint (DataFrame): cells footprint dataframe.\n\n    Returns:\n        DataFrame: events dataframe, with an additional 'is_move' boolean column.\n    \"\"\"\n    # left join -&gt; bring cell footprints to events data:\n    events = events.join(\n        cell_footprint,\n        (events[ColNames.cell_id] == cell_footprint[ColNames.cell_id])\n        &amp; (events[ColNames.year] == cell_footprint[ColNames.year])\n        &amp; (events[ColNames.month] == cell_footprint[ColNames.month])\n        &amp; (events[ColNames.day] == cell_footprint[ColNames.day]),\n        \"left\",\n    ).drop(\n        cell_footprint[ColNames.cell_id],\n        cell_footprint[ColNames.year],\n        cell_footprint[ColNames.month],\n        cell_footprint[ColNames.day],\n    )\n\n    # Add lags of timestamp, cell_id and grid_ids:\n    window = Window.partitionBy(ColNames.user_id).orderBy(ColNames.timestamp)\n    lag_fields = [ColNames.timestamp, ColNames.cell_id, \"grid_ids\"]\n    for lf in lag_fields:\n        events = events.withColumn(f\"{lf}_+1\", F.lag(lf, -1).over(window)).withColumn(\n            f\"{lf}_-1\", F.lag(lf, 1).over(window)\n        )\n\n    # Calculate distance between grid tiles associated to events -1, 0 and +1:\n    # Calculate speeds and determine which rows are moves:\n    events = (\n        events.withColumn(\"dist_0_+1\", self.grid_footprint_distance(F.col(\"grid_ids\"), F.col(\"grid_ids_+1\")))\n        .withColumn(\"dist_-1_0\", self.grid_footprint_distance(F.col(\"grid_ids_-1\"), F.col(\"grid_ids\")))\n        .withColumn(\"dist_-1_+1\", self.grid_footprint_distance(F.col(\"grid_ids_-1\"), F.col(\"grid_ids_+1\")))\n        .withColumn(\n            \"time_difference\",\n            F.unix_timestamp(events[f\"{ColNames.timestamp}_+1\"])\n            - F.unix_timestamp(events[f\"{ColNames.timestamp}_-1\"]),\n        )\n        .withColumn(\"max_dist\", F.greatest(F.col(\"dist_-1_0\") + F.col(\"dist_0_+1\"), F.col(\"dist_-1_+1\")))\n        .withColumn(\"speed\", F.col(\"max_dist\") / F.col(\"time_difference\"))\n        .withColumn(\"is_move\", F.when(F.col(\"speed\") &gt; self.max_speed_thresh, True).otherwise(False))\n        .drop(\n            \"dist_0_+1\",\n            \"dist_-1_0\",\n            \"dist_-1_+1\",\n            \"grid_ids_+1\",\n            \"grid_ids_-1\",\n            \"time_difference\",\n            \"max_dist\",\n            \"speed\",\n        )\n    )\n\n    return events\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.determine_stay_durations","title":"<code>determine_stay_durations(events)</code>","text":"<p>Determine the start time and end time for each stay event.</p> <p>Parameters:</p> Name Type Description Default <code>events</code> <code>DataFrame</code> <p>events dataframe.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>stays dataframe (filtering out moves).</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def determine_stay_durations(self, events: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Determine the start time and end time for each stay event.\n\n    Args:\n        events (DataFrame): events dataframe.\n\n    Returns:\n        DataFrame: stays dataframe (filtering out moves).\n    \"\"\"\n    current_datetime = datetime(self.current_date.year, self.current_date.month, self.current_date.day)\n    night_start_time = current_datetime - timedelta(hours=1)\n    night_end_time = current_datetime + timedelta(hours=9)\n\n    stays = (\n        events\n        # Set applicable time thresholds:\n        .withColumn(\n            \"threshold_-1\",\n            F.when(\n                (F.col(\"cell_id\") == F.col(\"cell_id_-1\"))\n                &amp; (F.col(\"timestamp_-1\") &gt;= night_start_time)\n                &amp; (F.col(\"timestamp\") &lt;= night_end_time),\n                self.max_time_thresh_night,\n            ).otherwise(\n                F.when(F.col(\"cell_id\") == F.col(\"cell_id_-1\"), self.max_time_thresh_day).otherwise(\n                    self.max_time_thresh\n                )\n            ),\n        )\n        .withColumn(\n            \"threshold_+1\",\n            F.when(\n                (F.col(\"cell_id\") == F.col(\"cell_id_+1\"))\n                &amp; (F.col(\"timestamp\") &gt;= night_start_time)\n                &amp; (F.col(\"timestamp_+1\") &lt;= night_end_time),\n                self.max_time_thresh_night,\n            ).otherwise(\n                F.when(F.col(\"cell_id\") == F.col(\"cell_id_+1\"), self.max_time_thresh_day).otherwise(\n                    self.max_time_thresh\n                )\n            ),\n        )\n        # Calculate init_time and end_time according to thresholds and time differences between events:\n        .withColumn(\n            \"init_time\",\n            F.when(\n                F.col(ColNames.timestamp) - F.col(f\"{ColNames.timestamp}_-1\") &lt;= F.col(\"threshold_-1\"),\n                F.col(ColNames.timestamp) - (F.col(ColNames.timestamp) - F.col(f\"{ColNames.timestamp}_-1\")) / 2,\n            ).otherwise(F.col(ColNames.timestamp) - self.max_time_thresh / 2),\n        )\n        .withColumn(\n            \"end_time\",\n            F.when(\n                F.col(f\"{ColNames.timestamp}_+1\") - F.col(ColNames.timestamp) &lt;= F.col(\"threshold_+1\"),\n                F.col(f\"{ColNames.timestamp}_+1\")\n                - (F.col(f\"{ColNames.timestamp}_+1\") - F.col(ColNames.timestamp)) / 2,\n            ).otherwise(F.col(ColNames.timestamp) + self.max_time_thresh / 2),\n        )\n        # Filter out 'move' events (keep only stays), and also drop unneeded columns:\n        .filter(F.col(\"is_move\") == False)\n        .drop(\n            ColNames.cell_id,\n            f\"{ColNames.cell_id}_-1\",\n            f\"{ColNames.cell_id}_+1\",\n            ColNames.timestamp,\n            f\"{ColNames.timestamp}_-1\",\n            f\"{ColNames.timestamp}_+1\",\n            ColNames.mcc,\n            \"is_move\",\n            \"threshold_-1\",\n            \"threshold_+1\",\n        )\n    )\n\n    return stays\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.filter_cell_footprint","title":"<code>filter_cell_footprint(current_date)</code>","text":"<p>Load cell footprints for a specific date.</p> <p>Parameters:</p> Name Type Description Default <code>current_date</code> <code>date</code> <p>current date.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>filtered cell footprint dataframe.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def filter_cell_footprint(self, current_date: date) -&gt; DataFrame:\n    \"\"\"\n    Load cell footprints for a specific date.\n\n    Args:\n        current_date (date): current date.\n\n    Returns:\n        DataFrame: filtered cell footprint dataframe.\n    \"\"\"\n    return self.input_data_objects[SilverCellFootprintDataObject.ID].df.filter(\n        (F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(current_date))\n    )\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.filter_events","title":"<code>filter_events(current_date)</code>","text":"<p>Load events with no errors for a specific date.</p> <p>Parameters:</p> Name Type Description Default <code>current_date</code> <code>date</code> <p>current date.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>filtered events dataframe.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def filter_events(self, current_date: date) -&gt; DataFrame:\n    \"\"\"\n    Load events with no errors for a specific date.\n\n    Args:\n        current_date (date): current date.\n\n    Returns:\n        DataFrame: filtered events dataframe.\n    \"\"\"\n    return self.input_data_objects[SilverEventFlaggedDataObject.ID].df.filter(\n        (F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(current_date))\n        &amp; (F.col(ColNames.error_flag).isin(self.event_error_flags_to_include))\n    )\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.get_grid_id_size_meters","title":"<code>get_grid_id_size_meters(grid_id)</code>  <code>staticmethod</code>","text":"<p>Reach the size of a grid tile from its identifier.</p> <p>Parameters:</p> Name Type Description Default <code>grid_id</code> <code>str</code> <p>ID of the corresponding grid tile.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>grid tile size, in meters.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>@staticmethod\ndef get_grid_id_size_meters(grid_id: str) -&gt; float:\n    \"\"\"\n    Reach the size of a grid tile from its identifier.\n\n    Args:\n        grid_id (str): ID of the corresponding grid tile.\n\n    Returns:\n        float: grid tile size, in meters.\n    \"\"\"\n    size_txt = re.search(r\"^.*m\", grid_id)[0]\n    if size_txt[-2:] == \"km\":\n        size = float(size_txt[:-2]) * 1000\n    else:\n        size = float(size_txt[:-1])\n    return size\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.get_grid_id_vertices","title":"<code>get_grid_id_vertices(grid_id)</code>  <code>staticmethod</code>","text":"<p>Obtain the coordinates of the vertices of a given grid tile.</p> <p>Parameters:</p> Name Type Description Default <code>grid_id</code> <code>str</code> <p>ID of the corresponding grid tile.</p> required <p>Returns:</p> Type Description <code>list[tuple[float]]</code> <p>list[tuple[float]]: list of tuples. Each tuple contains the x, y coordinates of a vertex of the corresponding grid tile.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>@staticmethod\ndef get_grid_id_vertices(grid_id: str) -&gt; list[tuple[float]]:\n    \"\"\"\n    Obtain the coordinates of the vertices of a given grid tile.\n\n    Args:\n        grid_id (str): ID of the corresponding grid tile.\n\n    Returns:\n        list[tuple[float]]: list of tuples. Each tuple contains the x, y\n            coordinates of a vertex of the corresponding grid tile.\n    \"\"\"\n    size = DailyPermanenceScore.get_grid_id_size_meters(grid_id)\n    xleft = float(re.search(r\"E\\s*(\\d+)\", grid_id)[0][1:])\n    ybottom = float(re.search(r\"N\\s*(\\d+)\", grid_id)[0][1:])\n    xright = xleft + size\n    ytop = ybottom + size\n    xs = (xleft, xright)\n    ys = (ybottom, ytop)\n    vertices = list(itertools.product(xs, ys))\n    return vertices\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.grid_footprint_distance","title":"<code>grid_footprint_distance(grid_ids_i, grid_ids_j)</code>  <code>staticmethod</code>","text":"<p>Calculate minimum distance between the grid tiles in one list and the grid tiles in another list, provided as grid tile IDs.</p> <p>Parameters:</p> Name Type Description Default <code>grid_ids_i</code> <code>list[str]</code> <p>IDs of the corresponding grid tiles.</p> required <code>grid_ids_j</code> <code>list[str]</code> <p>IDs of the corresponding grid tiles.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>minimum distance.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>@staticmethod\n@F.udf(returnType=FloatType())\ndef grid_footprint_distance(grid_ids_i: list[str], grid_ids_j: list[str]) -&gt; float:\n    \"\"\"\n    Calculate minimum distance between the grid tiles in one list and the\n    grid tiles in another list, provided as grid tile IDs.\n\n    Args:\n        grid_ids_i (list[str]): IDs of the corresponding grid tiles.\n        grid_ids_j (list[str]): IDs of the corresponding grid tiles.\n\n    Returns:\n        float: minimum distance.\n    \"\"\"\n    # TODO: optimise\n    if not grid_ids_i or not grid_ids_j:\n        return None\n\n    if set(grid_ids_i) == set(grid_ids_j):\n        return 0.0\n\n    gi_vertices = set()\n    for gi in grid_ids_i:\n        gi_vertices |= set(DailyPermanenceScore.get_grid_id_vertices(gi))\n    gj_vertices = set()\n    for gj in grid_ids_j:\n        gj_vertices |= set(DailyPermanenceScore.get_grid_id_vertices(gj))\n\n    min_distance = DailyPermanenceScore.calculate_min_distance_between_point_lists(gi_vertices, gj_vertices)\n\n    return min_distance\n</code></pre>"},{"location":"reference/components/execution/device_activity_statistics/","title":"device_activity_statistics","text":""},{"location":"reference/components/execution/device_activity_statistics/device_activity_statistics/","title":"device_activity_statistics","text":"<p>Module that cleans RAW MNO Event data.</p>"},{"location":"reference/components/execution/device_activity_statistics/device_activity_statistics/#components.execution.device_activity_statistics.device_activity_statistics.DeviceActivityStatistics","title":"<code>DeviceActivityStatistics</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class that removes duplicates from clean MNO Event data</p> Source code in <code>multimno/components/execution/device_activity_statistics/device_activity_statistics.py</code> <pre><code>class DeviceActivityStatistics(Component):\n    \"\"\"\n    Class that removes duplicates from clean MNO Event data\n    \"\"\"\n\n    COMPONENT_ID = \"DeviceActivityStatistics\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n        self.current_date = None\n        self.current_input_events = None\n        self.current_input_network = None\n        self.statistics_df = None\n\n    def initalize_data_objects(self):\n        # Input\n        # TODO: update this to semantically cleaned files after merge\n        self.input_events_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"event_data_silver\")\n        # TODO: Figure out how this would work with coverage areas. We don't have location of cells in those cases\n        self.input_topology_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"network_data_silver\")\n        self.output_statistics_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"device_activity_statistics\")\n\n        self.data_period_start = self.config.get(DeviceActivityStatistics.COMPONENT_ID, \"data_period_start\")\n        self.data_period_end = self.config.get(DeviceActivityStatistics.COMPONENT_ID, \"data_period_end\")\n        self.clear_destination_directory = self.config.get(\n            DeviceActivityStatistics.COMPONENT_ID, \"clear_destination_directory\"\n        )\n        self.local_timezone_str = self.config.get(TIMEZONE_CONFIG_KEY, \"local_timezone\")\n\n        # Create all possible dates between start and end\n        sdate = pd.to_datetime(self.data_period_start)\n        edate = pd.to_datetime(self.data_period_end)\n        self.to_process_dates = list(pd.date_range(sdate, edate, freq=\"d\"))\n\n        # Create all input data objects\n        self.input_data_objects = {SilverEventDataObject.ID: None}\n        if check_if_data_path_exists(self.spark, self.input_events_path):\n            # Single input option, with a filter follwed by it\n            self.input_data_objects[SilverEventDataObject.ID] = SilverEventDataObject(\n                self.spark, self.input_events_path\n            )\n        else:\n            self.logger.warning(f\"Expected path {self.input_events_path} to exist but it does not\")\n\n        self.input_data_objects[SilverNetworkDataObject.ID] = None\n\n        if check_if_data_path_exists(self.spark, self.input_topology_path):\n            # Single input option, with a filter follwed by it\n            self.input_data_objects[SilverNetworkDataObject.ID] = SilverNetworkDataObject(\n                self.spark, self.input_topology_path\n            )\n        else:\n            self.logger.warning(f\"Expected path {self.input_topology_path} to exist but it does not\")\n\n        # Output data objects dictionary\n        self.output_data_objects = {}\n        self.output_data_objects[SilverDeviceActivityStatistics.ID] = SilverDeviceActivityStatistics(\n            self.spark, self.output_statistics_path\n        )\n\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, self.output_statistics_path)\n\n        # Create timezones for transformations\n        self.local_tz = pytz.timezone(self.local_timezone_str)\n        self.utc_tz = pytz.timezone(\"UTC\")\n\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n\n        self.read()\n\n        for current_date in self.to_process_dates:\n            self.current_date = current_date\n\n            start = datetime(year=current_date.year, month=current_date.month, day=current_date.day)\n            start_utc = self.local_tz.localize(start).astimezone(self.utc_tz)\n            end_utc = start_utc + timedelta(days=1) - timedelta(seconds=1)\n\n            # TODO: should this selection also be based on user_id_modulo?\n            self.current_input_events = self.input_data_objects[SilverEventDataObject.ID].df.filter(\n                (F.col(ColNames.timestamp).between(start_utc, end_utc))\n            )\n\n            self.current_input_network = self.input_data_objects[SilverNetworkDataObject.ID].df.filter(\n                (\n                    (F.col(ColNames.year) == current_date.year)\n                    &amp; (F.col(ColNames.month) == current_date.month)\n                    &amp; (F.col(ColNames.day) == current_date.day)\n                )\n            )\n\n            self.transform()\n\n            self.write()\n\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n        # Prepare everything for metrics calculations\n        df_events = self.preprocess_events(self.current_input_events, self.current_input_network)\n\n        # Calculate count of events per user\n        self.statistics_df = df_events.groupby(ColNames.user_id).count().withColumnRenamed(\"count\", ColNames.event_cnt)\n\n        # Calculate count of unique cells per user\n        unique_cell_counts = (\n            df_events.groupBy(ColNames.user_id)\n            .agg(F.countDistinct(ColNames.cell_id))\n            .withColumnRenamed(f\"count(DISTINCT {ColNames.cell_id})\", ColNames.unique_cell_cnt)\n        )\n        self.statistics_df = self.statistics_df.join(unique_cell_counts, on=\"user_id\")\n        # Calculate count of unique locations per user\n        unique_location_counts = df_events.groupBy(ColNames.user_id).agg(\n            F.countDistinct(ColNames.latitude, ColNames.longitude).alias(ColNames.unique_location_cnt)\n        )\n        self.statistics_df = self.statistics_df.join(unique_location_counts, on=\"user_id\")\n\n        # Calculate sum of distances between cells\n        distance_per_user = df_events.groupBy(ColNames.user_id).agg(\n            F.sum(\"distance\").cast(IntegerType()).alias(ColNames.sum_distance_m)\n        )\n        self.statistics_df = self.statistics_df.join(distance_per_user, on=\"user_id\")\n\n        # Calculate number of unique hours in data per user\n        hourly_events_df = df_events.withColumn(\n            ColNames.timestamp, F.date_trunc(\"hour\", F.col(ColNames.timestamp))\n        ).select([ColNames.user_id, ColNames.timestamp])\n        hourly_counts = hourly_events_df.groupBy(ColNames.user_id).agg(\n            F.countDistinct(ColNames.timestamp).alias(ColNames.unique_hour_cnt)\n        )\n        self.statistics_df = self.statistics_df.join(hourly_counts, on=\"user_id\")\n\n        # mean_time_gap\n        mean_time_gaps = df_events.groupBy(ColNames.user_id).agg(\n            F.mean(\"time_gap_s\").cast(IntegerType()).alias(ColNames.mean_time_gap)\n        )\n        self.statistics_df = self.statistics_df.join(mean_time_gaps, on=\"user_id\")\n\n        # stdev_time_gap\n        stddev_time_gaps = df_events.groupBy(ColNames.user_id).agg(\n            F.stddev(\"time_gap_s\").alias(ColNames.stdev_time_gap)\n        )\n        self.statistics_df = self.statistics_df.join(stddev_time_gaps, on=\"user_id\")\n        # Add date to statistics\n        self.statistics_df = self.statistics_df.withColumns(\n            {\n                ColNames.year: F.lit(self.current_date.year).cast(\"smallint\"),\n                ColNames.month: F.lit(self.current_date.month).cast(\"tinyint\"),\n                ColNames.day: F.lit(self.current_date.day).cast(\"tinyint\"),\n            }\n        )\n        # Reorder rows\n        self.statistics_df = self.statistics_df.select([col.name for col in SilverDeviceActivityStatistics.SCHEMA])\n        for col in SilverDeviceActivityStatistics.SCHEMA:\n            self.statistics_df = self.statistics_df.withColumn(\n                col.name, self.statistics_df[col.name].cast(col.dataType)\n            )\n        self.output_data_objects[SilverDeviceActivityStatistics.ID].df = self.statistics_df\n\n        # after each chunk processing clear all Cache to free memory and disk\n        self.spark.catalog.clearCache()\n\n    def preprocess_events(\n        self,\n        df_events: pyspark.sql.dataframe.DataFrame,\n        df_network: pyspark.sql.dataframe.DataFrame,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Preprocesses events dataframe to be able to calculate all metrics. Steps:\n        1. Converts timestamp from UTC to local\n        2. Fills latitude and longitude columns from the location of the cell\n        3. Gets location of next event for each event\n        4. Gets timestamp of next event for each event\n        5. Calculates time gap to next event\n        6. Calculates distance to next event\n\n        Args:\n            df_events (pyspark.sql.dataframe.DataFrame): Events relating to the day that is currently being processed\n            df_network (pyspark.sql.dataframe.DataFrame): Network relating to the day that is currently being processed\n\n        Returns:\n            df_events: Events relating to the day that is currently being processed with\n                extra columns for metric calculation\n        \"\"\"\n\n        # Convert timestamp to local\n        df_events.withColumn(\n            ColNames.timestamp,\n            F.to_utc_timestamp(F.from_utc_timestamp(ColNames.timestamp, self.local_timezone_str), \"UTC\"),\n        )\n        df_events = df_events.withColumns(\n            {\n                ColNames.year: F.year(ColNames.timestamp).cast(\"smallint\"),\n                ColNames.month: F.month(ColNames.timestamp).cast(\"tinyint\"),\n                ColNames.day: F.dayofmonth(ColNames.timestamp).cast(\"tinyint\"),\n            }\n        )\n\n        # Join events with topology data to enable checking unique locations and travelled distances\n        df_events = df_events.join(\n            df_network.select(\n                [\n                    F.col(ColNames.cell_id),\n                    F.col(ColNames.latitude).alias(\"cell_lat\"),\n                    F.col(ColNames.longitude).alias(\"cell_lon\"),\n                ]\n            ),\n            on=ColNames.cell_id,\n            how=\"left\",\n        )\n\n        # Use latitude and longitude if they exist, otherwise use cells location\n        df_events = df_events.withColumn(ColNames.latitude, F.coalesce(ColNames.latitude, \"cell_lat\"))\n        df_events = df_events.withColumn(ColNames.longitude, F.coalesce(ColNames.longitude, \"cell_lon\"))\n\n        # Add timestamp and location of next record\n        window = Window.partitionBy(*[ColNames.user_id_modulo, ColNames.user_id]).orderBy(ColNames.timestamp)\n        df_events = df_events.withColumn(\n            f\"next_{ColNames.timestamp}\",\n            F.lead(F.col(ColNames.timestamp), 1).over(window),\n        )\n        df_events = df_events.withColumn(\n            f\"next_{ColNames.latitude}\",\n            F.lead(F.col(ColNames.latitude), 1).over(window),\n        )\n        df_events = df_events.withColumn(\n            f\"next_{ColNames.longitude}\",\n            F.lead(F.col(ColNames.longitude), 1).over(window),\n        )\n\n        # Calculate time gap to next event\n        df_events = df_events.withColumn(\n            \"time_gap_s\",\n            F.unix_timestamp(f\"next_{ColNames.timestamp}\") - F.unix_timestamp(ColNames.timestamp),\n        )\n\n        # Calculate the distance between current and next event\n        # TODO: check if this is the correct way to calculate, got varying results\n        # There are many ways to calculate distance between points and all of them give different results\n        df_events = df_events.withColumn(\n            \"source_geom\",\n            STF.ST_SetSRID(\n                STC.ST_Point(df_events[ColNames.latitude], df_events[ColNames.longitude]),\n                4326,\n            ),\n        )\n        df_events = df_events.withColumn(\n            \"destination_geom\",\n            STF.ST_SetSRID(\n                STC.ST_Point(\n                    df_events[f\"next_{ColNames.latitude}\"],\n                    df_events[f\"next_{ColNames.longitude}\"],\n                ),\n                4326,\n            ),\n        )\n        df_events = df_events.withColumn(\n            \"distance\",\n            STF.ST_DistanceSphere(df_events[\"source_geom\"], df_events[\"destination_geom\"]),\n        )\n\n        return df_events\n</code></pre>"},{"location":"reference/components/execution/device_activity_statistics/device_activity_statistics/#components.execution.device_activity_statistics.device_activity_statistics.DeviceActivityStatistics.preprocess_events","title":"<code>preprocess_events(df_events, df_network)</code>","text":"<p>Preprocesses events dataframe to be able to calculate all metrics. Steps: 1. Converts timestamp from UTC to local 2. Fills latitude and longitude columns from the location of the cell 3. Gets location of next event for each event 4. Gets timestamp of next event for each event 5. Calculates time gap to next event 6. Calculates distance to next event</p> <p>Parameters:</p> Name Type Description Default <code>df_events</code> <code>DataFrame</code> <p>Events relating to the day that is currently being processed</p> required <code>df_network</code> <code>DataFrame</code> <p>Network relating to the day that is currently being processed</p> required <p>Returns:</p> Name Type Description <code>df_events</code> <code>DataFrame</code> <p>Events relating to the day that is currently being processed with extra columns for metric calculation</p> Source code in <code>multimno/components/execution/device_activity_statistics/device_activity_statistics.py</code> <pre><code>def preprocess_events(\n    self,\n    df_events: pyspark.sql.dataframe.DataFrame,\n    df_network: pyspark.sql.dataframe.DataFrame,\n) -&gt; DataFrame:\n    \"\"\"\n    Preprocesses events dataframe to be able to calculate all metrics. Steps:\n    1. Converts timestamp from UTC to local\n    2. Fills latitude and longitude columns from the location of the cell\n    3. Gets location of next event for each event\n    4. Gets timestamp of next event for each event\n    5. Calculates time gap to next event\n    6. Calculates distance to next event\n\n    Args:\n        df_events (pyspark.sql.dataframe.DataFrame): Events relating to the day that is currently being processed\n        df_network (pyspark.sql.dataframe.DataFrame): Network relating to the day that is currently being processed\n\n    Returns:\n        df_events: Events relating to the day that is currently being processed with\n            extra columns for metric calculation\n    \"\"\"\n\n    # Convert timestamp to local\n    df_events.withColumn(\n        ColNames.timestamp,\n        F.to_utc_timestamp(F.from_utc_timestamp(ColNames.timestamp, self.local_timezone_str), \"UTC\"),\n    )\n    df_events = df_events.withColumns(\n        {\n            ColNames.year: F.year(ColNames.timestamp).cast(\"smallint\"),\n            ColNames.month: F.month(ColNames.timestamp).cast(\"tinyint\"),\n            ColNames.day: F.dayofmonth(ColNames.timestamp).cast(\"tinyint\"),\n        }\n    )\n\n    # Join events with topology data to enable checking unique locations and travelled distances\n    df_events = df_events.join(\n        df_network.select(\n            [\n                F.col(ColNames.cell_id),\n                F.col(ColNames.latitude).alias(\"cell_lat\"),\n                F.col(ColNames.longitude).alias(\"cell_lon\"),\n            ]\n        ),\n        on=ColNames.cell_id,\n        how=\"left\",\n    )\n\n    # Use latitude and longitude if they exist, otherwise use cells location\n    df_events = df_events.withColumn(ColNames.latitude, F.coalesce(ColNames.latitude, \"cell_lat\"))\n    df_events = df_events.withColumn(ColNames.longitude, F.coalesce(ColNames.longitude, \"cell_lon\"))\n\n    # Add timestamp and location of next record\n    window = Window.partitionBy(*[ColNames.user_id_modulo, ColNames.user_id]).orderBy(ColNames.timestamp)\n    df_events = df_events.withColumn(\n        f\"next_{ColNames.timestamp}\",\n        F.lead(F.col(ColNames.timestamp), 1).over(window),\n    )\n    df_events = df_events.withColumn(\n        f\"next_{ColNames.latitude}\",\n        F.lead(F.col(ColNames.latitude), 1).over(window),\n    )\n    df_events = df_events.withColumn(\n        f\"next_{ColNames.longitude}\",\n        F.lead(F.col(ColNames.longitude), 1).over(window),\n    )\n\n    # Calculate time gap to next event\n    df_events = df_events.withColumn(\n        \"time_gap_s\",\n        F.unix_timestamp(f\"next_{ColNames.timestamp}\") - F.unix_timestamp(ColNames.timestamp),\n    )\n\n    # Calculate the distance between current and next event\n    # TODO: check if this is the correct way to calculate, got varying results\n    # There are many ways to calculate distance between points and all of them give different results\n    df_events = df_events.withColumn(\n        \"source_geom\",\n        STF.ST_SetSRID(\n            STC.ST_Point(df_events[ColNames.latitude], df_events[ColNames.longitude]),\n            4326,\n        ),\n    )\n    df_events = df_events.withColumn(\n        \"destination_geom\",\n        STF.ST_SetSRID(\n            STC.ST_Point(\n                df_events[f\"next_{ColNames.latitude}\"],\n                df_events[f\"next_{ColNames.longitude}\"],\n            ),\n            4326,\n        ),\n    )\n    df_events = df_events.withColumn(\n        \"distance\",\n        STF.ST_DistanceSphere(df_events[\"source_geom\"], df_events[\"destination_geom\"]),\n    )\n\n    return df_events\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/","title":"event_cleaning","text":""},{"location":"reference/components/execution/event_cleaning/event_cleaning/","title":"event_cleaning","text":"<p>Module that cleans RAW MNO Event data.</p>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning","title":"<code>EventCleaning</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class that cleans MNO Event data</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>class EventCleaning(Component):\n    \"\"\"\n    Class that cleans MNO Event data\n    \"\"\"\n\n    COMPONENT_ID = \"EventCleaning\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.timestamp_format = self.config.get(EventCleaning.COMPONENT_ID, \"timestamp_format\")\n        self.input_timezone = self.config.get(EventCleaning.COMPONENT_ID, \"input_timezone\")\n        self.local_mcc = self.config.getint(EventCleaning.COMPONENT_ID, \"local_mcc\")\n\n        self.do_bounding_box_filtering = self.config.getboolean(\n            EventCleaning.COMPONENT_ID,\n            \"do_bounding_box_filtering\",\n            fallback=False,\n        )\n\n        self.do_same_location_deduplication = self.config.getboolean(\n            EventCleaning.COMPONENT_ID,\n            \"do_same_location_deduplication\",\n            fallback=False,\n        )\n\n        self.bounding_box = self.config.geteval(EventCleaning.COMPONENT_ID, \"bounding_box\")\n\n        self.spark_data_folder_date_format = self.config.get(\n            EventCleaning.COMPONENT_ID, \"spark_data_folder_date_format\"\n        )\n\n    def initalize_data_objects(self):\n        # Input\n        self.bronze_event_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"event_data_bronze\")\n\n        self.data_period_start = self.config.get(EventCleaning.COMPONENT_ID, \"data_period_start\")\n        self.data_period_end = self.config.get(EventCleaning.COMPONENT_ID, \"data_period_end\")\n        self.data_folder_date_format = self.config.get(EventCleaning.COMPONENT_ID, \"data_folder_date_format\")\n        self.clear_destination_directory = self.config.get(EventCleaning.COMPONENT_ID, \"clear_destination_directory\")\n        self.number_of_partitions = self.config.get(EventCleaning.COMPONENT_ID, \"number_of_partitions\")\n\n        # Create all possible dates between start and end\n        # It is suggested that data is already separated in date folders\n        # with names following self.data_folder_date_format (e.g. 20230101)\n        sdate = pd.to_datetime(self.data_period_start)\n        edate = pd.to_datetime(self.data_period_end)\n        self.to_process_dates = list(pd.date_range(sdate, edate, freq=\"d\"))\n\n        # Create all input data objects\n        self.input_event_data_objects = []\n        self.dates_to_process = []\n        for date in self.to_process_dates:\n            path = f\"{self.bronze_event_path}/year={date.year}/month={date.month}/day={date.day}\"\n            if check_if_data_path_exists(self.spark, path):\n                self.dates_to_process.append(date)\n                self.input_event_data_objects.append(BronzeEventDataObject(self.spark, path))\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")\n        # Output\n        self.output_data_objects = {}\n\n        silver_event_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"event_data_silver\")\n        silver_event_do = SilverEventDataObject(self.spark, silver_event_path)\n        self.output_data_objects[SilverEventDataObject.ID] = silver_event_do\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, silver_event_do.default_path)\n\n        event_syntactic_quality_metrics_by_column_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"event_syntactic_quality_metrics_by_column\"\n        )\n        event_syntactic_quality_metrics_by_column = SilverEventDataSyntacticQualityMetricsByColumn(\n            self.spark, event_syntactic_quality_metrics_by_column_path\n        )\n        self.output_data_objects[SilverEventDataSyntacticQualityMetricsByColumn.ID] = (\n            event_syntactic_quality_metrics_by_column\n        )\n        if self.clear_destination_directory:\n            delete_file_or_folder(\n                self.spark,\n                event_syntactic_quality_metrics_by_column.default_path,\n            )\n\n        self.output_qa_by_column = event_syntactic_quality_metrics_by_column\n\n        event_syntactic_quality_metrics_frequency_distribution_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY,\n            \"event_syntactic_quality_metrics_frequency_distribution\",\n        )\n        event_syntactic_quality_metrics_frequency_distribution = (\n            SilverEventDataSyntacticQualityMetricsFrequencyDistribution(\n                self.spark,\n                event_syntactic_quality_metrics_frequency_distribution_path,\n            )\n        )\n        if self.clear_destination_directory:\n            delete_file_or_folder(\n                self.spark,\n                event_syntactic_quality_metrics_frequency_distribution.default_path,\n            )\n\n        self.output_data_objects[SilverEventDataSyntacticQualityMetricsFrequencyDistribution.ID] = (\n            event_syntactic_quality_metrics_frequency_distribution\n        )\n        # this instance of SilverEventDataSyntacticQualityMetricsFrequencyDistribution class\n        # will be used to write frequency distrobution of each preprocessing date (chunk)\n        # the path argument will be changed dynamically\n        self.output_qa_freq_distribution = event_syntactic_quality_metrics_frequency_distribution\n\n    def read(self):\n        self.current_input_do.read()\n\n    def write(self):\n        self.output_data_objects[SilverEventDataObject.ID].write()\n        self.save_syntactic_quality_metrics_frequency_distribution()\n        self.save_syntactic_quality_metrics_by_column()\n\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        for input_do, current_date in zip(self.input_event_data_objects, self.dates_to_process):\n            self.current_date = current_date\n            self.logger.info(f\"Reading from path {input_do.default_path}\")\n            self.current_input_do = input_do\n            self.read()\n            self.transform()  # Transforms the input_df\n            self.write()\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n        df_events = self.current_input_do.df\n        # cache before each filter function because we appply action count()\n        df_events = df_events.cache()\n\n        self.quality_metrics_distribution_before = df_events.groupBy(ColNames.cell_id, ColNames.user_id).agg(\n            F.count(\"*\").alias(ColNames.initial_frequency)\n        )\n\n        df_events = self.filter_nulls_and_update_qa(\n            df_events,\n            [ColNames.user_id, ColNames.timestamp],\n            self.output_qa_by_column.error_and_transformation_counts,\n        )\n\n        # Check MCC, MNC, PLMN and add domain column\n        df_events = self.filter_invalid_domain_columns_and_update_qa(\n            df_events, self.output_qa_by_column.error_and_transformation_counts\n        )\n\n        # already cached in previous function\n        df_events = self.filter_null_locations_and_update_qa(\n            df_events, self.output_qa_by_column.error_and_transformation_counts\n        )\n\n        df_events = df_events.cache()\n\n        # Remove rows with invalid cell_ids\n        df_events = self.filter_invalid_cell_id_and_update_qa(\n            df_events, self.output_qa_by_column.error_and_transformation_counts\n        )\n\n        df_events = df_events.cache()\n\n        df_events = self.convert_time_column_to_timestamp_and_update_qa(\n            df_events,\n            self.timestamp_format,\n            self.input_timezone,\n            self.output_qa_by_column.error_and_transformation_counts,\n        )\n\n        df_events = df_events.cache()\n        # TODO: discuss is this step even needed (did since it was in Method description)\n        df_events = self.data_period_filter_and_update_qa(\n            df_events,\n            self.data_period_start,\n            self.data_period_end,\n            self.output_qa_by_column.error_and_transformation_counts,\n        )\n\n        if self.do_bounding_box_filtering:\n            df_events = df_events.cache()\n            df_events = self.bounding_box_filtering_and_update_qa(\n                df_events,\n                self.bounding_box,\n                self.output_qa_by_column.error_and_transformation_counts,\n            )\n\n        if self.do_same_location_deduplication:\n            df_events = df_events.cache()\n            df_events = self.remove_same_location_duplicates_and_update_qa(\n                df_events,\n                self.output_qa_by_column.error_and_transformation_counts,\n            )\n\n        self.quality_metrics_distribution_after = df_events.groupBy(ColNames.cell_id, ColNames.user_id).agg(\n            F.count(\"*\").alias(ColNames.final_frequency)\n        )\n\n        # TODO: discuss\n        # if we impose the rule on input data that data in a folder\n        # is of date specified in folder name - maybe better to use F.lit()\n        df_events = df_events.withColumns(\n            {\n                ColNames.year: F.year(ColNames.timestamp).cast(\"smallint\"),\n                ColNames.month: F.month(ColNames.timestamp).cast(\"tinyint\"),\n                ColNames.day: F.dayofmonth(ColNames.timestamp).cast(\"tinyint\"),\n            }\n        )\n\n        # The concept of device demultiplex is implemented here\n        # 1) Creates a modulo column, 2) repartitions according to it 3) sorts data within partitions\n\n        df_events = self.calculate_user_id_modulo(df_events, self.number_of_partitions)\n        df_events = df_events.repartition(ColNames.user_id_modulo)\n        df_events = df_events.sortWithinPartitions(ColNames.user_id, ColNames.timestamp)\n\n        # TODO: remove this, if we decide to save domain column\n        df_events = df_events.select(SilverEventDataObject.SCHEMA.fieldNames())\n\n        self.output_data_objects[SilverEventDataObject.ID].df = self.spark.createDataFrame(\n            df_events.rdd, SilverEventDataObject.SCHEMA\n        )\n        # after each chunk processing clear all Cache to free memory and disk\n        self.spark.catalog.clearCache()\n\n    def save_syntactic_quality_metrics_frequency_distribution(self):\n        \"\"\"\n        Join frequency distribution tables before and after,\n        from after table take only final_frequency and replace nulls with 0.\n        Create additional column date in DateType(),\n        match the schema of SilverEventDataSyntacticQualityMetricsByColumn class\n        Write chunk results in separate folders named by processing date\n        \"\"\"\n\n        # Using outer join and groupby after because Spark can not handle joining with null values in columns\n        self.output_qa_freq_distribution.df = self.quality_metrics_distribution_before.join(\n            self.quality_metrics_distribution_after,\n            [ColNames.cell_id, ColNames.user_id],\n            \"outer\",\n        ).select(self.quality_metrics_distribution_before.columns + [ColNames.final_frequency])\n\n        self.output_qa_freq_distribution.df = self.output_qa_freq_distribution.df.groupBy(\n            ColNames.cell_id, ColNames.user_id\n        ).sum(ColNames.initial_frequency, ColNames.final_frequency)\n        self.output_qa_freq_distribution.df = self.output_qa_freq_distribution.df.withColumnRenamed(\n            f\"sum({ColNames.initial_frequency})\", ColNames.initial_frequency\n        )\n        self.output_qa_freq_distribution.df = self.output_qa_freq_distribution.df.withColumnRenamed(\n            f\"sum({ColNames.final_frequency})\", ColNames.final_frequency\n        )\n\n        self.output_qa_freq_distribution.df = self.output_qa_freq_distribution.df.fillna(0, ColNames.final_frequency)\n\n        self.output_qa_freq_distribution.df = self.output_qa_freq_distribution.df.withColumn(\n            ColNames.date,\n            F.to_date(F.lit(self.current_date), self.spark_data_folder_date_format),\n        )\n\n        self.output_qa_freq_distribution.df = self.spark.createDataFrame(\n            self.output_qa_freq_distribution.df.rdd,\n            self.output_qa_freq_distribution.SCHEMA,\n        )\n\n        self.output_qa_freq_distribution.write()\n\n    def save_syntactic_quality_metrics_by_column(self):\n        \"\"\"\n        Convert output_qa_by_column.error_and_transformation_counts dictionary into spark df.\n        Add additional columns, match schema of SilverEventDataSyntacticQualityMetricsByColumn class.\n        Write results in default path of this class\n        \"\"\"\n        # self.output_qa_by_column( variable, type_of_error, type_of_transformation) : value\n        # 3 Nones to match the expected schema\n        df_tuples = [\n            (variable, type_of_error, type_of_transformation, value)\n            for (\n                variable,\n                type_of_error,\n                type_of_transformation,\n            ), value in self.output_qa_by_column.error_and_transformation_counts.items()\n        ]\n        # clear dictionary after each preprocessed day\n        self.output_qa_by_column.error_and_transformation_counts.clear()\n\n        temp_schema = StructType(\n            [\n                StructField(ColNames.variable, StringType(), nullable=True),\n                StructField(ColNames.type_of_error, ShortType(), nullable=True),\n                StructField(ColNames.type_of_transformation, ShortType(), nullable=True),\n                StructField(ColNames.value, IntegerType(), nullable=False),\n            ]\n        )\n\n        self.output_qa_by_column.df = self.spark.createDataFrame(df_tuples, temp_schema)\n\n        self.output_qa_by_column.df = self.output_qa_by_column.df.withColumns(\n            {\n                ColNames.result_timestamp: F.lit(F.current_timestamp()),\n                ColNames.date: F.to_date(F.lit(self.current_date), self.spark_data_folder_date_format),\n            }\n        ).select(self.output_qa_by_column.SCHEMA.fieldNames())\n\n        self.output_qa_by_column.df = self.spark.createDataFrame(\n            self.output_qa_by_column.df.rdd, self.output_qa_by_column.SCHEMA\n        )\n\n        self.output_qa_by_column.write()\n\n    def filter_nulls_and_update_qa(\n        self,\n        df: pyspark.sql.dataframe.DataFrame,\n        filter_columns: list[str],\n        error_and_transformation_counts: dict[tuple:int],\n    ) -&gt; pyspark.sql.dataframe.DataFrame:\n        \"\"\"\n        Loop throuh filter columns (user_id, timestamp)\n        delete rows that have null in the corresponfing column.\n        Counts the number of filtered records for quality metrics,\n        for user_id also calculates the overall correct values,\n        since null check is the one and only filter for this column\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): df events with possible nulls values\n            filter_columns (list[str], optional): columns to check for nulls\n            error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n        Returns:\n            pyspark.sql.dataframe.DataFrame: df without null values in specified columns\n        \"\"\"\n\n        for filter_column in filter_columns:\n            filtered_df = df.na.drop(how=\"any\", subset=filter_column)\n            error_and_transformation_counts[(filter_column, ErrorTypes.missing_value, None)] += (\n                df.count() - filtered_df.count()\n            )\n            # because timestamp column is then also used in another filters,\n            # and no error count should be done in the last filter\n            if filter_column not in [ColNames.timestamp, ColNames.mcc]:\n                error_and_transformation_counts[(filter_column, ErrorTypes.no_error, None)] += filtered_df.count()\n\n            df = filtered_df.cache()\n\n        return df\n\n    def filter_invalid_mcc_and_update_qa(\n        self,\n        df: pyspark.sql.dataframe.DataFrame,\n        error_and_transformation_counts: dict[tuple:int],\n    ) -&gt; pyspark.sql.dataframe.DataFrame:\n        \"\"\"\n        Remove any rows where the value for mcc is not a 3 digit number (between 100 and 999)\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): dataframe of events\n            error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: Dataframe with erroneous values of mcc filtered out\n        \"\"\"\n\n        filtered_df = df.filter(F.col(ColNames.mcc).between(100, 999))\n        error_and_transformation_counts[(ColNames.mcc, ErrorTypes.out_of_admissible_values, None)] += (\n            df.count() - filtered_df.count()\n        )\n        # because timestamp column is then also used in another filters,\n        # and no error count should be done in the last filter\n        error_and_transformation_counts[(ColNames.mcc, ErrorTypes.no_error, None)] += filtered_df.count()\n\n        df = filtered_df.cache()\n\n        return df\n\n    def filter_invalid_mnc_and_update_qa(\n        self,\n        df: pyspark.sql.dataframe.DataFrame,\n        error_and_transformation_counts: dict[tuple:int],\n    ) -&gt; pyspark.sql.dataframe.DataFrame:\n        \"\"\"\n        Remove any rows where the value for mnc is not a two or three digit\n        numerical string\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): dataframe of events\n            error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: Dataframe with erroneous values of mnc filtered out\n        \"\"\"\n\n        filtered_df = df.filter(\n            ((F.length(F.col(ColNames.mnc)) == 2) | (F.length(F.col(ColNames.mnc)) == 3))\n            &amp; (F.col(ColNames.mnc).cast(\"int\").isNotNull())\n        )\n\n        error_and_transformation_counts[(ColNames.mnc, ErrorTypes.out_of_admissible_values, None)] += (\n            df.count() - filtered_df.count()\n        )\n\n        error_and_transformation_counts[(ColNames.mnc, ErrorTypes.no_error, None)] += filtered_df.count()\n\n        df = filtered_df.cache()\n\n        return df\n\n    def filter_invalid_plmn_and_update_qa(\n        self,\n        df: pyspark.sql.dataframe.DataFrame,\n        error_and_transformation_counts: dict[tuple:int],\n    ) -&gt; pyspark.sql.dataframe.DataFrame:\n        \"\"\"\n        Remove any rows where the value for plmn is not a 5 or 6 digit number (between 10000 and 999999)\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): dataframe of events\n            error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: Dataframe with erroneous values of plmn filtered out\n        \"\"\"\n\n        filtered_df = df.filter(F.col(ColNames.plmn).between(10000, 999999))\n        error_and_transformation_counts[(ColNames.plmn, ErrorTypes.out_of_admissible_values, None)] += (\n            df.count() - filtered_df.count()\n        )\n\n        error_and_transformation_counts[(ColNames.plmn, ErrorTypes.no_error, None)] += filtered_df.count()\n        df = filtered_df.cache()\n\n        return df\n\n    def filter_invalid_domain_columns_and_update_qa(\n        self,\n        df: pyspark.sql.dataframe.DataFrame,\n        error_and_transformation_counts: dict[tuple:int],\n    ) -&gt; pyspark.sql.dataframe.DataFrame:\n        \"\"\"\n        Filters out rows that have no domain columns (mcc, mnc, plmn)\n        Checks MCC and MNC for domestic and inbound data\n        Checks PLMN for outbound data\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): dataframe of events\n            error_and_transformation_counts (_type_): dictionary to track qa\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: Dataframe with erroneous rows removed\n        \"\"\"\n\n        # Filter out columns that have no domain columns\n        filtered_df = df.filter(\n            (F.col(ColNames.plmn).isNotNull() | ((F.col(ColNames.mcc).isNotNull()) &amp; (F.col(ColNames.mnc).isNotNull())))\n        )\n\n        error_and_transformation_counts[(None, ErrorTypes.no_domain, None)] += df.count() - filtered_df.count()\n\n        # Make domain column\n        filtered_df = filtered_df.withColumn(\n            ColNames.domain,\n            F.when(F.col(ColNames.plmn).isNotNull(), ColNames.outbound).otherwise(\n                F.when(F.col(ColNames.mcc) == self.local_mcc, ColNames.domestic).otherwise(ColNames.inbound)\n            ),\n        )\n\n        # For Domestic and inbound check MCC and MNC\n        with_mcc_df = filtered_df.filter(F.col(ColNames.domain) != ColNames.outbound)\n        with_mcc_df = self.filter_invalid_mcc_and_update_qa(with_mcc_df, error_and_transformation_counts)\n        with_mcc_df = self.filter_invalid_mnc_and_update_qa(with_mcc_df, error_and_transformation_counts)\n\n        # Check PLMN for outbound\n        outbound_df = filtered_df.filter(F.col(ColNames.domain) == ColNames.outbound)\n        outbound_df = self.filter_invalid_plmn_and_update_qa(outbound_df, error_and_transformation_counts)\n\n        final_df = with_mcc_df.union(outbound_df)\n\n        return final_df\n\n    def filter_invalid_cell_id_and_update_qa(\n        self,\n        df: pyspark.sql.dataframe.DataFrame,\n        error_and_transformation_counts: dict[tuple:int],\n    ) -&gt; pyspark.sql.dataframe.DataFrame:\n        \"\"\"\n        Remove any rows where the value for cell_id is not a 14 or 15 digit number\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): dataframe of events\n            error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: Dataframe with erroneous values of cell_id filtered out\n        \"\"\"\n\n        filtered_df = df.filter(\n            (\n                ((F.length(F.col(ColNames.cell_id)) == 14) | (F.length(F.col(ColNames.cell_id)) == 15))\n                &amp; (F.col(ColNames.cell_id).cast(\"long\").isNotNull())\n                | F.col(\"cell_id\").isNull()\n            )\n        )\n        error_and_transformation_counts[(ColNames.cell_id, ErrorTypes.out_of_admissible_values, None)] += (\n            df.count() - filtered_df.count()\n        )\n        # because timestamp column is then also used in another filters,\n        # and no error count should be done in the last filter\n        error_and_transformation_counts[(ColNames.cell_id, ErrorTypes.no_error, None)] += filtered_df.count()\n\n        df = filtered_df.cache()\n\n        return df\n\n    def filter_null_locations_and_update_qa(\n        self,\n        df: pyspark.sql.dataframe.DataFrame,\n        error_and_transformation_counts: dict[tuple:int],\n    ) -&gt; pyspark.sql.dataframe.DataFrame:\n        \"\"\"\n        Filter rows with inappropriate location: neither cell_id\n        nor latitude&amp;longitude are specified. Count the number of errors,\n        since this filter depends on several columns \"variable\" in quality metrics table will be None.\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): df with possible null location columns\n            error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: filtered df with correctly specified location\n        \"\"\"\n\n        filtered_df = df.filter(\n            (F.col(ColNames.cell_id).isNotNull())\n            | (F.col(ColNames.longitude).isNotNull() &amp; F.col(ColNames.latitude).isNotNull())\n            | (F.col(ColNames.domain) == ColNames.outbound)\n        )\n\n        error_and_transformation_counts[(None, ErrorTypes.no_location, None)] += df.count() - filtered_df.count()\n\n        return filtered_df\n\n    def convert_time_column_to_timestamp_and_update_qa(\n        self,\n        df: pyspark.sql.dataframe.DataFrame,\n        timestampt_format: str,\n        input_timezone: str,\n        error_and_transformation_counts: dict[tuple:int],\n    ) -&gt; pyspark.sql.dataframe.DataFrame:\n        \"\"\"\n        Based on config params timestampt format and input timezone\n        convert timestampt column from string to timestampt type, if filter rows with failed conversion.\n        Count number of succesful timestampt transformations and number of errors.\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): df with timestampt column\n            timestampt_format (str): expected string format to use in time conversion\n            input_timezone (str): timezone of the input data\n            error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: df with time column parsed to timestamp\n        \"\"\"\n\n        # TODO: Check timestamp validation\n        # Validating timestamp of format '2023-01-03T03:12:00+00:00' raises Exception\n        # Error: Fail to parse '2023-01-03T03:12:00+00:00' in the new parser\n        # Can we use a conditional(F.when) to check if column can be casted?\n        filtered_df = df.withColumn(\n            ColNames.timestamp,\n            F.to_utc_timestamp(\n                F.to_timestamp(ColNames.timestamp, timestampt_format),\n                input_timezone,\n            ),\n        ).filter(F.col(ColNames.timestamp).isNotNull())\n\n        error_and_transformation_counts[\n            (ColNames.timestamp, None, Transformations.converted_timestamp)\n        ] += filtered_df.count()\n        error_and_transformation_counts[(ColNames.timestamp, ErrorTypes.not_right_syntactic_format, None)] += (\n            df.count() - filtered_df.count()\n        )\n\n        return filtered_df\n\n    def data_period_filter_and_update_qa(\n        self,\n        df: pyspark.sql.dataframe.DataFrame,\n        data_period_start: str,\n        data_period_end: str,\n        error_and_transformation_counts: dict[tuple:int],\n    ) -&gt; pyspark.sql.dataframe.DataFrame:\n        \"\"\"\n        Filter rows which timestampt is not in specified date range.\n        Count the number of error rows for quality metrics,\n        and the number of complitely correct timestampt (that pass all corresponding filters)\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): df with timestamp column\n            data_period_start (str): start of date period\n            data_period_end (str): end of date period\n            error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n        Returns:\n            pyspark.sql.dataframe.DataFrame: df with records in specified date period\n        \"\"\"\n        data_period_start = pd.to_datetime(data_period_start)\n        # timedelta is needed to include records happened in data_period_end\n        data_period_end = pd.to_datetime(data_period_end) + pd.Timedelta(days=1)\n        filtered_df = df.filter(F.col(ColNames.timestamp).between(data_period_start, data_period_end))\n\n        error_and_transformation_counts[(ColNames.timestamp, ErrorTypes.out_of_admissible_values, None)] += (\n            df.count() - filtered_df.count()\n        )\n        # TODO: if we decide not to use data_period_filtering don't forget to put this in convert_time_column_to_timestamp\n        error_and_transformation_counts[(ColNames.timestamp, ErrorTypes.no_error, None)] += filtered_df.count()\n\n        return filtered_df\n\n    def bounding_box_filtering_and_update_qa(\n        self,\n        df: pyspark.sql.dataframe.DataFrame,\n        bounding_box: dict,\n        error_and_transformation_counts: dict[tuple:int],\n    ) -&gt; pyspark.sql.dataframe.DataFrame:\n        \"\"\"\n        Filter rows which not null longitude &amp; latitude values are\n        within coordinates of bounding box. Count the number of errors,\n        since this filter depends on several columns \"variable\" in quality metrics table will be None\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): df with longitude &amp; latitude columns\n            bounding_box (dict): coordinates of bounding box in df_events crs\n            error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: filtered df with records within bbox\n        \"\"\"\n        # coordinates of bounding box should be of the same crs of mno data\n        lat_condition = (\n            F.col(ColNames.latitude).between(bounding_box[\"min_lat\"], bounding_box[\"max_lat\"])\n            | F.col(ColNames.latitude).isNull()\n        )\n        lon_condition = (\n            F.col(ColNames.longitude).between(bounding_box[\"min_lon\"], bounding_box[\"max_lon\"])\n            | F.col(ColNames.longitude).isNull()\n        )\n\n        filtered_df = df.filter(lat_condition &amp; lon_condition)\n        error_and_transformation_counts[(None, ErrorTypes.out_of_bounding_box, None)] += (\n            df.count() - filtered_df.count()\n        )\n\n        return filtered_df\n\n    def remove_same_location_duplicates_and_update_qa(\n        self,\n        df: pyspark.sql.dataframe.DataFrame,\n        error_and_transformation_counts: dict[tuple:int],\n    ) -&gt; pyspark.sql.dataframe.DataFrame:\n        \"\"\"\n        Remove rows that have identical records for\n        timestamp, cell_id, longitude, latitude, plmn and user_id.\n        Counts the number of removed records for quality metrics.\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): df events with possible duplicates\n            duplication_counts (dict[tuple, int]): dictionary to track qa\n        Returns:\n            pyspark.sql.dataframe.DataFrame: df without duplicate rows\n            in terms of user_id, cell_id, latitutde, longitude, plmn\n            and timestamp combination\n        \"\"\"\n\n        # if lot and lan are null, they are null for all rows (presumably)\n        # the same goes for cell_id\n\n        deduplicated_df = df.drop_duplicates(\n            [\n                ColNames.user_id,\n                ColNames.cell_id,\n                ColNames.latitude,\n                ColNames.longitude,\n                ColNames.timestamp,\n                ColNames.plmn,\n            ]\n        )\n\n        error_and_transformation_counts[(None, ErrorTypes.same_location_duplicate, None)] += (\n            df.count() - deduplicated_df.count()\n        )\n\n        return deduplicated_df\n\n    def calculate_user_id_modulo(\n        self,\n        df: pyspark.sql.dataframe.DataFrame,\n        modulo_value: int,\n        hex_truncation_end: int = 12,\n    ) -&gt; pyspark.sql.dataframe.DataFrame:\n        \"\"\"Calculates the extra column user_id_modulo, as the result of the modulo function\n        applied on the binary user id column. The modulo value will affect the number of\n        partitions in the final output.\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): df with user_id column\n            modulo_value (int): modulo value to be used when dividing user id.\n            hex_truncation_end (int): to which character truncate the hex, before sending it to conv function\n                and then to modulo. Anything upward of 13 is likely to result in distributional issues,\n                as the modulo value might not correspond to the number of final partitions.\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: dataframe with user_id_modulo column.\n        \"\"\"\n\n        # TODO make hex truncation (substring parameters) as configurable by user?\n\n        df = df.withColumn(\n            ColNames.user_id_modulo,\n            F.conv(\n                F.substring(F.hex(F.col(ColNames.user_id)), 1, hex_truncation_end),\n                16,\n                10,\n            ).cast(\"long\")\n            % F.lit(modulo_value).cast(\"bigint\"),\n        )\n\n        return df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.bounding_box_filtering_and_update_qa","title":"<code>bounding_box_filtering_and_update_qa(df, bounding_box, error_and_transformation_counts)</code>","text":"<p>Filter rows which not null longitude &amp; latitude values are within coordinates of bounding box. Count the number of errors, since this filter depends on several columns \"variable\" in quality metrics table will be None</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>df with longitude &amp; latitude columns</p> required <code>bounding_box</code> <code>dict</code> <p>coordinates of bounding box in df_events crs</p> required <code>error_and_transformation_counts</code> <code>dict[tuple, int]</code> <p>dictionary to track qa</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: filtered df with records within bbox</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>def bounding_box_filtering_and_update_qa(\n    self,\n    df: pyspark.sql.dataframe.DataFrame,\n    bounding_box: dict,\n    error_and_transformation_counts: dict[tuple:int],\n) -&gt; pyspark.sql.dataframe.DataFrame:\n    \"\"\"\n    Filter rows which not null longitude &amp; latitude values are\n    within coordinates of bounding box. Count the number of errors,\n    since this filter depends on several columns \"variable\" in quality metrics table will be None\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): df with longitude &amp; latitude columns\n        bounding_box (dict): coordinates of bounding box in df_events crs\n        error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: filtered df with records within bbox\n    \"\"\"\n    # coordinates of bounding box should be of the same crs of mno data\n    lat_condition = (\n        F.col(ColNames.latitude).between(bounding_box[\"min_lat\"], bounding_box[\"max_lat\"])\n        | F.col(ColNames.latitude).isNull()\n    )\n    lon_condition = (\n        F.col(ColNames.longitude).between(bounding_box[\"min_lon\"], bounding_box[\"max_lon\"])\n        | F.col(ColNames.longitude).isNull()\n    )\n\n    filtered_df = df.filter(lat_condition &amp; lon_condition)\n    error_and_transformation_counts[(None, ErrorTypes.out_of_bounding_box, None)] += (\n        df.count() - filtered_df.count()\n    )\n\n    return filtered_df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.calculate_user_id_modulo","title":"<code>calculate_user_id_modulo(df, modulo_value, hex_truncation_end=12)</code>","text":"<p>Calculates the extra column user_id_modulo, as the result of the modulo function applied on the binary user id column. The modulo value will affect the number of partitions in the final output.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>df with user_id column</p> required <code>modulo_value</code> <code>int</code> <p>modulo value to be used when dividing user id.</p> required <code>hex_truncation_end</code> <code>int</code> <p>to which character truncate the hex, before sending it to conv function and then to modulo. Anything upward of 13 is likely to result in distributional issues, as the modulo value might not correspond to the number of final partitions.</p> <code>12</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: dataframe with user_id_modulo column.</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>def calculate_user_id_modulo(\n    self,\n    df: pyspark.sql.dataframe.DataFrame,\n    modulo_value: int,\n    hex_truncation_end: int = 12,\n) -&gt; pyspark.sql.dataframe.DataFrame:\n    \"\"\"Calculates the extra column user_id_modulo, as the result of the modulo function\n    applied on the binary user id column. The modulo value will affect the number of\n    partitions in the final output.\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): df with user_id column\n        modulo_value (int): modulo value to be used when dividing user id.\n        hex_truncation_end (int): to which character truncate the hex, before sending it to conv function\n            and then to modulo. Anything upward of 13 is likely to result in distributional issues,\n            as the modulo value might not correspond to the number of final partitions.\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: dataframe with user_id_modulo column.\n    \"\"\"\n\n    # TODO make hex truncation (substring parameters) as configurable by user?\n\n    df = df.withColumn(\n        ColNames.user_id_modulo,\n        F.conv(\n            F.substring(F.hex(F.col(ColNames.user_id)), 1, hex_truncation_end),\n            16,\n            10,\n        ).cast(\"long\")\n        % F.lit(modulo_value).cast(\"bigint\"),\n    )\n\n    return df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.convert_time_column_to_timestamp_and_update_qa","title":"<code>convert_time_column_to_timestamp_and_update_qa(df, timestampt_format, input_timezone, error_and_transformation_counts)</code>","text":"<p>Based on config params timestampt format and input timezone convert timestampt column from string to timestampt type, if filter rows with failed conversion. Count number of succesful timestampt transformations and number of errors.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>df with timestampt column</p> required <code>timestampt_format</code> <code>str</code> <p>expected string format to use in time conversion</p> required <code>input_timezone</code> <code>str</code> <p>timezone of the input data</p> required <code>error_and_transformation_counts</code> <code>dict[tuple, int]</code> <p>dictionary to track qa</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: df with time column parsed to timestamp</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>def convert_time_column_to_timestamp_and_update_qa(\n    self,\n    df: pyspark.sql.dataframe.DataFrame,\n    timestampt_format: str,\n    input_timezone: str,\n    error_and_transformation_counts: dict[tuple:int],\n) -&gt; pyspark.sql.dataframe.DataFrame:\n    \"\"\"\n    Based on config params timestampt format and input timezone\n    convert timestampt column from string to timestampt type, if filter rows with failed conversion.\n    Count number of succesful timestampt transformations and number of errors.\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): df with timestampt column\n        timestampt_format (str): expected string format to use in time conversion\n        input_timezone (str): timezone of the input data\n        error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: df with time column parsed to timestamp\n    \"\"\"\n\n    # TODO: Check timestamp validation\n    # Validating timestamp of format '2023-01-03T03:12:00+00:00' raises Exception\n    # Error: Fail to parse '2023-01-03T03:12:00+00:00' in the new parser\n    # Can we use a conditional(F.when) to check if column can be casted?\n    filtered_df = df.withColumn(\n        ColNames.timestamp,\n        F.to_utc_timestamp(\n            F.to_timestamp(ColNames.timestamp, timestampt_format),\n            input_timezone,\n        ),\n    ).filter(F.col(ColNames.timestamp).isNotNull())\n\n    error_and_transformation_counts[\n        (ColNames.timestamp, None, Transformations.converted_timestamp)\n    ] += filtered_df.count()\n    error_and_transformation_counts[(ColNames.timestamp, ErrorTypes.not_right_syntactic_format, None)] += (\n        df.count() - filtered_df.count()\n    )\n\n    return filtered_df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.data_period_filter_and_update_qa","title":"<code>data_period_filter_and_update_qa(df, data_period_start, data_period_end, error_and_transformation_counts)</code>","text":"<p>Filter rows which timestampt is not in specified date range. Count the number of error rows for quality metrics, and the number of complitely correct timestampt (that pass all corresponding filters)</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>df with timestamp column</p> required <code>data_period_start</code> <code>str</code> <p>start of date period</p> required <code>data_period_end</code> <code>str</code> <p>end of date period</p> required <code>error_and_transformation_counts</code> <code>dict[tuple, int]</code> <p>dictionary to track qa</p> required <p>Returns:     pyspark.sql.dataframe.DataFrame: df with records in specified date period</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>def data_period_filter_and_update_qa(\n    self,\n    df: pyspark.sql.dataframe.DataFrame,\n    data_period_start: str,\n    data_period_end: str,\n    error_and_transformation_counts: dict[tuple:int],\n) -&gt; pyspark.sql.dataframe.DataFrame:\n    \"\"\"\n    Filter rows which timestampt is not in specified date range.\n    Count the number of error rows for quality metrics,\n    and the number of complitely correct timestampt (that pass all corresponding filters)\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): df with timestamp column\n        data_period_start (str): start of date period\n        data_period_end (str): end of date period\n        error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n    Returns:\n        pyspark.sql.dataframe.DataFrame: df with records in specified date period\n    \"\"\"\n    data_period_start = pd.to_datetime(data_period_start)\n    # timedelta is needed to include records happened in data_period_end\n    data_period_end = pd.to_datetime(data_period_end) + pd.Timedelta(days=1)\n    filtered_df = df.filter(F.col(ColNames.timestamp).between(data_period_start, data_period_end))\n\n    error_and_transformation_counts[(ColNames.timestamp, ErrorTypes.out_of_admissible_values, None)] += (\n        df.count() - filtered_df.count()\n    )\n    # TODO: if we decide not to use data_period_filtering don't forget to put this in convert_time_column_to_timestamp\n    error_and_transformation_counts[(ColNames.timestamp, ErrorTypes.no_error, None)] += filtered_df.count()\n\n    return filtered_df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.filter_invalid_cell_id_and_update_qa","title":"<code>filter_invalid_cell_id_and_update_qa(df, error_and_transformation_counts)</code>","text":"<p>Remove any rows where the value for cell_id is not a 14 or 15 digit number</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe of events</p> required <code>error_and_transformation_counts</code> <code>dict[tuple, int]</code> <p>dictionary to track qa</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: Dataframe with erroneous values of cell_id filtered out</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>def filter_invalid_cell_id_and_update_qa(\n    self,\n    df: pyspark.sql.dataframe.DataFrame,\n    error_and_transformation_counts: dict[tuple:int],\n) -&gt; pyspark.sql.dataframe.DataFrame:\n    \"\"\"\n    Remove any rows where the value for cell_id is not a 14 or 15 digit number\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): dataframe of events\n        error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: Dataframe with erroneous values of cell_id filtered out\n    \"\"\"\n\n    filtered_df = df.filter(\n        (\n            ((F.length(F.col(ColNames.cell_id)) == 14) | (F.length(F.col(ColNames.cell_id)) == 15))\n            &amp; (F.col(ColNames.cell_id).cast(\"long\").isNotNull())\n            | F.col(\"cell_id\").isNull()\n        )\n    )\n    error_and_transformation_counts[(ColNames.cell_id, ErrorTypes.out_of_admissible_values, None)] += (\n        df.count() - filtered_df.count()\n    )\n    # because timestamp column is then also used in another filters,\n    # and no error count should be done in the last filter\n    error_and_transformation_counts[(ColNames.cell_id, ErrorTypes.no_error, None)] += filtered_df.count()\n\n    df = filtered_df.cache()\n\n    return df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.filter_invalid_domain_columns_and_update_qa","title":"<code>filter_invalid_domain_columns_and_update_qa(df, error_and_transformation_counts)</code>","text":"<p>Filters out rows that have no domain columns (mcc, mnc, plmn) Checks MCC and MNC for domestic and inbound data Checks PLMN for outbound data</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe of events</p> required <code>error_and_transformation_counts</code> <code>_type_</code> <p>dictionary to track qa</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: Dataframe with erroneous rows removed</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>def filter_invalid_domain_columns_and_update_qa(\n    self,\n    df: pyspark.sql.dataframe.DataFrame,\n    error_and_transformation_counts: dict[tuple:int],\n) -&gt; pyspark.sql.dataframe.DataFrame:\n    \"\"\"\n    Filters out rows that have no domain columns (mcc, mnc, plmn)\n    Checks MCC and MNC for domestic and inbound data\n    Checks PLMN for outbound data\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): dataframe of events\n        error_and_transformation_counts (_type_): dictionary to track qa\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: Dataframe with erroneous rows removed\n    \"\"\"\n\n    # Filter out columns that have no domain columns\n    filtered_df = df.filter(\n        (F.col(ColNames.plmn).isNotNull() | ((F.col(ColNames.mcc).isNotNull()) &amp; (F.col(ColNames.mnc).isNotNull())))\n    )\n\n    error_and_transformation_counts[(None, ErrorTypes.no_domain, None)] += df.count() - filtered_df.count()\n\n    # Make domain column\n    filtered_df = filtered_df.withColumn(\n        ColNames.domain,\n        F.when(F.col(ColNames.plmn).isNotNull(), ColNames.outbound).otherwise(\n            F.when(F.col(ColNames.mcc) == self.local_mcc, ColNames.domestic).otherwise(ColNames.inbound)\n        ),\n    )\n\n    # For Domestic and inbound check MCC and MNC\n    with_mcc_df = filtered_df.filter(F.col(ColNames.domain) != ColNames.outbound)\n    with_mcc_df = self.filter_invalid_mcc_and_update_qa(with_mcc_df, error_and_transformation_counts)\n    with_mcc_df = self.filter_invalid_mnc_and_update_qa(with_mcc_df, error_and_transformation_counts)\n\n    # Check PLMN for outbound\n    outbound_df = filtered_df.filter(F.col(ColNames.domain) == ColNames.outbound)\n    outbound_df = self.filter_invalid_plmn_and_update_qa(outbound_df, error_and_transformation_counts)\n\n    final_df = with_mcc_df.union(outbound_df)\n\n    return final_df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.filter_invalid_mcc_and_update_qa","title":"<code>filter_invalid_mcc_and_update_qa(df, error_and_transformation_counts)</code>","text":"<p>Remove any rows where the value for mcc is not a 3 digit number (between 100 and 999)</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe of events</p> required <code>error_and_transformation_counts</code> <code>dict[tuple, int]</code> <p>dictionary to track qa</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: Dataframe with erroneous values of mcc filtered out</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>def filter_invalid_mcc_and_update_qa(\n    self,\n    df: pyspark.sql.dataframe.DataFrame,\n    error_and_transformation_counts: dict[tuple:int],\n) -&gt; pyspark.sql.dataframe.DataFrame:\n    \"\"\"\n    Remove any rows where the value for mcc is not a 3 digit number (between 100 and 999)\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): dataframe of events\n        error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: Dataframe with erroneous values of mcc filtered out\n    \"\"\"\n\n    filtered_df = df.filter(F.col(ColNames.mcc).between(100, 999))\n    error_and_transformation_counts[(ColNames.mcc, ErrorTypes.out_of_admissible_values, None)] += (\n        df.count() - filtered_df.count()\n    )\n    # because timestamp column is then also used in another filters,\n    # and no error count should be done in the last filter\n    error_and_transformation_counts[(ColNames.mcc, ErrorTypes.no_error, None)] += filtered_df.count()\n\n    df = filtered_df.cache()\n\n    return df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.filter_invalid_mnc_and_update_qa","title":"<code>filter_invalid_mnc_and_update_qa(df, error_and_transformation_counts)</code>","text":"<p>Remove any rows where the value for mnc is not a two or three digit numerical string</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe of events</p> required <code>error_and_transformation_counts</code> <code>dict[tuple, int]</code> <p>dictionary to track qa</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: Dataframe with erroneous values of mnc filtered out</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>def filter_invalid_mnc_and_update_qa(\n    self,\n    df: pyspark.sql.dataframe.DataFrame,\n    error_and_transformation_counts: dict[tuple:int],\n) -&gt; pyspark.sql.dataframe.DataFrame:\n    \"\"\"\n    Remove any rows where the value for mnc is not a two or three digit\n    numerical string\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): dataframe of events\n        error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: Dataframe with erroneous values of mnc filtered out\n    \"\"\"\n\n    filtered_df = df.filter(\n        ((F.length(F.col(ColNames.mnc)) == 2) | (F.length(F.col(ColNames.mnc)) == 3))\n        &amp; (F.col(ColNames.mnc).cast(\"int\").isNotNull())\n    )\n\n    error_and_transformation_counts[(ColNames.mnc, ErrorTypes.out_of_admissible_values, None)] += (\n        df.count() - filtered_df.count()\n    )\n\n    error_and_transformation_counts[(ColNames.mnc, ErrorTypes.no_error, None)] += filtered_df.count()\n\n    df = filtered_df.cache()\n\n    return df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.filter_invalid_plmn_and_update_qa","title":"<code>filter_invalid_plmn_and_update_qa(df, error_and_transformation_counts)</code>","text":"<p>Remove any rows where the value for plmn is not a 5 or 6 digit number (between 10000 and 999999)</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe of events</p> required <code>error_and_transformation_counts</code> <code>dict[tuple, int]</code> <p>dictionary to track qa</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: Dataframe with erroneous values of plmn filtered out</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>def filter_invalid_plmn_and_update_qa(\n    self,\n    df: pyspark.sql.dataframe.DataFrame,\n    error_and_transformation_counts: dict[tuple:int],\n) -&gt; pyspark.sql.dataframe.DataFrame:\n    \"\"\"\n    Remove any rows where the value for plmn is not a 5 or 6 digit number (between 10000 and 999999)\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): dataframe of events\n        error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: Dataframe with erroneous values of plmn filtered out\n    \"\"\"\n\n    filtered_df = df.filter(F.col(ColNames.plmn).between(10000, 999999))\n    error_and_transformation_counts[(ColNames.plmn, ErrorTypes.out_of_admissible_values, None)] += (\n        df.count() - filtered_df.count()\n    )\n\n    error_and_transformation_counts[(ColNames.plmn, ErrorTypes.no_error, None)] += filtered_df.count()\n    df = filtered_df.cache()\n\n    return df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.filter_null_locations_and_update_qa","title":"<code>filter_null_locations_and_update_qa(df, error_and_transformation_counts)</code>","text":"<p>Filter rows with inappropriate location: neither cell_id nor latitude&amp;longitude are specified. Count the number of errors, since this filter depends on several columns \"variable\" in quality metrics table will be None.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>df with possible null location columns</p> required <code>error_and_transformation_counts</code> <code>dict[tuple, int]</code> <p>dictionary to track qa</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: filtered df with correctly specified location</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>def filter_null_locations_and_update_qa(\n    self,\n    df: pyspark.sql.dataframe.DataFrame,\n    error_and_transformation_counts: dict[tuple:int],\n) -&gt; pyspark.sql.dataframe.DataFrame:\n    \"\"\"\n    Filter rows with inappropriate location: neither cell_id\n    nor latitude&amp;longitude are specified. Count the number of errors,\n    since this filter depends on several columns \"variable\" in quality metrics table will be None.\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): df with possible null location columns\n        error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: filtered df with correctly specified location\n    \"\"\"\n\n    filtered_df = df.filter(\n        (F.col(ColNames.cell_id).isNotNull())\n        | (F.col(ColNames.longitude).isNotNull() &amp; F.col(ColNames.latitude).isNotNull())\n        | (F.col(ColNames.domain) == ColNames.outbound)\n    )\n\n    error_and_transformation_counts[(None, ErrorTypes.no_location, None)] += df.count() - filtered_df.count()\n\n    return filtered_df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.filter_nulls_and_update_qa","title":"<code>filter_nulls_and_update_qa(df, filter_columns, error_and_transformation_counts)</code>","text":"<p>Loop throuh filter columns (user_id, timestamp) delete rows that have null in the corresponfing column. Counts the number of filtered records for quality metrics, for user_id also calculates the overall correct values, since null check is the one and only filter for this column</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>df events with possible nulls values</p> required <code>filter_columns</code> <code>list[str]</code> <p>columns to check for nulls</p> required <code>error_and_transformation_counts</code> <code>dict[tuple, int]</code> <p>dictionary to track qa</p> required <p>Returns:     pyspark.sql.dataframe.DataFrame: df without null values in specified columns</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>def filter_nulls_and_update_qa(\n    self,\n    df: pyspark.sql.dataframe.DataFrame,\n    filter_columns: list[str],\n    error_and_transformation_counts: dict[tuple:int],\n) -&gt; pyspark.sql.dataframe.DataFrame:\n    \"\"\"\n    Loop throuh filter columns (user_id, timestamp)\n    delete rows that have null in the corresponfing column.\n    Counts the number of filtered records for quality metrics,\n    for user_id also calculates the overall correct values,\n    since null check is the one and only filter for this column\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): df events with possible nulls values\n        filter_columns (list[str], optional): columns to check for nulls\n        error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n    Returns:\n        pyspark.sql.dataframe.DataFrame: df without null values in specified columns\n    \"\"\"\n\n    for filter_column in filter_columns:\n        filtered_df = df.na.drop(how=\"any\", subset=filter_column)\n        error_and_transformation_counts[(filter_column, ErrorTypes.missing_value, None)] += (\n            df.count() - filtered_df.count()\n        )\n        # because timestamp column is then also used in another filters,\n        # and no error count should be done in the last filter\n        if filter_column not in [ColNames.timestamp, ColNames.mcc]:\n            error_and_transformation_counts[(filter_column, ErrorTypes.no_error, None)] += filtered_df.count()\n\n        df = filtered_df.cache()\n\n    return df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.remove_same_location_duplicates_and_update_qa","title":"<code>remove_same_location_duplicates_and_update_qa(df, error_and_transformation_counts)</code>","text":"<p>Remove rows that have identical records for timestamp, cell_id, longitude, latitude, plmn and user_id. Counts the number of removed records for quality metrics.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>df events with possible duplicates</p> required <code>duplication_counts</code> <code>dict[tuple, int]</code> <p>dictionary to track qa</p> required <p>Returns:     pyspark.sql.dataframe.DataFrame: df without duplicate rows     in terms of user_id, cell_id, latitutde, longitude, plmn     and timestamp combination</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>def remove_same_location_duplicates_and_update_qa(\n    self,\n    df: pyspark.sql.dataframe.DataFrame,\n    error_and_transformation_counts: dict[tuple:int],\n) -&gt; pyspark.sql.dataframe.DataFrame:\n    \"\"\"\n    Remove rows that have identical records for\n    timestamp, cell_id, longitude, latitude, plmn and user_id.\n    Counts the number of removed records for quality metrics.\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): df events with possible duplicates\n        duplication_counts (dict[tuple, int]): dictionary to track qa\n    Returns:\n        pyspark.sql.dataframe.DataFrame: df without duplicate rows\n        in terms of user_id, cell_id, latitutde, longitude, plmn\n        and timestamp combination\n    \"\"\"\n\n    # if lot and lan are null, they are null for all rows (presumably)\n    # the same goes for cell_id\n\n    deduplicated_df = df.drop_duplicates(\n        [\n            ColNames.user_id,\n            ColNames.cell_id,\n            ColNames.latitude,\n            ColNames.longitude,\n            ColNames.timestamp,\n            ColNames.plmn,\n        ]\n    )\n\n    error_and_transformation_counts[(None, ErrorTypes.same_location_duplicate, None)] += (\n        df.count() - deduplicated_df.count()\n    )\n\n    return deduplicated_df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.save_syntactic_quality_metrics_by_column","title":"<code>save_syntactic_quality_metrics_by_column()</code>","text":"<p>Convert output_qa_by_column.error_and_transformation_counts dictionary into spark df. Add additional columns, match schema of SilverEventDataSyntacticQualityMetricsByColumn class. Write results in default path of this class</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>def save_syntactic_quality_metrics_by_column(self):\n    \"\"\"\n    Convert output_qa_by_column.error_and_transformation_counts dictionary into spark df.\n    Add additional columns, match schema of SilverEventDataSyntacticQualityMetricsByColumn class.\n    Write results in default path of this class\n    \"\"\"\n    # self.output_qa_by_column( variable, type_of_error, type_of_transformation) : value\n    # 3 Nones to match the expected schema\n    df_tuples = [\n        (variable, type_of_error, type_of_transformation, value)\n        for (\n            variable,\n            type_of_error,\n            type_of_transformation,\n        ), value in self.output_qa_by_column.error_and_transformation_counts.items()\n    ]\n    # clear dictionary after each preprocessed day\n    self.output_qa_by_column.error_and_transformation_counts.clear()\n\n    temp_schema = StructType(\n        [\n            StructField(ColNames.variable, StringType(), nullable=True),\n            StructField(ColNames.type_of_error, ShortType(), nullable=True),\n            StructField(ColNames.type_of_transformation, ShortType(), nullable=True),\n            StructField(ColNames.value, IntegerType(), nullable=False),\n        ]\n    )\n\n    self.output_qa_by_column.df = self.spark.createDataFrame(df_tuples, temp_schema)\n\n    self.output_qa_by_column.df = self.output_qa_by_column.df.withColumns(\n        {\n            ColNames.result_timestamp: F.lit(F.current_timestamp()),\n            ColNames.date: F.to_date(F.lit(self.current_date), self.spark_data_folder_date_format),\n        }\n    ).select(self.output_qa_by_column.SCHEMA.fieldNames())\n\n    self.output_qa_by_column.df = self.spark.createDataFrame(\n        self.output_qa_by_column.df.rdd, self.output_qa_by_column.SCHEMA\n    )\n\n    self.output_qa_by_column.write()\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.save_syntactic_quality_metrics_frequency_distribution","title":"<code>save_syntactic_quality_metrics_frequency_distribution()</code>","text":"<p>Join frequency distribution tables before and after, from after table take only final_frequency and replace nulls with 0. Create additional column date in DateType(), match the schema of SilverEventDataSyntacticQualityMetricsByColumn class Write chunk results in separate folders named by processing date</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>def save_syntactic_quality_metrics_frequency_distribution(self):\n    \"\"\"\n    Join frequency distribution tables before and after,\n    from after table take only final_frequency and replace nulls with 0.\n    Create additional column date in DateType(),\n    match the schema of SilverEventDataSyntacticQualityMetricsByColumn class\n    Write chunk results in separate folders named by processing date\n    \"\"\"\n\n    # Using outer join and groupby after because Spark can not handle joining with null values in columns\n    self.output_qa_freq_distribution.df = self.quality_metrics_distribution_before.join(\n        self.quality_metrics_distribution_after,\n        [ColNames.cell_id, ColNames.user_id],\n        \"outer\",\n    ).select(self.quality_metrics_distribution_before.columns + [ColNames.final_frequency])\n\n    self.output_qa_freq_distribution.df = self.output_qa_freq_distribution.df.groupBy(\n        ColNames.cell_id, ColNames.user_id\n    ).sum(ColNames.initial_frequency, ColNames.final_frequency)\n    self.output_qa_freq_distribution.df = self.output_qa_freq_distribution.df.withColumnRenamed(\n        f\"sum({ColNames.initial_frequency})\", ColNames.initial_frequency\n    )\n    self.output_qa_freq_distribution.df = self.output_qa_freq_distribution.df.withColumnRenamed(\n        f\"sum({ColNames.final_frequency})\", ColNames.final_frequency\n    )\n\n    self.output_qa_freq_distribution.df = self.output_qa_freq_distribution.df.fillna(0, ColNames.final_frequency)\n\n    self.output_qa_freq_distribution.df = self.output_qa_freq_distribution.df.withColumn(\n        ColNames.date,\n        F.to_date(F.lit(self.current_date), self.spark_data_folder_date_format),\n    )\n\n    self.output_qa_freq_distribution.df = self.spark.createDataFrame(\n        self.output_qa_freq_distribution.df.rdd,\n        self.output_qa_freq_distribution.SCHEMA,\n    )\n\n    self.output_qa_freq_distribution.write()\n</code></pre>"},{"location":"reference/components/execution/event_semantic_cleaning/","title":"event_semantic_cleaning","text":""},{"location":"reference/components/execution/event_semantic_cleaning/event_semantic_cleaning/","title":"event_semantic_cleaning","text":"<p>Module that computes semantic checks on event data and adds error flags</p>"},{"location":"reference/components/execution/event_semantic_cleaning/event_semantic_cleaning/#components.execution.event_semantic_cleaning.event_semantic_cleaning.SemanticCleaning","title":"<code>SemanticCleaning</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class that performs semantic checks on event data and adds error flags</p> Source code in <code>multimno/components/execution/event_semantic_cleaning/event_semantic_cleaning.py</code> <pre><code>class SemanticCleaning(Component):\n    \"\"\"\n    Class that performs semantic checks on event data and adds error flags\n    \"\"\"\n\n    COMPONENT_ID = \"SemanticCleaning\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n        self.timestamp = datetime.datetime.now()\n\n        # Read date of study\n        self.date_format = self.config.get(self.COMPONENT_ID, \"date_format\")\n        data_period_start = self.config.get(self.COMPONENT_ID, \"data_period_start\")\n        data_period_end = self.config.get(self.COMPONENT_ID, \"data_period_end\")\n        self.date_of_study: datetime.date = None\n\n        self.do_different_location_deduplication = self.config.getboolean(\n            self.COMPONENT_ID, \"do_different_location_deduplication\"\n        )\n\n        try:\n            self.data_period_start = datetime.datetime.strptime(data_period_start, self.date_format).date()\n        except ValueError as e:\n            self.logger.error(str(e), exc_info=True)\n            raise e\n\n        try:\n            self.data_period_end = datetime.datetime.strptime(data_period_end, self.date_format).date()\n        except ValueError as e:\n            self.logger.error(str(e), exc_info=True)\n            raise e\n\n        self.data_period_dates = [\n            self.data_period_start + datetime.timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n        # Unit: metre\n        self.semantic_min_distance = self.config.getfloat(self.COMPONENT_ID, \"semantic_min_distance_m\")\n\n        # Unit: metre / second\n        self.semantic_min_speed = self.config.getfloat(self.COMPONENT_ID, \"semantic_min_speed_m_s\")\n\n    def initalize_data_objects(self):\n        input_silver_event_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"event_data_silver\")\n        input_silver_network_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"network_data_silver\")\n        output_silver_event_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"event_data_silver_flagged\")\n        output_silver_semantic_metrics_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"event_device_semantic_quality_metrics\"\n        )\n\n        input_silver_network = SilverNetworkDataObject(\n            self.spark,\n            input_silver_network_path,\n            partition_columns=[ColNames.year, ColNames.month, ColNames.day],\n        )\n        input_silver_event = SilverEventDataObject(self.spark, input_silver_event_path)\n        output_silver_event = SilverEventFlaggedDataObject(self.spark, output_silver_event_path)\n        output_silver_semantic_metrics = SilverEventSemanticQualityMetrics(\n            self.spark,\n            output_silver_semantic_metrics_path,\n            partition_columns=[ColNames.year, ColNames.month, ColNames.day],\n        )\n\n        self.input_data_objects = {\n            SilverEventDataObject.ID: input_silver_event,\n            SilverNetworkDataObject.ID: input_silver_network,\n        }\n        self.output_data_objects = {\n            SilverEventFlaggedDataObject.ID: output_silver_event,\n            SilverEventSemanticQualityMetrics.ID: output_silver_semantic_metrics,\n        }\n\n    def transform(self):\n        events_df = self.events_df\n        cells_df = self.cells_df\n\n        # Pushup filter\n        events_df = events_df.filter(\n            F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(self.date_of_study)\n        )\n\n        cells_df = (\n            cells_df.filter(\n                F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n                == F.lit(self.date_of_study)\n            )\n            # Point geometry auxiliar column\n            .withColumn(\n                \"geometry\",\n                STC.ST_Point(F.col(ColNames.latitude), F.col(ColNames.latitude)),\n            ).select(\n                [\n                    F.col(ColNames.cell_id),\n                    F.col(ColNames.latitude).alias(\"cell_lat\"),\n                    F.col(ColNames.longitude).alias(\"cell_lon\"),\n                    F.col(ColNames.valid_date_start),\n                    F.col(ColNames.valid_date_end),\n                    F.col(\"geometry\"),\n                ]\n            )\n        )\n\n        # Perform a left join between events and cell IDs. Non-existent cell IDs will be matched\n        # with null values\n        df = events_df.join(cells_df, on=ColNames.cell_id, how=\"left\")\n\n        # Create error flag column alongside the first error flag: non existent column\n        df = self._flag_non_existent_cell_ids(df)\n\n        # Optional flagging of duplicates with different location info\n        if self.do_different_location_deduplication:\n            df = self._flag_different_location_duplicates(df)\n\n        # Error flag: Check rows which have valid date start and/or valid date end, and flag when timestamp is incompatible\n        df = self._flag_invalid_cell_ids(df)\n\n        # Error flag: suspicious and incorrect events based on location change distance and speed\n        df = self._flag_by_event_location(df)\n\n        # Keep only the necessary columns and remove auxiliar ones\n        df = df.select(SilverEventFlaggedDataObject.SCHEMA.names)\n\n        df.cache()\n\n        # Semantic metrics\n        metrics_df = self._compute_semantic_metrics(df)\n\n        self.output_data_objects[SilverEventFlaggedDataObject.ID].df = df\n        self.output_data_objects[SilverEventSemanticQualityMetrics.ID].df = metrics_df\n\n    def _flag_non_existent_cell_ids(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Method that creates a new integer column with the name of ColNames.error_flag, and\n        sets the corresponding flags to events that refer to non-existent cell IDs. The rest of\n        the column's values are left as null.\n\n        Args:\n            df (DataFrame): PySpark DataFrame resulting from the left join of events and cells\n\n        Returns:\n            DataFrame: same DataFrame as the input with a new error flag column, containing\n                flags for the events with a non existent cell ID\n        \"\"\"\n        df = df.withColumn(\n            ColNames.error_flag,\n            F.when(\n                F.col(\"geometry\").isNull(),\n                F.lit(SemanticErrorType.CELL_ID_NON_EXISTENT),\n            ),\n        )\n        return df\n\n    def _flag_invalid_cell_ids(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Method that finds and flags events which refer to an existent cell ID, but that happened outside the\n        time interval during which the cell was operationals. This flag cannot occur at the same time as a\n        non-existent cell. The rest of values (i.e., those with no previous flags) in the error flag column are left\n        as null.\n        The auxiliar Point geometry column will be set to null for these flagged events.\n\n        Args:\n            df (DataFrame): DataFrame in which invalid cells will be flagged\n\n        Returns:\n            DataFrame: DataFrame with flagged invalid cells\n        \"\"\"\n        df = df.withColumn(\n            ColNames.error_flag,\n            # Leave already flagged rows as is\n            F.when(F.col(ColNames.error_flag).isNotNull(), F.col(ColNames.error_flag)).otherwise(\n                # event happened before the cell was operational, or after the cell was operational\n                F.when(\n                    (\n                        (\n                            F.col(ColNames.valid_date_start).isNotNull()\n                            &amp; (F.col(ColNames.timestamp) &lt; F.col(ColNames.valid_date_start))\n                        )\n                        | (\n                            F.col(ColNames.valid_date_end).isNotNull()\n                            &amp; (F.col(ColNames.timestamp) &gt; F.col(ColNames.valid_date_end))\n                        )\n                    ),\n                    F.lit(SemanticErrorType.CELL_ID_NOT_VALID),\n                )\n            ),\n        )\n\n        df = df.withColumn(\n            \"geometry\",\n            F.when(F.col(ColNames.error_flag).isNotNull(), F.lit(None)).otherwise(F.col(\"geometry\")),\n        )\n        return df\n\n    def _flag_by_event_location(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Method that finds and flags events that are considered to be suspicious or incorrect in\n        terms of their timestamp and cell location with respect to their previous and/or following\n        events.\n        It is assumed that these are the last flags to be raised. Thus, non-flagged events are also\n        set to the no-error-flag value within this method.\n        Args:\n            df (DataFrame): DataFrame in which suspicious and/or incorrect events based on\n                location are to be found and flagged\n\n        Returns:\n            DataFrame: flagged DataFrame with suspicious and/or incorrect events\n        \"\"\"\n        # Windows that comprise all previous (following) rows ordered by time for each user.\n        # Partition pruning\n        # These windows have to be used, as all records have to be kept, and we skip them\n        forward_window = (\n            Window.partitionBy(\n                [\n                    ColNames.year,\n                    ColNames.month,\n                    ColNames.day,\n                    ColNames.user_id_modulo,\n                    ColNames.user_id,\n                ]\n            )\n            .orderBy(ColNames.timestamp)\n            .rowsBetween(Window.currentRow + 1, Window.unboundedFollowing)\n        )\n        backward_window = (\n            Window.partitionBy(\n                [\n                    ColNames.year,\n                    ColNames.month,\n                    ColNames.day,\n                    ColNames.user_id_modulo,\n                    ColNames.user_id,\n                ]\n            )\n            .orderBy(ColNames.timestamp)\n            .rowsBetween(Window.unboundedPreceding, Window.currentRow - 1)\n        )\n\n        # Columns to be evaluated for flags\n        # The order of the definition of these columns appears to affect the physical plan\n        # TODO: find best ordering\n        df = (\n            df\n            # auxiliar column containing timestamps of non-flagged events, and null for flagged events\n            .withColumn(\n                \"filtered_ts\",\n                F.when(F.col(ColNames.error_flag).isNull(), F.col(ColNames.timestamp)).otherwise(None),\n            )\n            .withColumn(\n                \"next_timediff\",  # time b/w curr event and first following non-flagged event timestamp\n                F.when(\n                    F.col(ColNames.error_flag).isNull(),\n                    (\n                        F.first(F.col(\"filtered_ts\"), ignorenulls=True).over(forward_window).cast(LongType())\n                        - F.col(ColNames.timestamp).cast(LongType())\n                    ),\n                ).otherwise(F.lit(None)),\n            )\n            .withColumn(\n                \"prev_timediff\",  # time b/w curr event and last previous non-flagged event timestamp\n                F.when(\n                    F.col(ColNames.error_flag).isNull(),\n                    (\n                        F.col(ColNames.timestamp).cast(LongType())\n                        - F.last(F.col(\"filtered_ts\"), ignorenulls=True).over(backward_window).cast(LongType())\n                    ),\n                ).otherwise(F.lit(None)),\n            )\n            .withColumn(\n                \"next_distance\",  # distance b/w curr location and first following non-flagged event location\n                F.when(\n                    F.col(ColNames.error_flag).isNull(),  # rows not flagged (yet)\n                    STF.ST_DistanceSpheroid(\n                        F.col(\"geometry\"),\n                        F.first(F.col(\"geometry\"), ignorenulls=True).over(forward_window),\n                    ),\n                ).otherwise(F.lit(None)),\n            )\n            .withColumn(\n                \"prev_distance\",  # distance b/w curr location and last previous non-flagged event location\n                F.when(\n                    F.col(ColNames.error_flag).isNull(),  # rows not flagged (yet)\n                    STF.ST_DistanceSpheroid(\n                        F.col(\"geometry\"),\n                        F.last(F.col(\"geometry\"), ignorenulls=True).over(backward_window),\n                    ),\n                ).otherwise(F.lit(None)),\n            )\n            .withColumn(  # mean speed in m/s for euclidean displacemente b/w curr and next non-flagged events\n                \"next_speed\", F.col(\"next_distance\") / F.col(\"next_timediff\")\n            )\n            .withColumn(  # mean speed in m/s for euclidean displacemente b/w curr and previous non-flagged events\n                \"prev_speed\", F.col(\"prev_distance\") / F.col(\"prev_timediff\")\n            )\n        )\n\n        # Conditions that must occur for the two location related error flags\n        incorrect_location_cond = (\n            (F.col(\"prev_speed\") &gt; self.semantic_min_speed)\n            &amp; (F.col(\"prev_distance\") &gt; self.semantic_min_distance)\n            &amp; (F.col(\"next_speed\") &gt; self.semantic_min_speed)\n            &amp; (F.col(\"next_distance\") &gt; self.semantic_min_distance)\n        )\n\n        suspicious_location_cond = F.coalesce(\n            (F.col(\"prev_speed\") &gt; self.semantic_min_speed) &amp; (F.col(\"prev_distance\") &gt; self.semantic_min_distance),\n            F.lit(False),\n        ) | F.coalesce(\n            (F.col(\"next_speed\") &gt; self.semantic_min_speed) &amp; (F.col(\"next_distance\") &gt; self.semantic_min_distance),\n            F.lit(False),\n        )\n        # Set the error flags\n        # NOTE: it is assumed that this is the last flag to be computed. Thus, all non-flagged events\n        # will be set to the code corresponding to no error flags. If new flags are to be added, one might\n        # want to change this.\n        df = df.withColumn(\n            ColNames.error_flag,\n            F.when(\n                F.col(ColNames.error_flag).isNull(),  # for non-flagged events\n                F.when(\n                    incorrect_location_cond,\n                    F.lit(SemanticErrorType.INCORRECT_EVENT_LOCATION),\n                ).otherwise(\n                    F.when(\n                        suspicious_location_cond,\n                        F.lit(SemanticErrorType.SUSPICIOUS_EVENT_LOCATION),\n                    ).otherwise(F.lit(SemanticErrorType.NO_ERROR))\n                ),\n            ).otherwise(F.col(ColNames.error_flag)),\n        )\n\n        return df\n\n    def _flag_different_location_duplicates(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Method that checkes for duplicates of a different location type and flags these rows with the corresponding error flag.\n        A different location duplicate is such where user_id and timestamp columns are identical,\n        but any of the cell_id, latitude or longitude columns are different.\n        In the current implementation, all column rows are counted for a given partition of user_id_modulo, user_id and timestamp.\n\n        Args:\n            df (DataFrame): PySpark DataFrame resulting from the left join of events and cells\n\n        Returns:\n            DataFrame: same DataFrame as the input with a new error flag column, containing\n                flags for the events with identical timestamps, but different cell_id or latitude or longitude column values\n        \"\"\"\n\n        window_dedupl = Window.partitionBy(*[ColNames.user_id_modulo, ColNames.user_id, ColNames.timestamp])\n        # The n\n\n        df = df.withColumn(\n            ColNames.error_flag,\n            F.when(\n                F.count(\"*\").over(window_dedupl) &gt; 1,\n                F.lit(SemanticErrorType.DIFFERENT_LOCATION_DUPLICATE),\n            ).otherwise(F.col(ColNames.error_flag)),\n        )\n\n        return df\n\n    def _compute_semantic_metrics(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Method that computes the semantic quality metrics of the semantic checks.\n        This amounts to counting the number of flagged events after the semantic checks.\n\n        Args:\n            df (DataFrame): Flagged event DataFrame\n\n        Returns:\n            DataFrame: semantic metrics DataFrame\n        \"\"\"\n        metrics_df = (\n            df.groupby(ColNames.error_flag)\n            .agg(F.count(F.col(ColNames.error_flag)).alias(ColNames.value))\n            .withColumnRenamed(ColNames.error_flag, ColNames.type_of_error)\n            .withColumns(\n                {\n                    ColNames.variable: F.lit(ColNames.cell_id),  # currently, only cell_id here\n                    ColNames.year: F.lit(self.date_of_study.year).cast(ShortType()),\n                    ColNames.month: F.lit(self.date_of_study.month).cast(ByteType()),\n                    ColNames.day: F.lit(self.date_of_study.day).cast(ByteType()),\n                }\n            )\n        )\n\n        all_error_codes = [error_name for error_name in dir(SemanticErrorType) if not error_name.startswith(\"__\")]\n        all_error_codes = [\n            Row(\n                **{\n                    ColNames.type_of_error: getattr(SemanticErrorType, error_name),\n                }\n            )\n            for error_name in all_error_codes\n        ]\n\n        all_errors_df = self.spark.createDataFrame(\n            all_error_codes,\n            schema=StructType([SilverEventSemanticQualityMetrics.SCHEMA[ColNames.type_of_error]]),\n        )\n\n        metrics_df = (\n            metrics_df.join(all_errors_df, on=ColNames.type_of_error, how=\"right\")\n            .fillna(\n                {\n                    ColNames.variable: ColNames.cell_id,\n                    ColNames.value: 0,\n                    ColNames.year: self.date_of_study.year,\n                    ColNames.month: self.date_of_study.month,\n                    ColNames.day: self.date_of_study.day,\n                }\n            )\n            .withColumn(ColNames.result_timestamp, F.lit(self.timestamp))\n        )\n\n        return metrics_df\n\n    def execute(self):\n        \"\"\"\n        Method that performs the read, transform and write methods of the component.\n        \"\"\"\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n\n        for date in self.data_period_dates:\n            self.date_of_study = date\n\n            self.events_df = self.input_data_objects[SilverEventDataObject.ID].df.filter(\n                F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n                == F.lit(self.date_of_study)\n            )\n            self.cells_df = self.input_data_objects[SilverNetworkDataObject.ID].df.filter(\n                F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n                == F.lit(self.date_of_study)\n            )\n\n            self.logger.info(f\"Processing data for {date}\")\n            self.transform()\n            self.write()\n            self.logger.info(f\"Finished {date}\")\n\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n</code></pre>"},{"location":"reference/components/execution/event_semantic_cleaning/event_semantic_cleaning/#components.execution.event_semantic_cleaning.event_semantic_cleaning.SemanticCleaning.execute","title":"<code>execute()</code>","text":"<p>Method that performs the read, transform and write methods of the component.</p> Source code in <code>multimno/components/execution/event_semantic_cleaning/event_semantic_cleaning.py</code> <pre><code>def execute(self):\n    \"\"\"\n    Method that performs the read, transform and write methods of the component.\n    \"\"\"\n    self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n    self.read()\n\n    for date in self.data_period_dates:\n        self.date_of_study = date\n\n        self.events_df = self.input_data_objects[SilverEventDataObject.ID].df.filter(\n            F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n            == F.lit(self.date_of_study)\n        )\n        self.cells_df = self.input_data_objects[SilverNetworkDataObject.ID].df.filter(\n            F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n            == F.lit(self.date_of_study)\n        )\n\n        self.logger.info(f\"Processing data for {date}\")\n        self.transform()\n        self.write()\n        self.logger.info(f\"Finished {date}\")\n\n    self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n</code></pre>"},{"location":"reference/components/execution/geozones_grid_mapping/","title":"geozones_grid_mapping","text":""},{"location":"reference/components/execution/geozones_grid_mapping/geozones_grid_mapping/","title":"geozones_grid_mapping","text":"<p>This module is responsible for enrichment of the operational grid with elevation and landuse data.</p>"},{"location":"reference/components/execution/geozones_grid_mapping/geozones_grid_mapping/#components.execution.geozones_grid_mapping.geozones_grid_mapping.GeozonesGridMapping","title":"<code>GeozonesGridMapping</code>","text":"<p>               Bases: <code>Component</code></p> <p>This class is responsible for mapping of zoning data to the operational grid.</p> Source code in <code>multimno/components/execution/geozones_grid_mapping/geozones_grid_mapping.py</code> <pre><code>class GeozonesGridMapping(Component):\n    \"\"\"\n    This class is responsible for mapping of zoning data to the operational grid.\n    \"\"\"\n\n    COMPONENT_ID = \"GeozonesGridMapping\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.zoning_dataset_ids = self.config.geteval(GeozonesGridMapping.COMPONENT_ID, \"dataset_ids\")\n\n    def initalize_data_objects(self):\n\n        # inputs\n        self.clear_destination_directory = self.config.getboolean(\n            GeozonesGridMapping.COMPONENT_ID, \"clear_destination_directory\"\n        )\n        self.zoning_type = self.config.get(GeozonesGridMapping.COMPONENT_ID, \"zoning_type\")\n\n        if self.zoning_type == \"admin\":\n            zoning_data = {\"admin_units_data_bronze\": BronzeAdminUnitsDataObject}\n        elif self.zoning_type == \"other\":\n            zoning_data = {\"geographic_zones_data_bronze\": BronzeGeographicZonesDataObject}\n\n        self.input_data_objects = {}\n        inputs = {\n            \"grid_data_silver\": SilverGridDataObject,\n        } | zoning_data\n\n        for key, value in inputs.items():\n            if self.config.has_option(CONFIG_BRONZE_PATHS_KEY, key):\n                path = self.config.get(CONFIG_BRONZE_PATHS_KEY, key)\n            else:\n                path = self.config.get(CONFIG_SILVER_PATHS_KEY, key)\n            if check_if_data_path_exists(self.spark, path):\n                self.input_data_objects[value.ID] = value(self.spark, path)\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")\n                raise ValueError(f\"Invalid path for {value.ID}: {path}\")\n\n        # outputs\n\n        grid_do_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"geozones_grid_map_data_silver\")\n\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, grid_do_path)\n\n        self.output_data_objects = {}\n        self.output_data_objects[SilverGeozonesGridMapDataObject.ID] = SilverGeozonesGridMapDataObject(\n            self.spark,\n            grid_do_path,\n            [\n                ColNames.dataset_id,\n                ColNames.year,\n                ColNames.month,\n                ColNames.day,\n            ],\n        )\n\n    def execute(self):\n\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n\n        # iterate over each dataset_id and do mapping separately\n        for dataset_id in self.zoning_dataset_ids:\n            self.logger.info(f\"Starting mapping for {dataset_id} dataset...\")\n            self.current_dataset_id = dataset_id\n            self.read()\n            self.transform()\n            self.write()\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}...\")\n\n        current_zoning_sdf = self.input_data_objects[BronzeGeographicZonesDataObject.ID].df\n        current_zoning_sdf = current_zoning_sdf.filter(\n            current_zoning_sdf[ColNames.dataset_id].isin(self.current_dataset_id)\n        )\n        grid_sdf = self.input_data_objects[SilverGridDataObject.ID].df\n\n        zoning_levels = self.get_hierarchy_levels(current_zoning_sdf)\n\n        zone_grid_sdf = self.map_zoning_units_to_grid(grid_sdf, current_zoning_sdf, zoning_levels)\n\n        zone_grid_sdf = self.extract_hierarchy_ids(zone_grid_sdf, current_zoning_sdf, zoning_levels)\n\n        # get year, month, day from the current_zone_sdf year, month, day columns, assign to the zone_grid_sdf\n        first_row = current_zoning_sdf.first()\n\n        zone_grid_sdf = zone_grid_sdf.withColumn(ColNames.year, F.lit(first_row[ColNames.year]))\n        zone_grid_sdf = zone_grid_sdf.withColumn(ColNames.month, F.lit(first_row[ColNames.month]))\n        zone_grid_sdf = zone_grid_sdf.withColumn(ColNames.day, F.lit(first_row[ColNames.day]))\n\n        zone_grid_sdf = zone_grid_sdf.withColumn(ColNames.dataset_id, F.lit(self.current_dataset_id))\n\n        zone_grid_sdf = utils.apply_schema_casting(zone_grid_sdf, SilverGeozonesGridMapDataObject.SCHEMA)\n\n        self.output_data_objects[SilverGeozonesGridMapDataObject.ID].df = zone_grid_sdf\n\n    @staticmethod\n    def get_hierarchy_levels(zone_units_df: DataFrame) -&gt; Dict[str, int]:\n        \"\"\"\n        Returns the distinct hierarchy levels of the zoning units in a sorted order.\n\n        This method takes a DataFrame of zoning units, selects the distinct values in the 'level' column,\n        and returns these levels in a sorted list.\n\n        Args:\n            zone_units_df (DataFrame): A DataFrame containing zoning units data.\n                                    It is expected to have a column named 'level' which\n                                    indicates the hierarchy level of each zoning unit.\n\n        Returns:\n            list: A sorted list of distinct hierarchy levels of the zoning units.\n        \"\"\"\n        levels = [row[ColNames.level] for row in zone_units_df.select(ColNames.level).distinct().collect()]\n\n        return sorted(levels)\n\n    def map_zoning_units_to_grid(\n        self, grid_sdf: DataFrame, zoning_units_df: DataFrame, zoning_levels: list\n    ) -&gt; DataFrame:\n        \"\"\"\n        Maps zoning units to a grid.\n\n        This method takes a DataFrame of grid data and a DataFrame of zoning units data,\n        and maps the zoning units to the grid. The mapping is done based on the maximum zoning level.\n        The method returns a DataFrame that contains the grid data with an additional column for the zoning unit ID.\n        If a grid cell does not intersect with any zoning unit, the zoning unit ID for that cell is set to 'undefined'.\n\n        Args:\n            grid_sdf (DataFrame): A DataFrame containing grid data.\n                                  It is expected to have a column named 'geometry' which indicates the geometry of each grid cell.\n            zoning_units_df (DataFrame): A DataFrame containing zoning units data.\n                                         It is expected to have columns named 'level' and 'geometry'\n                                         which indicate the hierarchy level and the geometry of each zoning unit, respectively.\n            zoning_levels (list): A list of zoning levels.\n\n        Returns:\n            DataFrame: A DataFrame that contains the grid data with an additional column for the zoning unit ID.\n        \"\"\"\n        zoning_units_df = zoning_units_df.filter(F.col(ColNames.level) == max(zoning_levels))\n\n        intersection_sdf = (\n            grid_sdf.alias(\"a\")\n            .join(\n                F.broadcast(zoning_units_df.alias(\"b\")),\n                STP.ST_Intersects(f\"a.{ColNames.geometry}\", f\"b.{ColNames.geometry}\"),\n            )\n            .select(\"a.*\", f\"b.{ColNames.zone_id}\")\n        )\n\n        non_intersection_sdf = grid_sdf.alias(\"a\").join(\n            intersection_sdf.alias(\"b\").select(\n                ColNames.grid_id,\n            ),\n            F.col(f\"a.{ColNames.grid_id}\") == F.col(f\"b.{ColNames.grid_id}\"),\n            \"left_anti\",\n        )\n\n        non_intersection_sdf = non_intersection_sdf.withColumn(ColNames.zone_id, F.lit(\"undefined\"))\n\n        lowest_zone_grid_sdf = intersection_sdf.union(non_intersection_sdf)\n\n        return lowest_zone_grid_sdf\n\n    def extract_hierarchy_ids(\n        self, zone_grid_sdf: DataFrame, zone_units_sdf: DataFrame, zoning_levels: list\n    ) -&gt; DataFrame:\n        \"\"\"\n        Extracts the hierarchy IDs for all zoning levels.\n\n        This method takes a DataFrame of grid data mapped to zoning units and a DataFrame of zoning units data,\n        and extracts the hierarchy IDs for all zoning levels. The hierarchy ID for a grid tile is a string that\n        contains the IDs of the zoning units that the tile intersects with, ordered by the zoning level.\n\n        Args:\n            zone_grid_sdf (DataFrame): A DataFrame containing grid data mapped to zoning units.\n                                       It is expected to have column named 'zone_id' which represents zone id on the lowest level.\n            zone_units_sdf (DataFrame): A DataFrame containing zoning units data.\n                                        It is expected to have columns named 'level', 'zone_id', and 'parent_id'.\n            zoning_levels (int): The number of zoning levels.\n\n        Returns:\n            DataFrame: A DataFrame that contains the grid data with an additional column for the hierarchy ID.\n        \"\"\"\n\n        for level in reversed(zoning_levels):\n\n            current_level_zone_units_sdf = zone_units_sdf.filter(F.col(ColNames.level) == level)\n            if level == max(zoning_levels):\n\n                zone_grid_sdf = zone_grid_sdf.withColumn(ColNames.hierarchical_id, F.col(ColNames.zone_id))\n                zone_grid_sdf = zone_grid_sdf.alias(\"a\").join(\n                    current_level_zone_units_sdf.alias(\"b\"),\n                    F.col(f\"a.{ColNames.zone_id}\") == F.col(f\"b.{ColNames.zone_id}\"),\n                    \"left\",\n                )\n                zone_grid_sdf = zone_grid_sdf.withColumn(\"next_level_join_id\", F.col(f\"b.{ColNames.parent_id}\"))\n\n                zone_grid_sdf = zone_grid_sdf.select(\"a.*\", \"next_level_join_id\")\n\n            else:\n                zone_grid_sdf = zone_grid_sdf.alias(\"a\").join(\n                    current_level_zone_units_sdf.alias(\"b\"),\n                    F.col(f\"a.next_level_join_id\") == F.col(f\"b.{ColNames.zone_id}\"),\n                    \"left\",\n                )\n\n                zone_grid_sdf = zone_grid_sdf.withColumn(\n                    ColNames.hierarchical_id,\n                    F.when(\n                        F.col(f\"b.{ColNames.zone_id}\").isNotNull(),\n                        F.concat(\n                            F.col(f\"b.{ColNames.zone_id}\"),\n                            F.lit(\"|\"),\n                            F.col(f\"{ColNames.hierarchical_id}\"),\n                        ),\n                    ).otherwise(\n                        F.concat(\n                            F.lit(\"undefined\"),\n                            F.lit(\"|\"),\n                            F.col(f\"a.{ColNames.hierarchical_id}\"),\n                        )\n                    ),\n                )\n                zone_grid_sdf = zone_grid_sdf.withColumn(\"next_level_join_id\", F.col(f\"b.{ColNames.parent_id}\"))\n                zone_grid_sdf = zone_grid_sdf.select(\"a.*\", \"next_level_join_id\", ColNames.hierarchical_id)\n\n        return zone_grid_sdf\n</code></pre>"},{"location":"reference/components/execution/geozones_grid_mapping/geozones_grid_mapping/#components.execution.geozones_grid_mapping.geozones_grid_mapping.GeozonesGridMapping.extract_hierarchy_ids","title":"<code>extract_hierarchy_ids(zone_grid_sdf, zone_units_sdf, zoning_levels)</code>","text":"<p>Extracts the hierarchy IDs for all zoning levels.</p> <p>This method takes a DataFrame of grid data mapped to zoning units and a DataFrame of zoning units data, and extracts the hierarchy IDs for all zoning levels. The hierarchy ID for a grid tile is a string that contains the IDs of the zoning units that the tile intersects with, ordered by the zoning level.</p> <p>Parameters:</p> Name Type Description Default <code>zone_grid_sdf</code> <code>DataFrame</code> <p>A DataFrame containing grid data mapped to zoning units.                        It is expected to have column named 'zone_id' which represents zone id on the lowest level.</p> required <code>zone_units_sdf</code> <code>DataFrame</code> <p>A DataFrame containing zoning units data.                         It is expected to have columns named 'level', 'zone_id', and 'parent_id'.</p> required <code>zoning_levels</code> <code>int</code> <p>The number of zoning levels.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame that contains the grid data with an additional column for the hierarchy ID.</p> Source code in <code>multimno/components/execution/geozones_grid_mapping/geozones_grid_mapping.py</code> <pre><code>def extract_hierarchy_ids(\n    self, zone_grid_sdf: DataFrame, zone_units_sdf: DataFrame, zoning_levels: list\n) -&gt; DataFrame:\n    \"\"\"\n    Extracts the hierarchy IDs for all zoning levels.\n\n    This method takes a DataFrame of grid data mapped to zoning units and a DataFrame of zoning units data,\n    and extracts the hierarchy IDs for all zoning levels. The hierarchy ID for a grid tile is a string that\n    contains the IDs of the zoning units that the tile intersects with, ordered by the zoning level.\n\n    Args:\n        zone_grid_sdf (DataFrame): A DataFrame containing grid data mapped to zoning units.\n                                   It is expected to have column named 'zone_id' which represents zone id on the lowest level.\n        zone_units_sdf (DataFrame): A DataFrame containing zoning units data.\n                                    It is expected to have columns named 'level', 'zone_id', and 'parent_id'.\n        zoning_levels (int): The number of zoning levels.\n\n    Returns:\n        DataFrame: A DataFrame that contains the grid data with an additional column for the hierarchy ID.\n    \"\"\"\n\n    for level in reversed(zoning_levels):\n\n        current_level_zone_units_sdf = zone_units_sdf.filter(F.col(ColNames.level) == level)\n        if level == max(zoning_levels):\n\n            zone_grid_sdf = zone_grid_sdf.withColumn(ColNames.hierarchical_id, F.col(ColNames.zone_id))\n            zone_grid_sdf = zone_grid_sdf.alias(\"a\").join(\n                current_level_zone_units_sdf.alias(\"b\"),\n                F.col(f\"a.{ColNames.zone_id}\") == F.col(f\"b.{ColNames.zone_id}\"),\n                \"left\",\n            )\n            zone_grid_sdf = zone_grid_sdf.withColumn(\"next_level_join_id\", F.col(f\"b.{ColNames.parent_id}\"))\n\n            zone_grid_sdf = zone_grid_sdf.select(\"a.*\", \"next_level_join_id\")\n\n        else:\n            zone_grid_sdf = zone_grid_sdf.alias(\"a\").join(\n                current_level_zone_units_sdf.alias(\"b\"),\n                F.col(f\"a.next_level_join_id\") == F.col(f\"b.{ColNames.zone_id}\"),\n                \"left\",\n            )\n\n            zone_grid_sdf = zone_grid_sdf.withColumn(\n                ColNames.hierarchical_id,\n                F.when(\n                    F.col(f\"b.{ColNames.zone_id}\").isNotNull(),\n                    F.concat(\n                        F.col(f\"b.{ColNames.zone_id}\"),\n                        F.lit(\"|\"),\n                        F.col(f\"{ColNames.hierarchical_id}\"),\n                    ),\n                ).otherwise(\n                    F.concat(\n                        F.lit(\"undefined\"),\n                        F.lit(\"|\"),\n                        F.col(f\"a.{ColNames.hierarchical_id}\"),\n                    )\n                ),\n            )\n            zone_grid_sdf = zone_grid_sdf.withColumn(\"next_level_join_id\", F.col(f\"b.{ColNames.parent_id}\"))\n            zone_grid_sdf = zone_grid_sdf.select(\"a.*\", \"next_level_join_id\", ColNames.hierarchical_id)\n\n    return zone_grid_sdf\n</code></pre>"},{"location":"reference/components/execution/geozones_grid_mapping/geozones_grid_mapping/#components.execution.geozones_grid_mapping.geozones_grid_mapping.GeozonesGridMapping.get_hierarchy_levels","title":"<code>get_hierarchy_levels(zone_units_df)</code>  <code>staticmethod</code>","text":"<p>Returns the distinct hierarchy levels of the zoning units in a sorted order.</p> <p>This method takes a DataFrame of zoning units, selects the distinct values in the 'level' column, and returns these levels in a sorted list.</p> <p>Parameters:</p> Name Type Description Default <code>zone_units_df</code> <code>DataFrame</code> <p>A DataFrame containing zoning units data.                     It is expected to have a column named 'level' which                     indicates the hierarchy level of each zoning unit.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>Dict[str, int]</code> <p>A sorted list of distinct hierarchy levels of the zoning units.</p> Source code in <code>multimno/components/execution/geozones_grid_mapping/geozones_grid_mapping.py</code> <pre><code>@staticmethod\ndef get_hierarchy_levels(zone_units_df: DataFrame) -&gt; Dict[str, int]:\n    \"\"\"\n    Returns the distinct hierarchy levels of the zoning units in a sorted order.\n\n    This method takes a DataFrame of zoning units, selects the distinct values in the 'level' column,\n    and returns these levels in a sorted list.\n\n    Args:\n        zone_units_df (DataFrame): A DataFrame containing zoning units data.\n                                It is expected to have a column named 'level' which\n                                indicates the hierarchy level of each zoning unit.\n\n    Returns:\n        list: A sorted list of distinct hierarchy levels of the zoning units.\n    \"\"\"\n    levels = [row[ColNames.level] for row in zone_units_df.select(ColNames.level).distinct().collect()]\n\n    return sorted(levels)\n</code></pre>"},{"location":"reference/components/execution/geozones_grid_mapping/geozones_grid_mapping/#components.execution.geozones_grid_mapping.geozones_grid_mapping.GeozonesGridMapping.map_zoning_units_to_grid","title":"<code>map_zoning_units_to_grid(grid_sdf, zoning_units_df, zoning_levels)</code>","text":"<p>Maps zoning units to a grid.</p> <p>This method takes a DataFrame of grid data and a DataFrame of zoning units data, and maps the zoning units to the grid. The mapping is done based on the maximum zoning level. The method returns a DataFrame that contains the grid data with an additional column for the zoning unit ID. If a grid cell does not intersect with any zoning unit, the zoning unit ID for that cell is set to 'undefined'.</p> <p>Parameters:</p> Name Type Description Default <code>grid_sdf</code> <code>DataFrame</code> <p>A DataFrame containing grid data.                   It is expected to have a column named 'geometry' which indicates the geometry of each grid cell.</p> required <code>zoning_units_df</code> <code>DataFrame</code> <p>A DataFrame containing zoning units data.                          It is expected to have columns named 'level' and 'geometry'                          which indicate the hierarchy level and the geometry of each zoning unit, respectively.</p> required <code>zoning_levels</code> <code>list</code> <p>A list of zoning levels.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame that contains the grid data with an additional column for the zoning unit ID.</p> Source code in <code>multimno/components/execution/geozones_grid_mapping/geozones_grid_mapping.py</code> <pre><code>def map_zoning_units_to_grid(\n    self, grid_sdf: DataFrame, zoning_units_df: DataFrame, zoning_levels: list\n) -&gt; DataFrame:\n    \"\"\"\n    Maps zoning units to a grid.\n\n    This method takes a DataFrame of grid data and a DataFrame of zoning units data,\n    and maps the zoning units to the grid. The mapping is done based on the maximum zoning level.\n    The method returns a DataFrame that contains the grid data with an additional column for the zoning unit ID.\n    If a grid cell does not intersect with any zoning unit, the zoning unit ID for that cell is set to 'undefined'.\n\n    Args:\n        grid_sdf (DataFrame): A DataFrame containing grid data.\n                              It is expected to have a column named 'geometry' which indicates the geometry of each grid cell.\n        zoning_units_df (DataFrame): A DataFrame containing zoning units data.\n                                     It is expected to have columns named 'level' and 'geometry'\n                                     which indicate the hierarchy level and the geometry of each zoning unit, respectively.\n        zoning_levels (list): A list of zoning levels.\n\n    Returns:\n        DataFrame: A DataFrame that contains the grid data with an additional column for the zoning unit ID.\n    \"\"\"\n    zoning_units_df = zoning_units_df.filter(F.col(ColNames.level) == max(zoning_levels))\n\n    intersection_sdf = (\n        grid_sdf.alias(\"a\")\n        .join(\n            F.broadcast(zoning_units_df.alias(\"b\")),\n            STP.ST_Intersects(f\"a.{ColNames.geometry}\", f\"b.{ColNames.geometry}\"),\n        )\n        .select(\"a.*\", f\"b.{ColNames.zone_id}\")\n    )\n\n    non_intersection_sdf = grid_sdf.alias(\"a\").join(\n        intersection_sdf.alias(\"b\").select(\n            ColNames.grid_id,\n        ),\n        F.col(f\"a.{ColNames.grid_id}\") == F.col(f\"b.{ColNames.grid_id}\"),\n        \"left_anti\",\n    )\n\n    non_intersection_sdf = non_intersection_sdf.withColumn(ColNames.zone_id, F.lit(\"undefined\"))\n\n    lowest_zone_grid_sdf = intersection_sdf.union(non_intersection_sdf)\n\n    return lowest_zone_grid_sdf\n</code></pre>"},{"location":"reference/components/execution/grid_enrichment/","title":"grid_enrichment","text":""},{"location":"reference/components/execution/grid_enrichment/grid_enrichment/","title":"grid_enrichment","text":"<p>This module is responsible for enrichment of the operational grid with elevation and landuse data.</p>"},{"location":"reference/components/execution/grid_enrichment/grid_enrichment/#components.execution.grid_enrichment.grid_enrichment.GridEnrichment","title":"<code>GridEnrichment</code>","text":"<p>               Bases: <code>Component</code></p> <p>This class is responsible for enrichment of the operational grid with elevation and landuse data.</p> Source code in <code>multimno/components/execution/grid_enrichment/grid_enrichment.py</code> <pre><code>class GridEnrichment(Component):\n    \"\"\"\n    This class is responsible for enrichment of the operational grid with elevation and landuse data.\n    \"\"\"\n\n    COMPONENT_ID = \"GridEnrichment\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.do_land_cover_enrichment = self.config.getboolean(GridEnrichment.COMPONENT_ID, \"do_landcover_enrichment\")\n        self.transportation_category_buffer_m = self.config.geteval(\n            GridEnrichment.COMPONENT_ID, \"transportation_category_buffer_m\"\n        )\n        self.prior_weights = self.config.geteval(GridEnrichment.COMPONENT_ID, \"prior_weights\")\n        self.ple_coefficient_weights = self.config.geteval(GridEnrichment.COMPONENT_ID, \"ple_coefficient_weights\")\n        # self.spatial_repartition_size_rows = self.config.getint(\n        #     InspireGridGeneration.COMPONENT_ID, \"spatial_repartition_size_rows\"\n        # )\n\n        self.do_elevation_enrichment = self.config.getboolean(GridEnrichment.COMPONENT_ID, \"do_elevation_enrichment\")\n\n        self.prior_calculation_repartition_size = self.config.getint(\n            GridEnrichment.COMPONENT_ID, \"prior_calculation_repartition_size\"\n        )\n\n        self.quadkey_udf = F.udf(utils.latlon_to_quadkey)\n\n        self.spark.sparkContext.setCheckpointDir(self.config.get(CONFIG_PATHS_KEY, \"spark_checkpoint_dir\"))\n\n        # TODO: For now set to default 100, but can be dynamic from config in future\n        self.grid_resolution = 100\n\n        self.grid_generator = InspireGridGenerator(\n            self.spark,\n            self.grid_resolution,\n            ColNames.geometry,\n            ColNames.grid_id,\n            1000,\n        )\n\n    def initalize_data_objects(self):\n\n        # inputs\n        self.clear_destination_directory = self.config.getboolean(\n            GridEnrichment.COMPONENT_ID, \"clear_destination_directory\"\n        )\n\n        self.input_data_objects = {}\n        inputs = {\n            \"grid_data_silver\": SilverGridDataObject,\n            \"transportation_data_bronze\": BronzeTransportationDataObject,\n            \"landuse_data_bronze\": BronzeLanduseDataObject,\n        }\n\n        for key, value in inputs.items():\n            if self.config.has_option(CONFIG_BRONZE_PATHS_KEY, key):\n                path = self.config.get(CONFIG_BRONZE_PATHS_KEY, key)\n            else:\n                path = self.config.get(CONFIG_SILVER_PATHS_KEY, key)\n            if check_if_data_path_exists(self.spark, path):\n                self.input_data_objects[value.ID] = value(self.spark, path)\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")\n                raise ValueError(f\"Invalid path for {value.ID}: {path}\")\n\n        # outputs\n\n        grid_do_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"enriched_grid_data_silver\")\n\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, grid_do_path)\n\n        self.output_data_objects = {}\n        self.output_data_objects[SilverEnrichedGridDataObject.ID] = SilverEnrichedGridDataObject(\n            self.spark, grid_do_path, [ColNames.quadkey]\n        )\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}...\")\n\n        grid_sdf = self.input_data_objects[SilverGridDataObject.ID].df\n\n        if self.do_elevation_enrichment:\n            grid_sdf = self.add_elevation_to_grid()\n\n        if self.do_land_cover_enrichment:\n\n            # Depending on avaliable resources and data size of an area of interest\n            # we might need to split computations into multiple parts using quadkey level\n            grid_parts = []\n            unique_quadkeys = grid_sdf.select(ColNames.quadkey).distinct().collect()\n            self.logger.info(f\"Calculating landuse prior...\")\n            self.logger.info(f\"Unique quadkeys to process: {len(unique_quadkeys)}\")\n            for quadkey in unique_quadkeys:\n                self.logger.info(f\"Processing quadkey {quadkey[ColNames.quadkey]}\")\n                self.current_quadkey = quadkey[ColNames.quadkey]\n                current_grid_part = grid_sdf.filter(F.col(ColNames.quadkey) == F.lit(self.current_quadkey))\n\n                # map landuse and roads to grid tiles and get landuse ratios\n                current_grid_part = self.map_landuse_to_grid(current_grid_part)\n\n                # Calculate the weighted sums\n                current_grid_part = self.calculated_landuse_ratios_weighted_sum(\n                    current_grid_part, self.prior_weights, \"weighted_sum\"\n                )\n\n                # calculate PLE coefficient\n                current_grid_part = self.calculated_landuse_ratios_weighted_sum(\n                    current_grid_part,\n                    self.ple_coefficient_weights,\n                    ColNames.ple_coefficient,\n                )\n\n                # TODO: asses if it would be possible to use persist instead of checkpoint\n                current_grid_part = current_grid_part.checkpoint()\n\n                # clear the cache\n                self.spark.catalog.clearCache()\n\n                grid_parts.append(current_grid_part)\n\n            grid_sdf = reduce(lambda x, y: x.union(y), grid_parts)\n            grid_sdf = grid_sdf.dropDuplicates([ColNames.grid_id])\n\n            grid_sdf = self.calculate_landuse_prior(grid_sdf)\n\n        grid_sdf = self.grid_generator.grid_ids_to_centroids(grid_sdf)\n\n        grid_sdf = grid_sdf.orderBy(\"quadkey\")\n        grid_sdf = grid_sdf.repartition(\"quadkey\")\n\n        # Cast column types to DO schema, add missing columns manually\n        df_columns = set(grid_sdf.columns)\n        schema_columns = set(field.name for field in SilverEnrichedGridDataObject.SCHEMA.fields)\n        missing_columns = schema_columns - df_columns\n\n        for column in missing_columns:\n            grid_sdf = grid_sdf.withColumn(\n                column,\n                F.lit(None).cast(SilverEnrichedGridDataObject.SCHEMA[column].dataType),\n            )\n\n        grid_sdf = utils.apply_schema_casting(grid_sdf, SilverEnrichedGridDataObject.SCHEMA)\n\n        self.output_data_objects[SilverEnrichedGridDataObject.ID].df = grid_sdf\n\n    def add_elevation_to_grid(self):\n        # TODO: implement elevation enrichment\n        pass\n\n    def map_landuse_to_grid(self, grid_sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Maps land use and transportation data to a grid.\n\n        This function takes a grid DataFrame, prepares the transportation and land use data\n        by filtering and cutting it to the extent of the current quadkey,\n        and then merges the prepared data with the grid. It also calculates the land use ratios per tile in the grid.\n\n        Args:\n            grid_sdf (pyspark.sql.DataFrame): The grid DataFrame to which the land use and transportation data will be mapped.\n\n        Returns:\n            pyspark.sql.DataFrame: The grid DataFrame with the mapped land use\n            and transportation data as ratios of a total area.\n\n        Raises:\n            Warning: If no data is found for the current quadkey,\n            a warning is logged and the function returns the grid DataFrame populated with empty ratios.\n        \"\"\"\n\n        current_quadkey_extent = utils.quadkey_to_extent(self.current_quadkey)\n        # prepare roads\n        transportation_sdf = self.input_data_objects[BronzeTransportationDataObject.ID].df.select(\n            ColNames.category,\n            ColNames.geometry,\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n        )\n\n        transportation_sdf = utils.filter_geodata_to_extent(transportation_sdf, current_quadkey_extent, 3035)\n\n        transportation_sdf = self.calculate_transportation_buffer(\n            transportation_sdf, self.transportation_category_buffer_m\n        )\n        transportation_sdf = transportation_sdf.withColumn(ColNames.category, F.lit(\"roads\"))\n        # This is needed to reduce number of geometries so all small roads are merged into one multipolygon based on quadkey\n        transportation_sdf = self.merge_transportation_by_grid(transportation_sdf, 1000)\n\n        transportation_sdf.persist(StorageLevel.MEMORY_AND_DISK)\n        transportation_sdf.count()\n        transportation_sdf = transportation_sdf.withColumn(\"quadkey\", F.lit(self.current_quadkey))\n        transportation_sdf.coalesce(1).write.mode(\"overwrite\").format(\"geoparquet\").partitionBy(\"quadkey\").save(\n            \"/opt/mobloc_data/roads_merged_test\"\n        )\n\n        transportation_sdf = transportation_sdf.select(\n            ColNames.category,\n            ColNames.geometry,\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n        )\n\n        self.logger.info(\"Roads prepared\")\n\n        # prepare landuse\n        landuse_sdf = self.input_data_objects[BronzeLanduseDataObject.ID].df.select(\n            ColNames.category,\n            ColNames.geometry,\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n        )\n\n        landuse_sdf = utils.filter_geodata_to_extent(landuse_sdf, current_quadkey_extent, 3035)\n        landuse_sdf = utils.cut_geodata_to_extent(landuse_sdf, current_quadkey_extent, 3035)\n\n        landuse_sdf.persist(StorageLevel.MEMORY_AND_DISK)\n        landuse_sdf.count()\n        self.logger.info(\"Landuse prepared\")\n\n        # merge roads with landuse\n        landuse_sdf = utils.cut_polygons_with_mask_polygons(\n            landuse_sdf,\n            transportation_sdf,\n            [\n                ColNames.category,\n                ColNames.geometry,\n                ColNames.year,\n                ColNames.month,\n                ColNames.day,\n            ],\n            False,\n            ColNames.geometry,\n        )\n\n        landuse_roads_sdf = landuse_sdf.union(transportation_sdf)\n\n        # blow up too big landuse polygons\n        # TODO: make vertices number parameter\n        landuse_roads_sdf = landuse_roads_sdf.withColumn(\n            ColNames.geometry, STF.ST_SubDivideExplode(ColNames.geometry, 1000)\n        )\n        landuse_roads_sdf = utils.fix_geometry(landuse_roads_sdf, 3)\n\n        landuse_roads_sdf.persist(StorageLevel.MEMORY_AND_DISK)\n        land_use_count = landuse_roads_sdf.count()\n\n        if land_use_count == 0:\n            self.logger.warning(f\"No data found for quadkey {self.current_quadkey}. Skipping\")\n            return self.populate_grid_tiles_with_empty_ratios(grid_sdf).drop(\"geometry\")\n\n        self.logger.info(\"Landuse and roads merged\")\n\n        # count landuse types ratios in grid cells\n        grid_tiles = self.grid_generator.grid_ids_to_tiles(grid_sdf)\n        grid_tiles = grid_tiles.withColumn(\"area\", STF.ST_Area(ColNames.geometry))\n\n        grid_tiles = grid_tiles.persist(StorageLevel.MEMORY_AND_DISK)\n        grid_tiles.count()\n\n        grid_tiles = self.calculate_landuse_ratios_per_tile(grid_tiles, landuse_roads_sdf)\n\n        transportation_sdf.unpersist()\n        landuse_roads_sdf.unpersist()\n        landuse_sdf.unpersist()\n\n        return grid_tiles\n\n    def calculated_landuse_ratios_weighted_sum(\n        self,\n        grid_tiles: DataFrame,\n        weights_dict: Dict[str, float],\n        sum_column_name: str,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Calculates the weighted sum of land use ratios for each grid tile.\n\n        This function takes a DataFrame of grid tiles and a dictionary of weights for each land use ratio.\n        It multiplies each land use ratio by its corresponding weight and then sums these weighted ratios\n        to calculate a weighted sum for each grid tile.\n\n        Args:\n            grid_tiles (pyspark.sql.DataFrame): The DataFrame of grid tiles with landuse ratios.\n            weights_dict (dict): A dictionary where the keys are the names of the land use ratios\n            and the values are the corresponding weights.\n            sum_column_name (str): The name of the new column that will contain the weighted sums.\n\n        Returns:\n            pyspark.sql.DataFrame: The DataFrame of grid tiles with the new column of weighted sums.\n        \"\"\"\n\n        weighted_columns = [F.col(f\"{ratio}_ratio\") * weight for ratio, weight in weights_dict.items()]\n        grid_tiles = grid_tiles.withColumn(sum_column_name, sum(weighted_columns))\n\n        return grid_tiles\n\n    def calculate_landuse_ratios_per_tile(self, grid_tiles: DataFrame, landuse_sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Calculates the land use ratios for each tile in a grid.\n\n        This function takes a DataFrame of grid tiles and a DataFrame of land use data.\n        For each land use class, it calculates the ratio of the area of the class that intersects\n        with each grid tile to the area of the tile.\n        If a tile does not intersect with a land use class, the ratio for that class is set to 0.0.\n        If a tile does not intersect with any land use class, the 'open_area_ratio' is set to 1.0.\n\n        Args:\n            grid_tiles (pyspark.sql.DataFrame): The DataFrame of grid tiles.\n            landuse_sdf (pyspark.sql.DataFrame): The DataFrame of land use data.\n\n        Returns:\n            pyspark.sql.DataFrame: The DataFrame of grid tiles with the new columns of land use ratios.\n        \"\"\"\n\n        classes = [prior_weights for prior_weights in self.prior_weights.keys()]\n\n        for landuse_class in classes:\n\n            class_area_ratio_sdf = self.find_intersection_ratio(grid_tiles, landuse_sdf, landuse_class)\n            if class_area_ratio_sdf.count() == 0:\n                grid_tiles = grid_tiles.withColumn(f\"{landuse_class}_ratio\", F.lit(0.0))\n            else:\n                grid_tiles = grid_tiles.join(class_area_ratio_sdf, \"grid_id\", \"left\")\n            grid_tiles = grid_tiles.persist(StorageLevel.MEMORY_AND_DISK)\n            grid_tiles.count()\n            self.logger.info(f\"Landuse class {landuse_class} ratio per tile calculated\")\n\n        # Fill null values in the DataFrame with a specific float number, e.g., 0.0\n        grid_tiles = grid_tiles.fillna(0.0)\n\n        # Update 'open_areas_ratio' to 1.0 if all other specific ratio columns are 0.0\n        # TODO: remove hardcoded values\n        grid_tiles = grid_tiles.withColumn(\n            \"open_area_ratio\",\n            F.when(\n                (F.col(\"roads_ratio\") == 0.0)\n                &amp; (F.col(\"residential_builtup_ratio\") == 0.0)\n                &amp; (F.col(\"other_builtup_ratio\") == 0.0)\n                &amp; (F.col(\"forest_ratio\") == 0.0)\n                &amp; (F.col(\"water_ratio\") == 0.0),\n                1.0,\n            ).otherwise(F.col(\"open_area_ratio\")),\n        ).drop(\"geometry\", \"area\")\n\n        return grid_tiles\n\n    def populate_grid_tiles_with_empty_ratios(self, grid_tiles: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Populates grid tiles with empty land use ratios.\n\n        This function takes a DataFrame of grid tiles and adds a new column for each land use class in weights dict.\n        Each new column is initialized with a value of 0.0, representing an empty land use ratio.\n\n        Args:\n            grid_tiles (pyspark.sql.DataFrame): The DataFrame of grid tiles.\n\n        Returns:\n            pyspark.sql.DataFrame: The DataFrame of grid tiles with the new columns of empty land use ratios.\n        \"\"\"\n\n        classes = [prior_weights for prior_weights in self.prior_weights.keys()]\n\n        for landuse_class in classes:\n            grid_tiles = grid_tiles.withColumn(f\"{landuse_class}_ratio\", F.lit(0.0))\n\n        return grid_tiles\n\n    def calculate_landuse_prior(self, grid_tiles: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Calculates the prior probability of land use for each tile in a grid.\n\n        This function takes a DataFrame of grid tiles, each with a 'weighted_sum' column representing the weighted sum of land use ratios.\n        It calculates the total weighted sum across all tiles and then divides the weighted sum of each tile by this\n        total to calculate the prior probability of land use for each tile.\n\n        Args:\n            grid_tiles (pyspark.sql.DataFrame): The DataFrame of grid tiles.\n\n        Returns:\n            pyspark.sql.DataFrame: The DataFrame of grid tiles with the new column of prior probabilities.\n        \"\"\"\n\n        # Compute total weighted sum for normalization\n        total_weighted_sum = grid_tiles.select(F.sum(\"weighted_sum\").alias(\"total_weighted_sum\")).collect()[0][\n            \"total_weighted_sum\"\n        ]\n\n        # Normalize the weighted sum across all grid tiles\n        grid_tiles = grid_tiles.withColumn(ColNames.prior_probability, F.col(\"weighted_sum\") / total_weighted_sum)\n\n        return grid_tiles\n\n    def calculate_transportation_buffer(\n        self, transportation_sdf: DataFrame, category_buffer_m: Dict[str, float]\n    ) -&gt; DataFrame:\n        \"\"\"\n        Calculates the buffer for each transportation feature based on its category.\n\n        This function takes a DataFrame of transportation features and a dictionary mapping categories to buffer distances.\n        It assigns the appropriate buffer distance to each feature based on its category and then calculates the buffer geometry.\n        The buffer geometry replaces the original geometry of each feature.\n\n        Args:\n            transportation_sdf (pyspark.sql.DataFrame): The DataFrame of transportation features.\n            category_buffer_m (dict): A dictionary mapping categories to buffer distances.\n\n        Returns:\n            pyspark.sql.DataFrame: The DataFrame of transportation features with the buffer geometries.\n        \"\"\"\n\n        transportation_sdf = self.assign_mapping_values(\n            transportation_sdf,\n            category_buffer_m,\n            ColNames.category,\n            \"buffer_dist\",\n            category_buffer_m[\"unknown\"],\n        )\n\n        transportation_sdf = transportation_sdf.filter(F.col(\"buffer_dist\") != 0.0)\n\n        transportation_sdf = transportation_sdf.withColumn(\n            ColNames.geometry,\n            STF.ST_Buffer(ColNames.geometry, F.col(\"buffer_dist\")),\n        ).drop(\"buffer_dist\")\n\n        return transportation_sdf\n\n    def merge_transportation_by_grid(self, transportation_sdf: DataFrame, resolution: int) -&gt; DataFrame:\n        \"\"\"\n        Merges transportation data with a generated grid based on the specified resolution.\n\n        This function takes a DataFrame containing transportation data and a grid resolution.\n        It first generates a grid that covers the extent of the transportation data. Then, it\n        intersects the transportation data with this grid, merging the transportation geometries\n        that fall within each grid cell. The result is a DataFrame where each row represents a\n        grid cell, aggregated by transportation category and date, with a merged geometry for\n        all transportation data within that cell.\n\n        Parameters:\n        - transportation_sdf (DataFrame): A Spark DataFrame containing transportation data.\n        - resolution (int): The resolution of the grid to generate, specified as the length\n        of the side of each square grid cell in meters.\n\n        Returns:\n        - DataFrame: A Spark DataFrame where each row represents merged transportation data.\n        \"\"\"\n        grid_gen = InspireGridGenerator(self.spark, resolution)\n        for_extent = transportation_sdf.groupBy().agg(STA.ST_Envelope_Aggr(\"geometry\").alias(\"envelope\"))\n        for_extent = for_extent.withColumn(\n            \"envelope\",\n            STF.ST_Transform(\"envelope\", F.lit(\"epsg:3035\"), F.lit(\"epsg:4326\")),\n        )\n        extent = (\n            for_extent.withColumn(\n                \"envelope\",\n                F.array(\n                    STF.ST_XMin(\"envelope\"),\n                    STF.ST_YMin(\"envelope\"),\n                    STF.ST_XMax(\"envelope\"),\n                    STF.ST_YMax(\"envelope\"),\n                ),\n            )\n            .collect()[0]\n            .envelope\n        )\n        for_extent = grid_gen.cover_extent_with_grid_tiles(extent)\n\n        intersection = (\n            transportation_sdf.alias(\"a\")\n            .join(\n                for_extent.alias(\"b\"),\n                STP.ST_Intersects(\"a.geometry\", \"b.geometry\"),\n            )\n            .withColumn(\"merge_geometry\", STF.ST_Intersection(\"a.geometry\", \"b.geometry\"))\n            .groupBy(\"category\", \"year\", \"month\", \"day\", \"b.grid_id\")\n            .agg(STA.ST_Union_Aggr(\"merge_geometry\").alias(\"geometry\"))\n        )\n\n        return intersection\n\n    def find_intersection_ratio(self, grid_tiles: DataFrame, landuse: DataFrame, landuse_class: str) -&gt; DataFrame:\n        \"\"\"\n        Finds the intersection ratio of a specific land use class for each tile in a grid.\n\n        This function takes a DataFrame of grid tiles, a DataFrame of land use data, and a land use class.\n        It finds the intersection of each grid tile with the land use data for the specified class,\n        calculates the area of the intersection, and divides this by the area of the tile to find the intersection ratio.\n        The intersection ratio is added as a new column to the grid DataFrame.\n\n        Args:\n            grid_tiles (pyspark.sql.DataFrame): The DataFrame of grid tiles.\n            landuse (pyspark.sql.DataFrame): The DataFrame of land use data.\n            landuse_class (str): The land use class to find the intersection ratio for.\n\n        Returns:\n            pyspark.sql.DataFrame: The DataFrame of grid tiles with the new column of intersection ratios.\n        \"\"\"\n\n        grid_tiles = grid_tiles.alias(\"a\").join(\n            landuse.filter(F.col(ColNames.category) == F.lit(landuse_class)).alias(\"b\"),\n            STP.ST_Intersects(f\"a.{ColNames.geometry}\", f\"b.{ColNames.geometry}\"),\n        )\n\n        grid_tiles = grid_tiles.withColumn(\n            \"shared_geom\",\n            STF.ST_Intersection(f\"a.{ColNames.geometry}\", f\"b.{ColNames.geometry}\"),\n        ).select(f\"b.{ColNames.category}\", f\"a.{ColNames.grid_id}\", \"a.area\", \"shared_geom\")\n\n        # grid_tiles = utils.fix_polygon_geometry(grid_tiles, \"shared_geom\")\n\n        grid_tiles = grid_tiles.groupBy(f\"b.{ColNames.category}\", f\"a.{ColNames.grid_id}\", \"a.area\").agg(\n            F.sum(STF.ST_Area(\"shared_geom\")).alias(f\"{landuse_class}_area\")\n        )\n\n        grid_tiles = grid_tiles.withColumn(f\"{landuse_class}_ratio\", F.col(f\"{landuse_class}_area\") / F.col(\"area\"))\n\n        return grid_tiles.select(ColNames.grid_id, f\"{landuse_class}_ratio\")\n\n    @staticmethod\n    def assign_mapping_values(\n        sdf: DataFrame,\n        values_map: Dict[Any, Any],\n        map_column: str,\n        values_column: str,\n        default_value: Any = 2,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Assigns mapping values to a DataFrame based on a specified column.\n\n        This function takes a DataFrame, a dictionary of mapping values, a column to map from,\n        a column to map to, and a default value. It creates a new column in the DataFrame by mapping values\n        from the map column to the values column using the values map.\n        If a value in the map column is not found in the values map, the default value is used.\n\n        Args:\n            sdf (pyspark.sql.DataFrame): The DataFrame to assign mapping values to.\n            values_map (dict): The dictionary of mapping values.\n            map_column (str): The column to map from.\n            values_column (str): The column to map to.\n            default_value (any, optional): The default value to use if a value in the map column is not found in the values map. Defaults to 2.\n\n        Returns:\n            pyspark.sql.DataFrame: The DataFrame with the new column of mapped values.\n        \"\"\"\n\n        keys = list(values_map.keys())\n        reclass_expr = F.when(F.col(map_column) == (keys[0]), values_map[keys[0]])\n\n        for key in keys[1:]:\n            reclass_expr = reclass_expr.when(F.col(map_column) == (key), values_map[key])\n\n        reclass_expr = reclass_expr.otherwise(default_value)\n        sdf = sdf.withColumn(values_column, reclass_expr)\n\n        return sdf\n</code></pre>"},{"location":"reference/components/execution/grid_enrichment/grid_enrichment/#components.execution.grid_enrichment.grid_enrichment.GridEnrichment.assign_mapping_values","title":"<code>assign_mapping_values(sdf, values_map, map_column, values_column, default_value=2)</code>  <code>staticmethod</code>","text":"<p>Assigns mapping values to a DataFrame based on a specified column.</p> <p>This function takes a DataFrame, a dictionary of mapping values, a column to map from, a column to map to, and a default value. It creates a new column in the DataFrame by mapping values from the map column to the values column using the values map. If a value in the map column is not found in the values map, the default value is used.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The DataFrame to assign mapping values to.</p> required <code>values_map</code> <code>dict</code> <p>The dictionary of mapping values.</p> required <code>map_column</code> <code>str</code> <p>The column to map from.</p> required <code>values_column</code> <code>str</code> <p>The column to map to.</p> required <code>default_value</code> <code>any</code> <p>The default value to use if a value in the map column is not found in the values map. Defaults to 2.</p> <code>2</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: The DataFrame with the new column of mapped values.</p> Source code in <code>multimno/components/execution/grid_enrichment/grid_enrichment.py</code> <pre><code>@staticmethod\ndef assign_mapping_values(\n    sdf: DataFrame,\n    values_map: Dict[Any, Any],\n    map_column: str,\n    values_column: str,\n    default_value: Any = 2,\n) -&gt; DataFrame:\n    \"\"\"\n    Assigns mapping values to a DataFrame based on a specified column.\n\n    This function takes a DataFrame, a dictionary of mapping values, a column to map from,\n    a column to map to, and a default value. It creates a new column in the DataFrame by mapping values\n    from the map column to the values column using the values map.\n    If a value in the map column is not found in the values map, the default value is used.\n\n    Args:\n        sdf (pyspark.sql.DataFrame): The DataFrame to assign mapping values to.\n        values_map (dict): The dictionary of mapping values.\n        map_column (str): The column to map from.\n        values_column (str): The column to map to.\n        default_value (any, optional): The default value to use if a value in the map column is not found in the values map. Defaults to 2.\n\n    Returns:\n        pyspark.sql.DataFrame: The DataFrame with the new column of mapped values.\n    \"\"\"\n\n    keys = list(values_map.keys())\n    reclass_expr = F.when(F.col(map_column) == (keys[0]), values_map[keys[0]])\n\n    for key in keys[1:]:\n        reclass_expr = reclass_expr.when(F.col(map_column) == (key), values_map[key])\n\n    reclass_expr = reclass_expr.otherwise(default_value)\n    sdf = sdf.withColumn(values_column, reclass_expr)\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/grid_enrichment/grid_enrichment/#components.execution.grid_enrichment.grid_enrichment.GridEnrichment.calculate_landuse_prior","title":"<code>calculate_landuse_prior(grid_tiles)</code>","text":"<p>Calculates the prior probability of land use for each tile in a grid.</p> <p>This function takes a DataFrame of grid tiles, each with a 'weighted_sum' column representing the weighted sum of land use ratios. It calculates the total weighted sum across all tiles and then divides the weighted sum of each tile by this total to calculate the prior probability of land use for each tile.</p> <p>Parameters:</p> Name Type Description Default <code>grid_tiles</code> <code>DataFrame</code> <p>The DataFrame of grid tiles.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: The DataFrame of grid tiles with the new column of prior probabilities.</p> Source code in <code>multimno/components/execution/grid_enrichment/grid_enrichment.py</code> <pre><code>def calculate_landuse_prior(self, grid_tiles: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Calculates the prior probability of land use for each tile in a grid.\n\n    This function takes a DataFrame of grid tiles, each with a 'weighted_sum' column representing the weighted sum of land use ratios.\n    It calculates the total weighted sum across all tiles and then divides the weighted sum of each tile by this\n    total to calculate the prior probability of land use for each tile.\n\n    Args:\n        grid_tiles (pyspark.sql.DataFrame): The DataFrame of grid tiles.\n\n    Returns:\n        pyspark.sql.DataFrame: The DataFrame of grid tiles with the new column of prior probabilities.\n    \"\"\"\n\n    # Compute total weighted sum for normalization\n    total_weighted_sum = grid_tiles.select(F.sum(\"weighted_sum\").alias(\"total_weighted_sum\")).collect()[0][\n        \"total_weighted_sum\"\n    ]\n\n    # Normalize the weighted sum across all grid tiles\n    grid_tiles = grid_tiles.withColumn(ColNames.prior_probability, F.col(\"weighted_sum\") / total_weighted_sum)\n\n    return grid_tiles\n</code></pre>"},{"location":"reference/components/execution/grid_enrichment/grid_enrichment/#components.execution.grid_enrichment.grid_enrichment.GridEnrichment.calculate_landuse_ratios_per_tile","title":"<code>calculate_landuse_ratios_per_tile(grid_tiles, landuse_sdf)</code>","text":"<p>Calculates the land use ratios for each tile in a grid.</p> <p>This function takes a DataFrame of grid tiles and a DataFrame of land use data. For each land use class, it calculates the ratio of the area of the class that intersects with each grid tile to the area of the tile. If a tile does not intersect with a land use class, the ratio for that class is set to 0.0. If a tile does not intersect with any land use class, the 'open_area_ratio' is set to 1.0.</p> <p>Parameters:</p> Name Type Description Default <code>grid_tiles</code> <code>DataFrame</code> <p>The DataFrame of grid tiles.</p> required <code>landuse_sdf</code> <code>DataFrame</code> <p>The DataFrame of land use data.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: The DataFrame of grid tiles with the new columns of land use ratios.</p> Source code in <code>multimno/components/execution/grid_enrichment/grid_enrichment.py</code> <pre><code>def calculate_landuse_ratios_per_tile(self, grid_tiles: DataFrame, landuse_sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Calculates the land use ratios for each tile in a grid.\n\n    This function takes a DataFrame of grid tiles and a DataFrame of land use data.\n    For each land use class, it calculates the ratio of the area of the class that intersects\n    with each grid tile to the area of the tile.\n    If a tile does not intersect with a land use class, the ratio for that class is set to 0.0.\n    If a tile does not intersect with any land use class, the 'open_area_ratio' is set to 1.0.\n\n    Args:\n        grid_tiles (pyspark.sql.DataFrame): The DataFrame of grid tiles.\n        landuse_sdf (pyspark.sql.DataFrame): The DataFrame of land use data.\n\n    Returns:\n        pyspark.sql.DataFrame: The DataFrame of grid tiles with the new columns of land use ratios.\n    \"\"\"\n\n    classes = [prior_weights for prior_weights in self.prior_weights.keys()]\n\n    for landuse_class in classes:\n\n        class_area_ratio_sdf = self.find_intersection_ratio(grid_tiles, landuse_sdf, landuse_class)\n        if class_area_ratio_sdf.count() == 0:\n            grid_tiles = grid_tiles.withColumn(f\"{landuse_class}_ratio\", F.lit(0.0))\n        else:\n            grid_tiles = grid_tiles.join(class_area_ratio_sdf, \"grid_id\", \"left\")\n        grid_tiles = grid_tiles.persist(StorageLevel.MEMORY_AND_DISK)\n        grid_tiles.count()\n        self.logger.info(f\"Landuse class {landuse_class} ratio per tile calculated\")\n\n    # Fill null values in the DataFrame with a specific float number, e.g., 0.0\n    grid_tiles = grid_tiles.fillna(0.0)\n\n    # Update 'open_areas_ratio' to 1.0 if all other specific ratio columns are 0.0\n    # TODO: remove hardcoded values\n    grid_tiles = grid_tiles.withColumn(\n        \"open_area_ratio\",\n        F.when(\n            (F.col(\"roads_ratio\") == 0.0)\n            &amp; (F.col(\"residential_builtup_ratio\") == 0.0)\n            &amp; (F.col(\"other_builtup_ratio\") == 0.0)\n            &amp; (F.col(\"forest_ratio\") == 0.0)\n            &amp; (F.col(\"water_ratio\") == 0.0),\n            1.0,\n        ).otherwise(F.col(\"open_area_ratio\")),\n    ).drop(\"geometry\", \"area\")\n\n    return grid_tiles\n</code></pre>"},{"location":"reference/components/execution/grid_enrichment/grid_enrichment/#components.execution.grid_enrichment.grid_enrichment.GridEnrichment.calculate_transportation_buffer","title":"<code>calculate_transportation_buffer(transportation_sdf, category_buffer_m)</code>","text":"<p>Calculates the buffer for each transportation feature based on its category.</p> <p>This function takes a DataFrame of transportation features and a dictionary mapping categories to buffer distances. It assigns the appropriate buffer distance to each feature based on its category and then calculates the buffer geometry. The buffer geometry replaces the original geometry of each feature.</p> <p>Parameters:</p> Name Type Description Default <code>transportation_sdf</code> <code>DataFrame</code> <p>The DataFrame of transportation features.</p> required <code>category_buffer_m</code> <code>dict</code> <p>A dictionary mapping categories to buffer distances.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: The DataFrame of transportation features with the buffer geometries.</p> Source code in <code>multimno/components/execution/grid_enrichment/grid_enrichment.py</code> <pre><code>def calculate_transportation_buffer(\n    self, transportation_sdf: DataFrame, category_buffer_m: Dict[str, float]\n) -&gt; DataFrame:\n    \"\"\"\n    Calculates the buffer for each transportation feature based on its category.\n\n    This function takes a DataFrame of transportation features and a dictionary mapping categories to buffer distances.\n    It assigns the appropriate buffer distance to each feature based on its category and then calculates the buffer geometry.\n    The buffer geometry replaces the original geometry of each feature.\n\n    Args:\n        transportation_sdf (pyspark.sql.DataFrame): The DataFrame of transportation features.\n        category_buffer_m (dict): A dictionary mapping categories to buffer distances.\n\n    Returns:\n        pyspark.sql.DataFrame: The DataFrame of transportation features with the buffer geometries.\n    \"\"\"\n\n    transportation_sdf = self.assign_mapping_values(\n        transportation_sdf,\n        category_buffer_m,\n        ColNames.category,\n        \"buffer_dist\",\n        category_buffer_m[\"unknown\"],\n    )\n\n    transportation_sdf = transportation_sdf.filter(F.col(\"buffer_dist\") != 0.0)\n\n    transportation_sdf = transportation_sdf.withColumn(\n        ColNames.geometry,\n        STF.ST_Buffer(ColNames.geometry, F.col(\"buffer_dist\")),\n    ).drop(\"buffer_dist\")\n\n    return transportation_sdf\n</code></pre>"},{"location":"reference/components/execution/grid_enrichment/grid_enrichment/#components.execution.grid_enrichment.grid_enrichment.GridEnrichment.calculated_landuse_ratios_weighted_sum","title":"<code>calculated_landuse_ratios_weighted_sum(grid_tiles, weights_dict, sum_column_name)</code>","text":"<p>Calculates the weighted sum of land use ratios for each grid tile.</p> <p>This function takes a DataFrame of grid tiles and a dictionary of weights for each land use ratio. It multiplies each land use ratio by its corresponding weight and then sums these weighted ratios to calculate a weighted sum for each grid tile.</p> <p>Parameters:</p> Name Type Description Default <code>grid_tiles</code> <code>DataFrame</code> <p>The DataFrame of grid tiles with landuse ratios.</p> required <code>weights_dict</code> <code>dict</code> <p>A dictionary where the keys are the names of the land use ratios</p> required <code>sum_column_name</code> <code>str</code> <p>The name of the new column that will contain the weighted sums.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: The DataFrame of grid tiles with the new column of weighted sums.</p> Source code in <code>multimno/components/execution/grid_enrichment/grid_enrichment.py</code> <pre><code>def calculated_landuse_ratios_weighted_sum(\n    self,\n    grid_tiles: DataFrame,\n    weights_dict: Dict[str, float],\n    sum_column_name: str,\n) -&gt; DataFrame:\n    \"\"\"\n    Calculates the weighted sum of land use ratios for each grid tile.\n\n    This function takes a DataFrame of grid tiles and a dictionary of weights for each land use ratio.\n    It multiplies each land use ratio by its corresponding weight and then sums these weighted ratios\n    to calculate a weighted sum for each grid tile.\n\n    Args:\n        grid_tiles (pyspark.sql.DataFrame): The DataFrame of grid tiles with landuse ratios.\n        weights_dict (dict): A dictionary where the keys are the names of the land use ratios\n        and the values are the corresponding weights.\n        sum_column_name (str): The name of the new column that will contain the weighted sums.\n\n    Returns:\n        pyspark.sql.DataFrame: The DataFrame of grid tiles with the new column of weighted sums.\n    \"\"\"\n\n    weighted_columns = [F.col(f\"{ratio}_ratio\") * weight for ratio, weight in weights_dict.items()]\n    grid_tiles = grid_tiles.withColumn(sum_column_name, sum(weighted_columns))\n\n    return grid_tiles\n</code></pre>"},{"location":"reference/components/execution/grid_enrichment/grid_enrichment/#components.execution.grid_enrichment.grid_enrichment.GridEnrichment.find_intersection_ratio","title":"<code>find_intersection_ratio(grid_tiles, landuse, landuse_class)</code>","text":"<p>Finds the intersection ratio of a specific land use class for each tile in a grid.</p> <p>This function takes a DataFrame of grid tiles, a DataFrame of land use data, and a land use class. It finds the intersection of each grid tile with the land use data for the specified class, calculates the area of the intersection, and divides this by the area of the tile to find the intersection ratio. The intersection ratio is added as a new column to the grid DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>grid_tiles</code> <code>DataFrame</code> <p>The DataFrame of grid tiles.</p> required <code>landuse</code> <code>DataFrame</code> <p>The DataFrame of land use data.</p> required <code>landuse_class</code> <code>str</code> <p>The land use class to find the intersection ratio for.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: The DataFrame of grid tiles with the new column of intersection ratios.</p> Source code in <code>multimno/components/execution/grid_enrichment/grid_enrichment.py</code> <pre><code>def find_intersection_ratio(self, grid_tiles: DataFrame, landuse: DataFrame, landuse_class: str) -&gt; DataFrame:\n    \"\"\"\n    Finds the intersection ratio of a specific land use class for each tile in a grid.\n\n    This function takes a DataFrame of grid tiles, a DataFrame of land use data, and a land use class.\n    It finds the intersection of each grid tile with the land use data for the specified class,\n    calculates the area of the intersection, and divides this by the area of the tile to find the intersection ratio.\n    The intersection ratio is added as a new column to the grid DataFrame.\n\n    Args:\n        grid_tiles (pyspark.sql.DataFrame): The DataFrame of grid tiles.\n        landuse (pyspark.sql.DataFrame): The DataFrame of land use data.\n        landuse_class (str): The land use class to find the intersection ratio for.\n\n    Returns:\n        pyspark.sql.DataFrame: The DataFrame of grid tiles with the new column of intersection ratios.\n    \"\"\"\n\n    grid_tiles = grid_tiles.alias(\"a\").join(\n        landuse.filter(F.col(ColNames.category) == F.lit(landuse_class)).alias(\"b\"),\n        STP.ST_Intersects(f\"a.{ColNames.geometry}\", f\"b.{ColNames.geometry}\"),\n    )\n\n    grid_tiles = grid_tiles.withColumn(\n        \"shared_geom\",\n        STF.ST_Intersection(f\"a.{ColNames.geometry}\", f\"b.{ColNames.geometry}\"),\n    ).select(f\"b.{ColNames.category}\", f\"a.{ColNames.grid_id}\", \"a.area\", \"shared_geom\")\n\n    # grid_tiles = utils.fix_polygon_geometry(grid_tiles, \"shared_geom\")\n\n    grid_tiles = grid_tiles.groupBy(f\"b.{ColNames.category}\", f\"a.{ColNames.grid_id}\", \"a.area\").agg(\n        F.sum(STF.ST_Area(\"shared_geom\")).alias(f\"{landuse_class}_area\")\n    )\n\n    grid_tiles = grid_tiles.withColumn(f\"{landuse_class}_ratio\", F.col(f\"{landuse_class}_area\") / F.col(\"area\"))\n\n    return grid_tiles.select(ColNames.grid_id, f\"{landuse_class}_ratio\")\n</code></pre>"},{"location":"reference/components/execution/grid_enrichment/grid_enrichment/#components.execution.grid_enrichment.grid_enrichment.GridEnrichment.map_landuse_to_grid","title":"<code>map_landuse_to_grid(grid_sdf)</code>","text":"<p>Maps land use and transportation data to a grid.</p> <p>This function takes a grid DataFrame, prepares the transportation and land use data by filtering and cutting it to the extent of the current quadkey, and then merges the prepared data with the grid. It also calculates the land use ratios per tile in the grid.</p> <p>Parameters:</p> Name Type Description Default <code>grid_sdf</code> <code>DataFrame</code> <p>The grid DataFrame to which the land use and transportation data will be mapped.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: The grid DataFrame with the mapped land use</p> <code>DataFrame</code> <p>and transportation data as ratios of a total area.</p> <p>Raises:</p> Type Description <code>Warning</code> <p>If no data is found for the current quadkey,</p> Source code in <code>multimno/components/execution/grid_enrichment/grid_enrichment.py</code> <pre><code>def map_landuse_to_grid(self, grid_sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Maps land use and transportation data to a grid.\n\n    This function takes a grid DataFrame, prepares the transportation and land use data\n    by filtering and cutting it to the extent of the current quadkey,\n    and then merges the prepared data with the grid. It also calculates the land use ratios per tile in the grid.\n\n    Args:\n        grid_sdf (pyspark.sql.DataFrame): The grid DataFrame to which the land use and transportation data will be mapped.\n\n    Returns:\n        pyspark.sql.DataFrame: The grid DataFrame with the mapped land use\n        and transportation data as ratios of a total area.\n\n    Raises:\n        Warning: If no data is found for the current quadkey,\n        a warning is logged and the function returns the grid DataFrame populated with empty ratios.\n    \"\"\"\n\n    current_quadkey_extent = utils.quadkey_to_extent(self.current_quadkey)\n    # prepare roads\n    transportation_sdf = self.input_data_objects[BronzeTransportationDataObject.ID].df.select(\n        ColNames.category,\n        ColNames.geometry,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    )\n\n    transportation_sdf = utils.filter_geodata_to_extent(transportation_sdf, current_quadkey_extent, 3035)\n\n    transportation_sdf = self.calculate_transportation_buffer(\n        transportation_sdf, self.transportation_category_buffer_m\n    )\n    transportation_sdf = transportation_sdf.withColumn(ColNames.category, F.lit(\"roads\"))\n    # This is needed to reduce number of geometries so all small roads are merged into one multipolygon based on quadkey\n    transportation_sdf = self.merge_transportation_by_grid(transportation_sdf, 1000)\n\n    transportation_sdf.persist(StorageLevel.MEMORY_AND_DISK)\n    transportation_sdf.count()\n    transportation_sdf = transportation_sdf.withColumn(\"quadkey\", F.lit(self.current_quadkey))\n    transportation_sdf.coalesce(1).write.mode(\"overwrite\").format(\"geoparquet\").partitionBy(\"quadkey\").save(\n        \"/opt/mobloc_data/roads_merged_test\"\n    )\n\n    transportation_sdf = transportation_sdf.select(\n        ColNames.category,\n        ColNames.geometry,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    )\n\n    self.logger.info(\"Roads prepared\")\n\n    # prepare landuse\n    landuse_sdf = self.input_data_objects[BronzeLanduseDataObject.ID].df.select(\n        ColNames.category,\n        ColNames.geometry,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    )\n\n    landuse_sdf = utils.filter_geodata_to_extent(landuse_sdf, current_quadkey_extent, 3035)\n    landuse_sdf = utils.cut_geodata_to_extent(landuse_sdf, current_quadkey_extent, 3035)\n\n    landuse_sdf.persist(StorageLevel.MEMORY_AND_DISK)\n    landuse_sdf.count()\n    self.logger.info(\"Landuse prepared\")\n\n    # merge roads with landuse\n    landuse_sdf = utils.cut_polygons_with_mask_polygons(\n        landuse_sdf,\n        transportation_sdf,\n        [\n            ColNames.category,\n            ColNames.geometry,\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n        ],\n        False,\n        ColNames.geometry,\n    )\n\n    landuse_roads_sdf = landuse_sdf.union(transportation_sdf)\n\n    # blow up too big landuse polygons\n    # TODO: make vertices number parameter\n    landuse_roads_sdf = landuse_roads_sdf.withColumn(\n        ColNames.geometry, STF.ST_SubDivideExplode(ColNames.geometry, 1000)\n    )\n    landuse_roads_sdf = utils.fix_geometry(landuse_roads_sdf, 3)\n\n    landuse_roads_sdf.persist(StorageLevel.MEMORY_AND_DISK)\n    land_use_count = landuse_roads_sdf.count()\n\n    if land_use_count == 0:\n        self.logger.warning(f\"No data found for quadkey {self.current_quadkey}. Skipping\")\n        return self.populate_grid_tiles_with_empty_ratios(grid_sdf).drop(\"geometry\")\n\n    self.logger.info(\"Landuse and roads merged\")\n\n    # count landuse types ratios in grid cells\n    grid_tiles = self.grid_generator.grid_ids_to_tiles(grid_sdf)\n    grid_tiles = grid_tiles.withColumn(\"area\", STF.ST_Area(ColNames.geometry))\n\n    grid_tiles = grid_tiles.persist(StorageLevel.MEMORY_AND_DISK)\n    grid_tiles.count()\n\n    grid_tiles = self.calculate_landuse_ratios_per_tile(grid_tiles, landuse_roads_sdf)\n\n    transportation_sdf.unpersist()\n    landuse_roads_sdf.unpersist()\n    landuse_sdf.unpersist()\n\n    return grid_tiles\n</code></pre>"},{"location":"reference/components/execution/grid_enrichment/grid_enrichment/#components.execution.grid_enrichment.grid_enrichment.GridEnrichment.merge_transportation_by_grid","title":"<code>merge_transportation_by_grid(transportation_sdf, resolution)</code>","text":"<p>Merges transportation data with a generated grid based on the specified resolution.</p> <p>This function takes a DataFrame containing transportation data and a grid resolution. It first generates a grid that covers the extent of the transportation data. Then, it intersects the transportation data with this grid, merging the transportation geometries that fall within each grid cell. The result is a DataFrame where each row represents a grid cell, aggregated by transportation category and date, with a merged geometry for all transportation data within that cell.</p> <p>Parameters: - transportation_sdf (DataFrame): A Spark DataFrame containing transportation data. - resolution (int): The resolution of the grid to generate, specified as the length of the side of each square grid cell in meters.</p> <p>Returns: - DataFrame: A Spark DataFrame where each row represents merged transportation data.</p> Source code in <code>multimno/components/execution/grid_enrichment/grid_enrichment.py</code> <pre><code>def merge_transportation_by_grid(self, transportation_sdf: DataFrame, resolution: int) -&gt; DataFrame:\n    \"\"\"\n    Merges transportation data with a generated grid based on the specified resolution.\n\n    This function takes a DataFrame containing transportation data and a grid resolution.\n    It first generates a grid that covers the extent of the transportation data. Then, it\n    intersects the transportation data with this grid, merging the transportation geometries\n    that fall within each grid cell. The result is a DataFrame where each row represents a\n    grid cell, aggregated by transportation category and date, with a merged geometry for\n    all transportation data within that cell.\n\n    Parameters:\n    - transportation_sdf (DataFrame): A Spark DataFrame containing transportation data.\n    - resolution (int): The resolution of the grid to generate, specified as the length\n    of the side of each square grid cell in meters.\n\n    Returns:\n    - DataFrame: A Spark DataFrame where each row represents merged transportation data.\n    \"\"\"\n    grid_gen = InspireGridGenerator(self.spark, resolution)\n    for_extent = transportation_sdf.groupBy().agg(STA.ST_Envelope_Aggr(\"geometry\").alias(\"envelope\"))\n    for_extent = for_extent.withColumn(\n        \"envelope\",\n        STF.ST_Transform(\"envelope\", F.lit(\"epsg:3035\"), F.lit(\"epsg:4326\")),\n    )\n    extent = (\n        for_extent.withColumn(\n            \"envelope\",\n            F.array(\n                STF.ST_XMin(\"envelope\"),\n                STF.ST_YMin(\"envelope\"),\n                STF.ST_XMax(\"envelope\"),\n                STF.ST_YMax(\"envelope\"),\n            ),\n        )\n        .collect()[0]\n        .envelope\n    )\n    for_extent = grid_gen.cover_extent_with_grid_tiles(extent)\n\n    intersection = (\n        transportation_sdf.alias(\"a\")\n        .join(\n            for_extent.alias(\"b\"),\n            STP.ST_Intersects(\"a.geometry\", \"b.geometry\"),\n        )\n        .withColumn(\"merge_geometry\", STF.ST_Intersection(\"a.geometry\", \"b.geometry\"))\n        .groupBy(\"category\", \"year\", \"month\", \"day\", \"b.grid_id\")\n        .agg(STA.ST_Union_Aggr(\"merge_geometry\").alias(\"geometry\"))\n    )\n\n    return intersection\n</code></pre>"},{"location":"reference/components/execution/grid_enrichment/grid_enrichment/#components.execution.grid_enrichment.grid_enrichment.GridEnrichment.populate_grid_tiles_with_empty_ratios","title":"<code>populate_grid_tiles_with_empty_ratios(grid_tiles)</code>","text":"<p>Populates grid tiles with empty land use ratios.</p> <p>This function takes a DataFrame of grid tiles and adds a new column for each land use class in weights dict. Each new column is initialized with a value of 0.0, representing an empty land use ratio.</p> <p>Parameters:</p> Name Type Description Default <code>grid_tiles</code> <code>DataFrame</code> <p>The DataFrame of grid tiles.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: The DataFrame of grid tiles with the new columns of empty land use ratios.</p> Source code in <code>multimno/components/execution/grid_enrichment/grid_enrichment.py</code> <pre><code>def populate_grid_tiles_with_empty_ratios(self, grid_tiles: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Populates grid tiles with empty land use ratios.\n\n    This function takes a DataFrame of grid tiles and adds a new column for each land use class in weights dict.\n    Each new column is initialized with a value of 0.0, representing an empty land use ratio.\n\n    Args:\n        grid_tiles (pyspark.sql.DataFrame): The DataFrame of grid tiles.\n\n    Returns:\n        pyspark.sql.DataFrame: The DataFrame of grid tiles with the new columns of empty land use ratios.\n    \"\"\"\n\n    classes = [prior_weights for prior_weights in self.prior_weights.keys()]\n\n    for landuse_class in classes:\n        grid_tiles = grid_tiles.withColumn(f\"{landuse_class}_ratio\", F.lit(0.0))\n\n    return grid_tiles\n</code></pre>"},{"location":"reference/components/execution/longterm_permanence_score/","title":"longterm_permanence_score","text":""},{"location":"reference/components/execution/longterm_permanence_score/longterm_permanence_score/","title":"longterm_permanence_score","text":"<p>Module that computes the Long-term Permanence Score</p>"},{"location":"reference/components/execution/longterm_permanence_score/longterm_permanence_score/#components.execution.longterm_permanence_score.longterm_permanence_score.LongtermPermanenceScore","title":"<code>LongtermPermanenceScore</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class that computes the long term permanence score and related metrics, for different combinations of seasons, day types and time intervals.</p> Source code in <code>multimno/components/execution/longterm_permanence_score/longterm_permanence_score.py</code> <pre><code>class LongtermPermanenceScore(Component):\n    \"\"\"\n    Class that computes the long term permanence score and related metrics, for different combinations of\n    seasons, day types and time intervals.\n    \"\"\"\n\n    COMPONENT_ID = \"LongtermPermanenceScore\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        # Months that compose the long-term period, at least one\n        start_month = self.config.get(self.COMPONENT_ID, \"start_month\")\n        try:\n            self.start_date = dt.datetime.strptime(start_month, \"%Y-%m\").date()\n        except ValueError as e:\n            self.logger.error(f\"Could not parse start_month = `{start_month}`. Expected format: YYYY-MM\")\n            raise e\n\n        end_month = self.config.get(self.COMPONENT_ID, \"end_month\")\n        try:\n            self.end_date = dt.datetime.strptime(end_month, \"%Y-%m\").date()\n        except ValueError as e:\n            self.logger.error(f\"Could not parse end_month = `{end_month}`. Expected format: YYYY-MM\")\n            raise e\n        self.end_date = self.end_date.replace(day=cal.monthrange(self.end_date.year, self.end_date.month)[1])\n\n        if self.end_date &lt; self.start_date:\n            raise ValueError(f\"End month `{end_month}` should not be an earlier than start month `{start_month}`\")\n\n        # Get months of the year in each season\n        self.winter_months = self._get_month_list(self.config.get(self.COMPONENT_ID, \"winter_months\"), \"winter\")\n        self.spring_months = self._get_month_list(self.config.get(self.COMPONENT_ID, \"spring_months\"), \"spring\")\n        self.summer_months = self._get_month_list(self.config.get(self.COMPONENT_ID, \"summer_months\"), \"summer\")\n        self.autumn_months = self._get_month_list(self.config.get(self.COMPONENT_ID, \"autumn_months\"), \"autumn\")\n\n        # Check for possible repeated months\n        all_season_months = self.winter_months + self.spring_months + self.summer_months + self.autumn_months\n        if len(all_season_months) != len(set(all_season_months)):\n            raise ValueError(\"at least one month belongs to more than one season -- please correct input parameters\")\n\n        # Read all subyearly-submonthly-subdaily combinations\n        period_combinations = self.config.geteval(self.COMPONENT_ID, \"period_combinations\")\n        self.period_combinations = {}\n        for season, day_types_dict in period_combinations.items():\n            if season.lower() not in SEASONS:\n                raise ValueError(f\"Unknown season `{season}` in period_combinations\")\n            self.period_combinations[season.lower()] = {}\n            for day_type, time_intervals in day_types_dict.items():\n                if day_type.lower() not in DAY_TYPES:\n                    raise ValueError(f\"Uknown day_type `{day_type}` under `{season}` in period_combinations\")\n                self.period_combinations[season.lower()][day_type.lower()] = []\n                if len(time_intervals) != len(set(time_intervals)):\n                    raise ValueError(\n                        f\"Repeated values for time_interval in under `{season}` and `{day_type}`:\",\n                        str(period_combinations[season][day_type]),\n                    )\n                for time_interval in time_intervals:\n                    if time_interval not in TIME_INTERVALS:\n                        raise ValueError(f\"Unknown time_interval `{time_interval}` under `{season}` and `{day_type}`\")\n                    self.period_combinations[season.lower()][day_type.lower()].append(time_interval)\n\n        # Check that all seasons for which an analysis is to be performed have been assigned at least one month\n        for season, periods in self.period_combinations.items():\n            if season == \"all\":\n                continue\n            if len(periods) &gt; 0 and len(getattr(self, f\"{season}_months\")) == 0:\n                raise ValueError(\n                    f\"Some period combinations have been requested for season `{season}` but no \"\n                    \"month has been included in this season -- please correct the configuration \"\n                    \"parameters\"\n                )\n\n        # Get list of all the months that will be used, together with the seasons they belong to\n        self.longterm_months = []\n        self.season_months = {}\n\n        for season in self.period_combinations:\n            self.season_months[season] = []\n\n        month_start_date = self.start_date\n        while month_start_date &lt; self.end_date:\n            self.longterm_months.append(month_start_date)\n            if \"all\" in self.season_months:\n                self.season_months[\"all\"].append(month_start_date)\n            if \"winter\" in self.season_months and month_start_date.month in self.winter_months:\n                self.season_months[\"winter\"].append(month_start_date)\n            if \"spring\" in self.season_months and month_start_date.month in self.spring_months:\n                self.season_months[\"spring\"].append(month_start_date)\n            if \"summer\" in self.season_months and month_start_date.month in self.summer_months:\n                self.season_months[\"summer\"].append(month_start_date)\n            if \"autumn\" in self.season_months and month_start_date.month in self.autumn_months:\n                self.season_months[\"autumn\"].append(month_start_date)\n\n            month_start_date += dt.timedelta(days=cal.monthrange(month_start_date.year, month_start_date.month)[1])\n\n        # Initialise variables for working with each longterm analysis\n        self.current_lt_analysis = None\n        self.longterm_analyses = None\n\n    def _get_month_list(self, months_input: str, context: str) -&gt; list[int]:\n        \"\"\"Read and parse a comma-separated list of months that will be assigned to a particular season. Months are\n        represented by an integer from 1 to 12 and must not be repeated within the list\n\n\n        Args:\n            months_input (str): comma-separated list of months to be parsed\n            context (str): name of the season, used for error tracking\n\n        Raises:\n            e: could not parse month to integer\n            ValueError: integer is not one between 1 and 12, inclusive\n            ValueError: repeated integers\n\n        Returns:\n            list[int]: list of integers representing the months of the year that will constitute a season\n        \"\"\"\n        months_input = months_input.replace(\" \", \"\").replace(\"\\t\", \"\")\n        if months_input == \"\":\n            return []\n\n        months_input = months_input.split(\",\")\n        months = []\n\n        for mm in months_input:\n            try:\n                mm = int(mm)\n            except ValueError as e:\n                self.logger.error(f\"expected integer as a month for {context} season, but found `{mm}`\")\n                raise e\n            if mm &lt; 1 or mm &gt; 12:\n                raise ValueError(\n                    f\"expected integer between 1 and 12 to represent a month for {context} season, \" f\"but found `{mm}`\"\n                )\n            months.append(mm)\n\n        if len(months) != len(set(months)):\n            raise ValueError(f\"found repeated month {months} in season {context} -- please remove any duplicates\")\n\n        return months\n\n    def initalize_data_objects(self):\n        input_silver_midterm_ps_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"midterm_permanence_score_data_silver\")\n        output_silver_longterm_ps_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"longterm_permanence_score_data_silver\"\n        )\n\n        midterm_ps = SilverMidtermPermanenceScoreDataObject(self.spark, input_silver_midterm_ps_path)\n        longterm_ps = SilverLongtermPermanenceScoreDataObject(self.spark, output_silver_longterm_ps_path)\n\n        self.input_data_objects = {midterm_ps.ID: midterm_ps}\n\n        self.output_data_objects = {longterm_ps.ID: longterm_ps}\n\n    def _check_midterm_data_exist(self) -&gt; list[dict]:\n        \"\"\"Checks that the mid-term permanence score data necessary for carrying out all long-term analyses exist,\n        based on the day_types and time_intervals requested for each analysis. Returns a list of dictionaries\n        containing the information necessary to filter the required mid-term analysis data of each analysis.\n\n        Raises:\n            FileNotFoundError: Whenever a certain (day_type, time_interval) combination has not been found in the\n                data of a particular month, required to compute some long-term analysis.\n\n        Returns:\n            list[dict]: information of each long-term analysis to be performed\n        \"\"\"\n        midterm_df = self.input_data_objects[SilverMidtermPermanenceScoreDataObject.ID].df\n        longterm_analyses = []\n        already_checked = set()\n        # For each season\n        for season, months in self.season_months.items():\n            # Check that all months of that season have the mid-term PS data of the required day type and time interval\n            for day_type, time_intervals in self.period_combinations[season].items():\n                for time_interval in time_intervals:\n                    for month in months:\n                        if (month, day_type, time_interval) in already_checked:\n                            continue\n\n                        partition_filter = (\n                            (F.col(ColNames.year) == F.lit(month.year))\n                            &amp; (F.col(ColNames.month) == F.lit(month.month))\n                            &amp; (F.col(ColNames.day_type) == F.lit(day_type))\n                            &amp; (F.col(ColNames.time_interval) == F.lit(time_interval))\n                        )\n\n                        data_exists = midterm_df.where(partition_filter).count() &gt; 0\n                        if not data_exists:\n                            raise FileNotFoundError(\n                                \"No Mid-term Permanence Score data has been found for month \"\n                                f\"{month.strftime('%Y-%m')}, day_type `{day_type}` and time_interval `{time_interval}`\"\n                            )\n                        already_checked.add((month, day_type, time_interval))\n\n                    longterm_analyses.append(\n                        {\n                            \"season\": season,\n                            \"months\": months,\n                            \"day_type\": day_type,\n                            \"time_interval\": time_interval,\n                        }\n                    )\n\n        return longterm_analyses\n\n    def filter_longterm_analysis_data(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Applies the push-up filters on the Mid-term Permanence Score data to only consider the data from the\n        months, day_type, and time_interval required for the current long-term analysis combination.\n\n        Args:\n            df (DataFrame): Mid-term Permanence Score DataFrame before filtering\n\n        Returns:\n            DataFrame: Mid-term Permanence Score DataFrame after filtering\n        \"\"\"\n        month_filters = [\n            (F.col(ColNames.year) == F.lit(month.year)) &amp; (F.col(ColNames.month) == F.lit(month.month))\n            for month in self.current_lt_analysis[\"months\"]\n        ]\n\n        def logical_or(x, y):\n            return x | y\n\n        month_filter = reduce(logical_or, month_filters)\n\n        day_type_filter = F.col(ColNames.day_type) == F.lit(self.current_lt_analysis[\"day_type\"])\n\n        time_interval_filter = F.col(ColNames.time_interval) == F.lit(self.current_lt_analysis[\"time_interval\"])\n\n        filtered_df = df.filter(month_filter &amp; day_type_filter &amp; time_interval_filter)\n\n        return filtered_df\n\n    def compute_longterm_metrics(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Compute the Long-term Permanence Score and metrics from the filtered Mid-term Permanence Score data\n\n        Args:\n            df (DataFrame): Mid-term Permanence Score DataFrame\n\n        Returns:\n            DataFrame: dataframe with the long-term permanence score aend metrics\n        \"\"\"\n        # Split dataframes by id_type\n        grid_df = df.filter(F.col(ColNames.id_type) == F.lit(\"grid\"))\n        unknown_df = df.filter(F.col(ColNames.id_type) == F.lit(\"unknown\"))\n        obs_df = df.filter(F.col(ColNames.id_type) == F.lit(\"device_observation\"))\n\n        grid_df = grid_df.groupby(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id).agg(\n            F.sum(ColNames.mps).cast(IntegerType()).alias(ColNames.lps),\n            F.sum(ColNames.frequency).cast(IntegerType()).alias(ColNames.total_frequency),\n            F.mean(ColNames.frequency).cast(FloatType()).alias(ColNames.frequency_mean),\n            F.stddev_samp(ColNames.frequency).cast(FloatType()).alias(ColNames.frequency_std),\n            F.mean(ColNames.regularity_mean).cast(FloatType()).alias(ColNames.regularity_mean),\n            F.stddev_samp(ColNames.regularity_mean).cast(FloatType()).alias(ColNames.regularity_std),\n        )\n\n        unknown_df = unknown_df.groupby(ColNames.user_id_modulo, ColNames.user_id).agg(\n            F.sum(ColNames.mps).cast(IntegerType()).alias(ColNames.lps),\n            F.sum(ColNames.frequency).cast(IntegerType()).alias(ColNames.total_frequency),\n            F.mean(ColNames.frequency).cast(FloatType()).alias(ColNames.frequency_mean),\n            F.stddev_samp(ColNames.frequency).cast(FloatType()).alias(ColNames.frequency_std),\n            F.mean(ColNames.regularity_mean).cast(FloatType()).alias(ColNames.regularity_mean),\n            F.stddev_samp(ColNames.regularity_mean).cast(FloatType()).alias(ColNames.regularity_std),\n        )\n\n        obs_df = obs_df.groupby(ColNames.user_id_modulo, ColNames.user_id).agg(\n            F.sum(ColNames.mps).cast(IntegerType()).alias(ColNames.lps),\n            F.sum(ColNames.frequency).cast(IntegerType()).alias(ColNames.total_frequency),\n        )\n\n        grid_df = grid_df.select(\n            ColNames.user_id_modulo,\n            ColNames.user_id,\n            ColNames.grid_id,\n            ColNames.lps,\n            ColNames.total_frequency,\n            ColNames.frequency_mean,\n            ColNames.frequency_std,\n            ColNames.regularity_mean,\n            ColNames.regularity_std,\n            F.lit(\"grid\").alias(ColNames.id_type),\n        )\n\n        unknown_df = unknown_df.select(\n            ColNames.user_id_modulo,\n            ColNames.user_id,\n            F.lit(\"unknown\").alias(ColNames.grid_id),\n            ColNames.lps,\n            ColNames.total_frequency,\n            ColNames.frequency_mean,\n            ColNames.frequency_std,\n            ColNames.regularity_mean,\n            ColNames.regularity_std,\n            F.lit(\"unknown\").alias(ColNames.id_type),\n        )\n\n        obs_df = obs_df.select(\n            ColNames.user_id_modulo,\n            ColNames.user_id,\n            F.lit(\"device_observation\").alias(ColNames.grid_id),\n            ColNames.lps,\n            ColNames.total_frequency,\n            F.lit(None).cast(FloatType()).alias(ColNames.frequency_mean),\n            F.lit(None).cast(FloatType()).alias(ColNames.frequency_std),\n            F.lit(None).cast(FloatType()).alias(ColNames.regularity_mean),\n            F.lit(None).cast(FloatType()).alias(ColNames.regularity_std),\n            F.lit(\"device_observation\").alias(ColNames.id_type),\n        )\n\n        return grid_df.union(unknown_df).union(obs_df)\n\n    def transform(self):\n        midterm_df = self.input_data_objects[SilverMidtermPermanenceScoreDataObject.ID].df\n\n        df = self.filter_longterm_analysis_data(midterm_df)\n\n        longterm_df = self.compute_longterm_metrics(df)\n\n        start_date = min(self.current_lt_analysis[\"months\"])\n        end_date = max(self.current_lt_analysis[\"months\"])\n        end_date = end_date.replace(day=cal.monthrange(end_date.year, end_date.month)[1])\n\n        longterm_df = longterm_df.withColumns(\n            {\n                ColNames.start_date: F.lit(start_date),\n                ColNames.end_date: F.lit(end_date),\n                ColNames.season: F.lit(self.current_lt_analysis[\"season\"]),\n                ColNames.day_type: F.lit(self.current_lt_analysis[\"day_type\"]),\n                ColNames.time_interval: F.lit(self.current_lt_analysis[\"time_interval\"]),\n            }\n        )\n\n        self.output_data_objects[SilverLongtermPermanenceScoreDataObject.ID].df = longterm_df\n\n    def execute(self):\n        self.logger.info(\"Reading data objects...\")\n        self.read()\n        self.logger.info(\"... data objects read!\")\n        self.logger.info(\"Checking that all required mid-term permanence score data exists...\")\n        self.longterm_analyses = self._check_midterm_data_exist()\n        self.logger.info(\"... check successful!\")\n        self.logger.info(\"Starting long-term analyses...\")\n        for lt_analysis in self.longterm_analyses:\n            self.current_lt_analysis = lt_analysis\n            self.logger.info(\n                f\"Starting analysis for season `{self.current_lt_analysis['season']}`, \"\n                f\"{min(self.current_lt_analysis['months']).strftime('%Y-%m')} to \"\n                f\"{max(self.current_lt_analysis['months']).strftime('%Y-%m')}, \"\n                f\"day_type `{self.current_lt_analysis['day_type']}` and \"\n                f\"time_interval `{self.current_lt_analysis['time_interval']}`...\"\n            )\n            self.transform()\n            self.write()\n            self.logger.info(\"... results saved\")\n\n        self.logger.info(\"... all analyses finished!\")\n</code></pre>"},{"location":"reference/components/execution/longterm_permanence_score/longterm_permanence_score/#components.execution.longterm_permanence_score.longterm_permanence_score.LongtermPermanenceScore.compute_longterm_metrics","title":"<code>compute_longterm_metrics(df)</code>","text":"<p>Compute the Long-term Permanence Score and metrics from the filtered Mid-term Permanence Score data</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Mid-term Permanence Score DataFrame</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>dataframe with the long-term permanence score aend metrics</p> Source code in <code>multimno/components/execution/longterm_permanence_score/longterm_permanence_score.py</code> <pre><code>def compute_longterm_metrics(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Compute the Long-term Permanence Score and metrics from the filtered Mid-term Permanence Score data\n\n    Args:\n        df (DataFrame): Mid-term Permanence Score DataFrame\n\n    Returns:\n        DataFrame: dataframe with the long-term permanence score aend metrics\n    \"\"\"\n    # Split dataframes by id_type\n    grid_df = df.filter(F.col(ColNames.id_type) == F.lit(\"grid\"))\n    unknown_df = df.filter(F.col(ColNames.id_type) == F.lit(\"unknown\"))\n    obs_df = df.filter(F.col(ColNames.id_type) == F.lit(\"device_observation\"))\n\n    grid_df = grid_df.groupby(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id).agg(\n        F.sum(ColNames.mps).cast(IntegerType()).alias(ColNames.lps),\n        F.sum(ColNames.frequency).cast(IntegerType()).alias(ColNames.total_frequency),\n        F.mean(ColNames.frequency).cast(FloatType()).alias(ColNames.frequency_mean),\n        F.stddev_samp(ColNames.frequency).cast(FloatType()).alias(ColNames.frequency_std),\n        F.mean(ColNames.regularity_mean).cast(FloatType()).alias(ColNames.regularity_mean),\n        F.stddev_samp(ColNames.regularity_mean).cast(FloatType()).alias(ColNames.regularity_std),\n    )\n\n    unknown_df = unknown_df.groupby(ColNames.user_id_modulo, ColNames.user_id).agg(\n        F.sum(ColNames.mps).cast(IntegerType()).alias(ColNames.lps),\n        F.sum(ColNames.frequency).cast(IntegerType()).alias(ColNames.total_frequency),\n        F.mean(ColNames.frequency).cast(FloatType()).alias(ColNames.frequency_mean),\n        F.stddev_samp(ColNames.frequency).cast(FloatType()).alias(ColNames.frequency_std),\n        F.mean(ColNames.regularity_mean).cast(FloatType()).alias(ColNames.regularity_mean),\n        F.stddev_samp(ColNames.regularity_mean).cast(FloatType()).alias(ColNames.regularity_std),\n    )\n\n    obs_df = obs_df.groupby(ColNames.user_id_modulo, ColNames.user_id).agg(\n        F.sum(ColNames.mps).cast(IntegerType()).alias(ColNames.lps),\n        F.sum(ColNames.frequency).cast(IntegerType()).alias(ColNames.total_frequency),\n    )\n\n    grid_df = grid_df.select(\n        ColNames.user_id_modulo,\n        ColNames.user_id,\n        ColNames.grid_id,\n        ColNames.lps,\n        ColNames.total_frequency,\n        ColNames.frequency_mean,\n        ColNames.frequency_std,\n        ColNames.regularity_mean,\n        ColNames.regularity_std,\n        F.lit(\"grid\").alias(ColNames.id_type),\n    )\n\n    unknown_df = unknown_df.select(\n        ColNames.user_id_modulo,\n        ColNames.user_id,\n        F.lit(\"unknown\").alias(ColNames.grid_id),\n        ColNames.lps,\n        ColNames.total_frequency,\n        ColNames.frequency_mean,\n        ColNames.frequency_std,\n        ColNames.regularity_mean,\n        ColNames.regularity_std,\n        F.lit(\"unknown\").alias(ColNames.id_type),\n    )\n\n    obs_df = obs_df.select(\n        ColNames.user_id_modulo,\n        ColNames.user_id,\n        F.lit(\"device_observation\").alias(ColNames.grid_id),\n        ColNames.lps,\n        ColNames.total_frequency,\n        F.lit(None).cast(FloatType()).alias(ColNames.frequency_mean),\n        F.lit(None).cast(FloatType()).alias(ColNames.frequency_std),\n        F.lit(None).cast(FloatType()).alias(ColNames.regularity_mean),\n        F.lit(None).cast(FloatType()).alias(ColNames.regularity_std),\n        F.lit(\"device_observation\").alias(ColNames.id_type),\n    )\n\n    return grid_df.union(unknown_df).union(obs_df)\n</code></pre>"},{"location":"reference/components/execution/longterm_permanence_score/longterm_permanence_score/#components.execution.longterm_permanence_score.longterm_permanence_score.LongtermPermanenceScore.filter_longterm_analysis_data","title":"<code>filter_longterm_analysis_data(df)</code>","text":"<p>Applies the push-up filters on the Mid-term Permanence Score data to only consider the data from the months, day_type, and time_interval required for the current long-term analysis combination.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Mid-term Permanence Score DataFrame before filtering</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Mid-term Permanence Score DataFrame after filtering</p> Source code in <code>multimno/components/execution/longterm_permanence_score/longterm_permanence_score.py</code> <pre><code>def filter_longterm_analysis_data(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Applies the push-up filters on the Mid-term Permanence Score data to only consider the data from the\n    months, day_type, and time_interval required for the current long-term analysis combination.\n\n    Args:\n        df (DataFrame): Mid-term Permanence Score DataFrame before filtering\n\n    Returns:\n        DataFrame: Mid-term Permanence Score DataFrame after filtering\n    \"\"\"\n    month_filters = [\n        (F.col(ColNames.year) == F.lit(month.year)) &amp; (F.col(ColNames.month) == F.lit(month.month))\n        for month in self.current_lt_analysis[\"months\"]\n    ]\n\n    def logical_or(x, y):\n        return x | y\n\n    month_filter = reduce(logical_or, month_filters)\n\n    day_type_filter = F.col(ColNames.day_type) == F.lit(self.current_lt_analysis[\"day_type\"])\n\n    time_interval_filter = F.col(ColNames.time_interval) == F.lit(self.current_lt_analysis[\"time_interval\"])\n\n    filtered_df = df.filter(month_filter &amp; day_type_filter &amp; time_interval_filter)\n\n    return filtered_df\n</code></pre>"},{"location":"reference/components/execution/midterm_permanence_score/","title":"midterm_permanence_score","text":""},{"location":"reference/components/execution/midterm_permanence_score/midterm_permanence_score/","title":"midterm_permanence_score","text":"<p>Module that computes the Mid-term Permanence Score.</p>"},{"location":"reference/components/execution/midterm_permanence_score/midterm_permanence_score/#components.execution.midterm_permanence_score.midterm_permanence_score.MidtermPermanenceScore","title":"<code>MidtermPermanenceScore</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class that computes the mid term permanence score and related metrics, for different combinations of day types and time intervals in the day</p> Source code in <code>multimno/components/execution/midterm_permanence_score/midterm_permanence_score.py</code> <pre><code>class MidtermPermanenceScore(Component):\n    \"\"\"\n    Class that computes the mid term permanence score and related metrics, for different\n    combinations of day types and time intervals in the day\n    \"\"\"\n\n    COMPONENT_ID = \"MidtermPermanenceScore\"\n\n    night_time_start, night_time_end = None, None\n    working_hours_start, working_hours_end = None, None\n    evening_time_start, evening_time_end = None, None\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        # Months to process as each mid-term period\n        start_month = self.config.get(self.COMPONENT_ID, \"start_month\")\n        try:\n            self.start_date = dt.datetime.strptime(start_month, \"%Y-%m\").date()\n        except ValueError as e:\n            self.logger.error(f\"Could not parse start_month = `{start_month}`. Expected format: YYYY-MM\")\n            raise e\n\n        end_month = self.config.get(self.COMPONENT_ID, \"end_month\")\n        try:\n            self.end_date = dt.datetime.strptime(end_month, \"%Y-%m\").date()\n        except ValueError as e:\n            self.logger.error(f\"Could not parse end_month = `{end_month}`. Expected format: YYYY-MM\")\n            raise e\n        self.end_date = self.end_date.replace(day=cal.monthrange(self.end_date.year, self.end_date.month)[1])\n\n        if end_month &lt; start_month:\n            raise ValueError(f\"End month `{end_month}` should not be an earlier than start month `{start_month}`\")\n\n        # Additional days before and after each month to use for calculating regularity metrics\n        self.before_reg_days = self.config.getint(self.COMPONENT_ID, \"before_regularity_days\")\n        if self.before_reg_days &lt; 0:\n            raise ValueError(f\"`before_reg_days` must be a non-negative integer, found {self.before_reg_days}\")\n\n        self.after_reg_days = self.config.getint(self.COMPONENT_ID, \"after_regularity_days\")\n        if self.after_reg_days &lt; 0:\n            raise ValueError(f\"`after_reg_days` must be a non-negative integer, found {self.after_reg_days}\")\n\n        # list of dictionaries with each mid-term to be analysed\n        self.midterm_periods = self._get_midterm_periods()\n\n        # Hour used to define the start of a day, e.g. 4 means that a Monday starts at 4AM Monday and ends at\n        # 4AM Tuesday\n        self.day_start_hour = self.config.getint(self.COMPONENT_ID, \"day_start_hour\")\n        if self.day_start_hour &lt; 0 or self.day_start_hour &gt;= 24:\n            raise ValueError(f\"`day_start_hour` must be between 0 and 23 inclusive, found {self.day_start_hour} \")\n\n        # Read the definition of each sub-daily period (or time interval) to be studied.\n        # Set to keep track of the minutes in each the intervals start/end, which must be compared with\n        # daily permanence score to verify compatibility\n        self.midterm_minutes = set()\n        for time_interval in TIME_INTERVALS:  # night, work, evening, all\n            if time_interval == \"all\":\n                continue\n            interval_start = self.config.get(self.COMPONENT_ID, f\"{time_interval}_start\")\n            interval_start = self._check_time_interval(interval_start, name=f\"{time_interval}_start\")\n            setattr(self, f\"{time_interval}_start\", interval_start)\n\n            interval_end = self.config.get(self.COMPONENT_ID, f\"{time_interval}_end\")\n            interval_end = self._check_time_interval(interval_end, name=f\"{time_interval}_end\")\n            setattr(self, f\"{time_interval}_end\", interval_end)\n\n            if interval_start == interval_end:\n                raise ValueError(\n                    f\"{time_interval}_start and {time_interval}_end are equal, when they must be strictly \"\n                    \"different -- please provide a valid time interval\"\n                )\n\n            # Non-allowed time interval limits. Example:\n            # self.day_start_hour = 4 (4AM)\n            # interval_start = 03:30, interval_end = 01:00\n            # The time interval starts at 03:30 of day D-1 and ends at 01:00 of day D, but would belong to day D-1\n            if (\n                interval_start.hour &lt; self.day_start_hour\n                and interval_end != dt.time(0, 0)\n                and (interval_end &lt; interval_start)\n            ):\n                raise ValueError(\n                    \"Invalid configuration: the following order of of parameters is not allowed:\\n\"\n                    f\"\\t {time_interval}_end ({interval_end}) &lt; {time_interval}_start ({interval_start}) &lt; \"\n                    f\"day_start_hour ({self.day_start_hour})\"\n                )\n\n            # Additional prohibited time interval (except for nights): time interval must not cross the self.dat_start_hour\n            if time_interval != \"night_time\":\n                if interval_start.hour &lt; self.day_start_hour and (\n                    interval_end.hour &gt; self.day_start_hour\n                    or (interval_end.hour == self.day_start_hour and interval_end.minute != 0)\n                ):\n                    raise ValueError(\n                        \"Invalid configuration: the following order of parameters is not allowed:\\n\"\n                        f\"{time_interval}_start ({interval_start}) &lt; day_start_hour ({self.day_start_hour}) &lt; {time_interval}_end ({interval_end})\"\n                    )\n\n            self.midterm_minutes.add(interval_start.minute)\n            self.midterm_minutes.add(interval_end.minute)\n\n        # Day of the week marking the start of the weekend, (starting in self.day_start_hour)\n        weekend_start_str = self.config.get(self.COMPONENT_ID, \"weekend_start\")\n        self.weekend_start_day = self._check_weekday_number(weekend_start_str, context=weekend_start_str)\n\n        # Day of the week marking the end of the weekend, date included, (ending right before self.day_start_hour)\n        weekend_end_str = self.config.get(self.COMPONENT_ID, \"weekend_end\")\n        self.weekend_end_day = self._check_weekday_number(weekend_end_str, context=weekend_end_str)\n\n        # List of days of the week composing the weekend\n        self.weekend_days = []\n        dd = self.weekend_start_day\n        while dd != self.weekend_end_day:\n            self.weekend_days.append(dd)\n            dd = (dd) % 7 + 1\n        self.weekend_days.append(dd)\n\n        # Work days are those that are not part of the weekend (also excluding holidays later on)\n        self.work_days = sorted(list({1, 2, 3, 4, 5, 6, 7}.difference(self.weekend_days)))\n\n        # Read from configuration the combination of sub-monthly and sub-daily pairs, i.e. day types and time intervals,\n        # to compute\n        period_combinations = self.config.geteval(self.COMPONENT_ID, \"period_combinations\")\n        self.period_combinations = {}\n        for key, vals in period_combinations.items():\n            if key.lower() not in DAY_TYPES:\n                raise ValueError(f\"Unknown day type `{key}` in period_combinations\")\n            self.period_combinations[key.lower()] = []\n            if len(vals) != len(set(vals)):\n                raise ValueError(\n                    f\"Repeated values for time interval in period_combinations under `{key}`:\",\n                    str(period_combinations[key]),\n                )\n            for val in vals:\n                if val not in TIME_INTERVALS:\n                    raise ValueError(f\"Unknown time interval `{val}` in period_combinations under `{key}`\")\n                self.period_combinations[key.lower()].append(val)\n\n        # Country of study, used to load its holidays\n        self.country_of_study = self.config.get(self.COMPONENT_ID, \"country_of_study\")\n\n        # Initialise variable for working in each midterm_period\n        self.day_type = None\n        self.time_interval = None\n        self.current_mt_period = None\n        self.current_dps_data = None\n\n    def _get_midterm_periods(self) -&gt; list[dict]:\n        \"\"\"Computes the date limits of each mid-term period, together with the limits of the regularity metrics' extra\n        dates\n\n        Returns:\n            list[dict]: list of dictionaries with the information of dates of each mid-term period\n        \"\"\"\n        midterm_periods = []\n        start_of_the_month = self.start_date\n\n        while True:\n            end_of_the_month = start_of_the_month.replace(\n                day=cal.monthrange(start_of_the_month.year, start_of_the_month.month)[1]\n            )\n            before_reg_date = start_of_the_month - dt.timedelta(days=self.before_reg_days)\n            after_reg_date = end_of_the_month + dt.timedelta(days=self.after_reg_days)\n\n            midterm_periods.append(\n                {\n                    \"month_start\": start_of_the_month,\n                    \"month_end\": end_of_the_month,\n                    \"extended_month_start\": before_reg_date,\n                    \"extended_month_end\": after_reg_date,\n                }\n            )\n\n            if end_of_the_month == self.end_date:\n                return midterm_periods\n\n            start_of_the_month = end_of_the_month + dt.timedelta(days=1)\n\n    def _check_weekday_number(self, num: str, context: str) -&gt; int:\n        \"\"\"Parses and validates a day of the week\n\n        Args:\n            num (str): string to be parsed to integer between 1 and 7\n            context (str): string for error tracking\n\n        Raises:\n            e: Error in parsing num to int\n            ValueError: num is not a valid day of the week (between 1 and 7 inclusive)\n\n        Returns:\n            int: integer representing a day of the week\n        \"\"\"\n        try:\n            num = int(num)\n        except Exception as e:\n            self.logger.error(f\"Must specify a day as an integer between 1 and 7, but found `{num}` in `{context}`\")\n            raise e\n        if num &lt; 1 or num &gt; 7:\n            raise ValueError(\n                f\"Days must take a value between 1 for Monday and 7 for Sunday, found {num} in `{context}`\"\n            )\n        return num\n\n    def _check_time_interval(self, interval: str, name: str) -&gt; dt.time:\n        \"\"\"Tries to parse time interval's start/end time from configuration file and check if it has\n        valid minutes (00, 15, 30, or 45). If so, returns the corresponding dt.time object\n\n        Args:\n            interval (str): interval string to be parsed to dt.datetime\n            name (str): name of the interval being parsed, used for error tracking\n\n        Raises:\n            e: Formatting error, cannot parse time as HH:MM (24h format)\n            ValueError: interval ends in non-allowed minutes\n\n        Returns:\n            dt.time: time of the start or end of the time interval\n        \"\"\"\n        try:\n            interval = dt.datetime.strptime(interval, \"%H:%M\")\n        except ValueError as e:\n            self.logger.error(f\"Could not parse {name}, expected HH:MM format, found {interval}\")\n            raise e\n\n        interval = interval.time()\n        if interval.minute not in [0, 15, 30, 45]:\n            raise ValueError(f\"Time interval {name} must have :00, :15, :30, or :45 minutes, found :{interval.minute}\")\n        return interval\n\n    def initalize_data_objects(self):\n        input_silver_daily_ps_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"daily_permanence_score_data_silver\")\n        input_bronze_holiday_calendar_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"holiday_calendar_data_bronze\")\n        output_silver_midterm_ps_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"midterm_permanence_score_data_silver\")\n\n        daily_ps = SilverDailyPermanenceScoreDataObject(self.spark, input_silver_daily_ps_path)\n        holiday_calendar = BronzeHolidayCalendarDataObject(self.spark, input_bronze_holiday_calendar_path)\n        midterm_ps = SilverMidtermPermanenceScoreDataObject(self.spark, output_silver_midterm_ps_path)\n\n        self.input_data_objects = {\n            holiday_calendar.ID: holiday_calendar,\n            daily_ps.ID: daily_ps,\n        }\n        self.output_data_objects = {midterm_ps.ID: midterm_ps}\n\n    def _validate_and_load_daily_permanence_score(self, mt_period: dict) -&gt; DataFrame:\n        \"\"\"Loads the Daily Permanence Score data to be used for the calculation of the Mid-term Permanence Score metrics\n        of a particular mid-term period. Filters out DPS values equal to zero and checks that the time slots are\n        compatible with the configuration-provided time intervals.\n\n        Raises:\n            ValueError: If DPS data has a time slot duration different from 15, 30, or 60 minutes.\n            ValueError: If DPS data has 60-min slots but 15- or 30-min lengths are required.\n            ValueError: If DPS data has 30-min slots but 15-min lengths are required.\n\n        Returns:\n            dps: DataFrame of all DPS data necessary to calcualte the mid-term permanence score &amp; metrics of\n                self.current_mt_period\n        \"\"\"\n        dps = self.input_data_objects[SilverDailyPermanenceScoreDataObject.ID].df\n\n        # First filter: take out DPS = 0\n        dps = dps.filter(F.col(ColNames.dps) &gt; 0)\n\n        # Add a one-day buffer, as later on the definition of a day does not match the midnight definition\n        dps = dps.filter(\n            F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n            &gt;= F.lit(mt_period[\"extended_month_start\"] - dt.timedelta(days=1))\n        ).filter(\n            F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n            &lt;= F.lit(mt_period[\"extended_month_end\"]) + dt.timedelta(days=1)\n        )\n\n        # If all time_intervals match a whole hour, no check needs to be done here.\n        if self.midterm_minutes == {0}:\n            return dps\n\n        # If the previous set was not disjoint, then the user has entered some time_interval that does not match a\n        # whole hour. We check, for all loaded dates, what their time slot length was. We only need to check the length\n        # of one time slot per DPS date.\n        time_slot_duration = (\n            dps.groupby([ColNames.year, ColNames.month, ColNames.day])\n            .agg(\n                F.first(F.col(ColNames.time_slot_end_time) - F.col(ColNames.time_slot_initial_time)).alias(\n                    \"time_slot_duration\"\n                )\n            )\n            .collect()\n        )\n        daily_duration = {\n            dt.date(row[\"year\"], row[\"month\"], row[\"day\"]): row[\"time_slot_duration\"].seconds // 60\n            for row in time_slot_duration\n        }\n\n        # If disjoint, user has specified intervals ending in :00 or :30. Then we admit 15- or 30-min time slot\n        # durations, but not whole hours.\n        # If not disjoint, user has specified some :15 or :45 intervals, and we can only admit 15-min time slots\n        check_for_half_hour_only = {15, 45}.isdisjoint(self.midterm_minutes)\n\n        for date, duration in daily_duration.items():\n            if duration not in {15, 30, 60}:\n                raise ValueError(\n                    f\"Found time_slot duration of {duration} min in DailyPermanenceScore of {date}, when \"\n                    \"accepted values are 15, 30, or 60 minutes.\"\n                )\n            if duration == 60:\n                if check_for_half_hour_only:\n                    msg = (\n                        f\"Found time_slot duration of 60 min in DailyPermanenceScore of {date}, when only accepted \"\n                        \"values are 15 and 30\"\n                    )\n                else:\n                    msg = (\n                        f\"Found time_slot duration of 60 min in DailyPermanenceScore of {date}, when only accepted \"\n                        \"value is 15\"\n                    )\n                raise ValueError(msg)\n            if duration == 30 and not check_for_half_hour_only:\n                raise ValueError(\n                    f\"Found time_slot duration of 30 min in DailyPermanenceScore of {date}, when only accepted \"\n                    \"value is 15\"\n                )\n        return dps\n\n    def filter_dps_by_time_interval(\n        self, df: DataFrame, subdaily_period: str, start: dt.time, end: dt.time\n    ) -&gt; DataFrame:\n        \"\"\"Filter DPS matrix by keeping the time slots that belong to the subdaily period or time interval\n        specified. Also create a new column, `date`, which contains to which day the time slot belongs to, as it\n        does not necessarily match the day the time slot belongs to (i.e. the `midnight` definition)\n\n        Args:\n            df (DataFrame): DPS dataframe to be filtered.\n            subdaily_period (str): name of the time interval or subdaily period. Must be one of the values in\n                MidtermPermanenceScore.TIME_INTERVALS.\n            start (dt.time): earliest time of accepted time slots that will not be filtered out\n            end (dt.time): latest time of accepted time slots that will not be filtered out\n\n        Raises:\n            ValueError: Whenever an unknown subdaily period is specified\n\n        Returns:\n            DataFrame: filtered DPS dataframe\n        \"\"\"\n        if subdaily_period not in TIME_INTERVALS:\n            raise ValueError(f\"Unknown subdaily/time_interval {subdaily_period}\")\n\n        # Auxiliary variables\n        start_hour = F.lit(start.hour)\n        start_min = F.lit(start.minute)\n        end_hour = end.hour\n        if end_hour == 0:\n            end_hour = F.lit(24)\n        else:\n            end_hour = F.lit(end_hour)\n        end_min = F.lit(end.minute)\n\n        slot_initial_hour = F.hour(ColNames.time_slot_initial_time)\n        slot_initial_min = F.minute(ColNames.time_slot_initial_time)\n        slot_end_hour = F.hour(ColNames.time_slot_end_time)\n        slot_end_hour = F.when(slot_end_hour == F.lit(0), F.lit(24)).otherwise(slot_end_hour)\n        slot_end_min = F.minute(ColNames.time_slot_end_time)\n\n        # Global time interval, taking all time slots\n        if subdaily_period == \"all\":\n            if start != end:\n                raise ValueError(\n                    \"`all` time interval must have matching start and end times to not overlap with \"\n                    f\"different dates, found start={start} and end={end}\"\n                )\n            # If the day starts and ends at midnight, no additional logic needs to be done for assigning the correct\n            # date\n            if start == dt.time(0, 0):\n                df = df.withColumn(ColNames.date, F.col(ColNames.time_slot_initial_time).cast(DateType()))\n                return df\n            # If not: the hour that defines the day always belongs to that day.\n            # e.g., if subdaily_start=4, then day D is defined as 4:00AM of day D to 4:00 AM of day D+1\n            df = df.withColumn(\n                ColNames.date,\n                F.when(\n                    F.lit(60) * (slot_initial_hour - start_hour) &gt;= (start_min - slot_initial_min),\n                    F.col(ColNames.time_slot_initial_time),\n                )\n                .otherwise(F.date_add(F.col(ColNames.time_slot_initial_time), -1))\n                .cast(DateType()),\n            )\n            return df\n\n        # Rest of time intervals: night_time, evening_time, working_hours.\n        # Filter out time slots not contained in the time interval\n        if start &lt; end or end == dt.time(0, 0):\n            df = df.filter(\n                (F.lit(60) * (slot_initial_hour - start_hour) &gt;= (start_min - slot_initial_min))\n                &amp; (F.lit(60) * (slot_end_hour - end_hour) &lt;= (end_min - slot_end_min))\n            )\n        else:\n            df = df.filter(\n                (F.lit(60) * (slot_initial_hour - start_hour) &gt;= (start_min - slot_initial_min))\n                | (F.lit(60) * (slot_end_hour - end_hour) &lt;= (end_min - slot_end_min))\n            )\n\n        # consider self.day_start_hour = 4 for the following examples\n        if subdaily_period == \"night_time\":\n            if start.hour &gt;= self.day_start_hour and (end &gt; start or end == dt.time(0, 0)):\n                # night-interval stays in the day, example [20:15 to 23:30] or [19:45 to 00:00]\n                df = df.withColumn(ColNames.date, F.col(ColNames.time_slot_initial_time).cast(DateType()))\n            elif start.hour &gt;= self.day_start_hour and end &lt; start and end != dt.time(0, 0):\n                # night-interval starts in the day and crosses midnight, example [20:15 to 03:30] or [19:00 to 18:45]\n                df = df.withColumn(\n                    ColNames.date,\n                    F.when(\n                        F.lit(60) * (slot_initial_hour - start_hour) &gt;= (start_min - slot_initial_min),\n                        F.col(ColNames.time_slot_initial_time),\n                    )\n                    .otherwise(F.date_add(F.col(ColNames.time_slot_initial_time), -1))\n                    .cast(DateType()),\n                )\n            elif start.hour &lt; self.day_start_hour and (end &gt; start or end == dt.time(0, 0)):\n                # night-interval starts in the next day and does not cross midnight, example [3:15 to 6:00] or [2:30 to 10:45]\n                df = df.withColumn(\n                    ColNames.date, F.date_add(F.col(ColNames.time_slot_initial_time), -1).cast(DateType())\n                )\n            return df\n\n        # if subdaily_period in (\"working_hours\", \"evening_time\"):\n        if start &gt;= end and end != dt.time(0, 0):\n            self.logger.log(\n                msg=f\"'inverted' start ({start}) and end ({end}) times found for subdaily period {subdaily_period}\"\n                \" -- the whole period will belong to the day of start of the interval\",\n                level=logging.INFO,\n            )\n        df = df.withColumn(\n            ColNames.date,\n            F.when(\n                F.lit(60) * (slot_initial_hour - start_hour) &gt;= (start_min - slot_initial_min),\n                F.col(ColNames.time_slot_initial_time),\n            )\n            .otherwise(F.date_add(F.col(ColNames.time_slot_initial_time), -1))\n            .cast(DateType()),\n        )\n        return df\n\n    def filter_dps_by_day_type(self, df: DataFrame, submonthly_period: str) -&gt; DataFrame:\n        \"\"\"Filter DPS matrix, with an assigned date column, keeping the time slots that belong to the day type or\n        submonthly period specified.\n\n        Args:\n            df (DataFrame): DPS dataframe, with assigned `date` column\n            submonthly_period (str): submonthly period or day type. Must be one of the values in\n                MidtermPermanenceScore.DAY_TYPE\n\n        Raises:\n            ValueError: Whenever an unknown submonthly period is specified\n\n        Returns:\n            DataFrame: Filtered dataframe\n        \"\"\"\n        # Now that we have the date column, we filter unneeded dates again, as we took one extra day before and after\n        df = df.filter(\n            F.col(ColNames.date).between(\n                lowerBound=self.current_mt_period[\"extended_month_start\"],\n                upperBound=self.current_mt_period[\"extended_month_end\"],\n            )\n        )\n\n        if submonthly_period not in DAY_TYPES:\n            raise ValueError(f\"Unknown submonthly period/day type `{submonthly_period}`\")\n        if submonthly_period == \"all\":\n            return df\n\n        if submonthly_period == \"weekends\":\n            df = df.filter((F.weekday(ColNames.date) + F.lit(1)).isin(self.weekend_days))\n            return df\n        if submonthly_period == \"mondays\":\n            df = df.filter((F.weekday(ColNames.date)).isin([0]))\n            return df\n\n        if submonthly_period == \"tuesdays\":\n            df = df.filter((F.weekday(ColNames.date)).isin([1]))\n            return df\n        if submonthly_period == \"wednesdays\":\n            df = df.filter((F.weekday(ColNames.date)).isin([2]))\n            return df\n        if submonthly_period == \"thursdays\":\n            df = df.filter((F.weekday(ColNames.date)).isin([3]))\n            return df\n        if submonthly_period == \"fridays\":\n            df = df.filter((F.weekday(ColNames.date)).isin([4]))\n            return df\n        if submonthly_period == \"saturdays\":\n            df = df.filter((F.weekday(ColNames.date)).isin([5]))\n            return df\n        if submonthly_period == \"sundays\":\n            df = df.filter((F.weekday(ColNames.date)).isin([6]))\n            return df\n\n        holidays = F.broadcast(self.input_data_objects[\"BronzeHolidayCalendarInfoDO\"].df)\n        holidays = holidays.filter(F.col(ColNames.iso2) == F.lit(self.country_of_study))\n        if submonthly_period == \"holidays\":\n            df = df.join(holidays.select(ColNames.date), on=ColNames.date, how=\"inner\")\n        # Workdays are all days falling in one of self.work_days and not being a holiday\n        if submonthly_period == \"workdays\":\n            df = df.join(holidays.select(ColNames.date), on=ColNames.date, how=\"left_anti\").filter(\n                (F.weekday(ColNames.date) + F.lit(1)).isin(self.work_days)\n            )\n        return df\n\n    def compute_midterm_metrics(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Compute the mid-term permanence score and metrics of the current mid-term period, submonthly (i.e. day type)\n        and subdaily (i.e. time interval) combination.\n\n        Args:\n            df (DataFrame): filtered DPS DataFrame with added `date` column\n\n        Returns:\n            DataFrame: resulting DataFrame\n        \"\"\"\n        # Find latest stay of the user in each grid tile during the lookback period for regularity metrics\n        # (if it exists)\n        before_reg = (\n            df.filter(F.col(ColNames.date) &lt; F.lit(self.current_mt_period[\"month_start\"]))\n            .groupBy(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id)\n            .agg(F.max(ColNames.date).alias(ColNames.date))\n        )\n\n        # Find earliest stay of the user in each grid tile during the lookback period for regularity metrics\n        # (if it exists)\n        after_reg = (\n            df.filter(F.col(ColNames.date) &gt; F.lit(self.current_mt_period[\"month_end\"]))\n            .groupBy(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id)\n            .agg(F.min(ColNames.date).alias(ColNames.date))\n        )\n\n        # Current month data\n        study_df = df.filter(\n            F.col(ColNames.date).between(\n                lowerBound=self.current_mt_period[\"month_start\"], upperBound=self.current_mt_period[\"month_end\"]\n            )\n        )\n\n        # Device observation metric\n        observation_df = (\n            study_df.filter(F.col(ColNames.id_type) == F.lit(\"grid\"))\n            .groupby(ColNames.user_id_modulo, ColNames.user_id, ColNames.date)\n            .agg(F.count_distinct(ColNames.time_slot_initial_time).alias(\"observed_day_dps\"))\n            .groupby(ColNames.user_id_modulo, ColNames.user_id)\n            .agg(\n                F.sum(\"observed_day_dps\").alias(ColNames.mps),\n                F.count_distinct(ColNames.date).cast(IntegerType()).alias(ColNames.frequency),\n            )\n            .select(\n                ColNames.user_id_modulo,\n                ColNames.user_id,\n                F.lit(\"device_observation\").alias(ColNames.grid_id),\n                F.col(ColNames.mps).cast(IntegerType()).alias(ColNames.mps),\n                ColNames.frequency,\n                F.lit(None).cast(FloatType()).alias(ColNames.regularity_mean),\n                F.lit(None).cast(FloatType()).alias(ColNames.regularity_std),\n                F.lit(\"device_observation\").alias(ColNames.id_type),\n            )\n        )\n\n        combined_df = (\n            study_df.select(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id, ColNames.date, ColNames.dps)\n            .union(before_reg.withColumn(ColNames.dps, F.lit(0)))\n            .union(after_reg.withColumn(ColNames.dps, F.lit(0)))\n            .groupby(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id)\n            .agg(\n                F.sum(ColNames.dps).cast(IntegerType()).alias(ColNames.mps),\n                F.array_sort(F.collect_set(ColNames.date)).alias(\"dates\"),\n            )\n            .filter(F.col(ColNames.mps) &gt; 0)\n            .withColumn(\n                \"more_metrics\",\n                frequency_and_regularity(\n                    F.col(\"dates\"),\n                    F.lit(self.current_mt_period[\"month_start\"]),\n                    F.lit(self.current_mt_period[\"extended_month_start\"]),\n                    F.lit(self.current_mt_period[\"month_end\"]),\n                    F.lit(self.current_mt_period[\"extended_month_end\"]),\n                ),\n            )\n            .drop(\"dates\")\n            .withColumn(ColNames.frequency, F.col(\"more_metrics\")[0].cast(IntegerType()))\n            .withColumn(\n                ColNames.regularity_mean,\n                F.col(\"more_metrics\")[1],\n            )\n            .withColumn(ColNames.regularity_std, F.col(\"more_metrics\")[2])\n            .withColumn(\n                ColNames.id_type,\n                F.when(F.col(ColNames.grid_id) == F.lit(\"unknown\"), F.lit(\"unknown\")).otherwise(F.lit(\"grid\")),\n            )\n            .drop(\"more_metrics\")\n            .union(observation_df)\n        )\n\n        return combined_df\n\n    def transform(self):\n        if self.time_interval == \"all\":\n            time_interval_start = dt.time(hour=self.day_start_hour)\n            time_interval_end = dt.time(hour=self.day_start_hour)\n        else:\n            time_interval_start = getattr(self, f\"{self.time_interval}_start\")\n            time_interval_end = getattr(self, f\"{self.time_interval}_end\")\n\n        # Keep only time slots belonging to the time interval\n        filtered = self.filter_dps_by_time_interval(\n            self.current_dps_data, self.time_interval, time_interval_start, time_interval_end\n        )\n        # Keep only time slots belonging to the day type\n        filtered = self.filter_dps_by_day_type(filtered, self.day_type)\n        # Compute metrics\n        mps = self.compute_midterm_metrics(filtered)\n\n        mps = (\n            mps.withColumn(ColNames.day_type, F.lit(self.day_type))\n            .withColumn(ColNames.time_interval, F.lit(self.time_interval))\n            .withColumn(ColNames.year, F.lit(self.current_mt_period[\"month_start\"].year).cast(ShortType()))\n            .withColumn(ColNames.month, F.lit(self.current_mt_period[\"month_start\"].month).cast(ByteType()))\n        )\n\n        self.output_data_objects[\"SilverMidtermPermanenceScoreDO\"].df = mps\n\n    def execute(self):\n        self.logger.info(\"Reading data objects...\")\n        self.read()\n        self.logger.info(\"... data objects read!\")\n\n        midterm_daily_data = []\n\n        self.logger.info(\"Validating DPS data for each mid-term period...\")\n        for i, mt_period in enumerate(self.midterm_periods):\n            self.logger.info(\n                f\"... validating {mt_period['extended_month_start']} to {mt_period['extended_month_end']} ...\"\n            )\n            midterm_daily_data.append(self._validate_and_load_daily_permanence_score(mt_period))\n        self.logger.info(\"... all mid-term periods validated!\")\n\n        self.logger.info(\"Starting mid-term permanece score &amp; metrics computation...\")\n        for i, mt_period in enumerate(self.midterm_periods):\n            self.current_mt_period = mt_period\n            self.current_dps_data = midterm_daily_data[i]\n            self.logger.info(f\"... working on month {mt_period['month_start']} to {mt_period['month_end']}\")\n            for day_type, time_intervals in self.period_combinations.items():\n                for time_interval in time_intervals:\n                    self.day_type = day_type\n                    self.time_interval = time_interval\n                    self.transform()\n                    self.write()\n                    self.logger.info(\n                        f\"... finished saving results for day_type `{self.day_type}` and time_interval `{self.time_interval}`\"\n                    )\n            self.logger.info(\n                f\"... finished saving results for month {mt_period['month_start']} to {mt_period['month_end']}\"\n            )\n\n        self.logger.info(\"... Finished!\")\n</code></pre>"},{"location":"reference/components/execution/midterm_permanence_score/midterm_permanence_score/#components.execution.midterm_permanence_score.midterm_permanence_score.MidtermPermanenceScore.compute_midterm_metrics","title":"<code>compute_midterm_metrics(df)</code>","text":"<p>Compute the mid-term permanence score and metrics of the current mid-term period, submonthly (i.e. day type) and subdaily (i.e. time interval) combination.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>filtered DPS DataFrame with added <code>date</code> column</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>resulting DataFrame</p> Source code in <code>multimno/components/execution/midterm_permanence_score/midterm_permanence_score.py</code> <pre><code>def compute_midterm_metrics(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Compute the mid-term permanence score and metrics of the current mid-term period, submonthly (i.e. day type)\n    and subdaily (i.e. time interval) combination.\n\n    Args:\n        df (DataFrame): filtered DPS DataFrame with added `date` column\n\n    Returns:\n        DataFrame: resulting DataFrame\n    \"\"\"\n    # Find latest stay of the user in each grid tile during the lookback period for regularity metrics\n    # (if it exists)\n    before_reg = (\n        df.filter(F.col(ColNames.date) &lt; F.lit(self.current_mt_period[\"month_start\"]))\n        .groupBy(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id)\n        .agg(F.max(ColNames.date).alias(ColNames.date))\n    )\n\n    # Find earliest stay of the user in each grid tile during the lookback period for regularity metrics\n    # (if it exists)\n    after_reg = (\n        df.filter(F.col(ColNames.date) &gt; F.lit(self.current_mt_period[\"month_end\"]))\n        .groupBy(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id)\n        .agg(F.min(ColNames.date).alias(ColNames.date))\n    )\n\n    # Current month data\n    study_df = df.filter(\n        F.col(ColNames.date).between(\n            lowerBound=self.current_mt_period[\"month_start\"], upperBound=self.current_mt_period[\"month_end\"]\n        )\n    )\n\n    # Device observation metric\n    observation_df = (\n        study_df.filter(F.col(ColNames.id_type) == F.lit(\"grid\"))\n        .groupby(ColNames.user_id_modulo, ColNames.user_id, ColNames.date)\n        .agg(F.count_distinct(ColNames.time_slot_initial_time).alias(\"observed_day_dps\"))\n        .groupby(ColNames.user_id_modulo, ColNames.user_id)\n        .agg(\n            F.sum(\"observed_day_dps\").alias(ColNames.mps),\n            F.count_distinct(ColNames.date).cast(IntegerType()).alias(ColNames.frequency),\n        )\n        .select(\n            ColNames.user_id_modulo,\n            ColNames.user_id,\n            F.lit(\"device_observation\").alias(ColNames.grid_id),\n            F.col(ColNames.mps).cast(IntegerType()).alias(ColNames.mps),\n            ColNames.frequency,\n            F.lit(None).cast(FloatType()).alias(ColNames.regularity_mean),\n            F.lit(None).cast(FloatType()).alias(ColNames.regularity_std),\n            F.lit(\"device_observation\").alias(ColNames.id_type),\n        )\n    )\n\n    combined_df = (\n        study_df.select(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id, ColNames.date, ColNames.dps)\n        .union(before_reg.withColumn(ColNames.dps, F.lit(0)))\n        .union(after_reg.withColumn(ColNames.dps, F.lit(0)))\n        .groupby(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id)\n        .agg(\n            F.sum(ColNames.dps).cast(IntegerType()).alias(ColNames.mps),\n            F.array_sort(F.collect_set(ColNames.date)).alias(\"dates\"),\n        )\n        .filter(F.col(ColNames.mps) &gt; 0)\n        .withColumn(\n            \"more_metrics\",\n            frequency_and_regularity(\n                F.col(\"dates\"),\n                F.lit(self.current_mt_period[\"month_start\"]),\n                F.lit(self.current_mt_period[\"extended_month_start\"]),\n                F.lit(self.current_mt_period[\"month_end\"]),\n                F.lit(self.current_mt_period[\"extended_month_end\"]),\n            ),\n        )\n        .drop(\"dates\")\n        .withColumn(ColNames.frequency, F.col(\"more_metrics\")[0].cast(IntegerType()))\n        .withColumn(\n            ColNames.regularity_mean,\n            F.col(\"more_metrics\")[1],\n        )\n        .withColumn(ColNames.regularity_std, F.col(\"more_metrics\")[2])\n        .withColumn(\n            ColNames.id_type,\n            F.when(F.col(ColNames.grid_id) == F.lit(\"unknown\"), F.lit(\"unknown\")).otherwise(F.lit(\"grid\")),\n        )\n        .drop(\"more_metrics\")\n        .union(observation_df)\n    )\n\n    return combined_df\n</code></pre>"},{"location":"reference/components/execution/midterm_permanence_score/midterm_permanence_score/#components.execution.midterm_permanence_score.midterm_permanence_score.MidtermPermanenceScore.filter_dps_by_day_type","title":"<code>filter_dps_by_day_type(df, submonthly_period)</code>","text":"<p>Filter DPS matrix, with an assigned date column, keeping the time slots that belong to the day type or submonthly period specified.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DPS dataframe, with assigned <code>date</code> column</p> required <code>submonthly_period</code> <code>str</code> <p>submonthly period or day type. Must be one of the values in MidtermPermanenceScore.DAY_TYPE</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Whenever an unknown submonthly period is specified</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered dataframe</p> Source code in <code>multimno/components/execution/midterm_permanence_score/midterm_permanence_score.py</code> <pre><code>def filter_dps_by_day_type(self, df: DataFrame, submonthly_period: str) -&gt; DataFrame:\n    \"\"\"Filter DPS matrix, with an assigned date column, keeping the time slots that belong to the day type or\n    submonthly period specified.\n\n    Args:\n        df (DataFrame): DPS dataframe, with assigned `date` column\n        submonthly_period (str): submonthly period or day type. Must be one of the values in\n            MidtermPermanenceScore.DAY_TYPE\n\n    Raises:\n        ValueError: Whenever an unknown submonthly period is specified\n\n    Returns:\n        DataFrame: Filtered dataframe\n    \"\"\"\n    # Now that we have the date column, we filter unneeded dates again, as we took one extra day before and after\n    df = df.filter(\n        F.col(ColNames.date).between(\n            lowerBound=self.current_mt_period[\"extended_month_start\"],\n            upperBound=self.current_mt_period[\"extended_month_end\"],\n        )\n    )\n\n    if submonthly_period not in DAY_TYPES:\n        raise ValueError(f\"Unknown submonthly period/day type `{submonthly_period}`\")\n    if submonthly_period == \"all\":\n        return df\n\n    if submonthly_period == \"weekends\":\n        df = df.filter((F.weekday(ColNames.date) + F.lit(1)).isin(self.weekend_days))\n        return df\n    if submonthly_period == \"mondays\":\n        df = df.filter((F.weekday(ColNames.date)).isin([0]))\n        return df\n\n    if submonthly_period == \"tuesdays\":\n        df = df.filter((F.weekday(ColNames.date)).isin([1]))\n        return df\n    if submonthly_period == \"wednesdays\":\n        df = df.filter((F.weekday(ColNames.date)).isin([2]))\n        return df\n    if submonthly_period == \"thursdays\":\n        df = df.filter((F.weekday(ColNames.date)).isin([3]))\n        return df\n    if submonthly_period == \"fridays\":\n        df = df.filter((F.weekday(ColNames.date)).isin([4]))\n        return df\n    if submonthly_period == \"saturdays\":\n        df = df.filter((F.weekday(ColNames.date)).isin([5]))\n        return df\n    if submonthly_period == \"sundays\":\n        df = df.filter((F.weekday(ColNames.date)).isin([6]))\n        return df\n\n    holidays = F.broadcast(self.input_data_objects[\"BronzeHolidayCalendarInfoDO\"].df)\n    holidays = holidays.filter(F.col(ColNames.iso2) == F.lit(self.country_of_study))\n    if submonthly_period == \"holidays\":\n        df = df.join(holidays.select(ColNames.date), on=ColNames.date, how=\"inner\")\n    # Workdays are all days falling in one of self.work_days and not being a holiday\n    if submonthly_period == \"workdays\":\n        df = df.join(holidays.select(ColNames.date), on=ColNames.date, how=\"left_anti\").filter(\n            (F.weekday(ColNames.date) + F.lit(1)).isin(self.work_days)\n        )\n    return df\n</code></pre>"},{"location":"reference/components/execution/midterm_permanence_score/midterm_permanence_score/#components.execution.midterm_permanence_score.midterm_permanence_score.MidtermPermanenceScore.filter_dps_by_time_interval","title":"<code>filter_dps_by_time_interval(df, subdaily_period, start, end)</code>","text":"<p>Filter DPS matrix by keeping the time slots that belong to the subdaily period or time interval specified. Also create a new column, <code>date</code>, which contains to which day the time slot belongs to, as it does not necessarily match the day the time slot belongs to (i.e. the <code>midnight</code> definition)</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DPS dataframe to be filtered.</p> required <code>subdaily_period</code> <code>str</code> <p>name of the time interval or subdaily period. Must be one of the values in MidtermPermanenceScore.TIME_INTERVALS.</p> required <code>start</code> <code>time</code> <p>earliest time of accepted time slots that will not be filtered out</p> required <code>end</code> <code>time</code> <p>latest time of accepted time slots that will not be filtered out</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Whenever an unknown subdaily period is specified</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>filtered DPS dataframe</p> Source code in <code>multimno/components/execution/midterm_permanence_score/midterm_permanence_score.py</code> <pre><code>def filter_dps_by_time_interval(\n    self, df: DataFrame, subdaily_period: str, start: dt.time, end: dt.time\n) -&gt; DataFrame:\n    \"\"\"Filter DPS matrix by keeping the time slots that belong to the subdaily period or time interval\n    specified. Also create a new column, `date`, which contains to which day the time slot belongs to, as it\n    does not necessarily match the day the time slot belongs to (i.e. the `midnight` definition)\n\n    Args:\n        df (DataFrame): DPS dataframe to be filtered.\n        subdaily_period (str): name of the time interval or subdaily period. Must be one of the values in\n            MidtermPermanenceScore.TIME_INTERVALS.\n        start (dt.time): earliest time of accepted time slots that will not be filtered out\n        end (dt.time): latest time of accepted time slots that will not be filtered out\n\n    Raises:\n        ValueError: Whenever an unknown subdaily period is specified\n\n    Returns:\n        DataFrame: filtered DPS dataframe\n    \"\"\"\n    if subdaily_period not in TIME_INTERVALS:\n        raise ValueError(f\"Unknown subdaily/time_interval {subdaily_period}\")\n\n    # Auxiliary variables\n    start_hour = F.lit(start.hour)\n    start_min = F.lit(start.minute)\n    end_hour = end.hour\n    if end_hour == 0:\n        end_hour = F.lit(24)\n    else:\n        end_hour = F.lit(end_hour)\n    end_min = F.lit(end.minute)\n\n    slot_initial_hour = F.hour(ColNames.time_slot_initial_time)\n    slot_initial_min = F.minute(ColNames.time_slot_initial_time)\n    slot_end_hour = F.hour(ColNames.time_slot_end_time)\n    slot_end_hour = F.when(slot_end_hour == F.lit(0), F.lit(24)).otherwise(slot_end_hour)\n    slot_end_min = F.minute(ColNames.time_slot_end_time)\n\n    # Global time interval, taking all time slots\n    if subdaily_period == \"all\":\n        if start != end:\n            raise ValueError(\n                \"`all` time interval must have matching start and end times to not overlap with \"\n                f\"different dates, found start={start} and end={end}\"\n            )\n        # If the day starts and ends at midnight, no additional logic needs to be done for assigning the correct\n        # date\n        if start == dt.time(0, 0):\n            df = df.withColumn(ColNames.date, F.col(ColNames.time_slot_initial_time).cast(DateType()))\n            return df\n        # If not: the hour that defines the day always belongs to that day.\n        # e.g., if subdaily_start=4, then day D is defined as 4:00AM of day D to 4:00 AM of day D+1\n        df = df.withColumn(\n            ColNames.date,\n            F.when(\n                F.lit(60) * (slot_initial_hour - start_hour) &gt;= (start_min - slot_initial_min),\n                F.col(ColNames.time_slot_initial_time),\n            )\n            .otherwise(F.date_add(F.col(ColNames.time_slot_initial_time), -1))\n            .cast(DateType()),\n        )\n        return df\n\n    # Rest of time intervals: night_time, evening_time, working_hours.\n    # Filter out time slots not contained in the time interval\n    if start &lt; end or end == dt.time(0, 0):\n        df = df.filter(\n            (F.lit(60) * (slot_initial_hour - start_hour) &gt;= (start_min - slot_initial_min))\n            &amp; (F.lit(60) * (slot_end_hour - end_hour) &lt;= (end_min - slot_end_min))\n        )\n    else:\n        df = df.filter(\n            (F.lit(60) * (slot_initial_hour - start_hour) &gt;= (start_min - slot_initial_min))\n            | (F.lit(60) * (slot_end_hour - end_hour) &lt;= (end_min - slot_end_min))\n        )\n\n    # consider self.day_start_hour = 4 for the following examples\n    if subdaily_period == \"night_time\":\n        if start.hour &gt;= self.day_start_hour and (end &gt; start or end == dt.time(0, 0)):\n            # night-interval stays in the day, example [20:15 to 23:30] or [19:45 to 00:00]\n            df = df.withColumn(ColNames.date, F.col(ColNames.time_slot_initial_time).cast(DateType()))\n        elif start.hour &gt;= self.day_start_hour and end &lt; start and end != dt.time(0, 0):\n            # night-interval starts in the day and crosses midnight, example [20:15 to 03:30] or [19:00 to 18:45]\n            df = df.withColumn(\n                ColNames.date,\n                F.when(\n                    F.lit(60) * (slot_initial_hour - start_hour) &gt;= (start_min - slot_initial_min),\n                    F.col(ColNames.time_slot_initial_time),\n                )\n                .otherwise(F.date_add(F.col(ColNames.time_slot_initial_time), -1))\n                .cast(DateType()),\n            )\n        elif start.hour &lt; self.day_start_hour and (end &gt; start or end == dt.time(0, 0)):\n            # night-interval starts in the next day and does not cross midnight, example [3:15 to 6:00] or [2:30 to 10:45]\n            df = df.withColumn(\n                ColNames.date, F.date_add(F.col(ColNames.time_slot_initial_time), -1).cast(DateType())\n            )\n        return df\n\n    # if subdaily_period in (\"working_hours\", \"evening_time\"):\n    if start &gt;= end and end != dt.time(0, 0):\n        self.logger.log(\n            msg=f\"'inverted' start ({start}) and end ({end}) times found for subdaily period {subdaily_period}\"\n            \" -- the whole period will belong to the day of start of the interval\",\n            level=logging.INFO,\n        )\n    df = df.withColumn(\n        ColNames.date,\n        F.when(\n            F.lit(60) * (slot_initial_hour - start_hour) &gt;= (start_min - slot_initial_min),\n            F.col(ColNames.time_slot_initial_time),\n        )\n        .otherwise(F.date_add(F.col(ColNames.time_slot_initial_time), -1))\n        .cast(DateType()),\n    )\n    return df\n</code></pre>"},{"location":"reference/components/execution/midterm_permanence_score/midterm_permanence_score/#components.execution.midterm_permanence_score.midterm_permanence_score.frequency_and_regularity","title":"<code>frequency_and_regularity(arr, month_start, extended_start, month_end, extended_end)</code>","text":"<p>PySpark UDF that calculates the mid-term frequency and regularity metrics of a device and grid tile/unknown location</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <p>ArrayType() PySpark SQL Type (seen as a list?) containing the dates in which the device has been observed in a particular grid tile or in the \"unknown\" location. All dates belong to the mid-term's month of analysis, with the possible exception of a date in the look-back dates and a date in the look-forward dates used for the regularity indices' calculation.</p> required <code>month_start</code> <code>date</code> <p>First date of the month of the current mid-term period</p> required <code>extended_start</code> <code>date</code> <p>First date of the extended dates in the look-back dates</p> required <code>month_end</code> <code>date</code> <p>Last date of the month of the current mid-term period</p> required <code>extended_end</code> <code>date</code> <p>Last date of the extended dates in the look-forward dates</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Whenever a record reaches this function with an empty list of dates, which is unexpected behaviour</p> <p>Returns:</p> Name Type Description <code>list</code> <p>returns a list with the frequency, regularity mean and regularity sample standard deviation</p> Source code in <code>multimno/components/execution/midterm_permanence_score/midterm_permanence_score.py</code> <pre><code>@F.udf(returnType=ArrayType(FloatType()))\ndef frequency_and_regularity(\n    arr, month_start: dt.date, extended_start: dt.date, month_end: dt.date, extended_end: dt.date\n):\n    \"\"\"PySpark UDF that calculates the mid-term frequency and regularity metrics of a device and grid tile/unknown\n    location\n\n    Args:\n        arr: ArrayType() PySpark SQL Type (seen as a list?) containing the dates in which the device has been observed\n            in a particular grid tile or in the \"unknown\" location. All dates belong to the mid-term's month of\n            analysis, with the possible exception of a date in the look-back dates and a date in the look-forward\n            dates used for the regularity indices' calculation.\n        month_start (dt.date): First date of the month of the current mid-term period\n        extended_start (dt.date): First date of the extended dates in the look-back dates\n        month_end (dt.date): Last date of the month of the current mid-term period\n        extended_end (dt.date): Last date of the extended dates in the look-forward dates\n\n    Raises:\n        ValueError: Whenever a record reaches this function with an empty list of dates, which is unexpected behaviour\n\n    Returns:\n        list: returns a list with the frequency, regularity mean and regularity sample standard deviation\n    \"\"\"\n    if len(arr) == 0:\n        raise ValueError(\"Found empty array of dates during regularity mean calculation. which should not be possible\")\n\n    earliest_date = arr[0]\n    latest_date = arr[-1]\n\n    array_length = len(arr)\n\n    # this variable represents the number of dates in the current month that have been observed. since some dates in\n    # `arr` might belong to the look-back or look-forward period, this value might be adjusted later on\n    frequency = array_length\n\n    # if there is only one date in the array (so earliest_date equals latest_date)\n    if array_length == 1:\n        # if the single date does not belong to the study month, there is no stay in this grid tile and month\n        if earliest_date &lt; month_start or earliest_date &gt; month_end:\n            return None\n\n        return (\n            1.0,\n            (extended_end - extended_start).days / (array_length + 1),\n            (extended_end - extended_start).days / (2**0.5),\n        )\n    # if there are only two dates\n    elif array_length == 2:\n        # If the dates belong to the reg extended periods, there is no stay in this grid tile and month\n        if earliest_date &lt; month_start or earliest_date &gt; month_end:\n            return None\n\n    diffs = [(arr[i + 1] - arr[i]).days for i in range(len(arr) - 1)]\n    # If there is no date in the before (after) reg period, we consider the start (end) date as the extended_start\n    # (extended_end) date.\n    if earliest_date &gt;= month_start:\n        diffs.append((earliest_date - extended_start).days)\n        earliest_date = extended_start\n        array_length += 1\n    else:\n        frequency -= 1\n    if latest_date &lt;= month_end:\n        diffs.append((extended_end - latest_date).days)\n        latest_date = extended_end\n        array_length += 1\n    else:\n        frequency -= 1\n\n    mean = (latest_date - earliest_date).days / (array_length - 1)\n    std = (sum((dd - mean) ** 2 for dd in diffs) / (array_length - 2)) ** 0.5\n    return [float(frequency), mean, std]\n</code></pre>"},{"location":"reference/components/execution/network_cleaning/","title":"network_cleaning","text":""},{"location":"reference/components/execution/network_cleaning/network_cleaning/","title":"network_cleaning","text":"<p>Module that cleans raw MNO Network Topology data.</p>"},{"location":"reference/components/execution/network_cleaning/network_cleaning/#components.execution.network_cleaning.network_cleaning.NetworkCleaning","title":"<code>NetworkCleaning</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class that cleans MNO Network Topology Data (based on physical properties of the cell)</p> Source code in <code>multimno/components/execution/network_cleaning/network_cleaning.py</code> <pre><code>class NetworkCleaning(Component):\n    \"\"\"\n    Class that cleans MNO Network Topology Data (based on physical properties of the cell)\n    \"\"\"\n\n    COMPONENT_ID = \"NetworkCleaning\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n        self.latitude_max = self.config.getfloat(self.COMPONENT_ID, \"latitude_max\")\n        self.latitude_min = self.config.getfloat(self.COMPONENT_ID, \"latitude_min\")\n        self.longitude_max = self.config.getfloat(self.COMPONENT_ID, \"longitude_max\")\n        self.longitude_min = self.config.getfloat(self.COMPONENT_ID, \"longitude_min\")\n\n        # list of possible technologies\n        self.tech = self.config.get(self.COMPONENT_ID, \"technology_options\").strip().replace(\" \", \"\").split(\",\")\n\n        # list of possible cell types\n        self.cell_type_options = (\n            self.config.get(self.COMPONENT_ID, \"cell_type_options\").strip().replace(\" \", \"\").split(\",\")\n        )\n\n        self.data_period_start = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n        self.data_period_end = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n\n        # List of datetime.dates for which to perform cleaning of the raw network input data.\n        # Notice that date_period_end is included\n        self.data_period_dates = [\n            self.data_period_start + datetime.timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n        # Timestamp format that the function pyspark.sql.functions.to_timestamp expects.\n        # Format must follow guidelines in https://spark.apache.org/docs/3.4.2/sql-ref-datetime-pattern.html\n        self.valid_date_timestamp_format = self.config.get(self.COMPONENT_ID, \"valid_date_timestamp_format\")\n\n        self.frequent_error_criterion = self.config.get(self.COMPONENT_ID, \"frequent_error_criterion\")\n        if self.frequent_error_criterion not in (\"absolute\", \"percentage\"):\n            raise ValueError(\n                \"unexpected value in frequent_error_criterion: expected `absolute` or `percentage`, got\",\n                self.frequent_error_criterion,\n            )\n\n        if self.frequent_error_criterion == \"absolute\":\n            self.top_k_errors = self.config.getint(self.COMPONENT_ID, \"top_k_errors\")\n        else:  # percentage\n            self.top_k_errors = self.config.getfloat(self.COMPONENT_ID, \"top_k_errors\")\n\n        self.timestamp = datetime.datetime.now()\n        self.current_date: datetime.date = None\n        self.cells_df: DataFrame = None\n        self.accdf: DataFrame = None\n\n    def initalize_data_objects(self):\n        input_bronze_network_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"network_data_bronze\")\n        output_silver_network_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"network_data_silver\")\n        output_silver_network_syntactic_quality_metrics_by_column = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"network_syntactic_quality_metrics_by_column\"\n        )\n        output_silver_network_top_errors_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"network_top_frequent_errors\")\n        output_silver_network_row_error_metrics_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"network_row_error_metrics\"\n        )\n\n        bronze_network = BronzeNetworkDataObject(\n            self.spark, input_bronze_network_path, partition_columns=[ColNames.year, ColNames.month, ColNames.day]\n        )\n        silver_network = SilverNetworkDataObject(\n            self.spark, output_silver_network_path, partition_columns=[ColNames.year, ColNames.month, ColNames.day]\n        )\n\n        silver_network_quality_metrics_by_column = SilverNetworkDataQualityMetricsByColumn(\n            self.spark,\n            output_silver_network_syntactic_quality_metrics_by_column,\n            partition_columns=[ColNames.year, ColNames.month, ColNames.day],\n        )\n\n        silver_network_row_error_metrics = SilverNetworkRowErrorMetrics(\n            self.spark,\n            output_silver_network_row_error_metrics_path,\n            partition_columns=[ColNames.year, ColNames.month, ColNames.day],\n        )\n\n        silver_network_top_errors = SilverNetworkDataTopFrequentErrors(\n            self.spark,\n            output_silver_network_top_errors_path,\n            partition_columns=[ColNames.year, ColNames.month, ColNames.day],\n        )\n\n        self.input_data_objects = {bronze_network.ID: bronze_network}\n        self.output_data_objects = {\n            silver_network.ID: silver_network,\n            silver_network_quality_metrics_by_column.ID: silver_network_quality_metrics_by_column,\n            silver_network_top_errors.ID: silver_network_top_errors,\n            silver_network_row_error_metrics.ID: silver_network_row_error_metrics,\n        }\n\n    def transform(self):\n        # Raw/Bronze Network Topology DF\n        self.cells_df = self.input_data_objects[BronzeNetworkDataObject.ID].df\n\n        # Read only desired dates, specified via config\n        self.cells_df = self.cells_df.filter(\n            F.make_date(ColNames.year, ColNames.month, ColNames.day) == F.lit(self.current_date)\n        )\n\n        # List that will contain all the columns created to keep track of every kind of error\n        auxiliar_columns = []\n\n        # Columns for which we will check for null values\n        # Notice that currently, valid_date_end can have null values by definition as long as for the current date the tower is\n        # still operational. Thus, it is not taken into account for the deletion of rows/records\n        check_for_null_columns = [\n            ColNames.cell_id,\n            ColNames.valid_date_start,\n            ColNames.valid_date_end,  # should not be counted for discarding rows!!\n            ColNames.latitude,\n            ColNames.longitude,\n            ColNames.altitude,\n            ColNames.antenna_height,\n            ColNames.directionality,\n            ColNames.elevation_angle,\n            ColNames.horizontal_beam_width,\n            ColNames.vertical_beam_width,\n            ColNames.power,\n            ColNames.range,\n            ColNames.frequency,\n            ColNames.technology,\n            ColNames.cell_type,\n        ]\n\n        # Add auxiliar columns to track instances where a row has a null value\n        # Note that currently valid_date_end has a permited null value, as well as\n        # azimith_angle when directionality is 0\n        self.cells_df = self.cells_df.withColumns(\n            {f\"{col}_{NetworkErrorType.NULL_VALUE}\": F.col(col).isNull() for col in check_for_null_columns}\n        ).withColumn(\n            f\"{ColNames.azimuth_angle}_{NetworkErrorType.NULL_VALUE}\",\n            F.when(F.col(ColNames.directionality) == F.lit(1), F.col(ColNames.azimuth_angle).isNull()).otherwise(\n                F.lit(False)\n            ),\n        )\n\n        auxiliar_columns.extend([f\"{col}_{NetworkErrorType.NULL_VALUE}\" for col in check_for_null_columns])\n        auxiliar_columns.append(f\"{ColNames.azimuth_angle}_{NetworkErrorType.NULL_VALUE}\")\n\n        # Now, we try to parse the valid_date_start and valid_date_end columns, from a string to a timestamp\n        self.cells_df = (\n            self.cells_df.withColumn(\n                f\"{ColNames.valid_date_start}_parsed\",\n                F.to_timestamp(F.col(ColNames.valid_date_start), self.valid_date_timestamp_format),\n            )\n            .withColumn(\n                f\"{ColNames.valid_date_end}_parsed\",\n                F.to_timestamp(F.col(ColNames.valid_date_end), self.valid_date_timestamp_format),\n            )\n            # Check when parsing failed, excluding the cases where the field was null to begi with\n            .withColumn(\n                f\"{ColNames.valid_date_start}_{NetworkErrorType.CANNOT_PARSE}\",\n                F.when(\n                    F.col(ColNames.valid_date_start).isNotNull()\n                    &amp; F.col(f\"{ColNames.valid_date_start}_parsed\").isNull(),\n                    F.lit(True),\n                ).otherwise(F.lit(False)),\n            )\n            .withColumn(\n                f\"{ColNames.valid_date_end}_{NetworkErrorType.CANNOT_PARSE}\",\n                F.when(\n                    F.col(ColNames.valid_date_end).isNotNull() &amp; F.col(f\"{ColNames.valid_date_end}_parsed\").isNull(),\n                    F.lit(True),\n                ).otherwise(F.lit(False)),\n            )\n        )\n\n        auxiliar_columns.extend(\n            [\n                f\"{ColNames.valid_date_start}_{NetworkErrorType.CANNOT_PARSE}\",\n                f\"{ColNames.valid_date_end}_{NetworkErrorType.CANNOT_PARSE}\",\n            ]\n        )\n\n        # Now, we check for incoherent dates (valid_date_end is earlier in time than valid_date_start)\n        self.cells_df = self.cells_df.withColumn(\n            f\"dates_{NetworkErrorType.OUT_OF_RANGE}\",\n            F.when(\n                F.col(f\"{ColNames.valid_date_start}_parsed\").isNotNull()\n                &amp; F.col(f\"{ColNames.valid_date_end}_parsed\").isNotNull(),\n                F.col(f\"{ColNames.valid_date_start}_parsed\") &gt; F.col(f\"{ColNames.valid_date_end}_parsed\"),\n            ).otherwise(F.lit(False)),\n        )\n\n        auxiliar_columns.append(f\"dates_{NetworkErrorType.OUT_OF_RANGE}\")\n\n        # Now we check for invalid values that are outside of the range defined for the data object.\n\n        # TODO: correct check for CGI in cell ids\n        self.cells_df = self.cells_df.withColumn(\n            f\"{ColNames.cell_id}_{NetworkErrorType.OUT_OF_RANGE}\",\n            (F.length(F.col(ColNames.cell_id)) != F.lit(14)) &amp; (F.length(F.col(ColNames.cell_id)) != F.lit(15)),\n        )\n\n        # TODO: cover case where bounding box crosses the -180/180 longitude\n        self.cells_df = (\n            self.cells_df.withColumn(\n                f\"{ColNames.latitude}_{NetworkErrorType.OUT_OF_RANGE}\",\n                (F.col(ColNames.latitude) &gt; F.lit(self.latitude_max))\n                | (F.lit(ColNames.latitude) &lt; F.lit(self.latitude_min)),\n            )\n            .withColumn(\n                f\"{ColNames.longitude}_{NetworkErrorType.OUT_OF_RANGE}\",\n                (F.col(ColNames.longitude) &gt; F.lit(self.longitude_max))\n                | (F.lit(ColNames.longitude) &lt; F.lit(self.longitude_min)),\n            )\n            # altitude: must only be float, no checks\n            # antenna height: must be positive\n            .withColumn(\n                f\"{ColNames.antenna_height}_{NetworkErrorType.OUT_OF_RANGE}\",\n                F.col(ColNames.antenna_height) &lt;= F.lit(0),\n            )\n            # directionality: 0 or 1\n            .withColumn(\n                f\"{ColNames.directionality}_{NetworkErrorType.OUT_OF_RANGE}\",\n                (F.col(ColNames.directionality) != F.lit(0)) &amp; (F.col(ColNames.directionality) != F.lit(1)),\n            )\n            .withColumn(\n                f\"{ColNames.azimuth_angle}_{NetworkErrorType.OUT_OF_RANGE}\",\n                F.when(\n                    (F.col(ColNames.directionality) == F.lit(1)),  # &amp; F.col(ColNames.azimuth_angle).isNotNull(),\n                    (F.col(ColNames.azimuth_angle) &lt; F.lit(0)) | (F.col(ColNames.azimuth_angle) &gt; F.lit(360)),\n                ).otherwise(F.lit(False)),\n            )\n            .withColumn(\n                f\"{ColNames.elevation_angle}_{NetworkErrorType.OUT_OF_RANGE}\",\n                (F.col(ColNames.elevation_angle) &lt; F.lit(-90)) | (F.col(ColNames.elevation_angle) &gt; F.lit(90)),\n            )\n            .withColumn(\n                f\"{ColNames.horizontal_beam_width}_{NetworkErrorType.OUT_OF_RANGE}\",\n                (F.col(ColNames.horizontal_beam_width) &lt; F.lit(0))\n                | (F.col(ColNames.horizontal_beam_width) &gt; F.lit(360)),\n            )\n            .withColumn(\n                f\"{ColNames.vertical_beam_width}_{NetworkErrorType.OUT_OF_RANGE}\",\n                (F.col(ColNames.vertical_beam_width) &lt; F.lit(0)) | (F.col(ColNames.vertical_beam_width) &gt; F.lit(360)),\n            )\n            .withColumn(\n                f\"{ColNames.power}_{NetworkErrorType.OUT_OF_RANGE}\",\n                F.col(ColNames.power) &lt; F.lit(0),\n            )\n            .withColumn(\n                f\"{ColNames.range}_{NetworkErrorType.OUT_OF_RANGE}\",\n                F.col(ColNames.range) &lt; F.lit(0),\n            )\n            .withColumn(\n                f\"{ColNames.frequency}_{NetworkErrorType.OUT_OF_RANGE}\",\n                F.col(ColNames.frequency) &lt; F.lit(0),\n            )\n            .withColumn(\n                f\"{ColNames.technology}_{NetworkErrorType.OUT_OF_RANGE}\",\n                ~F.col(ColNames.technology).isin(self.tech),\n            )\n            .withColumn(\n                f\"{ColNames.cell_type}_{NetworkErrorType.OUT_OF_RANGE}\",\n                ~F.col(ColNames.cell_type).isin(self.cell_type_options),\n            )\n        )\n\n        # Null values will appear for the above checks when the raw data was null. Thus, for these columns\n        # we change null for False by using the .fillna() method\n        self.cells_df = self.cells_df.fillna(\n            False,\n            [\n                f\"{ColNames.cell_id}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.latitude}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.longitude}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.antenna_height}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.directionality}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.azimuth_angle}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.elevation_angle}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.horizontal_beam_width}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.vertical_beam_width}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.power}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.range}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.frequency}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.technology}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.cell_type}_{NetworkErrorType.OUT_OF_RANGE}\",\n            ],\n        )\n\n        auxiliar_columns.extend(\n            [\n                f\"{ColNames.cell_id}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.latitude}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.longitude}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.antenna_height}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.directionality}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.azimuth_angle}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.elevation_angle}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.horizontal_beam_width}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.vertical_beam_width}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.power}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.range}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.frequency}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.technology}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.cell_type}_{NetworkErrorType.OUT_OF_RANGE}\",\n            ]\n        )\n\n        # Auxiliar dict, relating the DO columns with its auxiliar columns\n        column_groups = dict()\n        for col in self.output_data_objects[\"SilverNetworkDO\"].SCHEMA.names:\n            # for each auxiliar column\n            column_groups[col] = []\n            for cc in auxiliar_columns:\n                # if it is related to the DO's column:\n                if col == \"_\".join(cc.split(\"_\")[:-1]):\n                    # Ignore nulls for valid date end\n                    if cc == f\"{ColNames.valid_date_end}_{NetworkErrorType.NULL_VALUE}\":\n                        continue\n                    column_groups[col].append(F.col(cc))\n\n        # For each column, create an abstract conditional whenever a value of that column has ANY type of error.\n        # Example: latitude can have two types of error: a) being null, or b) being out of range.\n        # The coniditonal for this column is then&gt; (isNull(latitude) OR isOutOfRange(latitude))\n        column_conditions = {\n            col: reduce(lambda a, b: a | b, column_groups[col]) for col in column_groups if len(column_groups[col]) &gt; 0\n        }\n\n        # Negate the conditionals above to get those records without ANY type of error\n        field_without_errors = {col: ~column_conditions[col] for col in column_conditions}\n\n        auxiliar_columns.extend([f\"{col}_{NetworkErrorType.NO_ERROR}\" for col in field_without_errors])\n\n        # Abstract conditional ,indicating those records that do not have any type of error in any\n        # mandatory column, i.e. all accepted values.\n        mandatory_columns = [\n            ColNames.cell_id,\n            ColNames.latitude,\n            ColNames.longitude,\n            ColNames.directionality,\n            ColNames.azimuth_angle,\n        ]\n\n        # Rows to be preserved are those without errors in their mandatory fields\n        preserve_row = reduce(lambda a, b: a &amp; b, [field_without_errors[col] for col in mandatory_columns])\n\n        # Rows to be deleted\n\n        # Rows with any type of error\n        any_error_row = reduce(lambda a, b: a | b, column_conditions.values())\n\n        self.cells_df = self.cells_df.withColumns(\n            {f\"{col}_{NetworkErrorType.NO_ERROR}\": field_without_errors[col] for col in field_without_errors}\n        )\n\n        self.cells_df = self.cells_df.withColumn(\"to_preserve\", preserve_row)\n\n        self.cells_df.cache()\n\n        rows_to_be_deleted = (\n            self.cells_df.select((~preserve_row).cast(ByteType()).alias(\"to_be_deleted\")).withColumn(\n                \"to_be_deleted\", F.sum(\"to_be_deleted\")\n            )\n        ).collect()[0][\"to_be_deleted\"]\n\n        rows_with_any_error = (\n            self.cells_df.select((any_error_row).cast(ByteType()).alias(\"row_with_some_error\")).withColumn(\n                \"row_with_some_error\", F.sum(\"row_with_some_error\")\n            )\n        ).collect()[0][\"row_with_some_error\"]\n\n        row_error_metrics = []\n        row_error_metrics.append(\n            Row(\n                **{\n                    ColNames.result_timestamp: self.timestamp,\n                    ColNames.variable: \"rows_deleted\",\n                    ColNames.value: rows_to_be_deleted,\n                    ColNames.year: self.current_date.year,\n                    ColNames.month: self.current_date.month,\n                    ColNames.day: self.current_date.day,\n                }\n            )\n        )\n\n        row_error_metrics.append(\n            Row(\n                **{\n                    ColNames.result_timestamp: self.timestamp,\n                    ColNames.variable: \"rows_with_some_error\",\n                    ColNames.value: rows_with_any_error,\n                    ColNames.year: self.current_date.year,\n                    ColNames.month: self.current_date.month,\n                    ColNames.day: self.current_date.day,\n                }\n            )\n        )\n\n        row_error_df = self.spark.createDataFrame(row_error_metrics, schema=SilverNetworkRowErrorMetrics.SCHEMA)\n\n        self.output_data_objects[SilverNetworkRowErrorMetrics.ID].df = row_error_df\n\n        # Collect the number of True values in each auxiliar column, that counts the number of each error type, or\n        # any error, in each of the columns of the data object.\n        # TODO: possible improvement if pyspark.sql.GroupedData.pivot can be used instead.\n        metrics = (\n            self.cells_df.withColumns({col: F.col(col).cast(IntegerType()) for col in auxiliar_columns})\n            .groupBy([F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)])\n            .agg({col: \"sum\" for col in auxiliar_columns})\n            .withColumnsRenamed({f\"sum({col})\": col for col in auxiliar_columns})\n            .collect()\n        )\n\n        # Extract the collected values and reformat them into the shape of the metrics DO dataframe.\n        metrics_long_format = []\n\n        for row in metrics:\n            row = row.asDict()\n            year = row[ColNames.year]\n            month = row[ColNames.month]\n            day = row[ColNames.day]\n            date = datetime.date(year=year, month=month, day=day)\n            row_dict = dict()\n\n            for col in row:\n                if col in [ColNames.year, ColNames.month, ColNames.day]:\n                    continue\n\n                row_dict = {\n                    ColNames.field_name: \"_\".join(col.split(\"_\")[:-1]),\n                    ColNames.type_code: int(col.split(\"_\")[-1]),\n                    ColNames.value: row[col],\n                    ColNames.date: date,\n                    ColNames.year: year,\n                    ColNames.month: month,\n                    ColNames.day: day,\n                }\n\n                metrics_long_format.append(Row(**row_dict))\n\n        # Initial records (before cleaning)\n        initial_records = (\n            self.cells_df.groupBy([F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)]).count().collect()\n        )\n\n        for row in initial_records:\n            row = row.asDict()\n            year = row[ColNames.year]\n            month = row[ColNames.month]\n            day = row[ColNames.day]\n            date = datetime.date(year=year, month=month, day=day)\n\n            row_dict = {\n                ColNames.field_name: None,\n                ColNames.type_code: NetworkErrorType.INITIAL_ROWS,\n                ColNames.value: row[\"count\"],\n                ColNames.date: date,\n                ColNames.year: year,\n                ColNames.month: month,\n                ColNames.day: day,\n            }\n\n            metrics_long_format.append(Row(**row_dict))\n\n        # Final records (after cleaning)\n        final_records = (\n            self.cells_df.withColumn(\"to_preserve\", F.col(\"to_preserve\").cast(IntegerType()))\n            .groupBy([F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)])\n            .sum(\"to_preserve\")\n            .withColumnRenamed(\"sum(to_preserve)\", \"count\")\n            .collect()\n        )\n        for row in final_records:\n            row = row.asDict()\n            year = row[ColNames.year]\n            month = row[ColNames.month]\n            day = row[ColNames.day]\n            date = datetime.date(year=year, month=month, day=day)\n\n            row_dict = {\n                ColNames.field_name: None,\n                ColNames.type_code: NetworkErrorType.FINAL_ROWS,\n                ColNames.value: row[\"count\"],\n                ColNames.date: date,\n                ColNames.year: year,\n                ColNames.month: month,\n                ColNames.day: day,\n            }\n\n            metrics_long_format.append(Row(**row_dict))\n\n        # Final result\n        metrics_df = self.spark.createDataFrame(\n            metrics_long_format,\n            schema=StructType(\n                [\n                    StructField(ColNames.field_name, StringType(), nullable=True),\n                    StructField(ColNames.type_code, IntegerType(), nullable=False),\n                    StructField(ColNames.value, IntegerType(), nullable=False),\n                    StructField(ColNames.date, DateType(), nullable=False),\n                    StructField(ColNames.year, ShortType(), nullable=False),\n                    StructField(ColNames.month, ByteType(), nullable=False),\n                    StructField(ColNames.day, ByteType(), nullable=False),\n                ]\n            ),\n        )\n        metrics_df = metrics_df.withColumn(ColNames.result_timestamp, F.lit(self.timestamp).cast(TimestampType()))\n\n        self.output_data_objects[SilverNetworkDataQualityMetricsByColumn.ID].df = metrics_df\n\n        silver_cells_df = self.cells_df.filter(F.col(\"to_preserve\"))\n        # Prepare the clean, silver network data by imputing null values in invalid optional fields\n        for auxcol in auxiliar_columns:\n            # do not impute fields without error or that are already null\n            if int(auxcol.split(\"_\")[-1]) not in (NetworkErrorType.NO_ERROR, NetworkErrorType.NULL_VALUE):\n                split_col = auxcol.split(\"_\")\n                variable = \"_\".join(split_col[:-1])\n\n                if variable == \"dates\":\n                    silver_cells_df = silver_cells_df.withColumn(\n                        ColNames.valid_date_start,\n                        F.when(F.col(auxcol), F.lit(None)).otherwise(F.col(ColNames.valid_date_start)),\n                    ).withColumn(\n                        ColNames.valid_date_end,\n                        F.when(F.col(auxcol), F.lit(None)).otherwise(F.col(ColNames.valid_date_end)),\n                    )\n                    continue\n\n                silver_cells_df = silver_cells_df.withColumn(\n                    variable, F.when(F.col(auxcol), F.lit(None)).otherwise(F.col(variable))\n                )\n\n        silver_cells_df = (\n            silver_cells_df.withColumn(ColNames.valid_date_start, F.col(f\"{ColNames.valid_date_start}_parsed\"))\n            .withColumn(ColNames.valid_date_end, F.col(f\"{ColNames.valid_date_end}_parsed\"))\n            .select(SilverNetworkDataObject.SCHEMA.names)\n        )\n\n        self.output_data_objects[SilverNetworkDataObject.ID].df = silver_cells_df\n\n        # Top Frequent Error Metrics\n        error_counts_df = []\n        for field_name, cols in column_groups.items():\n            if len(cols) == 0:\n                continue\n\n            # Get name of the column and its error code\n            col_name = cols[0]._jc.toString()\n            type_code_column = F.when(F.col(col_name), F.lit(int(col_name.split(\"_\")[-1])))\n\n            # if len(cols) == 1, the loop is not entered\n            for i in range(1, len(cols)):\n                col_name = cols[i]._jc.toString()\n                type_code_column = type_code_column.when(F.col(col_name), F.lit(int(col_name.split(\"_\")[-1])))\n            type_code_column = type_code_column.otherwise(None)\n\n            error_counts_df.append(\n                self.cells_df.filter(~field_without_errors[field_name])  # rows with errors in this field\n                .select(field_name, *cols)  # select field and its aux columns\n                .withColumn(ColNames.type_code, type_code_column)  # new column with error code\n                .groupBy(field_name, ColNames.type_code)\n                .count()  # count frequency of each particular error value\n                .withColumnsRenamed(\n                    {\n                        \"count\": ColNames.error_count,\n                        field_name: ColNames.error_value,\n                    }\n                )\n                .withColumn(ColNames.error_count, F.col(ColNames.error_count).cast(IntegerType()))\n                .withColumn(  # cast values as strings\n                    ColNames.error_value, F.col(ColNames.error_value).cast(StringType())\n                )\n                .withColumn(ColNames.field_name, F.lit(field_name))\n            )\n\n        # Join all error count dataframes\n        errors_df = reduce(lambda x, y: DataFrame.union(x, y), error_counts_df)\n\n        errors_df.cache()\n\n        total_errors = errors_df.select(F.sum(ColNames.error_count).alias(ColNames.error_count)).collect()[0][\n            ColNames.error_count\n        ]\n\n        if total_errors is None:\n            self.output_data_objects[SilverNetworkDataTopFrequentErrors.ID].df = self.spark.createDataFrame(\n                [], schema=SilverNetworkDataTopFrequentErrors.SCHEMA\n            )\n            return\n\n        window = (\n            Window()\n            .orderBy(F.col(ColNames.error_count).desc())\n            .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n        )\n\n        self.accdf = errors_df.withColumn(\n            ColNames.accumulated_percentage,\n            (F.lit(100 / total_errors) * F.sum(F.col(ColNames.error_count)).over(window)).cast(FloatType()),\n        ).withColumn(\"id\", F.row_number().over(window))\n\n        if self.frequent_error_criterion == \"absolute\":\n            self.accdf = self.accdf.filter(F.col(\"id\") &lt;= F.lit(self.top_k_errors)).drop(\"id\")\n        else:  # percentage\n            self.accdf.cache()\n            prev_id = (\n                self.accdf.filter(F.col(ColNames.accumulated_percentage) &lt;= F.lit(self.top_k_errors)).select(\n                    F.max(\"id\")\n                )\n            ).collect()[0][\"max(id)\"]\n\n            if prev_id is None:\n                self.accdf = self.accdf.filter(F.lit(False)).drop(\"id\")\n            else:\n                prev_id = prev_id[0][\"max(id)\"]\n                self.accdf = self.accdf.filter(F.col(\"id\") &lt;= F.lit(prev_id + 1)).drop(\"id\")\n\n        self.accdf = (\n            self.accdf.withColumn(ColNames.result_timestamp, F.lit(self.timestamp).cast(TimestampType()))\n            .withColumn(ColNames.year, F.lit(self.current_date.year).cast(ShortType()))\n            .withColumn(ColNames.month, F.lit(self.current_date.month).cast(ByteType()))\n            .withColumn(ColNames.day, F.lit(self.current_date.day).cast(ByteType()))\n            .select(SilverNetworkDataTopFrequentErrors.SCHEMA.fieldNames())\n        )\n\n        self.output_data_objects[SilverNetworkDataTopFrequentErrors.ID].df = self.accdf\n\n    def execute(self):\n        self.read()\n        for date in self.data_period_dates:\n            self.logger.info(f\"Processing {date}...\")\n            self.current_date = date\n            self.accdf = None\n            self.transform()\n            self.write()\n            self.cells_df.unpersist()\n            if self.accdf is not None:\n                self.accdf.unpersist()\n            else:\n                self.logger.info(f\"No errors found for {date} -- no error frequency metrics generated\")\n            self.logger.info(f\"... {date} finished\")\n\n    def write(self):\n        \"\"\"\n        Method that performs the write operation of the output data objects.\n        \"\"\"\n        for data_object in self.output_data_objects.values():\n            self.logger.info(f\"Writing {data_object.ID}...\")\n            data_object.write()\n            self.logger.info(\"... finished\")\n\n        for data_object in self.output_data_objects.values():\n            data_object.df.unpersist()\n</code></pre>"},{"location":"reference/components/execution/network_cleaning/network_cleaning/#components.execution.network_cleaning.network_cleaning.NetworkCleaning.write","title":"<code>write()</code>","text":"<p>Method that performs the write operation of the output data objects.</p> Source code in <code>multimno/components/execution/network_cleaning/network_cleaning.py</code> <pre><code>def write(self):\n    \"\"\"\n    Method that performs the write operation of the output data objects.\n    \"\"\"\n    for data_object in self.output_data_objects.values():\n        self.logger.info(f\"Writing {data_object.ID}...\")\n        data_object.write()\n        self.logger.info(\"... finished\")\n\n    for data_object in self.output_data_objects.values():\n        data_object.df.unpersist()\n</code></pre>"},{"location":"reference/components/execution/present_population/","title":"present_population","text":""},{"location":"reference/components/execution/present_population/present_population_estimation/","title":"present_population_estimation","text":"<p>Module for estimating the present population of a geographical area at a given time.</p>"},{"location":"reference/components/execution/present_population/present_population_estimation/#components.execution.present_population.present_population_estimation.PresentPopulationEstimation","title":"<code>PresentPopulationEstimation</code>","text":"<p>               Bases: <code>Component</code></p> <p>This component calculates the estimated actual population (number of people spatially present) for a specified spatial area (country, municipality, grid).</p> <p>NOTE: In the current variant 1 of implementation, this module implements only the counting of one MNO's users instead of extrapolating to the entire population.</p> Source code in <code>multimno/components/execution/present_population/present_population_estimation.py</code> <pre><code>class PresentPopulationEstimation(Component):\n    \"\"\"This component calculates the estimated actual population (number of people spatially present)\n    for a specified spatial area (country, municipality, grid).\n\n    NOTE: In the current variant 1 of implementation, this module implements only the counting of one\n    MNO's users instead of extrapolating to the entire population.\n    \"\"\"\n\n    COMPONENT_ID = \"PresentPopulationEstimation\"\n    supported_aggregation_levels = [\"grid\", \"zone\"]\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        # Maximum allowed time difference for an event to be included in a time point.\n        self.tolerance_period_s = self.config.getint(self.COMPONENT_ID, \"tolerance_period_s\")\n\n        # Time boundaries for result calculation.\n        self.data_period_start = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d %H:%M:%S\"\n        )\n        self.data_period_end = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d  %H:%M:%S\"\n        )\n\n        # Total number of user id modulo partitions.\n        self.nr_of_user_id_partitions = self.config.getint(\n            self.COMPONENT_ID, \"nr_of_user_id_partitions\"\n        )  # TODO this should come from global config?\n        # Number of user id modulo partitions to process at a time.\n        self.nr_of_user_id_partitions_per_slice = self.config.getint(\n            self.COMPONENT_ID, \"nr_of_user_id_partitions_per_slice\"\n        )\n\n        # Time gap (time distance in seconds between time points).\n        self.time_point_gap_s = timedelta(seconds=self.config.getint(self.COMPONENT_ID, \"time_point_gap_s\"))\n\n        # Maximum number of iterations for the Bayesian process.\n        self.max_iterations = self.config.getint(self.COMPONENT_ID, \"max_iterations\")\n\n        # Minimum difference threshold between prior and posterior to continue iterating the Bayesian process.\n        # Compares sum of absolute differences of each row.\n        self.min_difference_threshold = self.config.getfloat(self.COMPONENT_ID, \"min_difference_threshold\")\n\n        # Set output aggregation level (grid-level or zone-level).\n        self.output_aggregation_level = self.config.get(self.COMPONENT_ID, \"output_aggregation_level\")\n        if self.output_aggregation_level not in self.supported_aggregation_levels:\n            raise ValueError(\n                f\"Invalid output_aggregation_level config value ({self.output_aggregation_level}). Expected one of : {self.supported_aggregation_levels}\"\n            )\n\n        # If using zone-level aggregation, require dataset id and zone hierarchical level.\n        if self.output_aggregation_level == \"zone\":\n            self.zoning_dataset_id = self.config.get(self.COMPONENT_ID, \"zoning_dataset_id\")\n            self.zoning_hierarchical_level = self.config.getint(self.COMPONENT_ID, \"zoning_hierarchical_level\")\n\n        self.time_point = None\n\n    def initalize_data_objects(self):\n        input_silver_event_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"event_data_silver_flagged\")\n        input_silver_cell_connection_prob_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"cell_connection_probabilities_data_silver\"\n        )\n        input_silver_grid_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"grid_data_silver\")\n        input_silver_event = SilverEventFlaggedDataObject(self.spark, input_silver_event_path)\n        input_silver_cell_connection_prob = SilverCellConnectionProbabilitiesDataObject(\n            self.spark, input_silver_cell_connection_prob_path\n        )\n        input_silver_grid = SilverGridDataObject(self.spark, input_silver_grid_path)\n        self.input_data_objects = {\n            SilverEventFlaggedDataObject.ID: input_silver_event,\n            SilverCellConnectionProbabilitiesDataObject.ID: input_silver_cell_connection_prob,\n            SilverGridDataObject.ID: input_silver_grid,\n        }\n        # If aggregating by zone, additionally require zone to grid mapping input data.\n        output_aggregation_level = self.config.get(self.COMPONENT_ID, \"output_aggregation_level\")\n        if output_aggregation_level not in self.supported_aggregation_levels:\n            raise ValueError(\n                f\"Invalid output_aggregation_level config value ({self.output_aggregation_level}). Expected one of : {self.supported_aggregation_levels}\"\n            )\n        if output_aggregation_level == \"zone\":\n            input_silver_zone_to_grid_map_path = self.config.get(\n                CONFIG_SILVER_PATHS_KEY, \"geozones_grid_map_data_silver\"\n            )\n            input_zone_to_grid_map = SilverGeozonesGridMapDataObject(self.spark, input_silver_zone_to_grid_map_path)\n            self.input_data_objects[SilverGeozonesGridMapDataObject.ID] = input_zone_to_grid_map\n\n        # Output\n        # Output data object depends on whether results are aggregated per grid or per zone.\n\n        if output_aggregation_level == \"grid\":\n            self.silver_present_population_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"present_population_silver\")\n            output_present_population = SilverPresentPopulationDataObject(\n                self.spark,\n                self.silver_present_population_path,\n                partition_columns=[ColNames.year, ColNames.month, ColNames.day, ColNames.timestamp],\n            )\n            self.output_data_objects = {SilverPresentPopulationDataObject.ID: output_present_population}\n        elif output_aggregation_level == \"zone\":\n            self.silver_present_population_path = self.config.get(\n                CONFIG_SILVER_PATHS_KEY, \"present_population_zone_silver\"\n            )\n            output_present_population = SilverPresentPopulationZoneDataObject(\n                self.spark,\n                self.silver_present_population_path,\n                partition_columns=[ColNames.year, ColNames.month, ColNames.day, ColNames.timestamp],\n            )\n            self.output_data_objects = {SilverPresentPopulationZoneDataObject.ID: output_present_population}\n\n    def execute(self):\n        self.logger.info(\"STARTING: Present Population Estimation\")\n\n        self.read()\n\n        # Generate desired time points.\n        time_points = generate_time_points(self.data_period_start, self.data_period_end, self.time_point_gap_s)\n\n        # Processing logic: handle time points independently one at a time. Write results after each time point.\n        for time_point in time_points:\n            self.logger.info(f\"Present Population: Starting time point {time_point}\")\n            self.time_point = time_point\n            self.transform()\n            self.write()\n            self.spark.catalog.clearCache()\n            self.logger.info(f\"Present Population: Finished time point {time_point}\")\n        # TODO: optimizing when to write.\n        # As time points are independent, it seems reasonable to write each out separately.\n        # Currently we have hard-coded write mode \"overwrite\", which does not allow for this,\n        # so DO write and pre-write deletion needs handling first.\n        self.logger.info(\"FINISHED: Present Population Estimation\")\n\n    def transform(self):\n        time_point = self.time_point\n        # Filter event data to dates within allowed time bounds.\n        events_df = self.input_data_objects[SilverEventFlaggedDataObject.ID].df\n\n        # Apply date-level filtering to omit events from dates unrelated to this time point.\n        events_df = select_where_dates_include_time_point_window(time_point, self.tolerance_period_s, events_df)\n\n        # Number of devices connected to each cell, taking only their event closest to the time_point\n        count_per_cell_df = self.calculate_devices_per_cell(events_df, time_point)\n\n        cell_conn_prob_df = self.get_cell_connection_probabilities(time_point)\n\n        # calculate population estimates per grid tile.\n        population_per_grid_df = self.calculate_population_per_grid(count_per_cell_df, cell_conn_prob_df)\n\n        # If output aggregation level is grid, then the population estimation is finished.\n        # Prepare the results.\n        if self.output_aggregation_level == \"grid\":\n            population_per_grid_df = (\n                population_per_grid_df.withColumn(ColNames.timestamp, F.lit(time_point))\n                .withColumn(ColNames.year, F.lit(time_point.year))\n                .withColumn(ColNames.month, F.lit(time_point.month))\n                .withColumn(ColNames.day, F.lit(time_point.day))\n            )\n            # Set results data object\n            self.output_data_objects[SilverPresentPopulationDataObject.ID].df = population_per_grid_df\n        # If output aggregation level is zone, then additionally do grid to zone mapping.\n        # Then prepare the results.\n        elif self.output_aggregation_level == \"zone\":\n            zone_stats_df = self.calculate_population_per_zone(population_per_grid_df)\n            zone_stats_df = (\n                zone_stats_df.withColumn(ColNames.timestamp, F.lit(time_point))\n                .withColumn(ColNames.year, F.lit(time_point.year))\n                .withColumn(ColNames.month, F.lit(time_point.month))\n                .withColumn(ColNames.day, F.lit(time_point.day))\n            )\n            # Set results data object\n            self.output_data_objects[SilverPresentPopulationZoneDataObject.ID].df = zone_stats_df\n\n    def get_cell_connection_probabilities(self, time_point: datetime) -&gt; DataFrame:\n        \"\"\"\n        Filter the cell connection probabilities of the dates needed for the time_point provided.\n        Args:\n            time_point (datetime.datetime): timestamp of time point\n\n        Returns:\n            DataFrame: (grid_id, cell_id, cell_connection_probability) dataframe\n        \"\"\"\n        time_bound_lower = time_point - timedelta(seconds=self.tolerance_period_s)\n        time_bound_upper = time_point + timedelta(seconds=self.tolerance_period_s)\n\n        cell_conn_prob_df = (\n            self.input_data_objects[SilverCellConnectionProbabilitiesDataObject.ID]\n            .df.select(\n                ColNames.year,\n                ColNames.month,\n                ColNames.day,\n                ColNames.cell_id,\n                ColNames.grid_id,\n                ColNames.cell_connection_probability,\n            )\n            .filter(\n                F.make_date(ColNames.year, ColNames.month, ColNames.day).between(\n                    lowerBound=F.lit(time_bound_lower.date()), upperBound=F.lit(time_bound_upper.date())\n                )\n            )\n        )\n\n        return cell_conn_prob_df\n\n    def calculate_devices_per_cell(\n        self,\n        events_df: DataFrame,\n        time_point: datetime,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Calculates the number of unique users/devices per cell for one time point based on the events inside the\n        interval around the time_point. If a device has multiple events inside the interval, the one closest to the\n        time_point is selected. In case of a tie, the earliest event is chosen.\n\n        Args:\n            events_df (DataFrame): Event data. For each user, expected to contain all of that user's events that can be\n                included in this time point.\n            time_point (datetime): The timestamp for which the population counts are calculated for.\n\n        Returns:\n            DataFrame: Count of devices per cell\n        \"\"\"\n        # Filter to include only events within the time window of the time point.\n        time_bound_lower = time_point - timedelta(seconds=self.tolerance_period_s)\n        time_bound_upper = time_point + timedelta(seconds=self.tolerance_period_s)\n        events_df = events_df.where(\n            (time_bound_lower &lt;= F.col(ColNames.timestamp)) &amp; (F.col(ColNames.timestamp) &lt;= time_bound_upper)\n        )\n\n        # For each user, order events by time distance from time point. If tied, prefer earlier timestamp.\n        window = Window.partitionBy(ColNames.user_id).orderBy(\n            F.abs(F.col(ColNames.timestamp) - time_point).asc(),\n            F.col(ColNames.timestamp),\n        )\n\n        events_df = (\n            events_df.withColumn(\"rank\", F.row_number().over(window))\n            .filter(F.col(\"rank\") == F.lit(1))  # event closes to the time_point\n            .drop(\"rank\")\n        )\n\n        counts_df = events_df.groupBy(ColNames.year, ColNames.month, ColNames.day, ColNames.cell_id).agg(\n            F.count(ColNames.user_id).alias(ColNames.device_count)\n        )\n\n        return counts_df\n\n    def calculate_population_per_grid(self, devices_per_cell_df: DataFrame, cell_conn_prob_df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Calculates population estimates for each grid tile Using an iterative Bayesian process.\n\n        Args:\n            devices_per_cell_df (DataFrame): (cell_id, device_count) dataframe\n            cell_conn_prob_df (DataFrame): (grid_id, cell_id, cell_connection_probability) dataframe\n        Returns:\n            DataFrame: (grid_id, population) dataframe\n        \"\"\"\n        devices_per_cell_df.cache()\n\n        # First, calculate total number of devices and grid tiles to initialise the prior\n        total_devices = devices_per_cell_df.select(F.sum(ColNames.device_count).alias(\"total_devices\")).collect()[0][\n            \"total_devices\"\n        ]\n\n        if total_devices is None or total_devices == 0:\n            total_devices = 0\n            # TODO: do not enter iterations, as everything will be zero!\n\n        grid_df = self.input_data_objects[SilverGridDataObject.ID].df.select(ColNames.grid_id)\n\n        # TODO: total number of grid tiles, or total number of grid tiles covered by the cells our events refer to?\n        total_tiles = grid_df.count()\n\n        # Initial prior value of population per tile\n        initial_prior_value = float(total_devices / total_tiles)\n\n        # Create master dataframe\n        # Fields: year, month, day, cell_id, grid_id, cell_connection_probability, device_count(in the cell)\n        master_df = cell_conn_prob_df.join(\n            devices_per_cell_df, on=[ColNames.year, ColNames.month, ColNames.day, ColNames.cell_id]\n        ).withColumn(ColNames.population, F.lit(initial_prior_value))\n\n        niter = 0\n        diff = sys.float_info.max\n\n        normalisation_window = Window().partitionBy(ColNames.year, ColNames.month, ColNames.day, ColNames.cell_id)\n\n        master_df.cache()\n\n        # Declare variables here for clarity: they store the result of the previous and the current iterations\n        pop_df = None\n        new_pop_df = None\n        new_master_df = None\n\n        while niter &lt; self.max_iterations and diff &gt;= self.min_difference_threshold:\n            # If this is not the first iteration, we need to update the population value used as prior\n            if new_pop_df is not None:\n                pop_df = new_pop_df\n\n            if pop_df is not None:\n                new_master_df = master_df.drop(ColNames.population).join(pop_df, on=ColNames.grid_id)\n            else:\n                new_master_df = master_df\n\n            new_master_df = (\n                new_master_df.withColumn(  # numerator of Bayes' rule\n                    ColNames.population, F.col(ColNames.population) * F.col(ColNames.cell_connection_probability)\n                )\n                # nb of devices in cell i TIMES prob(tile j | cell i)\n                # i.e. Devices in cell i * proportion of devices of cell i that belong to tile j\n                .withColumn(\n                    ColNames.population,\n                    (\n                        F.col(ColNames.device_count)\n                        * F.col(ColNames.population)\n                        / F.sum(ColNames.population).over(normalisation_window)\n                    ),\n                )\n            )\n\n            # Compute new population estimation\n            new_pop_df = new_master_df.groupby(ColNames.grid_id).agg(\n                F.sum(ColNames.population).alias(ColNames.population)\n            )\n\n            new_pop_df.cache()\n\n            if pop_df is None:\n                diff_df = new_pop_df.select(\n                    F.sum(F.abs(F.col(ColNames.population) - F.lit(initial_prior_value))).alias(\"difference\")\n                )\n            else:\n                diff_df = (\n                    new_pop_df.withColumnRenamed(ColNames.population, \"new_population\")\n                    .join(pop_df, on=ColNames.grid_id)\n                    .select(F.sum(F.abs(F.col(ColNames.population) - F.col(\"new_population\"))).alias(\"difference\"))\n                )\n            diff = diff_df.collect()[0][\"difference\"]\n            if diff is None:\n                diff = 0\n\n            if pop_df is not None:\n                pop_df.unpersist()\n\n            niter += 1\n            self.logger.info(f\"Finished iteration {niter}, diff {diff} vs threshold {self.min_difference_threshold}\")\n\n        if diff &lt; self.min_difference_threshold:\n            self.logger.info(f\"Algorithm convergence for tolerance {self.min_difference_threshold}!\")\n        else:\n            self.logger.info(f\"Stopped iterations after reaching max iterations {self.max_iterations}\")\n        # At the end of the iteration, we have our population estimation over the grid tiles\n        return new_pop_df.withColumn(ColNames.population, F.col(ColNames.population).cast(FloatType()))\n\n    def calculate_population_per_zone(self, population_per_grid_df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Calculate present population per zone by aggregating together grid populations that belong to the same zone.\n        Aggregation level depends on configuration parameters.\n\n        Args:\n            population_per_grid_df (DataFrame): (grid_id, population)\n\n        Returns:\n            DataFrame: (zone_id, population)\n        \"\"\"\n        zone_to_grid_df = self.input_data_objects[SilverGeozonesGridMapDataObject.ID].df.where(\n            F.col(ColNames.dataset_id) == self.zoning_dataset_id\n        )\n        # Override zone_id with the desired hierarchical zone level.\n        zone_to_grid_df = zone_to_grid_df.withColumn(\n            ColNames.zone_id,\n            F.element_at(F.split(F.col(ColNames.hierarchical_id), pattern=\"\\\\|\"), self.zoning_hierarchical_level),\n        )\n        population_per_zone_df = (\n            population_per_grid_df.join(zone_to_grid_df, on=ColNames.grid_id)\n            .groupBy(ColNames.zone_id)\n            .agg(F.sum(ColNames.population).alias(ColNames.population))\n        )\n        # TODO Casting type to float. Should this happen earlier, or not at all (have result col type be Double)?\n        population_per_zone_df = population_per_zone_df.withColumn(\n            ColNames.population, F.col(ColNames.population).cast(\"float\")\n        )\n        return population_per_zone_df\n</code></pre>"},{"location":"reference/components/execution/present_population/present_population_estimation/#components.execution.present_population.present_population_estimation.PresentPopulationEstimation.calculate_devices_per_cell","title":"<code>calculate_devices_per_cell(events_df, time_point)</code>","text":"<p>Calculates the number of unique users/devices per cell for one time point based on the events inside the interval around the time_point. If a device has multiple events inside the interval, the one closest to the time_point is selected. In case of a tie, the earliest event is chosen.</p> <p>Parameters:</p> Name Type Description Default <code>events_df</code> <code>DataFrame</code> <p>Event data. For each user, expected to contain all of that user's events that can be included in this time point.</p> required <code>time_point</code> <code>datetime</code> <p>The timestamp for which the population counts are calculated for.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Count of devices per cell</p> Source code in <code>multimno/components/execution/present_population/present_population_estimation.py</code> <pre><code>def calculate_devices_per_cell(\n    self,\n    events_df: DataFrame,\n    time_point: datetime,\n) -&gt; DataFrame:\n    \"\"\"\n    Calculates the number of unique users/devices per cell for one time point based on the events inside the\n    interval around the time_point. If a device has multiple events inside the interval, the one closest to the\n    time_point is selected. In case of a tie, the earliest event is chosen.\n\n    Args:\n        events_df (DataFrame): Event data. For each user, expected to contain all of that user's events that can be\n            included in this time point.\n        time_point (datetime): The timestamp for which the population counts are calculated for.\n\n    Returns:\n        DataFrame: Count of devices per cell\n    \"\"\"\n    # Filter to include only events within the time window of the time point.\n    time_bound_lower = time_point - timedelta(seconds=self.tolerance_period_s)\n    time_bound_upper = time_point + timedelta(seconds=self.tolerance_period_s)\n    events_df = events_df.where(\n        (time_bound_lower &lt;= F.col(ColNames.timestamp)) &amp; (F.col(ColNames.timestamp) &lt;= time_bound_upper)\n    )\n\n    # For each user, order events by time distance from time point. If tied, prefer earlier timestamp.\n    window = Window.partitionBy(ColNames.user_id).orderBy(\n        F.abs(F.col(ColNames.timestamp) - time_point).asc(),\n        F.col(ColNames.timestamp),\n    )\n\n    events_df = (\n        events_df.withColumn(\"rank\", F.row_number().over(window))\n        .filter(F.col(\"rank\") == F.lit(1))  # event closes to the time_point\n        .drop(\"rank\")\n    )\n\n    counts_df = events_df.groupBy(ColNames.year, ColNames.month, ColNames.day, ColNames.cell_id).agg(\n        F.count(ColNames.user_id).alias(ColNames.device_count)\n    )\n\n    return counts_df\n</code></pre>"},{"location":"reference/components/execution/present_population/present_population_estimation/#components.execution.present_population.present_population_estimation.PresentPopulationEstimation.calculate_population_per_grid","title":"<code>calculate_population_per_grid(devices_per_cell_df, cell_conn_prob_df)</code>","text":"<p>Calculates population estimates for each grid tile Using an iterative Bayesian process.</p> <p>Parameters:</p> Name Type Description Default <code>devices_per_cell_df</code> <code>DataFrame</code> <p>(cell_id, device_count) dataframe</p> required <code>cell_conn_prob_df</code> <code>DataFrame</code> <p>(grid_id, cell_id, cell_connection_probability) dataframe</p> required <p>Returns:     DataFrame: (grid_id, population) dataframe</p> Source code in <code>multimno/components/execution/present_population/present_population_estimation.py</code> <pre><code>def calculate_population_per_grid(self, devices_per_cell_df: DataFrame, cell_conn_prob_df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Calculates population estimates for each grid tile Using an iterative Bayesian process.\n\n    Args:\n        devices_per_cell_df (DataFrame): (cell_id, device_count) dataframe\n        cell_conn_prob_df (DataFrame): (grid_id, cell_id, cell_connection_probability) dataframe\n    Returns:\n        DataFrame: (grid_id, population) dataframe\n    \"\"\"\n    devices_per_cell_df.cache()\n\n    # First, calculate total number of devices and grid tiles to initialise the prior\n    total_devices = devices_per_cell_df.select(F.sum(ColNames.device_count).alias(\"total_devices\")).collect()[0][\n        \"total_devices\"\n    ]\n\n    if total_devices is None or total_devices == 0:\n        total_devices = 0\n        # TODO: do not enter iterations, as everything will be zero!\n\n    grid_df = self.input_data_objects[SilverGridDataObject.ID].df.select(ColNames.grid_id)\n\n    # TODO: total number of grid tiles, or total number of grid tiles covered by the cells our events refer to?\n    total_tiles = grid_df.count()\n\n    # Initial prior value of population per tile\n    initial_prior_value = float(total_devices / total_tiles)\n\n    # Create master dataframe\n    # Fields: year, month, day, cell_id, grid_id, cell_connection_probability, device_count(in the cell)\n    master_df = cell_conn_prob_df.join(\n        devices_per_cell_df, on=[ColNames.year, ColNames.month, ColNames.day, ColNames.cell_id]\n    ).withColumn(ColNames.population, F.lit(initial_prior_value))\n\n    niter = 0\n    diff = sys.float_info.max\n\n    normalisation_window = Window().partitionBy(ColNames.year, ColNames.month, ColNames.day, ColNames.cell_id)\n\n    master_df.cache()\n\n    # Declare variables here for clarity: they store the result of the previous and the current iterations\n    pop_df = None\n    new_pop_df = None\n    new_master_df = None\n\n    while niter &lt; self.max_iterations and diff &gt;= self.min_difference_threshold:\n        # If this is not the first iteration, we need to update the population value used as prior\n        if new_pop_df is not None:\n            pop_df = new_pop_df\n\n        if pop_df is not None:\n            new_master_df = master_df.drop(ColNames.population).join(pop_df, on=ColNames.grid_id)\n        else:\n            new_master_df = master_df\n\n        new_master_df = (\n            new_master_df.withColumn(  # numerator of Bayes' rule\n                ColNames.population, F.col(ColNames.population) * F.col(ColNames.cell_connection_probability)\n            )\n            # nb of devices in cell i TIMES prob(tile j | cell i)\n            # i.e. Devices in cell i * proportion of devices of cell i that belong to tile j\n            .withColumn(\n                ColNames.population,\n                (\n                    F.col(ColNames.device_count)\n                    * F.col(ColNames.population)\n                    / F.sum(ColNames.population).over(normalisation_window)\n                ),\n            )\n        )\n\n        # Compute new population estimation\n        new_pop_df = new_master_df.groupby(ColNames.grid_id).agg(\n            F.sum(ColNames.population).alias(ColNames.population)\n        )\n\n        new_pop_df.cache()\n\n        if pop_df is None:\n            diff_df = new_pop_df.select(\n                F.sum(F.abs(F.col(ColNames.population) - F.lit(initial_prior_value))).alias(\"difference\")\n            )\n        else:\n            diff_df = (\n                new_pop_df.withColumnRenamed(ColNames.population, \"new_population\")\n                .join(pop_df, on=ColNames.grid_id)\n                .select(F.sum(F.abs(F.col(ColNames.population) - F.col(\"new_population\"))).alias(\"difference\"))\n            )\n        diff = diff_df.collect()[0][\"difference\"]\n        if diff is None:\n            diff = 0\n\n        if pop_df is not None:\n            pop_df.unpersist()\n\n        niter += 1\n        self.logger.info(f\"Finished iteration {niter}, diff {diff} vs threshold {self.min_difference_threshold}\")\n\n    if diff &lt; self.min_difference_threshold:\n        self.logger.info(f\"Algorithm convergence for tolerance {self.min_difference_threshold}!\")\n    else:\n        self.logger.info(f\"Stopped iterations after reaching max iterations {self.max_iterations}\")\n    # At the end of the iteration, we have our population estimation over the grid tiles\n    return new_pop_df.withColumn(ColNames.population, F.col(ColNames.population).cast(FloatType()))\n</code></pre>"},{"location":"reference/components/execution/present_population/present_population_estimation/#components.execution.present_population.present_population_estimation.PresentPopulationEstimation.calculate_population_per_zone","title":"<code>calculate_population_per_zone(population_per_grid_df)</code>","text":"<p>Calculate present population per zone by aggregating together grid populations that belong to the same zone. Aggregation level depends on configuration parameters.</p> <p>Parameters:</p> Name Type Description Default <code>population_per_grid_df</code> <code>DataFrame</code> <p>(grid_id, population)</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>(zone_id, population)</p> Source code in <code>multimno/components/execution/present_population/present_population_estimation.py</code> <pre><code>def calculate_population_per_zone(self, population_per_grid_df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Calculate present population per zone by aggregating together grid populations that belong to the same zone.\n    Aggregation level depends on configuration parameters.\n\n    Args:\n        population_per_grid_df (DataFrame): (grid_id, population)\n\n    Returns:\n        DataFrame: (zone_id, population)\n    \"\"\"\n    zone_to_grid_df = self.input_data_objects[SilverGeozonesGridMapDataObject.ID].df.where(\n        F.col(ColNames.dataset_id) == self.zoning_dataset_id\n    )\n    # Override zone_id with the desired hierarchical zone level.\n    zone_to_grid_df = zone_to_grid_df.withColumn(\n        ColNames.zone_id,\n        F.element_at(F.split(F.col(ColNames.hierarchical_id), pattern=\"\\\\|\"), self.zoning_hierarchical_level),\n    )\n    population_per_zone_df = (\n        population_per_grid_df.join(zone_to_grid_df, on=ColNames.grid_id)\n        .groupBy(ColNames.zone_id)\n        .agg(F.sum(ColNames.population).alias(ColNames.population))\n    )\n    # TODO Casting type to float. Should this happen earlier, or not at all (have result col type be Double)?\n    population_per_zone_df = population_per_zone_df.withColumn(\n        ColNames.population, F.col(ColNames.population).cast(\"float\")\n    )\n    return population_per_zone_df\n</code></pre>"},{"location":"reference/components/execution/present_population/present_population_estimation/#components.execution.present_population.present_population_estimation.PresentPopulationEstimation.get_cell_connection_probabilities","title":"<code>get_cell_connection_probabilities(time_point)</code>","text":"<p>Filter the cell connection probabilities of the dates needed for the time_point provided. Args:     time_point (datetime.datetime): timestamp of time point</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>(grid_id, cell_id, cell_connection_probability) dataframe</p> Source code in <code>multimno/components/execution/present_population/present_population_estimation.py</code> <pre><code>def get_cell_connection_probabilities(self, time_point: datetime) -&gt; DataFrame:\n    \"\"\"\n    Filter the cell connection probabilities of the dates needed for the time_point provided.\n    Args:\n        time_point (datetime.datetime): timestamp of time point\n\n    Returns:\n        DataFrame: (grid_id, cell_id, cell_connection_probability) dataframe\n    \"\"\"\n    time_bound_lower = time_point - timedelta(seconds=self.tolerance_period_s)\n    time_bound_upper = time_point + timedelta(seconds=self.tolerance_period_s)\n\n    cell_conn_prob_df = (\n        self.input_data_objects[SilverCellConnectionProbabilitiesDataObject.ID]\n        .df.select(\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            ColNames.cell_id,\n            ColNames.grid_id,\n            ColNames.cell_connection_probability,\n        )\n        .filter(\n            F.make_date(ColNames.year, ColNames.month, ColNames.day).between(\n                lowerBound=F.lit(time_bound_lower.date()), upperBound=F.lit(time_bound_upper.date())\n            )\n        )\n    )\n\n    return cell_conn_prob_df\n</code></pre>"},{"location":"reference/components/execution/present_population/present_population_estimation/#components.execution.present_population.present_population_estimation.generate_slice_bounds","title":"<code>generate_slice_bounds(nr_of_partitions, partitions_per_slice, starting_id=0)</code>","text":"<p>Generates list of (lower_bound, upper_bound) pairs to be used for selecting equal-sized slices of partitioned data. The last slice may be smaller than others if the number of partitions is not a multiple of slice size.</p> <p>Parameters:</p> Name Type Description Default <code>nr_of_partitions</code> <code>int</code> <p>total number of partitions</p> required <code>nr_of_partitions_per_slice</code> <code>int</code> <p>Desired number of partitions per slice.</p> required <code>starting_id</code> <code>int</code> <p>Number to start first slice from. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>List[tuple[int, int]]</code> <p>List[tuple[int,int]]: list of (lower_bound, upper_bound) pairs</p> Source code in <code>multimno/components/execution/present_population/present_population_estimation.py</code> <pre><code>def generate_slice_bounds(\n    nr_of_partitions: int, partitions_per_slice: int, starting_id: int = 0\n) -&gt; List[tuple[int, int]]:\n    \"\"\"\n    Generates list of (lower_bound, upper_bound) pairs to be used for selecting equal-sized slices of partitioned data.\n    The last slice may be smaller than others if the number of partitions is not a multiple of slice size.\n\n    Args:\n        nr_of_partitions (int): total number of partitions\n        nr_of_partitions_per_slice (int): Desired number of partitions per slice.\n        starting_id (int, optional): Number to start first slice from. Defaults to 0.\n\n    Returns:\n        List[tuple[int,int]]: list of (lower_bound, upper_bound) pairs\n    \"\"\"\n    # TODO this might be reusable across components.\n    lower_bound = starting_id\n    slices_bounds = []\n    while lower_bound &lt; nr_of_partitions:\n        if lower_bound + partitions_per_slice &gt; nr_of_partitions:\n            upper_bound = nr_of_partitions\n        else:\n            upper_bound = lower_bound + partitions_per_slice\n        slices_bounds.append((lower_bound, upper_bound))\n        lower_bound = upper_bound\n    return slices_bounds\n</code></pre>"},{"location":"reference/components/execution/present_population/present_population_estimation/#components.execution.present_population.present_population_estimation.generate_time_points","title":"<code>generate_time_points(period_start, period_end, time_point_gap_s)</code>","text":"<p>Generates time points within the specified period with the specified spacing.</p> <p>Parameters:</p> Name Type Description Default <code>period_start</code> <code>datetime</code> <p>Start timestamp of generation.</p> required <code>period_end</code> <code>datetime</code> <p>End timestamp of generation.</p> required <code>time_point_gap_s</code> <code>timedelta</code> <p>Time delta object defining the space between consectuive time points.</p> required <p>Returns:</p> Type Description <code>List[datetime]</code> <p>[datetime]: List of time point timestamps.</p> Source code in <code>multimno/components/execution/present_population/present_population_estimation.py</code> <pre><code>def generate_time_points(period_start: datetime, period_end: datetime, time_point_gap_s: timedelta) -&gt; List[datetime]:\n    \"\"\"\n    Generates time points within the specified period with the specified spacing.\n\n    Args:\n        period_start (datetime): Start timestamp of generation.\n        period_end (datetime): End timestamp of generation.\n        time_point_gap_s (timedelta): Time delta object defining the space between consectuive time points.\n\n    Returns:\n        [datetime]: List of time point timestamps.\n    \"\"\"\n    # TODO this might be reusable across components.\n    time_points = []\n    one_time_point = period_start\n    while one_time_point &lt;= period_end:\n        time_points.append(one_time_point)\n        one_time_point = one_time_point + time_point_gap_s\n    return time_points\n</code></pre>"},{"location":"reference/components/execution/present_population/present_population_estimation/#components.execution.present_population.present_population_estimation.select_where_dates_include_time_point_window","title":"<code>select_where_dates_include_time_point_window(time_point, tolerance_period_s, df)</code>","text":"<p>Applies filtering to the DataFrame to omit rows from dates which are outside time point boundaries. The purpose is to leverage our Parquet partitioning schema (partitioned by year, month, day) and use predicate pushdown to avoid reading event data from irrelevant days.</p> <p>Parameters:</p> Name Type Description Default <code>time_point</code> <code>datetime</code> <p>Fixed timestamp to calculate results for.</p> required <code>tolerance_period_s</code> <code>int</code> <p>Time window size. Time in seconds before and after the time point</p> required <code>df</code> <code>DataFrame</code> <p>DataFrame of event data storage partitioned by year, month, day.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>df including only data from dates which include some part of the time point's window.</p> Source code in <code>multimno/components/execution/present_population/present_population_estimation.py</code> <pre><code>def select_where_dates_include_time_point_window(\n    time_point: datetime, tolerance_period_s: int, df: DataFrame\n) -&gt; DataFrame:\n    \"\"\"\n    Applies filtering to the DataFrame to omit rows from dates which are outside time point boundaries.\n    The purpose is to leverage our Parquet partitioning schema (partitioned by year, month, day) and\n    use predicate pushdown to avoid reading event data from irrelevant days.\n\n    Args:\n        time_point (datetime): Fixed timestamp to calculate results for.\n        tolerance_period_s (int): Time window size. Time in seconds before and after the time point\n        within which the event data is included.\n        df (DataFrame): DataFrame of event data storage partitioned by year, month, day.\n\n    Returns:\n        DataFrame: df including only data from dates which include some part of the time point's window.\n    \"\"\"\n    time_bound_lower = time_point - timedelta(seconds=tolerance_period_s)\n    date_lower = time_bound_lower.date()\n    time_bound_upper = time_point + timedelta(seconds=tolerance_period_s)\n    date_upper = time_bound_upper.date()\n    return df.where(\n        (F.make_date(ColNames.year, ColNames.month, ColNames.day) &gt;= date_lower)\n        &amp; (F.make_date(ColNames.year, ColNames.month, ColNames.day) &lt;= date_upper)\n    )\n</code></pre>"},{"location":"reference/components/execution/signal_strength/","title":"signal_strength","text":""},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/","title":"signal_stength_modeling","text":"<p>Module that cleans RAW MNO Event data.</p>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling","title":"<code>SignalStrengthModeling</code>","text":"<p>               Bases: <code>Component</code></p> <p>This class is responsible for modeling the signal strength of a cellular network.</p> <p>It takes as input a configuration file and a set of data representing the network's cells and their properties. The class then calculates the signal strength at various points of a grid, taking into account factors such as the distance to the cell, the azimuth and elevation angles, and the directionality of the cell.</p> <p>The class provides methods for adjusting the signal strength based on the horizontal and vertical angles, imputing default cell properties.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>class SignalStrengthModeling(Component):\n    \"\"\"\n    This class is responsible for modeling the signal strength of a cellular network.\n\n    It takes as input a configuration file and a set of data representing the network's cells and their properties.\n    The class then calculates the signal strength at various points of a grid, taking into account factors such\n    as the distance to the cell, the azimuth and elevation angles, and the directionality of the cell.\n\n    The class provides methods for adjusting the signal strength based on the horizontal and vertical angles,\n    imputing default cell properties.\n    \"\"\"\n\n    COMPONENT_ID = \"SignalStrengthModeling\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.data_period_start = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n        self.data_period_end = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n        self.use_elevation = self.config.getboolean(SignalStrengthModeling.COMPONENT_ID, \"use_elevation\")\n        self.do_azimuth_angle_adjustments = self.config.getboolean(\n            SignalStrengthModeling.COMPONENT_ID, \"do_azimuth_angle_adjustments\"\n        )\n        self.do_elevation_angle_adjustments = self.config.getboolean(\n            SignalStrengthModeling.COMPONENT_ID, \"do_elevation_angle_adjustments\"\n        )\n        self.default_cell_properties = self.config.geteval(\n            SignalStrengthModeling.COMPONENT_ID, \"default_cell_physical_properties\"\n        )\n\n        self.data_period_dates = [\n            self.data_period_start + datetime.timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n    def initalize_data_objects(self):\n\n        # Input\n        self.input_data_objects = {}\n        self.cartesian_crs = self.config.get(SignalStrengthModeling.COMPONENT_ID, \"cartesian_crs\")\n\n        grid_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"grid_data_silver\")\n        if check_if_data_path_exists(self.spark, grid_path):\n            self.input_data_objects[SilverGridDataObject.ID] = SilverGridDataObject(\n                self.spark, grid_path, default_crs=self.cartesian_crs\n            )\n        else:\n            self.logger.warning(f\"Expected path {grid_path} to exist but it does not\")\n            raise ValueError(f\"Invalid path for {grid_path} {SignalStrengthModeling.COMPONENT_ID}\")\n\n        network_topology_cells_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"network_data_silver\")\n        if check_if_data_path_exists(self.spark, network_topology_cells_path):\n            self.input_data_objects[SilverNetworkDataObject.ID] = SilverNetworkDataObject(\n                self.spark, network_topology_cells_path\n            )\n        else:\n            self.logger.warning(f\"Expected path {network_topology_cells_path} to exist but it does not\")\n            raise ValueError(f\"Invalid path for {network_topology_cells_path} {SignalStrengthModeling.COMPONENT_ID}\")\n\n        # Output\n        self.output_data_objects = {}\n        self.silver_signal_strength_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"signal_strength_data_silver\")\n        self.output_data_objects[SilverSignalStrengthDataObject.ID] = SilverSignalStrengthDataObject(\n            self.spark,\n            self.silver_signal_strength_path,\n            partition_columns=[ColNames.year, ColNames.month, ColNames.day],\n        )\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n\n        grid_sdf = self.input_data_objects[SilverGridDataObject.ID].df\n        grid_sdf = self.add_z_to_point_geometry(grid_sdf, ColNames.geometry, self.use_elevation)\n\n        # TODO: We might need to iterate over dates and process the data for each date separately\n        # have to do tests with larger datasets.\n\n        # TODO: Potentially\n        current_cells_sdf = self.input_data_objects[SilverNetworkDataObject.ID].df.filter(\n            F.make_date(\"year\", \"month\", \"day\").isin(self.data_period_dates)\n        )\n\n        current_cells_sdf = self.impute_default_cell_properties(current_cells_sdf)\n\n        current_cells_sdf = self.watt_to_dbm(current_cells_sdf)\n\n        # TODO: Add Path Loss Exponent calculation based on grid landuse data\n\n        # Create geometries\n        current_cells_sdf = self.create_cell_point_geometry(current_cells_sdf, self.use_elevation)\n        current_cells_sdf = self.project_to_crs(current_cells_sdf, 4326, self.cartesian_crs)\n\n        # Spatial join\n        current_cell_grid_sdf = self.spatial_join_within_distance(current_cells_sdf, grid_sdf, ColNames.range)\n\n        # Calculate planar and 3D distances\n        current_cell_grid_sdf = self.calculate_cartesian_distances(current_cell_grid_sdf)\n\n        # Calculate initial signal strength without azimuth and tilt angles adjustments\n        current_cell_grid_sdf = self.calculate_distance_power_loss(current_cell_grid_sdf)\n\n        # next step is to calculate azimuth and tilt angles adjustments, if set in the configuration file\n        # otherwise all cells are assumed to be omnideirectional and there is no need for these adjustments\n\n        # first need to find all standard deviations in signal strength distributions for azimuth and elevation\n        # angles for each signal strength back loss. This is done only once for each unique combination of\n        # signal strength back loss\n        # and beam width in cells dataset\n\n        if self.do_azimuth_angle_adjustments:\n\n            # get standard deviation mapping table for azimuth beam width azimuth back loss pairs\n            sd_azimuth_mapping_sdf = self.get_angular_adjustments_sd_mapping(\n                current_cells_sdf,\n                ColNames.horizontal_beam_width,\n                ColNames.azimuth_signal_strength_back_loss,\n                \"azimuth\",\n            )\n\n            current_cell_grid_directional = current_cell_grid_sdf.where(F.col(ColNames.directionality) == 1)\n\n            current_cell_grid_directional = self.join_sd_mapping(\n                current_cell_grid_directional,\n                sd_azimuth_mapping_sdf,\n                ColNames.horizontal_beam_width,\n                ColNames.azimuth_signal_strength_back_loss,\n            )\n\n            current_cell_grid_directional = self.calculate_horizontal_angle_power_adjustment(\n                current_cell_grid_directional\n            )\n\n            current_cell_grid_sdf = current_cell_grid_directional.unionByName(\n                current_cell_grid_sdf.where(F.col(ColNames.directionality) == 0),\n                allowMissingColumns=True,\n            )\n\n        if self.do_elevation_angle_adjustments:\n            # get standard deviation mapping table for elevation beam width elevation back loss pairs\n            sd_elevation_mapping_sdf = self.get_angular_adjustments_sd_mapping(\n                current_cells_sdf,\n                ColNames.vertical_beam_width,\n                ColNames.elevation_signal_strength_back_loss,\n                \"elevation\",\n            )\n            current_cell_grid_directional = current_cell_grid_sdf.where(F.col(ColNames.directionality) == 1)\n\n            current_cell_grid_directional = self.join_sd_mapping(\n                current_cell_grid_directional,\n                sd_elevation_mapping_sdf,\n                ColNames.vertical_beam_width,\n                ColNames.elevation_signal_strength_back_loss,\n            )\n\n            current_cell_grid_directional = self.calculate_vertical_angle_power_adjustment(\n                current_cell_grid_directional\n            )\n\n            current_cell_grid_sdf = current_cell_grid_directional.unionByName(\n                current_cell_grid_sdf.where(F.col(ColNames.directionality) == 0),\n                allowMissingColumns=True,\n            )\n\n        # amend start and end valid dates\n        current_cell_grid_sdf = current_cell_grid_sdf.withColumn(\n            \"valid_date_start\", F.make_date(F.col(\"year\"), F.col(\"month\"), F.col(\"day\"))\n        )\n        current_cell_grid_sdf = current_cell_grid_sdf.withColumn(\n            \"valid_date_end\", F.date_add(F.col(\"valid_date_start\"), 1)\n        )\n\n        current_cell_grid_sdf = current_cell_grid_sdf.select(\n            SilverSignalStrengthDataObject.MANDATORY_COLUMNS + SilverSignalStrengthDataObject.OPTIONAL_COLUMNS\n        )\n        columns = {\n            field.name: F.col(field.name).cast(field.dataType) for field in SilverSignalStrengthDataObject.SCHEMA.fields\n        }\n        current_cell_grid_sdf = current_cell_grid_sdf.withColumns(columns)\n\n        self.output_data_objects[SilverSignalStrengthDataObject.ID].df = current_cell_grid_sdf\n\n    def impute_default_cell_properties(self, sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Imputes default cell properties for null values in the input DataFrame using\n        default properties for cell types from config.\n\n        Args:\n            sdf (DataFrame): Input DataFrame.\n\n        Returns:\n            DataFrame: DataFrame with imputed default cell properties.\n        \"\"\"\n        default_properties_df = self.create_default_properties_df()\n\n        # add default prefix to the columns of default_properties_df\n        default_properties_df = default_properties_df.select(\n            [F.col(col).alias(f\"default_{col}\") for col in default_properties_df.columns]\n        )\n\n        # assign default cell type to cell types not present in config\n        sdf = sdf.withColumn(\n            ColNames.cell_type,\n            F.when(\n                ~F.col(ColNames.cell_type).isin(list(self.default_cell_properties.keys())),\n                \"default\",\n            ).otherwise(F.col(ColNames.cell_type)),\n        )\n\n        # all cell types which are absent from the default_properties_df will be assigned default values\n        sdf = sdf.join(\n            default_properties_df,\n            sdf[ColNames.cell_type] == default_properties_df[f\"default_{ColNames.cell_type}\"],\n            how=\"inner\",\n        )\n        # if orignal column is null, assign the default value\n        for col in default_properties_df.columns:\n            col = col.replace(\"default_\", \"\")\n            if col not in sdf.columns:\n                sdf = sdf.withColumn(col, F.lit(None))\n            sdf = sdf.withColumn(col, F.coalesce(F.col(col), F.col(f\"default_{col}\")))\n\n        return sdf\n\n    def create_default_properties_df(self) -&gt; DataFrame:\n        \"\"\"\n        Creates a DataFrame with default cell properties from config dict.\n\n        Returns:\n            DataFrame: A DataFrame with default cell properties.\n        \"\"\"\n\n        rows = [Row(cell_type=k, **v) for k, v in self.default_cell_properties.items()]\n        return self.spark.createDataFrame(rows).withColumnRenamed(\"cell_type\", ColNames.cell_type)\n\n    # create geometry for cells. Set Z values if elevation is taken into account from z column, otherwise to 0\n    @staticmethod\n    def create_cell_point_geometry(sdf: DataFrame, use_elevation: bool) -&gt; DataFrame:\n        \"\"\"\n        Creates cell point geometry.\n        If elevation is taken into account, set Z values from z column, otherwise to 0.\n\n        Args:\n            sdf (DataFrame): Input DataFrame.\n            use_elevation (bool): Whether to use elevation.\n\n        Returns:\n            DataFrame: DataFrame with cell point geometry.\n        \"\"\"\n        if use_elevation:\n            sdf = sdf.withColumn(\n                ColNames.geometry,\n                STC.ST_MakePoint(\n                    F.col(ColNames.longitude),\n                    F.col(ColNames.latitude),\n                    F.col(ColNames.altitude) + F.col(ColNames.antenna_height),\n                ),\n            )\n        else:\n            sdf = sdf.withColumn(\n                ColNames.geometry,\n                STC.ST_MakePoint(\n                    F.col(ColNames.longitude),\n                    F.col(ColNames.latitude),\n                    F.col(ColNames.antenna_height),\n                ),\n            )\n        # assign crs\n        sdf = sdf.withColumn(ColNames.geometry, STF.ST_SetSRID(F.col(ColNames.geometry), F.lit(4326)))\n\n        return sdf\n\n    # add z value to the grid geometry if elevation is taken into account from z column in the grid otherwise set to 0\n    @staticmethod\n    def add_z_to_point_geometry(sdf: DataFrame, geometry_col: str, use_elevation: bool) -&gt; DataFrame:\n        \"\"\"\n        Adds z value to the point geometry (grid centroids).\n        If elevation is taken into account, set Z values from z column, otherwise to 0.\n\n        Args:\n            sdf (DataFrame): Input DataFrame.\n            use_elevation (bool): Whether to use elevation.\n\n        Returns:\n            DataFrame: DataFrame with z value added to point geometry.\n        \"\"\"\n        if use_elevation:\n            sdf = sdf.withColumn(\n                geometry_col,\n                STC.ST_MakePoint(\n                    STF.ST_X(F.col(geometry_col)),\n                    STF.ST_Y(F.col(geometry_col)),\n                    F.col(ColNames.elevation),\n                ),\n            )\n        else:\n            sdf = sdf.withColumn(\n                geometry_col,\n                STC.ST_MakePoint(\n                    STF.ST_X(F.col(geometry_col)),\n                    STF.ST_Y(F.col(geometry_col)),\n                    F.lit(0.0),\n                ),\n            )\n        return sdf\n\n    # project geometry to cartesian crs\n    @staticmethod\n    def project_to_crs(sdf: DataFrame, crs_in: int, crs_out: int) -&gt; DataFrame:\n        \"\"\"\n        Projects geometry to cartesian CRS.\n\n        Args:\n            sdf (DataFrame): Input DataFrame.\n            crs_in (int): Input CRS.\n            crs_out (int): Output CRS.\n\n        Returns:\n            DataFrame: DataFrame with geometry projected to cartesian CRS.\n        \"\"\"\n        crs_in = f\"EPSG:{crs_in}\"\n        crs_out = f\"EPSG:{crs_out}\"\n\n        sdf = sdf.withColumn(\n            ColNames.geometry,\n            STF.ST_Transform(sdf[ColNames.geometry], F.lit(crs_in), F.lit(crs_out)),\n        )\n        return sdf\n\n    @staticmethod\n    def spatial_join_within_distance(sdf_from: DataFrame, sdf_to: DataFrame, within_distance_col: str) -&gt; DataFrame:\n        \"\"\"\n        Performs a spatial join within a specified distance.\n\n        Args:\n            sdf_from (DataFrame): Input DataFrame.\n            sdf_to (DataFrame): DataFrame to join with.\n            within_distance_col (str): Column name for the within distance.\n\n        Returns:\n            DataFrame: DataFrame after performing the spatial join.\n        \"\"\"\n        sdf_to = sdf_to.withColumnRenamed(ColNames.geometry, ColNames.joined_geometry)\n\n        sdf_merged = sdf_from.join(\n            sdf_to,\n            STP.ST_Intersects(\n                STF.ST_Buffer(sdf_from[ColNames.geometry], sdf_from[within_distance_col]),\n                sdf_to[ColNames.joined_geometry],\n            ),\n        )\n\n        return sdf_merged\n\n    @staticmethod\n    def calculate_cartesian_distances(sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Calculates cartesian distances.\n\n        Args:\n            sdf (DataFrame): Input DataFrame.\n\n        Returns:\n            DataFrame: DataFrame with calculated cartesian distances.\n        \"\"\"\n\n        sdf = sdf.withColumns(\n            {\n                ColNames.distance_to_cell_3D: STF.ST_3DDistance(\n                    F.col(ColNames.geometry), F.col(ColNames.joined_geometry)\n                ),\n                ColNames.distance_to_cell: STF.ST_Distance(F.col(ColNames.geometry), F.col(ColNames.joined_geometry)),\n            }\n        )\n\n        return sdf\n\n    @staticmethod\n    def watt_to_dbm(sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Converts power from watt to dBm.\n\n        Args:\n            sdf (DataFrame): Input DataFrame.\n\n        Returns:\n            DataFrame: DataFrame with power converted to dBm.\n        \"\"\"\n        return sdf.withColumn(ColNames.power, 10.0 * F.log10(F.col(ColNames.power)) + 30.0)\n\n    @staticmethod\n    def calculate_distance_power_loss(sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Calculates distance power loss caluclated as\n        power - path_loss_exponent * 10 * log10(distance_to_cell_3D).\n\n        Args:\n            sdf (DataFrame): Input DataFrame.\n\n        Returns:\n            DataFrame: DataFrame with calculated distance power loss.\n        \"\"\"\n        return sdf.withColumn(\n            ColNames.signal_strength,\n            F.col(ColNames.power)\n            - F.col(ColNames.path_loss_exponent) * 10 * F.log10(F.col(ColNames.distance_to_cell_3D)),\n        )\n\n    @staticmethod\n    def join_sd_mapping(\n        sdf: DataFrame,\n        sd_mapping_sdf: DataFrame,\n        beam_width_col: str,\n        signal_front_back_difference_col: str,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Joins DataFrame with standard deviation mapping.\n\n        Args:\n            sdf (DataFrame): Input DataFrame.\n            sd_mapping_sdf (DataFrame): DataFrame with standard deviation mapping.\n            beam_width_col (str): Column name for the beam width.\n            signal_front_back_difference_col (str): Column name for the signal front-back difference.\n\n        Returns:\n            DataFrame: DataFrame after joining with standard deviation mapping.\n        \"\"\"\n\n        join_condition = (sdf[beam_width_col] == sd_mapping_sdf[beam_width_col]) &amp; (\n            sdf[signal_front_back_difference_col] == sd_mapping_sdf[signal_front_back_difference_col]\n        )\n\n        sdf = sdf.join(sd_mapping_sdf, join_condition).drop(\n            sd_mapping_sdf[beam_width_col],\n            sd_mapping_sdf[signal_front_back_difference_col],\n        )\n\n        return sdf\n\n    @staticmethod\n    def get_sd_to_signal_back_loss_mappings(cells_sdf: DataFrame, signal_front_back_difference_col: str) -&gt; DataFrame:\n        \"\"\"\n        Generates a DataFrame with mapping of signal strength standard deviation for each\n            elevation/azimuth angle degree.\n\n        Parameters:\n        cells_sdf (DataFrame): A Spark DataFrame containing information about the cells.\n        signal_front_back_difference_col (str): The name of the column that contains the difference\n            in signal strength between\n        the front and back of the cell.\n\n        Returns:\n        DataFrame: A pandas DataFrame with standard deviation mappings.\n\n        \"\"\"\n        db_back_diffs = (\n            cells_sdf.select(F.col(signal_front_back_difference_col))\n            .where(F.col(ColNames.directionality) == 1)\n            .distinct()\n        )\n        db_back_diffs = [row.asDict()[signal_front_back_difference_col] for row in db_back_diffs.collect()]\n        mappings = [\n            SignalStrengthModeling.create_mapping(item, signal_front_back_difference_col) for item in db_back_diffs\n        ]\n\n        return pd.concat(mappings)\n\n    def get_angular_adjustments_sd_mapping(\n        self,\n        cells_sdf: DataFrame,\n        beam_width_col: str,\n        signal_front_back_difference_col: str,\n        angular_adjustment_type: str,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Calculates standard deviations in signal strength based on beam width and front-back cell signal difference.\n\n        Args:\n            cells_sdf (DataFrame): Input DataFrame.\n            beam_width_col (str): Column name for the beam width.\n            signal_front_back_difference_col (str): Column name for the signal front-back difference.\n            angular_adjustment_type (str): Type of angular adjustment.\n\n        Returns:\n            DataFrame: DataFrame with angular adjustments standard deviation mapping.\n        \"\"\"\n\n        sd_mappings = SignalStrengthModeling.get_sd_to_signal_back_loss_mappings(\n            cells_sdf, signal_front_back_difference_col\n        )\n        beam_widths_diff = (\n            cells_sdf.select(F.col(beam_width_col), F.col(signal_front_back_difference_col))\n            .where(F.col(ColNames.directionality) == 1)\n            .distinct()\n        )\n        beam_widths_diff = [row.asDict() for row in beam_widths_diff.collect()]\n\n        beam_sds = []\n        for item in beam_widths_diff:\n            item[f\"sd_{angular_adjustment_type}\"] = self.find_sd(\n                item[beam_width_col],\n                sd_mappings[sd_mappings[signal_front_back_difference_col] == item[signal_front_back_difference_col]],\n            )\n            beam_sds.append(item)\n\n        beam_sd_sdf = self.spark.createDataFrame(beam_sds)\n\n        return beam_sd_sdf\n\n    @staticmethod\n    def normalize_angle(a: float) -&gt; float:\n        \"\"\"\n        Adjusts the given angle to fall within the range of -180 to 180 degrees.\n\n        Args:\n            a (float): The angle in degrees to be normalized.\n\n        Returns:\n            float: The input angle adjusted to fall within the range of -180 to 180 degrees.\n        \"\"\"\n        return ((a + 180) % 360) - 180\n\n    @staticmethod\n    def normal_distribution(x: float, mean: float, sd: float, return_type: str) -&gt; Union[np.array, list]:\n        \"\"\"\n        Computes the value of the normal distribution with the given mean\n        and standard deviation at the given point.\n\n        Args:\n            x (float): The point at which to evaluate the normal distribution.\n            mean (float): The mean of the normal distribution.\n            sd (float): The standard deviation of the normal distribution.\n            return_type (str): The desired return type, either 'np_array' or 'list'.\n\n        Returns:\n            np.array or list: The value of the normal distribution at the given point,\n            returned as either a numpy array or a list.\n        \"\"\"\n        n_dist = (1.0 / (sqrt(2.0 * pi) * sd)) * np.exp(-0.5 * ((x - mean) / sd) ** 2)\n\n        if return_type == \"np_array\":\n            return n_dist\n\n        elif return_type == \"list\":\n            return n_dist.tolist()\n\n    @staticmethod\n    def norm_dBloss(a: float, sd: float, db_back: float) -&gt; float:\n        \"\"\"\n        Computes the loss in signal strength in dB as a function of\n        angle from the direction of maximum signal strength.\n\n        Args:\n            a (float): The angle from the direction of maximum signal strength.\n            sd (float): The standard deviation of the normal distribution modeling the signal strength.\n            db_back (float): The difference in signal strength in dB between the front and back of the signal.\n\n        Returns:\n            float: The loss in signal strength in dB at the given angle.\n        \"\"\"\n        a = SignalStrengthModeling.normalize_angle(a)\n        inflate = -db_back / (\n            SignalStrengthModeling.normal_distribution(0, 0, sd, \"np_array\")\n            - SignalStrengthModeling.normal_distribution(180, 0, sd, \"np_array\")\n        )\n        return (\n            SignalStrengthModeling.normal_distribution(a, 0, sd, \"np_array\")\n            - SignalStrengthModeling.normal_distribution(0, 0, sd, \"np_array\")\n        ) * inflate\n\n    @staticmethod\n    @F.udf(FloatType())\n    def norm_dBloss_udf(a, sd, db_back):\n        a = SignalStrengthModeling.normalize_angle(a)  # You need to define this function\n        inflate = -db_back / (\n            SignalStrengthModeling.normal_distribution(0, 0, sd, \"list\")\n            - SignalStrengthModeling.normal_distribution(180, 0, sd, \"list\")\n        )\n        return (\n            SignalStrengthModeling.normal_distribution(a, 0, sd, \"list\")\n            - SignalStrengthModeling.normal_distribution(0, 0, sd, \"list\")\n        ) * inflate\n\n    @staticmethod\n    def get_min3db(sd: float, db_back: float) -&gt; float:\n        \"\"\"\n        Finds the angle at which the signal strength falls to 3 dB below its maximum value.\n\n        Args:\n            sd (float): The standard deviation of the normal distribution modeling the signal strength.\n            db_back (float): The difference in signal strength in dB between the front and back of the signal.\n\n        Returns:\n            float: The angle at which the signal strength falls to 3 dB below its maximum value.\n        \"\"\"\n        df = pd.DataFrame({\"a\": np.linspace(0, 180, 720)})\n        df[\"dbLoss\"] = SignalStrengthModeling.norm_dBloss(df[\"a\"], db_back=db_back, sd=sd)\n        return df.loc[np.abs(-3 - df[\"dbLoss\"]).idxmin(), \"a\"]\n\n    @staticmethod\n    def create_mapping(db_back: float, signal_front_back_difference_col) -&gt; DataFrame:\n        \"\"\"\n        Creates a mapping between standard deviation and the angle\n        at which the signal strength falls to 3 dB below its maximum value.\n\n        Args:\n            db_back (float): The difference in signal strength in dB between the front and back of the signal.\n\n        Returns:\n            DataFrame: A DataFrame where each row corresponds to a\n            standard deviation and contains the corresponding angle.\n        \"\"\"\n        idf = pd.DataFrame({\"sd\": np.arange(180 / 1000, 180, 180 / 1000)})\n        idf[\"deg\"] = idf[\"sd\"].apply(SignalStrengthModeling.get_min3db, db_back=db_back)\n        df = pd.DataFrame({\"deg\": np.arange(1, 181)})\n        df[\"sd\"] = df[\"deg\"].apply(lambda dg: idf.loc[np.abs(idf[\"deg\"] - dg).idxmin(), \"sd\"])\n        df[signal_front_back_difference_col] = db_back\n        return df\n\n    @staticmethod\n    def find_sd(beam_width: float, mapping: DataFrame) -&gt; float:\n        \"\"\"\n        Finds the standard deviation corresponding to the given beam width using the provided mapping.\n\n        Args:\n            beam_width (float): The width of the beam in degrees.\n            mapping (DataFrame): A DataFrame where each row corresponds to a standard deviation\n                and contains the corresponding angle.\n\n        Returns:\n            float: The standard deviation corresponding to the given beam width.\n        \"\"\"\n        min_diff_index = (abs(mapping[\"deg\"] - beam_width / 2)).idxmin()\n        return float(mapping.loc[min_diff_index, \"sd\"])\n\n    @staticmethod\n    def calculate_horizontal_angle_power_adjustment(sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Adjusts the signal strength of each cell in the input DataFrame based on the horizontal angle.\n\n        This function calculates the azimuth angle between each cell and a reference point,\n        projects the data to the elevation plane, and adjusts the signal strength based on the\n        relative azimuth angle and the distance to the cell. The adjustment is calculated using\n        a normal distribution model of signal strength.\n\n        Based on https://github.com/MobilePhoneESSnetBigData/mobloc/blob/master/R/signal_strength.R\n\n        Args:\n            sdf (DataFrame): A Spark DataFrame\n\n        Returns:\n            DataFrame: The input DataFrame with the 'signal_strength' column adjusted and intermediate columns dropped.\n        \"\"\"\n\n        # TODO: simplify math in this function by using Sedona built in spatial methods\n        sdf = sdf.withColumn(\n            \"theta_azim\",\n            (\n                90\n                - F.degrees(\n                    (\n                        F.atan2(\n                            STF.ST_Y(F.col(ColNames.joined_geometry)) - STF.ST_Y(F.col(ColNames.geometry)),\n                            STF.ST_X(F.col(ColNames.joined_geometry)) - STF.ST_X(F.col(ColNames.geometry)),\n                        )\n                    )\n                )\n            ),\n        )\n        sdf = sdf.withColumn(\n            \"theta_azim\",\n            F.when(F.col(\"theta_azim\") &lt; 0, F.col(\"theta_azim\") + 360).otherwise(F.col(\"theta_azim\")),\n        )\n        sdf = sdf.withColumn(\"azim\", (F.col(\"theta_azim\") - F.col(ColNames.azimuth_angle)) % 360)\n        sdf = sdf.withColumn(\n            \"azim\",\n            F.when(F.col(\"azim\") &gt; 180, F.col(\"azim\") - 360).otherwise(\n                F.when(F.col(\"azim\") &lt; -180, F.col(\"azim\") + 360).otherwise(F.col(\"azim\"))\n            ),\n        )\n        sdf = sdf.withColumn(\"a\", F.sin(F.radians(F.col(\"azim\"))) * F.col(ColNames.distance_to_cell))\n\n        # project to elevation plane\n        sdf = sdf.withColumn(\"b\", F.cos(F.radians(F.col(\"azim\"))) * F.col(ColNames.distance_to_cell))\n        sdf = sdf.withColumn(\"c\", STF.ST_Z(ColNames.geometry) - STF.ST_Z(ColNames.joined_geometry))\n\n        sdf = sdf.withColumn(\"d\", F.sqrt(F.col(\"b\") ** 2 + F.col(\"c\") ** 2))\n        sdf = sdf.withColumn(\"lambda\", F.degrees(F.atan2(F.col(\"c\"), F.abs(F.col(\"b\")))))\n        sdf = sdf.withColumn(\n            \"cases\",\n            F.when(\n                F.col(\"b\") &gt; 0,\n                F.when(F.col(ColNames.elevation_angle) &lt; F.col(\"lambda\"), F.lit(1)).otherwise(F.lit(2)),\n            ).otherwise(F.when(F.col(\"lambda\") + F.col(ColNames.elevation_angle) &lt; 90, F.lit(3)).otherwise(F.lit(4))),\n        )\n\n        sdf = sdf.withColumn(\n            \"e\",\n            F.when(\n                F.col(\"cases\") == 1,\n                F.cos(F.radians(F.col(\"lambda\") - F.col(ColNames.elevation_angle))) * F.col(\"d\"),\n            )\n            .when(\n                F.col(\"cases\") == 2,\n                F.cos(F.radians(F.col(ColNames.elevation_angle) - F.col(\"lambda\"))) * F.col(\"d\"),\n            )\n            .when(\n                F.col(\"cases\") == 3,\n                -F.cos(F.radians(F.col(\"lambda\") + F.col(ColNames.elevation_angle))) * F.col(\"d\"),\n            )\n            .otherwise(F.cos(F.radians(180 - F.col(\"lambda\") - F.col(ColNames.elevation_angle))) * F.col(\"d\")),\n        )\n\n        sdf = sdf.withColumn(\"azim2\", F.degrees(F.atan2(F.col(\"a\"), F.col(\"e\"))))\n\n        # finally get power adjustments\n        sdf = sdf.withColumn(\n            ColNames.signal_strength,\n            F.col(ColNames.signal_strength)\n            + SignalStrengthModeling.norm_dBloss_udf(\n                F.col(\"azim2\"),\n                F.col(\"sd_azimuth\"),\n                F.col(ColNames.azimuth_signal_strength_back_loss),\n            ),\n        )\n\n        # cleanup\n        sdf = sdf.drop(\n            \"theta_azim\",\n            \"azim\",\n            \"a\",\n            \"b\",\n            \"c\",\n            \"d\",\n            \"_lambda\",\n            \"cases\",\n            \"e\",\n            \"azim2\",\n            \"sd_azimuth\",\n        )\n\n        return sdf\n\n    @staticmethod\n    def calculate_vertical_angle_power_adjustment(sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Adjusts the signal strength of each cell in the input DataFrame based on the vertical angle.\n\n        This function calculates the elevation angle between each cell and a reference point,\n        and adjusts the signal strength based on the relative elevation angle and the distance to the cell.\n        The adjustment is calculated using a normal distribution model of signal strength.\n\n        Based on https://github.com/MobilePhoneESSnetBigData/mobloc/blob/master/R/signal_strength.R\n\n        Args:\n            sdf (DataFrame): A Spark DataFrame\n\n        Returns:\n            DataFrame: The input DataFrame with the 'signal_strength' column adjusted and intermediate columns dropped.\n        \"\"\"\n\n        sdf = sdf.withColumn(\n            \"gamma_elev\",\n            F.degrees(\n                F.atan2(\n                    STF.ST_Z(ColNames.geometry) - STF.ST_Z(ColNames.joined_geometry),\n                    F.col(ColNames.distance_to_cell),\n                )\n            ),\n        )\n        sdf = sdf.withColumn(\"elev\", (F.col(\"gamma_elev\") - F.col(ColNames.elevation_angle)) % 360)\n\n        sdf = sdf.withColumn(\n            \"elev\",\n            F.when(F.col(\"elev\") &gt; 180, F.col(\"elev\") - 360).otherwise(\n                F.when(F.col(\"elev\") &lt; -180, F.col(\"elev\") + 360).otherwise(F.col(\"elev\"))\n            ),\n        )\n\n        # get power adjustments\n        sdf = sdf.withColumn(\n            ColNames.signal_strength,\n            F.col(ColNames.signal_strength)\n            + SignalStrengthModeling.norm_dBloss_udf(\n                F.col(\"elev\"),\n                F.col(\"sd_elevation\"),\n                F.col(ColNames.elevation_signal_strength_back_loss),\n            ),\n        )\n\n        # cleanup\n        sdf = sdf.drop(\"gamma_elev\", \"elev\", \"sd_elevation\")\n\n        return sdf\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.add_z_to_point_geometry","title":"<code>add_z_to_point_geometry(sdf, geometry_col, use_elevation)</code>  <code>staticmethod</code>","text":"<p>Adds z value to the point geometry (grid centroids). If elevation is taken into account, set Z values from z column, otherwise to 0.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>use_elevation</code> <code>bool</code> <p>Whether to use elevation.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with z value added to point geometry.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>@staticmethod\ndef add_z_to_point_geometry(sdf: DataFrame, geometry_col: str, use_elevation: bool) -&gt; DataFrame:\n    \"\"\"\n    Adds z value to the point geometry (grid centroids).\n    If elevation is taken into account, set Z values from z column, otherwise to 0.\n\n    Args:\n        sdf (DataFrame): Input DataFrame.\n        use_elevation (bool): Whether to use elevation.\n\n    Returns:\n        DataFrame: DataFrame with z value added to point geometry.\n    \"\"\"\n    if use_elevation:\n        sdf = sdf.withColumn(\n            geometry_col,\n            STC.ST_MakePoint(\n                STF.ST_X(F.col(geometry_col)),\n                STF.ST_Y(F.col(geometry_col)),\n                F.col(ColNames.elevation),\n            ),\n        )\n    else:\n        sdf = sdf.withColumn(\n            geometry_col,\n            STC.ST_MakePoint(\n                STF.ST_X(F.col(geometry_col)),\n                STF.ST_Y(F.col(geometry_col)),\n                F.lit(0.0),\n            ),\n        )\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.calculate_cartesian_distances","title":"<code>calculate_cartesian_distances(sdf)</code>  <code>staticmethod</code>","text":"<p>Calculates cartesian distances.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with calculated cartesian distances.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>@staticmethod\ndef calculate_cartesian_distances(sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Calculates cartesian distances.\n\n    Args:\n        sdf (DataFrame): Input DataFrame.\n\n    Returns:\n        DataFrame: DataFrame with calculated cartesian distances.\n    \"\"\"\n\n    sdf = sdf.withColumns(\n        {\n            ColNames.distance_to_cell_3D: STF.ST_3DDistance(\n                F.col(ColNames.geometry), F.col(ColNames.joined_geometry)\n            ),\n            ColNames.distance_to_cell: STF.ST_Distance(F.col(ColNames.geometry), F.col(ColNames.joined_geometry)),\n        }\n    )\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.calculate_distance_power_loss","title":"<code>calculate_distance_power_loss(sdf)</code>  <code>staticmethod</code>","text":"<p>Calculates distance power loss caluclated as power - path_loss_exponent * 10 * log10(distance_to_cell_3D).</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with calculated distance power loss.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>@staticmethod\ndef calculate_distance_power_loss(sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Calculates distance power loss caluclated as\n    power - path_loss_exponent * 10 * log10(distance_to_cell_3D).\n\n    Args:\n        sdf (DataFrame): Input DataFrame.\n\n    Returns:\n        DataFrame: DataFrame with calculated distance power loss.\n    \"\"\"\n    return sdf.withColumn(\n        ColNames.signal_strength,\n        F.col(ColNames.power)\n        - F.col(ColNames.path_loss_exponent) * 10 * F.log10(F.col(ColNames.distance_to_cell_3D)),\n    )\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.calculate_horizontal_angle_power_adjustment","title":"<code>calculate_horizontal_angle_power_adjustment(sdf)</code>  <code>staticmethod</code>","text":"<p>Adjusts the signal strength of each cell in the input DataFrame based on the horizontal angle.</p> <p>This function calculates the azimuth angle between each cell and a reference point, projects the data to the elevation plane, and adjusts the signal strength based on the relative azimuth angle and the distance to the cell. The adjustment is calculated using a normal distribution model of signal strength.</p> <p>Based on https://github.com/MobilePhoneESSnetBigData/mobloc/blob/master/R/signal_strength.R</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>A Spark DataFrame</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The input DataFrame with the 'signal_strength' column adjusted and intermediate columns dropped.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>@staticmethod\ndef calculate_horizontal_angle_power_adjustment(sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Adjusts the signal strength of each cell in the input DataFrame based on the horizontal angle.\n\n    This function calculates the azimuth angle between each cell and a reference point,\n    projects the data to the elevation plane, and adjusts the signal strength based on the\n    relative azimuth angle and the distance to the cell. The adjustment is calculated using\n    a normal distribution model of signal strength.\n\n    Based on https://github.com/MobilePhoneESSnetBigData/mobloc/blob/master/R/signal_strength.R\n\n    Args:\n        sdf (DataFrame): A Spark DataFrame\n\n    Returns:\n        DataFrame: The input DataFrame with the 'signal_strength' column adjusted and intermediate columns dropped.\n    \"\"\"\n\n    # TODO: simplify math in this function by using Sedona built in spatial methods\n    sdf = sdf.withColumn(\n        \"theta_azim\",\n        (\n            90\n            - F.degrees(\n                (\n                    F.atan2(\n                        STF.ST_Y(F.col(ColNames.joined_geometry)) - STF.ST_Y(F.col(ColNames.geometry)),\n                        STF.ST_X(F.col(ColNames.joined_geometry)) - STF.ST_X(F.col(ColNames.geometry)),\n                    )\n                )\n            )\n        ),\n    )\n    sdf = sdf.withColumn(\n        \"theta_azim\",\n        F.when(F.col(\"theta_azim\") &lt; 0, F.col(\"theta_azim\") + 360).otherwise(F.col(\"theta_azim\")),\n    )\n    sdf = sdf.withColumn(\"azim\", (F.col(\"theta_azim\") - F.col(ColNames.azimuth_angle)) % 360)\n    sdf = sdf.withColumn(\n        \"azim\",\n        F.when(F.col(\"azim\") &gt; 180, F.col(\"azim\") - 360).otherwise(\n            F.when(F.col(\"azim\") &lt; -180, F.col(\"azim\") + 360).otherwise(F.col(\"azim\"))\n        ),\n    )\n    sdf = sdf.withColumn(\"a\", F.sin(F.radians(F.col(\"azim\"))) * F.col(ColNames.distance_to_cell))\n\n    # project to elevation plane\n    sdf = sdf.withColumn(\"b\", F.cos(F.radians(F.col(\"azim\"))) * F.col(ColNames.distance_to_cell))\n    sdf = sdf.withColumn(\"c\", STF.ST_Z(ColNames.geometry) - STF.ST_Z(ColNames.joined_geometry))\n\n    sdf = sdf.withColumn(\"d\", F.sqrt(F.col(\"b\") ** 2 + F.col(\"c\") ** 2))\n    sdf = sdf.withColumn(\"lambda\", F.degrees(F.atan2(F.col(\"c\"), F.abs(F.col(\"b\")))))\n    sdf = sdf.withColumn(\n        \"cases\",\n        F.when(\n            F.col(\"b\") &gt; 0,\n            F.when(F.col(ColNames.elevation_angle) &lt; F.col(\"lambda\"), F.lit(1)).otherwise(F.lit(2)),\n        ).otherwise(F.when(F.col(\"lambda\") + F.col(ColNames.elevation_angle) &lt; 90, F.lit(3)).otherwise(F.lit(4))),\n    )\n\n    sdf = sdf.withColumn(\n        \"e\",\n        F.when(\n            F.col(\"cases\") == 1,\n            F.cos(F.radians(F.col(\"lambda\") - F.col(ColNames.elevation_angle))) * F.col(\"d\"),\n        )\n        .when(\n            F.col(\"cases\") == 2,\n            F.cos(F.radians(F.col(ColNames.elevation_angle) - F.col(\"lambda\"))) * F.col(\"d\"),\n        )\n        .when(\n            F.col(\"cases\") == 3,\n            -F.cos(F.radians(F.col(\"lambda\") + F.col(ColNames.elevation_angle))) * F.col(\"d\"),\n        )\n        .otherwise(F.cos(F.radians(180 - F.col(\"lambda\") - F.col(ColNames.elevation_angle))) * F.col(\"d\")),\n    )\n\n    sdf = sdf.withColumn(\"azim2\", F.degrees(F.atan2(F.col(\"a\"), F.col(\"e\"))))\n\n    # finally get power adjustments\n    sdf = sdf.withColumn(\n        ColNames.signal_strength,\n        F.col(ColNames.signal_strength)\n        + SignalStrengthModeling.norm_dBloss_udf(\n            F.col(\"azim2\"),\n            F.col(\"sd_azimuth\"),\n            F.col(ColNames.azimuth_signal_strength_back_loss),\n        ),\n    )\n\n    # cleanup\n    sdf = sdf.drop(\n        \"theta_azim\",\n        \"azim\",\n        \"a\",\n        \"b\",\n        \"c\",\n        \"d\",\n        \"_lambda\",\n        \"cases\",\n        \"e\",\n        \"azim2\",\n        \"sd_azimuth\",\n    )\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.calculate_vertical_angle_power_adjustment","title":"<code>calculate_vertical_angle_power_adjustment(sdf)</code>  <code>staticmethod</code>","text":"<p>Adjusts the signal strength of each cell in the input DataFrame based on the vertical angle.</p> <p>This function calculates the elevation angle between each cell and a reference point, and adjusts the signal strength based on the relative elevation angle and the distance to the cell. The adjustment is calculated using a normal distribution model of signal strength.</p> <p>Based on https://github.com/MobilePhoneESSnetBigData/mobloc/blob/master/R/signal_strength.R</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>A Spark DataFrame</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The input DataFrame with the 'signal_strength' column adjusted and intermediate columns dropped.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>@staticmethod\ndef calculate_vertical_angle_power_adjustment(sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Adjusts the signal strength of each cell in the input DataFrame based on the vertical angle.\n\n    This function calculates the elevation angle between each cell and a reference point,\n    and adjusts the signal strength based on the relative elevation angle and the distance to the cell.\n    The adjustment is calculated using a normal distribution model of signal strength.\n\n    Based on https://github.com/MobilePhoneESSnetBigData/mobloc/blob/master/R/signal_strength.R\n\n    Args:\n        sdf (DataFrame): A Spark DataFrame\n\n    Returns:\n        DataFrame: The input DataFrame with the 'signal_strength' column adjusted and intermediate columns dropped.\n    \"\"\"\n\n    sdf = sdf.withColumn(\n        \"gamma_elev\",\n        F.degrees(\n            F.atan2(\n                STF.ST_Z(ColNames.geometry) - STF.ST_Z(ColNames.joined_geometry),\n                F.col(ColNames.distance_to_cell),\n            )\n        ),\n    )\n    sdf = sdf.withColumn(\"elev\", (F.col(\"gamma_elev\") - F.col(ColNames.elevation_angle)) % 360)\n\n    sdf = sdf.withColumn(\n        \"elev\",\n        F.when(F.col(\"elev\") &gt; 180, F.col(\"elev\") - 360).otherwise(\n            F.when(F.col(\"elev\") &lt; -180, F.col(\"elev\") + 360).otherwise(F.col(\"elev\"))\n        ),\n    )\n\n    # get power adjustments\n    sdf = sdf.withColumn(\n        ColNames.signal_strength,\n        F.col(ColNames.signal_strength)\n        + SignalStrengthModeling.norm_dBloss_udf(\n            F.col(\"elev\"),\n            F.col(\"sd_elevation\"),\n            F.col(ColNames.elevation_signal_strength_back_loss),\n        ),\n    )\n\n    # cleanup\n    sdf = sdf.drop(\"gamma_elev\", \"elev\", \"sd_elevation\")\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.create_cell_point_geometry","title":"<code>create_cell_point_geometry(sdf, use_elevation)</code>  <code>staticmethod</code>","text":"<p>Creates cell point geometry. If elevation is taken into account, set Z values from z column, otherwise to 0.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>use_elevation</code> <code>bool</code> <p>Whether to use elevation.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with cell point geometry.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>@staticmethod\ndef create_cell_point_geometry(sdf: DataFrame, use_elevation: bool) -&gt; DataFrame:\n    \"\"\"\n    Creates cell point geometry.\n    If elevation is taken into account, set Z values from z column, otherwise to 0.\n\n    Args:\n        sdf (DataFrame): Input DataFrame.\n        use_elevation (bool): Whether to use elevation.\n\n    Returns:\n        DataFrame: DataFrame with cell point geometry.\n    \"\"\"\n    if use_elevation:\n        sdf = sdf.withColumn(\n            ColNames.geometry,\n            STC.ST_MakePoint(\n                F.col(ColNames.longitude),\n                F.col(ColNames.latitude),\n                F.col(ColNames.altitude) + F.col(ColNames.antenna_height),\n            ),\n        )\n    else:\n        sdf = sdf.withColumn(\n            ColNames.geometry,\n            STC.ST_MakePoint(\n                F.col(ColNames.longitude),\n                F.col(ColNames.latitude),\n                F.col(ColNames.antenna_height),\n            ),\n        )\n    # assign crs\n    sdf = sdf.withColumn(ColNames.geometry, STF.ST_SetSRID(F.col(ColNames.geometry), F.lit(4326)))\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.create_default_properties_df","title":"<code>create_default_properties_df()</code>","text":"<p>Creates a DataFrame with default cell properties from config dict.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame with default cell properties.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>def create_default_properties_df(self) -&gt; DataFrame:\n    \"\"\"\n    Creates a DataFrame with default cell properties from config dict.\n\n    Returns:\n        DataFrame: A DataFrame with default cell properties.\n    \"\"\"\n\n    rows = [Row(cell_type=k, **v) for k, v in self.default_cell_properties.items()]\n    return self.spark.createDataFrame(rows).withColumnRenamed(\"cell_type\", ColNames.cell_type)\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.create_mapping","title":"<code>create_mapping(db_back, signal_front_back_difference_col)</code>  <code>staticmethod</code>","text":"<p>Creates a mapping between standard deviation and the angle at which the signal strength falls to 3 dB below its maximum value.</p> <p>Parameters:</p> Name Type Description Default <code>db_back</code> <code>float</code> <p>The difference in signal strength in dB between the front and back of the signal.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame where each row corresponds to a</p> <code>DataFrame</code> <p>standard deviation and contains the corresponding angle.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>@staticmethod\ndef create_mapping(db_back: float, signal_front_back_difference_col) -&gt; DataFrame:\n    \"\"\"\n    Creates a mapping between standard deviation and the angle\n    at which the signal strength falls to 3 dB below its maximum value.\n\n    Args:\n        db_back (float): The difference in signal strength in dB between the front and back of the signal.\n\n    Returns:\n        DataFrame: A DataFrame where each row corresponds to a\n        standard deviation and contains the corresponding angle.\n    \"\"\"\n    idf = pd.DataFrame({\"sd\": np.arange(180 / 1000, 180, 180 / 1000)})\n    idf[\"deg\"] = idf[\"sd\"].apply(SignalStrengthModeling.get_min3db, db_back=db_back)\n    df = pd.DataFrame({\"deg\": np.arange(1, 181)})\n    df[\"sd\"] = df[\"deg\"].apply(lambda dg: idf.loc[np.abs(idf[\"deg\"] - dg).idxmin(), \"sd\"])\n    df[signal_front_back_difference_col] = db_back\n    return df\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.find_sd","title":"<code>find_sd(beam_width, mapping)</code>  <code>staticmethod</code>","text":"<p>Finds the standard deviation corresponding to the given beam width using the provided mapping.</p> <p>Parameters:</p> Name Type Description Default <code>beam_width</code> <code>float</code> <p>The width of the beam in degrees.</p> required <code>mapping</code> <code>DataFrame</code> <p>A DataFrame where each row corresponds to a standard deviation and contains the corresponding angle.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The standard deviation corresponding to the given beam width.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>@staticmethod\ndef find_sd(beam_width: float, mapping: DataFrame) -&gt; float:\n    \"\"\"\n    Finds the standard deviation corresponding to the given beam width using the provided mapping.\n\n    Args:\n        beam_width (float): The width of the beam in degrees.\n        mapping (DataFrame): A DataFrame where each row corresponds to a standard deviation\n            and contains the corresponding angle.\n\n    Returns:\n        float: The standard deviation corresponding to the given beam width.\n    \"\"\"\n    min_diff_index = (abs(mapping[\"deg\"] - beam_width / 2)).idxmin()\n    return float(mapping.loc[min_diff_index, \"sd\"])\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.get_angular_adjustments_sd_mapping","title":"<code>get_angular_adjustments_sd_mapping(cells_sdf, beam_width_col, signal_front_back_difference_col, angular_adjustment_type)</code>","text":"<p>Calculates standard deviations in signal strength based on beam width and front-back cell signal difference.</p> <p>Parameters:</p> Name Type Description Default <code>cells_sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>beam_width_col</code> <code>str</code> <p>Column name for the beam width.</p> required <code>signal_front_back_difference_col</code> <code>str</code> <p>Column name for the signal front-back difference.</p> required <code>angular_adjustment_type</code> <code>str</code> <p>Type of angular adjustment.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with angular adjustments standard deviation mapping.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>def get_angular_adjustments_sd_mapping(\n    self,\n    cells_sdf: DataFrame,\n    beam_width_col: str,\n    signal_front_back_difference_col: str,\n    angular_adjustment_type: str,\n) -&gt; DataFrame:\n    \"\"\"\n    Calculates standard deviations in signal strength based on beam width and front-back cell signal difference.\n\n    Args:\n        cells_sdf (DataFrame): Input DataFrame.\n        beam_width_col (str): Column name for the beam width.\n        signal_front_back_difference_col (str): Column name for the signal front-back difference.\n        angular_adjustment_type (str): Type of angular adjustment.\n\n    Returns:\n        DataFrame: DataFrame with angular adjustments standard deviation mapping.\n    \"\"\"\n\n    sd_mappings = SignalStrengthModeling.get_sd_to_signal_back_loss_mappings(\n        cells_sdf, signal_front_back_difference_col\n    )\n    beam_widths_diff = (\n        cells_sdf.select(F.col(beam_width_col), F.col(signal_front_back_difference_col))\n        .where(F.col(ColNames.directionality) == 1)\n        .distinct()\n    )\n    beam_widths_diff = [row.asDict() for row in beam_widths_diff.collect()]\n\n    beam_sds = []\n    for item in beam_widths_diff:\n        item[f\"sd_{angular_adjustment_type}\"] = self.find_sd(\n            item[beam_width_col],\n            sd_mappings[sd_mappings[signal_front_back_difference_col] == item[signal_front_back_difference_col]],\n        )\n        beam_sds.append(item)\n\n    beam_sd_sdf = self.spark.createDataFrame(beam_sds)\n\n    return beam_sd_sdf\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.get_min3db","title":"<code>get_min3db(sd, db_back)</code>  <code>staticmethod</code>","text":"<p>Finds the angle at which the signal strength falls to 3 dB below its maximum value.</p> <p>Parameters:</p> Name Type Description Default <code>sd</code> <code>float</code> <p>The standard deviation of the normal distribution modeling the signal strength.</p> required <code>db_back</code> <code>float</code> <p>The difference in signal strength in dB between the front and back of the signal.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The angle at which the signal strength falls to 3 dB below its maximum value.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>@staticmethod\ndef get_min3db(sd: float, db_back: float) -&gt; float:\n    \"\"\"\n    Finds the angle at which the signal strength falls to 3 dB below its maximum value.\n\n    Args:\n        sd (float): The standard deviation of the normal distribution modeling the signal strength.\n        db_back (float): The difference in signal strength in dB between the front and back of the signal.\n\n    Returns:\n        float: The angle at which the signal strength falls to 3 dB below its maximum value.\n    \"\"\"\n    df = pd.DataFrame({\"a\": np.linspace(0, 180, 720)})\n    df[\"dbLoss\"] = SignalStrengthModeling.norm_dBloss(df[\"a\"], db_back=db_back, sd=sd)\n    return df.loc[np.abs(-3 - df[\"dbLoss\"]).idxmin(), \"a\"]\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.get_sd_to_signal_back_loss_mappings","title":"<code>get_sd_to_signal_back_loss_mappings(cells_sdf, signal_front_back_difference_col)</code>  <code>staticmethod</code>","text":"<p>Generates a DataFrame with mapping of signal strength standard deviation for each     elevation/azimuth angle degree.</p> <p>Parameters: cells_sdf (DataFrame): A Spark DataFrame containing information about the cells. signal_front_back_difference_col (str): The name of the column that contains the difference     in signal strength between the front and back of the cell.</p> <p>Returns: DataFrame: A pandas DataFrame with standard deviation mappings.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>@staticmethod\ndef get_sd_to_signal_back_loss_mappings(cells_sdf: DataFrame, signal_front_back_difference_col: str) -&gt; DataFrame:\n    \"\"\"\n    Generates a DataFrame with mapping of signal strength standard deviation for each\n        elevation/azimuth angle degree.\n\n    Parameters:\n    cells_sdf (DataFrame): A Spark DataFrame containing information about the cells.\n    signal_front_back_difference_col (str): The name of the column that contains the difference\n        in signal strength between\n    the front and back of the cell.\n\n    Returns:\n    DataFrame: A pandas DataFrame with standard deviation mappings.\n\n    \"\"\"\n    db_back_diffs = (\n        cells_sdf.select(F.col(signal_front_back_difference_col))\n        .where(F.col(ColNames.directionality) == 1)\n        .distinct()\n    )\n    db_back_diffs = [row.asDict()[signal_front_back_difference_col] for row in db_back_diffs.collect()]\n    mappings = [\n        SignalStrengthModeling.create_mapping(item, signal_front_back_difference_col) for item in db_back_diffs\n    ]\n\n    return pd.concat(mappings)\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.impute_default_cell_properties","title":"<code>impute_default_cell_properties(sdf)</code>","text":"<p>Imputes default cell properties for null values in the input DataFrame using default properties for cell types from config.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with imputed default cell properties.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>def impute_default_cell_properties(self, sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Imputes default cell properties for null values in the input DataFrame using\n    default properties for cell types from config.\n\n    Args:\n        sdf (DataFrame): Input DataFrame.\n\n    Returns:\n        DataFrame: DataFrame with imputed default cell properties.\n    \"\"\"\n    default_properties_df = self.create_default_properties_df()\n\n    # add default prefix to the columns of default_properties_df\n    default_properties_df = default_properties_df.select(\n        [F.col(col).alias(f\"default_{col}\") for col in default_properties_df.columns]\n    )\n\n    # assign default cell type to cell types not present in config\n    sdf = sdf.withColumn(\n        ColNames.cell_type,\n        F.when(\n            ~F.col(ColNames.cell_type).isin(list(self.default_cell_properties.keys())),\n            \"default\",\n        ).otherwise(F.col(ColNames.cell_type)),\n    )\n\n    # all cell types which are absent from the default_properties_df will be assigned default values\n    sdf = sdf.join(\n        default_properties_df,\n        sdf[ColNames.cell_type] == default_properties_df[f\"default_{ColNames.cell_type}\"],\n        how=\"inner\",\n    )\n    # if orignal column is null, assign the default value\n    for col in default_properties_df.columns:\n        col = col.replace(\"default_\", \"\")\n        if col not in sdf.columns:\n            sdf = sdf.withColumn(col, F.lit(None))\n        sdf = sdf.withColumn(col, F.coalesce(F.col(col), F.col(f\"default_{col}\")))\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.join_sd_mapping","title":"<code>join_sd_mapping(sdf, sd_mapping_sdf, beam_width_col, signal_front_back_difference_col)</code>  <code>staticmethod</code>","text":"<p>Joins DataFrame with standard deviation mapping.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>sd_mapping_sdf</code> <code>DataFrame</code> <p>DataFrame with standard deviation mapping.</p> required <code>beam_width_col</code> <code>str</code> <p>Column name for the beam width.</p> required <code>signal_front_back_difference_col</code> <code>str</code> <p>Column name for the signal front-back difference.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame after joining with standard deviation mapping.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>@staticmethod\ndef join_sd_mapping(\n    sdf: DataFrame,\n    sd_mapping_sdf: DataFrame,\n    beam_width_col: str,\n    signal_front_back_difference_col: str,\n) -&gt; DataFrame:\n    \"\"\"\n    Joins DataFrame with standard deviation mapping.\n\n    Args:\n        sdf (DataFrame): Input DataFrame.\n        sd_mapping_sdf (DataFrame): DataFrame with standard deviation mapping.\n        beam_width_col (str): Column name for the beam width.\n        signal_front_back_difference_col (str): Column name for the signal front-back difference.\n\n    Returns:\n        DataFrame: DataFrame after joining with standard deviation mapping.\n    \"\"\"\n\n    join_condition = (sdf[beam_width_col] == sd_mapping_sdf[beam_width_col]) &amp; (\n        sdf[signal_front_back_difference_col] == sd_mapping_sdf[signal_front_back_difference_col]\n    )\n\n    sdf = sdf.join(sd_mapping_sdf, join_condition).drop(\n        sd_mapping_sdf[beam_width_col],\n        sd_mapping_sdf[signal_front_back_difference_col],\n    )\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.norm_dBloss","title":"<code>norm_dBloss(a, sd, db_back)</code>  <code>staticmethod</code>","text":"<p>Computes the loss in signal strength in dB as a function of angle from the direction of maximum signal strength.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>The angle from the direction of maximum signal strength.</p> required <code>sd</code> <code>float</code> <p>The standard deviation of the normal distribution modeling the signal strength.</p> required <code>db_back</code> <code>float</code> <p>The difference in signal strength in dB between the front and back of the signal.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The loss in signal strength in dB at the given angle.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>@staticmethod\ndef norm_dBloss(a: float, sd: float, db_back: float) -&gt; float:\n    \"\"\"\n    Computes the loss in signal strength in dB as a function of\n    angle from the direction of maximum signal strength.\n\n    Args:\n        a (float): The angle from the direction of maximum signal strength.\n        sd (float): The standard deviation of the normal distribution modeling the signal strength.\n        db_back (float): The difference in signal strength in dB between the front and back of the signal.\n\n    Returns:\n        float: The loss in signal strength in dB at the given angle.\n    \"\"\"\n    a = SignalStrengthModeling.normalize_angle(a)\n    inflate = -db_back / (\n        SignalStrengthModeling.normal_distribution(0, 0, sd, \"np_array\")\n        - SignalStrengthModeling.normal_distribution(180, 0, sd, \"np_array\")\n    )\n    return (\n        SignalStrengthModeling.normal_distribution(a, 0, sd, \"np_array\")\n        - SignalStrengthModeling.normal_distribution(0, 0, sd, \"np_array\")\n    ) * inflate\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.normal_distribution","title":"<code>normal_distribution(x, mean, sd, return_type)</code>  <code>staticmethod</code>","text":"<p>Computes the value of the normal distribution with the given mean and standard deviation at the given point.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float</code> <p>The point at which to evaluate the normal distribution.</p> required <code>mean</code> <code>float</code> <p>The mean of the normal distribution.</p> required <code>sd</code> <code>float</code> <p>The standard deviation of the normal distribution.</p> required <code>return_type</code> <code>str</code> <p>The desired return type, either 'np_array' or 'list'.</p> required <p>Returns:</p> Type Description <code>Union[array, list]</code> <p>np.array or list: The value of the normal distribution at the given point,</p> <code>Union[array, list]</code> <p>returned as either a numpy array or a list.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>@staticmethod\ndef normal_distribution(x: float, mean: float, sd: float, return_type: str) -&gt; Union[np.array, list]:\n    \"\"\"\n    Computes the value of the normal distribution with the given mean\n    and standard deviation at the given point.\n\n    Args:\n        x (float): The point at which to evaluate the normal distribution.\n        mean (float): The mean of the normal distribution.\n        sd (float): The standard deviation of the normal distribution.\n        return_type (str): The desired return type, either 'np_array' or 'list'.\n\n    Returns:\n        np.array or list: The value of the normal distribution at the given point,\n        returned as either a numpy array or a list.\n    \"\"\"\n    n_dist = (1.0 / (sqrt(2.0 * pi) * sd)) * np.exp(-0.5 * ((x - mean) / sd) ** 2)\n\n    if return_type == \"np_array\":\n        return n_dist\n\n    elif return_type == \"list\":\n        return n_dist.tolist()\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.normalize_angle","title":"<code>normalize_angle(a)</code>  <code>staticmethod</code>","text":"<p>Adjusts the given angle to fall within the range of -180 to 180 degrees.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>The angle in degrees to be normalized.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The input angle adjusted to fall within the range of -180 to 180 degrees.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>@staticmethod\ndef normalize_angle(a: float) -&gt; float:\n    \"\"\"\n    Adjusts the given angle to fall within the range of -180 to 180 degrees.\n\n    Args:\n        a (float): The angle in degrees to be normalized.\n\n    Returns:\n        float: The input angle adjusted to fall within the range of -180 to 180 degrees.\n    \"\"\"\n    return ((a + 180) % 360) - 180\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.project_to_crs","title":"<code>project_to_crs(sdf, crs_in, crs_out)</code>  <code>staticmethod</code>","text":"<p>Projects geometry to cartesian CRS.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>crs_in</code> <code>int</code> <p>Input CRS.</p> required <code>crs_out</code> <code>int</code> <p>Output CRS.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with geometry projected to cartesian CRS.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>@staticmethod\ndef project_to_crs(sdf: DataFrame, crs_in: int, crs_out: int) -&gt; DataFrame:\n    \"\"\"\n    Projects geometry to cartesian CRS.\n\n    Args:\n        sdf (DataFrame): Input DataFrame.\n        crs_in (int): Input CRS.\n        crs_out (int): Output CRS.\n\n    Returns:\n        DataFrame: DataFrame with geometry projected to cartesian CRS.\n    \"\"\"\n    crs_in = f\"EPSG:{crs_in}\"\n    crs_out = f\"EPSG:{crs_out}\"\n\n    sdf = sdf.withColumn(\n        ColNames.geometry,\n        STF.ST_Transform(sdf[ColNames.geometry], F.lit(crs_in), F.lit(crs_out)),\n    )\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.spatial_join_within_distance","title":"<code>spatial_join_within_distance(sdf_from, sdf_to, within_distance_col)</code>  <code>staticmethod</code>","text":"<p>Performs a spatial join within a specified distance.</p> <p>Parameters:</p> Name Type Description Default <code>sdf_from</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>sdf_to</code> <code>DataFrame</code> <p>DataFrame to join with.</p> required <code>within_distance_col</code> <code>str</code> <p>Column name for the within distance.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame after performing the spatial join.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>@staticmethod\ndef spatial_join_within_distance(sdf_from: DataFrame, sdf_to: DataFrame, within_distance_col: str) -&gt; DataFrame:\n    \"\"\"\n    Performs a spatial join within a specified distance.\n\n    Args:\n        sdf_from (DataFrame): Input DataFrame.\n        sdf_to (DataFrame): DataFrame to join with.\n        within_distance_col (str): Column name for the within distance.\n\n    Returns:\n        DataFrame: DataFrame after performing the spatial join.\n    \"\"\"\n    sdf_to = sdf_to.withColumnRenamed(ColNames.geometry, ColNames.joined_geometry)\n\n    sdf_merged = sdf_from.join(\n        sdf_to,\n        STP.ST_Intersects(\n            STF.ST_Buffer(sdf_from[ColNames.geometry], sdf_from[within_distance_col]),\n            sdf_to[ColNames.joined_geometry],\n        ),\n    )\n\n    return sdf_merged\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.watt_to_dbm","title":"<code>watt_to_dbm(sdf)</code>  <code>staticmethod</code>","text":"<p>Converts power from watt to dBm.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with power converted to dBm.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>@staticmethod\ndef watt_to_dbm(sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Converts power from watt to dBm.\n\n    Args:\n        sdf (DataFrame): Input DataFrame.\n\n    Returns:\n        DataFrame: DataFrame with power converted to dBm.\n    \"\"\"\n    return sdf.withColumn(ColNames.power, 10.0 * F.log10(F.col(ColNames.power)) + 30.0)\n</code></pre>"},{"location":"reference/components/execution/time_segments/","title":"time_segments","text":""},{"location":"reference/components/execution/time_segments/continuous_time_segmentation/","title":"continuous_time_segmentation","text":"<p>Module that implements the Continuous Time Segmentations functionality</p>"},{"location":"reference/components/execution/time_segments/continuous_time_segmentation/#components.execution.time_segments.continuous_time_segmentation.ContinuousTimeSegmentation","title":"<code>ContinuousTimeSegmentation</code>","text":"<p>               Bases: <code>Component</code></p> <p>A class to aggregate events into time segments.</p> Source code in <code>multimno/components/execution/time_segments/continuous_time_segmentation.py</code> <pre><code>class ContinuousTimeSegmentation(Component):\n    \"\"\"\n    A class to aggregate events into time segments.\n    \"\"\"\n\n    COMPONENT_ID = \"ContinuousTimeSegmentation\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.data_period_start = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n        self.data_period_end = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n\n        self.min_time_stay = timedelta(seconds=self.config.getint(self.COMPONENT_ID, \"min_time_stay_s\"))\n\n        self.max_time_missing_stay = timedelta(seconds=self.config.getint(self.COMPONENT_ID, \"max_time_missing_stay_s\"))\n\n        self.max_time_missing_move = timedelta(seconds=self.config.getint(self.COMPONENT_ID, \"max_time_missing_move_s\"))\n\n        self.pad_time = timedelta(seconds=self.config.getint(self.COMPONENT_ID, \"pad_time_s\"))\n\n        self.event_error_flags_to_include = self.config.geteval(self.COMPONENT_ID, \"event_error_flags_to_include\")\n        # this is for UDF\n        self.segmentation_return_schema = StructType(\n            [\n                StructField(ColNames.start_timestamp, TimestampType()),\n                StructField(ColNames.end_timestamp, TimestampType()),\n                StructField(ColNames.cells, ArrayType(StringType())),\n                StructField(ColNames.state, StringType()),\n                StructField(ColNames.is_last, BooleanType()),\n                StructField(ColNames.time_segment_id, IntegerType()),\n                StructField(ColNames.user_id, StringType()),\n                StructField(ColNames.mcc, ShortType()),\n                StructField(ColNames.user_id_modulo, IntegerType()),\n            ]\n        )\n\n        self.data_period_dates = [\n            self.data_period_start + timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n    def initalize_data_objects(self):\n\n        # Input\n        self.input_data_objects = {}\n        self.is_first_run = self.config.getboolean(self.COMPONENT_ID, \"is_first_run\")\n\n        inputs = {\n            \"event_data_silver_flagged\": SilverEventFlaggedDataObject,\n            \"cell_intersection_groups_data_silver\": SilverCellIntersectionGroupsDataObject,\n        }\n        if not self.is_first_run:\n            inputs[\"time_segments_silver\"] = SilverTimeSegmentsDataObject\n\n        for key, value in inputs.items():\n            path = self.config.get(CONFIG_SILVER_PATHS_KEY, key)\n            if check_if_data_path_exists(self.spark, path):\n                self.input_data_objects[value.ID] = value(self.spark, path)\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")\n                raise ValueError(f\"Invalid path for {value.ID}: {path}\")\n\n        # Output\n        self.output_data_objects = {}\n        self.silver_signal_strength_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"time_segments_silver\")\n        self.output_data_objects[SilverTimeSegmentsDataObject.ID] = SilverTimeSegmentsDataObject(\n            self.spark,\n            self.silver_signal_strength_path,\n            partition_columns=[\n                ColNames.year,\n                ColNames.month,\n                ColNames.day,\n                ColNames.user_id_modulo,\n            ],\n        )\n\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n\n        # If segements was already calculated and this is continuation of the previous run\n        # we need to get the last time segment for each user.\n        # If this is the first run, we will create an empty dataframe\n        if self.is_first_run:\n            self.intital_time_segment = self.spark.createDataFrame([], SilverTimeSegmentsDataObject.SCHEMA)\n        else:\n            previous_date = self.data_period_start - timedelta(days=1)\n            self.intital_time_segment = self.input_data_objects[SilverTimeSegmentsDataObject.ID].df.filter(\n                (F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(previous_date))\n                &amp; (F.col(ColNames.is_last) == True)\n            )\n\n            # this is needed to join the last time segement with the events of the current date\n            self.intital_time_segment = self.intital_time_segment.withColumn(\n                ColNames.start_timestamp, F.date_add(F.col(ColNames.start_timestamp), 1)\n            ).withColumns(\n                {\n                    ColNames.year: F.year(ColNames.start_timestamp).cast(\"smallint\"),\n                    ColNames.month: F.month(ColNames.start_timestamp).cast(\"tinyint\"),\n                    ColNames.day: F.dayofmonth(ColNames.start_timestamp).cast(\"tinyint\"),\n                }\n            )\n\n            self.intital_time_segment = self.intital_time_segment.withColumn(\n                ColNames.user_id, F.hex(F.col(ColNames.user_id))\n            )\n\n        # for every date in the data period, get the events and the intersection groups\n        # for that date and calculate the time segments\n        for current_date in self.data_period_dates:\n\n            self.logger.info(f\"Processing events for {current_date.strftime('%Y-%m-%d')}\")\n\n            self.current_date = current_date\n            self.current_input_events_sdf = self.input_data_objects[SilverEventFlaggedDataObject.ID].df.filter(\n                (F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(current_date))\n                &amp; (F.col(ColNames.error_flag).isin(self.event_error_flags_to_include))\n            )\n\n            self.current_interesection_groups_sdf = (\n                self.input_data_objects[SilverCellIntersectionGroupsDataObject.ID]\n                .df.filter(\n                    (\n                        F.make_date(\n                            F.col(ColNames.year),\n                            F.col(ColNames.month),\n                            F.col(ColNames.day),\n                        )\n                        == F.lit(current_date)\n                    )\n                )\n                .select(ColNames.cells)\n            )\n\n            self.transform()\n            self.write()\n            self.current_segments_sdf.unpersist()\n\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n\n        current_events = self.current_input_events_sdf\n        last_time_segment = self.intital_time_segment\n\n        # TODO: This conversion is needed for Pandas serialisation/deserialisation,\n        # to remove it when user_id will be stored as string, not as binary\n        current_events = current_events.withColumn(ColNames.user_id, F.hex(F.col(ColNames.user_id)))\n\n        # Conversion to string is needed for easier intersection groups lookup in the aggregation function\n        groups_sdf = self.current_interesection_groups_sdf.withColumn(\n            ColNames.cells, F.concat_ws(\",\", F.col(ColNames.cells))\n        )\n\n        # Initialize an empty set\n        intersections_set = set()\n\n        # Iterate over each Row object and add the 'cells' value to the set\n        for row in groups_sdf.collect():\n            intersections_set.add(row.cells)\n\n        # Broadcast the intersection groups to all the workers\n        # TODO: To test this approach with large datasets, might not be feasible\n        intersections_set = self.spark.sparkContext.broadcast(intersections_set)\n\n        # Partial function to pass the current date and other parameters to the aggregation function\n        aggregate_stays_partial = partial(\n            self.aggregate_stays,\n            current_date=self.current_date,\n            min_time_stay=self.min_time_stay,\n            max_time_missing_stay=self.max_time_missing_stay,\n            max_time_missing_move=self.max_time_missing_move,\n            pad_time=self.pad_time,\n            intersections_set=intersections_set,\n        )\n\n        groupby_cols = self.input_data_objects[SilverEventFlaggedDataObject.ID].partition_columns + [ColNames.user_id]\n\n        # Using cogroup to join the current events with the last time segment.\n        # Handy to avoid joining last segments to every row of the current events\n        # Also helps to detect missing events for the user for the last day or for the current day\n        # TODO: To test this approach with large datasets, might not be feasible\n        current_segments_sdf = (\n            current_events.groupby(*groupby_cols)\n            .cogroup(last_time_segment.groupby(*groupby_cols))\n            .applyInPandas(aggregate_stays_partial, self.segmentation_return_schema)\n        )\n\n        current_segments_sdf = current_segments_sdf.withColumns(\n            {\n                ColNames.year: F.year(ColNames.start_timestamp).cast(\"smallint\"),\n                ColNames.month: F.month(ColNames.start_timestamp).cast(\"tinyint\"),\n                ColNames.day: F.dayofmonth(ColNames.start_timestamp).cast(\"tinyint\"),\n            }\n        )\n\n        self.current_segments_sdf = current_segments_sdf.cache()\n        self.current_segments_sdf.count()\n\n        # Need to keep last segment for the next date iteration\n        # Have to add one day to time columns to be able to cogroup with the next day\n        last_segments = current_segments_sdf.filter(F.col(ColNames.is_last) == True)\n        last_segments = last_segments.withColumn(\n            ColNames.start_timestamp, F.date_add(F.col(ColNames.start_timestamp), 1)\n        )\n\n        last_segments = last_segments.withColumns(\n            {\n                ColNames.year: F.year(ColNames.start_timestamp).cast(\"smallint\"),\n                ColNames.month: F.month(ColNames.start_timestamp).cast(\"tinyint\"),\n                ColNames.day: F.dayofmonth(ColNames.start_timestamp).cast(\"tinyint\"),\n            }\n        )\n        self.intital_time_segment.unpersist()\n        self.intital_time_segment = last_segments.cache()\n        last_segments.count()\n\n        # TODO: This conversion is needed to get back to binary after Pandas serialisation/deserialisation,\n        # to remove it when user_id will be stored as string, not as binary\n        current_segments_sdf = current_segments_sdf.withColumn(ColNames.user_id, F.unhex(F.col(ColNames.user_id)))\n\n        current_segments_sdf = current_segments_sdf.select(\n            *[field.name for field in SilverTimeSegmentsDataObject.SCHEMA.fields]\n        )\n        columns = {\n            field.name: F.col(field.name).cast(field.dataType) for field in SilverTimeSegmentsDataObject.SCHEMA.fields\n        }\n        current_segments_sdf = current_segments_sdf.withColumns(columns)\n\n        current_segments_sdf = current_segments_sdf.repartition(\n            ColNames.year, ColNames.month, ColNames.day, ColNames.user_id_modulo\n        ).sortWithinPartitions(ColNames.user_id, ColNames.start_timestamp)\n\n        self.output_data_objects[SilverTimeSegmentsDataObject.ID].df = current_segments_sdf\n\n    @staticmethod\n    def aggregate_stays(\n        pdf: pdDataFrame,\n        last_segments_pdf: pdDataFrame,\n        current_date: date,\n        min_time_stay: timedelta,\n        max_time_missing_stay: timedelta,\n        max_time_missing_move: timedelta,\n        pad_time: timedelta,\n        intersections_set: set,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Aggregates events into Time Segments for a given user.\n\n        This method processes a Pandas DataFrame of user events, and aggregates them into time segments based on\n        certain conditions. It handles the first event separately, then iterates over the remaining events. For each\n        event, it checks if there's an intersection of an event cell and the current time segment cells and if the time\n        gap is within anacceptable range. Depending on the state of the current time segment and the result of\n        the intersection check, it either updates the current time segment or creates a new one.\n\n        Input user event data is expected to be sorted by timestamp.\n\n        Parameters:\n        pdf (pdDataFrame): The input Pandas DataFrame containing user events to be processed.\n        last_segments_pdf (pdDataFrame): A Pandas DataFrame containing the last segments.\n        current_date (date): The current date.\n        min_time_stay (timedelta): The minimum time to consider a segment as a 'stay'.\n        max_time_missing_stay (timedelta): The maximum time gap to consider continuation of a 'stay'.\n        max_time_missing_move (timedelta): The maximum time gap to consider continuation of a 'move'.\n        pad_time (timedelta): The padding time to have between 'unknown' segment.\n        groups_sdf (DataFrame): A PySpark DataFrame containing the groups.\n\n        Returns:\n        DataFrame: A DataFrame containing the aggregated time segments.\n        \"\"\"\n        segments = []\n        is_first_ts = True\n\n        current_date_start = datetime.combine(current_date, time())\n        current_date_end = datetime.combine(current_date, time(23, 59, 59))\n        previous_date_start = current_date_start - timedelta(days=1)\n        previous_date_end = current_date_end - timedelta(days=1)\n\n        # pdf = pdf.sort_values(by=[ColNames.timestamp])\n\n        # Depending on the presence of events and last segments, initialize the user info and the last time segment\n        # If there are no events, but last segment exists use the last time segment to derrive the user info\n        # and create 'unknown' segment for current date\n        # If there are events, use the first event to derrive the user info\n        # if there are no last segments, assume last segment as 'unknown' for the whole previous date\n        # if there are last segments, use the last segment\n        user_id, user_mod, mcc, current_ts = ContinuousTimeSegmentation.initialize_user_and_ts(\n            pdf,\n            last_segments_pdf,\n            current_date_start,\n            current_date_end,\n            previous_date_start,\n            previous_date_end,\n        )\n        # We process events only if there are any\n        if not pdf.empty:\n            for event in pdf.itertuples(index=False):\n                next_ts = {}\n                ts_to_add = []\n                event_timestamp = event.timestamp\n                event_cell = event.cell_id\n\n                # For the first time segment to start, look at the previous day's last segment\n                # and create a new time segment with the same state starting from the day start till the first event\n                if is_first_ts:\n                    next_ts = ContinuousTimeSegmentation.handle_first_segment(\n                        current_ts,\n                        event_timestamp,\n                        current_date_start,\n                        max_time_missing_stay,\n                        max_time_missing_move,\n                        pad_time,\n                    )\n                    current_ts = next_ts\n                    current_ts[ColNames.time_segment_id] = 1\n                    is_first_ts = False\n\n                current_intersection = list(set(current_ts[ColNames.cells] + [event_cell]))\n                is_intersected = ContinuousTimeSegmentation.check_intersection(\n                    current_ts[ColNames.cells], current_intersection, intersections_set\n                )\n\n                if current_ts[ColNames.state] == \"unknown\":\n                    # If the current state is 'unknown' (from the previous day),\n                    # create a new 'undetermined' time segment with pad_time adjustment\n                    next_ts = ContinuousTimeSegmentation.create_time_segment(\n                        current_ts[ColNames.end_timestamp],\n                        event_timestamp,\n                        [event_cell],\n                        \"undetermined\",\n                        current_ts[ColNames.time_segment_id],\n                    )\n                    ts_to_add = [current_ts]\n                    current_ts = next_ts\n\n                elif is_intersected and event_timestamp - current_ts[ColNames.end_timestamp] &lt;= max_time_missing_stay:\n                    # If there's an intersection, check if we should update the current_ts or create a new one\n                    if current_ts[ColNames.state] in [\"undetermined\", \"stay\"]:\n                        # If the current state is 'undetermined' or 'stay' and the time gap is within\n                        # the acceptable range for a stay update the current time segment with the new cell\n                        # and the new end timestamp and set state to stay\n                        current_ts[ColNames.end_timestamp] = event_timestamp\n                        current_ts[ColNames.cells] = current_intersection\n                        if event_timestamp - current_ts[ColNames.start_timestamp] &gt; min_time_stay:\n                            current_ts[ColNames.state] = \"stay\"\n                    elif current_ts[ColNames.state] == \"move\":\n                        # If the current state is 'move' and the time gap is within the acceptable range\n                        # create new time segment with state 'undetermined' after move segment\n                        next_ts = ContinuousTimeSegmentation.create_time_segment(\n                            current_ts[ColNames.end_timestamp],\n                            event_timestamp,\n                            [event_cell],\n                            \"undetermined\",\n                            current_ts[ColNames.time_segment_id],\n                        )\n                        # if time gap is big enough to assume that its stay change the state to stay\n                        if next_ts[ColNames.end_timestamp] - next_ts[ColNames.start_timestamp] &gt; min_time_stay:\n                            next_ts[ColNames.state] = \"stay\"\n                        ts_to_add = [current_ts]\n                        current_ts = next_ts\n\n                elif (\n                    not is_intersected and event_timestamp - current_ts[ColNames.end_timestamp] &lt;= max_time_missing_move\n                ):\n                    # If there's no intersection and the time gap is within the acceptable range for a move\n                    mid_point = (\n                        current_ts[ColNames.end_timestamp] + (event_timestamp - current_ts[ColNames.end_timestamp]) / 2\n                    )\n\n                    next_ts_1 = ContinuousTimeSegmentation.create_time_segment(\n                        current_ts[ColNames.end_timestamp],\n                        mid_point,\n                        current_ts[ColNames.cells],\n                        \"move\",\n                        current_ts[ColNames.time_segment_id],\n                    )\n\n                    next_ts_2 = ContinuousTimeSegmentation.create_time_segment(\n                        mid_point,\n                        event_timestamp,\n                        [event_cell],\n                        \"move\",\n                        next_ts_1[ColNames.time_segment_id],\n                    )\n\n                    ts_to_add = [current_ts, next_ts_1]\n                    current_ts = next_ts_2\n\n                else:\n                    # If the time gap is too big, create 'unknown' segment for missing time with pad_time adjustment\n                    # create new time segment with state 'undetermined' with pad_time adjustment\n                    current_ts[ColNames.end_timestamp] = current_ts[ColNames.end_timestamp] + pad_time\n\n                    next_ts_1 = ContinuousTimeSegmentation.create_time_segment(\n                        current_ts[ColNames.end_timestamp],\n                        event_timestamp - pad_time,\n                        [],\n                        \"unknown\",\n                        current_ts[ColNames.time_segment_id],\n                    )\n\n                    next_ts_2 = ContinuousTimeSegmentation.create_time_segment(\n                        event_timestamp - pad_time,\n                        event_timestamp,\n                        [event_cell],\n                        \"undetermined\",\n                        next_ts_1[ColNames.time_segment_id],\n                    )\n\n                    ts_to_add = [current_ts, next_ts_1]\n                    current_ts = next_ts_2\n\n                segments.extend(ts_to_add)\n\n        # TODO: NOT IMPLEMENTED.\n        # Currently there is no methodological description on how to handle the time from the last event to the end of the day.\n        # Create extra time segment that covers the duration from the last event-based time segment until the end of the date.\n        # if (current_ts[ColNames.end_timestamp] &lt; current_date_end):\n        #     last_ts = ContinuousTimeSegmentation.create_time_segment(\n        #                     current_ts[ColNames.end_timestamp],\n        #                     current_date_end,\n        #                     [],\n        #                     \"unknown\",\n        #                     current_ts[ColNames.time_segment_id],\n        #                 )\n        #     last_ts[ColNames.is_last] = True\n        #     segments.append(last_ts)\n        # else:\n        #     # If no extra time segment was generated, the final event-generated time segment is the last of the date.\n        #     current_ts[ColNames.is_last] = True\n\n        # Add final event-generated time segment to output list.\n        current_ts[ColNames.is_last] = True\n        segments.append(current_ts)\n\n        # Prepare return columns\n        segments_df = pd.DataFrame(segments)\n        segments_df[ColNames.user_id] = user_id\n        segments_df[ColNames.mcc] = mcc\n        segments_df[ColNames.user_id_modulo] = user_mod\n\n        return segments_df\n\n    @staticmethod\n    def handle_first_segment(\n        current_ts: Dict,\n        event_timestamp: datetime,\n        current_date_start: datetime,\n        max_time_missing_stay: timedelta,\n        max_time_missing_move: timedelta,\n        pad_time: timedelta,\n    ) -&gt; dict:\n        \"\"\"\n        Handles the first segment for a user for a date based on a previous date last segment.\n\n        This method takes the last time segment of previous date and the timestamp of the first\n            event in the current date.\n        It checks the state of the current time segment and the time difference between the end of the current\n        time segment and the first event in the next date.\n\n        If the state is 'undetermined' or 'stay' and the time difference is within the maximum missing stay time,\n        or if the state is 'move' and the time difference is within the maximum missing move time, it creates a new\n        time segment with the same cells, state. The start timestamp\n        of the new time segment is the start of the current date, and the end timestamp\n            is the timestamp of the first event.\n\n        If neither of these conditions are met, it creates a new time segment with\n            an empty list of cells, state 'unknown'.\n        The start timestamp of the new time segment is the start of the next date,\n            and the end timestamp is the timestamp of the first event minus the padding time.\n\n        Parameters:\n        current_ts (Dict): The last time segment from previous date.\n        event_timestamp (datetime): The timestamp of the first event in the current date.\n        current_date_start (datetime): The start of the current date.\n        max_time_missing_stay (timedelta): The maximum time gap to consider continuation of a 'stay'.\n        max_time_missing_move (timedelta): The maximum time gap to consider continuation of a 'move'.\n        pad_time (timedelta): The padding time to have between 'unknown' segment.\n\n        Returns:\n        dict: The new first time segment for the current date.\n        \"\"\"\n\n        if (\n            current_ts[ColNames.state] in [\"undetermined\", \"stay\"]\n            and event_timestamp - current_ts[ColNames.end_timestamp] &lt;= max_time_missing_stay\n        ):\n            next_ts = ContinuousTimeSegmentation.create_time_segment(\n                current_date_start,\n                event_timestamp,\n                list(current_ts[ColNames.cells]),\n                current_ts[ColNames.state],\n                current_ts[ColNames.time_segment_id],\n            )\n        elif (\n            current_ts[ColNames.state] == \"move\"\n            and event_timestamp - current_ts[ColNames.end_timestamp] &lt;= max_time_missing_move\n        ):\n            next_ts = ContinuousTimeSegmentation.create_time_segment(\n                current_date_start,\n                event_timestamp,\n                list(current_ts[ColNames.cells]),\n                current_ts[ColNames.state],\n                current_ts[ColNames.time_segment_id],\n            )\n        else:\n            pad_time = timedelta(seconds=0) if event_timestamp - current_date_start &lt; pad_time else pad_time\n            next_ts = ContinuousTimeSegmentation.create_time_segment(\n                current_date_start,\n                event_timestamp - pad_time,\n                [],\n                \"unknown\",\n                current_ts[ColNames.time_segment_id],\n            )\n\n        return next_ts\n\n    @staticmethod\n    def create_time_segment(\n        start_timestamp: datetime,\n        end_timestamp: datetime,\n        cells: List[str],\n        state: str,\n        previous_segment_id: Optional[int] = None,\n    ) -&gt; Dict:\n        \"\"\"\n        Creates a new time segment.\n\n        It creates a new time segment with these values, incrementing the segment ID by 1\n        if a previous segment ID is provided, or setting it to 1 if not.\n\n        Parameters:\n        start_timestamp (datetime): The start timestamp of the time segment.\n        end_timestamp (datetime): The end timestamp of the time segment.\n        cells (List[str]): The cells of the time segment.\n        state (str): The state of the time segment.\n        previous_segment_id (Optional[int]): The ID of the previous time segment, if any.\n\n        Returns:\n        Dict: The new time segment.\n        \"\"\"\n\n        previous_segment_id = previous_segment_id if previous_segment_id else 0\n\n        return {\n            ColNames.time_segment_id: previous_segment_id + 1,\n            ColNames.start_timestamp: start_timestamp,\n            ColNames.end_timestamp: end_timestamp,\n            ColNames.cells: cells,\n            ColNames.state: state,\n            ColNames.is_last: False,\n        }\n\n    @staticmethod\n    def initialize_user_and_ts(\n        pdf: pdDataFrame,\n        last_segments_pdf: pdDataFrame,\n        current_date_start: datetime,\n        current_date_end: datetime,\n        previous_date_start: datetime,\n        previous_date_end: datetime,\n    ) -&gt; Tuple[str, int, str, Dict]:\n        \"\"\"\n        Initializes the user ID, user ID modulo, MCC, and last time segment for time segmenation.\n\n        If the events DataFrame for the current date is empty, it uses the user ID, user ID modulo,\n            and MCC from the last segments DataFrame and creates a new 'unknown' time segment for the current date.\n\n        If the events DataFrame for the current date is not empty, it uses the user ID, user ID modulo,\n        and MCC from this DataFrame. If the last segments DataFrame is not empty, it uses the first\n        time segment from this DataFrame as the last time segment. If the last segments DataFrame is empty,\n        it creates a new 'unknown' time segment for the previous date.\n\n        Parameters:\n        pdf (pdDataFrame): The input Pandas DataFrame for the current date.\n        last_segments_pdf (pdDataFrame): A Pandas DataFrame containing the last segments.\n        current_date_start (datetime): The start of the current date.\n        current_date_end (datetime): The end of the current date.\n        previous_date_start (datetime): The start of the previous date.\n        previous_date_end (datetime): The end of the previous date.\n\n        Returns:\n        Tuple[str, int, str, Dict]: The user ID, user ID modulo, MCC, and last time segment.\n        \"\"\"\n\n        if pdf.empty:\n            user_id = last_segments_pdf[ColNames.user_id][0]\n            user_id_mod = last_segments_pdf[ColNames.user_id_modulo][0]\n            mcc = last_segments_pdf[ColNames.mcc][0]\n            last_time_segment = ContinuousTimeSegmentation.create_time_segment(\n                current_date_start, current_date_end, [], \"unknown\"\n            )\n        else:\n            user_id = pdf[ColNames.user_id][0]\n            user_id_mod = pdf[ColNames.user_id_modulo][0]\n            mcc = pdf[ColNames.mcc][0]\n            if not last_segments_pdf.empty:\n                last_time_segment = last_segments_pdf.iloc[0][\n                    [\n                        ColNames.time_segment_id,\n                        ColNames.start_timestamp,\n                        ColNames.end_timestamp,\n                        ColNames.cells,\n                        ColNames.state,\n                    ]\n                ].to_dict()\n            else:\n                last_time_segment = ContinuousTimeSegmentation.create_time_segment(\n                    previous_date_start, previous_date_end, [], \"unknown\"\n                )\n\n        return user_id, user_id_mod, mcc, last_time_segment\n\n    @staticmethod\n    def check_intersection(\n        previous_ts_inersection: List[str],\n        current_intersection: List[str],\n        intersections_set: set,\n    ) -&gt; bool:\n        \"\"\"\n        Checks if there is an intersection between the current and previous time segments.\n\n        This method takes two lists of cells, one for the previous time segment and one for the current time segment,\n        and a Pandas DataFrame of intersections.\n\n        If the list for the previous time segment is empty, it returns False, indicating that there is no intersection.\n\n        If the list for the previous time segment is not empty, it checks if the sorted, comma-separated string of\n        cells for the current time segment is in the DataFrame of intersections. If the list for the current time\n        segment has more than one cell, it returns the result of this check. If the list for the current time segment\n        has one cell, it returns True, indicating that there is an intersection.\n\n        Parameters:\n        previous_ts_inersection (List[str]): The cells of the previous time segment.\n        current_intersection (List[str]): The cells of the current time segment.\n        intersection_pd_df (pdDataFrame): A Pandas DataFrame containing the intersections.\n\n        Returns:\n        bool: True if there is an intersection, False otherwise.\n        \"\"\"\n        if len(previous_ts_inersection) == 0:\n            is_intersected = False\n        else:\n            is_intersected = (\n                \",\".join(sorted(current_intersection)) in intersections_set.value\n                if len(current_intersection) &gt; 1\n                else True\n            )\n        return is_intersected\n</code></pre>"},{"location":"reference/components/execution/time_segments/continuous_time_segmentation/#components.execution.time_segments.continuous_time_segmentation.ContinuousTimeSegmentation.aggregate_stays","title":"<code>aggregate_stays(pdf, last_segments_pdf, current_date, min_time_stay, max_time_missing_stay, max_time_missing_move, pad_time, intersections_set)</code>  <code>staticmethod</code>","text":"<p>Aggregates events into Time Segments for a given user.</p> <p>This method processes a Pandas DataFrame of user events, and aggregates them into time segments based on certain conditions. It handles the first event separately, then iterates over the remaining events. For each event, it checks if there's an intersection of an event cell and the current time segment cells and if the time gap is within anacceptable range. Depending on the state of the current time segment and the result of the intersection check, it either updates the current time segment or creates a new one.</p> <p>Input user event data is expected to be sorted by timestamp.</p> <p>Parameters: pdf (pdDataFrame): The input Pandas DataFrame containing user events to be processed. last_segments_pdf (pdDataFrame): A Pandas DataFrame containing the last segments. current_date (date): The current date. min_time_stay (timedelta): The minimum time to consider a segment as a 'stay'. max_time_missing_stay (timedelta): The maximum time gap to consider continuation of a 'stay'. max_time_missing_move (timedelta): The maximum time gap to consider continuation of a 'move'. pad_time (timedelta): The padding time to have between 'unknown' segment. groups_sdf (DataFrame): A PySpark DataFrame containing the groups.</p> <p>Returns: DataFrame: A DataFrame containing the aggregated time segments.</p> Source code in <code>multimno/components/execution/time_segments/continuous_time_segmentation.py</code> <pre><code>@staticmethod\ndef aggregate_stays(\n    pdf: pdDataFrame,\n    last_segments_pdf: pdDataFrame,\n    current_date: date,\n    min_time_stay: timedelta,\n    max_time_missing_stay: timedelta,\n    max_time_missing_move: timedelta,\n    pad_time: timedelta,\n    intersections_set: set,\n) -&gt; DataFrame:\n    \"\"\"\n    Aggregates events into Time Segments for a given user.\n\n    This method processes a Pandas DataFrame of user events, and aggregates them into time segments based on\n    certain conditions. It handles the first event separately, then iterates over the remaining events. For each\n    event, it checks if there's an intersection of an event cell and the current time segment cells and if the time\n    gap is within anacceptable range. Depending on the state of the current time segment and the result of\n    the intersection check, it either updates the current time segment or creates a new one.\n\n    Input user event data is expected to be sorted by timestamp.\n\n    Parameters:\n    pdf (pdDataFrame): The input Pandas DataFrame containing user events to be processed.\n    last_segments_pdf (pdDataFrame): A Pandas DataFrame containing the last segments.\n    current_date (date): The current date.\n    min_time_stay (timedelta): The minimum time to consider a segment as a 'stay'.\n    max_time_missing_stay (timedelta): The maximum time gap to consider continuation of a 'stay'.\n    max_time_missing_move (timedelta): The maximum time gap to consider continuation of a 'move'.\n    pad_time (timedelta): The padding time to have between 'unknown' segment.\n    groups_sdf (DataFrame): A PySpark DataFrame containing the groups.\n\n    Returns:\n    DataFrame: A DataFrame containing the aggregated time segments.\n    \"\"\"\n    segments = []\n    is_first_ts = True\n\n    current_date_start = datetime.combine(current_date, time())\n    current_date_end = datetime.combine(current_date, time(23, 59, 59))\n    previous_date_start = current_date_start - timedelta(days=1)\n    previous_date_end = current_date_end - timedelta(days=1)\n\n    # pdf = pdf.sort_values(by=[ColNames.timestamp])\n\n    # Depending on the presence of events and last segments, initialize the user info and the last time segment\n    # If there are no events, but last segment exists use the last time segment to derrive the user info\n    # and create 'unknown' segment for current date\n    # If there are events, use the first event to derrive the user info\n    # if there are no last segments, assume last segment as 'unknown' for the whole previous date\n    # if there are last segments, use the last segment\n    user_id, user_mod, mcc, current_ts = ContinuousTimeSegmentation.initialize_user_and_ts(\n        pdf,\n        last_segments_pdf,\n        current_date_start,\n        current_date_end,\n        previous_date_start,\n        previous_date_end,\n    )\n    # We process events only if there are any\n    if not pdf.empty:\n        for event in pdf.itertuples(index=False):\n            next_ts = {}\n            ts_to_add = []\n            event_timestamp = event.timestamp\n            event_cell = event.cell_id\n\n            # For the first time segment to start, look at the previous day's last segment\n            # and create a new time segment with the same state starting from the day start till the first event\n            if is_first_ts:\n                next_ts = ContinuousTimeSegmentation.handle_first_segment(\n                    current_ts,\n                    event_timestamp,\n                    current_date_start,\n                    max_time_missing_stay,\n                    max_time_missing_move,\n                    pad_time,\n                )\n                current_ts = next_ts\n                current_ts[ColNames.time_segment_id] = 1\n                is_first_ts = False\n\n            current_intersection = list(set(current_ts[ColNames.cells] + [event_cell]))\n            is_intersected = ContinuousTimeSegmentation.check_intersection(\n                current_ts[ColNames.cells], current_intersection, intersections_set\n            )\n\n            if current_ts[ColNames.state] == \"unknown\":\n                # If the current state is 'unknown' (from the previous day),\n                # create a new 'undetermined' time segment with pad_time adjustment\n                next_ts = ContinuousTimeSegmentation.create_time_segment(\n                    current_ts[ColNames.end_timestamp],\n                    event_timestamp,\n                    [event_cell],\n                    \"undetermined\",\n                    current_ts[ColNames.time_segment_id],\n                )\n                ts_to_add = [current_ts]\n                current_ts = next_ts\n\n            elif is_intersected and event_timestamp - current_ts[ColNames.end_timestamp] &lt;= max_time_missing_stay:\n                # If there's an intersection, check if we should update the current_ts or create a new one\n                if current_ts[ColNames.state] in [\"undetermined\", \"stay\"]:\n                    # If the current state is 'undetermined' or 'stay' and the time gap is within\n                    # the acceptable range for a stay update the current time segment with the new cell\n                    # and the new end timestamp and set state to stay\n                    current_ts[ColNames.end_timestamp] = event_timestamp\n                    current_ts[ColNames.cells] = current_intersection\n                    if event_timestamp - current_ts[ColNames.start_timestamp] &gt; min_time_stay:\n                        current_ts[ColNames.state] = \"stay\"\n                elif current_ts[ColNames.state] == \"move\":\n                    # If the current state is 'move' and the time gap is within the acceptable range\n                    # create new time segment with state 'undetermined' after move segment\n                    next_ts = ContinuousTimeSegmentation.create_time_segment(\n                        current_ts[ColNames.end_timestamp],\n                        event_timestamp,\n                        [event_cell],\n                        \"undetermined\",\n                        current_ts[ColNames.time_segment_id],\n                    )\n                    # if time gap is big enough to assume that its stay change the state to stay\n                    if next_ts[ColNames.end_timestamp] - next_ts[ColNames.start_timestamp] &gt; min_time_stay:\n                        next_ts[ColNames.state] = \"stay\"\n                    ts_to_add = [current_ts]\n                    current_ts = next_ts\n\n            elif (\n                not is_intersected and event_timestamp - current_ts[ColNames.end_timestamp] &lt;= max_time_missing_move\n            ):\n                # If there's no intersection and the time gap is within the acceptable range for a move\n                mid_point = (\n                    current_ts[ColNames.end_timestamp] + (event_timestamp - current_ts[ColNames.end_timestamp]) / 2\n                )\n\n                next_ts_1 = ContinuousTimeSegmentation.create_time_segment(\n                    current_ts[ColNames.end_timestamp],\n                    mid_point,\n                    current_ts[ColNames.cells],\n                    \"move\",\n                    current_ts[ColNames.time_segment_id],\n                )\n\n                next_ts_2 = ContinuousTimeSegmentation.create_time_segment(\n                    mid_point,\n                    event_timestamp,\n                    [event_cell],\n                    \"move\",\n                    next_ts_1[ColNames.time_segment_id],\n                )\n\n                ts_to_add = [current_ts, next_ts_1]\n                current_ts = next_ts_2\n\n            else:\n                # If the time gap is too big, create 'unknown' segment for missing time with pad_time adjustment\n                # create new time segment with state 'undetermined' with pad_time adjustment\n                current_ts[ColNames.end_timestamp] = current_ts[ColNames.end_timestamp] + pad_time\n\n                next_ts_1 = ContinuousTimeSegmentation.create_time_segment(\n                    current_ts[ColNames.end_timestamp],\n                    event_timestamp - pad_time,\n                    [],\n                    \"unknown\",\n                    current_ts[ColNames.time_segment_id],\n                )\n\n                next_ts_2 = ContinuousTimeSegmentation.create_time_segment(\n                    event_timestamp - pad_time,\n                    event_timestamp,\n                    [event_cell],\n                    \"undetermined\",\n                    next_ts_1[ColNames.time_segment_id],\n                )\n\n                ts_to_add = [current_ts, next_ts_1]\n                current_ts = next_ts_2\n\n            segments.extend(ts_to_add)\n\n    # TODO: NOT IMPLEMENTED.\n    # Currently there is no methodological description on how to handle the time from the last event to the end of the day.\n    # Create extra time segment that covers the duration from the last event-based time segment until the end of the date.\n    # if (current_ts[ColNames.end_timestamp] &lt; current_date_end):\n    #     last_ts = ContinuousTimeSegmentation.create_time_segment(\n    #                     current_ts[ColNames.end_timestamp],\n    #                     current_date_end,\n    #                     [],\n    #                     \"unknown\",\n    #                     current_ts[ColNames.time_segment_id],\n    #                 )\n    #     last_ts[ColNames.is_last] = True\n    #     segments.append(last_ts)\n    # else:\n    #     # If no extra time segment was generated, the final event-generated time segment is the last of the date.\n    #     current_ts[ColNames.is_last] = True\n\n    # Add final event-generated time segment to output list.\n    current_ts[ColNames.is_last] = True\n    segments.append(current_ts)\n\n    # Prepare return columns\n    segments_df = pd.DataFrame(segments)\n    segments_df[ColNames.user_id] = user_id\n    segments_df[ColNames.mcc] = mcc\n    segments_df[ColNames.user_id_modulo] = user_mod\n\n    return segments_df\n</code></pre>"},{"location":"reference/components/execution/time_segments/continuous_time_segmentation/#components.execution.time_segments.continuous_time_segmentation.ContinuousTimeSegmentation.check_intersection","title":"<code>check_intersection(previous_ts_inersection, current_intersection, intersections_set)</code>  <code>staticmethod</code>","text":"<p>Checks if there is an intersection between the current and previous time segments.</p> <p>This method takes two lists of cells, one for the previous time segment and one for the current time segment, and a Pandas DataFrame of intersections.</p> <p>If the list for the previous time segment is empty, it returns False, indicating that there is no intersection.</p> <p>If the list for the previous time segment is not empty, it checks if the sorted, comma-separated string of cells for the current time segment is in the DataFrame of intersections. If the list for the current time segment has more than one cell, it returns the result of this check. If the list for the current time segment has one cell, it returns True, indicating that there is an intersection.</p> <p>Parameters: previous_ts_inersection (List[str]): The cells of the previous time segment. current_intersection (List[str]): The cells of the current time segment. intersection_pd_df (pdDataFrame): A Pandas DataFrame containing the intersections.</p> <p>Returns: bool: True if there is an intersection, False otherwise.</p> Source code in <code>multimno/components/execution/time_segments/continuous_time_segmentation.py</code> <pre><code>@staticmethod\ndef check_intersection(\n    previous_ts_inersection: List[str],\n    current_intersection: List[str],\n    intersections_set: set,\n) -&gt; bool:\n    \"\"\"\n    Checks if there is an intersection between the current and previous time segments.\n\n    This method takes two lists of cells, one for the previous time segment and one for the current time segment,\n    and a Pandas DataFrame of intersections.\n\n    If the list for the previous time segment is empty, it returns False, indicating that there is no intersection.\n\n    If the list for the previous time segment is not empty, it checks if the sorted, comma-separated string of\n    cells for the current time segment is in the DataFrame of intersections. If the list for the current time\n    segment has more than one cell, it returns the result of this check. If the list for the current time segment\n    has one cell, it returns True, indicating that there is an intersection.\n\n    Parameters:\n    previous_ts_inersection (List[str]): The cells of the previous time segment.\n    current_intersection (List[str]): The cells of the current time segment.\n    intersection_pd_df (pdDataFrame): A Pandas DataFrame containing the intersections.\n\n    Returns:\n    bool: True if there is an intersection, False otherwise.\n    \"\"\"\n    if len(previous_ts_inersection) == 0:\n        is_intersected = False\n    else:\n        is_intersected = (\n            \",\".join(sorted(current_intersection)) in intersections_set.value\n            if len(current_intersection) &gt; 1\n            else True\n        )\n    return is_intersected\n</code></pre>"},{"location":"reference/components/execution/time_segments/continuous_time_segmentation/#components.execution.time_segments.continuous_time_segmentation.ContinuousTimeSegmentation.create_time_segment","title":"<code>create_time_segment(start_timestamp, end_timestamp, cells, state, previous_segment_id=None)</code>  <code>staticmethod</code>","text":"<p>Creates a new time segment.</p> <p>It creates a new time segment with these values, incrementing the segment ID by 1 if a previous segment ID is provided, or setting it to 1 if not.</p> <p>Parameters: start_timestamp (datetime): The start timestamp of the time segment. end_timestamp (datetime): The end timestamp of the time segment. cells (List[str]): The cells of the time segment. state (str): The state of the time segment. previous_segment_id (Optional[int]): The ID of the previous time segment, if any.</p> <p>Returns: Dict: The new time segment.</p> Source code in <code>multimno/components/execution/time_segments/continuous_time_segmentation.py</code> <pre><code>@staticmethod\ndef create_time_segment(\n    start_timestamp: datetime,\n    end_timestamp: datetime,\n    cells: List[str],\n    state: str,\n    previous_segment_id: Optional[int] = None,\n) -&gt; Dict:\n    \"\"\"\n    Creates a new time segment.\n\n    It creates a new time segment with these values, incrementing the segment ID by 1\n    if a previous segment ID is provided, or setting it to 1 if not.\n\n    Parameters:\n    start_timestamp (datetime): The start timestamp of the time segment.\n    end_timestamp (datetime): The end timestamp of the time segment.\n    cells (List[str]): The cells of the time segment.\n    state (str): The state of the time segment.\n    previous_segment_id (Optional[int]): The ID of the previous time segment, if any.\n\n    Returns:\n    Dict: The new time segment.\n    \"\"\"\n\n    previous_segment_id = previous_segment_id if previous_segment_id else 0\n\n    return {\n        ColNames.time_segment_id: previous_segment_id + 1,\n        ColNames.start_timestamp: start_timestamp,\n        ColNames.end_timestamp: end_timestamp,\n        ColNames.cells: cells,\n        ColNames.state: state,\n        ColNames.is_last: False,\n    }\n</code></pre>"},{"location":"reference/components/execution/time_segments/continuous_time_segmentation/#components.execution.time_segments.continuous_time_segmentation.ContinuousTimeSegmentation.handle_first_segment","title":"<code>handle_first_segment(current_ts, event_timestamp, current_date_start, max_time_missing_stay, max_time_missing_move, pad_time)</code>  <code>staticmethod</code>","text":"<p>Handles the first segment for a user for a date based on a previous date last segment.</p> <p>This method takes the last time segment of previous date and the timestamp of the first     event in the current date. It checks the state of the current time segment and the time difference between the end of the current time segment and the first event in the next date.</p> <p>If the state is 'undetermined' or 'stay' and the time difference is within the maximum missing stay time, or if the state is 'move' and the time difference is within the maximum missing move time, it creates a new time segment with the same cells, state. The start timestamp of the new time segment is the start of the current date, and the end timestamp     is the timestamp of the first event.</p> <p>If neither of these conditions are met, it creates a new time segment with     an empty list of cells, state 'unknown'. The start timestamp of the new time segment is the start of the next date,     and the end timestamp is the timestamp of the first event minus the padding time.</p> <p>Parameters: current_ts (Dict): The last time segment from previous date. event_timestamp (datetime): The timestamp of the first event in the current date. current_date_start (datetime): The start of the current date. max_time_missing_stay (timedelta): The maximum time gap to consider continuation of a 'stay'. max_time_missing_move (timedelta): The maximum time gap to consider continuation of a 'move'. pad_time (timedelta): The padding time to have between 'unknown' segment.</p> <p>Returns: dict: The new first time segment for the current date.</p> Source code in <code>multimno/components/execution/time_segments/continuous_time_segmentation.py</code> <pre><code>@staticmethod\ndef handle_first_segment(\n    current_ts: Dict,\n    event_timestamp: datetime,\n    current_date_start: datetime,\n    max_time_missing_stay: timedelta,\n    max_time_missing_move: timedelta,\n    pad_time: timedelta,\n) -&gt; dict:\n    \"\"\"\n    Handles the first segment for a user for a date based on a previous date last segment.\n\n    This method takes the last time segment of previous date and the timestamp of the first\n        event in the current date.\n    It checks the state of the current time segment and the time difference between the end of the current\n    time segment and the first event in the next date.\n\n    If the state is 'undetermined' or 'stay' and the time difference is within the maximum missing stay time,\n    or if the state is 'move' and the time difference is within the maximum missing move time, it creates a new\n    time segment with the same cells, state. The start timestamp\n    of the new time segment is the start of the current date, and the end timestamp\n        is the timestamp of the first event.\n\n    If neither of these conditions are met, it creates a new time segment with\n        an empty list of cells, state 'unknown'.\n    The start timestamp of the new time segment is the start of the next date,\n        and the end timestamp is the timestamp of the first event minus the padding time.\n\n    Parameters:\n    current_ts (Dict): The last time segment from previous date.\n    event_timestamp (datetime): The timestamp of the first event in the current date.\n    current_date_start (datetime): The start of the current date.\n    max_time_missing_stay (timedelta): The maximum time gap to consider continuation of a 'stay'.\n    max_time_missing_move (timedelta): The maximum time gap to consider continuation of a 'move'.\n    pad_time (timedelta): The padding time to have between 'unknown' segment.\n\n    Returns:\n    dict: The new first time segment for the current date.\n    \"\"\"\n\n    if (\n        current_ts[ColNames.state] in [\"undetermined\", \"stay\"]\n        and event_timestamp - current_ts[ColNames.end_timestamp] &lt;= max_time_missing_stay\n    ):\n        next_ts = ContinuousTimeSegmentation.create_time_segment(\n            current_date_start,\n            event_timestamp,\n            list(current_ts[ColNames.cells]),\n            current_ts[ColNames.state],\n            current_ts[ColNames.time_segment_id],\n        )\n    elif (\n        current_ts[ColNames.state] == \"move\"\n        and event_timestamp - current_ts[ColNames.end_timestamp] &lt;= max_time_missing_move\n    ):\n        next_ts = ContinuousTimeSegmentation.create_time_segment(\n            current_date_start,\n            event_timestamp,\n            list(current_ts[ColNames.cells]),\n            current_ts[ColNames.state],\n            current_ts[ColNames.time_segment_id],\n        )\n    else:\n        pad_time = timedelta(seconds=0) if event_timestamp - current_date_start &lt; pad_time else pad_time\n        next_ts = ContinuousTimeSegmentation.create_time_segment(\n            current_date_start,\n            event_timestamp - pad_time,\n            [],\n            \"unknown\",\n            current_ts[ColNames.time_segment_id],\n        )\n\n    return next_ts\n</code></pre>"},{"location":"reference/components/execution/time_segments/continuous_time_segmentation/#components.execution.time_segments.continuous_time_segmentation.ContinuousTimeSegmentation.initialize_user_and_ts","title":"<code>initialize_user_and_ts(pdf, last_segments_pdf, current_date_start, current_date_end, previous_date_start, previous_date_end)</code>  <code>staticmethod</code>","text":"<p>Initializes the user ID, user ID modulo, MCC, and last time segment for time segmenation.</p> <p>If the events DataFrame for the current date is empty, it uses the user ID, user ID modulo,     and MCC from the last segments DataFrame and creates a new 'unknown' time segment for the current date.</p> <p>If the events DataFrame for the current date is not empty, it uses the user ID, user ID modulo, and MCC from this DataFrame. If the last segments DataFrame is not empty, it uses the first time segment from this DataFrame as the last time segment. If the last segments DataFrame is empty, it creates a new 'unknown' time segment for the previous date.</p> <p>Parameters: pdf (pdDataFrame): The input Pandas DataFrame for the current date. last_segments_pdf (pdDataFrame): A Pandas DataFrame containing the last segments. current_date_start (datetime): The start of the current date. current_date_end (datetime): The end of the current date. previous_date_start (datetime): The start of the previous date. previous_date_end (datetime): The end of the previous date.</p> <p>Returns: Tuple[str, int, str, Dict]: The user ID, user ID modulo, MCC, and last time segment.</p> Source code in <code>multimno/components/execution/time_segments/continuous_time_segmentation.py</code> <pre><code>@staticmethod\ndef initialize_user_and_ts(\n    pdf: pdDataFrame,\n    last_segments_pdf: pdDataFrame,\n    current_date_start: datetime,\n    current_date_end: datetime,\n    previous_date_start: datetime,\n    previous_date_end: datetime,\n) -&gt; Tuple[str, int, str, Dict]:\n    \"\"\"\n    Initializes the user ID, user ID modulo, MCC, and last time segment for time segmenation.\n\n    If the events DataFrame for the current date is empty, it uses the user ID, user ID modulo,\n        and MCC from the last segments DataFrame and creates a new 'unknown' time segment for the current date.\n\n    If the events DataFrame for the current date is not empty, it uses the user ID, user ID modulo,\n    and MCC from this DataFrame. If the last segments DataFrame is not empty, it uses the first\n    time segment from this DataFrame as the last time segment. If the last segments DataFrame is empty,\n    it creates a new 'unknown' time segment for the previous date.\n\n    Parameters:\n    pdf (pdDataFrame): The input Pandas DataFrame for the current date.\n    last_segments_pdf (pdDataFrame): A Pandas DataFrame containing the last segments.\n    current_date_start (datetime): The start of the current date.\n    current_date_end (datetime): The end of the current date.\n    previous_date_start (datetime): The start of the previous date.\n    previous_date_end (datetime): The end of the previous date.\n\n    Returns:\n    Tuple[str, int, str, Dict]: The user ID, user ID modulo, MCC, and last time segment.\n    \"\"\"\n\n    if pdf.empty:\n        user_id = last_segments_pdf[ColNames.user_id][0]\n        user_id_mod = last_segments_pdf[ColNames.user_id_modulo][0]\n        mcc = last_segments_pdf[ColNames.mcc][0]\n        last_time_segment = ContinuousTimeSegmentation.create_time_segment(\n            current_date_start, current_date_end, [], \"unknown\"\n        )\n    else:\n        user_id = pdf[ColNames.user_id][0]\n        user_id_mod = pdf[ColNames.user_id_modulo][0]\n        mcc = pdf[ColNames.mcc][0]\n        if not last_segments_pdf.empty:\n            last_time_segment = last_segments_pdf.iloc[0][\n                [\n                    ColNames.time_segment_id,\n                    ColNames.start_timestamp,\n                    ColNames.end_timestamp,\n                    ColNames.cells,\n                    ColNames.state,\n                ]\n            ].to_dict()\n        else:\n            last_time_segment = ContinuousTimeSegmentation.create_time_segment(\n                previous_date_start, previous_date_end, [], \"unknown\"\n            )\n\n    return user_id, user_id_mod, mcc, last_time_segment\n</code></pre>"},{"location":"reference/components/execution/usual_environment_aggregation/","title":"usual_environment_aggregation","text":""},{"location":"reference/components/execution/usual_environment_aggregation/usual_environment_aggregation/","title":"usual_environment_aggregation","text":"<p>This module is responsible for usual environment and location labels aggregation to reference grid</p>"},{"location":"reference/components/execution/usual_environment_aggregation/usual_environment_aggregation/#components.execution.usual_environment_aggregation.usual_environment_aggregation.UsualEnvironmentAggregation","title":"<code>UsualEnvironmentAggregation</code>","text":"<p>               Bases: <code>Component</code></p> <p>A class to aggregate devices usual environment and location labels to reference grid.</p> Source code in <code>multimno/components/execution/usual_environment_aggregation/usual_environment_aggregation.py</code> <pre><code>class UsualEnvironmentAggregation(Component):\n    \"\"\"\n    A class to aggregate devices usual environment and location labels to reference grid.\n    \"\"\"\n\n    COMPONENT_ID = \"UsualEnvironmentAggregation\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        # Months that compose the long-term period, at least one\n        start_month = self.config.get(self.COMPONENT_ID, \"start_month\")\n        try:\n            self.start_date = dt.datetime.strptime(start_month, \"%Y-%m\").date()\n        except ValueError as e:\n            self.logger.error(f\"Could not parse start_month = `{start_month}`. Expected format: YYYY-MM\")\n            raise e\n\n        end_month = self.config.get(self.COMPONENT_ID, \"end_month\")\n        try:\n            self.end_date = dt.datetime.strptime(end_month, \"%Y-%m\").date()\n        except ValueError as e:\n            self.logger.error(f\"Could not parse end_month = `{end_month}`. Expected format: YYYY-MM\")\n            raise e\n        self.end_date = self.end_date.replace(day=cal.monthrange(self.end_date.year, self.end_date.month)[1])\n\n        if self.end_date &lt; self.start_date:\n            raise ValueError(f\"End month `{end_month}` should not be an earlier than start month `{start_month}`\")\n\n        self.uniform_tile_weights = self.config.getboolean(\n            UsualEnvironmentAggregation.COMPONENT_ID, \"uniform_tile_weights\"\n        )\n\n        self.season = self.config.get(self.COMPONENT_ID, \"season\")\n        if self.season not in SEASONS:\n            error_msg = f\"season: expected one of: {', '.join(SEASONS)} - found: {self.season}\"\n            self.logger.error(error_msg)\n            raise ValueError(error_msg)\n\n    def initalize_data_objects(self):\n\n        # inputs\n        self.clear_destination_directory = self.config.getboolean(\n            UsualEnvironmentAggregation.COMPONENT_ID, \"clear_destination_directory\"\n        )\n\n        self.input_data_objects = {}\n        # Check uniform for getting the grid or the enriched grid data\n        uniform_tile_weights = self.config.getboolean(UsualEnvironmentAggregation.COMPONENT_ID, \"uniform_tile_weights\")\n\n        if uniform_tile_weights:\n            inputs = {\n                \"grid_data_silver\": SilverGridDataObject,\n                \"usual_environment_labels_data_silver\": SilverUsualEnvironmentLabelsDataObject,\n            }\n        else:\n            inputs = {\n                \"enriched_grid_data_silver\": SilverEnrichedGridDataObject,\n                \"usual_environment_labels_data_silver\": SilverUsualEnvironmentLabelsDataObject,\n            }\n\n        for key, value in inputs.items():\n            path = self.config.get(CONFIG_SILVER_PATHS_KEY, key)\n            if check_if_data_path_exists(self.spark, path):\n                self.input_data_objects[value.ID] = value(self.spark, path)\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")\n                raise ValueError(f\"Invalid path for {value.ID}: {path}\")\n\n        # outputs\n\n        output_do_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"aggregated_usual_environments_silver\")\n\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, output_do_path)\n\n        self.output_data_objects = {}\n        self.output_data_objects[SilverAggregatedUsualEnvironmentsDataObject.ID] = (\n            SilverAggregatedUsualEnvironmentsDataObject(\n                self.spark, output_do_path, [ColNames.start_date, ColNames.end_date, ColNames.season]\n            )\n        )\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}...\")\n\n        # prepare grid with tile weights\n        if self.uniform_tile_weights:\n            grid_sdf = self.input_data_objects[SilverGridDataObject.ID].df\n        else:\n            grid_sdf = self.input_data_objects[SilverEnrichedGridDataObject.ID].df\n            grid_sdf = grid_sdf.select(\n                ColNames.grid_id,\n                ColNames.prior_probability,\n            )\n\n        grid_sdf = self.assign_tile_weights(grid_sdf, self.uniform_tile_weights)\n\n        # prepare usual environment labels\n        ue_labels_sdf = self.input_data_objects[SilverUsualEnvironmentLabelsDataObject.ID].df\n        ue_labels_sdf = ue_labels_sdf.filter(\n            (F.col(ColNames.start_date) == F.lit(self.start_date))\n            &amp; (F.col(ColNames.end_date) == F.lit(self.end_date))\n            &amp; (F.col(ColNames.season) == F.lit(self.season))\n        )\n        ue_labels_sdf = ue_labels_sdf.select(\n            ColNames.user_id, ColNames.grid_id, ColNames.label, ColNames.user_id_modulo\n        )\n        # aggregate usual environments\n        aggregated_ue_sdf = self.aggregate_usual_environments(ue_labels_sdf, grid_sdf)\n\n        # aggreagate location labels\n        aggregated_home_labels_sdf = self.aggregate_location_labels(ue_labels_sdf, grid_sdf, \"home\")\n\n        aggregated_work_labels_sdf = self.aggregate_location_labels(ue_labels_sdf, grid_sdf, \"work\")\n\n        # union all aggregated results\n        aggregated_results_sdf = reduce(\n            lambda df1, df2: df1.union(df2), [aggregated_ue_sdf, aggregated_home_labels_sdf, aggregated_work_labels_sdf]\n        )\n\n        # Cast column types to DO schema, add missing columns manually\n        aggregated_results_sdf = (\n            aggregated_results_sdf.withColumn(ColNames.start_date, F.lit(self.start_date))\n            .withColumn(ColNames.end_date, F.lit(self.end_date))\n            .withColumn(ColNames.season, F.lit(self.season))\n        )\n\n        aggregated_results_sdf = utils.apply_schema_casting(\n            aggregated_results_sdf, SilverAggregatedUsualEnvironmentsDataObject.SCHEMA\n        )\n\n        self.output_data_objects[SilverAggregatedUsualEnvironmentsDataObject.ID].df = aggregated_results_sdf\n\n    def assign_tile_weights(self, grid_sdf: DataFrame, uniform_tile_weights: bool) -&gt; DataFrame:\n        \"\"\"\n        Assigns weights to each tile in a DataFrame based on the specified weighting strategy.\n\n        This method updates the input DataFrame by adding a new column that contains the weight of each tile.\n        The weighting strategy is determined by the `uniform_tile_weights` parameter. If `uniform_tile_weights` is True,\n        all tiles are assigned a uniform weight of 1.0. Otherwise, the tile weights are set to the values derrived from landuse\n        information in an input grid data object.\n\n        Parameters:\n        - grid_sdf (DataFrame): The input grid DataFrame.\n        - uniform_tile_weights (bool): A flag indicating whether to assign uniform weights to all tiles (True)\n        or to use the values from grid data object column as weights (False).\n\n        Returns:\n        - DataFrame: The updated DataFrame with a new column containing the weights of each tile.\n        \"\"\"\n        if uniform_tile_weights:\n            grid_sdf = grid_sdf.withColumn(ColNames.tile_weight, F.lit(1.0))\n        else:\n            grid_sdf = grid_sdf.withColumn(ColNames.tile_weight, F.col(ColNames.prior_probability))\n\n        return grid_sdf\n\n    def get_device_tile_weights(self, ue_labels_sdf: DataFrame, grid_sdf: DataFrame, label: str = \"ue\") -&gt; DataFrame:\n        \"\"\"\n        Calculates and assigns weights to each device UE tiles based on the tile weights in the grid DataFrame.\n\n        This method performs a join operation between ue labels DataFrame and a grid DataFrame based on grid IDs.\n        It then calculates the weight of each device tile as the ratio of the tile's weight to the sum of all tile weights\n        for the same user for a given label.\n\n        If a label is specified, the method first filters the UE labels DataFrame to include only rows with the matching label\n        before proceeding with the join and weight calculation.\n\n        If a label is not specified, the method uses all tiles of a device to calculate the weights.\n\n        Parameters:\n        - ue_labels_sdf (DataFrame): The DataFrame containing usual environment and location labels for each device.\n        - grid_sdf (DataFrame): The DataFrame containing grid information, including grid IDs and tile weights.\n        - label (str, optional): A specific label to filter the UE labels DataFrame.\n\n        Returns:\n        - DataFrame: A DataFrame containing the calculated weights for each device tile.\n        \"\"\"\n\n        if label == \"ue\":\n            ue_labels_sdf = ue_labels_sdf.filter(F.col(ColNames.ue_label_rule) != F.lit(\"ue_na\"))\n\n            # as the same tile can be labeled into multiple label categories (e.g. home and work), we need to remove duplicates for ue weights\n            ue_labels_sdf = ue_labels_sdf.dropDuplicates([ColNames.user_id, ColNames.grid_id, ColNames.user_id_modulo])\n        else:\n            ue_labels_sdf = ue_labels_sdf.filter(\n                (F.col(ColNames.label) == F.lit(label)) &amp; (F.col(ColNames.location_label_rule) != F.lit(\"loc_na\"))\n            )\n\n        ue_labels_sdf = ue_labels_sdf.join(grid_sdf, on=ColNames.grid_id, how=\"inner\")\n\n        window_spec = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id)\n\n        ue_labels_sdf = ue_labels_sdf.withColumn(\n            ColNames.device_tile_weight, F.col(ColNames.tile_weight) / F.sum(ColNames.tile_weight).over(window_spec)\n        )\n\n        return ue_labels_sdf\n\n    def aggregate_usual_environments(self, ue_labels_sdf: DataFrame, grid_sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Aggregates usual environment by grid ID and calculates the sum of weighted device count.\n\n        This method first calculates device tile weights for usual environment tiles.\n        It then aggregates these weights by grid ID to compute the total weighted device count for each grid.\n        Finally, it assigns a label \"ue\" to all aggregated entries to indicate their association with usual environments.\n\n        Parameters:\n        - ue_labels_sdf (DataFrame): The DataFrame containing ue labels.\n        - grid_sdf (DataFrame): The DataFrame containing grid information with tile weights.\n\n        Returns:\n        - DataFrame: A DataFrame with sum of weighted device count per grid tile.\n        \"\"\"\n        ue_with_device_weights_sdf = self.get_device_tile_weights(ue_labels_sdf, grid_sdf)\n\n        aggregated_ue_sdf = ue_with_device_weights_sdf.groupBy(ColNames.grid_id).agg(\n            F.sum(ColNames.device_tile_weight).alias(ColNames.weighted_device_count)\n        )\n\n        aggregated_ue_sdf = aggregated_ue_sdf.withColumn(ColNames.label, F.lit(\"ue\"))\n\n        return aggregated_ue_sdf\n\n    def aggregate_location_labels(self, ue_labels_sdf: DataFrame, grid_sdf: DataFrame, label: str) -&gt; DataFrame:\n        \"\"\"\n        Aggregates location labels by grid ID and calculates the sum of weighted device count.\n\n        This method first calculates device tile weights for location label tiles.\n        It then aggregates these weights by grid ID to compute the total weighted device count for each grid for each label.\n        Finally, it assigns corresponding label name to all aggregated entries to indicate their association with usual environments.\n\n        Parameters:\n        - ue_labels_sdf (DataFrame): The DataFrame containing ue labels.\n        - grid_sdf (DataFrame): The DataFrame containing grid information with tile weights.\n\n        Returns:\n        - DataFrame: A DataFrame with sum of weighted device count per grid tile.\n        \"\"\"\n\n        loc_label_with_device_weights_sdf = self.get_device_tile_weights(ue_labels_sdf, grid_sdf, label)\n\n        aggregated_labels_sdf = loc_label_with_device_weights_sdf.groupBy(ColNames.grid_id).agg(\n            F.sum(ColNames.device_tile_weight).alias(ColNames.weighted_device_count)\n        )\n\n        aggregated_labels_sdf = aggregated_labels_sdf.withColumn(ColNames.label, F.lit(label))\n\n        return aggregated_labels_sdf\n</code></pre>"},{"location":"reference/components/execution/usual_environment_aggregation/usual_environment_aggregation/#components.execution.usual_environment_aggregation.usual_environment_aggregation.UsualEnvironmentAggregation.aggregate_location_labels","title":"<code>aggregate_location_labels(ue_labels_sdf, grid_sdf, label)</code>","text":"<p>Aggregates location labels by grid ID and calculates the sum of weighted device count.</p> <p>This method first calculates device tile weights for location label tiles. It then aggregates these weights by grid ID to compute the total weighted device count for each grid for each label. Finally, it assigns corresponding label name to all aggregated entries to indicate their association with usual environments.</p> <p>Parameters: - ue_labels_sdf (DataFrame): The DataFrame containing ue labels. - grid_sdf (DataFrame): The DataFrame containing grid information with tile weights.</p> <p>Returns: - DataFrame: A DataFrame with sum of weighted device count per grid tile.</p> Source code in <code>multimno/components/execution/usual_environment_aggregation/usual_environment_aggregation.py</code> <pre><code>def aggregate_location_labels(self, ue_labels_sdf: DataFrame, grid_sdf: DataFrame, label: str) -&gt; DataFrame:\n    \"\"\"\n    Aggregates location labels by grid ID and calculates the sum of weighted device count.\n\n    This method first calculates device tile weights for location label tiles.\n    It then aggregates these weights by grid ID to compute the total weighted device count for each grid for each label.\n    Finally, it assigns corresponding label name to all aggregated entries to indicate their association with usual environments.\n\n    Parameters:\n    - ue_labels_sdf (DataFrame): The DataFrame containing ue labels.\n    - grid_sdf (DataFrame): The DataFrame containing grid information with tile weights.\n\n    Returns:\n    - DataFrame: A DataFrame with sum of weighted device count per grid tile.\n    \"\"\"\n\n    loc_label_with_device_weights_sdf = self.get_device_tile_weights(ue_labels_sdf, grid_sdf, label)\n\n    aggregated_labels_sdf = loc_label_with_device_weights_sdf.groupBy(ColNames.grid_id).agg(\n        F.sum(ColNames.device_tile_weight).alias(ColNames.weighted_device_count)\n    )\n\n    aggregated_labels_sdf = aggregated_labels_sdf.withColumn(ColNames.label, F.lit(label))\n\n    return aggregated_labels_sdf\n</code></pre>"},{"location":"reference/components/execution/usual_environment_aggregation/usual_environment_aggregation/#components.execution.usual_environment_aggregation.usual_environment_aggregation.UsualEnvironmentAggregation.aggregate_usual_environments","title":"<code>aggregate_usual_environments(ue_labels_sdf, grid_sdf)</code>","text":"<p>Aggregates usual environment by grid ID and calculates the sum of weighted device count.</p> <p>This method first calculates device tile weights for usual environment tiles. It then aggregates these weights by grid ID to compute the total weighted device count for each grid. Finally, it assigns a label \"ue\" to all aggregated entries to indicate their association with usual environments.</p> <p>Parameters: - ue_labels_sdf (DataFrame): The DataFrame containing ue labels. - grid_sdf (DataFrame): The DataFrame containing grid information with tile weights.</p> <p>Returns: - DataFrame: A DataFrame with sum of weighted device count per grid tile.</p> Source code in <code>multimno/components/execution/usual_environment_aggregation/usual_environment_aggregation.py</code> <pre><code>def aggregate_usual_environments(self, ue_labels_sdf: DataFrame, grid_sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Aggregates usual environment by grid ID and calculates the sum of weighted device count.\n\n    This method first calculates device tile weights for usual environment tiles.\n    It then aggregates these weights by grid ID to compute the total weighted device count for each grid.\n    Finally, it assigns a label \"ue\" to all aggregated entries to indicate their association with usual environments.\n\n    Parameters:\n    - ue_labels_sdf (DataFrame): The DataFrame containing ue labels.\n    - grid_sdf (DataFrame): The DataFrame containing grid information with tile weights.\n\n    Returns:\n    - DataFrame: A DataFrame with sum of weighted device count per grid tile.\n    \"\"\"\n    ue_with_device_weights_sdf = self.get_device_tile_weights(ue_labels_sdf, grid_sdf)\n\n    aggregated_ue_sdf = ue_with_device_weights_sdf.groupBy(ColNames.grid_id).agg(\n        F.sum(ColNames.device_tile_weight).alias(ColNames.weighted_device_count)\n    )\n\n    aggregated_ue_sdf = aggregated_ue_sdf.withColumn(ColNames.label, F.lit(\"ue\"))\n\n    return aggregated_ue_sdf\n</code></pre>"},{"location":"reference/components/execution/usual_environment_aggregation/usual_environment_aggregation/#components.execution.usual_environment_aggregation.usual_environment_aggregation.UsualEnvironmentAggregation.assign_tile_weights","title":"<code>assign_tile_weights(grid_sdf, uniform_tile_weights)</code>","text":"<p>Assigns weights to each tile in a DataFrame based on the specified weighting strategy.</p> <p>This method updates the input DataFrame by adding a new column that contains the weight of each tile. The weighting strategy is determined by the <code>uniform_tile_weights</code> parameter. If <code>uniform_tile_weights</code> is True, all tiles are assigned a uniform weight of 1.0. Otherwise, the tile weights are set to the values derrived from landuse information in an input grid data object.</p> <p>Parameters: - grid_sdf (DataFrame): The input grid DataFrame. - uniform_tile_weights (bool): A flag indicating whether to assign uniform weights to all tiles (True) or to use the values from grid data object column as weights (False).</p> <p>Returns: - DataFrame: The updated DataFrame with a new column containing the weights of each tile.</p> Source code in <code>multimno/components/execution/usual_environment_aggregation/usual_environment_aggregation.py</code> <pre><code>def assign_tile_weights(self, grid_sdf: DataFrame, uniform_tile_weights: bool) -&gt; DataFrame:\n    \"\"\"\n    Assigns weights to each tile in a DataFrame based on the specified weighting strategy.\n\n    This method updates the input DataFrame by adding a new column that contains the weight of each tile.\n    The weighting strategy is determined by the `uniform_tile_weights` parameter. If `uniform_tile_weights` is True,\n    all tiles are assigned a uniform weight of 1.0. Otherwise, the tile weights are set to the values derrived from landuse\n    information in an input grid data object.\n\n    Parameters:\n    - grid_sdf (DataFrame): The input grid DataFrame.\n    - uniform_tile_weights (bool): A flag indicating whether to assign uniform weights to all tiles (True)\n    or to use the values from grid data object column as weights (False).\n\n    Returns:\n    - DataFrame: The updated DataFrame with a new column containing the weights of each tile.\n    \"\"\"\n    if uniform_tile_weights:\n        grid_sdf = grid_sdf.withColumn(ColNames.tile_weight, F.lit(1.0))\n    else:\n        grid_sdf = grid_sdf.withColumn(ColNames.tile_weight, F.col(ColNames.prior_probability))\n\n    return grid_sdf\n</code></pre>"},{"location":"reference/components/execution/usual_environment_aggregation/usual_environment_aggregation/#components.execution.usual_environment_aggregation.usual_environment_aggregation.UsualEnvironmentAggregation.get_device_tile_weights","title":"<code>get_device_tile_weights(ue_labels_sdf, grid_sdf, label='ue')</code>","text":"<p>Calculates and assigns weights to each device UE tiles based on the tile weights in the grid DataFrame.</p> <p>This method performs a join operation between ue labels DataFrame and a grid DataFrame based on grid IDs. It then calculates the weight of each device tile as the ratio of the tile's weight to the sum of all tile weights for the same user for a given label.</p> <p>If a label is specified, the method first filters the UE labels DataFrame to include only rows with the matching label before proceeding with the join and weight calculation.</p> <p>If a label is not specified, the method uses all tiles of a device to calculate the weights.</p> <p>Parameters: - ue_labels_sdf (DataFrame): The DataFrame containing usual environment and location labels for each device. - grid_sdf (DataFrame): The DataFrame containing grid information, including grid IDs and tile weights. - label (str, optional): A specific label to filter the UE labels DataFrame.</p> <p>Returns: - DataFrame: A DataFrame containing the calculated weights for each device tile.</p> Source code in <code>multimno/components/execution/usual_environment_aggregation/usual_environment_aggregation.py</code> <pre><code>def get_device_tile_weights(self, ue_labels_sdf: DataFrame, grid_sdf: DataFrame, label: str = \"ue\") -&gt; DataFrame:\n    \"\"\"\n    Calculates and assigns weights to each device UE tiles based on the tile weights in the grid DataFrame.\n\n    This method performs a join operation between ue labels DataFrame and a grid DataFrame based on grid IDs.\n    It then calculates the weight of each device tile as the ratio of the tile's weight to the sum of all tile weights\n    for the same user for a given label.\n\n    If a label is specified, the method first filters the UE labels DataFrame to include only rows with the matching label\n    before proceeding with the join and weight calculation.\n\n    If a label is not specified, the method uses all tiles of a device to calculate the weights.\n\n    Parameters:\n    - ue_labels_sdf (DataFrame): The DataFrame containing usual environment and location labels for each device.\n    - grid_sdf (DataFrame): The DataFrame containing grid information, including grid IDs and tile weights.\n    - label (str, optional): A specific label to filter the UE labels DataFrame.\n\n    Returns:\n    - DataFrame: A DataFrame containing the calculated weights for each device tile.\n    \"\"\"\n\n    if label == \"ue\":\n        ue_labels_sdf = ue_labels_sdf.filter(F.col(ColNames.ue_label_rule) != F.lit(\"ue_na\"))\n\n        # as the same tile can be labeled into multiple label categories (e.g. home and work), we need to remove duplicates for ue weights\n        ue_labels_sdf = ue_labels_sdf.dropDuplicates([ColNames.user_id, ColNames.grid_id, ColNames.user_id_modulo])\n    else:\n        ue_labels_sdf = ue_labels_sdf.filter(\n            (F.col(ColNames.label) == F.lit(label)) &amp; (F.col(ColNames.location_label_rule) != F.lit(\"loc_na\"))\n        )\n\n    ue_labels_sdf = ue_labels_sdf.join(grid_sdf, on=ColNames.grid_id, how=\"inner\")\n\n    window_spec = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id)\n\n    ue_labels_sdf = ue_labels_sdf.withColumn(\n        ColNames.device_tile_weight, F.col(ColNames.tile_weight) / F.sum(ColNames.tile_weight).over(window_spec)\n    )\n\n    return ue_labels_sdf\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/","title":"usual_environment_labeling","text":""},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/","title":"usual_environment_labeling","text":"<p>Module that implements the Usual Environment Labeling functionality</p>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling","title":"<code>UsualEnvironmentLabeling</code>","text":"<p>               Bases: <code>Component</code></p> <p>A class to calculate the grid tiles that conform the usual environment, home and work locations of each user.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>class UsualEnvironmentLabeling(Component):\n    \"\"\"\n    A class to calculate the grid tiles that conform the usual environment, home and work locations of each user.\n    \"\"\"\n\n    COMPONENT_ID = \"UsualEnvironmentLabeling\"\n    LABEL_TO_LABELNAMES = {\"ue\": \"no_label\", \"home\": \"home\", \"work\": \"work\"}\n    LABEL_TO_SHORT_LABELNAMES = {\"ue\": \"ue\", \"home\": \"h\", \"work\": \"w\"}\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.gap_ps_thresholds = {\n            \"ue\": self.config.getfloat(self.COMPONENT_ID, \"ue_gap_ps_threshold\"),\n            \"home\": self.config.getint(self.COMPONENT_ID, \"gap_ps_threshold\"),\n            \"work\": self.config.getint(self.COMPONENT_ID, \"gap_ps_threshold\"),\n        }\n\n        self.gap_ps_threshold_is_absolute = {\n            \"ue\": False,\n            \"home\": True,\n            \"work\": True,\n        }  # TODO:\n\n        self.ps_thresholds = {\n            \"ue\": self.config.getfloat(self.COMPONENT_ID, \"ue_ps_threshold\"),\n            \"home\": self.config.getfloat(self.COMPONENT_ID, \"home_ps_threshold\"),\n            \"work\": self.config.getfloat(self.COMPONENT_ID, \"work_ps_threshold\"),\n        }\n\n        self.freq_thresholds = {\n            \"ue\": self.config.getfloat(self.COMPONENT_ID, \"ue_ndays_threshold\"),\n            \"home\": self.config.getfloat(self.COMPONENT_ID, \"home_ndays_threshold\"),\n            \"work\": self.config.getfloat(self.COMPONENT_ID, \"work_ndays_threshold\"),\n        }\n\n        self.ps_threshold_for_rare_devices = self.config.getfloat(self.COMPONENT_ID, \"total_ps_threshold\")\n        self.freq_threshold_for_discontinuous_devices = self.config.getfloat(self.COMPONENT_ID, \"total_ndays_threshold\")\n\n        self.day_and_interval_type_combinations = {\n            \"ue\": [(\"all\", \"all\"), (\"all\", \"night_time\"), (\"workdays\", \"working_hours\")],\n            \"home\": [(\"all\", \"all\"), (\"all\", \"night_time\")],\n            \"work\": [(\"workdays\", \"working_hours\")],\n        }\n\n        self.season = self.config.get(self.COMPONENT_ID, \"season\")\n        if self.season not in SEASONS:\n            error_msg = f\"season: expected one of: {', '.join(SEASONS)} - found: {self.season}\"\n            self.logger.error(error_msg)\n            raise ValueError(error_msg)\n\n        # Months that compose the long-term period, at least one\n        start_month = self.config.get(self.COMPONENT_ID, \"start_month\")\n        try:\n            self.start_date = dt.datetime.strptime(start_month, \"%Y-%m\").date()\n        except ValueError as e:\n            self.logger.error(f\"Could not parse start_month = `{start_month}`. Expected format: YYYY-MM\")\n            raise e\n\n        end_month = self.config.get(self.COMPONENT_ID, \"end_month\")\n        try:\n            self.end_date = dt.datetime.strptime(end_month, \"%Y-%m\").date()\n        except ValueError as e:\n            self.logger.error(f\"Could not parse end_month = `{end_month}`. Expected format: YYYY-MM\")\n            raise e\n        self.end_date = self.end_date.replace(day=cal.monthrange(self.end_date.year, self.end_date.month)[1])\n\n        if self.end_date &lt; self.start_date:\n            raise ValueError(f\"End month `{end_month}` should not be earlier than start month `{start_month}`\")\n\n        self.ltps_df: DataFrame = None\n        self.rare_devices_count = 0\n        self.discontinuous_devices_count = 0\n\n    def initalize_data_objects(self):\n        # Load paths from configuration file:\n        input_ltps_silver_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"longterm_permanence_score_data_silver\")\n        output_uelabels_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"usual_environment_labels_data_silver\")\n        output_quality_metrics_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"usual_environment_labeling_quality_metrics_data_silver\"\n        )\n\n        # Initialise input and output data objects:\n        silver_ltps = SilverLongtermPermanenceScoreDataObject(self.spark, input_ltps_silver_path)\n        ue_labels = SilverUsualEnvironmentLabelsDataObject(self.spark, output_uelabels_path)\n        ue_quality_metrics = SilverUsualEnvironmentLabelingQualityMetricsDataObject(\n            self.spark, output_quality_metrics_path\n        )\n\n        # Store data objects in the corresponding attributes:\n        self.input_data_objects = {silver_ltps.ID: silver_ltps}\n        self.output_data_objects = {ue_labels.ID: ue_labels, ue_quality_metrics.ID: ue_quality_metrics}\n\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        # read input data object:\n        self.read()\n        full_ltps_df = self.input_data_objects[SilverLongtermPermanenceScoreDataObject.ID].df\n        # filtering to obtain the main dataset which this method will work with:\n        self.ltps_df = self.filter_ltps_by_target_dates(full_ltps_df, self.start_date, self.end_date, self.season)\n        # assert that all the needed day type and interval times are available in the main dataset:\n        self.check_needed_day_and_interval_types(self.ltps_df, self.day_and_interval_type_combinations)\n        # main transformations of this method:\n        self.transform()\n        # write output data objects:\n        self.write()\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    @staticmethod\n    def filter_ltps_by_target_dates(\n        full_ltps_df: DataFrame, start_date: dt.date, end_date: dt.date, season: str\n    ) -&gt; DataFrame:\n        \"\"\"\n        Keep only the rows of the input SilverLongtermPermanenceScoreDataObject in which the start\n        date, end date and season match the ones specified for the processing of this method.\n\n        Args:\n            full_ltps_df (DataFrame): full dataset.\n            start_date (dt.date): specified target start date for the execution of the method.\n            end_date (dt.date): specified target end date for the execution of the method.\n            season (str): specified target season for the execution of the method.\n\n        Returns:\n            DataFrame: filtered dataset.\n        \"\"\"\n        filtered_ltps_df = full_ltps_df.filter(\n            (F.col(ColNames.start_date) == start_date)\n            &amp; (F.col(ColNames.end_date) == end_date)\n            &amp; (F.col(ColNames.season) == season)\n        ).select(\n            ColNames.user_id,\n            ColNames.grid_id,\n            ColNames.lps,\n            ColNames.total_frequency,\n            ColNames.day_type,\n            ColNames.time_interval,\n            ColNames.user_id_modulo,\n            ColNames.id_type,\n        )\n        return filtered_ltps_df\n\n    @staticmethod\n    def check_needed_day_and_interval_types(\n        ltps_df: DataFrame, day_and_interval_type_combinations: dict[str, list[tuple[str, str]]]\n    ):\n        \"\"\"\n        Method that checks if the needed combinations of day type and interval type are available\n        in the provided dataset.\n\n        Args:\n            ltps_df (DataFrame): provided Long-Term Permanence Score dataset\n            day_and_interval_type_combinations (dict[str,list[tuple[str,str]]]): day type and interval type\n                combinations that are needed for the execution of the method.\n\n        Raises:\n            FileNotFoundError: If there is no data for one or more of the needed combinations of day type\n                and interval type.\n        \"\"\"\n        # Build set of all needed day-interval type combinations:\n        all_day_and_interval_type_combinations = {\n            comb for el in day_and_interval_type_combinations.values() for comb in el\n        }\n\n        # Assert that these combinations appear at least once in the input Long-Term Permanence\n        # Score data object:\n        for day_type, time_interval in all_day_and_interval_type_combinations:\n\n            filtered_ltps_df = ltps_df.filter(\n                (F.col(ColNames.day_type) == day_type) &amp; (F.col(ColNames.time_interval) == time_interval)\n            )\n\n            data_exists = filtered_ltps_df.count() &gt; 0\n            if not data_exists:\n                raise FileNotFoundError(\n                    \"No Long-term Permanence Score data has been found for \"\n                    f\"day_type `{day_type}` and time_interval `{time_interval}`\"\n                )\n\n    @staticmethod\n    def find_rarely_observed_devices(total_observations_df: DataFrame, total_device_ps_threshold: int) -&gt; DataFrame:\n        \"\"\"\n        Find devices (user ids) which match the condition for being considered \"rarely observed\".\n        This condition consists in having a total device observation of: lps &gt; total_ps_threshold.\n\n        Args:\n            total_observations_df (DataFrame): Long-Term Permanence Score dataset, filtered by the\n                corresponding start and end dates, and filtered by id_type == 'device_observation'.\n            total_device_ps_threshold (int): ps threshold.\n\n        Returns:\n            DataFrame: two-column dataframe with the ids of the devices to discard and the corresponding user modulos.\n        \"\"\"\n        rarely_observed_user_ids = total_observations_df.filter(F.col(ColNames.lps) &lt; total_device_ps_threshold).select(\n            ColNames.user_id_modulo, ColNames.user_id\n        )\n        return rarely_observed_user_ids\n\n    @staticmethod\n    def find_discontinuously_observed_devices(\n        total_observations_df: DataFrame, total_device_freq_threshold: int\n    ) -&gt; DataFrame:\n        \"\"\"\n        Find devices (user ids) which match the condition for being considered \"discontinuously observed\".\n        This condition consists in having a total device observation of: freq &gt; total_freq_threshold.\n\n        Args:\n            total_observations_df (DataFrame): Long-Term Permanence Score dataset, filtered by the\n                corresponding start and end dates, and filtered by id_type == 'device_observation'.\n            total_device_freq_threshold (int): ps threshold.\n\n        Returns:\n            DataFrame: two-column dataframe with the ids of the devices to discard and the corresponding user modulos.\n        \"\"\"\n        discontinuously_observed_user_ids = total_observations_df.filter(\n            F.col(ColNames.total_frequency) &lt; total_device_freq_threshold\n        ).select(ColNames.user_id_modulo, ColNames.user_id)\n        return discontinuously_observed_user_ids\n\n    @staticmethod\n    def discard_user_ids(ltps_df: DataFrame, user_ids: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Given a list of user ids, discard them from a Long-Term Permanence Score dataset.\n\n        Args:\n            ltps_df (DataFrame): Long-Term Permanence Score dataset.\n            user_ids (DataFrame): one-column dataframe with the ids of the devices to discard.\n\n        Returns:\n            DataFrame: filtered Long-Term Permanence Score dataset.\n        \"\"\"\n        ltps_df = ltps_df.join(user_ids, on=[ColNames.user_id_modulo, ColNames.user_id], how=\"left_anti\")\n        return ltps_df\n\n    def discard_devices(self, ltps_df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Given a Long-Term Permanence Score dataset (filtered for the corresponding start and end dates),\n        discard the rows corresponding to some devices.\n\n        There are 2 type of devices to discard:\n            - rarely observed devices, based on LPS (long-term permanence score).\n            - discontinuously observed devices, based on frequency.\n\n        The user ids that are classified in any of these 2 groups are discarded from the Long-Term\n        Permanence Score dataset, and the number of discarded users of each kind is saved to the\n        corresponding attributes.\n\n        Args:\n            ltps_df (DataFrame): Long-Term Permanence Score dataset (filtered for the corresponding\n                start and end dates).\n\n        Returns:\n            DataFrame: Long-Term Permanence Score dataset (filtered for the corresponding start and\n                end dates), without the rows associated to rarely or discontinuously observed\n                devices.\n        \"\"\"\n        # Initial filter of ltps dataset to keep total device observation values:\n        total_observations_df = ltps_df.filter(\n            (F.col(ColNames.id_type) == \"device_observation\")\n            &amp; (F.col(ColNames.day_type) == \"all\")\n            &amp; (F.col(ColNames.time_interval) == \"all\")\n        )\n\n        # Rarely observed:\n        rarely_observed_user_ids = self.find_rarely_observed_devices(\n            total_observations_df, self.ps_threshold_for_rare_devices\n        )\n        self.rare_devices_count = rarely_observed_user_ids.count()\n\n        # Discontinuously observed:\n        discontinuously_observed_user_ids = self.find_discontinuously_observed_devices(\n            total_observations_df, self.freq_threshold_for_discontinuous_devices\n        )\n        self.discontinuous_devices_count = discontinuously_observed_user_ids.count()\n\n        # All user ids to discard:\n        discardable_user_ids = rarely_observed_user_ids.union(discontinuously_observed_user_ids)\n\n        # Filter dataset:\n        filtered_ltps_df = self.discard_user_ids(ltps_df, discardable_user_ids)\n\n        return filtered_ltps_df\n\n    @staticmethod\n    def add_abs_ps_threshold(\n        ltps_df: DataFrame, window: Window, gap_ps_threshold: int | float, threshold_is_absolute: bool\n    ) -&gt; DataFrame:\n        \"\"\"\n        Add \"abs_ps_threshold\" field to the ltps dataset.\n\n        If \"threshold_is_absolute\" is True, then just assign \"abs_ps_threshold\" = \"gap_ps_threshold\"\n        value to all registers.\n\n        If threshold_is_absolute is False, then reach the maximum value of lps for a grid tile for\n        each device (ps_max) and calculate \"abs_ps_threshold\" = \"gap_ps_threshold\" * \"ps_max\".\n\n        Args:\n            ltps_df (DataFrame): Long-Term Permanence Score dataset.\n            window (Window): window, partitioned by user id and ordered by lps.\n            gap_ps_threshold (int|float): absolute/relative lps threshold.\n            threshold_is_absolute (bool): indicates if gap_ps_threshold is a relative/absolute value.\n\n        Returns:\n            DataFrame: Long-Term Permanence Score dataset with the additional \"abs_ps_threshold\"\n                column.\n        \"\"\"\n        if threshold_is_absolute is True:\n            ltps_df = ltps_df.withColumn(\"abs_ps_threshold\", F.lit(gap_ps_threshold))\n        else:\n            ltps_df = (\n                ltps_df.withColumn(\"ps_max\", F.first(ColNames.lps).over(window))\n                .withColumn(\"abs_ps_threshold\", F.col(\"ps_max\") * F.lit(gap_ps_threshold / 100))\n                .drop(\"ps_max\")\n            )\n        return ltps_df\n\n    def cut_tiles_at_gap(self, ltps_df: DataFrame, gap_ps_threshold: float, threshold_is_absolute: bool) -&gt; DataFrame:\n        \"\"\"\n        Preprocessing function. Given a Long-Term Permanence Metrics dataset, for each user:\n        - sort the grid tiles of the user by LPS\n        - find a high difference (gap) in the LPS values between one grid tile and the next one,\n          where what is a high difference is defined through the gap_ps_threshold argument.\n        - for each agent, filter out all tiles after this high difference: the remaining tiles are\n          the \"pre-selected tiles\", which are the output of this function.\n\n        Args:\n            ltps_df (DataFrame): Long-Term Permanence Metrics dataset, for a specific day type and\n                time interval combination.\n            gap_ps_threshold (int/float): absolute/relative lps threshold.\n            threshold_is_absolute (bool): indicates if gap_ps_threshold is a relative/absolute value.\n\n        Returns:\n            DataFrame: dataset with the pre-selected tiles.\n        \"\"\"\n        ltps_df = ltps_df.filter(F.col(ColNames.id_type) == \"grid\")\n\n        window = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id).orderBy(F.desc(ColNames.lps))\n        cumulative_window = window.rowsBetween(Window.unboundedPreceding, Window.currentRow)\n\n        ltps_df = self.add_abs_ps_threshold(ltps_df, window, gap_ps_threshold, threshold_is_absolute)\n\n        pre_selected_tiles_df = (\n            ltps_df.withColumn(\"previous_lps\", F.lag(ColNames.lps, 1).over(window))\n            .withColumn(\"lps_difference\", F.col(\"previous_lps\") - F.col(ColNames.lps))\n            .fillna({\"lps_difference\": 0})\n            .withColumn(\"high_difference\", F.when(F.col(\"lps_difference\") &gt;= F.col(\"abs_ps_threshold\"), 1).otherwise(0))\n            .withColumn(\"cumulative_condition\", F.sum(F.col(\"high_difference\")).over(cumulative_window))\n            .filter(F.col(\"cumulative_condition\") == F.lit(0))\n            .drop(\"lps_difference\", \"previous_lps\", \"high_difference\", \"cumulative_condition\", \"abs_ps_threshold\")\n        )\n\n        return pre_selected_tiles_df\n\n    @staticmethod\n    def calculate_device_abs_ps_threshold(\n        ltps_df_i: DataFrame, target_rows_ltps_df: DataFrame, perc_ps_threshold: float\n    ) -&gt; DataFrame:\n        \"\"\"\n        Calculate the total assigned long-term permanence score for each device (total_device_ps) for a given day type\n        and time interval and add this information to an additional column of the provided dataset. Then, based on\n        this column, generate the absolute ps threshold to consider for each device by applying the corresponding\n        configured percentage (perc_ps_threshold).\n\n        Args:\n            ltps_df_i (DataFrame): Long-Term Permanence Metrics dataset, for a specific day type and time interval\n                combination.\n            target_rows_ltps_df (DataFrame): Long-Term Permanence Metrics dataset, for a specific day type and time\n                interval combination, with one 'id_type' == 'grid'.\n            perc_ps_threshold (float): specified ps threshold (in percentage).\n\n        Returns:\n            DataFrame: Long-Term Permanence Metrics dataset, for a specific day type and time interval combination,\n                with one 'id_type' == 'grid', with an additional column named \"abs_ps_threshold\".\n        \"\"\"\n        device_total_ps_df = (\n            ltps_df_i.filter(F.col(ColNames.id_type) == \"device_observation\")\n            .withColumnRenamed(ColNames.lps, \"total_device_ps\")\n            .select(ColNames.user_id_modulo, ColNames.user_id, \"total_device_ps\")\n        )\n\n        target_rows_ltps_df = (\n            target_rows_ltps_df.join(device_total_ps_df, on=[ColNames.user_id_modulo, ColNames.user_id])\n            .withColumn(\"abs_ps_threshold\", F.col(\"total_device_ps\") * F.lit(perc_ps_threshold / 100))\n            .drop(\"total_device_ps\")\n        )\n\n        return target_rows_ltps_df\n\n    @staticmethod\n    def filter_by_ps(ltps_df: DataFrame) -&gt; tuple[DataFrame, DataFrame]:\n        \"\"\"\n        Apply the long-term permanence score filter (ps) to a Long-Term Permanence Metrics dataset. Filter rows for\n        which 'lps' &gt; abs_ps_threshold.\n\n        Args:\n            ltps_df (DataFrame): Long-Term Permanence Metrics dataset, for a specific day type and time interval\n                combination.\n\n        Returns:\n            DataFrame: Long-Term Permanence Metrics dataset, for a specific day type and time interval combination,\n                keeping only the rows that DO pass the ps filter.\n            DataFrame: Long-Term Permanence Metrics dataset, for a specific day type and time interval combination,\n                keeping only the rows that DO NOT pass the ps filter.\n        \"\"\"\n        common_df = ltps_df.withColumn(\n            \"selected_flag\", F.when(F.col(ColNames.lps) &gt;= F.col(\"abs_ps_threshold\"), True).otherwise(False)\n        )\n\n        selected_tiles_df = common_df.filter(F.col(\"selected_flag\")).drop(\"abs_ps_threshold\", \"selected_flag\")\n        not_selected_tiles_df = common_df.filter(~F.col(\"selected_flag\")).drop(\"abs_ps_threshold\", \"selected_flag\")\n\n        return selected_tiles_df, not_selected_tiles_df\n\n    @staticmethod\n    def calculate_device_abs_ndays_threshold(\n        ltps_df_i: DataFrame, target_rows_ltps_df: DataFrame, perc_ndays_threshold: float\n    ) -&gt; DataFrame:\n        \"\"\"\n        Calculate the total frequency (observed number of days) for each device (total_device_ndays) for a given day\n        type and time interval and add this information to an additional column of the provided dataset. Then, based on\n        this column, generate the absolute ndays threshold to consider for each device by applying the corresponding\n        configured percentage (perc_ndays_threshold).\n\n        Args:\n            ltps_df_i (DataFrame): Long-Term Permanence Metrics dataset, for a specific day type and time interval\n                combination.\n            target_rows_ltps_df (DataFrame): Long-Term Permanence Metrics dataset, for a specific day type and time\n                interval combination, with one 'id_type' == 'grid'.\n            perc_ndays_threshold (float): specified ndays threshold (in percentage).\n\n        Returns:\n            DataFrame: Long-Term Permanence Metrics dataset, for a specific day type and time interval combination,\n                with one 'id_type' == 'grid', with an additional column named \"abs_ndays_threshold\".\n        \"\"\"\n        device_total_ndays_df = (\n            ltps_df_i.filter(F.col(ColNames.id_type) == \"device_observation\")\n            .withColumnRenamed(ColNames.total_frequency, \"total_device_ndays\")\n            .select(ColNames.user_id_modulo, ColNames.user_id, \"total_device_ndays\")\n        )\n\n        target_rows_ltps_df = (\n            target_rows_ltps_df.join(device_total_ndays_df, on=[ColNames.user_id_modulo, ColNames.user_id])\n            .withColumn(\"abs_ndays_threshold\", F.col(\"total_device_ndays\") * F.lit(perc_ndays_threshold / 100))\n            .drop(\"total_device_ndays\")\n        )\n\n        return target_rows_ltps_df\n\n    @staticmethod\n    def filter_by_ndays(ltps_df: DataFrame) -&gt; tuple[DataFrame, DataFrame]:\n        \"\"\"\n        Apply the frequency filter (number of days) to a Long-Term Permanence Metrics dataset. Filter rows\n        for which 'total_frequency' &gt; abs_ndays_threshold.\n\n        Args:\n            ltps_df (DataFrame): Long-Term Permanence Metrics dataset, for a specific day type and time interval\n                combination.\n\n        Returns:\n            DataFrame: Long-Term Permanence Metrics dataset, for a specific day type and time interval combination,\n                keeping only the rows that DO pass the ndays filter.\n            DataFrame: Long-Term Permanence Metrics dataset, for a specific day type and time interval combination,\n                keeping only the rows that DO NOT pass the ndays filter.\n        \"\"\"\n        common_df = ltps_df.withColumn(\n            \"selected_flag\",\n            F.when(F.col(ColNames.total_frequency) &gt;= F.col(\"abs_ndays_threshold\"), True).otherwise(False),\n        )\n\n        selected_tiles_df = common_df.filter(F.col(\"selected_flag\")).drop(\"abs_ndays_threshold\", \"selected_flag\")\n        not_selected_tiles_df = common_df.filter(~F.col(\"selected_flag\")).drop(\"abs_ndays_threshold\", \"selected_flag\")\n\n        return selected_tiles_df, not_selected_tiles_df\n\n    def format_selected_tiles(self, selected_tiles_df: DataFrame, label_type: str, n_rule: int) -&gt; DataFrame:\n        \"\"\"\n        Add the \"label\", \"ue_label_rule\" and \"location_label_rule\" columns to the given selected tiles dataframe:\n        - the \"label\" column is assigned directly depending on the label type being computed by mapping with the\n          provided dictionary: LABEL_TO_LABELNAMES[label_type].\n        - the \"ue_label_rule\" is equal to \"ue_na\" if the label type being computed is different from \"ue\", or is\n          equal to \"{ue}_{N}\" if the label type being computed is \"ue\", where:\n            - {N} = n_rule\n            - {ue} = LABEL_TO_SHORT_LABELNAMES[label_type].\n        - the \"location_label_rule\" is equal to \"loc_na\" if the label type being computed is \"ue\", or is equal to\n          \"{loc}_{N}\" if the label type being computed is NOT \"ue\", where:\n            - {N} = n_rule\n            - {loc} = LABEL_TO_SHORT_LABELNAMES[label_type].\n\n        Args:\n            selected_tiles_df (DataFrame): selected tiles dataset.\n            label_type (str): label type to compute: 'ue', 'home' or 'work'.\n            n_rule (int): number of the current rule in order to generate the label_rule column values.\n\n        Returns:\n            DataFrame: selected tiles dataset, with the additional columns: \"label\", \"ue_label_rule\" and\n                \"location_label_rule\".\n        \"\"\"\n        short_labelname = self.LABEL_TO_SHORT_LABELNAMES[label_type]\n\n        if label_type == \"ue\":\n            ue_rule_txt = f\"{short_labelname}_{n_rule}\"\n            selected_tiles_df = selected_tiles_df.withColumn(ColNames.ue_label_rule, F.lit(ue_rule_txt))\n        else:\n            labelname = self.LABEL_TO_LABELNAMES[label_type]\n            selected_tiles_df = selected_tiles_df.withColumn(ColNames.label, F.lit(labelname))\n            loc_rule_txt = f\"{short_labelname}_{n_rule}\"\n            selected_tiles_df = selected_tiles_df.withColumn(ColNames.location_label_rule, F.lit(loc_rule_txt))\n\n        return selected_tiles_df\n\n    def format_not_selected_tiles(self, not_selected_tiles_df: DataFrame, label_type: str) -&gt; DataFrame:\n        \"\"\"\n        Add the \"label\", \"ue_label_rule\" and \"location_label_rule\" columns to the not selected tiles dataframe:\n        - the \"label\" column is assigned directly depending on the label type being computed by mapping with the\n          provided dictionary: LABEL_TO_LABELNAMES[label_type].\n        - the \"ue_label_rule\" is equal to \"ue_na\".\n        - the \"location_label_rule\" is equal to \"loc_na\".\n\n        Args:\n            not_selected_tiles_df (DataFrame): not selected tiles dataset.\n            label_type (str): label type to compute: 'ue', 'home' or 'work'.\n\n        Returns:\n            DataFrame: not selected tiles dataset, with the additional columns: \"label\", \"ue_label_rule\" and\n                \"location_label_rule\".\n        \"\"\"\n        if label_type == \"ue\":\n            not_selected_tiles_df = not_selected_tiles_df.withColumn(ColNames.ue_label_rule, F.lit(\"ue_na\"))\n        else:\n            labelname = self.LABEL_TO_LABELNAMES[label_type]\n            not_selected_tiles_df = not_selected_tiles_df.withColumn(ColNames.label, F.lit(labelname))\n            not_selected_tiles_df = not_selected_tiles_df.withColumn(ColNames.location_label_rule, F.lit(\"loc_na\"))\n\n        return not_selected_tiles_df\n\n    def compute_generic_labeling(self, label_type: str, apply_ndays_filter: bool) -&gt; DataFrame:\n        \"\"\"\n        Generate the labeled tiles dataset for the specified label type. This function is generic and works for all\n        label types.\n\n        Args:\n            label_type (str): label type to compute: 'ue', 'home' or 'work'.\n            apply_ndays_filter (bool): Indicates if the final ndays frequency filter shall be applied in the computation\n            of the specified label type.\n\n        Returns:\n            DataFrame: labeled tiles dataset for the specified label type.\n        \"\"\"\n        ps_threshold = self.ps_thresholds[label_type]\n        gap_ps_threshold = self.gap_ps_thresholds[label_type]\n        gap_ps_threshold_is_absolute = self.gap_ps_threshold_is_absolute[label_type]\n        ndays_threshold = self.freq_thresholds[label_type]\n        day_and_interval_combinations = self.day_and_interval_type_combinations[label_type]\n\n        selected_tiles_dfs_list = []\n\n        ##### 1st STAGE #####\n        for i, (day_type, time_interval) in enumerate(day_and_interval_combinations):\n\n            # filter ltps df for the current day type and time interval combination:\n            ltps_df_i = self.ltps_df.filter(\n                (F.col(ColNames.day_type) == day_type) &amp; (F.col(ColNames.time_interval) == time_interval)\n            )\n\n            # if first day_type, time_interval combination:\n            if i == 0:\n                # cut tiles at gap to generate preselected tiles\n                tiles_before_gap_df = self.cut_tiles_at_gap(ltps_df_i, gap_ps_threshold, gap_ps_threshold_is_absolute)\n                preselected_tiles_df = tiles_before_gap_df\n\n            # rest of day_type, time_interval combinations:\n            else:\n                # reach not selected tiles from previous iteration to generate preselected tiles\n                preselected_tiles_df = ltps_df_i.join(\n                    not_selected_tiles_df.select(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id),\n                    on=[ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id],\n                )\n\n            # add abs_ps_threshold column to dataframe:\n            preselected_tiles_df = self.calculate_device_abs_ps_threshold(ltps_df_i, preselected_tiles_df, ps_threshold)\n\n            # apply relative lps filter to obtain selected tiles and not selected tiles:\n            selected_tiles_df, not_selected_tiles_df = self.filter_by_ps(preselected_tiles_df)\n\n            # format current selected tiles df and add to list:\n            n_rule = 1 if i == 0 else 2\n            selected_tiles_df = self.format_selected_tiles(selected_tiles_df, label_type, n_rule)\n            selected_tiles_dfs_list.append(selected_tiles_df)\n\n        ##### 2nd STAGE #####\n        if apply_ndays_filter is True:\n\n            # filter ltps df for the first day type and time interval combination:\n            first_day_type, first_time_interval = day_and_interval_combinations[0]\n            ltps_df_i = self.ltps_df.filter(\n                (F.col(ColNames.day_type) == first_day_type) &amp; (F.col(ColNames.time_interval) == first_time_interval)\n            )\n\n            # add abs_ndays_threshold column to dataframe:\n            preselected_tiles_df = self.calculate_device_abs_ndays_threshold(\n                ltps_df_i, not_selected_tiles_df, ndays_threshold\n            )\n\n            # apply relative lps filter to obtain selected tiles and not selected tiles:\n            selected_tiles_df, not_selected_tiles_df = self.filter_by_ndays(preselected_tiles_df)\n\n            # format current selected tiles df and add to list:\n            n_rule = 2 if len(day_and_interval_combinations) == 1 else 3\n            selected_tiles_df = self.format_selected_tiles(selected_tiles_df, label_type, n_rule)\n            selected_tiles_dfs_list.append(selected_tiles_df)\n\n        ##### Concatenate to generate final dataset for this label type #####\n        # format not selected tiles df and add to list\n        not_selected_tiles_df = self.format_not_selected_tiles(not_selected_tiles_df, label_type)\n        selected_tiles_dfs_list.append(not_selected_tiles_df)\n\n        # concatenate all selected and not selected\n        labeled_tiles_df = reduce(DataFrame.unionAll, selected_tiles_dfs_list)\n\n        # select columns for output\n        base_columns = [\n            ColNames.user_id,\n            ColNames.grid_id,\n            ColNames.user_id_modulo,\n        ]\n        if label_type == \"ue\":\n            columns = base_columns + [ColNames.ue_label_rule]\n        else:\n            columns = base_columns + [ColNames.label, ColNames.location_label_rule]\n\n        labeled_tiles_df = labeled_tiles_df.select(columns)\n\n        return labeled_tiles_df\n\n    def compute_ue_labeling(self) -&gt; DataFrame:\n        \"\"\"\n        Generate dataset with the tiles that conform the UE (Usual Environment) of each device.\n\n        Returns:\n            DataFrame: UE tiles dataset.\n        \"\"\"\n        ue_tiles_df = self.compute_generic_labeling(label_type=\"ue\", apply_ndays_filter=False)\n        return ue_tiles_df\n\n    def compute_home_labeling(self) -&gt; DataFrame:\n        \"\"\"\n        Generate dataset with the tiles that conform the home location of each device.\n\n        Returns:\n            DataFrame: home tiles dataset.\n        \"\"\"\n        home_tiles_df = self.compute_generic_labeling(label_type=\"home\", apply_ndays_filter=True)\n        return home_tiles_df\n\n    def compute_work_labeling(self) -&gt; DataFrame:\n        \"\"\"\n        Generate dataset with the tiles that conform the work location of each device.\n\n        Returns:\n            DataFrame: work tiles dataset.\n        \"\"\"\n        work_tiles_df = self.compute_generic_labeling(label_type=\"work\", apply_ndays_filter=True)\n        return work_tiles_df\n\n    def concatenate_labeled_tiles(\n        self, ue_tiles_df: DataFrame, home_tiles_df: DataFrame, work_tiles_df: DataFrame\n    ) -&gt; DataFrame:\n        \"\"\"\n        Concatenate UE tiles, home tiles and work tiles datasets into a unique dataset.\n\n        Args:\n            ue_tiles_df (DataFrame): UE tiles dataset.\n            home_tiles_df (DataFrame): home tiles dataset.\n            work_tiles_df (DataFrame): work tiles dataset.\n\n        Returns:\n            DataFrame: Usual Environment Labels dataframe.\n        \"\"\"\n        # just concatenate home and work tiles:\n        loc_tiles_df = home_tiles_df.union(work_tiles_df)\n\n        labels_df = (\n            loc_tiles_df.join(\n                ue_tiles_df,\n                on=[ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id],\n                how=\"outer\",\n            )\n            .select(\n                F.col(ColNames.user_id_modulo),\n                F.col(ColNames.user_id),\n                F.col(ColNames.grid_id),\n                F.col(ColNames.label),\n                F.col(ColNames.ue_label_rule),\n                F.col(ColNames.location_label_rule),\n            )\n            .fillna(\n                {ColNames.label: \"no_label\", ColNames.ue_label_rule: \"ue_na\", ColNames.location_label_rule: \"loc_na\"}\n            )\n        )\n\n        labels_df = (\n            labels_df.withColumn(ColNames.season, F.lit(self.season))\n            .withColumn(ColNames.start_date, F.lit(self.start_date))\n            .withColumn(ColNames.end_date, F.lit(self.end_date))\n        )\n\n        return labels_df\n\n    @staticmethod\n    def get_rule_count(df: DataFrame, col_to_value: dict[str, str]) -&gt; int:\n        \"\"\"\n        Sums the count column values of the given dataframe for the corresponding filter (col_to_value).\n\n        Args:\n            df (DataFrame): input dataframe.\n            col_to_value (dict[str, str]): filter to apply.\n\n        Returns:\n            int: count column sum.\n        \"\"\"\n        conditions = (F.col(colname) == colvalue for colname, colvalue in col_to_value.items())\n        combined_condition = reduce(lambda x, y: x &amp; y, conditions)\n        count_value = df.filter(combined_condition).agg(F.sum(\"count\")).collect()[0][0]\n        if count_value is None:\n            count_value = 0\n        return count_value\n\n    def generate_quality_metrics(self, ue_labels_df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Build usual environment labeling quality metrics dataframe.\n\n        Args:\n            ue_labels_df (DataFrame): usual environment labels dataframe.\n\n        Returns:\n            DataFrame: quality metrics dataframe.\n        \"\"\"\n        grouped_df = ue_labels_df.groupby(ColNames.label, ColNames.ue_label_rule, ColNames.location_label_rule).count()\n        grouped_df.cache()\n\n        ue_1_rule_count = self.get_rule_count(grouped_df, {ColNames.ue_label_rule: \"ue_1\"})\n        ue_2_rule_count = self.get_rule_count(grouped_df, {ColNames.ue_label_rule: \"ue_2\"})\n        ue_3_rule_count = self.get_rule_count(grouped_df, {ColNames.ue_label_rule: \"ue_3\"})\n        h_1_rule_count = self.get_rule_count(grouped_df, {ColNames.location_label_rule: \"h_1\"})\n        h_2_rule_count = self.get_rule_count(grouped_df, {ColNames.location_label_rule: \"h_2\"})\n        h_3_rule_count = self.get_rule_count(grouped_df, {ColNames.location_label_rule: \"h_3\"})\n        w_1_rule_count = self.get_rule_count(grouped_df, {ColNames.location_label_rule: \"w_1\"})\n        w_2_rule_count = self.get_rule_count(grouped_df, {ColNames.location_label_rule: \"w_2\"})\n        w_3_rule_count = self.get_rule_count(grouped_df, {ColNames.location_label_rule: \"w_3\"})\n        ue_na_rule_count = self.get_rule_count(grouped_df, {ColNames.ue_label_rule: \"ue_na\"})\n        loc_na_rule_count = self.get_rule_count(grouped_df, {ColNames.location_label_rule: \"loc_na\"})\n        h_non_ue_count = self.get_rule_count(grouped_df, {ColNames.label: \"home\", ColNames.ue_label_rule: \"ue_na\"})\n        w_non_ue_count = self.get_rule_count(grouped_df, {ColNames.label: \"work\", ColNames.ue_label_rule: \"ue_na\"})\n\n        grouped_df.unpersist()\n\n        data = [\n            (\"device_filter_1_rule\", self.rare_devices_count, self.start_date, self.end_date, self.season),\n            (\"device_filter_2_rule\", self.discontinuous_devices_count, self.start_date, self.end_date, self.season),\n            (\"ue_1_rule\", ue_1_rule_count, self.start_date, self.end_date, self.season),\n            (\"ue_2_rule\", ue_2_rule_count, self.start_date, self.end_date, self.season),\n            (\"ue_3_rule\", ue_3_rule_count, self.start_date, self.end_date, self.season),\n            (\"h_1_rule\", h_1_rule_count, self.start_date, self.end_date, self.season),\n            (\"h_2_rule\", h_2_rule_count, self.start_date, self.end_date, self.season),\n            (\"h_3_rule\", h_3_rule_count, self.start_date, self.end_date, self.season),\n            (\"w_1_rule\", w_1_rule_count, self.start_date, self.end_date, self.season),\n            (\"w_2_rule\", w_2_rule_count, self.start_date, self.end_date, self.season),\n            (\"w_3_rule\", w_3_rule_count, self.start_date, self.end_date, self.season),\n            (\"ue_na_rule\", ue_na_rule_count, self.start_date, self.end_date, self.season),\n            (\"loc_na_rule\", loc_na_rule_count, self.start_date, self.end_date, self.season),\n            (\"h_non_ue\", h_non_ue_count, self.start_date, self.end_date, self.season),\n            (\"w_non_ue\", w_non_ue_count, self.start_date, self.end_date, self.season),\n        ]\n\n        # Create DataFrame\n        schema = SilverUsualEnvironmentLabelingQualityMetricsDataObject.SCHEMA\n        quality_metrics_df = self.spark.createDataFrame(data, schema)\n\n        return quality_metrics_df\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n\n        # discard devices that will not be analysed ('rarely observed' or 'discontinuously observed'):\n        self.ltps_df = self.discard_devices(self.ltps_df)\n\n        # calculate tiles that belong to the ue (usual environment) of each device:\n        ue_tiles_df = self.compute_ue_labeling()\n\n        # calculate tiles that belong to the home location of each device:\n        home_tiles_df = self.compute_home_labeling()\n\n        # calculate tiles that belong to the work location of each device:\n        work_tiles_df = self.compute_work_labeling()\n\n        # join ue tiles, home tiles and work tiles datasets into a ue labels dataset:\n        labeled_tiles_df = self.concatenate_labeled_tiles(ue_tiles_df, home_tiles_df, work_tiles_df)\n\n        # generate labeling quality metrics dataset:\n        labeling_quality_metrics_df = self.generate_quality_metrics(labeled_tiles_df)\n\n        # save objects to output data dict:\n        self.output_data_objects[SilverUsualEnvironmentLabelsDataObject.ID].df = labeled_tiles_df\n        self.output_data_objects[SilverUsualEnvironmentLabelingQualityMetricsDataObject.ID].df = (\n            labeling_quality_metrics_df\n        )\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.add_abs_ps_threshold","title":"<code>add_abs_ps_threshold(ltps_df, window, gap_ps_threshold, threshold_is_absolute)</code>  <code>staticmethod</code>","text":"<p>Add \"abs_ps_threshold\" field to the ltps dataset.</p> <p>If \"threshold_is_absolute\" is True, then just assign \"abs_ps_threshold\" = \"gap_ps_threshold\" value to all registers.</p> <p>If threshold_is_absolute is False, then reach the maximum value of lps for a grid tile for each device (ps_max) and calculate \"abs_ps_threshold\" = \"gap_ps_threshold\" * \"ps_max\".</p> <p>Parameters:</p> Name Type Description Default <code>ltps_df</code> <code>DataFrame</code> <p>Long-Term Permanence Score dataset.</p> required <code>window</code> <code>Window</code> <p>window, partitioned by user id and ordered by lps.</p> required <code>gap_ps_threshold</code> <code>int | float</code> <p>absolute/relative lps threshold.</p> required <code>threshold_is_absolute</code> <code>bool</code> <p>indicates if gap_ps_threshold is a relative/absolute value.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Long-Term Permanence Score dataset with the additional \"abs_ps_threshold\" column.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>@staticmethod\ndef add_abs_ps_threshold(\n    ltps_df: DataFrame, window: Window, gap_ps_threshold: int | float, threshold_is_absolute: bool\n) -&gt; DataFrame:\n    \"\"\"\n    Add \"abs_ps_threshold\" field to the ltps dataset.\n\n    If \"threshold_is_absolute\" is True, then just assign \"abs_ps_threshold\" = \"gap_ps_threshold\"\n    value to all registers.\n\n    If threshold_is_absolute is False, then reach the maximum value of lps for a grid tile for\n    each device (ps_max) and calculate \"abs_ps_threshold\" = \"gap_ps_threshold\" * \"ps_max\".\n\n    Args:\n        ltps_df (DataFrame): Long-Term Permanence Score dataset.\n        window (Window): window, partitioned by user id and ordered by lps.\n        gap_ps_threshold (int|float): absolute/relative lps threshold.\n        threshold_is_absolute (bool): indicates if gap_ps_threshold is a relative/absolute value.\n\n    Returns:\n        DataFrame: Long-Term Permanence Score dataset with the additional \"abs_ps_threshold\"\n            column.\n    \"\"\"\n    if threshold_is_absolute is True:\n        ltps_df = ltps_df.withColumn(\"abs_ps_threshold\", F.lit(gap_ps_threshold))\n    else:\n        ltps_df = (\n            ltps_df.withColumn(\"ps_max\", F.first(ColNames.lps).over(window))\n            .withColumn(\"abs_ps_threshold\", F.col(\"ps_max\") * F.lit(gap_ps_threshold / 100))\n            .drop(\"ps_max\")\n        )\n    return ltps_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.calculate_device_abs_ndays_threshold","title":"<code>calculate_device_abs_ndays_threshold(ltps_df_i, target_rows_ltps_df, perc_ndays_threshold)</code>  <code>staticmethod</code>","text":"<p>Calculate the total frequency (observed number of days) for each device (total_device_ndays) for a given day type and time interval and add this information to an additional column of the provided dataset. Then, based on this column, generate the absolute ndays threshold to consider for each device by applying the corresponding configured percentage (perc_ndays_threshold).</p> <p>Parameters:</p> Name Type Description Default <code>ltps_df_i</code> <code>DataFrame</code> <p>Long-Term Permanence Metrics dataset, for a specific day type and time interval combination.</p> required <code>target_rows_ltps_df</code> <code>DataFrame</code> <p>Long-Term Permanence Metrics dataset, for a specific day type and time interval combination, with one 'id_type' == 'grid'.</p> required <code>perc_ndays_threshold</code> <code>float</code> <p>specified ndays threshold (in percentage).</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Long-Term Permanence Metrics dataset, for a specific day type and time interval combination, with one 'id_type' == 'grid', with an additional column named \"abs_ndays_threshold\".</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>@staticmethod\ndef calculate_device_abs_ndays_threshold(\n    ltps_df_i: DataFrame, target_rows_ltps_df: DataFrame, perc_ndays_threshold: float\n) -&gt; DataFrame:\n    \"\"\"\n    Calculate the total frequency (observed number of days) for each device (total_device_ndays) for a given day\n    type and time interval and add this information to an additional column of the provided dataset. Then, based on\n    this column, generate the absolute ndays threshold to consider for each device by applying the corresponding\n    configured percentage (perc_ndays_threshold).\n\n    Args:\n        ltps_df_i (DataFrame): Long-Term Permanence Metrics dataset, for a specific day type and time interval\n            combination.\n        target_rows_ltps_df (DataFrame): Long-Term Permanence Metrics dataset, for a specific day type and time\n            interval combination, with one 'id_type' == 'grid'.\n        perc_ndays_threshold (float): specified ndays threshold (in percentage).\n\n    Returns:\n        DataFrame: Long-Term Permanence Metrics dataset, for a specific day type and time interval combination,\n            with one 'id_type' == 'grid', with an additional column named \"abs_ndays_threshold\".\n    \"\"\"\n    device_total_ndays_df = (\n        ltps_df_i.filter(F.col(ColNames.id_type) == \"device_observation\")\n        .withColumnRenamed(ColNames.total_frequency, \"total_device_ndays\")\n        .select(ColNames.user_id_modulo, ColNames.user_id, \"total_device_ndays\")\n    )\n\n    target_rows_ltps_df = (\n        target_rows_ltps_df.join(device_total_ndays_df, on=[ColNames.user_id_modulo, ColNames.user_id])\n        .withColumn(\"abs_ndays_threshold\", F.col(\"total_device_ndays\") * F.lit(perc_ndays_threshold / 100))\n        .drop(\"total_device_ndays\")\n    )\n\n    return target_rows_ltps_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.calculate_device_abs_ps_threshold","title":"<code>calculate_device_abs_ps_threshold(ltps_df_i, target_rows_ltps_df, perc_ps_threshold)</code>  <code>staticmethod</code>","text":"<p>Calculate the total assigned long-term permanence score for each device (total_device_ps) for a given day type and time interval and add this information to an additional column of the provided dataset. Then, based on this column, generate the absolute ps threshold to consider for each device by applying the corresponding configured percentage (perc_ps_threshold).</p> <p>Parameters:</p> Name Type Description Default <code>ltps_df_i</code> <code>DataFrame</code> <p>Long-Term Permanence Metrics dataset, for a specific day type and time interval combination.</p> required <code>target_rows_ltps_df</code> <code>DataFrame</code> <p>Long-Term Permanence Metrics dataset, for a specific day type and time interval combination, with one 'id_type' == 'grid'.</p> required <code>perc_ps_threshold</code> <code>float</code> <p>specified ps threshold (in percentage).</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Long-Term Permanence Metrics dataset, for a specific day type and time interval combination, with one 'id_type' == 'grid', with an additional column named \"abs_ps_threshold\".</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>@staticmethod\ndef calculate_device_abs_ps_threshold(\n    ltps_df_i: DataFrame, target_rows_ltps_df: DataFrame, perc_ps_threshold: float\n) -&gt; DataFrame:\n    \"\"\"\n    Calculate the total assigned long-term permanence score for each device (total_device_ps) for a given day type\n    and time interval and add this information to an additional column of the provided dataset. Then, based on\n    this column, generate the absolute ps threshold to consider for each device by applying the corresponding\n    configured percentage (perc_ps_threshold).\n\n    Args:\n        ltps_df_i (DataFrame): Long-Term Permanence Metrics dataset, for a specific day type and time interval\n            combination.\n        target_rows_ltps_df (DataFrame): Long-Term Permanence Metrics dataset, for a specific day type and time\n            interval combination, with one 'id_type' == 'grid'.\n        perc_ps_threshold (float): specified ps threshold (in percentage).\n\n    Returns:\n        DataFrame: Long-Term Permanence Metrics dataset, for a specific day type and time interval combination,\n            with one 'id_type' == 'grid', with an additional column named \"abs_ps_threshold\".\n    \"\"\"\n    device_total_ps_df = (\n        ltps_df_i.filter(F.col(ColNames.id_type) == \"device_observation\")\n        .withColumnRenamed(ColNames.lps, \"total_device_ps\")\n        .select(ColNames.user_id_modulo, ColNames.user_id, \"total_device_ps\")\n    )\n\n    target_rows_ltps_df = (\n        target_rows_ltps_df.join(device_total_ps_df, on=[ColNames.user_id_modulo, ColNames.user_id])\n        .withColumn(\"abs_ps_threshold\", F.col(\"total_device_ps\") * F.lit(perc_ps_threshold / 100))\n        .drop(\"total_device_ps\")\n    )\n\n    return target_rows_ltps_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.check_needed_day_and_interval_types","title":"<code>check_needed_day_and_interval_types(ltps_df, day_and_interval_type_combinations)</code>  <code>staticmethod</code>","text":"<p>Method that checks if the needed combinations of day type and interval type are available in the provided dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ltps_df</code> <code>DataFrame</code> <p>provided Long-Term Permanence Score dataset</p> required <code>day_and_interval_type_combinations</code> <code>dict[str, list[tuple[str, str]]]</code> <p>day type and interval type combinations that are needed for the execution of the method.</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If there is no data for one or more of the needed combinations of day type and interval type.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>@staticmethod\ndef check_needed_day_and_interval_types(\n    ltps_df: DataFrame, day_and_interval_type_combinations: dict[str, list[tuple[str, str]]]\n):\n    \"\"\"\n    Method that checks if the needed combinations of day type and interval type are available\n    in the provided dataset.\n\n    Args:\n        ltps_df (DataFrame): provided Long-Term Permanence Score dataset\n        day_and_interval_type_combinations (dict[str,list[tuple[str,str]]]): day type and interval type\n            combinations that are needed for the execution of the method.\n\n    Raises:\n        FileNotFoundError: If there is no data for one or more of the needed combinations of day type\n            and interval type.\n    \"\"\"\n    # Build set of all needed day-interval type combinations:\n    all_day_and_interval_type_combinations = {\n        comb for el in day_and_interval_type_combinations.values() for comb in el\n    }\n\n    # Assert that these combinations appear at least once in the input Long-Term Permanence\n    # Score data object:\n    for day_type, time_interval in all_day_and_interval_type_combinations:\n\n        filtered_ltps_df = ltps_df.filter(\n            (F.col(ColNames.day_type) == day_type) &amp; (F.col(ColNames.time_interval) == time_interval)\n        )\n\n        data_exists = filtered_ltps_df.count() &gt; 0\n        if not data_exists:\n            raise FileNotFoundError(\n                \"No Long-term Permanence Score data has been found for \"\n                f\"day_type `{day_type}` and time_interval `{time_interval}`\"\n            )\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.compute_generic_labeling","title":"<code>compute_generic_labeling(label_type, apply_ndays_filter)</code>","text":"<p>Generate the labeled tiles dataset for the specified label type. This function is generic and works for all label types.</p> <p>Parameters:</p> Name Type Description Default <code>label_type</code> <code>str</code> <p>label type to compute: 'ue', 'home' or 'work'.</p> required <code>apply_ndays_filter</code> <code>bool</code> <p>Indicates if the final ndays frequency filter shall be applied in the computation</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>labeled tiles dataset for the specified label type.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>def compute_generic_labeling(self, label_type: str, apply_ndays_filter: bool) -&gt; DataFrame:\n    \"\"\"\n    Generate the labeled tiles dataset for the specified label type. This function is generic and works for all\n    label types.\n\n    Args:\n        label_type (str): label type to compute: 'ue', 'home' or 'work'.\n        apply_ndays_filter (bool): Indicates if the final ndays frequency filter shall be applied in the computation\n        of the specified label type.\n\n    Returns:\n        DataFrame: labeled tiles dataset for the specified label type.\n    \"\"\"\n    ps_threshold = self.ps_thresholds[label_type]\n    gap_ps_threshold = self.gap_ps_thresholds[label_type]\n    gap_ps_threshold_is_absolute = self.gap_ps_threshold_is_absolute[label_type]\n    ndays_threshold = self.freq_thresholds[label_type]\n    day_and_interval_combinations = self.day_and_interval_type_combinations[label_type]\n\n    selected_tiles_dfs_list = []\n\n    ##### 1st STAGE #####\n    for i, (day_type, time_interval) in enumerate(day_and_interval_combinations):\n\n        # filter ltps df for the current day type and time interval combination:\n        ltps_df_i = self.ltps_df.filter(\n            (F.col(ColNames.day_type) == day_type) &amp; (F.col(ColNames.time_interval) == time_interval)\n        )\n\n        # if first day_type, time_interval combination:\n        if i == 0:\n            # cut tiles at gap to generate preselected tiles\n            tiles_before_gap_df = self.cut_tiles_at_gap(ltps_df_i, gap_ps_threshold, gap_ps_threshold_is_absolute)\n            preselected_tiles_df = tiles_before_gap_df\n\n        # rest of day_type, time_interval combinations:\n        else:\n            # reach not selected tiles from previous iteration to generate preselected tiles\n            preselected_tiles_df = ltps_df_i.join(\n                not_selected_tiles_df.select(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id),\n                on=[ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id],\n            )\n\n        # add abs_ps_threshold column to dataframe:\n        preselected_tiles_df = self.calculate_device_abs_ps_threshold(ltps_df_i, preselected_tiles_df, ps_threshold)\n\n        # apply relative lps filter to obtain selected tiles and not selected tiles:\n        selected_tiles_df, not_selected_tiles_df = self.filter_by_ps(preselected_tiles_df)\n\n        # format current selected tiles df and add to list:\n        n_rule = 1 if i == 0 else 2\n        selected_tiles_df = self.format_selected_tiles(selected_tiles_df, label_type, n_rule)\n        selected_tiles_dfs_list.append(selected_tiles_df)\n\n    ##### 2nd STAGE #####\n    if apply_ndays_filter is True:\n\n        # filter ltps df for the first day type and time interval combination:\n        first_day_type, first_time_interval = day_and_interval_combinations[0]\n        ltps_df_i = self.ltps_df.filter(\n            (F.col(ColNames.day_type) == first_day_type) &amp; (F.col(ColNames.time_interval) == first_time_interval)\n        )\n\n        # add abs_ndays_threshold column to dataframe:\n        preselected_tiles_df = self.calculate_device_abs_ndays_threshold(\n            ltps_df_i, not_selected_tiles_df, ndays_threshold\n        )\n\n        # apply relative lps filter to obtain selected tiles and not selected tiles:\n        selected_tiles_df, not_selected_tiles_df = self.filter_by_ndays(preselected_tiles_df)\n\n        # format current selected tiles df and add to list:\n        n_rule = 2 if len(day_and_interval_combinations) == 1 else 3\n        selected_tiles_df = self.format_selected_tiles(selected_tiles_df, label_type, n_rule)\n        selected_tiles_dfs_list.append(selected_tiles_df)\n\n    ##### Concatenate to generate final dataset for this label type #####\n    # format not selected tiles df and add to list\n    not_selected_tiles_df = self.format_not_selected_tiles(not_selected_tiles_df, label_type)\n    selected_tiles_dfs_list.append(not_selected_tiles_df)\n\n    # concatenate all selected and not selected\n    labeled_tiles_df = reduce(DataFrame.unionAll, selected_tiles_dfs_list)\n\n    # select columns for output\n    base_columns = [\n        ColNames.user_id,\n        ColNames.grid_id,\n        ColNames.user_id_modulo,\n    ]\n    if label_type == \"ue\":\n        columns = base_columns + [ColNames.ue_label_rule]\n    else:\n        columns = base_columns + [ColNames.label, ColNames.location_label_rule]\n\n    labeled_tiles_df = labeled_tiles_df.select(columns)\n\n    return labeled_tiles_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.compute_home_labeling","title":"<code>compute_home_labeling()</code>","text":"<p>Generate dataset with the tiles that conform the home location of each device.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>home tiles dataset.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>def compute_home_labeling(self) -&gt; DataFrame:\n    \"\"\"\n    Generate dataset with the tiles that conform the home location of each device.\n\n    Returns:\n        DataFrame: home tiles dataset.\n    \"\"\"\n    home_tiles_df = self.compute_generic_labeling(label_type=\"home\", apply_ndays_filter=True)\n    return home_tiles_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.compute_ue_labeling","title":"<code>compute_ue_labeling()</code>","text":"<p>Generate dataset with the tiles that conform the UE (Usual Environment) of each device.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>UE tiles dataset.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>def compute_ue_labeling(self) -&gt; DataFrame:\n    \"\"\"\n    Generate dataset with the tiles that conform the UE (Usual Environment) of each device.\n\n    Returns:\n        DataFrame: UE tiles dataset.\n    \"\"\"\n    ue_tiles_df = self.compute_generic_labeling(label_type=\"ue\", apply_ndays_filter=False)\n    return ue_tiles_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.compute_work_labeling","title":"<code>compute_work_labeling()</code>","text":"<p>Generate dataset with the tiles that conform the work location of each device.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>work tiles dataset.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>def compute_work_labeling(self) -&gt; DataFrame:\n    \"\"\"\n    Generate dataset with the tiles that conform the work location of each device.\n\n    Returns:\n        DataFrame: work tiles dataset.\n    \"\"\"\n    work_tiles_df = self.compute_generic_labeling(label_type=\"work\", apply_ndays_filter=True)\n    return work_tiles_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.concatenate_labeled_tiles","title":"<code>concatenate_labeled_tiles(ue_tiles_df, home_tiles_df, work_tiles_df)</code>","text":"<p>Concatenate UE tiles, home tiles and work tiles datasets into a unique dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ue_tiles_df</code> <code>DataFrame</code> <p>UE tiles dataset.</p> required <code>home_tiles_df</code> <code>DataFrame</code> <p>home tiles dataset.</p> required <code>work_tiles_df</code> <code>DataFrame</code> <p>work tiles dataset.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Usual Environment Labels dataframe.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>def concatenate_labeled_tiles(\n    self, ue_tiles_df: DataFrame, home_tiles_df: DataFrame, work_tiles_df: DataFrame\n) -&gt; DataFrame:\n    \"\"\"\n    Concatenate UE tiles, home tiles and work tiles datasets into a unique dataset.\n\n    Args:\n        ue_tiles_df (DataFrame): UE tiles dataset.\n        home_tiles_df (DataFrame): home tiles dataset.\n        work_tiles_df (DataFrame): work tiles dataset.\n\n    Returns:\n        DataFrame: Usual Environment Labels dataframe.\n    \"\"\"\n    # just concatenate home and work tiles:\n    loc_tiles_df = home_tiles_df.union(work_tiles_df)\n\n    labels_df = (\n        loc_tiles_df.join(\n            ue_tiles_df,\n            on=[ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id],\n            how=\"outer\",\n        )\n        .select(\n            F.col(ColNames.user_id_modulo),\n            F.col(ColNames.user_id),\n            F.col(ColNames.grid_id),\n            F.col(ColNames.label),\n            F.col(ColNames.ue_label_rule),\n            F.col(ColNames.location_label_rule),\n        )\n        .fillna(\n            {ColNames.label: \"no_label\", ColNames.ue_label_rule: \"ue_na\", ColNames.location_label_rule: \"loc_na\"}\n        )\n    )\n\n    labels_df = (\n        labels_df.withColumn(ColNames.season, F.lit(self.season))\n        .withColumn(ColNames.start_date, F.lit(self.start_date))\n        .withColumn(ColNames.end_date, F.lit(self.end_date))\n    )\n\n    return labels_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.cut_tiles_at_gap","title":"<code>cut_tiles_at_gap(ltps_df, gap_ps_threshold, threshold_is_absolute)</code>","text":"<p>Preprocessing function. Given a Long-Term Permanence Metrics dataset, for each user: - sort the grid tiles of the user by LPS - find a high difference (gap) in the LPS values between one grid tile and the next one,   where what is a high difference is defined through the gap_ps_threshold argument. - for each agent, filter out all tiles after this high difference: the remaining tiles are   the \"pre-selected tiles\", which are the output of this function.</p> <p>Parameters:</p> Name Type Description Default <code>ltps_df</code> <code>DataFrame</code> <p>Long-Term Permanence Metrics dataset, for a specific day type and time interval combination.</p> required <code>gap_ps_threshold</code> <code>int / float</code> <p>absolute/relative lps threshold.</p> required <code>threshold_is_absolute</code> <code>bool</code> <p>indicates if gap_ps_threshold is a relative/absolute value.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>dataset with the pre-selected tiles.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>def cut_tiles_at_gap(self, ltps_df: DataFrame, gap_ps_threshold: float, threshold_is_absolute: bool) -&gt; DataFrame:\n    \"\"\"\n    Preprocessing function. Given a Long-Term Permanence Metrics dataset, for each user:\n    - sort the grid tiles of the user by LPS\n    - find a high difference (gap) in the LPS values between one grid tile and the next one,\n      where what is a high difference is defined through the gap_ps_threshold argument.\n    - for each agent, filter out all tiles after this high difference: the remaining tiles are\n      the \"pre-selected tiles\", which are the output of this function.\n\n    Args:\n        ltps_df (DataFrame): Long-Term Permanence Metrics dataset, for a specific day type and\n            time interval combination.\n        gap_ps_threshold (int/float): absolute/relative lps threshold.\n        threshold_is_absolute (bool): indicates if gap_ps_threshold is a relative/absolute value.\n\n    Returns:\n        DataFrame: dataset with the pre-selected tiles.\n    \"\"\"\n    ltps_df = ltps_df.filter(F.col(ColNames.id_type) == \"grid\")\n\n    window = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id).orderBy(F.desc(ColNames.lps))\n    cumulative_window = window.rowsBetween(Window.unboundedPreceding, Window.currentRow)\n\n    ltps_df = self.add_abs_ps_threshold(ltps_df, window, gap_ps_threshold, threshold_is_absolute)\n\n    pre_selected_tiles_df = (\n        ltps_df.withColumn(\"previous_lps\", F.lag(ColNames.lps, 1).over(window))\n        .withColumn(\"lps_difference\", F.col(\"previous_lps\") - F.col(ColNames.lps))\n        .fillna({\"lps_difference\": 0})\n        .withColumn(\"high_difference\", F.when(F.col(\"lps_difference\") &gt;= F.col(\"abs_ps_threshold\"), 1).otherwise(0))\n        .withColumn(\"cumulative_condition\", F.sum(F.col(\"high_difference\")).over(cumulative_window))\n        .filter(F.col(\"cumulative_condition\") == F.lit(0))\n        .drop(\"lps_difference\", \"previous_lps\", \"high_difference\", \"cumulative_condition\", \"abs_ps_threshold\")\n    )\n\n    return pre_selected_tiles_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.discard_devices","title":"<code>discard_devices(ltps_df)</code>","text":"<p>Given a Long-Term Permanence Score dataset (filtered for the corresponding start and end dates), discard the rows corresponding to some devices.</p> There are 2 type of devices to discard <ul> <li>rarely observed devices, based on LPS (long-term permanence score).</li> <li>discontinuously observed devices, based on frequency.</li> </ul> <p>The user ids that are classified in any of these 2 groups are discarded from the Long-Term Permanence Score dataset, and the number of discarded users of each kind is saved to the corresponding attributes.</p> <p>Parameters:</p> Name Type Description Default <code>ltps_df</code> <code>DataFrame</code> <p>Long-Term Permanence Score dataset (filtered for the corresponding start and end dates).</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Long-Term Permanence Score dataset (filtered for the corresponding start and end dates), without the rows associated to rarely or discontinuously observed devices.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>def discard_devices(self, ltps_df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Given a Long-Term Permanence Score dataset (filtered for the corresponding start and end dates),\n    discard the rows corresponding to some devices.\n\n    There are 2 type of devices to discard:\n        - rarely observed devices, based on LPS (long-term permanence score).\n        - discontinuously observed devices, based on frequency.\n\n    The user ids that are classified in any of these 2 groups are discarded from the Long-Term\n    Permanence Score dataset, and the number of discarded users of each kind is saved to the\n    corresponding attributes.\n\n    Args:\n        ltps_df (DataFrame): Long-Term Permanence Score dataset (filtered for the corresponding\n            start and end dates).\n\n    Returns:\n        DataFrame: Long-Term Permanence Score dataset (filtered for the corresponding start and\n            end dates), without the rows associated to rarely or discontinuously observed\n            devices.\n    \"\"\"\n    # Initial filter of ltps dataset to keep total device observation values:\n    total_observations_df = ltps_df.filter(\n        (F.col(ColNames.id_type) == \"device_observation\")\n        &amp; (F.col(ColNames.day_type) == \"all\")\n        &amp; (F.col(ColNames.time_interval) == \"all\")\n    )\n\n    # Rarely observed:\n    rarely_observed_user_ids = self.find_rarely_observed_devices(\n        total_observations_df, self.ps_threshold_for_rare_devices\n    )\n    self.rare_devices_count = rarely_observed_user_ids.count()\n\n    # Discontinuously observed:\n    discontinuously_observed_user_ids = self.find_discontinuously_observed_devices(\n        total_observations_df, self.freq_threshold_for_discontinuous_devices\n    )\n    self.discontinuous_devices_count = discontinuously_observed_user_ids.count()\n\n    # All user ids to discard:\n    discardable_user_ids = rarely_observed_user_ids.union(discontinuously_observed_user_ids)\n\n    # Filter dataset:\n    filtered_ltps_df = self.discard_user_ids(ltps_df, discardable_user_ids)\n\n    return filtered_ltps_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.discard_user_ids","title":"<code>discard_user_ids(ltps_df, user_ids)</code>  <code>staticmethod</code>","text":"<p>Given a list of user ids, discard them from a Long-Term Permanence Score dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ltps_df</code> <code>DataFrame</code> <p>Long-Term Permanence Score dataset.</p> required <code>user_ids</code> <code>DataFrame</code> <p>one-column dataframe with the ids of the devices to discard.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>filtered Long-Term Permanence Score dataset.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>@staticmethod\ndef discard_user_ids(ltps_df: DataFrame, user_ids: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Given a list of user ids, discard them from a Long-Term Permanence Score dataset.\n\n    Args:\n        ltps_df (DataFrame): Long-Term Permanence Score dataset.\n        user_ids (DataFrame): one-column dataframe with the ids of the devices to discard.\n\n    Returns:\n        DataFrame: filtered Long-Term Permanence Score dataset.\n    \"\"\"\n    ltps_df = ltps_df.join(user_ids, on=[ColNames.user_id_modulo, ColNames.user_id], how=\"left_anti\")\n    return ltps_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.filter_by_ndays","title":"<code>filter_by_ndays(ltps_df)</code>  <code>staticmethod</code>","text":"<p>Apply the frequency filter (number of days) to a Long-Term Permanence Metrics dataset. Filter rows for which 'total_frequency' &gt; abs_ndays_threshold.</p> <p>Parameters:</p> Name Type Description Default <code>ltps_df</code> <code>DataFrame</code> <p>Long-Term Permanence Metrics dataset, for a specific day type and time interval combination.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Long-Term Permanence Metrics dataset, for a specific day type and time interval combination, keeping only the rows that DO pass the ndays filter.</p> <code>DataFrame</code> <code>DataFrame</code> <p>Long-Term Permanence Metrics dataset, for a specific day type and time interval combination, keeping only the rows that DO NOT pass the ndays filter.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>@staticmethod\ndef filter_by_ndays(ltps_df: DataFrame) -&gt; tuple[DataFrame, DataFrame]:\n    \"\"\"\n    Apply the frequency filter (number of days) to a Long-Term Permanence Metrics dataset. Filter rows\n    for which 'total_frequency' &gt; abs_ndays_threshold.\n\n    Args:\n        ltps_df (DataFrame): Long-Term Permanence Metrics dataset, for a specific day type and time interval\n            combination.\n\n    Returns:\n        DataFrame: Long-Term Permanence Metrics dataset, for a specific day type and time interval combination,\n            keeping only the rows that DO pass the ndays filter.\n        DataFrame: Long-Term Permanence Metrics dataset, for a specific day type and time interval combination,\n            keeping only the rows that DO NOT pass the ndays filter.\n    \"\"\"\n    common_df = ltps_df.withColumn(\n        \"selected_flag\",\n        F.when(F.col(ColNames.total_frequency) &gt;= F.col(\"abs_ndays_threshold\"), True).otherwise(False),\n    )\n\n    selected_tiles_df = common_df.filter(F.col(\"selected_flag\")).drop(\"abs_ndays_threshold\", \"selected_flag\")\n    not_selected_tiles_df = common_df.filter(~F.col(\"selected_flag\")).drop(\"abs_ndays_threshold\", \"selected_flag\")\n\n    return selected_tiles_df, not_selected_tiles_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.filter_by_ps","title":"<code>filter_by_ps(ltps_df)</code>  <code>staticmethod</code>","text":"<p>Apply the long-term permanence score filter (ps) to a Long-Term Permanence Metrics dataset. Filter rows for which 'lps' &gt; abs_ps_threshold.</p> <p>Parameters:</p> Name Type Description Default <code>ltps_df</code> <code>DataFrame</code> <p>Long-Term Permanence Metrics dataset, for a specific day type and time interval combination.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Long-Term Permanence Metrics dataset, for a specific day type and time interval combination, keeping only the rows that DO pass the ps filter.</p> <code>DataFrame</code> <code>DataFrame</code> <p>Long-Term Permanence Metrics dataset, for a specific day type and time interval combination, keeping only the rows that DO NOT pass the ps filter.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>@staticmethod\ndef filter_by_ps(ltps_df: DataFrame) -&gt; tuple[DataFrame, DataFrame]:\n    \"\"\"\n    Apply the long-term permanence score filter (ps) to a Long-Term Permanence Metrics dataset. Filter rows for\n    which 'lps' &gt; abs_ps_threshold.\n\n    Args:\n        ltps_df (DataFrame): Long-Term Permanence Metrics dataset, for a specific day type and time interval\n            combination.\n\n    Returns:\n        DataFrame: Long-Term Permanence Metrics dataset, for a specific day type and time interval combination,\n            keeping only the rows that DO pass the ps filter.\n        DataFrame: Long-Term Permanence Metrics dataset, for a specific day type and time interval combination,\n            keeping only the rows that DO NOT pass the ps filter.\n    \"\"\"\n    common_df = ltps_df.withColumn(\n        \"selected_flag\", F.when(F.col(ColNames.lps) &gt;= F.col(\"abs_ps_threshold\"), True).otherwise(False)\n    )\n\n    selected_tiles_df = common_df.filter(F.col(\"selected_flag\")).drop(\"abs_ps_threshold\", \"selected_flag\")\n    not_selected_tiles_df = common_df.filter(~F.col(\"selected_flag\")).drop(\"abs_ps_threshold\", \"selected_flag\")\n\n    return selected_tiles_df, not_selected_tiles_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.filter_ltps_by_target_dates","title":"<code>filter_ltps_by_target_dates(full_ltps_df, start_date, end_date, season)</code>  <code>staticmethod</code>","text":"<p>Keep only the rows of the input SilverLongtermPermanenceScoreDataObject in which the start date, end date and season match the ones specified for the processing of this method.</p> <p>Parameters:</p> Name Type Description Default <code>full_ltps_df</code> <code>DataFrame</code> <p>full dataset.</p> required <code>start_date</code> <code>date</code> <p>specified target start date for the execution of the method.</p> required <code>end_date</code> <code>date</code> <p>specified target end date for the execution of the method.</p> required <code>season</code> <code>str</code> <p>specified target season for the execution of the method.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>filtered dataset.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>@staticmethod\ndef filter_ltps_by_target_dates(\n    full_ltps_df: DataFrame, start_date: dt.date, end_date: dt.date, season: str\n) -&gt; DataFrame:\n    \"\"\"\n    Keep only the rows of the input SilverLongtermPermanenceScoreDataObject in which the start\n    date, end date and season match the ones specified for the processing of this method.\n\n    Args:\n        full_ltps_df (DataFrame): full dataset.\n        start_date (dt.date): specified target start date for the execution of the method.\n        end_date (dt.date): specified target end date for the execution of the method.\n        season (str): specified target season for the execution of the method.\n\n    Returns:\n        DataFrame: filtered dataset.\n    \"\"\"\n    filtered_ltps_df = full_ltps_df.filter(\n        (F.col(ColNames.start_date) == start_date)\n        &amp; (F.col(ColNames.end_date) == end_date)\n        &amp; (F.col(ColNames.season) == season)\n    ).select(\n        ColNames.user_id,\n        ColNames.grid_id,\n        ColNames.lps,\n        ColNames.total_frequency,\n        ColNames.day_type,\n        ColNames.time_interval,\n        ColNames.user_id_modulo,\n        ColNames.id_type,\n    )\n    return filtered_ltps_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.find_discontinuously_observed_devices","title":"<code>find_discontinuously_observed_devices(total_observations_df, total_device_freq_threshold)</code>  <code>staticmethod</code>","text":"<p>Find devices (user ids) which match the condition for being considered \"discontinuously observed\". This condition consists in having a total device observation of: freq &gt; total_freq_threshold.</p> <p>Parameters:</p> Name Type Description Default <code>total_observations_df</code> <code>DataFrame</code> <p>Long-Term Permanence Score dataset, filtered by the corresponding start and end dates, and filtered by id_type == 'device_observation'.</p> required <code>total_device_freq_threshold</code> <code>int</code> <p>ps threshold.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>two-column dataframe with the ids of the devices to discard and the corresponding user modulos.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>@staticmethod\ndef find_discontinuously_observed_devices(\n    total_observations_df: DataFrame, total_device_freq_threshold: int\n) -&gt; DataFrame:\n    \"\"\"\n    Find devices (user ids) which match the condition for being considered \"discontinuously observed\".\n    This condition consists in having a total device observation of: freq &gt; total_freq_threshold.\n\n    Args:\n        total_observations_df (DataFrame): Long-Term Permanence Score dataset, filtered by the\n            corresponding start and end dates, and filtered by id_type == 'device_observation'.\n        total_device_freq_threshold (int): ps threshold.\n\n    Returns:\n        DataFrame: two-column dataframe with the ids of the devices to discard and the corresponding user modulos.\n    \"\"\"\n    discontinuously_observed_user_ids = total_observations_df.filter(\n        F.col(ColNames.total_frequency) &lt; total_device_freq_threshold\n    ).select(ColNames.user_id_modulo, ColNames.user_id)\n    return discontinuously_observed_user_ids\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.find_rarely_observed_devices","title":"<code>find_rarely_observed_devices(total_observations_df, total_device_ps_threshold)</code>  <code>staticmethod</code>","text":"<p>Find devices (user ids) which match the condition for being considered \"rarely observed\". This condition consists in having a total device observation of: lps &gt; total_ps_threshold.</p> <p>Parameters:</p> Name Type Description Default <code>total_observations_df</code> <code>DataFrame</code> <p>Long-Term Permanence Score dataset, filtered by the corresponding start and end dates, and filtered by id_type == 'device_observation'.</p> required <code>total_device_ps_threshold</code> <code>int</code> <p>ps threshold.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>two-column dataframe with the ids of the devices to discard and the corresponding user modulos.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>@staticmethod\ndef find_rarely_observed_devices(total_observations_df: DataFrame, total_device_ps_threshold: int) -&gt; DataFrame:\n    \"\"\"\n    Find devices (user ids) which match the condition for being considered \"rarely observed\".\n    This condition consists in having a total device observation of: lps &gt; total_ps_threshold.\n\n    Args:\n        total_observations_df (DataFrame): Long-Term Permanence Score dataset, filtered by the\n            corresponding start and end dates, and filtered by id_type == 'device_observation'.\n        total_device_ps_threshold (int): ps threshold.\n\n    Returns:\n        DataFrame: two-column dataframe with the ids of the devices to discard and the corresponding user modulos.\n    \"\"\"\n    rarely_observed_user_ids = total_observations_df.filter(F.col(ColNames.lps) &lt; total_device_ps_threshold).select(\n        ColNames.user_id_modulo, ColNames.user_id\n    )\n    return rarely_observed_user_ids\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.format_not_selected_tiles","title":"<code>format_not_selected_tiles(not_selected_tiles_df, label_type)</code>","text":"<p>Add the \"label\", \"ue_label_rule\" and \"location_label_rule\" columns to the not selected tiles dataframe: - the \"label\" column is assigned directly depending on the label type being computed by mapping with the   provided dictionary: LABEL_TO_LABELNAMES[label_type]. - the \"ue_label_rule\" is equal to \"ue_na\". - the \"location_label_rule\" is equal to \"loc_na\".</p> <p>Parameters:</p> Name Type Description Default <code>not_selected_tiles_df</code> <code>DataFrame</code> <p>not selected tiles dataset.</p> required <code>label_type</code> <code>str</code> <p>label type to compute: 'ue', 'home' or 'work'.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>not selected tiles dataset, with the additional columns: \"label\", \"ue_label_rule\" and \"location_label_rule\".</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>def format_not_selected_tiles(self, not_selected_tiles_df: DataFrame, label_type: str) -&gt; DataFrame:\n    \"\"\"\n    Add the \"label\", \"ue_label_rule\" and \"location_label_rule\" columns to the not selected tiles dataframe:\n    - the \"label\" column is assigned directly depending on the label type being computed by mapping with the\n      provided dictionary: LABEL_TO_LABELNAMES[label_type].\n    - the \"ue_label_rule\" is equal to \"ue_na\".\n    - the \"location_label_rule\" is equal to \"loc_na\".\n\n    Args:\n        not_selected_tiles_df (DataFrame): not selected tiles dataset.\n        label_type (str): label type to compute: 'ue', 'home' or 'work'.\n\n    Returns:\n        DataFrame: not selected tiles dataset, with the additional columns: \"label\", \"ue_label_rule\" and\n            \"location_label_rule\".\n    \"\"\"\n    if label_type == \"ue\":\n        not_selected_tiles_df = not_selected_tiles_df.withColumn(ColNames.ue_label_rule, F.lit(\"ue_na\"))\n    else:\n        labelname = self.LABEL_TO_LABELNAMES[label_type]\n        not_selected_tiles_df = not_selected_tiles_df.withColumn(ColNames.label, F.lit(labelname))\n        not_selected_tiles_df = not_selected_tiles_df.withColumn(ColNames.location_label_rule, F.lit(\"loc_na\"))\n\n    return not_selected_tiles_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.format_selected_tiles","title":"<code>format_selected_tiles(selected_tiles_df, label_type, n_rule)</code>","text":"<p>Add the \"label\", \"ue_label_rule\" and \"location_label_rule\" columns to the given selected tiles dataframe: - the \"label\" column is assigned directly depending on the label type being computed by mapping with the   provided dictionary: LABEL_TO_LABELNAMES[label_type]. - the \"ue_label_rule\" is equal to \"ue_na\" if the label type being computed is different from \"ue\", or is   equal to \"{ue}{N}\" if the label type being computed is \"ue\", where:     - {N} = n_rule     - {ue} = LABEL_TO_SHORT_LABELNAMES[label_type]. - the \"location_label_rule\" is equal to \"loc_na\" if the label type being computed is \"ue\", or is equal to   \"{loc}{N}\" if the label type being computed is NOT \"ue\", where:     - {N} = n_rule     - {loc} = LABEL_TO_SHORT_LABELNAMES[label_type].</p> <p>Parameters:</p> Name Type Description Default <code>selected_tiles_df</code> <code>DataFrame</code> <p>selected tiles dataset.</p> required <code>label_type</code> <code>str</code> <p>label type to compute: 'ue', 'home' or 'work'.</p> required <code>n_rule</code> <code>int</code> <p>number of the current rule in order to generate the label_rule column values.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>selected tiles dataset, with the additional columns: \"label\", \"ue_label_rule\" and \"location_label_rule\".</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>def format_selected_tiles(self, selected_tiles_df: DataFrame, label_type: str, n_rule: int) -&gt; DataFrame:\n    \"\"\"\n    Add the \"label\", \"ue_label_rule\" and \"location_label_rule\" columns to the given selected tiles dataframe:\n    - the \"label\" column is assigned directly depending on the label type being computed by mapping with the\n      provided dictionary: LABEL_TO_LABELNAMES[label_type].\n    - the \"ue_label_rule\" is equal to \"ue_na\" if the label type being computed is different from \"ue\", or is\n      equal to \"{ue}_{N}\" if the label type being computed is \"ue\", where:\n        - {N} = n_rule\n        - {ue} = LABEL_TO_SHORT_LABELNAMES[label_type].\n    - the \"location_label_rule\" is equal to \"loc_na\" if the label type being computed is \"ue\", or is equal to\n      \"{loc}_{N}\" if the label type being computed is NOT \"ue\", where:\n        - {N} = n_rule\n        - {loc} = LABEL_TO_SHORT_LABELNAMES[label_type].\n\n    Args:\n        selected_tiles_df (DataFrame): selected tiles dataset.\n        label_type (str): label type to compute: 'ue', 'home' or 'work'.\n        n_rule (int): number of the current rule in order to generate the label_rule column values.\n\n    Returns:\n        DataFrame: selected tiles dataset, with the additional columns: \"label\", \"ue_label_rule\" and\n            \"location_label_rule\".\n    \"\"\"\n    short_labelname = self.LABEL_TO_SHORT_LABELNAMES[label_type]\n\n    if label_type == \"ue\":\n        ue_rule_txt = f\"{short_labelname}_{n_rule}\"\n        selected_tiles_df = selected_tiles_df.withColumn(ColNames.ue_label_rule, F.lit(ue_rule_txt))\n    else:\n        labelname = self.LABEL_TO_LABELNAMES[label_type]\n        selected_tiles_df = selected_tiles_df.withColumn(ColNames.label, F.lit(labelname))\n        loc_rule_txt = f\"{short_labelname}_{n_rule}\"\n        selected_tiles_df = selected_tiles_df.withColumn(ColNames.location_label_rule, F.lit(loc_rule_txt))\n\n    return selected_tiles_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.generate_quality_metrics","title":"<code>generate_quality_metrics(ue_labels_df)</code>","text":"<p>Build usual environment labeling quality metrics dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>ue_labels_df</code> <code>DataFrame</code> <p>usual environment labels dataframe.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>quality metrics dataframe.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>def generate_quality_metrics(self, ue_labels_df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Build usual environment labeling quality metrics dataframe.\n\n    Args:\n        ue_labels_df (DataFrame): usual environment labels dataframe.\n\n    Returns:\n        DataFrame: quality metrics dataframe.\n    \"\"\"\n    grouped_df = ue_labels_df.groupby(ColNames.label, ColNames.ue_label_rule, ColNames.location_label_rule).count()\n    grouped_df.cache()\n\n    ue_1_rule_count = self.get_rule_count(grouped_df, {ColNames.ue_label_rule: \"ue_1\"})\n    ue_2_rule_count = self.get_rule_count(grouped_df, {ColNames.ue_label_rule: \"ue_2\"})\n    ue_3_rule_count = self.get_rule_count(grouped_df, {ColNames.ue_label_rule: \"ue_3\"})\n    h_1_rule_count = self.get_rule_count(grouped_df, {ColNames.location_label_rule: \"h_1\"})\n    h_2_rule_count = self.get_rule_count(grouped_df, {ColNames.location_label_rule: \"h_2\"})\n    h_3_rule_count = self.get_rule_count(grouped_df, {ColNames.location_label_rule: \"h_3\"})\n    w_1_rule_count = self.get_rule_count(grouped_df, {ColNames.location_label_rule: \"w_1\"})\n    w_2_rule_count = self.get_rule_count(grouped_df, {ColNames.location_label_rule: \"w_2\"})\n    w_3_rule_count = self.get_rule_count(grouped_df, {ColNames.location_label_rule: \"w_3\"})\n    ue_na_rule_count = self.get_rule_count(grouped_df, {ColNames.ue_label_rule: \"ue_na\"})\n    loc_na_rule_count = self.get_rule_count(grouped_df, {ColNames.location_label_rule: \"loc_na\"})\n    h_non_ue_count = self.get_rule_count(grouped_df, {ColNames.label: \"home\", ColNames.ue_label_rule: \"ue_na\"})\n    w_non_ue_count = self.get_rule_count(grouped_df, {ColNames.label: \"work\", ColNames.ue_label_rule: \"ue_na\"})\n\n    grouped_df.unpersist()\n\n    data = [\n        (\"device_filter_1_rule\", self.rare_devices_count, self.start_date, self.end_date, self.season),\n        (\"device_filter_2_rule\", self.discontinuous_devices_count, self.start_date, self.end_date, self.season),\n        (\"ue_1_rule\", ue_1_rule_count, self.start_date, self.end_date, self.season),\n        (\"ue_2_rule\", ue_2_rule_count, self.start_date, self.end_date, self.season),\n        (\"ue_3_rule\", ue_3_rule_count, self.start_date, self.end_date, self.season),\n        (\"h_1_rule\", h_1_rule_count, self.start_date, self.end_date, self.season),\n        (\"h_2_rule\", h_2_rule_count, self.start_date, self.end_date, self.season),\n        (\"h_3_rule\", h_3_rule_count, self.start_date, self.end_date, self.season),\n        (\"w_1_rule\", w_1_rule_count, self.start_date, self.end_date, self.season),\n        (\"w_2_rule\", w_2_rule_count, self.start_date, self.end_date, self.season),\n        (\"w_3_rule\", w_3_rule_count, self.start_date, self.end_date, self.season),\n        (\"ue_na_rule\", ue_na_rule_count, self.start_date, self.end_date, self.season),\n        (\"loc_na_rule\", loc_na_rule_count, self.start_date, self.end_date, self.season),\n        (\"h_non_ue\", h_non_ue_count, self.start_date, self.end_date, self.season),\n        (\"w_non_ue\", w_non_ue_count, self.start_date, self.end_date, self.season),\n    ]\n\n    # Create DataFrame\n    schema = SilverUsualEnvironmentLabelingQualityMetricsDataObject.SCHEMA\n    quality_metrics_df = self.spark.createDataFrame(data, schema)\n\n    return quality_metrics_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.get_rule_count","title":"<code>get_rule_count(df, col_to_value)</code>  <code>staticmethod</code>","text":"<p>Sums the count column values of the given dataframe for the corresponding filter (col_to_value).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>input dataframe.</p> required <code>col_to_value</code> <code>dict[str, str]</code> <p>filter to apply.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>count column sum.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>@staticmethod\ndef get_rule_count(df: DataFrame, col_to_value: dict[str, str]) -&gt; int:\n    \"\"\"\n    Sums the count column values of the given dataframe for the corresponding filter (col_to_value).\n\n    Args:\n        df (DataFrame): input dataframe.\n        col_to_value (dict[str, str]): filter to apply.\n\n    Returns:\n        int: count column sum.\n    \"\"\"\n    conditions = (F.col(colname) == colvalue for colname, colvalue in col_to_value.items())\n    combined_condition = reduce(lambda x, y: x &amp; y, conditions)\n    count_value = df.filter(combined_condition).agg(F.sum(\"count\")).collect()[0][0]\n    if count_value is None:\n        count_value = 0\n    return count_value\n</code></pre>"},{"location":"reference/components/ingestion/","title":"ingestion","text":""},{"location":"reference/components/ingestion/grid_generation/","title":"grid_generation","text":""},{"location":"reference/components/ingestion/grid_generation/inspire_grid_generation/","title":"inspire_grid_generation","text":"<p>This module contains the InspireGridGeneration class which is responsible for generating the INSPIRE grid  and enrich it with elevation and landuse data.</p>"},{"location":"reference/components/ingestion/grid_generation/inspire_grid_generation/#components.ingestion.grid_generation.inspire_grid_generation.InspireGridGeneration","title":"<code>InspireGridGeneration</code>","text":"<p>               Bases: <code>Component</code></p> <p>This class is responsible for generating the INSPIRE grid for given extent or polygon and enrich it with elevation and landuse data.</p> Source code in <code>multimno/components/ingestion/grid_generation/inspire_grid_generation.py</code> <pre><code>class InspireGridGeneration(Component):\n    \"\"\"\n    This class is responsible for generating the INSPIRE grid for given extent or polygon\n    and enrich it with elevation and landuse data.\n    \"\"\"\n\n    COMPONENT_ID = \"InspireGridGeneration\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.grid_extent = self.config.geteval(InspireGridGeneration.COMPONENT_ID, \"extent\")\n        self.grid_partition_size = self.config.get(InspireGridGeneration.COMPONENT_ID, \"grid_generation_partition_size\")\n\n        self.grid_quadkey_level = self.config.getint(\n            InspireGridGeneration.COMPONENT_ID,\n            \"grid_processing_partition_quadkey_level\",\n        )\n\n        self.reference_country = self.config.get(InspireGridGeneration.COMPONENT_ID, \"reference_country\")\n\n        self.country_buffer = self.config.get(InspireGridGeneration.COMPONENT_ID, \"country_buffer\")\n\n        # TODO: For now set to default 100, but can be dynamic from config in future\n        self.grid_resolution = 100\n\n        self.grid_generator = InspireGridGenerator(\n            self.spark,\n            self.grid_resolution,\n            ColNames.geometry,\n            ColNames.grid_id,\n            self.grid_partition_size,\n        )\n\n        self.quadkey_udf = F.udf(utils.latlon_to_quadkey)\n\n    def initalize_data_objects(self):\n\n        self.clear_destination_directory = self.config.getboolean(\n            InspireGridGeneration.COMPONENT_ID, \"clear_destination_directory\"\n        )\n\n        self.grid_mask = self.config.get(InspireGridGeneration.COMPONENT_ID, \"grid_mask\")\n\n        if self.grid_mask == \"polygon\":\n            # inputs\n            self.input_data_objects = {}\n            self.input_data_objects[BronzeCountriesDataObject.ID] = BronzeCountriesDataObject(\n                self.spark,\n                self.config.get(CONFIG_BRONZE_PATHS_KEY, \"countries_data_bronze\"),\n            )\n\n        # outputs\n\n        grid_do_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"grid_data_silver\")\n\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, grid_do_path)\n\n        self.output_data_objects = {}\n        self.output_data_objects[SilverGridDataObject.ID] = SilverGridDataObject(\n            self.spark, grid_do_path, [ColNames.quadkey]\n        )\n\n    def execute(self):\n\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n\n        if self.grid_mask == \"polygon\":\n            self.read()\n\n            countries = self.get_country_mask(self.reference_country, self.country_buffer)\n            ids = [row[\"temp_id\"] for row in countries.select(\"temp_id\").collect()]\n\n            self.logger.info(f\"Processing {len(ids)} parts of the country\")\n            processed_parts = 0\n            for id in ids:\n                self.current_country_part = countries.filter(F.col(\"temp_id\") == id)\n                self.transform()\n                self.write()\n                processed_parts += 1\n                self.logger.info(f\"Finished processing {processed_parts} parts\")\n        else:\n            self.transform()\n            self.write()\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}...\")\n\n        if self.grid_mask == \"polygon\":\n            grid_sdf = self.grid_generator.cover_polygon_with_grid_centroids(self.current_country_part)\n        else:\n            grid_sdf = self.grid_generator.cover_extent_with_grid_centroids(self.grid_extent)\n        grid_sdf = utils.assign_quadkey(grid_sdf, 3035, self.grid_quadkey_level)\n\n        grid_sdf = grid_sdf.orderBy(ColNames.quadkey)\n        grid_sdf = grid_sdf.repartition(ColNames.quadkey)\n\n        grid_sdf = utils.apply_schema_casting(grid_sdf, SilverGridDataObject.SCHEMA)\n\n        self.output_data_objects[SilverGridDataObject.ID].df = grid_sdf\n\n    def get_country_mask(self, reference_country, country_buffer):\n\n        countries = self.input_data_objects[BronzeCountriesDataObject.ID].df\n        countries = (\n            countries.filter(F.col(ColNames.iso2) == reference_country)\n            .withColumn(\n                ColNames.geometry,\n                STF.ST_Buffer(ColNames.geometry, F.lit(country_buffer)),\n            )\n            .groupBy()\n            .agg(STA.ST_Union_Aggr(ColNames.geometry).alias(ColNames.geometry))\n            .withColumn(ColNames.geometry, F.explode(STF.ST_Dump(ColNames.geometry)))\n            .withColumn(\"temp_id\", F.monotonically_increasing_id())\n        )\n\n        countries = utils.project_to_crs(countries, 3035, 4326)\n\n        return countries\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/","title":"spatial_data_ingestion","text":""},{"location":"reference/components/ingestion/spatial_data_ingestion/gisco_data_ingestion/","title":"gisco_data_ingestion","text":""},{"location":"reference/components/ingestion/spatial_data_ingestion/gisco_data_ingestion/#components.ingestion.spatial_data_ingestion.gisco_data_ingestion.GiscoDataIngestion","title":"<code>GiscoDataIngestion</code>","text":"<p>               Bases: <code>Component</code></p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/gisco_data_ingestion.py</code> <pre><code>class GiscoDataIngestion(Component):\n    \"\"\" \"\"\"\n\n    COMPONENT_ID = \"GiscoDataIngestion\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n    def initalize_data_objects(self):\n\n        base_url = self.config.get(GiscoDataIngestion.COMPONENT_ID, \"base_url\")\n\n        self.get_countries = self.config.getboolean(GiscoDataIngestion.COMPONENT_ID, \"get_countries\")\n\n        self.get_nuts = self.config.getboolean(GiscoDataIngestion.COMPONENT_ID, \"get_nuts\")\n\n        self.default_crs = self.config.getint(GiscoDataIngestion.COMPONENT_ID, \"default_crs\")\n\n        self.input_data_objects = {}\n        self.output_data_objects = {}\n        if self.get_countries:\n\n            countries_resolution = self.config.get(GiscoDataIngestion.COMPONENT_ID, \"countries_resolution\")\n            countries_year = self.config.get(GiscoDataIngestion.COMPONENT_ID, \"countries_year\")\n\n            countries_url = (\n                f\"{base_url}/countries/geojson/CNTR_RG_{countries_resolution}M_{countries_year}_4326.geojson\"\n            )\n\n            self.input_data_objects[\"countries\"] = LandingHttpGeoJsonDataObject(self.spark, countries_url, 240, 3)\n\n            countries_do_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"countries_data_bronze\")\n\n            self.output_data_objects[BronzeCountriesDataObject.ID] = BronzeCountriesDataObject(\n                self.spark,\n                countries_do_path,\n                [],\n                self.default_crs,\n            )\n\n        if self.get_nuts:\n\n            nuts_resolution = self.config.get(GiscoDataIngestion.COMPONENT_ID, \"nuts_resolution\")\n            self.nuts_year = self.config.get(GiscoDataIngestion.COMPONENT_ID, \"nuts_year\")\n            nuts_levels = self.config.get(GiscoDataIngestion.COMPONENT_ID, \"nuts_levels\")\n            self.nuts_levels = nuts_levels.split(\",\")\n\n            self.reference_country = self.config.get(GiscoDataIngestion.COMPONENT_ID, \"reference_country\")\n            for level in self.nuts_levels:\n                nuts_url = (\n                    f\"{base_url}/nuts/geojson/NUTS_RG_{nuts_resolution}M_{self.nuts_year}_4326_LEVL_{level}.geojson\"\n                )\n                self.input_data_objects[f\"nuts_{level}\"] = LandingHttpGeoJsonDataObject(self.spark, nuts_url, 300, 5)\n\n            geographic_zones_do_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"geographic_zones_data_bronze\")\n\n            self.output_data_objects[BronzeGeographicZonesDataObject.ID] = BronzeGeographicZonesDataObject(\n                self.spark,\n                geographic_zones_do_path,\n                [ColNames.dataset_id],\n                self.default_crs,\n            )\n\n        self.clear_destination_directory = self.config.getboolean(\n            GiscoDataIngestion.COMPONENT_ID, \"clear_destination_directory\"\n        )\n\n        if self.clear_destination_directory:\n            for do in self.output_data_objects.values():\n                self.logger.info(f\"Clearing {do.default_path}\")\n                delete_file_or_folder(self.spark, do.default_path)\n\n    def read(self):\n        # need to read the data from the input data objects separately\n        pass\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n\n        if self.get_countries:\n\n            countries = self.get_countries_data()\n            countries = utils.apply_schema_casting(countries, BronzeCountriesDataObject.SCHEMA)\n            self.output_data_objects[BronzeCountriesDataObject.ID].df = countries\n\n        if self.get_nuts:\n            self.output_data_objects[BronzeGeographicZonesDataObject.ID].df = self.spark.createDataFrame(\n                [], BronzeGeographicZonesDataObject.SCHEMA\n            )\n            for level in self.nuts_levels:\n\n                nuts = self.get_nuts_data(level)\n                nuts = nuts.withColumn(ColNames.dataset_id, F.lit(\"nuts\"))\n                nuts = utils.apply_schema_casting(nuts, BronzeGeographicZonesDataObject.SCHEMA)\n                self.output_data_objects[BronzeGeographicZonesDataObject.ID].df = self.output_data_objects[\n                    BronzeGeographicZonesDataObject.ID\n                ].df.union(nuts)\n\n    def get_countries_data(self) -&gt; DataFrame:\n        \"\"\"\n        Retrieves and processes country data.\n\n        This method reads country data from the GISCO portal\n        and processes the data by renaming columns, exploding the 'geometry' column,\n        and projecting the data to a default CRS.\n        The processed data is returned as a DataFrame.\n\n        Returns:\n            DataFrame: A DataFrame containing processed country data.\n                    The DataFrame has columns for the ISO 2 country code, country name, and geometry.\n        \"\"\"\n\n        self.input_data_objects[\"countries\"].read()\n        self.logger.info(f\"got countries data\")\n        countries_sdf = self.input_data_objects[\"countries\"].df\n        countries_sdf = countries_sdf.withColumnRenamed(\"CNTR_ID\", ColNames.iso2).withColumnRenamed(\n            \"NAME_ENGL\", ColNames.name\n        )\n\n        countries_sdf = countries_sdf.withColumn(\"geometry\", F.explode(STF.ST_Dump(\"geometry\")))\n\n        countries_sdf = utils.project_to_crs(countries_sdf, 4326, self.default_crs)\n        return countries_sdf\n\n    def get_nuts_data(self, level: str) -&gt; DataFrame:\n        \"\"\"\n        Retrieves and processes NUTS (Nomenclature of Territorial Units for Statistics) data for a specific level.\n\n        This method reads NUTS data from GISCO portal and processes the data by\n        renaming columns, filtering by reference country, and projecting the data to a default CRS.\n        It also adds a 'parent_id' column that contains the ID of the parent NUTS unit for each NUTS unit,\n        and adds columns for the year, month, and day with fixed values.\n        The processed data is returned as a DataFrame.\n\n        Args:\n            level (str): The NUTS level for which to retrieve and process data.\n\n        Returns:\n            DataFrame: A DataFrame containing processed NUTS data.\n                    The DataFrame has columns for the NUTS ID, name, ISO 2 country code, level,\n                    geometry, parent ID, year, month, and day.\n        \"\"\"\n\n        self.input_data_objects[f\"nuts_{level}\"].read()\n        self.logger.info(f\"got NUTS data for level {level}\")\n        nuts_sdf = self.input_data_objects[f\"nuts_{level}\"].df\n        nuts_sdf = nuts_sdf.drop(\"id\")\n        nuts_sdf = nuts_sdf.filter(F.col(\"CNTR_CODE\") == self.reference_country)\n\n        nuts_sdf = (\n            nuts_sdf.withColumnRenamed(\"NUTS_ID\", ColNames.zone_id)\n            .withColumnRenamed(\"NAME_LATN\", ColNames.name)\n            .withColumnRenamed(\"CNTR_CODE\", ColNames.iso2)\n            .withColumnRenamed(\"LEVL_CODE\", ColNames.level)\n        )\n\n        nuts_sdf = nuts_sdf.select(\n            ColNames.zone_id,\n            ColNames.name,\n            ColNames.iso2,\n            ColNames.level,\n            ColNames.geometry,\n        )\n\n        if level == 0:\n            nuts_sdf = nuts_sdf.withColumn(\"parent_id\", F.lit(None))\n        else:\n            nuts_sdf = nuts_sdf.withColumn(\"parent_id\", F.expr(\"substring(zone_id, 1, length(zone_id)-1)\"))\n\n        nuts_sdf = utils.project_to_crs(nuts_sdf, 4326, self.default_crs)\n\n        nuts_sdf = nuts_sdf.withColumns(\n            {\n                ColNames.year: F.lit(self.nuts_year),\n                ColNames.month: F.lit(1),\n                ColNames.day: F.lit(1),\n            }\n        )\n        return nuts_sdf\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/gisco_data_ingestion/#components.ingestion.spatial_data_ingestion.gisco_data_ingestion.GiscoDataIngestion.get_countries_data","title":"<code>get_countries_data()</code>","text":"<p>Retrieves and processes country data.</p> <p>This method reads country data from the GISCO portal and processes the data by renaming columns, exploding the 'geometry' column, and projecting the data to a default CRS. The processed data is returned as a DataFrame.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing processed country data.     The DataFrame has columns for the ISO 2 country code, country name, and geometry.</p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/gisco_data_ingestion.py</code> <pre><code>def get_countries_data(self) -&gt; DataFrame:\n    \"\"\"\n    Retrieves and processes country data.\n\n    This method reads country data from the GISCO portal\n    and processes the data by renaming columns, exploding the 'geometry' column,\n    and projecting the data to a default CRS.\n    The processed data is returned as a DataFrame.\n\n    Returns:\n        DataFrame: A DataFrame containing processed country data.\n                The DataFrame has columns for the ISO 2 country code, country name, and geometry.\n    \"\"\"\n\n    self.input_data_objects[\"countries\"].read()\n    self.logger.info(f\"got countries data\")\n    countries_sdf = self.input_data_objects[\"countries\"].df\n    countries_sdf = countries_sdf.withColumnRenamed(\"CNTR_ID\", ColNames.iso2).withColumnRenamed(\n        \"NAME_ENGL\", ColNames.name\n    )\n\n    countries_sdf = countries_sdf.withColumn(\"geometry\", F.explode(STF.ST_Dump(\"geometry\")))\n\n    countries_sdf = utils.project_to_crs(countries_sdf, 4326, self.default_crs)\n    return countries_sdf\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/gisco_data_ingestion/#components.ingestion.spatial_data_ingestion.gisco_data_ingestion.GiscoDataIngestion.get_nuts_data","title":"<code>get_nuts_data(level)</code>","text":"<p>Retrieves and processes NUTS (Nomenclature of Territorial Units for Statistics) data for a specific level.</p> <p>This method reads NUTS data from GISCO portal and processes the data by renaming columns, filtering by reference country, and projecting the data to a default CRS. It also adds a 'parent_id' column that contains the ID of the parent NUTS unit for each NUTS unit, and adds columns for the year, month, and day with fixed values. The processed data is returned as a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>str</code> <p>The NUTS level for which to retrieve and process data.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing processed NUTS data.     The DataFrame has columns for the NUTS ID, name, ISO 2 country code, level,     geometry, parent ID, year, month, and day.</p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/gisco_data_ingestion.py</code> <pre><code>def get_nuts_data(self, level: str) -&gt; DataFrame:\n    \"\"\"\n    Retrieves and processes NUTS (Nomenclature of Territorial Units for Statistics) data for a specific level.\n\n    This method reads NUTS data from GISCO portal and processes the data by\n    renaming columns, filtering by reference country, and projecting the data to a default CRS.\n    It also adds a 'parent_id' column that contains the ID of the parent NUTS unit for each NUTS unit,\n    and adds columns for the year, month, and day with fixed values.\n    The processed data is returned as a DataFrame.\n\n    Args:\n        level (str): The NUTS level for which to retrieve and process data.\n\n    Returns:\n        DataFrame: A DataFrame containing processed NUTS data.\n                The DataFrame has columns for the NUTS ID, name, ISO 2 country code, level,\n                geometry, parent ID, year, month, and day.\n    \"\"\"\n\n    self.input_data_objects[f\"nuts_{level}\"].read()\n    self.logger.info(f\"got NUTS data for level {level}\")\n    nuts_sdf = self.input_data_objects[f\"nuts_{level}\"].df\n    nuts_sdf = nuts_sdf.drop(\"id\")\n    nuts_sdf = nuts_sdf.filter(F.col(\"CNTR_CODE\") == self.reference_country)\n\n    nuts_sdf = (\n        nuts_sdf.withColumnRenamed(\"NUTS_ID\", ColNames.zone_id)\n        .withColumnRenamed(\"NAME_LATN\", ColNames.name)\n        .withColumnRenamed(\"CNTR_CODE\", ColNames.iso2)\n        .withColumnRenamed(\"LEVL_CODE\", ColNames.level)\n    )\n\n    nuts_sdf = nuts_sdf.select(\n        ColNames.zone_id,\n        ColNames.name,\n        ColNames.iso2,\n        ColNames.level,\n        ColNames.geometry,\n    )\n\n    if level == 0:\n        nuts_sdf = nuts_sdf.withColumn(\"parent_id\", F.lit(None))\n    else:\n        nuts_sdf = nuts_sdf.withColumn(\"parent_id\", F.expr(\"substring(zone_id, 1, length(zone_id)-1)\"))\n\n    nuts_sdf = utils.project_to_crs(nuts_sdf, 4326, self.default_crs)\n\n    nuts_sdf = nuts_sdf.withColumns(\n        {\n            ColNames.year: F.lit(self.nuts_year),\n            ColNames.month: F.lit(1),\n            ColNames.day: F.lit(1),\n        }\n    )\n    return nuts_sdf\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/overture_data_ingestion/","title":"overture_data_ingestion","text":""},{"location":"reference/components/ingestion/spatial_data_ingestion/overture_data_ingestion/#components.ingestion.spatial_data_ingestion.overture_data_ingestion.OvertureDataIngestion","title":"<code>OvertureDataIngestion</code>","text":"<p>               Bases: <code>Component</code></p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/overture_data_ingestion.py</code> <pre><code>class OvertureDataIngestion(Component):\n    \"\"\" \"\"\"\n\n    COMPONENT_ID = \"OvertureDataIngestion\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.transportation_reclass_map = self.config.geteval(\n            OvertureDataIngestion.COMPONENT_ID, \"transportation_reclass_map\"\n        )\n        self.landuse_reclass_map = self.config.geteval(\n            OvertureDataIngestion.COMPONENT_ID, \"landuse_landcover_reclass_map\"\n        )\n        self.bulding_reclass_map = self.config.geteval(OvertureDataIngestion.COMPONENT_ID, \"buildings_reclass_map\")\n\n        self.landuse_filter_subtypes = self.config.geteval(\n            OvertureDataIngestion.COMPONENT_ID, \"landuse_filter_subtypes\"\n        )\n        self.landcover_filter_subtypes = self.config.geteval(\n            OvertureDataIngestion.COMPONENT_ID, \"landcover_filter_subtypes\"\n        )\n\n        self.transportation_filter_subtypes = self.config.geteval(\n            OvertureDataIngestion.COMPONENT_ID, \"transportation_filter_subtypes\"\n        )\n\n        self.extent = self.config.geteval(OvertureDataIngestion.COMPONENT_ID, \"extent\")\n\n        self.spatial_repartition_size_rows = self.config.getint(\n            OvertureDataIngestion.COMPONENT_ID, \"spatial_repartition_size_rows\"\n        )\n\n        self.min_partition_quadkey_level = self.config.getint(\n            OvertureDataIngestion.COMPONENT_ID, \"min_partition_quadkey_level\"\n        )\n\n        self.max_partition_quadkey_level = self.config.getint(\n            OvertureDataIngestion.COMPONENT_ID, \"max_partition_quadkey_level\"\n        )\n\n        self.extraction_quadkey_level = self.config.getint(\n            OvertureDataIngestion.COMPONENT_ID, \"extraction_quadkey_level\"\n        )\n\n        self.use_buildings = self.config.getboolean(OvertureDataIngestion.COMPONENT_ID, \"use_buildings\")\n\n        self.quadkey_udf = F.udf(utils.latlon_to_quadkey)\n\n    def initalize_data_objects(self):\n\n        overture_url = self.config.get(OvertureDataIngestion.COMPONENT_ID, \"overture_url\")\n\n        transportation_url = overture_url + \"/theme=transportation/type=segment\"\n\n        buildings_url = overture_url + \"/theme=buildings/type=building\"\n\n        landcover_url = overture_url + \"/theme=base/type=land\"\n        landuse_url = overture_url + \"/theme=base/type=land_use\"\n        water_url = overture_url + \"/theme=base/type=water\"\n\n        self.input_data_objects = {}\n        self.input_data_objects[\"transportation\"] = LandingGeoParquetDataObject(self.spark, transportation_url, [])\n        self.input_data_objects[\"buildings\"] = LandingGeoParquetDataObject(self.spark, buildings_url, [])\n        self.input_data_objects[\"landcover\"] = LandingGeoParquetDataObject(self.spark, landcover_url, [])\n        self.input_data_objects[\"landuse\"] = LandingGeoParquetDataObject(self.spark, landuse_url, [])\n        self.input_data_objects[\"water\"] = LandingGeoParquetDataObject(self.spark, water_url, [])\n\n        self.clear_destination_directory = self.config.getboolean(\n            OvertureDataIngestion.COMPONENT_ID, \"clear_destination_directory\"\n        )\n\n        transportation_do_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"transportation_data_bronze\")\n\n        landuse_do_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"landuse_data_bronze\")\n\n        self.output_data_objects = {}\n\n        self.output_data_objects[BronzeTransportationDataObject.ID] = BronzeTransportationDataObject(\n            self.spark,\n            transportation_do_path,\n            [ColNames.year, ColNames.month, ColNames.day, ColNames.quadkey],\n        )\n        self.output_data_objects[BronzeLanduseDataObject.ID] = BronzeLanduseDataObject(\n            self.spark,\n            landuse_do_path,\n            [ColNames.year, ColNames.month, ColNames.day, ColNames.quadkey],\n        )\n\n        if self.clear_destination_directory:\n            for do in self.output_data_objects.values():\n                self.logger.info(f\"Clearing {do.default_path}\")\n                delete_file_or_folder(self.spark, do.default_path)\n\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n\n        quadkeys = utils.get_quadkeys_for_bbox(self.extent, self.extraction_quadkey_level)\n\n        self.logger.info(f\"Extraction will be done in {len(quadkeys)} parts.\")\n        for quadkey in quadkeys:\n            self.logger.info(f\"Processing quadkey {quadkey}\")\n            self.current_extent = utils.quadkey_to_extent(quadkey)\n            self.current_quadkey = quadkey\n            self.transform()\n            self.write()\n            self.spark.catalog.clearCache()\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n\n        # process transportaion\n        transportation_sdf = self.get_transportation_data()\n        transportation_sdf = transportation_sdf.withColumns(\n            {\n                ColNames.year: F.year(F.current_date()),\n                ColNames.month: F.month(F.current_date()),\n                ColNames.day: F.day(F.current_date()),\n            }\n        )\n        transportation_sdf = self.apply_schema_casting(transportation_sdf, BronzeTransportationDataObject.SCHEMA)\n\n        self.output_data_objects[BronzeTransportationDataObject.ID].df = transportation_sdf\n\n        # process landuse\n        landuse_cols_to_select = [\"subtype\", \"geometry\"]\n\n        landcover_sdf = self.get_raw_data_for_landuse(\n            \"landcover\",\n            landuse_cols_to_select,\n            self.landcover_filter_subtypes,\n            persist=True,\n        )\n\n        landuse_sdf = self.get_raw_data_for_landuse(\n            \"landuse\",\n            landuse_cols_to_select,\n            self.landuse_filter_subtypes,\n            persist=True,\n        )\n\n        # combine landcover and landuse\n        landcover_sdf = utils.cut_polygons_with_mask_polygons(landcover_sdf, landuse_sdf, landuse_cols_to_select)\n\n        landcover_sdf = landcover_sdf.persist(StorageLevel.MEMORY_AND_DISK)\n        landcover_sdf.count()\n\n        landuse_sdf = utils.cut_polygons_with_mask_polygons(landuse_sdf, landcover_sdf, landuse_cols_to_select)\n\n        landuse_sdf = landuse_sdf.persist(StorageLevel.MEMORY_AND_DISK)\n        landuse_sdf.count()\n\n        landuse_sdf = utils.cut_polygons_with_mask_polygons(landuse_sdf, landuse_sdf, landuse_cols_to_select, True)\n\n        landuse_sdf = landuse_sdf.persist(StorageLevel.MEMORY_AND_DISK)\n        landuse_sdf.count()\n\n        landuse_landcover_sdf = landcover_sdf.union(landuse_sdf)\n\n        # blow up too big landuse polygons\n        # TODO: asses feasibility of this\n        # TODO: make vertices number parameter\n        # TODO: introduce cut by qaudkeys\n        # landuse_landcover_sdf = landuse_landcover_sdf.withColumn(ColNames.geometry, STF.ST_SubDivideExplode(ColNames.geometry, 1000))\n\n        landuse_landcover_sdf = landuse_landcover_sdf.persist(StorageLevel.MEMORY_AND_DISK)\n        landuse_landcover_sdf.count()\n        self.logger.info(f\"merged landuse and landcover\")\n\n        water_sdf = self.get_raw_data_for_landuse(\"water\", landuse_cols_to_select, persist=True)\n        water_sdf = water_sdf.withColumn(\"subtype\", F.lit(\"water\"))\n\n        # combine landuse with water\n        landuse_landcover_sdf = utils.cut_polygons_with_mask_polygons(\n            landuse_landcover_sdf, water_sdf, landuse_cols_to_select\n        )\n        landuse_cover_water_sdf = landuse_landcover_sdf.union(water_sdf)\n\n        # reclassify to config categories\n        landuse_cover_water_sdf = self.reclassify(\n            landuse_cover_water_sdf,\n            self.landuse_reclass_map,\n            \"subtype\",\n            ColNames.category,\n            \"open_area\",\n        )\n\n        landuse_cover_water_sdf = utils.fix_geometry(landuse_cover_water_sdf, 3)\n\n        landuse_cover_water_sdf = landuse_cover_water_sdf.persist(StorageLevel.MEMORY_AND_DISK)\n        landuse_cover_water_sdf.count()\n        self.logger.info(f\"merged landuse and water\")\n\n        # add buildings data to full landuse\n        if self.use_buildings:\n\n            buildings_sdf = self.get_raw_data_for_landuse(\"buildings\", [\"class\", \"geometry\"], persist=True)\n\n            buildings_sdf = self.reclassify(\n                buildings_sdf,\n                self.bulding_reclass_map,\n                \"class\",\n                ColNames.category,\n                \"other_builtup\",\n            )\n\n            buildings_sdf = self.merge_buildings_by_quadkey(buildings_sdf, 3035, 16)\n            buildings_sdf = buildings_sdf.drop(\"quadkey\")\n            buildings_sdf = buildings_sdf.persist(StorageLevel.MEMORY_AND_DISK)\n            buildings_sdf.count()\n            self.logger.info(f\"merged buildings\")\n\n            # combine landuse with buildings\n            landuse_cover_water_sdf = utils.cut_polygons_with_mask_polygons(\n                landuse_cover_water_sdf,\n                buildings_sdf,\n                [ColNames.category, ColNames.geometry],\n            )\n\n            landuse_cover_water_sdf = utils.fix_geometry(landuse_cover_water_sdf, 3, ColNames.geometry)\n\n            landuse_cover_water_sdf = landuse_cover_water_sdf.union(buildings_sdf)\n\n        landuse_cover_water_sdf = utils.assign_quadkey(landuse_cover_water_sdf, 3035, self.max_partition_quadkey_level)\n\n        # TODO: Figure out if this optimization is ever needed and how to implement it properly\n        # landuse_cover_water_sdf = utils.coarsen_quadkey_to_partition_size(landuse_cover_water_sdf,\n        #                                                                   self.spatial_repartition_size_rows,\n        #                                                                   self.min_partition_quadkey_level)\n\n        landuse_cover_water_sdf = landuse_cover_water_sdf.withColumns(\n            {\n                ColNames.year: F.year(F.current_date()),\n                ColNames.month: F.month(F.current_date()),\n                ColNames.day: F.day(F.current_date()),\n            }\n        )\n        landuse_cover_water_sdf = landuse_cover_water_sdf.orderBy(\"quadkey\")\n        landuse_cover_water_sdf = landuse_cover_water_sdf.repartition(\"quadkey\")\n\n        landuse_cover_water_sdf = self.apply_schema_casting(landuse_cover_water_sdf, BronzeLanduseDataObject.SCHEMA)\n\n        self.output_data_objects[BronzeLanduseDataObject.ID].df = landuse_cover_water_sdf\n\n    def get_transportation_data(self) -&gt; DataFrame:\n        \"\"\"\n        Processes and returns Overture Maps transportation data.\n\n        This function filters input data objects based on the transportation class and specified subtypes,\n        reclassifies the transportation data based on a predefined map, and selects specific columns for further processing.\n        It then fixes the line geometry, assigns quadkeys, projects the data to a specific CRS,\n        orders the data by quadkey, and repartitions the data based on quadkey.\n\n        Returns:\n            DataFrame: A DataFrame containing the processed transportation data.\n            The DataFrame includes a category column, a geometry column, and a quadkey column.\n        \"\"\"\n        # function implementation...\n\n        transportation_sdf = self.filter_input_data_objects(\n            \"transportation\",\n            [\"class\", \"geometry\"],\n            \"class\",\n            self.transportation_filter_subtypes,\n        )\n        transportation_sdf = self.reclassify(\n            transportation_sdf,\n            self.transportation_reclass_map,\n            \"class\",\n            ColNames.category,\n        )\n        transportation_sdf = transportation_sdf.select(ColNames.category, ColNames.geometry).drop(\"subtype\")\n\n        # transportation_sdf.persist(StorageLevel.MEMORY_AND_DISK)\n        # transportation_sdf.count()\n        # self.logger.info(f\"got transportation\")\n        transportation_sdf = utils.project_to_crs(transportation_sdf, 4326, 3035)\n        transportation_sdf = utils.fix_geometry(transportation_sdf, 2)\n        transportation_sdf = utils.assign_quadkey(transportation_sdf, 3035, self.max_partition_quadkey_level)\n\n        # TODO: Figure out how to implement this\n        # transportation_sdf = utils.coarsen_quadkey_to_partition_size(transportation_sdf, self.spatial_repartition_size_rows, self.min_partition_quadkey_level)\n        transportation_sdf = transportation_sdf.orderBy(\"quadkey\")\n        transportation_sdf = transportation_sdf.repartition(\"quadkey\")\n\n        return transportation_sdf\n\n    def get_raw_data_for_landuse(\n        self,\n        data_type: str,\n        cols_to_select,\n        filter_types: Optional[List[str]] = None,\n        persist: bool = True,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Retrieves and processes Overture Maps raw data for a specific land use type.\n\n        This function filters input data objects based on the specified data type and optional filter types,\n        fixes the polygon geometry, and projects the data to a specific CRS.\n        If the persist parameter is set to True, the resulting DataFrame is persisted in memory and disk.\n\n        Args:\n            data_type (str): The type of land use data to retrieve.\n            filter_types (list, optional): A list of subtypes to filter the data by. Each subtype is a string.\n                If None, no filtering is performed. Defaults to None.\n            persist (bool, optional): Whether to persist the resulting DataFrame in memory and disk. Defaults to True.\n\n        Returns:\n            DataFrame: A DataFrame containing the processed land use data.\n            The DataFrame includes a subtype column and a geometry column.\n        \"\"\"\n        # function implementation...\n\n        sdf = self.filter_input_data_objects(data_type, cols_to_select, \"subtype\", filter_types)\n\n        sdf = utils.project_to_crs(sdf, 4326, 3035)\n        sdf = utils.fix_geometry(sdf, 3, ColNames.geometry)\n\n        if persist:\n            sdf = sdf.persist(StorageLevel.MEMORY_AND_DISK)\n            sdf.count()\n            self.logger.info(f\"got {data_type}\")\n\n        return sdf\n\n    def merge_buildings_by_quadkey(self, sdf: DataFrame, crs: int, quadkey_level: int = 16) -&gt; DataFrame:\n        \"\"\"\n        Merges building polygons within each quadkey.\n\n        This function assigns a quadkey to each building polygon in the input DataFrame,\n        then groups the DataFrame by quadkey and building category, and merges the polygons within each group.\n\n        Args:\n            sdf (DataFrame): A DataFrame containing the building polygons.\n            crs (int): The coordinate reference system of the input geometries.\n            qadkey_level (int, optional): The zoom level to use when assigning quadkeys. Defaults to 16.\n\n        Returns:\n            DataFrame: A DataFrame containing the merged building polygons.\n        \"\"\"\n\n        sdf = utils.assign_quadkey(sdf, crs, quadkey_level)\n\n        # TODO: test more if this would make any difference\n\n        # sdf = utils.coarsen_quadkey_to_partition_size(\n        #     sdf, self.spatial_repartition_size_rows, 10\n        # )\n\n        # sdf = sdf.withColumn(\"quadkey_merge\", F.col(\"quadkey\").substr(1, self.max_partition_quadkey_level))\n        # sdf = sdf.repartition(\"quadkey_merge\").drop(\"quadkey_merge\")\n\n        sdf = sdf.groupBy(\"quadkey\", \"category\").agg(STA.ST_Union_Aggr(ColNames.geometry).alias(ColNames.geometry))\n\n        return sdf\n\n    def filter_input_data_objects(\n        self,\n        data_type: str,\n        required_columns: List[str],\n        category_col: str,\n        subtypes: Optional[List[str]] = None,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Filters and processes input Overture Maps data based on the specified data type and columns.\n\n        This function selects the required columns from the input data objects,\n        filters the data to the current processing iteration extent, and cuts the data to the general extent.\n        If the data type is \"landcover\", \"landuse\", or \"transportation\", it further filters the data by the specified subtypes.\n        If the data type is not \"transportation\", it filters out invalid polygons.\n\n        Args:\n            data_type (str): The type of data to filter and process. \"landcover\", \"landuse\", \"transportation\", \"buildings\", or \"water\".\n            required_columns (list): A list of column names to select from the data. Each column name is a string.\n            category_col (str): The name of the category column to filter by when the data type is \"landcover\", \"landuse\", or \"transportation\".\n            subtypes (list, optional): A list of subtypes to filter the data by when the data type is \"landcover\", \"landuse\", or \"transportation\".\n                Each subtype is a string. If None, no subtype filtering is performed. Defaults to None.\n\n        Returns:\n            DataFrame: A DataFrame containing the filtered and processed data.\n        \"\"\"\n        # function implementation...\n\n        do_sdf = self.input_data_objects[data_type].df.select(*required_columns)\n        do_sdf = self.filter_data_to_extent(do_sdf, self.extent)\n        do_sdf = utils.cut_geodata_to_extent(do_sdf, self.current_extent, 4326)\n        do_sdf = self.filter_to_quadkey(do_sdf, self.current_quadkey, self.extraction_quadkey_level)\n        if data_type in [\"landcover\", \"landuse\", \"transportation\"]:\n            do_sdf = do_sdf.filter(F.col(category_col).isin(subtypes))\n\n        if data_type not in [\"transportation\"]:\n            do_sdf = self.filter_polygons(do_sdf)\n\n        return do_sdf\n\n    @staticmethod\n    def filter_to_quadkey(sdf: DataFrame, current_quadkey: str, quadkey_level: int) -&gt; DataFrame:\n        \"\"\"\n        Filters a DataFrame to include only rows with polygon geometries.\n\n        Args:\n            sdf (DataFrame): A DataFrame that includes a geometry column.\n\n        Returns:\n            DataFrame: A DataFrame containing only the rows from the input DataFrame where the geometry is a polygon or multipolygon.\n        \"\"\"\n\n        sdf = utils.assign_quadkey(sdf, 4326, quadkey_level)\n\n        return sdf.filter(F.col(\"quadkey\") == F.lit(current_quadkey)).drop(\"quadkey\")\n\n    @staticmethod\n    def filter_polygons(sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Filters a DataFrame to include only rows with polygon geometries.\n\n        Args:\n            sdf (DataFrame): A DataFrame that includes a geometry column.\n\n        Returns:\n            DataFrame: A DataFrame containing only the rows from the input DataFrame where the geometry is a polygon or multipolygon.\n        \"\"\"\n        return sdf.filter(STF.ST_GeometryType(F.col(ColNames.geometry)).like(\"%Polygon%\"))\n\n    @staticmethod\n    def apply_schema_casting(sdf: DataFrame, schema: StructType) -&gt; DataFrame:\n        \"\"\"\n        This function takes a DataFrame and a schema, and applies the schema to the DataFrame.\n        It selects the columns in the DataFrame that are in the schema, and casts each column to the type specified in the schema.\n\n        Args:\n            sdf (DataFrame): The DataFrame to apply the schema to.\n            schema (StructType): The schema to apply to the DataFrame.\n\n        Returns:\n            DataFrame: A new DataFrame that includes the same rows as the input DataFrame,\n            but with the columns cast to the types specified in the schema.\n        \"\"\"\n\n        sdf = sdf.select(*[F.col(field.name) for field in schema.fields])\n        for field in schema.fields:\n            sdf = sdf.withColumn(field.name, F.col(field.name).cast(field.dataType))\n\n        return sdf\n\n    @staticmethod\n    def filter_data_to_extent(sdf: DataFrame, extent: Tuple[float, float, float, float]) -&gt; DataFrame:\n        \"\"\"\n        Filters an Overture Maps DataFrame to include only rows within a specified extent.\n\n        Args:\n            sdf (DataFrame): The DataFrame to filter.\n            extent (tuple): A tuple representing the extent. The tuple contains four elements:\n                (west, south, east, north), which are the western, southern, eastern, and northern bounds of the WGS84 extent.\n\n        Returns:\n            DataFrame: A DataFrame containing only the rows from the input DataFrame where the bbox is within the extent.\n        \"\"\"\n\n        sdf = sdf.filter(\n            ((F.col(\"bbox\")[\"xmin\"]).between(F.lit(extent[0]), F.lit(extent[2])))\n            &amp; ((F.col(\"bbox\")[\"ymin\"]).between(F.lit(extent[1]), F.lit(extent[3])))\n        )\n        return sdf\n\n    @staticmethod\n    def reclassify(\n        sdf: DataFrame,\n        reclass_map: Dict[str, List[str]],\n        class_column: str,\n        reclass_column: str,\n        default_reclass: str = \"unknown\",\n    ) -&gt; DataFrame:\n        \"\"\"\n        Reclassifies a column in a DataFrame based on a reclassification map.\n\n        This function takes a DataFrame, a reclassification map, and the names of a class column and a reclassification column.\n        It creates a new column in the DataFrame by reclassifying the values in the class column based on the reclassification map.\n        If a value in the class column is not in the reclassification map, it is classified as the default reclassification.\n\n        Args:\n            sdf (DataFrame): The DataFrame to reclassify.\n            reclass_map (dict): The reclassification map. The keys are the reclassified classes, and the values are lists of original classes.\n            class_column (str): The name of the column in the DataFrame to reclassify.\n            reclass_column (str): The name of the new column to create with the reclassified classes.\n            default_reclass (str, optional): The class to assign to values in the class column that are not in the map. Defaults to \"unknown\".\n\n        Returns:\n            DataFrame: A DataFrame containing the same rows as the input DataFrame, but with the class column replaced by the reclassification column.\n        \"\"\"\n        # function implementation...\n\n        keys = list(reclass_map.keys())\n        reclass_expr = F.when(F.col(class_column).isin(reclass_map[keys[0]]), keys[0])\n\n        for key in keys[1:]:\n            reclass_expr = reclass_expr.when(F.col(class_column).isin(reclass_map[key]), key)\n\n        reclass_expr = reclass_expr.otherwise(default_reclass)\n\n        sdf = sdf.withColumn(reclass_column, reclass_expr)\n\n        return sdf.select(ColNames.category, ColNames.geometry).drop(class_column)\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/overture_data_ingestion/#components.ingestion.spatial_data_ingestion.overture_data_ingestion.OvertureDataIngestion.apply_schema_casting","title":"<code>apply_schema_casting(sdf, schema)</code>  <code>staticmethod</code>","text":"<p>This function takes a DataFrame and a schema, and applies the schema to the DataFrame. It selects the columns in the DataFrame that are in the schema, and casts each column to the type specified in the schema.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The DataFrame to apply the schema to.</p> required <code>schema</code> <code>StructType</code> <p>The schema to apply to the DataFrame.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame that includes the same rows as the input DataFrame,</p> <code>DataFrame</code> <p>but with the columns cast to the types specified in the schema.</p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/overture_data_ingestion.py</code> <pre><code>@staticmethod\ndef apply_schema_casting(sdf: DataFrame, schema: StructType) -&gt; DataFrame:\n    \"\"\"\n    This function takes a DataFrame and a schema, and applies the schema to the DataFrame.\n    It selects the columns in the DataFrame that are in the schema, and casts each column to the type specified in the schema.\n\n    Args:\n        sdf (DataFrame): The DataFrame to apply the schema to.\n        schema (StructType): The schema to apply to the DataFrame.\n\n    Returns:\n        DataFrame: A new DataFrame that includes the same rows as the input DataFrame,\n        but with the columns cast to the types specified in the schema.\n    \"\"\"\n\n    sdf = sdf.select(*[F.col(field.name) for field in schema.fields])\n    for field in schema.fields:\n        sdf = sdf.withColumn(field.name, F.col(field.name).cast(field.dataType))\n\n    return sdf\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/overture_data_ingestion/#components.ingestion.spatial_data_ingestion.overture_data_ingestion.OvertureDataIngestion.filter_data_to_extent","title":"<code>filter_data_to_extent(sdf, extent)</code>  <code>staticmethod</code>","text":"<p>Filters an Overture Maps DataFrame to include only rows within a specified extent.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The DataFrame to filter.</p> required <code>extent</code> <code>tuple</code> <p>A tuple representing the extent. The tuple contains four elements: (west, south, east, north), which are the western, southern, eastern, and northern bounds of the WGS84 extent.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing only the rows from the input DataFrame where the bbox is within the extent.</p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/overture_data_ingestion.py</code> <pre><code>@staticmethod\ndef filter_data_to_extent(sdf: DataFrame, extent: Tuple[float, float, float, float]) -&gt; DataFrame:\n    \"\"\"\n    Filters an Overture Maps DataFrame to include only rows within a specified extent.\n\n    Args:\n        sdf (DataFrame): The DataFrame to filter.\n        extent (tuple): A tuple representing the extent. The tuple contains four elements:\n            (west, south, east, north), which are the western, southern, eastern, and northern bounds of the WGS84 extent.\n\n    Returns:\n        DataFrame: A DataFrame containing only the rows from the input DataFrame where the bbox is within the extent.\n    \"\"\"\n\n    sdf = sdf.filter(\n        ((F.col(\"bbox\")[\"xmin\"]).between(F.lit(extent[0]), F.lit(extent[2])))\n        &amp; ((F.col(\"bbox\")[\"ymin\"]).between(F.lit(extent[1]), F.lit(extent[3])))\n    )\n    return sdf\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/overture_data_ingestion/#components.ingestion.spatial_data_ingestion.overture_data_ingestion.OvertureDataIngestion.filter_input_data_objects","title":"<code>filter_input_data_objects(data_type, required_columns, category_col, subtypes=None)</code>","text":"<p>Filters and processes input Overture Maps data based on the specified data type and columns.</p> <p>This function selects the required columns from the input data objects, filters the data to the current processing iteration extent, and cuts the data to the general extent. If the data type is \"landcover\", \"landuse\", or \"transportation\", it further filters the data by the specified subtypes. If the data type is not \"transportation\", it filters out invalid polygons.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>str</code> <p>The type of data to filter and process. \"landcover\", \"landuse\", \"transportation\", \"buildings\", or \"water\".</p> required <code>required_columns</code> <code>list</code> <p>A list of column names to select from the data. Each column name is a string.</p> required <code>category_col</code> <code>str</code> <p>The name of the category column to filter by when the data type is \"landcover\", \"landuse\", or \"transportation\".</p> required <code>subtypes</code> <code>list</code> <p>A list of subtypes to filter the data by when the data type is \"landcover\", \"landuse\", or \"transportation\". Each subtype is a string. If None, no subtype filtering is performed. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing the filtered and processed data.</p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/overture_data_ingestion.py</code> <pre><code>def filter_input_data_objects(\n    self,\n    data_type: str,\n    required_columns: List[str],\n    category_col: str,\n    subtypes: Optional[List[str]] = None,\n) -&gt; DataFrame:\n    \"\"\"\n    Filters and processes input Overture Maps data based on the specified data type and columns.\n\n    This function selects the required columns from the input data objects,\n    filters the data to the current processing iteration extent, and cuts the data to the general extent.\n    If the data type is \"landcover\", \"landuse\", or \"transportation\", it further filters the data by the specified subtypes.\n    If the data type is not \"transportation\", it filters out invalid polygons.\n\n    Args:\n        data_type (str): The type of data to filter and process. \"landcover\", \"landuse\", \"transportation\", \"buildings\", or \"water\".\n        required_columns (list): A list of column names to select from the data. Each column name is a string.\n        category_col (str): The name of the category column to filter by when the data type is \"landcover\", \"landuse\", or \"transportation\".\n        subtypes (list, optional): A list of subtypes to filter the data by when the data type is \"landcover\", \"landuse\", or \"transportation\".\n            Each subtype is a string. If None, no subtype filtering is performed. Defaults to None.\n\n    Returns:\n        DataFrame: A DataFrame containing the filtered and processed data.\n    \"\"\"\n    # function implementation...\n\n    do_sdf = self.input_data_objects[data_type].df.select(*required_columns)\n    do_sdf = self.filter_data_to_extent(do_sdf, self.extent)\n    do_sdf = utils.cut_geodata_to_extent(do_sdf, self.current_extent, 4326)\n    do_sdf = self.filter_to_quadkey(do_sdf, self.current_quadkey, self.extraction_quadkey_level)\n    if data_type in [\"landcover\", \"landuse\", \"transportation\"]:\n        do_sdf = do_sdf.filter(F.col(category_col).isin(subtypes))\n\n    if data_type not in [\"transportation\"]:\n        do_sdf = self.filter_polygons(do_sdf)\n\n    return do_sdf\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/overture_data_ingestion/#components.ingestion.spatial_data_ingestion.overture_data_ingestion.OvertureDataIngestion.filter_polygons","title":"<code>filter_polygons(sdf)</code>  <code>staticmethod</code>","text":"<p>Filters a DataFrame to include only rows with polygon geometries.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>A DataFrame that includes a geometry column.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing only the rows from the input DataFrame where the geometry is a polygon or multipolygon.</p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/overture_data_ingestion.py</code> <pre><code>@staticmethod\ndef filter_polygons(sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Filters a DataFrame to include only rows with polygon geometries.\n\n    Args:\n        sdf (DataFrame): A DataFrame that includes a geometry column.\n\n    Returns:\n        DataFrame: A DataFrame containing only the rows from the input DataFrame where the geometry is a polygon or multipolygon.\n    \"\"\"\n    return sdf.filter(STF.ST_GeometryType(F.col(ColNames.geometry)).like(\"%Polygon%\"))\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/overture_data_ingestion/#components.ingestion.spatial_data_ingestion.overture_data_ingestion.OvertureDataIngestion.filter_to_quadkey","title":"<code>filter_to_quadkey(sdf, current_quadkey, quadkey_level)</code>  <code>staticmethod</code>","text":"<p>Filters a DataFrame to include only rows with polygon geometries.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>A DataFrame that includes a geometry column.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing only the rows from the input DataFrame where the geometry is a polygon or multipolygon.</p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/overture_data_ingestion.py</code> <pre><code>@staticmethod\ndef filter_to_quadkey(sdf: DataFrame, current_quadkey: str, quadkey_level: int) -&gt; DataFrame:\n    \"\"\"\n    Filters a DataFrame to include only rows with polygon geometries.\n\n    Args:\n        sdf (DataFrame): A DataFrame that includes a geometry column.\n\n    Returns:\n        DataFrame: A DataFrame containing only the rows from the input DataFrame where the geometry is a polygon or multipolygon.\n    \"\"\"\n\n    sdf = utils.assign_quadkey(sdf, 4326, quadkey_level)\n\n    return sdf.filter(F.col(\"quadkey\") == F.lit(current_quadkey)).drop(\"quadkey\")\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/overture_data_ingestion/#components.ingestion.spatial_data_ingestion.overture_data_ingestion.OvertureDataIngestion.get_raw_data_for_landuse","title":"<code>get_raw_data_for_landuse(data_type, cols_to_select, filter_types=None, persist=True)</code>","text":"<p>Retrieves and processes Overture Maps raw data for a specific land use type.</p> <p>This function filters input data objects based on the specified data type and optional filter types, fixes the polygon geometry, and projects the data to a specific CRS. If the persist parameter is set to True, the resulting DataFrame is persisted in memory and disk.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>str</code> <p>The type of land use data to retrieve.</p> required <code>filter_types</code> <code>list</code> <p>A list of subtypes to filter the data by. Each subtype is a string. If None, no filtering is performed. Defaults to None.</p> <code>None</code> <code>persist</code> <code>bool</code> <p>Whether to persist the resulting DataFrame in memory and disk. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing the processed land use data.</p> <code>DataFrame</code> <p>The DataFrame includes a subtype column and a geometry column.</p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/overture_data_ingestion.py</code> <pre><code>def get_raw_data_for_landuse(\n    self,\n    data_type: str,\n    cols_to_select,\n    filter_types: Optional[List[str]] = None,\n    persist: bool = True,\n) -&gt; DataFrame:\n    \"\"\"\n    Retrieves and processes Overture Maps raw data for a specific land use type.\n\n    This function filters input data objects based on the specified data type and optional filter types,\n    fixes the polygon geometry, and projects the data to a specific CRS.\n    If the persist parameter is set to True, the resulting DataFrame is persisted in memory and disk.\n\n    Args:\n        data_type (str): The type of land use data to retrieve.\n        filter_types (list, optional): A list of subtypes to filter the data by. Each subtype is a string.\n            If None, no filtering is performed. Defaults to None.\n        persist (bool, optional): Whether to persist the resulting DataFrame in memory and disk. Defaults to True.\n\n    Returns:\n        DataFrame: A DataFrame containing the processed land use data.\n        The DataFrame includes a subtype column and a geometry column.\n    \"\"\"\n    # function implementation...\n\n    sdf = self.filter_input_data_objects(data_type, cols_to_select, \"subtype\", filter_types)\n\n    sdf = utils.project_to_crs(sdf, 4326, 3035)\n    sdf = utils.fix_geometry(sdf, 3, ColNames.geometry)\n\n    if persist:\n        sdf = sdf.persist(StorageLevel.MEMORY_AND_DISK)\n        sdf.count()\n        self.logger.info(f\"got {data_type}\")\n\n    return sdf\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/overture_data_ingestion/#components.ingestion.spatial_data_ingestion.overture_data_ingestion.OvertureDataIngestion.get_transportation_data","title":"<code>get_transportation_data()</code>","text":"<p>Processes and returns Overture Maps transportation data.</p> <p>This function filters input data objects based on the transportation class and specified subtypes, reclassifies the transportation data based on a predefined map, and selects specific columns for further processing. It then fixes the line geometry, assigns quadkeys, projects the data to a specific CRS, orders the data by quadkey, and repartitions the data based on quadkey.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing the processed transportation data.</p> <code>DataFrame</code> <p>The DataFrame includes a category column, a geometry column, and a quadkey column.</p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/overture_data_ingestion.py</code> <pre><code>def get_transportation_data(self) -&gt; DataFrame:\n    \"\"\"\n    Processes and returns Overture Maps transportation data.\n\n    This function filters input data objects based on the transportation class and specified subtypes,\n    reclassifies the transportation data based on a predefined map, and selects specific columns for further processing.\n    It then fixes the line geometry, assigns quadkeys, projects the data to a specific CRS,\n    orders the data by quadkey, and repartitions the data based on quadkey.\n\n    Returns:\n        DataFrame: A DataFrame containing the processed transportation data.\n        The DataFrame includes a category column, a geometry column, and a quadkey column.\n    \"\"\"\n    # function implementation...\n\n    transportation_sdf = self.filter_input_data_objects(\n        \"transportation\",\n        [\"class\", \"geometry\"],\n        \"class\",\n        self.transportation_filter_subtypes,\n    )\n    transportation_sdf = self.reclassify(\n        transportation_sdf,\n        self.transportation_reclass_map,\n        \"class\",\n        ColNames.category,\n    )\n    transportation_sdf = transportation_sdf.select(ColNames.category, ColNames.geometry).drop(\"subtype\")\n\n    # transportation_sdf.persist(StorageLevel.MEMORY_AND_DISK)\n    # transportation_sdf.count()\n    # self.logger.info(f\"got transportation\")\n    transportation_sdf = utils.project_to_crs(transportation_sdf, 4326, 3035)\n    transportation_sdf = utils.fix_geometry(transportation_sdf, 2)\n    transportation_sdf = utils.assign_quadkey(transportation_sdf, 3035, self.max_partition_quadkey_level)\n\n    # TODO: Figure out how to implement this\n    # transportation_sdf = utils.coarsen_quadkey_to_partition_size(transportation_sdf, self.spatial_repartition_size_rows, self.min_partition_quadkey_level)\n    transportation_sdf = transportation_sdf.orderBy(\"quadkey\")\n    transportation_sdf = transportation_sdf.repartition(\"quadkey\")\n\n    return transportation_sdf\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/overture_data_ingestion/#components.ingestion.spatial_data_ingestion.overture_data_ingestion.OvertureDataIngestion.merge_buildings_by_quadkey","title":"<code>merge_buildings_by_quadkey(sdf, crs, quadkey_level=16)</code>","text":"<p>Merges building polygons within each quadkey.</p> <p>This function assigns a quadkey to each building polygon in the input DataFrame, then groups the DataFrame by quadkey and building category, and merges the polygons within each group.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>A DataFrame containing the building polygons.</p> required <code>crs</code> <code>int</code> <p>The coordinate reference system of the input geometries.</p> required <code>qadkey_level</code> <code>int</code> <p>The zoom level to use when assigning quadkeys. Defaults to 16.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing the merged building polygons.</p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/overture_data_ingestion.py</code> <pre><code>def merge_buildings_by_quadkey(self, sdf: DataFrame, crs: int, quadkey_level: int = 16) -&gt; DataFrame:\n    \"\"\"\n    Merges building polygons within each quadkey.\n\n    This function assigns a quadkey to each building polygon in the input DataFrame,\n    then groups the DataFrame by quadkey and building category, and merges the polygons within each group.\n\n    Args:\n        sdf (DataFrame): A DataFrame containing the building polygons.\n        crs (int): The coordinate reference system of the input geometries.\n        qadkey_level (int, optional): The zoom level to use when assigning quadkeys. Defaults to 16.\n\n    Returns:\n        DataFrame: A DataFrame containing the merged building polygons.\n    \"\"\"\n\n    sdf = utils.assign_quadkey(sdf, crs, quadkey_level)\n\n    # TODO: test more if this would make any difference\n\n    # sdf = utils.coarsen_quadkey_to_partition_size(\n    #     sdf, self.spatial_repartition_size_rows, 10\n    # )\n\n    # sdf = sdf.withColumn(\"quadkey_merge\", F.col(\"quadkey\").substr(1, self.max_partition_quadkey_level))\n    # sdf = sdf.repartition(\"quadkey_merge\").drop(\"quadkey_merge\")\n\n    sdf = sdf.groupBy(\"quadkey\", \"category\").agg(STA.ST_Union_Aggr(ColNames.geometry).alias(ColNames.geometry))\n\n    return sdf\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/overture_data_ingestion/#components.ingestion.spatial_data_ingestion.overture_data_ingestion.OvertureDataIngestion.reclassify","title":"<code>reclassify(sdf, reclass_map, class_column, reclass_column, default_reclass='unknown')</code>  <code>staticmethod</code>","text":"<p>Reclassifies a column in a DataFrame based on a reclassification map.</p> <p>This function takes a DataFrame, a reclassification map, and the names of a class column and a reclassification column. It creates a new column in the DataFrame by reclassifying the values in the class column based on the reclassification map. If a value in the class column is not in the reclassification map, it is classified as the default reclassification.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The DataFrame to reclassify.</p> required <code>reclass_map</code> <code>dict</code> <p>The reclassification map. The keys are the reclassified classes, and the values are lists of original classes.</p> required <code>class_column</code> <code>str</code> <p>The name of the column in the DataFrame to reclassify.</p> required <code>reclass_column</code> <code>str</code> <p>The name of the new column to create with the reclassified classes.</p> required <code>default_reclass</code> <code>str</code> <p>The class to assign to values in the class column that are not in the map. Defaults to \"unknown\".</p> <code>'unknown'</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing the same rows as the input DataFrame, but with the class column replaced by the reclassification column.</p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/overture_data_ingestion.py</code> <pre><code>@staticmethod\ndef reclassify(\n    sdf: DataFrame,\n    reclass_map: Dict[str, List[str]],\n    class_column: str,\n    reclass_column: str,\n    default_reclass: str = \"unknown\",\n) -&gt; DataFrame:\n    \"\"\"\n    Reclassifies a column in a DataFrame based on a reclassification map.\n\n    This function takes a DataFrame, a reclassification map, and the names of a class column and a reclassification column.\n    It creates a new column in the DataFrame by reclassifying the values in the class column based on the reclassification map.\n    If a value in the class column is not in the reclassification map, it is classified as the default reclassification.\n\n    Args:\n        sdf (DataFrame): The DataFrame to reclassify.\n        reclass_map (dict): The reclassification map. The keys are the reclassified classes, and the values are lists of original classes.\n        class_column (str): The name of the column in the DataFrame to reclassify.\n        reclass_column (str): The name of the new column to create with the reclassified classes.\n        default_reclass (str, optional): The class to assign to values in the class column that are not in the map. Defaults to \"unknown\".\n\n    Returns:\n        DataFrame: A DataFrame containing the same rows as the input DataFrame, but with the class column replaced by the reclassification column.\n    \"\"\"\n    # function implementation...\n\n    keys = list(reclass_map.keys())\n    reclass_expr = F.when(F.col(class_column).isin(reclass_map[keys[0]]), keys[0])\n\n    for key in keys[1:]:\n        reclass_expr = reclass_expr.when(F.col(class_column).isin(reclass_map[key]), key)\n\n    reclass_expr = reclass_expr.otherwise(default_reclass)\n\n    sdf = sdf.withColumn(reclass_column, reclass_expr)\n\n    return sdf.select(ColNames.category, ColNames.geometry).drop(class_column)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/","title":"synthetic","text":""},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/","title":"synthetic_diaries","text":"<p>Module that generates a MNO synthetic network.</p>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries","title":"<code>SyntheticDiaries</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class that generates the synthetic activity-trip diaries data. It inherits from the Component abstract class.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>class SyntheticDiaries(Component):\n    \"\"\"\n    Class that generates the synthetic activity-trip diaries data.\n    It inherits from the Component abstract class.\n    \"\"\"\n\n    COMPONENT_ID = \"SyntheticDiaries\"\n\n    def __init__(self, general_config_path: str, component_config_path: str):\n        # keep super class init method:\n        super().__init__(general_config_path=general_config_path, component_config_path=component_config_path)\n\n        # and additionally:\n        # self.n_partitions = self.config.getint(self.COMPONENT_ID, \"n_partitions\")\n\n        self.number_of_users = self.config.getint(self.COMPONENT_ID, \"number_of_users\")\n\n        self.date_format = self.config.get(self.COMPONENT_ID, \"date_format\")\n        self.initial_date = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"initial_date\"), self.date_format\n        ).date()\n        self.number_of_dates = self.config.getint(self.COMPONENT_ID, \"number_of_dates\")\n        self.date_range = [(self.initial_date + datetime.timedelta(days=d)) for d in range(self.number_of_dates)]\n\n        self.longitude_min = self.config.getfloat(self.COMPONENT_ID, \"longitude_min\")\n        self.longitude_max = self.config.getfloat(self.COMPONENT_ID, \"longitude_max\")\n        self.latitude_min = self.config.getfloat(self.COMPONENT_ID, \"latitude_min\")\n        self.latitude_max = self.config.getfloat(self.COMPONENT_ID, \"latitude_max\")\n\n        self.home_work_distance_min = self.config.getfloat(self.COMPONENT_ID, \"home_work_distance_min\")\n        self.home_work_distance_max = self.config.getfloat(self.COMPONENT_ID, \"home_work_distance_max\")\n        self.other_distance_min = self.config.getfloat(self.COMPONENT_ID, \"other_distance_min\")\n        self.other_distance_max = self.config.getfloat(self.COMPONENT_ID, \"other_distance_max\")\n\n        self.home_duration_min = self.config.getfloat(self.COMPONENT_ID, \"home_duration_min\")\n        self.home_duration_max = self.config.getfloat(self.COMPONENT_ID, \"home_duration_max\")\n        self.work_duration_min = self.config.getfloat(self.COMPONENT_ID, \"work_duration_min\")\n        self.work_duration_max = self.config.getfloat(self.COMPONENT_ID, \"work_duration_max\")\n        self.other_duration_min = self.config.getfloat(self.COMPONENT_ID, \"other_duration_min\")\n        self.other_duration_max = self.config.getfloat(self.COMPONENT_ID, \"other_duration_max\")\n\n        self.displacement_speed = self.config.getfloat(self.COMPONENT_ID, \"displacement_speed\")\n\n        self.stay_sequence_superset = self.config.get(self.COMPONENT_ID, \"stay_sequence_superset\").split(\",\")\n        self.stay_sequence_probabilities = [\n            float(w)\n            for w in self.config.get(self.COMPONENT_ID, \"stay_sequence_probabilities\").split(\n                \",\"\n            )  # TODO: cambiar por stay_sequence\n        ]\n        assert len(self.stay_sequence_superset) == len(self.stay_sequence_probabilities)\n\n    def initalize_data_objects(self):\n        output_synthetic_diaries_data_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"diaries_data_bronze\")\n        bronze_synthetic_diaries = BronzeSyntheticDiariesDataObject(\n            self.spark,\n            output_synthetic_diaries_data_path,\n            partition_columns=[ColNames.year, ColNames.month, ColNames.day],\n        )\n        self.output_data_objects = {BronzeSyntheticDiariesDataObject.ID: bronze_synthetic_diaries}\n\n    def read(self):\n        pass  # No input datasets are used in this component\n\n    def transform(self):\n        spark = self.spark\n        activities_df = spark.createDataFrame(self.generate_activities())\n        activities_df = calc_hashed_user_id(activities_df)\n        columns = {\n            field.name: F.col(field.name).cast(field.dataType)\n            for field in BronzeSyntheticDiariesDataObject.SCHEMA.fields\n        }\n        activities_df = activities_df.withColumns(columns)\n        self.output_data_objects[BronzeSyntheticDiariesDataObject.ID].df = activities_df\n\n    def haversine(self, lon1: float, lat1: float, lon2: float, lat2: float) -&gt; float:\n        \"\"\"\n        Calculate the haversine distance in meters between two points.\n\n        Args:\n            lon1 (float): longitude of first point, in decimal degrees.\n            lat1 (float): latitude of first point, in decimal degrees.\n            lon2 (float): longitude of second point, in decimal degrees.\n            lat2 (float): latitude of second point, in decimal degrees.\n\n        Returns:\n            float: distance between both points, in meters.\n        \"\"\"\n        r = 6_371_000  # Radius of earth in meters.\n\n        # convert decimal degrees to radians\n        lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n\n        # haversine formula\n        dlon = lon2 - lon1\n        dlat = lat2 - lat1\n        a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n        c = 2 * asin(sqrt(a))\n        return c * r\n\n    def random_seed_number_generator(\n        self, base_seed: int, agent_id: int = None, date: datetime.date = None, i: int = None\n    ) -&gt; int:\n        \"\"\"\n        Generate random seed integer based on provided arguments.\n\n        Args:\n            base_seed (int): base integer for operations.\n            agent_id (int, optional): agent identifier. Defaults to None.\n            date (datetime.date, optional): date. Defaults to None.\n            i (int, optional): position integer. Defaults to None.\n\n        Returns:\n            int: generated random seed integer.\n        \"\"\"\n        seed = base_seed\n        if agent_id is not None:\n            seed += int(agent_id) * 100\n        if date is not None:\n            start_datetime = datetime.datetime.combine(date, datetime.time(0))\n            seed += int(start_datetime.timestamp())\n        if i is not None:\n            seed += i\n        return seed\n\n    def calculate_trip_time(self, o_location: tuple[float, float], d_location: tuple[float, float]) -&gt; float:\n        \"\"\"\n        Calculate trip time given an origin location and a destination\n        location, according to the specified trip speed.\n\n        Args:\n            o_location (tuple[float,float]): lon, lat of 1st point,\n                in decimal degrees.\n            d_location (tuple[float,float]): lon, lat of 2nd point,\n                in decimal degrees.\n\n        Returns:\n            float: trip time, in seconds.\n        \"\"\"\n        trip_distance = self.haversine(o_location[0], o_location[1], d_location[0], d_location[1])  # m\n        trip_speed = self.displacement_speed  # m/s\n        trip_time = trip_distance / trip_speed  # s\n        return trip_time\n\n    def calculate_trip_final_time(\n        self,\n        origin_location: tuple[float, float],\n        destin_location: tuple[float, float],\n        origin_timestamp: datetime.datetime,\n    ) -&gt; datetime.datetime:\n        \"\"\"\n        Calculate end time of a trip given an origin time, an origin location,\n        a destination location and a speed.\n\n        Args:\n            origin_location (tuple[float,float]): lon, lat of 1st point,\n                in decimal degrees.\n            destin_location (tuple[float,float]): lon, lat of 2nd point,\n                in decimal degrees.\n            origin_timestamp (datetime.datetime): start time of trip.\n\n        Returns:\n            datetime.datetime: end time of trip.\n        \"\"\"\n\n        trip_time = self.calculate_trip_time(origin_location, destin_location)  # s\n        return origin_timestamp + datetime.timedelta(seconds=trip_time)\n\n    def generate_stay_location(\n        self,\n        stay_type: str,\n        home_location: tuple[float, float],\n        work_location: tuple[float, float],\n        previous_location: tuple[float, float],\n        user_id: int,\n        date: datetime.date,\n        i: int,\n    ) -&gt; tuple[float, float]:\n        \"\"\"\n        Generate a random activity location within the bounding box limits based\n        on the activity type and previous activity locations.\n\n        Args:\n            stay_type (str): type of stay (\"home\", \"work\" or \"other\").\n            home_location (tuple[float,float]): coordinates of home location.\n            work_location (tuple[float,float]): coordinates of work location.\n            previous_location (tuple[float,float]): coordinates of previous\n                activity location.\n            user_id (int): agent identifier, used for random seed generation.\n            date (datetime.date): date, used for random seed generation.\n            i (int): activity position, used for random seed generation.\n\n        Returns:\n            tuple[float,float]: randomly generated activity location coordinates.\n        \"\"\"\n        if stay_type == \"home\":\n            location = home_location\n        elif stay_type == \"work\":\n            location = work_location\n        else:\n            location = self.generate_other_location(user_id, date, i, home_location, previous_location)\n        return location\n\n    def create_agent_activities_min_duration(\n        self,\n        user_id: int,\n        agent_stay_type_sequence: list[str],\n        home_location: tuple[float, float],\n        work_location: tuple[float, float],\n        date: datetime.date,\n        start_of_date: datetime.datetime,\n        end_of_date: datetime.datetime,\n    ) -&gt; list[Row]:\n        \"\"\"\n        Generate activities of the minimum duration following the specified agent\n        activity sequence for this agent and date.\n\n        Args:\n            user_id (int): agent identifier.\n            agent_stay_type_sequence (list[str]): list of generated stay types,\n                each represented by a string indicating the stay type.\n            home_location (tuple[float,float]): coordinates of home location.\n            work_location (tuple[float,float]): coordinates of work location.\n            date (datetime.date): date for activity sequence generation, used for\n                timestamps and random seed generation.\n            start_of_date (datetime.datetime): timestamp of current date at 00:00:00.\n            end_of_date (datetime.datetime): timestamp of current date at 23:59:59.\n\n        Returns:\n            list[Row]: list of generated activities and trips, each represented by a\n                spark row object with all its information.\n        \"\"\"\n        date_activities = []\n        previous_location = None\n        for i, stay_type in enumerate(agent_stay_type_sequence):\n            # activity location:\n            location = self.generate_stay_location(\n                stay_type, home_location, work_location, previous_location, user_id, date, i\n            )\n            # previous move (unless first stay)\n            if i != 0:\n                # move timestamps:\n                trip_initial_timestamp = stay_final_timestamp\n                trip_final_timestamp = self.calculate_trip_final_time(\n                    previous_location, location, trip_initial_timestamp\n                )\n                # add move:\n                date_activities.append(\n                    Row(\n                        user_id=user_id,\n                        activity_type=\"move\",\n                        stay_type=\"move\",\n                        longitude=float(\"nan\"),\n                        latitude=float(\"nan\"),\n                        initial_timestamp=trip_initial_timestamp,\n                        final_timestamp=trip_final_timestamp,\n                        year=date.year,\n                        month=date.month,\n                        day=date.day,\n                    )\n                )\n            # stay timestamps:\n            stay_initial_timestamp = start_of_date if i == 0 else trip_final_timestamp\n            stay_duration = self.generate_min_stay_duration(stay_type)\n            stay_final_timestamp = stay_initial_timestamp + datetime.timedelta(hours=stay_duration)\n\n            # add stay:\n            date_activities.append(\n                Row(\n                    user_id=user_id,\n                    activity_type=\"stay\",\n                    stay_type=stay_type,\n                    longitude=location[0],\n                    latitude=location[1],\n                    initial_timestamp=stay_initial_timestamp,\n                    final_timestamp=stay_final_timestamp,\n                    year=date.year,\n                    month=date.month,\n                    day=date.day,\n                )\n            )\n\n            previous_location = location\n\n        # after the iterations:\n        if not date_activities:  # 0 stays\n            condition_for_full_home = True\n        elif stay_final_timestamp &gt; end_of_date:  # too many stays\n            condition_for_full_home = True\n        else:\n            condition_for_full_home = False\n\n        if condition_for_full_home:  # simple \"only home\" diary\n            return [\n                Row(\n                    user_id=user_id,\n                    activity_type=\"stay\",\n                    stay_type=\"home\",\n                    longitude=home_location[0],\n                    latitude=home_location[1],\n                    initial_timestamp=start_of_date,\n                    final_timestamp=end_of_date,\n                    year=date.year,\n                    month=date.month,\n                    day=date.day,\n                )\n            ]\n        else:\n            return date_activities  # actual generated diary\n\n    def update_spark_row(self, row: Row, column_name: str, new_value: Any) -&gt; Row:\n        \"\"\"\n        Return an updated spark row object, changing the value of a column.\n\n        Args:\n            row (Row): input spark row.\n            column_name (str): name of column to modify.\n            new_value (Any): new value to assign.\n\n        Returns:\n            Row: modified spark row\n        \"\"\"\n        return Row(**{**row.asDict(), **{column_name: new_value}})\n\n    def adjust_activity_times(\n        self,\n        date_activities: list[Row],\n        remaining_time: float,\n        user_id: int,\n        date: datetime.date,\n        start_of_date: datetime.datetime,\n        end_of_date: datetime.datetime,\n    ):\n        \"\"\"\n        Modifies the \"date_activities\" list, changing the initial and\n        final timestamps of both stays and moves probablilistically in order to\n        generate stay durations different from the minimum and adjust the\n        durations of the activities to the 24h of the day.\n\n        Args:\n            date_activities (list[Row]): list of generated activities (stays and\n                moves) of the agent for the specified date. Each activity/trip is a\n                spark row object.\n            user_id (int): agent identifier.\n            date (datetime.date): date for activity sequence generation, used for\n                timestamps and random seed generation.\n            start_of_date (datetime.datetime): timestamp of current date at 00:00:00.\n            end_of_date (datetime.datetime): timestamp of current date at 23:59:59.\n        \"\"\"\n        current_timestamp = start_of_date\n        for i, activity_row in enumerate(date_activities):\n            if activity_row.activity_type == \"stay\":  # stay:\n                stay_type = activity_row.stay_type\n                old_stay_duration = (\n                    activity_row.final_timestamp - activity_row.initial_timestamp\n                ).total_seconds() / 3600.0\n                new_initial_timestamp = current_timestamp\n                if i == len(date_activities) - 1:\n                    new_final_timestamp = end_of_date\n                    remaining_time = 0.0\n                else:\n                    new_stay_duration = self.generate_stay_duration(user_id, date, i, stay_type, remaining_time)\n                    new_duration_td = datetime.timedelta(seconds=new_stay_duration * 3600.0)\n                    new_final_timestamp = new_initial_timestamp + new_duration_td\n                    remaining_time -= new_stay_duration - old_stay_duration\n            else:  # move:\n                old_move_duration = activity_row.final_timestamp - activity_row.initial_timestamp\n                new_initial_timestamp = current_timestamp\n                new_final_timestamp = new_initial_timestamp + old_move_duration\n\n            # common for all activities (stays and moves):\n            activity_row = self.update_spark_row(activity_row, \"initial_timestamp\", new_initial_timestamp)\n            activity_row = self.update_spark_row(activity_row, \"final_timestamp\", new_final_timestamp)\n            date_activities[i] = activity_row\n            current_timestamp = new_final_timestamp\n\n    def add_agent_date_activities(\n        self,\n        activities: list[Row],\n        user_id: int,\n        agent_stay_type_sequence: list[str],\n        home_location: tuple[float, float],\n        work_location: tuple[float, float],\n        date: datetime.date,\n        start_of_date: datetime.datetime,\n        end_of_date: datetime.datetime,\n    ):\n        \"\"\"\n        For a specific date and user, generate a sequence of activities probabilistically\n        according to the specified activity superset and the activity probabilities.\n        Firstly, assign to each of these activities the minimum duration considered for\n        that activity type. Trip times are based on Pythagorean distance and a specified\n        average speed.\n        If the sum of all minimum duration of the activities and the duration of the trips\n        is higher than the 24h of the day, then assign just one \"home\" activity to the\n        agent from 00:00:00 to 23:59:59.\n        Else, there will be a remaining time. E.g., the diary of an agent, after adding\n        up all trip durations and minimum activity durations may end at 20:34:57. There is\n        a remaining time to complete the full diary (23:59:59 - 20:34:57).\n        Adjust activity times probabilistically according to the maximum activity duration\n        and this remaining time, making the diary end at exactly 23:59:59.\n\n        Args:\n            activities (list[Row]): list of generated activities (stays and moves) for\n                the agent for all of the specified dates. Each activity is a spark\n                row object.\n            user_id (int): agent identifier.\n            agent_stay_type_sequence (list[str]): list of generated stay types,\n                each represented by a string indicating the stay type.\n            home_location (tuple[float,float]): coordinates of home location.\n            work_location (tuple[float,float]): coordinates of work location.\n            date (datetime.date): date for activity sequence generation, used for\n                timestamps and random seed generation.\n            start_of_date (datetime.datetime): timestamp of current date at 00:00:00.\n            end_of_date (datetime.datetime): timestamp of current date at 23:59:59.\n        \"\"\"\n        date_activities = self.create_agent_activities_min_duration(\n            user_id, agent_stay_type_sequence, home_location, work_location, date, start_of_date, end_of_date\n        )\n        remaining_time = (end_of_date - date_activities[-1].final_timestamp).total_seconds() / 3600.0\n\n        if remaining_time != 0:\n            self.adjust_activity_times(\n                date_activities,\n                remaining_time,\n                user_id,\n                date,\n                start_of_date,\n                end_of_date,\n            )\n        activities += date_activities\n\n    def add_date_activities(self, date: datetime.date, activities: list[Row]):\n        \"\"\"\n        Generate activity (stays and moves) rows for a specific date according to\n        parameters.\n\n        Args:\n            date (datetime.date): date for activity sequence generation, used for\n                timestamps and random seed generation.\n            activities (list[Row]): list of generated activities (stays and moves) for\n                the agent for all of the specified dates. Each activity is a spark\n                row object.\n        \"\"\"\n        # Start of date, end of date: datetime object generation\n        start_of_date = datetime.datetime.combine(date, datetime.time(0, 0, 0))\n        end_of_date = datetime.datetime.combine(date, datetime.time(23, 59, 59))\n        for user_id in range(self.number_of_users):\n            # generate user information:\n            agent_stay_type_sequence = self.generate_stay_type_sequence(user_id, date)\n            home_location = self.generate_home_location(user_id)\n            work_location = self.generate_work_location(user_id, home_location)\n            self.add_agent_date_activities(\n                activities,\n                user_id,\n                agent_stay_type_sequence,\n                home_location,\n                work_location,\n                date,\n                start_of_date,\n                end_of_date,\n            )\n\n    def generate_activities(self) -&gt; list[Row]:\n        \"\"\"\n        Generate activity and trip rows according to parameters.\n\n        Returns:\n            list[Row]: list of generated activities and trips for the agent for all\n                of the specified dates. Each activity/trip is a spark row object.\n        \"\"\"\n        activities = []\n        for date in self.date_range:\n            self.add_date_activities(date, activities)\n        return activities\n\n    def generate_lonlat_at_distance(self, lon1: float, lat1: float, d: float, seed: int) -&gt; tuple[float, float]:\n        \"\"\"\n        Given a point (lon, lat) and a distance, in meters, calculate a new random\n        point that is exactly at the specified distance of the provided lon, lat.\n\n        Args:\n            lon1 (float): longitude of point, specified in decimal degrees.\n            lat1 (float): latitude of point, specified in decimal degrees.\n            d (float): distance, in meters.\n            seed (int): random seed integer.\n\n        Returns:\n            tuple[float, float]: coordinates of randomly generated point.\n        \"\"\"\n        r = 6_371_000  # Radius of earth in meters.\n\n        d_x = Random(seed).uniform(0, d)\n        d_y = sqrt(d**2 - d_x**2)\n\n        # firstly, convert lat to radians for later\n        lat1_radians = lat1 * pi / 180.0\n\n        # how many meters correspond to one degree of latitude?\n        deg_to_meters = r * pi / 180  # aprox. 111111 meters\n        # thus, the northwards displacement, in degrees of latitude is:\n        north_delta = d_y / deg_to_meters\n\n        # but one degree of longitude does not always correspond to the\n        # same distance... depends on the latitude at where you are!\n        parallel_radius = abs(r * cos(lat1_radians))\n        deg_to_meters = parallel_radius * pi / 180  # variable\n        # thus, the eastwards displacement, in degrees of longitude is:\n        east_delta = d_x / deg_to_meters\n\n        final_lon = lon1 + east_delta * Random(seed).choice([-1, 1])\n        final_lat = lat1 + north_delta * Random(seed).choice([-1, 1])\n\n        return (final_lon, final_lat)\n\n    def generate_home_location(self, agent_id: int) -&gt; tuple[float, float]:\n        \"\"\"\n        Generate random home location based on bounding box limits.\n\n        Args:\n            agent_id (int): identifier of agent, used for random seed generation.\n\n        Returns:\n            tuple[float,float]: coordinates of generated home location.\n        \"\"\"\n        seed_lon = self.random_seed_number_generator(1, agent_id)\n        seed_lat = self.random_seed_number_generator(2, agent_id)\n        hlon = Random(seed_lon).uniform(self.longitude_min, self.longitude_max)\n        hlat = Random(seed_lat).uniform(self.latitude_min, self.latitude_max)\n        return (hlon, hlat)\n\n    def generate_work_location(\n        self, agent_id: int, home_location: tuple[float, float], seed: int = 4\n    ) -&gt; tuple[float, float]:\n        \"\"\"\n        Generate random work location based on home location and maximum distance to\n        home. If the work location falls outside of bounding box limits, try again.\n\n        Args:\n            agent_id (int): identifier of agent, used for random seed generation.\n            home_location (tuple[float,float]): coordinates of home location.\n            seed (int, optional): random seed integer. Defaults to 4.\n\n        Returns:\n            tuple[float,float]: coordinates of generated work location.\n        \"\"\"\n        seed_distance = self.random_seed_number_generator(seed - 1, agent_id)\n        random_distance = Random(seed_distance).uniform(self.home_work_distance_min, self.home_work_distance_max)\n        hlon, hlat = home_location\n        seed_coords = self.random_seed_number_generator(seed, agent_id)\n        wlon, wlat = self.generate_lonlat_at_distance(hlon, hlat, random_distance, seed_coords)\n\n        if not (self.longitude_min &lt; wlon &lt; self.longitude_max) or not (\n            self.latitude_min &lt; wlat &lt; self.latitude_max\n        ):  # outside limits\n            seed += 1\n            wlon, wlat = self.generate_work_location(agent_id, home_location, seed=seed)\n\n        return (wlon, wlat)\n\n    def generate_other_location(\n        self,\n        agent_id: int,\n        date: datetime.date,\n        activity_number: int,\n        home_location: tuple[float, float],\n        previous_location: tuple[float, float],\n        seed: int = 6,\n    ) -&gt; tuple[float, float]:\n        \"\"\"\n        Generate other activity location based on previous location and maximum distance\n        to previous location. If there is no previous location (this is the first\n        activity of the day), then the home location is considered as previous location.\n        If the location falls outside of bounding box limits, try again.\n\n        Args:\n            agent_id (int): identifier of agent, used for random seed generation.\n            date (datetime.date): date, used for random seed generation.\n            activity_number (int): act position, used for random seed generation.\n            home_location (tuple[float,float]): coordinates of home location.\n            previous_location (tuple[float,float]): coordinates of previous location.\n            seed (int, optional): random seed integer. Defaults to 6.\n\n        Returns:\n            tuple[float,float]: coordinates of generated location.\n        \"\"\"\n        seed_distance = self.random_seed_number_generator(seed - 1, agent_id)\n        random_distance = Random(seed_distance).uniform(self.other_distance_min, self.other_distance_max)\n        if previous_location is None:\n            plon, plat = home_location\n        else:\n            plon, plat = previous_location\n\n        seed_coords = self.random_seed_number_generator(seed, agent_id)\n        olon, olat = self.generate_lonlat_at_distance(plon, plat, random_distance, seed_coords)\n        if not (self.longitude_min &lt; olon &lt; self.longitude_max) or not (\n            self.latitude_min &lt; olat &lt; self.latitude_max\n        ):  # outside limits\n            seed += 1\n            olon, olat = self.generate_other_location(\n                agent_id, date, activity_number, home_location, previous_location, seed=seed\n            )\n\n        return (olon, olat)\n\n    def generate_stay_duration(\n        self, agent_id: int, date: datetime.date, i: int, stay_type: str, remaining_time: float\n    ) -&gt; float:\n        \"\"\"\n        Generate stay duration probabilistically based on activity type\n        abd remaining time.\n\n        Args:\n            agent_id (int): identifier of agent, used for random seed generation.\n            date (datetime.date): date, used for random seed generation.\n            i (int): activity position, used for random seed generation.\n            stay_type (str): type of stay. Shall be \"home\", \"work\" or \"other\".\n            remaining_time (float): same units as durations.\n\n        Returns:\n            float: generated activity duration.\n        \"\"\"\n        if stay_type == \"home\":\n            min_duration = self.home_duration_min\n            max_duration = self.home_duration_max\n        elif stay_type == \"work\":\n            min_duration = self.work_duration_min\n            max_duration = self.work_duration_max\n        elif stay_type == \"other\":\n            min_duration = self.other_duration_min\n            max_duration = self.other_duration_max\n        else:\n            raise ValueError\n        seed = self.random_seed_number_generator(7, agent_id, date, i)\n        max_value = min(max_duration, min_duration + remaining_time)\n        return Random(seed).uniform(min_duration, max_value)\n\n    def generate_min_stay_duration(self, stay_type: str) -&gt; float:\n        \"\"\"\n        Generate minimum stay duration based on stay type specifications.\n\n        Args:\n            stay_type (str): type of stay. Shall be \"home\", \"work\" or \"other\".\n\n        Returns:\n            float: minimum stay duration.\n        \"\"\"\n        if stay_type == \"home\":\n            return self.home_duration_min\n        elif stay_type == \"work\":\n            return self.work_duration_min\n        elif stay_type == \"other\":\n            return self.other_duration_min\n        else:\n            raise ValueError\n\n    def remove_consecutive_stay_types(self, stay_sequence_list: list[str], stay_types_to_group: set[str]) -&gt; list[str]:\n        \"\"\"\n        Generate new list replacing consecutive stays of the same type by\n        a unique stay as long as the stay type is contained in the\n        \"stay_types_to_group\" list.\n\n        Args:\n            stay_sequence_list (list[str]): input stay type list.\n            stay_types_to_group (set[str]): stay types to group.\n\n        Returns:\n            list[str]: output stay sequence list.\n        \"\"\"\n        new_stay_sequence_list = []\n        previous_stay = None\n        for stay in stay_sequence_list:\n            if stay == previous_stay and stay in stay_types_to_group:\n                pass\n            else:\n                new_stay_sequence_list.append(stay)\n            previous_stay = stay\n        return new_stay_sequence_list\n\n    def generate_stay_type_sequence(self, agent_id: int, date: datetime.date) -&gt; list[str]:\n        \"\"\"\n        Generate the sequence of stay types for an agent for a specific date\n        probabilistically based on the superset sequence and specified\n        probabilities.\n        Replace 'home'-'home' and 'work'-'work' sequences by just 'home' or\n        'work'.\n\n        Args:\n            agent_id (int): identifier of agent, used for random seed generation.\n            date (datetime.date): date for activity sequence generation, used for\n                random seed generation.\n\n        Returns:\n            list[str]: list of generated stay types, each represented by a string\n                indicating the stay type (e.g. \"home\", \"work\", \"other\").\n        \"\"\"\n        stay_type_sequence = []\n        for i, stay_type in enumerate(self.stay_sequence_superset):\n            stay_weight = self.stay_sequence_probabilities[i]\n            seed = self.random_seed_number_generator(0, agent_id, date, i)\n            if Random(seed).random() &lt; stay_weight:\n                stay_type_sequence.append(stay_type)\n        stay_type_sequence = self.remove_consecutive_stay_types(stay_type_sequence, {\"home\", \"work\"})\n        return stay_type_sequence\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.add_agent_date_activities","title":"<code>add_agent_date_activities(activities, user_id, agent_stay_type_sequence, home_location, work_location, date, start_of_date, end_of_date)</code>","text":"<p>For a specific date and user, generate a sequence of activities probabilistically according to the specified activity superset and the activity probabilities. Firstly, assign to each of these activities the minimum duration considered for that activity type. Trip times are based on Pythagorean distance and a specified average speed. If the sum of all minimum duration of the activities and the duration of the trips is higher than the 24h of the day, then assign just one \"home\" activity to the agent from 00:00:00 to 23:59:59. Else, there will be a remaining time. E.g., the diary of an agent, after adding up all trip durations and minimum activity durations may end at 20:34:57. There is a remaining time to complete the full diary (23:59:59 - 20:34:57). Adjust activity times probabilistically according to the maximum activity duration and this remaining time, making the diary end at exactly 23:59:59.</p> <p>Parameters:</p> Name Type Description Default <code>activities</code> <code>list[Row]</code> <p>list of generated activities (stays and moves) for the agent for all of the specified dates. Each activity is a spark row object.</p> required <code>user_id</code> <code>int</code> <p>agent identifier.</p> required <code>agent_stay_type_sequence</code> <code>list[str]</code> <p>list of generated stay types, each represented by a string indicating the stay type.</p> required <code>home_location</code> <code>tuple[float, float]</code> <p>coordinates of home location.</p> required <code>work_location</code> <code>tuple[float, float]</code> <p>coordinates of work location.</p> required <code>date</code> <code>date</code> <p>date for activity sequence generation, used for timestamps and random seed generation.</p> required <code>start_of_date</code> <code>datetime</code> <p>timestamp of current date at 00:00:00.</p> required <code>end_of_date</code> <code>datetime</code> <p>timestamp of current date at 23:59:59.</p> required Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def add_agent_date_activities(\n    self,\n    activities: list[Row],\n    user_id: int,\n    agent_stay_type_sequence: list[str],\n    home_location: tuple[float, float],\n    work_location: tuple[float, float],\n    date: datetime.date,\n    start_of_date: datetime.datetime,\n    end_of_date: datetime.datetime,\n):\n    \"\"\"\n    For a specific date and user, generate a sequence of activities probabilistically\n    according to the specified activity superset and the activity probabilities.\n    Firstly, assign to each of these activities the minimum duration considered for\n    that activity type. Trip times are based on Pythagorean distance and a specified\n    average speed.\n    If the sum of all minimum duration of the activities and the duration of the trips\n    is higher than the 24h of the day, then assign just one \"home\" activity to the\n    agent from 00:00:00 to 23:59:59.\n    Else, there will be a remaining time. E.g., the diary of an agent, after adding\n    up all trip durations and minimum activity durations may end at 20:34:57. There is\n    a remaining time to complete the full diary (23:59:59 - 20:34:57).\n    Adjust activity times probabilistically according to the maximum activity duration\n    and this remaining time, making the diary end at exactly 23:59:59.\n\n    Args:\n        activities (list[Row]): list of generated activities (stays and moves) for\n            the agent for all of the specified dates. Each activity is a spark\n            row object.\n        user_id (int): agent identifier.\n        agent_stay_type_sequence (list[str]): list of generated stay types,\n            each represented by a string indicating the stay type.\n        home_location (tuple[float,float]): coordinates of home location.\n        work_location (tuple[float,float]): coordinates of work location.\n        date (datetime.date): date for activity sequence generation, used for\n            timestamps and random seed generation.\n        start_of_date (datetime.datetime): timestamp of current date at 00:00:00.\n        end_of_date (datetime.datetime): timestamp of current date at 23:59:59.\n    \"\"\"\n    date_activities = self.create_agent_activities_min_duration(\n        user_id, agent_stay_type_sequence, home_location, work_location, date, start_of_date, end_of_date\n    )\n    remaining_time = (end_of_date - date_activities[-1].final_timestamp).total_seconds() / 3600.0\n\n    if remaining_time != 0:\n        self.adjust_activity_times(\n            date_activities,\n            remaining_time,\n            user_id,\n            date,\n            start_of_date,\n            end_of_date,\n        )\n    activities += date_activities\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.add_date_activities","title":"<code>add_date_activities(date, activities)</code>","text":"<p>Generate activity (stays and moves) rows for a specific date according to parameters.</p> <p>Parameters:</p> Name Type Description Default <code>date</code> <code>date</code> <p>date for activity sequence generation, used for timestamps and random seed generation.</p> required <code>activities</code> <code>list[Row]</code> <p>list of generated activities (stays and moves) for the agent for all of the specified dates. Each activity is a spark row object.</p> required Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def add_date_activities(self, date: datetime.date, activities: list[Row]):\n    \"\"\"\n    Generate activity (stays and moves) rows for a specific date according to\n    parameters.\n\n    Args:\n        date (datetime.date): date for activity sequence generation, used for\n            timestamps and random seed generation.\n        activities (list[Row]): list of generated activities (stays and moves) for\n            the agent for all of the specified dates. Each activity is a spark\n            row object.\n    \"\"\"\n    # Start of date, end of date: datetime object generation\n    start_of_date = datetime.datetime.combine(date, datetime.time(0, 0, 0))\n    end_of_date = datetime.datetime.combine(date, datetime.time(23, 59, 59))\n    for user_id in range(self.number_of_users):\n        # generate user information:\n        agent_stay_type_sequence = self.generate_stay_type_sequence(user_id, date)\n        home_location = self.generate_home_location(user_id)\n        work_location = self.generate_work_location(user_id, home_location)\n        self.add_agent_date_activities(\n            activities,\n            user_id,\n            agent_stay_type_sequence,\n            home_location,\n            work_location,\n            date,\n            start_of_date,\n            end_of_date,\n        )\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.adjust_activity_times","title":"<code>adjust_activity_times(date_activities, remaining_time, user_id, date, start_of_date, end_of_date)</code>","text":"<p>Modifies the \"date_activities\" list, changing the initial and final timestamps of both stays and moves probablilistically in order to generate stay durations different from the minimum and adjust the durations of the activities to the 24h of the day.</p> <p>Parameters:</p> Name Type Description Default <code>date_activities</code> <code>list[Row]</code> <p>list of generated activities (stays and moves) of the agent for the specified date. Each activity/trip is a spark row object.</p> required <code>user_id</code> <code>int</code> <p>agent identifier.</p> required <code>date</code> <code>date</code> <p>date for activity sequence generation, used for timestamps and random seed generation.</p> required <code>start_of_date</code> <code>datetime</code> <p>timestamp of current date at 00:00:00.</p> required <code>end_of_date</code> <code>datetime</code> <p>timestamp of current date at 23:59:59.</p> required Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def adjust_activity_times(\n    self,\n    date_activities: list[Row],\n    remaining_time: float,\n    user_id: int,\n    date: datetime.date,\n    start_of_date: datetime.datetime,\n    end_of_date: datetime.datetime,\n):\n    \"\"\"\n    Modifies the \"date_activities\" list, changing the initial and\n    final timestamps of both stays and moves probablilistically in order to\n    generate stay durations different from the minimum and adjust the\n    durations of the activities to the 24h of the day.\n\n    Args:\n        date_activities (list[Row]): list of generated activities (stays and\n            moves) of the agent for the specified date. Each activity/trip is a\n            spark row object.\n        user_id (int): agent identifier.\n        date (datetime.date): date for activity sequence generation, used for\n            timestamps and random seed generation.\n        start_of_date (datetime.datetime): timestamp of current date at 00:00:00.\n        end_of_date (datetime.datetime): timestamp of current date at 23:59:59.\n    \"\"\"\n    current_timestamp = start_of_date\n    for i, activity_row in enumerate(date_activities):\n        if activity_row.activity_type == \"stay\":  # stay:\n            stay_type = activity_row.stay_type\n            old_stay_duration = (\n                activity_row.final_timestamp - activity_row.initial_timestamp\n            ).total_seconds() / 3600.0\n            new_initial_timestamp = current_timestamp\n            if i == len(date_activities) - 1:\n                new_final_timestamp = end_of_date\n                remaining_time = 0.0\n            else:\n                new_stay_duration = self.generate_stay_duration(user_id, date, i, stay_type, remaining_time)\n                new_duration_td = datetime.timedelta(seconds=new_stay_duration * 3600.0)\n                new_final_timestamp = new_initial_timestamp + new_duration_td\n                remaining_time -= new_stay_duration - old_stay_duration\n        else:  # move:\n            old_move_duration = activity_row.final_timestamp - activity_row.initial_timestamp\n            new_initial_timestamp = current_timestamp\n            new_final_timestamp = new_initial_timestamp + old_move_duration\n\n        # common for all activities (stays and moves):\n        activity_row = self.update_spark_row(activity_row, \"initial_timestamp\", new_initial_timestamp)\n        activity_row = self.update_spark_row(activity_row, \"final_timestamp\", new_final_timestamp)\n        date_activities[i] = activity_row\n        current_timestamp = new_final_timestamp\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.calculate_trip_final_time","title":"<code>calculate_trip_final_time(origin_location, destin_location, origin_timestamp)</code>","text":"<p>Calculate end time of a trip given an origin time, an origin location, a destination location and a speed.</p> <p>Parameters:</p> Name Type Description Default <code>origin_location</code> <code>tuple[float, float]</code> <p>lon, lat of 1st point, in decimal degrees.</p> required <code>destin_location</code> <code>tuple[float, float]</code> <p>lon, lat of 2nd point, in decimal degrees.</p> required <code>origin_timestamp</code> <code>datetime</code> <p>start time of trip.</p> required <p>Returns:</p> Type Description <code>datetime</code> <p>datetime.datetime: end time of trip.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def calculate_trip_final_time(\n    self,\n    origin_location: tuple[float, float],\n    destin_location: tuple[float, float],\n    origin_timestamp: datetime.datetime,\n) -&gt; datetime.datetime:\n    \"\"\"\n    Calculate end time of a trip given an origin time, an origin location,\n    a destination location and a speed.\n\n    Args:\n        origin_location (tuple[float,float]): lon, lat of 1st point,\n            in decimal degrees.\n        destin_location (tuple[float,float]): lon, lat of 2nd point,\n            in decimal degrees.\n        origin_timestamp (datetime.datetime): start time of trip.\n\n    Returns:\n        datetime.datetime: end time of trip.\n    \"\"\"\n\n    trip_time = self.calculate_trip_time(origin_location, destin_location)  # s\n    return origin_timestamp + datetime.timedelta(seconds=trip_time)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.calculate_trip_time","title":"<code>calculate_trip_time(o_location, d_location)</code>","text":"<p>Calculate trip time given an origin location and a destination location, according to the specified trip speed.</p> <p>Parameters:</p> Name Type Description Default <code>o_location</code> <code>tuple[float, float]</code> <p>lon, lat of 1st point, in decimal degrees.</p> required <code>d_location</code> <code>tuple[float, float]</code> <p>lon, lat of 2nd point, in decimal degrees.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>trip time, in seconds.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def calculate_trip_time(self, o_location: tuple[float, float], d_location: tuple[float, float]) -&gt; float:\n    \"\"\"\n    Calculate trip time given an origin location and a destination\n    location, according to the specified trip speed.\n\n    Args:\n        o_location (tuple[float,float]): lon, lat of 1st point,\n            in decimal degrees.\n        d_location (tuple[float,float]): lon, lat of 2nd point,\n            in decimal degrees.\n\n    Returns:\n        float: trip time, in seconds.\n    \"\"\"\n    trip_distance = self.haversine(o_location[0], o_location[1], d_location[0], d_location[1])  # m\n    trip_speed = self.displacement_speed  # m/s\n    trip_time = trip_distance / trip_speed  # s\n    return trip_time\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.create_agent_activities_min_duration","title":"<code>create_agent_activities_min_duration(user_id, agent_stay_type_sequence, home_location, work_location, date, start_of_date, end_of_date)</code>","text":"<p>Generate activities of the minimum duration following the specified agent activity sequence for this agent and date.</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>int</code> <p>agent identifier.</p> required <code>agent_stay_type_sequence</code> <code>list[str]</code> <p>list of generated stay types, each represented by a string indicating the stay type.</p> required <code>home_location</code> <code>tuple[float, float]</code> <p>coordinates of home location.</p> required <code>work_location</code> <code>tuple[float, float]</code> <p>coordinates of work location.</p> required <code>date</code> <code>date</code> <p>date for activity sequence generation, used for timestamps and random seed generation.</p> required <code>start_of_date</code> <code>datetime</code> <p>timestamp of current date at 00:00:00.</p> required <code>end_of_date</code> <code>datetime</code> <p>timestamp of current date at 23:59:59.</p> required <p>Returns:</p> Type Description <code>list[Row]</code> <p>list[Row]: list of generated activities and trips, each represented by a spark row object with all its information.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def create_agent_activities_min_duration(\n    self,\n    user_id: int,\n    agent_stay_type_sequence: list[str],\n    home_location: tuple[float, float],\n    work_location: tuple[float, float],\n    date: datetime.date,\n    start_of_date: datetime.datetime,\n    end_of_date: datetime.datetime,\n) -&gt; list[Row]:\n    \"\"\"\n    Generate activities of the minimum duration following the specified agent\n    activity sequence for this agent and date.\n\n    Args:\n        user_id (int): agent identifier.\n        agent_stay_type_sequence (list[str]): list of generated stay types,\n            each represented by a string indicating the stay type.\n        home_location (tuple[float,float]): coordinates of home location.\n        work_location (tuple[float,float]): coordinates of work location.\n        date (datetime.date): date for activity sequence generation, used for\n            timestamps and random seed generation.\n        start_of_date (datetime.datetime): timestamp of current date at 00:00:00.\n        end_of_date (datetime.datetime): timestamp of current date at 23:59:59.\n\n    Returns:\n        list[Row]: list of generated activities and trips, each represented by a\n            spark row object with all its information.\n    \"\"\"\n    date_activities = []\n    previous_location = None\n    for i, stay_type in enumerate(agent_stay_type_sequence):\n        # activity location:\n        location = self.generate_stay_location(\n            stay_type, home_location, work_location, previous_location, user_id, date, i\n        )\n        # previous move (unless first stay)\n        if i != 0:\n            # move timestamps:\n            trip_initial_timestamp = stay_final_timestamp\n            trip_final_timestamp = self.calculate_trip_final_time(\n                previous_location, location, trip_initial_timestamp\n            )\n            # add move:\n            date_activities.append(\n                Row(\n                    user_id=user_id,\n                    activity_type=\"move\",\n                    stay_type=\"move\",\n                    longitude=float(\"nan\"),\n                    latitude=float(\"nan\"),\n                    initial_timestamp=trip_initial_timestamp,\n                    final_timestamp=trip_final_timestamp,\n                    year=date.year,\n                    month=date.month,\n                    day=date.day,\n                )\n            )\n        # stay timestamps:\n        stay_initial_timestamp = start_of_date if i == 0 else trip_final_timestamp\n        stay_duration = self.generate_min_stay_duration(stay_type)\n        stay_final_timestamp = stay_initial_timestamp + datetime.timedelta(hours=stay_duration)\n\n        # add stay:\n        date_activities.append(\n            Row(\n                user_id=user_id,\n                activity_type=\"stay\",\n                stay_type=stay_type,\n                longitude=location[0],\n                latitude=location[1],\n                initial_timestamp=stay_initial_timestamp,\n                final_timestamp=stay_final_timestamp,\n                year=date.year,\n                month=date.month,\n                day=date.day,\n            )\n        )\n\n        previous_location = location\n\n    # after the iterations:\n    if not date_activities:  # 0 stays\n        condition_for_full_home = True\n    elif stay_final_timestamp &gt; end_of_date:  # too many stays\n        condition_for_full_home = True\n    else:\n        condition_for_full_home = False\n\n    if condition_for_full_home:  # simple \"only home\" diary\n        return [\n            Row(\n                user_id=user_id,\n                activity_type=\"stay\",\n                stay_type=\"home\",\n                longitude=home_location[0],\n                latitude=home_location[1],\n                initial_timestamp=start_of_date,\n                final_timestamp=end_of_date,\n                year=date.year,\n                month=date.month,\n                day=date.day,\n            )\n        ]\n    else:\n        return date_activities  # actual generated diary\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_activities","title":"<code>generate_activities()</code>","text":"<p>Generate activity and trip rows according to parameters.</p> <p>Returns:</p> Type Description <code>list[Row]</code> <p>list[Row]: list of generated activities and trips for the agent for all of the specified dates. Each activity/trip is a spark row object.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_activities(self) -&gt; list[Row]:\n    \"\"\"\n    Generate activity and trip rows according to parameters.\n\n    Returns:\n        list[Row]: list of generated activities and trips for the agent for all\n            of the specified dates. Each activity/trip is a spark row object.\n    \"\"\"\n    activities = []\n    for date in self.date_range:\n        self.add_date_activities(date, activities)\n    return activities\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_home_location","title":"<code>generate_home_location(agent_id)</code>","text":"<p>Generate random home location based on bounding box limits.</p> <p>Parameters:</p> Name Type Description Default <code>agent_id</code> <code>int</code> <p>identifier of agent, used for random seed generation.</p> required <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>tuple[float,float]: coordinates of generated home location.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_home_location(self, agent_id: int) -&gt; tuple[float, float]:\n    \"\"\"\n    Generate random home location based on bounding box limits.\n\n    Args:\n        agent_id (int): identifier of agent, used for random seed generation.\n\n    Returns:\n        tuple[float,float]: coordinates of generated home location.\n    \"\"\"\n    seed_lon = self.random_seed_number_generator(1, agent_id)\n    seed_lat = self.random_seed_number_generator(2, agent_id)\n    hlon = Random(seed_lon).uniform(self.longitude_min, self.longitude_max)\n    hlat = Random(seed_lat).uniform(self.latitude_min, self.latitude_max)\n    return (hlon, hlat)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_lonlat_at_distance","title":"<code>generate_lonlat_at_distance(lon1, lat1, d, seed)</code>","text":"<p>Given a point (lon, lat) and a distance, in meters, calculate a new random point that is exactly at the specified distance of the provided lon, lat.</p> <p>Parameters:</p> Name Type Description Default <code>lon1</code> <code>float</code> <p>longitude of point, specified in decimal degrees.</p> required <code>lat1</code> <code>float</code> <p>latitude of point, specified in decimal degrees.</p> required <code>d</code> <code>float</code> <p>distance, in meters.</p> required <code>seed</code> <code>int</code> <p>random seed integer.</p> required <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>tuple[float, float]: coordinates of randomly generated point.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_lonlat_at_distance(self, lon1: float, lat1: float, d: float, seed: int) -&gt; tuple[float, float]:\n    \"\"\"\n    Given a point (lon, lat) and a distance, in meters, calculate a new random\n    point that is exactly at the specified distance of the provided lon, lat.\n\n    Args:\n        lon1 (float): longitude of point, specified in decimal degrees.\n        lat1 (float): latitude of point, specified in decimal degrees.\n        d (float): distance, in meters.\n        seed (int): random seed integer.\n\n    Returns:\n        tuple[float, float]: coordinates of randomly generated point.\n    \"\"\"\n    r = 6_371_000  # Radius of earth in meters.\n\n    d_x = Random(seed).uniform(0, d)\n    d_y = sqrt(d**2 - d_x**2)\n\n    # firstly, convert lat to radians for later\n    lat1_radians = lat1 * pi / 180.0\n\n    # how many meters correspond to one degree of latitude?\n    deg_to_meters = r * pi / 180  # aprox. 111111 meters\n    # thus, the northwards displacement, in degrees of latitude is:\n    north_delta = d_y / deg_to_meters\n\n    # but one degree of longitude does not always correspond to the\n    # same distance... depends on the latitude at where you are!\n    parallel_radius = abs(r * cos(lat1_radians))\n    deg_to_meters = parallel_radius * pi / 180  # variable\n    # thus, the eastwards displacement, in degrees of longitude is:\n    east_delta = d_x / deg_to_meters\n\n    final_lon = lon1 + east_delta * Random(seed).choice([-1, 1])\n    final_lat = lat1 + north_delta * Random(seed).choice([-1, 1])\n\n    return (final_lon, final_lat)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_min_stay_duration","title":"<code>generate_min_stay_duration(stay_type)</code>","text":"<p>Generate minimum stay duration based on stay type specifications.</p> <p>Parameters:</p> Name Type Description Default <code>stay_type</code> <code>str</code> <p>type of stay. Shall be \"home\", \"work\" or \"other\".</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>minimum stay duration.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_min_stay_duration(self, stay_type: str) -&gt; float:\n    \"\"\"\n    Generate minimum stay duration based on stay type specifications.\n\n    Args:\n        stay_type (str): type of stay. Shall be \"home\", \"work\" or \"other\".\n\n    Returns:\n        float: minimum stay duration.\n    \"\"\"\n    if stay_type == \"home\":\n        return self.home_duration_min\n    elif stay_type == \"work\":\n        return self.work_duration_min\n    elif stay_type == \"other\":\n        return self.other_duration_min\n    else:\n        raise ValueError\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_other_location","title":"<code>generate_other_location(agent_id, date, activity_number, home_location, previous_location, seed=6)</code>","text":"<p>Generate other activity location based on previous location and maximum distance to previous location. If there is no previous location (this is the first activity of the day), then the home location is considered as previous location. If the location falls outside of bounding box limits, try again.</p> <p>Parameters:</p> Name Type Description Default <code>agent_id</code> <code>int</code> <p>identifier of agent, used for random seed generation.</p> required <code>date</code> <code>date</code> <p>date, used for random seed generation.</p> required <code>activity_number</code> <code>int</code> <p>act position, used for random seed generation.</p> required <code>home_location</code> <code>tuple[float, float]</code> <p>coordinates of home location.</p> required <code>previous_location</code> <code>tuple[float, float]</code> <p>coordinates of previous location.</p> required <code>seed</code> <code>int</code> <p>random seed integer. Defaults to 6.</p> <code>6</code> <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>tuple[float,float]: coordinates of generated location.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_other_location(\n    self,\n    agent_id: int,\n    date: datetime.date,\n    activity_number: int,\n    home_location: tuple[float, float],\n    previous_location: tuple[float, float],\n    seed: int = 6,\n) -&gt; tuple[float, float]:\n    \"\"\"\n    Generate other activity location based on previous location and maximum distance\n    to previous location. If there is no previous location (this is the first\n    activity of the day), then the home location is considered as previous location.\n    If the location falls outside of bounding box limits, try again.\n\n    Args:\n        agent_id (int): identifier of agent, used for random seed generation.\n        date (datetime.date): date, used for random seed generation.\n        activity_number (int): act position, used for random seed generation.\n        home_location (tuple[float,float]): coordinates of home location.\n        previous_location (tuple[float,float]): coordinates of previous location.\n        seed (int, optional): random seed integer. Defaults to 6.\n\n    Returns:\n        tuple[float,float]: coordinates of generated location.\n    \"\"\"\n    seed_distance = self.random_seed_number_generator(seed - 1, agent_id)\n    random_distance = Random(seed_distance).uniform(self.other_distance_min, self.other_distance_max)\n    if previous_location is None:\n        plon, plat = home_location\n    else:\n        plon, plat = previous_location\n\n    seed_coords = self.random_seed_number_generator(seed, agent_id)\n    olon, olat = self.generate_lonlat_at_distance(plon, plat, random_distance, seed_coords)\n    if not (self.longitude_min &lt; olon &lt; self.longitude_max) or not (\n        self.latitude_min &lt; olat &lt; self.latitude_max\n    ):  # outside limits\n        seed += 1\n        olon, olat = self.generate_other_location(\n            agent_id, date, activity_number, home_location, previous_location, seed=seed\n        )\n\n    return (olon, olat)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_stay_duration","title":"<code>generate_stay_duration(agent_id, date, i, stay_type, remaining_time)</code>","text":"<p>Generate stay duration probabilistically based on activity type abd remaining time.</p> <p>Parameters:</p> Name Type Description Default <code>agent_id</code> <code>int</code> <p>identifier of agent, used for random seed generation.</p> required <code>date</code> <code>date</code> <p>date, used for random seed generation.</p> required <code>i</code> <code>int</code> <p>activity position, used for random seed generation.</p> required <code>stay_type</code> <code>str</code> <p>type of stay. Shall be \"home\", \"work\" or \"other\".</p> required <code>remaining_time</code> <code>float</code> <p>same units as durations.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>generated activity duration.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_stay_duration(\n    self, agent_id: int, date: datetime.date, i: int, stay_type: str, remaining_time: float\n) -&gt; float:\n    \"\"\"\n    Generate stay duration probabilistically based on activity type\n    abd remaining time.\n\n    Args:\n        agent_id (int): identifier of agent, used for random seed generation.\n        date (datetime.date): date, used for random seed generation.\n        i (int): activity position, used for random seed generation.\n        stay_type (str): type of stay. Shall be \"home\", \"work\" or \"other\".\n        remaining_time (float): same units as durations.\n\n    Returns:\n        float: generated activity duration.\n    \"\"\"\n    if stay_type == \"home\":\n        min_duration = self.home_duration_min\n        max_duration = self.home_duration_max\n    elif stay_type == \"work\":\n        min_duration = self.work_duration_min\n        max_duration = self.work_duration_max\n    elif stay_type == \"other\":\n        min_duration = self.other_duration_min\n        max_duration = self.other_duration_max\n    else:\n        raise ValueError\n    seed = self.random_seed_number_generator(7, agent_id, date, i)\n    max_value = min(max_duration, min_duration + remaining_time)\n    return Random(seed).uniform(min_duration, max_value)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_stay_location","title":"<code>generate_stay_location(stay_type, home_location, work_location, previous_location, user_id, date, i)</code>","text":"<p>Generate a random activity location within the bounding box limits based on the activity type and previous activity locations.</p> <p>Parameters:</p> Name Type Description Default <code>stay_type</code> <code>str</code> <p>type of stay (\"home\", \"work\" or \"other\").</p> required <code>home_location</code> <code>tuple[float, float]</code> <p>coordinates of home location.</p> required <code>work_location</code> <code>tuple[float, float]</code> <p>coordinates of work location.</p> required <code>previous_location</code> <code>tuple[float, float]</code> <p>coordinates of previous activity location.</p> required <code>user_id</code> <code>int</code> <p>agent identifier, used for random seed generation.</p> required <code>date</code> <code>date</code> <p>date, used for random seed generation.</p> required <code>i</code> <code>int</code> <p>activity position, used for random seed generation.</p> required <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>tuple[float,float]: randomly generated activity location coordinates.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_stay_location(\n    self,\n    stay_type: str,\n    home_location: tuple[float, float],\n    work_location: tuple[float, float],\n    previous_location: tuple[float, float],\n    user_id: int,\n    date: datetime.date,\n    i: int,\n) -&gt; tuple[float, float]:\n    \"\"\"\n    Generate a random activity location within the bounding box limits based\n    on the activity type and previous activity locations.\n\n    Args:\n        stay_type (str): type of stay (\"home\", \"work\" or \"other\").\n        home_location (tuple[float,float]): coordinates of home location.\n        work_location (tuple[float,float]): coordinates of work location.\n        previous_location (tuple[float,float]): coordinates of previous\n            activity location.\n        user_id (int): agent identifier, used for random seed generation.\n        date (datetime.date): date, used for random seed generation.\n        i (int): activity position, used for random seed generation.\n\n    Returns:\n        tuple[float,float]: randomly generated activity location coordinates.\n    \"\"\"\n    if stay_type == \"home\":\n        location = home_location\n    elif stay_type == \"work\":\n        location = work_location\n    else:\n        location = self.generate_other_location(user_id, date, i, home_location, previous_location)\n    return location\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_stay_type_sequence","title":"<code>generate_stay_type_sequence(agent_id, date)</code>","text":"<p>Generate the sequence of stay types for an agent for a specific date probabilistically based on the superset sequence and specified probabilities. Replace 'home'-'home' and 'work'-'work' sequences by just 'home' or 'work'.</p> <p>Parameters:</p> Name Type Description Default <code>agent_id</code> <code>int</code> <p>identifier of agent, used for random seed generation.</p> required <code>date</code> <code>date</code> <p>date for activity sequence generation, used for random seed generation.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: list of generated stay types, each represented by a string indicating the stay type (e.g. \"home\", \"work\", \"other\").</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_stay_type_sequence(self, agent_id: int, date: datetime.date) -&gt; list[str]:\n    \"\"\"\n    Generate the sequence of stay types for an agent for a specific date\n    probabilistically based on the superset sequence and specified\n    probabilities.\n    Replace 'home'-'home' and 'work'-'work' sequences by just 'home' or\n    'work'.\n\n    Args:\n        agent_id (int): identifier of agent, used for random seed generation.\n        date (datetime.date): date for activity sequence generation, used for\n            random seed generation.\n\n    Returns:\n        list[str]: list of generated stay types, each represented by a string\n            indicating the stay type (e.g. \"home\", \"work\", \"other\").\n    \"\"\"\n    stay_type_sequence = []\n    for i, stay_type in enumerate(self.stay_sequence_superset):\n        stay_weight = self.stay_sequence_probabilities[i]\n        seed = self.random_seed_number_generator(0, agent_id, date, i)\n        if Random(seed).random() &lt; stay_weight:\n            stay_type_sequence.append(stay_type)\n    stay_type_sequence = self.remove_consecutive_stay_types(stay_type_sequence, {\"home\", \"work\"})\n    return stay_type_sequence\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_work_location","title":"<code>generate_work_location(agent_id, home_location, seed=4)</code>","text":"<p>Generate random work location based on home location and maximum distance to home. If the work location falls outside of bounding box limits, try again.</p> <p>Parameters:</p> Name Type Description Default <code>agent_id</code> <code>int</code> <p>identifier of agent, used for random seed generation.</p> required <code>home_location</code> <code>tuple[float, float]</code> <p>coordinates of home location.</p> required <code>seed</code> <code>int</code> <p>random seed integer. Defaults to 4.</p> <code>4</code> <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>tuple[float,float]: coordinates of generated work location.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_work_location(\n    self, agent_id: int, home_location: tuple[float, float], seed: int = 4\n) -&gt; tuple[float, float]:\n    \"\"\"\n    Generate random work location based on home location and maximum distance to\n    home. If the work location falls outside of bounding box limits, try again.\n\n    Args:\n        agent_id (int): identifier of agent, used for random seed generation.\n        home_location (tuple[float,float]): coordinates of home location.\n        seed (int, optional): random seed integer. Defaults to 4.\n\n    Returns:\n        tuple[float,float]: coordinates of generated work location.\n    \"\"\"\n    seed_distance = self.random_seed_number_generator(seed - 1, agent_id)\n    random_distance = Random(seed_distance).uniform(self.home_work_distance_min, self.home_work_distance_max)\n    hlon, hlat = home_location\n    seed_coords = self.random_seed_number_generator(seed, agent_id)\n    wlon, wlat = self.generate_lonlat_at_distance(hlon, hlat, random_distance, seed_coords)\n\n    if not (self.longitude_min &lt; wlon &lt; self.longitude_max) or not (\n        self.latitude_min &lt; wlat &lt; self.latitude_max\n    ):  # outside limits\n        seed += 1\n        wlon, wlat = self.generate_work_location(agent_id, home_location, seed=seed)\n\n    return (wlon, wlat)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.haversine","title":"<code>haversine(lon1, lat1, lon2, lat2)</code>","text":"<p>Calculate the haversine distance in meters between two points.</p> <p>Parameters:</p> Name Type Description Default <code>lon1</code> <code>float</code> <p>longitude of first point, in decimal degrees.</p> required <code>lat1</code> <code>float</code> <p>latitude of first point, in decimal degrees.</p> required <code>lon2</code> <code>float</code> <p>longitude of second point, in decimal degrees.</p> required <code>lat2</code> <code>float</code> <p>latitude of second point, in decimal degrees.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>distance between both points, in meters.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def haversine(self, lon1: float, lat1: float, lon2: float, lat2: float) -&gt; float:\n    \"\"\"\n    Calculate the haversine distance in meters between two points.\n\n    Args:\n        lon1 (float): longitude of first point, in decimal degrees.\n        lat1 (float): latitude of first point, in decimal degrees.\n        lon2 (float): longitude of second point, in decimal degrees.\n        lat2 (float): latitude of second point, in decimal degrees.\n\n    Returns:\n        float: distance between both points, in meters.\n    \"\"\"\n    r = 6_371_000  # Radius of earth in meters.\n\n    # convert decimal degrees to radians\n    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n\n    # haversine formula\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n    a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n    c = 2 * asin(sqrt(a))\n    return c * r\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.random_seed_number_generator","title":"<code>random_seed_number_generator(base_seed, agent_id=None, date=None, i=None)</code>","text":"<p>Generate random seed integer based on provided arguments.</p> <p>Parameters:</p> Name Type Description Default <code>base_seed</code> <code>int</code> <p>base integer for operations.</p> required <code>agent_id</code> <code>int</code> <p>agent identifier. Defaults to None.</p> <code>None</code> <code>date</code> <code>date</code> <p>date. Defaults to None.</p> <code>None</code> <code>i</code> <code>int</code> <p>position integer. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>generated random seed integer.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def random_seed_number_generator(\n    self, base_seed: int, agent_id: int = None, date: datetime.date = None, i: int = None\n) -&gt; int:\n    \"\"\"\n    Generate random seed integer based on provided arguments.\n\n    Args:\n        base_seed (int): base integer for operations.\n        agent_id (int, optional): agent identifier. Defaults to None.\n        date (datetime.date, optional): date. Defaults to None.\n        i (int, optional): position integer. Defaults to None.\n\n    Returns:\n        int: generated random seed integer.\n    \"\"\"\n    seed = base_seed\n    if agent_id is not None:\n        seed += int(agent_id) * 100\n    if date is not None:\n        start_datetime = datetime.datetime.combine(date, datetime.time(0))\n        seed += int(start_datetime.timestamp())\n    if i is not None:\n        seed += i\n    return seed\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.remove_consecutive_stay_types","title":"<code>remove_consecutive_stay_types(stay_sequence_list, stay_types_to_group)</code>","text":"<p>Generate new list replacing consecutive stays of the same type by a unique stay as long as the stay type is contained in the \"stay_types_to_group\" list.</p> <p>Parameters:</p> Name Type Description Default <code>stay_sequence_list</code> <code>list[str]</code> <p>input stay type list.</p> required <code>stay_types_to_group</code> <code>set[str]</code> <p>stay types to group.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: output stay sequence list.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def remove_consecutive_stay_types(self, stay_sequence_list: list[str], stay_types_to_group: set[str]) -&gt; list[str]:\n    \"\"\"\n    Generate new list replacing consecutive stays of the same type by\n    a unique stay as long as the stay type is contained in the\n    \"stay_types_to_group\" list.\n\n    Args:\n        stay_sequence_list (list[str]): input stay type list.\n        stay_types_to_group (set[str]): stay types to group.\n\n    Returns:\n        list[str]: output stay sequence list.\n    \"\"\"\n    new_stay_sequence_list = []\n    previous_stay = None\n    for stay in stay_sequence_list:\n        if stay == previous_stay and stay in stay_types_to_group:\n            pass\n        else:\n            new_stay_sequence_list.append(stay)\n        previous_stay = stay\n    return new_stay_sequence_list\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.update_spark_row","title":"<code>update_spark_row(row, column_name, new_value)</code>","text":"<p>Return an updated spark row object, changing the value of a column.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Row</code> <p>input spark row.</p> required <code>column_name</code> <code>str</code> <p>name of column to modify.</p> required <code>new_value</code> <code>Any</code> <p>new value to assign.</p> required <p>Returns:</p> Name Type Description <code>Row</code> <code>Row</code> <p>modified spark row</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def update_spark_row(self, row: Row, column_name: str, new_value: Any) -&gt; Row:\n    \"\"\"\n    Return an updated spark row object, changing the value of a column.\n\n    Args:\n        row (Row): input spark row.\n        column_name (str): name of column to modify.\n        new_value (Any): new value to assign.\n\n    Returns:\n        Row: modified spark row\n    \"\"\"\n    return Row(**{**row.asDict(), **{column_name: new_value}})\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/","title":"synthetic_events","text":"<p>This module contains the SyntheticEvents class, which is responsible for generating the synthetic event data.</p>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents","title":"<code>SyntheticEvents</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class that generates the event synthetic data. It inherits from the Component abstract class.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>class SyntheticEvents(Component):\n    \"\"\"\n    Class that generates the event synthetic data. It inherits from the Component abstract class.\n    \"\"\"\n\n    COMPONENT_ID = \"SyntheticEvents\"\n\n    def __init__(self, general_config_path: str, component_config_path: str):\n        super().__init__(\n            general_config_path=general_config_path,\n            component_config_path=component_config_path,\n        )\n\n        self.seed = self.config.getint(self.COMPONENT_ID, \"seed\")\n        self.event_freq_stays = self.config.getint(self.COMPONENT_ID, \"event_freq_stays\")\n        self.event_freq_moves = self.config.getint(self.COMPONENT_ID, \"event_freq_moves\")\n        self.closest_cell_distance_max = self.config.getint(self.COMPONENT_ID, \"closest_cell_distance_max\")\n        self.closest_cell_distance_max_for_errors = self.config.getint(\n            self.COMPONENT_ID, \"closest_cell_distance_max_for_errors\"\n        )\n        self.error_location_probability = self.config.getfloat(self.COMPONENT_ID, \"error_location_probability\")\n        self.error_location_distance_min = self.config.getint(self.COMPONENT_ID, \"error_location_distance_min\")\n        self.error_location_distance_max = self.config.getint(self.COMPONENT_ID, \"error_location_distance_max\")\n        self.cartesian_crs = self.config.getint(self.COMPONENT_ID, \"cartesian_crs\")\n        self.error_cell_id_probability = self.config.getfloat(self.COMPONENT_ID, \"error_cell_id_probability\")\n        self.maximum_number_of_cells_for_event = self.config.getfloat(\n            self.COMPONENT_ID, \"maximum_number_of_cells_for_event\"\n        )\n\n        self.mcc = self.config.getint(self.COMPONENT_ID, \"mcc\")\n        self.mnc = self.config.get(self.COMPONENT_ID, \"mnc\")\n\n        # Parameters for synthetic event errors generation (these are not locational errors)\n\n        self.do_event_error_generation = self.config.getboolean(self.COMPONENT_ID, \"do_event_error_generation\")\n        self.column_is_null_probability = self.config.getfloat(self.COMPONENT_ID, \"column_is_null_probability\")\n        self.null_row_prob = self.config.getfloat(self.COMPONENT_ID, \"null_row_probability\")\n        self.data_type_error_prob = self.config.getfloat(self.COMPONENT_ID, \"data_type_error_probability\")\n        self.out_of_bounds_prob = self.config.getfloat(self.COMPONENT_ID, \"out_of_bounds_probability\")\n        self.same_location_duplicate_prob = self.config.getfloat(\n            self.COMPONENT_ID, \"same_location_duplicates_probability\"\n        )\n        self.different_location_duplicate_prob = self.config.getfloat(\n            self.COMPONENT_ID, \"different_location_duplicates_probability\"\n        )\n\n        self.mandatory_columns = [i.name for i in BronzeEventDataObject.SCHEMA]\n        self.error_generation_allowed_columns = set(self.mandatory_columns) - set(\n            [ColNames.year, ColNames.month, ColNames.day]\n        )\n\n        self.order_output_by_timestamp = self.config.getboolean(self.COMPONENT_ID, \"order_output_by_timestamp\")\n\n    def initalize_data_objects(self):\n\n        pop_diares_input_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"diaries_data_bronze\")\n        pop_diaries_bronze_event = BronzeSyntheticDiariesDataObject(\n            self.spark,\n            pop_diares_input_path,\n            partition_columns=[ColNames.year, ColNames.month, ColNames.day],\n        )\n\n        # Input for cell attributes\n        network_data_input_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"network_data_bronze\")\n\n        cell_locations_bronze = BronzeNetworkDataObject(\n            self.spark,\n            network_data_input_path,\n            partition_columns=[ColNames.year, ColNames.month, ColNames.day],\n        )\n\n        self.input_data_objects = {\n            BronzeSyntheticDiariesDataObject.ID: pop_diaries_bronze_event,\n            BronzeNetworkDataObject.ID: cell_locations_bronze,\n        }\n\n        output_records_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"event_data_bronze\")\n        bronze_event = BronzeEventDataObject(\n            self.spark,\n            output_records_path,\n            partition_columns=[ColNames.year, ColNames.month, ColNames.day],\n        )\n\n        self.output_data_objects = {BronzeEventDataObject.ID: bronze_event}\n\n    def transform(self):\n\n        pop_diaries_df = self.input_data_objects[BronzeSyntheticDiariesDataObject.ID].df\n        cells_df = self.input_data_objects[BronzeNetworkDataObject.ID].df.select(\n            ColNames.cell_id,\n            ColNames.latitude,\n            ColNames.longitude,\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n        )\n\n        # Filtering stays to get the lat and lon of movement starting and end point\n        stays_df = pop_diaries_df.filter(F.col(ColNames.activity_type) == \"stay\")\n\n        move_events_df = self.generate_event_timestamps_for_moves(\n            stays_df, self.event_freq_moves, self.cartesian_crs, self.seed\n        )\n        move_events_with_locations_df = self.generate_locations_for_moves(move_events_df, self.cartesian_crs)\n\n        stay_events_df = self.generate_event_timestamps_for_stays(\n            stays_df, self.event_freq_stays, self.cartesian_crs, self.seed\n        )\n\n        generated_stays_and_moves = stay_events_df.union(move_events_with_locations_df)\n\n        # Add geometry column to cells\n        cells_df = cells_df.withColumn(\n            \"cell_geometry\",\n            STF.ST_Transform(\n                STC.ST_Point(F.col(ColNames.longitude), F.col(ColNames.latitude)),\n                F.lit(\"EPSG:4326\"),\n                F.lit(f\"EPSG:{self.cartesian_crs}\"),\n            ),\n        ).select(ColNames.cell_id, \"cell_geometry\")\n\n        # 1) From the clean records, sample records for location errors\n        sampled_records = generated_stays_and_moves.sample(self.error_location_probability, self.seed)\n        generated_stays_and_moves = generated_stays_and_moves.subtract(sampled_records)\n        generated_stays_and_moves = generated_stays_and_moves.withColumn(\n            ColNames.loc_error, F.lit(None).cast(FloatType())\n        )\n\n        records_with_location_errors = self.generate_location_errors(\n            sampled_records,\n            self.error_location_distance_max,\n            self.error_location_distance_min,\n            self.closest_cell_distance_max_for_errors,\n            self.cartesian_crs,\n            self.seed,\n        )\n\n        # 2) From the clean records, sample records for erroneous cell id creation\n        sampled_records = generated_stays_and_moves.sample(self.error_cell_id_probability, self.seed)\n        records_with_cell_id_errors = self.generate_records_with_non_existant_cell_ids(\n            sampled_records, cells_df, self.seed\n        )\n\n        generated_stays_and_moves = generated_stays_and_moves.subtract(sampled_records)\n        generated_stays_and_moves = generated_stays_and_moves.withColumn(\n            \"closest_cell_distance_max\", F.lit(self.closest_cell_distance_max)\n        )\n\n        # Label error rows so that these would be ignored in syntactic error generation\n        records_with_location_errors = records_with_location_errors.withColumn(\"is_modified\", F.lit(True))\n        records_with_cell_id_errors = records_with_cell_id_errors.withColumn(\"is_modified\", F.lit(True))\n        generated_stays_and_moves = generated_stays_and_moves.withColumn(\"is_modified\", F.lit(False))\n\n        # 3) Link a cell id to each location\n\n        records_sdf = generated_stays_and_moves.union(records_with_location_errors)\n\n        records_sdf = self.add_cell_ids_to_locations(\n            records_sdf,\n            cells_df,\n            self.maximum_number_of_cells_for_event,\n            self.seed,\n        )\n\n        # 4) Continuing with the combined dataframe\n\n        records_sdf = records_sdf.union(records_with_cell_id_errors)\n\n        records_sdf = records_sdf.dropDuplicates([ColNames.user_id, ColNames.timestamp])\n\n        records_sdf = records_sdf.withColumn(\n            \"generated_geometry\",\n            STF.ST_Transform(\n                \"generated_geometry\",\n                F.lit(f\"EPSG:{self.cartesian_crs}\"),\n                F.lit(\"EPSG:4326\"),\n            ),\n        )\n\n        records_sdf = (\n            records_sdf.withColumn(ColNames.longitude, STF.ST_X(F.col(\"generated_geometry\")))\n            .withColumn(ColNames.latitude, STF.ST_Y(F.col(\"generated_geometry\")))\n            .drop(\"generated_geometry\")\n        )\n\n        # TODO: add rows with PLMN\n        # MCC and loc_error are added to the records\n        records_sdf = records_sdf.withColumn(ColNames.mcc, F.lit(self.mcc).cast(IntegerType()))\n\n        records_sdf = records_sdf.withColumn(ColNames.mnc, F.lit(self.mnc))\n\n        records_sdf = records_sdf.withColumn(ColNames.plmn, F.lit(None).cast(IntegerType()))\n\n        records_sdf = records_sdf.withColumn(ColNames.year, F.year(F.col(ColNames.timestamp)))\n        records_sdf = records_sdf.withColumn(ColNames.month, F.month(F.col(ColNames.timestamp)))\n        records_sdf = records_sdf.withColumn(ColNames.day, F.dayofmonth(F.col(ColNames.timestamp)))\n\n        # Generate errors\n\n        if self.do_event_error_generation:\n            records_sdf = self.generate_errors(synth_df_raw=records_sdf)\n\n        # Select bronze schema columns\n        columns = {field.name: F.col(field.name).cast(field.dataType) for field in BronzeEventDataObject.SCHEMA.fields}\n        records_sdf = records_sdf.withColumns(columns)\n\n        if self.order_output_by_timestamp:\n            records_sdf = records_sdf.orderBy(ColNames.timestamp)\n\n        self.output_data_objects[BronzeEventDataObject.ID].df = records_sdf\n\n    @staticmethod\n    def generate_event_timestamps_for_moves(\n        stays_sdf: DataFrame, event_freq_moves: int, cartesian_crs: int, seed: int\n    ) -&gt; DataFrame:\n        \"\"\"\n        Generates a DataFrame of event timestamps for movements between stays.\n\n        For each stay in the input DataFrame, this method generates a random number of timestamps equal to the time\n            difference between the end of the current stay and the start of the next stay, divided by the event\n            frequency for moves.\n\n        Args:\n            stays_sdf (pyspark.sql.DataFrame): A DataFrame of stays.\n            event_freq_moves (int): The frequency of events for movements.\n            cartesian_crs (int): The EPSG code of the Cartesian coordinate reference system to use for the geometries.\n            seed (int): The seed that determines the randomness of timestamp generation, and subsequent lat/lon generation.\n\n        Returns:\n            pyspark.sql.DataFrame: A DataFrame of event timestamps for movements.\n        \"\"\"\n\n        # Since the rows with activity_type = movement don't have any locations in the population diaries,\n        # we select the stay points and start generating timestamps in between the start and end of the stay\n\n        stays_sdf = stays_sdf.withColumn(\n            ColNames.geometry,\n            STF.ST_Transform(\n                STC.ST_Point(F.col(ColNames.longitude), F.col(ColNames.latitude)),\n                F.lit(\"EPSG:4326\"),\n                F.lit(f\"EPSG:{cartesian_crs}\"),\n            ),\n        )\n\n        # Define the window specification\u00a0\u2022\n        window_spec = Window.partitionBy(ColNames.year, ColNames.month, ColNames.day, ColNames.user_id).orderBy(\n            ColNames.initial_timestamp\n        )\n\n        # Add columns for next stay's geometry and start timestamp using the lead function\n        stays_sdf = stays_sdf.withColumn(\n            \"next_stay_initial_timestamp\",\n            F.lead(ColNames.initial_timestamp, 1).over(window_spec),\n        )\n\n        stays_sdf = stays_sdf.withColumn(\"next_stay_geometry\", F.lead(ColNames.geometry, 1).over(window_spec))\n\n        stays_sdf = stays_sdf.withColumn(\n            \"time_diff_seconds\",\n            F.unix_timestamp(\"next_stay_initial_timestamp\") - F.unix_timestamp(ColNames.final_timestamp),\n        )\n\n        # Calculate how many timestamps fit in the interval for the given frequency\n        stays_sdf = stays_sdf.withColumn(\n            \"timestamps_count\",\n            (F.col(\"time_diff_seconds\") / event_freq_moves).cast(\"integer\"),\n        )\n\n        expr_str = f\"transform(sequence(1, timestamps_count), x -&gt; rand({seed}))\"\n\n        stays_sdf = stays_sdf.withColumn(\n            \"random_fraction_on_line\",\n            F.expr(expr_str),\n        ).withColumn(\"random_fraction_on_line\", F.explode(F.col(\"random_fraction_on_line\")))\n\n        # Generate timestamps\n        stays_sdf = stays_sdf.withColumn(\n            \"offset_seconds\",\n            F.col(\"random_fraction_on_line\") * F.col(\"time_diff_seconds\"),\n        )\n        stays_sdf = stays_sdf.withColumn(\n            ColNames.timestamp,\n            F.from_unixtime(F.unix_timestamp(ColNames.final_timestamp) + F.col(\"offset_seconds\")),\n        )\n\n        moves_sdf = stays_sdf.withColumn(ColNames.activity_type, F.lit(\"move\"))\n\n        # Keep only necessary columns\n        moves_sdf = moves_sdf.select(\n            ColNames.user_id,\n            ColNames.timestamp,\n            ColNames.geometry,\n            \"next_stay_geometry\",\n            \"random_fraction_on_line\",\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n        )\n\n        return moves_sdf\n\n    @staticmethod\n    def generate_event_timestamps_for_stays(\n        stays_df: DataFrame, event_freq_stays: int, cartesian_crs: int, seed: int\n    ) -&gt; DataFrame:\n        \"\"\"\n        Generates a DataFrame of event timestamps for stays based on the event frequency for stays.\n\n        For each stay in the input DataFrame, this method calculates\n        the time difference between the initial and final timestamps of the stay.\n        It then generates a number of timestamps equal to this time difference divided by the event\n        frequency for stays. Each timestamp is associated with the location of the stay.\n\n        Args:\n            stays_df (pyspark.sql.DataFrame): A DataFrame of stays.\n            event_freq_stays (int): The frequency of events for stays.\n            cartesian_crs (int): The EPSG code of the Cartesian coordinate reference system to use for the geometries.\n            seed (int): The seed for generating timestamps randomly.\n        Returns:\n            pyspark.sql.DataFrame: A DataFrame of event timestamps for stays.\n        \"\"\"\n\n        stays_df = stays_df.withColumn(\n            \"time_diff_seconds\",\n            F.unix_timestamp(F.col(ColNames.final_timestamp)) - F.unix_timestamp(F.col(ColNames.initial_timestamp)),\n        )\n\n        stays_df = stays_df.withColumn(\n            \"timestamps_count\",\n            (F.col(\"time_diff_seconds\") / event_freq_stays).cast(\"integer\"),\n        )\n\n        expr_str = f\"transform(sequence(1, timestamps_count), x -&gt; rand({seed}))\"\n        stays_df = stays_df.withColumn(\n            \"random_fraction_between_timestamps\",\n            F.expr(expr_str),\n        )\n\n        stays_df = stays_df.withColumn(\n            \"random_fraction_between_timestamps\",\n            F.explode(F.col(\"random_fraction_between_timestamps\")),\n        )\n        stays_df = stays_df.withColumn(\n            \"offset_seconds\",\n            F.col(\"random_fraction_between_timestamps\") * F.col(\"time_diff_seconds\"),\n        )\n        stays_df = stays_df.withColumn(\n            ColNames.timestamp,\n            F.from_unixtime(F.unix_timestamp(ColNames.initial_timestamp) + F.col(\"offset_seconds\")),\n        )\n\n        stays_df = stays_df.withColumn(\n            \"generated_geometry\",\n            STF.ST_Transform(\n                STC.ST_Point(F.col(ColNames.longitude), F.col(ColNames.latitude)),\n                F.lit(\"EPSG:4326\"),\n                F.lit(f\"EPSG:{cartesian_crs}\"),\n            ),\n        )\n\n        stays_df = stays_df.select(\n            ColNames.user_id,\n            ColNames.timestamp,\n            \"generated_geometry\",\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n        )\n\n        return stays_df\n\n    @staticmethod\n    def generate_locations_for_moves(event_timestamps_df: DataFrame, cartesian_crs: int) -&gt; DataFrame:\n        \"\"\"\n        Generates locations for moves based on the event timestamps dataframe.\n        Returns a dataframe, where for each move in the event timestamps dataframe\n        a geometry column is added, representing the location of the move.\n\n        Performs interpolation along the line between the starting move point (previous stay point)\n        and next move point (next stay point) using the randomly generated values in the column random_fraction_on_line.\n\n        Args:\n            event_timestamps_df (pyspark.sql.DataFrame): The event timestamps dataframe.\n\n        Returns:\n            pyspark.sql.DataFrame: The event timestamps dataframe with the added geometry column.\n        \"\"\"\n\n        moves_with_geometry = event_timestamps_df.withColumn(\n            \"line\",\n            STF.ST_MakeLine(F.col(ColNames.geometry), F.col(\"next_stay_geometry\")),\n        )\n        moves_with_geometry = moves_with_geometry.withColumn(\n            \"generated_geometry\",\n            STF.ST_SetSRID(\n                STF.ST_LineInterpolatePoint(F.col(\"line\"), F.col(\"random_fraction_on_line\")),\n                cartesian_crs,\n            ),\n        )\n\n        moves_with_geometry = moves_with_geometry.select(\n            ColNames.user_id,\n            ColNames.timestamp,\n            \"generated_geometry\",\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n        )\n\n        return moves_with_geometry\n\n    @staticmethod\n    def add_cell_ids_to_locations(\n        events_with_locations_df: DataFrame,\n        cells_df: DataFrame,\n        max_n_of_cells: int,\n        seed: int,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Links cell IDs to locations in the events DataFrame.\n\n        This method performs a spatial join between the events and cells DataFrames to add cell IDs to each event.\n        It first creates a buffer around each event location and finds cells that intersect with this buffer.\n        It then calculates the distance from each event location to the cell and ranks the cells based on this distance.\n        It keeps only the top 'max_n_of_cells' closest cells for each event.\n\n        The method also adds a random index to each event-cell pair and filters to keep only one pair per event,\n        randomly selecting one of the closest cells for each event.\n\n        Args:\n            events_with_locations_df (pyspark.sql.DataFrame): A DataFrame of events\n            cells_df (pyspark.sql.DataFrame): A DataFrame of cells\n            max_n_of_cells (int): The maximum number of closest cells to consider for each event.\n\n        Returns:\n            pyspark.sql.DataFrame: A DataFrame of events with cell IDs added.\n        \"\"\"\n        events_with_cells_sdf = events_with_locations_df.join(\n            cells_df[[ColNames.cell_id, \"cell_geometry\"]],\n            (\n                STP.ST_Intersects(\n                    STF.ST_Buffer(\n                        events_with_locations_df[\"generated_geometry\"],\n                        F.col(\"closest_cell_distance_max\"),\n                    ),\n                    cells_df[\"cell_geometry\"],\n                )\n            ),\n        ).withColumn(\n            \"distance_to_cell\",\n            STF.ST_Distance(\n                events_with_locations_df[\"generated_geometry\"],\n                cells_df[\"cell_geometry\"],\n            ),\n        )\n\n        # Selection of different cell ids for a given timestamp is random\n        window_spec = Window.partitionBy(\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            ColNames.user_id,\n            ColNames.timestamp,\n        ).orderBy(F.col(\"distance_to_cell\"))\n\n        events_with_cells_sdf = events_with_cells_sdf.withColumn(\n            \"closest_cells_index\", F.row_number().over(window_spec)\n        ).filter(F.col(\"closest_cells_index\") &lt;= max_n_of_cells)\n\n        window_spec_random = Window.partitionBy(\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            ColNames.user_id,\n            ColNames.timestamp,\n        ).orderBy(F.rand(seed=seed))\n        events_with_cells_sdf = events_with_cells_sdf.withColumn(\n            \"random_cell_index\", F.row_number().over(window_spec_random)\n        ).filter(F.col(\"random_cell_index\") == 1)\n\n        records_sdf = events_with_cells_sdf.select(\n            ColNames.user_id,\n            ColNames.timestamp,\n            \"generated_geometry\",\n            ColNames.cell_id,\n            ColNames.loc_error,\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            \"is_modified\",\n        )\n\n        return records_sdf\n\n    @staticmethod\n    def generate_location_errors(\n        records_sdf: DataFrame,\n        error_location_distance_max: float,\n        error_location_distance_min: float,\n        closest_cell_distance_max: float,\n        cartesian_crs: int,\n        seed: int,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Generates location errors for x and y coordinates of each record in the DataFrame.\n\n        This method adds a random location error to the x and y coordinates of each record in the input DataFrame.\n        The location error is a random value between error_location_distance_min and error_location_distance_max,\n        and is added or subtracted from the x and y coordinates based on a random sign.\n\n        Args:\n            records_sdf (DataFrame): A DataFrame of records\n            error_location_distance_max (float): The maximum location error distance.\n            error_location_distance_min (float): The minimum location error distance.\n            closest_cell_distance_max (float): The maximum distance to the closest cell.\n            cartesian_crs (int): The EPSG code of the Cartesian coordinate reference system to use for the geometries.\n            seed (int): The seed for the random number generator.\n\n        Returns:\n            DataFrame: A DataFrame of records with location errors added to the x and y coordinates.\n        \"\"\"\n\n        errors_df = (\n            records_sdf.withColumn(\"y\", STF.ST_Y(F.col(\"generated_geometry\")))\n            .withColumn(\"x\", STF.ST_X(F.col(\"generated_geometry\")))\n            .drop(\"generated_geometry\")\n        )\n\n        errors_df = errors_df.withColumn(\n            ColNames.loc_error,\n            (F.rand(seed=seed) * (error_location_distance_max - error_location_distance_min))\n            + error_location_distance_min,\n        )\n\n        errors_df = (\n            errors_df.withColumn(\"sign\", F.when(F.rand(seed=seed) &gt; 0.5, 1).otherwise(-1))\n            .withColumn(\"new_x\", F.col(\"x\") + (F.col(ColNames.loc_error) * F.col(\"sign\")))\n            .withColumn(\"new_y\", F.col(\"y\") + (F.col(ColNames.loc_error) * F.col(\"sign\")))\n        )\n\n        errors_df = errors_df.withColumn(\n            \"generated_geometry\",\n            STF.ST_SetSRID(STC.ST_Point(F.col(\"new_x\"), F.col(\"new_y\")), cartesian_crs),\n        ).drop(\"new_x\", \"new_y\")\n\n        errors_df = errors_df.withColumn(\"closest_cell_distance_max\", F.lit(closest_cell_distance_max))\n\n        errors_df = errors_df.select(\n            ColNames.user_id,\n            ColNames.timestamp,\n            \"generated_geometry\",\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            ColNames.loc_error,\n            \"closest_cell_distance_max\",\n        )\n\n        return errors_df\n\n    @staticmethod\n    def generate_records_with_non_existant_cell_ids(records_sdf: DataFrame, cells_sdf: DataFrame, seed) -&gt; DataFrame:\n        \"\"\"\n        Adds the cell_id column so that it will contain cell_ids that\n        are not present in the cells_df dataframe, yet follow the format of a cell id.\n\n        Args:\n            records_sdf (DataFrame): generated records\n            cells_sdf (DataFrame): cells dataframe\n\n            DataFrame: records with cell ids that are not present in the cells_df dataframe\n        \"\"\"\n\n        # Generates random cell ids for cells_df, and selects those\n        # Join to records is implemented with a monotonically increasing id\n        # So to limit that, this number of all unique cells is used\n        # TODO check how to make this more optimal\n\n        n_of_actual_cell_ids_as_base = cells_sdf.count()\n\n        cells_df_with_random_cell_ids = cells_sdf[[ColNames.cell_id]].withColumn(\n            \"random_cell_id\",\n            (F.rand(seed=seed) * (999_999_999_999_999 - 10_000_000_000_000) + 10_000_000_000_000).cast(\"bigint\"),\n        )\n\n        # Do an  left anti join to ensure that now generated cell ids are not among actual cell ids\n\n        cells_df_inner_joined = cells_df_with_random_cell_ids[[\"random_cell_id\"]].join(\n            cells_sdf[[ColNames.cell_id]],\n            on=F.col(\"random_cell_id\") == F.col(ColNames.cell_id),\n            how=\"leftanti\",\n        )\n\n        cells_df_inner_joined = cells_df_inner_joined.withColumn(\"row_number\", (F.monotonically_increasing_id()))\n        records_sdf = records_sdf.withColumn(\n            \"row_number\",\n            (F.monotonically_increasing_id() % n_of_actual_cell_ids_as_base) + 1,\n        )\n\n        cells_df_inner_joined = cells_df_inner_joined.select(\n            \"row_number\", F.col(\"random_cell_id\").alias(ColNames.cell_id)\n        )\n\n        records_with_random_cell_id = records_sdf.join(cells_df_inner_joined, on=\"row_number\", how=\"left\").drop(\n            \"row_number\"\n        )\n\n        records_with_random_cell_id = records_with_random_cell_id.select(\n            ColNames.user_id,\n            ColNames.timestamp,\n            \"generated_geometry\",\n            ColNames.cell_id,\n            ColNames.loc_error,\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n        )\n\n        return records_with_random_cell_id\n\n    # @staticmethod\n    def generate_nulls_in_mandatory_fields(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Generates null values in some fields of some rows based on configuration parameters.\n\n        Args:\n            df (pyspark.sql.DataFrame): clean synthetic data\n\n        Returns:\n            pyspark.sql.DataFrame: synthetic records dataframe with nulls in some columns of some rows\n        \"\"\"\n\n        # Two probability parameters from config apply:\n        # First one sets how many rows (as fraction of all rows) are selected for possible value nulling.\n        # Second one sets the likelyhood for each column to be set to null.\n        # If 1.0, then all mandatory columns of each selected row will be nulled.\n\n        if self.null_row_prob == 0.0:\n            # TODO logging\n            return df\n\n        # Split input dataframe to unchanged and changed portions\n        df = df.cache()\n        error_row_prob = self.null_row_prob\n        unchanged_row_prob = 1.0 - error_row_prob\n        unchanged_rows_df, error_rows_df = df.randomSplit([unchanged_row_prob, error_row_prob], seed=self.seed + 1)\n\n        columns_for_null_selection = list(self.error_generation_allowed_columns)\n        columns_for_null_selection.sort()\n\n        random.seed(self.seed)\n        columns_to_set_as_null = random.sample(\n            columns_for_null_selection,\n            int(round(self.column_is_null_probability * len(columns_for_null_selection), 0)),\n        )\n\n        for column in columns_to_set_as_null:\n            error_rows_df = error_rows_df.withColumn(column, F.lit(None))\n\n        error_rows_df = error_rows_df.withColumn(\"is_modified\", F.lit(True))\n\n        # Re-combine unchanged and changed rows of the dataframe.\n        return unchanged_rows_df.union(error_rows_df)\n\n    # @staticmethod\n    def generate_out_of_bounds_dates(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Transforms the timestamp column values to be out of bound of the selected period,\n        based on probabilities from configuration.\n        Only rows with non-null timestamp values can become altered here.\n\n        Args:\n            df (pyspark.sql.DataFrame): Dataframe of clean synthetic events.\n\n        Returns:\n            pyspark.sql.DataFrame: Data where some timestamp column values are out of bounds as per config.\n        \"\"\"\n\n        if self.out_of_bounds_prob == 0.0:\n            # TODO logging\n            return df\n\n        # Calculate approximate span in months from config parameters.\n        # The parameters should approximately cover the range of \"clean\" dates, so that the erroneous values can be generated outside the range.\n\n        # TODO\n        # This now uses the whole input data to set the bounds\n        ending_timestamp = df.agg(F.max(ColNames.timestamp)).collect()[0][0]\n        starting_timestamp = df.agg(F.min(ColNames.timestamp)).collect()[0][0]\n        events_span_in_months = max(1, (pd.Timestamp(ending_timestamp) - pd.Timestamp(starting_timestamp)).days / 30)\n\n        # Split rows by null/non-null timestamp.\n        df = df.cache()\n        null_timestamp_df = df.where(F.col(ColNames.timestamp).isNull())\n        nonnull_timestamp_df = df.where(F.col(ColNames.timestamp).isNotNull())\n        df.unpersist()\n\n        # From non-null timestamp rows, select a subset for adding errors to, depending on config parameter.\n        nonnull_timestamp_df = nonnull_timestamp_df.cache()\n        error_row_prob = self.out_of_bounds_prob\n        unchanged_row_prob = 1.0 - error_row_prob\n        unchanged_rows_df, error_rows_df = nonnull_timestamp_df.randomSplit(\n            [unchanged_row_prob, error_row_prob], seed=self.seed + 3\n        )\n        # Combine null timestamp rows and not-modified non-null timestamp rows.\n        unchanged_rows_df = unchanged_rows_df.union(null_timestamp_df)\n\n        # Add months offset to error rows to make their timestamp values become outside expected range.\n        months_to_add_col = (F.lit(2) + F.rand(seed=self.seed)) * F.lit(events_span_in_months)\n        modified_date_col = F.add_months(F.col(ColNames.timestamp), months_to_add_col)\n        time_col = F.date_format(F.col(ColNames.timestamp), \" HH:mm:ss\")\n        error_rows_df = error_rows_df.withColumn(ColNames.timestamp, F.concat(modified_date_col, time_col))\n        error_rows_df = error_rows_df.withColumn(\"is_modified\", F.lit(True))\n\n        # Combine changed and unchanged rows dataframes.\n        return unchanged_rows_df.union(error_rows_df)\n\n    # @staticmethod\n    def generate_erroneous_type_values(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Generates errors for sampled rows. Errors are custom defined, for instance a random string, or corrupt timestamp.\n        Does not cast the columns to a different type.\n\n        Args:\n            df (pyspark.sql.DataFrame): dataframe that may have out of bound and null records.\n\n        Returns:\n            pyspark.sql.DataFrame: dataframe with erroneous rows, and possibly, with nulls and out of bound records.\n        \"\"\"\n\n        if self.data_type_error_prob == 0:\n            # TODO logging\n            return df\n\n        # Split dataframe by whether the row has been modified during the error-adding process already.\n        # Already errored rows do not get further changes.\n        df = df.cache()\n        previously_modified_rows_df = df.where(F.col(\"is_modified\"))\n        unmodified_rows_df = df.where(~(F.col(\"is_modified\")))\n        df.unpersist()\n\n        # From unmodified rows, select a subset for adding errors to, depending on config parameter.\n        unmodified_rows_df = unmodified_rows_df.cache()\n        error_row_prob = self.data_type_error_prob\n        unchanged_row_prob = 1.0 - error_row_prob\n        unchanged_rows_df, error_rows_df = unmodified_rows_df.randomSplit(\n            [unchanged_row_prob, error_row_prob], seed=self.seed + 5\n        )\n        # Gather rows that are not modified in this step (combine previously-modified rows and not-selected unmodified rows).\n        unchanged_rows_df = unchanged_rows_df.union(previously_modified_rows_df)\n\n        # Iterate over mandatory columns to mutate the value, depending on column data type.\n        for struct_schema in BronzeEventDataObject.SCHEMA:\n            if struct_schema.name not in self.error_generation_allowed_columns:\n                continue\n\n            column = struct_schema.name\n            col_dtype = struct_schema.dataType\n\n            if col_dtype in [BinaryType()]:\n                # md5 is a smaller hash,\n                to_value = F.unhex(F.md5(F.base64(F.col(column)).cast(StringType())))\n\n            if col_dtype in [FloatType(), IntegerType()]:\n                # changes mcc, lat, lon\n                to_value = (F.col(column) + ((F.rand(seed=self.seed) + F.lit(180)) * 10000)).cast(\"int\")\n\n            if column == ColNames.timestamp and col_dtype == StringType():\n                # Timezone difference manipulation may be performed here, if cleaning module were to support it.\n                # statically one timezone difference\n                # timezone_to = random.randint(0, 12)\n                to_value = F.concat(\n                    F.substring(F.col(column), 1, 10),\n                    F.lit(\"T\"),\n                    F.substring(F.col(column), 12, 9),\n                    # TODO: Temporary remove of timezone addition as cleaning\n                    # module does not support it\n                    # F.lit(f\"+0{timezone_to}:00\")\n                )\n\n            if column == ColNames.cell_id and col_dtype == StringType():\n                random.seed(self.seed)\n                random_string = \"\".join(random.choice(string.ascii_uppercase + string.digits) for _ in range(6)) + \"_\"\n                to_value = F.concat(F.lit(random_string), (F.rand(seed=self.seed) * 100).cast(\"int\"))\n\n            error_rows_df = error_rows_df.withColumn(column, to_value)\n        error_rows_df = error_rows_df.withColumn(\"is_modified\", F.lit(True))\n        return unchanged_rows_df.union(error_rows_df)\n\n    def generate_same_location_duplicates(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Selects a subset of the previously generated data (syntactically clean data) and creates same location duplicates.\n        If input has odd number of rows, one row is discarded.\n\n        Args:\n            df (pyspark.sql.DataFrame): Dataframe of clean synthetic events.\n\n        Returns:\n            pyspark.sql.DataFrame: Dataframe with same location duplicates.\n\n        \"\"\"\n\n        if self.same_location_duplicate_prob == 0:\n            return df\n\n        # Split dataframe by whether the row has been modified during the error-adding process already.\n        # Already errored rows do not get further changes.\n        df = df.cache()\n        previously_modified_rows_df = df.where(F.col(\"is_modified\"))\n        unmodified_rows_df = df.where(~(F.col(\"is_modified\")))\n        df.unpersist()\n\n        # From unmodified rows, select a subset for adding errors to, depending on config parameter.\n        unmodified_rows_df = unmodified_rows_df.cache()\n        error_row_prob = self.same_location_duplicate_prob\n        unchanged_row_prob = 1.0 - error_row_prob\n\n        unchanged_rows_df, error_rows_df = unmodified_rows_df.randomSplit(\n            [unchanged_row_prob, error_row_prob], seed=self.seed + 7\n        )\n\n        # Gather rows that are not modified in this step (combine previously-modified rows and not-selected unmodified rows).\n        unchanged_rows_df = unchanged_rows_df.union(previously_modified_rows_df)\n\n        # Select all even rows and duplicate these\n        error_rows_df = error_rows_df.withColumn(\n            \"user_row_num\",\n            F.row_number().over(Window.partitionBy(ColNames.user_id).orderBy(ColNames.timestamp)),\n        )\n\n        even_rows = error_rows_df.where(F.col(\"user_row_num\") % 2 == 0).drop(\"user_row_num\")\n\n        error_rows_df = even_rows.union(even_rows)\n        error_rows_df = error_rows_df.withColumn(\"is_modified\", F.lit(True))\n\n        return unchanged_rows_df.union(error_rows_df)\n\n    def generate_different_location_duplicates(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Selects a subset of the previously generated data (syntactically clean data) and creates different location duplicates.\n        If input has odd number of rows, one row is discarded.\n\n        Args:\n            df (pyspark.sql.DataFrame): Dataframe of clean synthetic events.\n\n        Returns:\n            pyspark.sql.DataFrame: Dataframe with same different location duplicates.\n\n        \"\"\"\n\n        if self.different_location_duplicate_prob == 0:\n            return df\n\n        # Split dataframe by whether the row has been modified during the error-adding process already.\n        # Already errored rows do not get further changes.\n        df = df.cache()\n        previously_modified_rows_df = df.where(F.col(\"is_modified\"))\n        unmodified_rows_df = df.where(~(F.col(\"is_modified\")))\n        df.unpersist()\n\n        # From unmodified rows, select a subset for adding errors to, depending on config parameter.\n        unmodified_rows_df = unmodified_rows_df.cache()\n        error_row_prob = self.different_location_duplicate_prob\n        unchanged_row_prob = 1.0 - error_row_prob\n        unchanged_rows_df, error_rows_df = unmodified_rows_df.randomSplit(\n            [unchanged_row_prob, error_row_prob], seed=self.seed + 9\n        )\n\n        # Gather rows that are not modified in this step (combine previously-modified rows and not-selected unmodified rows).\n        unchanged_rows_df = unchanged_rows_df.union(previously_modified_rows_df)\n\n        # Select all even rows and modify one set of these to offset the location\n\n        error_rows_df = error_rows_df.withColumn(\n            \"user_row_num\",\n            F.row_number().over(Window.partitionBy(ColNames.user_id).orderBy(ColNames.timestamp)),\n        )\n\n        even_rows = error_rows_df.where(F.col(\"user_row_num\") % 2 == 0).drop(\"user_row_num\")\n\n        # No change to cell id here, as that would additionnally need to check that the modified cell id is actual in the diaries\n        modified_rows = even_rows.withColumn(ColNames.latitude, F.col(ColNames.latitude) * F.lit(0.95)).withColumn(\n            ColNames.longitude, F.col(ColNames.longitude) * F.lit(0.95)\n        )\n\n        error_rows_df = even_rows.union(modified_rows)\n        error_rows_df = error_rows_df.withColumn(\"is_modified\", F.lit(True))\n\n        return unchanged_rows_df.union(error_rows_df)\n\n    def generate_errors(self, synth_df_raw: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Inputs a dataframe that contains synthetic records based on diaries.\n        These records include locational errors, etc. This function only selects the clean generated records from previous steps.\n        Generates errors for those clean records.\n        Calls all error generation functions.\n\n        Args:\n            synth_df_raw (pyspark.sql.DataFrame): Data of raw and clean synthetic events.\n\n        Returns:\n            pyspark.sql.DataFrame: Dataframe, with erroneous records, according to probabilities defined in the configuration.\n        \"\"\"\n\n        synth_df_raw = synth_df_raw.where(~F.col(\"is_modified\"))\n\n        synth_df = synth_df_raw.cache()\n\n        synth_df = self.generate_nulls_in_mandatory_fields(synth_df)\n        synth_df = self.generate_out_of_bounds_dates(synth_df)\n        synth_df = self.generate_erroneous_type_values(synth_df)\n        synth_df = self.generate_same_location_duplicates(synth_df)\n        synth_df = self.generate_different_location_duplicates(synth_df)\n\n        return synth_df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.add_cell_ids_to_locations","title":"<code>add_cell_ids_to_locations(events_with_locations_df, cells_df, max_n_of_cells, seed)</code>  <code>staticmethod</code>","text":"<p>Links cell IDs to locations in the events DataFrame.</p> <p>This method performs a spatial join between the events and cells DataFrames to add cell IDs to each event. It first creates a buffer around each event location and finds cells that intersect with this buffer. It then calculates the distance from each event location to the cell and ranks the cells based on this distance. It keeps only the top 'max_n_of_cells' closest cells for each event.</p> <p>The method also adds a random index to each event-cell pair and filters to keep only one pair per event, randomly selecting one of the closest cells for each event.</p> <p>Parameters:</p> Name Type Description Default <code>events_with_locations_df</code> <code>DataFrame</code> <p>A DataFrame of events</p> required <code>cells_df</code> <code>DataFrame</code> <p>A DataFrame of cells</p> required <code>max_n_of_cells</code> <code>int</code> <p>The maximum number of closest cells to consider for each event.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: A DataFrame of events with cell IDs added.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>@staticmethod\ndef add_cell_ids_to_locations(\n    events_with_locations_df: DataFrame,\n    cells_df: DataFrame,\n    max_n_of_cells: int,\n    seed: int,\n) -&gt; DataFrame:\n    \"\"\"\n    Links cell IDs to locations in the events DataFrame.\n\n    This method performs a spatial join between the events and cells DataFrames to add cell IDs to each event.\n    It first creates a buffer around each event location and finds cells that intersect with this buffer.\n    It then calculates the distance from each event location to the cell and ranks the cells based on this distance.\n    It keeps only the top 'max_n_of_cells' closest cells for each event.\n\n    The method also adds a random index to each event-cell pair and filters to keep only one pair per event,\n    randomly selecting one of the closest cells for each event.\n\n    Args:\n        events_with_locations_df (pyspark.sql.DataFrame): A DataFrame of events\n        cells_df (pyspark.sql.DataFrame): A DataFrame of cells\n        max_n_of_cells (int): The maximum number of closest cells to consider for each event.\n\n    Returns:\n        pyspark.sql.DataFrame: A DataFrame of events with cell IDs added.\n    \"\"\"\n    events_with_cells_sdf = events_with_locations_df.join(\n        cells_df[[ColNames.cell_id, \"cell_geometry\"]],\n        (\n            STP.ST_Intersects(\n                STF.ST_Buffer(\n                    events_with_locations_df[\"generated_geometry\"],\n                    F.col(\"closest_cell_distance_max\"),\n                ),\n                cells_df[\"cell_geometry\"],\n            )\n        ),\n    ).withColumn(\n        \"distance_to_cell\",\n        STF.ST_Distance(\n            events_with_locations_df[\"generated_geometry\"],\n            cells_df[\"cell_geometry\"],\n        ),\n    )\n\n    # Selection of different cell ids for a given timestamp is random\n    window_spec = Window.partitionBy(\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n        ColNames.user_id,\n        ColNames.timestamp,\n    ).orderBy(F.col(\"distance_to_cell\"))\n\n    events_with_cells_sdf = events_with_cells_sdf.withColumn(\n        \"closest_cells_index\", F.row_number().over(window_spec)\n    ).filter(F.col(\"closest_cells_index\") &lt;= max_n_of_cells)\n\n    window_spec_random = Window.partitionBy(\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n        ColNames.user_id,\n        ColNames.timestamp,\n    ).orderBy(F.rand(seed=seed))\n    events_with_cells_sdf = events_with_cells_sdf.withColumn(\n        \"random_cell_index\", F.row_number().over(window_spec_random)\n    ).filter(F.col(\"random_cell_index\") == 1)\n\n    records_sdf = events_with_cells_sdf.select(\n        ColNames.user_id,\n        ColNames.timestamp,\n        \"generated_geometry\",\n        ColNames.cell_id,\n        ColNames.loc_error,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n        \"is_modified\",\n    )\n\n    return records_sdf\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_different_location_duplicates","title":"<code>generate_different_location_duplicates(df)</code>","text":"<p>Selects a subset of the previously generated data (syntactically clean data) and creates different location duplicates. If input has odd number of rows, one row is discarded.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe of clean synthetic events.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: Dataframe with same different location duplicates.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>def generate_different_location_duplicates(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Selects a subset of the previously generated data (syntactically clean data) and creates different location duplicates.\n    If input has odd number of rows, one row is discarded.\n\n    Args:\n        df (pyspark.sql.DataFrame): Dataframe of clean synthetic events.\n\n    Returns:\n        pyspark.sql.DataFrame: Dataframe with same different location duplicates.\n\n    \"\"\"\n\n    if self.different_location_duplicate_prob == 0:\n        return df\n\n    # Split dataframe by whether the row has been modified during the error-adding process already.\n    # Already errored rows do not get further changes.\n    df = df.cache()\n    previously_modified_rows_df = df.where(F.col(\"is_modified\"))\n    unmodified_rows_df = df.where(~(F.col(\"is_modified\")))\n    df.unpersist()\n\n    # From unmodified rows, select a subset for adding errors to, depending on config parameter.\n    unmodified_rows_df = unmodified_rows_df.cache()\n    error_row_prob = self.different_location_duplicate_prob\n    unchanged_row_prob = 1.0 - error_row_prob\n    unchanged_rows_df, error_rows_df = unmodified_rows_df.randomSplit(\n        [unchanged_row_prob, error_row_prob], seed=self.seed + 9\n    )\n\n    # Gather rows that are not modified in this step (combine previously-modified rows and not-selected unmodified rows).\n    unchanged_rows_df = unchanged_rows_df.union(previously_modified_rows_df)\n\n    # Select all even rows and modify one set of these to offset the location\n\n    error_rows_df = error_rows_df.withColumn(\n        \"user_row_num\",\n        F.row_number().over(Window.partitionBy(ColNames.user_id).orderBy(ColNames.timestamp)),\n    )\n\n    even_rows = error_rows_df.where(F.col(\"user_row_num\") % 2 == 0).drop(\"user_row_num\")\n\n    # No change to cell id here, as that would additionnally need to check that the modified cell id is actual in the diaries\n    modified_rows = even_rows.withColumn(ColNames.latitude, F.col(ColNames.latitude) * F.lit(0.95)).withColumn(\n        ColNames.longitude, F.col(ColNames.longitude) * F.lit(0.95)\n    )\n\n    error_rows_df = even_rows.union(modified_rows)\n    error_rows_df = error_rows_df.withColumn(\"is_modified\", F.lit(True))\n\n    return unchanged_rows_df.union(error_rows_df)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_erroneous_type_values","title":"<code>generate_erroneous_type_values(df)</code>","text":"<p>Generates errors for sampled rows. Errors are custom defined, for instance a random string, or corrupt timestamp. Does not cast the columns to a different type.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe that may have out of bound and null records.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: dataframe with erroneous rows, and possibly, with nulls and out of bound records.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>def generate_erroneous_type_values(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Generates errors for sampled rows. Errors are custom defined, for instance a random string, or corrupt timestamp.\n    Does not cast the columns to a different type.\n\n    Args:\n        df (pyspark.sql.DataFrame): dataframe that may have out of bound and null records.\n\n    Returns:\n        pyspark.sql.DataFrame: dataframe with erroneous rows, and possibly, with nulls and out of bound records.\n    \"\"\"\n\n    if self.data_type_error_prob == 0:\n        # TODO logging\n        return df\n\n    # Split dataframe by whether the row has been modified during the error-adding process already.\n    # Already errored rows do not get further changes.\n    df = df.cache()\n    previously_modified_rows_df = df.where(F.col(\"is_modified\"))\n    unmodified_rows_df = df.where(~(F.col(\"is_modified\")))\n    df.unpersist()\n\n    # From unmodified rows, select a subset for adding errors to, depending on config parameter.\n    unmodified_rows_df = unmodified_rows_df.cache()\n    error_row_prob = self.data_type_error_prob\n    unchanged_row_prob = 1.0 - error_row_prob\n    unchanged_rows_df, error_rows_df = unmodified_rows_df.randomSplit(\n        [unchanged_row_prob, error_row_prob], seed=self.seed + 5\n    )\n    # Gather rows that are not modified in this step (combine previously-modified rows and not-selected unmodified rows).\n    unchanged_rows_df = unchanged_rows_df.union(previously_modified_rows_df)\n\n    # Iterate over mandatory columns to mutate the value, depending on column data type.\n    for struct_schema in BronzeEventDataObject.SCHEMA:\n        if struct_schema.name not in self.error_generation_allowed_columns:\n            continue\n\n        column = struct_schema.name\n        col_dtype = struct_schema.dataType\n\n        if col_dtype in [BinaryType()]:\n            # md5 is a smaller hash,\n            to_value = F.unhex(F.md5(F.base64(F.col(column)).cast(StringType())))\n\n        if col_dtype in [FloatType(), IntegerType()]:\n            # changes mcc, lat, lon\n            to_value = (F.col(column) + ((F.rand(seed=self.seed) + F.lit(180)) * 10000)).cast(\"int\")\n\n        if column == ColNames.timestamp and col_dtype == StringType():\n            # Timezone difference manipulation may be performed here, if cleaning module were to support it.\n            # statically one timezone difference\n            # timezone_to = random.randint(0, 12)\n            to_value = F.concat(\n                F.substring(F.col(column), 1, 10),\n                F.lit(\"T\"),\n                F.substring(F.col(column), 12, 9),\n                # TODO: Temporary remove of timezone addition as cleaning\n                # module does not support it\n                # F.lit(f\"+0{timezone_to}:00\")\n            )\n\n        if column == ColNames.cell_id and col_dtype == StringType():\n            random.seed(self.seed)\n            random_string = \"\".join(random.choice(string.ascii_uppercase + string.digits) for _ in range(6)) + \"_\"\n            to_value = F.concat(F.lit(random_string), (F.rand(seed=self.seed) * 100).cast(\"int\"))\n\n        error_rows_df = error_rows_df.withColumn(column, to_value)\n    error_rows_df = error_rows_df.withColumn(\"is_modified\", F.lit(True))\n    return unchanged_rows_df.union(error_rows_df)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_errors","title":"<code>generate_errors(synth_df_raw)</code>","text":"<p>Inputs a dataframe that contains synthetic records based on diaries. These records include locational errors, etc. This function only selects the clean generated records from previous steps. Generates errors for those clean records. Calls all error generation functions.</p> <p>Parameters:</p> Name Type Description Default <code>synth_df_raw</code> <code>DataFrame</code> <p>Data of raw and clean synthetic events.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: Dataframe, with erroneous records, according to probabilities defined in the configuration.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>def generate_errors(self, synth_df_raw: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Inputs a dataframe that contains synthetic records based on diaries.\n    These records include locational errors, etc. This function only selects the clean generated records from previous steps.\n    Generates errors for those clean records.\n    Calls all error generation functions.\n\n    Args:\n        synth_df_raw (pyspark.sql.DataFrame): Data of raw and clean synthetic events.\n\n    Returns:\n        pyspark.sql.DataFrame: Dataframe, with erroneous records, according to probabilities defined in the configuration.\n    \"\"\"\n\n    synth_df_raw = synth_df_raw.where(~F.col(\"is_modified\"))\n\n    synth_df = synth_df_raw.cache()\n\n    synth_df = self.generate_nulls_in_mandatory_fields(synth_df)\n    synth_df = self.generate_out_of_bounds_dates(synth_df)\n    synth_df = self.generate_erroneous_type_values(synth_df)\n    synth_df = self.generate_same_location_duplicates(synth_df)\n    synth_df = self.generate_different_location_duplicates(synth_df)\n\n    return synth_df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_event_timestamps_for_moves","title":"<code>generate_event_timestamps_for_moves(stays_sdf, event_freq_moves, cartesian_crs, seed)</code>  <code>staticmethod</code>","text":"<p>Generates a DataFrame of event timestamps for movements between stays.</p> <p>For each stay in the input DataFrame, this method generates a random number of timestamps equal to the time     difference between the end of the current stay and the start of the next stay, divided by the event     frequency for moves.</p> <p>Parameters:</p> Name Type Description Default <code>stays_sdf</code> <code>DataFrame</code> <p>A DataFrame of stays.</p> required <code>event_freq_moves</code> <code>int</code> <p>The frequency of events for movements.</p> required <code>cartesian_crs</code> <code>int</code> <p>The EPSG code of the Cartesian coordinate reference system to use for the geometries.</p> required <code>seed</code> <code>int</code> <p>The seed that determines the randomness of timestamp generation, and subsequent lat/lon generation.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: A DataFrame of event timestamps for movements.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>@staticmethod\ndef generate_event_timestamps_for_moves(\n    stays_sdf: DataFrame, event_freq_moves: int, cartesian_crs: int, seed: int\n) -&gt; DataFrame:\n    \"\"\"\n    Generates a DataFrame of event timestamps for movements between stays.\n\n    For each stay in the input DataFrame, this method generates a random number of timestamps equal to the time\n        difference between the end of the current stay and the start of the next stay, divided by the event\n        frequency for moves.\n\n    Args:\n        stays_sdf (pyspark.sql.DataFrame): A DataFrame of stays.\n        event_freq_moves (int): The frequency of events for movements.\n        cartesian_crs (int): The EPSG code of the Cartesian coordinate reference system to use for the geometries.\n        seed (int): The seed that determines the randomness of timestamp generation, and subsequent lat/lon generation.\n\n    Returns:\n        pyspark.sql.DataFrame: A DataFrame of event timestamps for movements.\n    \"\"\"\n\n    # Since the rows with activity_type = movement don't have any locations in the population diaries,\n    # we select the stay points and start generating timestamps in between the start and end of the stay\n\n    stays_sdf = stays_sdf.withColumn(\n        ColNames.geometry,\n        STF.ST_Transform(\n            STC.ST_Point(F.col(ColNames.longitude), F.col(ColNames.latitude)),\n            F.lit(\"EPSG:4326\"),\n            F.lit(f\"EPSG:{cartesian_crs}\"),\n        ),\n    )\n\n    # Define the window specification\u00a0\u2022\n    window_spec = Window.partitionBy(ColNames.year, ColNames.month, ColNames.day, ColNames.user_id).orderBy(\n        ColNames.initial_timestamp\n    )\n\n    # Add columns for next stay's geometry and start timestamp using the lead function\n    stays_sdf = stays_sdf.withColumn(\n        \"next_stay_initial_timestamp\",\n        F.lead(ColNames.initial_timestamp, 1).over(window_spec),\n    )\n\n    stays_sdf = stays_sdf.withColumn(\"next_stay_geometry\", F.lead(ColNames.geometry, 1).over(window_spec))\n\n    stays_sdf = stays_sdf.withColumn(\n        \"time_diff_seconds\",\n        F.unix_timestamp(\"next_stay_initial_timestamp\") - F.unix_timestamp(ColNames.final_timestamp),\n    )\n\n    # Calculate how many timestamps fit in the interval for the given frequency\n    stays_sdf = stays_sdf.withColumn(\n        \"timestamps_count\",\n        (F.col(\"time_diff_seconds\") / event_freq_moves).cast(\"integer\"),\n    )\n\n    expr_str = f\"transform(sequence(1, timestamps_count), x -&gt; rand({seed}))\"\n\n    stays_sdf = stays_sdf.withColumn(\n        \"random_fraction_on_line\",\n        F.expr(expr_str),\n    ).withColumn(\"random_fraction_on_line\", F.explode(F.col(\"random_fraction_on_line\")))\n\n    # Generate timestamps\n    stays_sdf = stays_sdf.withColumn(\n        \"offset_seconds\",\n        F.col(\"random_fraction_on_line\") * F.col(\"time_diff_seconds\"),\n    )\n    stays_sdf = stays_sdf.withColumn(\n        ColNames.timestamp,\n        F.from_unixtime(F.unix_timestamp(ColNames.final_timestamp) + F.col(\"offset_seconds\")),\n    )\n\n    moves_sdf = stays_sdf.withColumn(ColNames.activity_type, F.lit(\"move\"))\n\n    # Keep only necessary columns\n    moves_sdf = moves_sdf.select(\n        ColNames.user_id,\n        ColNames.timestamp,\n        ColNames.geometry,\n        \"next_stay_geometry\",\n        \"random_fraction_on_line\",\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    )\n\n    return moves_sdf\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_event_timestamps_for_stays","title":"<code>generate_event_timestamps_for_stays(stays_df, event_freq_stays, cartesian_crs, seed)</code>  <code>staticmethod</code>","text":"<p>Generates a DataFrame of event timestamps for stays based on the event frequency for stays.</p> <p>For each stay in the input DataFrame, this method calculates the time difference between the initial and final timestamps of the stay. It then generates a number of timestamps equal to this time difference divided by the event frequency for stays. Each timestamp is associated with the location of the stay.</p> <p>Parameters:</p> Name Type Description Default <code>stays_df</code> <code>DataFrame</code> <p>A DataFrame of stays.</p> required <code>event_freq_stays</code> <code>int</code> <p>The frequency of events for stays.</p> required <code>cartesian_crs</code> <code>int</code> <p>The EPSG code of the Cartesian coordinate reference system to use for the geometries.</p> required <code>seed</code> <code>int</code> <p>The seed for generating timestamps randomly.</p> required <p>Returns:     pyspark.sql.DataFrame: A DataFrame of event timestamps for stays.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>@staticmethod\ndef generate_event_timestamps_for_stays(\n    stays_df: DataFrame, event_freq_stays: int, cartesian_crs: int, seed: int\n) -&gt; DataFrame:\n    \"\"\"\n    Generates a DataFrame of event timestamps for stays based on the event frequency for stays.\n\n    For each stay in the input DataFrame, this method calculates\n    the time difference between the initial and final timestamps of the stay.\n    It then generates a number of timestamps equal to this time difference divided by the event\n    frequency for stays. Each timestamp is associated with the location of the stay.\n\n    Args:\n        stays_df (pyspark.sql.DataFrame): A DataFrame of stays.\n        event_freq_stays (int): The frequency of events for stays.\n        cartesian_crs (int): The EPSG code of the Cartesian coordinate reference system to use for the geometries.\n        seed (int): The seed for generating timestamps randomly.\n    Returns:\n        pyspark.sql.DataFrame: A DataFrame of event timestamps for stays.\n    \"\"\"\n\n    stays_df = stays_df.withColumn(\n        \"time_diff_seconds\",\n        F.unix_timestamp(F.col(ColNames.final_timestamp)) - F.unix_timestamp(F.col(ColNames.initial_timestamp)),\n    )\n\n    stays_df = stays_df.withColumn(\n        \"timestamps_count\",\n        (F.col(\"time_diff_seconds\") / event_freq_stays).cast(\"integer\"),\n    )\n\n    expr_str = f\"transform(sequence(1, timestamps_count), x -&gt; rand({seed}))\"\n    stays_df = stays_df.withColumn(\n        \"random_fraction_between_timestamps\",\n        F.expr(expr_str),\n    )\n\n    stays_df = stays_df.withColumn(\n        \"random_fraction_between_timestamps\",\n        F.explode(F.col(\"random_fraction_between_timestamps\")),\n    )\n    stays_df = stays_df.withColumn(\n        \"offset_seconds\",\n        F.col(\"random_fraction_between_timestamps\") * F.col(\"time_diff_seconds\"),\n    )\n    stays_df = stays_df.withColumn(\n        ColNames.timestamp,\n        F.from_unixtime(F.unix_timestamp(ColNames.initial_timestamp) + F.col(\"offset_seconds\")),\n    )\n\n    stays_df = stays_df.withColumn(\n        \"generated_geometry\",\n        STF.ST_Transform(\n            STC.ST_Point(F.col(ColNames.longitude), F.col(ColNames.latitude)),\n            F.lit(\"EPSG:4326\"),\n            F.lit(f\"EPSG:{cartesian_crs}\"),\n        ),\n    )\n\n    stays_df = stays_df.select(\n        ColNames.user_id,\n        ColNames.timestamp,\n        \"generated_geometry\",\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    )\n\n    return stays_df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_location_errors","title":"<code>generate_location_errors(records_sdf, error_location_distance_max, error_location_distance_min, closest_cell_distance_max, cartesian_crs, seed)</code>  <code>staticmethod</code>","text":"<p>Generates location errors for x and y coordinates of each record in the DataFrame.</p> <p>This method adds a random location error to the x and y coordinates of each record in the input DataFrame. The location error is a random value between error_location_distance_min and error_location_distance_max, and is added or subtracted from the x and y coordinates based on a random sign.</p> <p>Parameters:</p> Name Type Description Default <code>records_sdf</code> <code>DataFrame</code> <p>A DataFrame of records</p> required <code>error_location_distance_max</code> <code>float</code> <p>The maximum location error distance.</p> required <code>error_location_distance_min</code> <code>float</code> <p>The minimum location error distance.</p> required <code>closest_cell_distance_max</code> <code>float</code> <p>The maximum distance to the closest cell.</p> required <code>cartesian_crs</code> <code>int</code> <p>The EPSG code of the Cartesian coordinate reference system to use for the geometries.</p> required <code>seed</code> <code>int</code> <p>The seed for the random number generator.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame of records with location errors added to the x and y coordinates.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>@staticmethod\ndef generate_location_errors(\n    records_sdf: DataFrame,\n    error_location_distance_max: float,\n    error_location_distance_min: float,\n    closest_cell_distance_max: float,\n    cartesian_crs: int,\n    seed: int,\n) -&gt; DataFrame:\n    \"\"\"\n    Generates location errors for x and y coordinates of each record in the DataFrame.\n\n    This method adds a random location error to the x and y coordinates of each record in the input DataFrame.\n    The location error is a random value between error_location_distance_min and error_location_distance_max,\n    and is added or subtracted from the x and y coordinates based on a random sign.\n\n    Args:\n        records_sdf (DataFrame): A DataFrame of records\n        error_location_distance_max (float): The maximum location error distance.\n        error_location_distance_min (float): The minimum location error distance.\n        closest_cell_distance_max (float): The maximum distance to the closest cell.\n        cartesian_crs (int): The EPSG code of the Cartesian coordinate reference system to use for the geometries.\n        seed (int): The seed for the random number generator.\n\n    Returns:\n        DataFrame: A DataFrame of records with location errors added to the x and y coordinates.\n    \"\"\"\n\n    errors_df = (\n        records_sdf.withColumn(\"y\", STF.ST_Y(F.col(\"generated_geometry\")))\n        .withColumn(\"x\", STF.ST_X(F.col(\"generated_geometry\")))\n        .drop(\"generated_geometry\")\n    )\n\n    errors_df = errors_df.withColumn(\n        ColNames.loc_error,\n        (F.rand(seed=seed) * (error_location_distance_max - error_location_distance_min))\n        + error_location_distance_min,\n    )\n\n    errors_df = (\n        errors_df.withColumn(\"sign\", F.when(F.rand(seed=seed) &gt; 0.5, 1).otherwise(-1))\n        .withColumn(\"new_x\", F.col(\"x\") + (F.col(ColNames.loc_error) * F.col(\"sign\")))\n        .withColumn(\"new_y\", F.col(\"y\") + (F.col(ColNames.loc_error) * F.col(\"sign\")))\n    )\n\n    errors_df = errors_df.withColumn(\n        \"generated_geometry\",\n        STF.ST_SetSRID(STC.ST_Point(F.col(\"new_x\"), F.col(\"new_y\")), cartesian_crs),\n    ).drop(\"new_x\", \"new_y\")\n\n    errors_df = errors_df.withColumn(\"closest_cell_distance_max\", F.lit(closest_cell_distance_max))\n\n    errors_df = errors_df.select(\n        ColNames.user_id,\n        ColNames.timestamp,\n        \"generated_geometry\",\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n        ColNames.loc_error,\n        \"closest_cell_distance_max\",\n    )\n\n    return errors_df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_locations_for_moves","title":"<code>generate_locations_for_moves(event_timestamps_df, cartesian_crs)</code>  <code>staticmethod</code>","text":"<p>Generates locations for moves based on the event timestamps dataframe. Returns a dataframe, where for each move in the event timestamps dataframe a geometry column is added, representing the location of the move.</p> <p>Performs interpolation along the line between the starting move point (previous stay point) and next move point (next stay point) using the randomly generated values in the column random_fraction_on_line.</p> <p>Parameters:</p> Name Type Description Default <code>event_timestamps_df</code> <code>DataFrame</code> <p>The event timestamps dataframe.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: The event timestamps dataframe with the added geometry column.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>@staticmethod\ndef generate_locations_for_moves(event_timestamps_df: DataFrame, cartesian_crs: int) -&gt; DataFrame:\n    \"\"\"\n    Generates locations for moves based on the event timestamps dataframe.\n    Returns a dataframe, where for each move in the event timestamps dataframe\n    a geometry column is added, representing the location of the move.\n\n    Performs interpolation along the line between the starting move point (previous stay point)\n    and next move point (next stay point) using the randomly generated values in the column random_fraction_on_line.\n\n    Args:\n        event_timestamps_df (pyspark.sql.DataFrame): The event timestamps dataframe.\n\n    Returns:\n        pyspark.sql.DataFrame: The event timestamps dataframe with the added geometry column.\n    \"\"\"\n\n    moves_with_geometry = event_timestamps_df.withColumn(\n        \"line\",\n        STF.ST_MakeLine(F.col(ColNames.geometry), F.col(\"next_stay_geometry\")),\n    )\n    moves_with_geometry = moves_with_geometry.withColumn(\n        \"generated_geometry\",\n        STF.ST_SetSRID(\n            STF.ST_LineInterpolatePoint(F.col(\"line\"), F.col(\"random_fraction_on_line\")),\n            cartesian_crs,\n        ),\n    )\n\n    moves_with_geometry = moves_with_geometry.select(\n        ColNames.user_id,\n        ColNames.timestamp,\n        \"generated_geometry\",\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    )\n\n    return moves_with_geometry\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_nulls_in_mandatory_fields","title":"<code>generate_nulls_in_mandatory_fields(df)</code>","text":"<p>Generates null values in some fields of some rows based on configuration parameters.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>clean synthetic data</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: synthetic records dataframe with nulls in some columns of some rows</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>def generate_nulls_in_mandatory_fields(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Generates null values in some fields of some rows based on configuration parameters.\n\n    Args:\n        df (pyspark.sql.DataFrame): clean synthetic data\n\n    Returns:\n        pyspark.sql.DataFrame: synthetic records dataframe with nulls in some columns of some rows\n    \"\"\"\n\n    # Two probability parameters from config apply:\n    # First one sets how many rows (as fraction of all rows) are selected for possible value nulling.\n    # Second one sets the likelyhood for each column to be set to null.\n    # If 1.0, then all mandatory columns of each selected row will be nulled.\n\n    if self.null_row_prob == 0.0:\n        # TODO logging\n        return df\n\n    # Split input dataframe to unchanged and changed portions\n    df = df.cache()\n    error_row_prob = self.null_row_prob\n    unchanged_row_prob = 1.0 - error_row_prob\n    unchanged_rows_df, error_rows_df = df.randomSplit([unchanged_row_prob, error_row_prob], seed=self.seed + 1)\n\n    columns_for_null_selection = list(self.error_generation_allowed_columns)\n    columns_for_null_selection.sort()\n\n    random.seed(self.seed)\n    columns_to_set_as_null = random.sample(\n        columns_for_null_selection,\n        int(round(self.column_is_null_probability * len(columns_for_null_selection), 0)),\n    )\n\n    for column in columns_to_set_as_null:\n        error_rows_df = error_rows_df.withColumn(column, F.lit(None))\n\n    error_rows_df = error_rows_df.withColumn(\"is_modified\", F.lit(True))\n\n    # Re-combine unchanged and changed rows of the dataframe.\n    return unchanged_rows_df.union(error_rows_df)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_out_of_bounds_dates","title":"<code>generate_out_of_bounds_dates(df)</code>","text":"<p>Transforms the timestamp column values to be out of bound of the selected period, based on probabilities from configuration. Only rows with non-null timestamp values can become altered here.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe of clean synthetic events.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: Data where some timestamp column values are out of bounds as per config.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>def generate_out_of_bounds_dates(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the timestamp column values to be out of bound of the selected period,\n    based on probabilities from configuration.\n    Only rows with non-null timestamp values can become altered here.\n\n    Args:\n        df (pyspark.sql.DataFrame): Dataframe of clean synthetic events.\n\n    Returns:\n        pyspark.sql.DataFrame: Data where some timestamp column values are out of bounds as per config.\n    \"\"\"\n\n    if self.out_of_bounds_prob == 0.0:\n        # TODO logging\n        return df\n\n    # Calculate approximate span in months from config parameters.\n    # The parameters should approximately cover the range of \"clean\" dates, so that the erroneous values can be generated outside the range.\n\n    # TODO\n    # This now uses the whole input data to set the bounds\n    ending_timestamp = df.agg(F.max(ColNames.timestamp)).collect()[0][0]\n    starting_timestamp = df.agg(F.min(ColNames.timestamp)).collect()[0][0]\n    events_span_in_months = max(1, (pd.Timestamp(ending_timestamp) - pd.Timestamp(starting_timestamp)).days / 30)\n\n    # Split rows by null/non-null timestamp.\n    df = df.cache()\n    null_timestamp_df = df.where(F.col(ColNames.timestamp).isNull())\n    nonnull_timestamp_df = df.where(F.col(ColNames.timestamp).isNotNull())\n    df.unpersist()\n\n    # From non-null timestamp rows, select a subset for adding errors to, depending on config parameter.\n    nonnull_timestamp_df = nonnull_timestamp_df.cache()\n    error_row_prob = self.out_of_bounds_prob\n    unchanged_row_prob = 1.0 - error_row_prob\n    unchanged_rows_df, error_rows_df = nonnull_timestamp_df.randomSplit(\n        [unchanged_row_prob, error_row_prob], seed=self.seed + 3\n    )\n    # Combine null timestamp rows and not-modified non-null timestamp rows.\n    unchanged_rows_df = unchanged_rows_df.union(null_timestamp_df)\n\n    # Add months offset to error rows to make their timestamp values become outside expected range.\n    months_to_add_col = (F.lit(2) + F.rand(seed=self.seed)) * F.lit(events_span_in_months)\n    modified_date_col = F.add_months(F.col(ColNames.timestamp), months_to_add_col)\n    time_col = F.date_format(F.col(ColNames.timestamp), \" HH:mm:ss\")\n    error_rows_df = error_rows_df.withColumn(ColNames.timestamp, F.concat(modified_date_col, time_col))\n    error_rows_df = error_rows_df.withColumn(\"is_modified\", F.lit(True))\n\n    # Combine changed and unchanged rows dataframes.\n    return unchanged_rows_df.union(error_rows_df)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_records_with_non_existant_cell_ids","title":"<code>generate_records_with_non_existant_cell_ids(records_sdf, cells_sdf, seed)</code>  <code>staticmethod</code>","text":"<p>Adds the cell_id column so that it will contain cell_ids that are not present in the cells_df dataframe, yet follow the format of a cell id.</p> <p>Parameters:</p> Name Type Description Default <code>records_sdf</code> <code>DataFrame</code> <p>generated records</p> required <code>cells_sdf</code> <code>DataFrame</code> <p>cells dataframe</p> required <code>DataFrame</code> <p>records with cell ids that are not present in the cells_df dataframe</p> required Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>@staticmethod\ndef generate_records_with_non_existant_cell_ids(records_sdf: DataFrame, cells_sdf: DataFrame, seed) -&gt; DataFrame:\n    \"\"\"\n    Adds the cell_id column so that it will contain cell_ids that\n    are not present in the cells_df dataframe, yet follow the format of a cell id.\n\n    Args:\n        records_sdf (DataFrame): generated records\n        cells_sdf (DataFrame): cells dataframe\n\n        DataFrame: records with cell ids that are not present in the cells_df dataframe\n    \"\"\"\n\n    # Generates random cell ids for cells_df, and selects those\n    # Join to records is implemented with a monotonically increasing id\n    # So to limit that, this number of all unique cells is used\n    # TODO check how to make this more optimal\n\n    n_of_actual_cell_ids_as_base = cells_sdf.count()\n\n    cells_df_with_random_cell_ids = cells_sdf[[ColNames.cell_id]].withColumn(\n        \"random_cell_id\",\n        (F.rand(seed=seed) * (999_999_999_999_999 - 10_000_000_000_000) + 10_000_000_000_000).cast(\"bigint\"),\n    )\n\n    # Do an  left anti join to ensure that now generated cell ids are not among actual cell ids\n\n    cells_df_inner_joined = cells_df_with_random_cell_ids[[\"random_cell_id\"]].join(\n        cells_sdf[[ColNames.cell_id]],\n        on=F.col(\"random_cell_id\") == F.col(ColNames.cell_id),\n        how=\"leftanti\",\n    )\n\n    cells_df_inner_joined = cells_df_inner_joined.withColumn(\"row_number\", (F.monotonically_increasing_id()))\n    records_sdf = records_sdf.withColumn(\n        \"row_number\",\n        (F.monotonically_increasing_id() % n_of_actual_cell_ids_as_base) + 1,\n    )\n\n    cells_df_inner_joined = cells_df_inner_joined.select(\n        \"row_number\", F.col(\"random_cell_id\").alias(ColNames.cell_id)\n    )\n\n    records_with_random_cell_id = records_sdf.join(cells_df_inner_joined, on=\"row_number\", how=\"left\").drop(\n        \"row_number\"\n    )\n\n    records_with_random_cell_id = records_with_random_cell_id.select(\n        ColNames.user_id,\n        ColNames.timestamp,\n        \"generated_geometry\",\n        ColNames.cell_id,\n        ColNames.loc_error,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    )\n\n    return records_with_random_cell_id\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_same_location_duplicates","title":"<code>generate_same_location_duplicates(df)</code>","text":"<p>Selects a subset of the previously generated data (syntactically clean data) and creates same location duplicates. If input has odd number of rows, one row is discarded.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe of clean synthetic events.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: Dataframe with same location duplicates.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>def generate_same_location_duplicates(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Selects a subset of the previously generated data (syntactically clean data) and creates same location duplicates.\n    If input has odd number of rows, one row is discarded.\n\n    Args:\n        df (pyspark.sql.DataFrame): Dataframe of clean synthetic events.\n\n    Returns:\n        pyspark.sql.DataFrame: Dataframe with same location duplicates.\n\n    \"\"\"\n\n    if self.same_location_duplicate_prob == 0:\n        return df\n\n    # Split dataframe by whether the row has been modified during the error-adding process already.\n    # Already errored rows do not get further changes.\n    df = df.cache()\n    previously_modified_rows_df = df.where(F.col(\"is_modified\"))\n    unmodified_rows_df = df.where(~(F.col(\"is_modified\")))\n    df.unpersist()\n\n    # From unmodified rows, select a subset for adding errors to, depending on config parameter.\n    unmodified_rows_df = unmodified_rows_df.cache()\n    error_row_prob = self.same_location_duplicate_prob\n    unchanged_row_prob = 1.0 - error_row_prob\n\n    unchanged_rows_df, error_rows_df = unmodified_rows_df.randomSplit(\n        [unchanged_row_prob, error_row_prob], seed=self.seed + 7\n    )\n\n    # Gather rows that are not modified in this step (combine previously-modified rows and not-selected unmodified rows).\n    unchanged_rows_df = unchanged_rows_df.union(previously_modified_rows_df)\n\n    # Select all even rows and duplicate these\n    error_rows_df = error_rows_df.withColumn(\n        \"user_row_num\",\n        F.row_number().over(Window.partitionBy(ColNames.user_id).orderBy(ColNames.timestamp)),\n    )\n\n    even_rows = error_rows_df.where(F.col(\"user_row_num\") % 2 == 0).drop(\"user_row_num\")\n\n    error_rows_df = even_rows.union(even_rows)\n    error_rows_df = error_rows_df.withColumn(\"is_modified\", F.lit(True))\n\n    return unchanged_rows_df.union(error_rows_df)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/","title":"synthetic_network","text":"<p>Module that generates a MNO synthetic network.</p>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.CellIDGenerator","title":"<code>CellIDGenerator</code>","text":"<p>Abstract class for cell ID generation.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>class CellIDGenerator(metaclass=ABCMeta):\n    \"\"\"\n    Abstract class for cell ID generation.\n    \"\"\"\n\n    def __init__(self, rng: int | Random) -&gt; None:\n        \"\"\"Cell ID Generator constructor\n\n        Args:\n            rng (int | Random): either an integer to act as a seed for RNG, or an instantiated Random object.\n        \"\"\"\n        if isinstance(rng, int):\n            self.rng = Random(rng)\n        elif isinstance(rng, Random):\n            self.rng = rng\n\n    @abstractmethod\n    def generate_cell_ids(self, n_cells: int) -&gt; list[str]:\n        \"\"\"Method that generates random cell IDs.\n\n        Args:\n            n_cells (int): number of cell IDs to generate.\n\n        Returns:\n            list[str]: list of cell IDs.\n        \"\"\"\n        return None\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.CellIDGenerator.__init__","title":"<code>__init__(rng)</code>","text":"<p>Cell ID Generator constructor</p> <p>Parameters:</p> Name Type Description Default <code>rng</code> <code>int | Random</code> <p>either an integer to act as a seed for RNG, or an instantiated Random object.</p> required Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>def __init__(self, rng: int | Random) -&gt; None:\n    \"\"\"Cell ID Generator constructor\n\n    Args:\n        rng (int | Random): either an integer to act as a seed for RNG, or an instantiated Random object.\n    \"\"\"\n    if isinstance(rng, int):\n        self.rng = Random(rng)\n    elif isinstance(rng, Random):\n        self.rng = rng\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.CellIDGenerator.generate_cell_ids","title":"<code>generate_cell_ids(n_cells)</code>  <code>abstractmethod</code>","text":"<p>Method that generates random cell IDs.</p> <p>Parameters:</p> Name Type Description Default <code>n_cells</code> <code>int</code> <p>number of cell IDs to generate.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: list of cell IDs.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>@abstractmethod\ndef generate_cell_ids(self, n_cells: int) -&gt; list[str]:\n    \"\"\"Method that generates random cell IDs.\n\n    Args:\n        n_cells (int): number of cell IDs to generate.\n\n    Returns:\n        list[str]: list of cell IDs.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.CellIDGeneratorBuilder","title":"<code>CellIDGeneratorBuilder</code>","text":"<p>Type/method of cell ID generation enumeration class.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>class CellIDGeneratorBuilder:\n    \"\"\"\n    Type/method of cell ID generation enumeration class.\n    \"\"\"\n\n    RANDOM_CELL_ID = \"random_cell_id\"\n\n    CONSTRUCTORS = {RANDOM_CELL_ID: RandomCellIDGenerator}\n\n    @staticmethod\n    def build(constructor_key: str, rng: int | Random) -&gt; CellIDGenerator:\n        \"\"\"\n        Method that builds a CellIDGenerator.\n\n        Args:\n            constructor_key (str): Key of the constructor\n            rng (int | Random): either an integer to act as a seed for RNG, or an instantiated Random object.\n\n        Raises:\n            ValueError: If the given constructor_key is not supported\n\n        Returns:\n            CellIDGenerator: Class that generates random cell_id's\n        \"\"\"\n        try:\n            constructor = CellIDGeneratorBuilder.CONSTRUCTORS[constructor_key]\n        except KeyError as e:\n            raise ValueError(f\"Random cell ID generator: {constructor_key} is not supported\") from e\n\n        return constructor(rng)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.CellIDGeneratorBuilder.build","title":"<code>build(constructor_key, rng)</code>  <code>staticmethod</code>","text":"<p>Method that builds a CellIDGenerator.</p> <p>Parameters:</p> Name Type Description Default <code>constructor_key</code> <code>str</code> <p>Key of the constructor</p> required <code>rng</code> <code>int | Random</code> <p>either an integer to act as a seed for RNG, or an instantiated Random object.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the given constructor_key is not supported</p> <p>Returns:</p> Name Type Description <code>CellIDGenerator</code> <code>CellIDGenerator</code> <p>Class that generates random cell_id's</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>@staticmethod\ndef build(constructor_key: str, rng: int | Random) -&gt; CellIDGenerator:\n    \"\"\"\n    Method that builds a CellIDGenerator.\n\n    Args:\n        constructor_key (str): Key of the constructor\n        rng (int | Random): either an integer to act as a seed for RNG, or an instantiated Random object.\n\n    Raises:\n        ValueError: If the given constructor_key is not supported\n\n    Returns:\n        CellIDGenerator: Class that generates random cell_id's\n    \"\"\"\n    try:\n        constructor = CellIDGeneratorBuilder.CONSTRUCTORS[constructor_key]\n    except KeyError as e:\n        raise ValueError(f\"Random cell ID generator: {constructor_key} is not supported\") from e\n\n    return constructor(rng)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.RandomCellIDGenerator","title":"<code>RandomCellIDGenerator</code>","text":"<p>               Bases: <code>CellIDGenerator</code></p> <p>Class that generates completely random cell IDs. Inherits from the AbstractCellIDGenerator class.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>class RandomCellIDGenerator(CellIDGenerator):\n    \"\"\"\n    Class that generates completely random cell IDs. Inherits from the AbstractCellIDGenerator class.\n    \"\"\"\n\n    def generate_cell_ids(self, n_cells: int) -&gt; list[str]:\n        \"\"\"Generate UNIQUE random cell IDs with no logic behind it, i.e. not following CGI/eCGI standards.\n        The resuling cell IDs are 14- or 15-digit strings.\n\n        Args:\n            n_cells (int): number of cell IDs to generate.\n\n        Returns:\n            list[str]: list of cell IDs.\n        \"\"\"\n        return list(map(str, self.rng.sample(range(10_000_000_000_000, 999_999_999_999_999), n_cells)))\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.RandomCellIDGenerator.generate_cell_ids","title":"<code>generate_cell_ids(n_cells)</code>","text":"<p>Generate UNIQUE random cell IDs with no logic behind it, i.e. not following CGI/eCGI standards. The resuling cell IDs are 14- or 15-digit strings.</p> <p>Parameters:</p> Name Type Description Default <code>n_cells</code> <code>int</code> <p>number of cell IDs to generate.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: list of cell IDs.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>def generate_cell_ids(self, n_cells: int) -&gt; list[str]:\n    \"\"\"Generate UNIQUE random cell IDs with no logic behind it, i.e. not following CGI/eCGI standards.\n    The resuling cell IDs are 14- or 15-digit strings.\n\n    Args:\n        n_cells (int): number of cell IDs to generate.\n\n    Returns:\n        list[str]: list of cell IDs.\n    \"\"\"\n    return list(map(str, self.rng.sample(range(10_000_000_000_000, 999_999_999_999_999), n_cells)))\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.SyntheticNetwork","title":"<code>SyntheticNetwork</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class that generates the synthetic network topology data. It inherits from the Component abstract class.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>class SyntheticNetwork(Component):\n    \"\"\"\n    Class that generates the synthetic network topology data. It inherits from the Component abstract class.\n    \"\"\"\n\n    COMPONENT_ID = \"SyntheticNetwork\"\n\n    def __init__(self, general_config_path: str, component_config_path: str):\n        super().__init__(general_config_path=general_config_path, component_config_path=component_config_path)\n        self.seed = self.config.getint(self.COMPONENT_ID, \"seed\")\n        self.rng = Random(self.seed)\n        self.n_cells = self.config.getint(self.COMPONENT_ID, \"n_cells\")\n        self.cell_type_options = (\n            self.config.get(self.COMPONENT_ID, \"cell_type_options\").strip().replace(\" \", \"\").split(\",\")\n        )\n        self.tech = [\"5G\", \"LTE\", \"UMTS\", \"GSM\"]\n        self.latitude_min = self.config.getfloat(self.COMPONENT_ID, \"latitude_min\")\n        self.latitude_max = self.config.getfloat(self.COMPONENT_ID, \"latitude_max\")\n        self.longitude_min = self.config.getfloat(self.COMPONENT_ID, \"longitude_min\")\n        self.longitude_max = self.config.getfloat(self.COMPONENT_ID, \"longitude_max\")\n        self.altitude_min = self.config.getfloat(self.COMPONENT_ID, \"altitude_min\")\n        self.altitude_max = self.config.getfloat(self.COMPONENT_ID, \"altitude_max\")\n        self.antenna_height_max = self.config.getfloat(self.COMPONENT_ID, \"antenna_height_max\")\n        self.power_min = self.config.getfloat(self.COMPONENT_ID, \"power_min\")\n        self.power_max = self.config.getfloat(self.COMPONENT_ID, \"power_max\")\n        self.range_min = self.config.getfloat(self.COMPONENT_ID, \"range_min\")\n        self.range_max = self.config.getfloat(self.COMPONENT_ID, \"range_max\")\n        self.frequency_min = self.config.getfloat(self.COMPONENT_ID, \"frequency_min\")\n        self.frequency_max = self.config.getfloat(self.COMPONENT_ID, \"frequency_max\")\n\n        self.timestamp_format = self.config.get(self.COMPONENT_ID, \"timestamp_format\")\n        self.earliest_valid_date_start = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"earliest_valid_date_start\"), self.timestamp_format\n        )\n        self.latest_valid_date_end = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"latest_valid_date_end\"), self.timestamp_format\n        )\n\n        self.date_format = self.config.get(self.COMPONENT_ID, \"date_format\")\n\n        self.starting_date = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"starting_date\"), self.date_format\n        ).date()\n        self.ending_date = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"ending_date\"), self.date_format\n        ).date()\n\n        self.date_range = [\n            (self.starting_date + datetime.timedelta(days=dd))\n            for dd in range((self.ending_date - self.starting_date).days + 1)\n        ]\n\n        # Cell generation object\n        cell_id_generation_type = self.config.get(self.COMPONENT_ID, \"cell_id_generation_type\")\n\n        self.cell_id_generator = CellIDGeneratorBuilder.build(cell_id_generation_type, self.seed)\n\n        self.no_optional_fields_probability = self.config.getfloat(self.COMPONENT_ID, \"no_optional_fields_probability\")\n        self.mandatory_null_probability = self.config.getfloat(self.COMPONENT_ID, \"mandatory_null_probability\")\n        self.out_of_bounds_values_probability = self.config.getfloat(\n            self.COMPONENT_ID, \"out_of_bounds_values_probability\"\n        )\n        self.erroneous_values_probability = self.config.getfloat(self.COMPONENT_ID, \"erroneous_values_probability\")\n\n    def initalize_data_objects(self):\n        output_network_data_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"network_data_bronze\")\n        bronze_network = BronzeNetworkDataObject(\n            self.spark, output_network_data_path, partition_columns=[ColNames.year, ColNames.month, ColNames.day]\n        )\n        self.output_data_objects = {bronze_network.ID: bronze_network}\n\n    def read(self):\n        pass  # No input datasets are used in this component\n\n    def transform(self):\n        spark = self.spark\n\n        # Create Spark DataFrame with all valid cells and all optional fields\n\n        cells_df = spark.createDataFrame(self.clean_cells_generator(), schema=BronzeNetworkDataObject.SCHEMA)\n\n        # With certain probability, set ALL optional fields of a row to null.\n        # Fixing F.rand(seed=self.seed) will generate the same random column for every column, so\n        # it could be optimized to be generated only once\n        # If random optional fields should be set to zero (not all at the same time), use seed = self.seed + i(col_name)\n\n        for col_name in BronzeNetworkDataObject.OPTIONAL_COLUMNS:\n            cells_df = cells_df.withColumn(\n                col_name,\n                F.when(F.rand(seed=self.seed) &lt; self.no_optional_fields_probability, None).otherwise(F.col(col_name)),\n            )\n\n        cells_df = self.generate_errors(cells_df)\n\n        self.output_data_objects[BronzeNetworkDataObject.ID].df = cells_df\n\n    def clean_cells_generator(self):\n        \"\"\"Method that generates valid and fully complete (mandatory and optional) cell physical attibutes.\n\n        An underlying set of cells are created, covering the config-specified date interval.\n        Then, for each cell and date,\n        the valid_date_start and valid_date_end fields, marking the time interval in which the cell is operational, are\n        comparted with the date:\n            a) If  date &lt; valid_date_start, the cell-date row will not appear.\n            b) If valid_date_start &lt;= date &lt; valid_date_end, the cell-date row will appear and\n                the valid_date_end will be null, as the cell was currently operational\n            c) If valid_date_end &lt;= date, the cell-date row will appear and\n                the valid_date_end will NOT be null, marking the past, now known, time interval of operation.\n\n        Yields:\n            (\n                cell_id (str),\n                latitude (float),\n                longitude (float),\n                altitudes (float),\n                antenna_height (float),\n                directionality (int),\n                azimuth_angle (float | None),\n                elevation_angle (float),\n                hor_beam_width (float),\n                ver_beam_width (float),\n                power (float),\n                range (float),\n                frequency (int),\n                technology (str),\n                valid_date_start (str),\n                valid_date_end (str | None)\n                cell_type (str),\n                year (int),\n                month (int),\n                day (int)\n            )\n        \"\"\"\n        # MANDATORY FIELDS\n        cell_ids = self.cell_id_generator.generate_cell_ids(self.n_cells)\n\n        latitudes = [self.rng.uniform(self.latitude_min, self.latitude_max) for _ in range(self.n_cells)]\n        longitudes = [self.rng.uniform(self.longitude_min, self.longitude_max) for _ in range(self.n_cells)]\n\n        # OPTIONAL FIELDS\n        altitudes = [self.rng.uniform(self.altitude_min, self.altitude_max) for _ in range(self.n_cells)]\n\n        # antenna height always positive\n        antenna_heights = [self.rng.uniform(0, self.antenna_height_max) for _ in range(self.n_cells)]\n\n        # Directionality: 0 or 1\n        directionalities = [self.rng.choice([0, 1]) for _ in range(self.n_cells)]\n\n        def random_azimuth_angle(directionality):\n            if directionality == 0:\n                return None\n            else:\n                return self.rng.uniform(0, 360)\n\n        # Azimuth angle: None if directionality is 0, float in [0, 360] if directionality is 1\n        azimuth_angles = [random_azimuth_angle(direc) for direc in directionalities]\n\n        # Eleveation angle: in [-90, 90]\n        elevation_angles = [self.rng.uniform(-90, 90) for _ in range(self.n_cells)]\n\n        # Horizontal/Vertical beam width: float in [0, 360]\n        hor_beam_widths = [self.rng.uniform(0, 360) for _ in range(self.n_cells)]\n        ver_beam_widths = [self.rng.uniform(0, 360) for _ in range(self.n_cells)]\n\n        # Power, float in specified range (unit: watts, W)\n        powers = [self.rng.uniform(self.power_min, self.power_max) for _ in range(self.n_cells)]\n\n        # Range, float in specified range (unit: metres, m)\n        ranges = [self.rng.uniform(self.range_min, self.range_max) for _ in range(self.n_cells)]\n\n        # Frequency: int in specifed range (unit: MHz)\n        frequencies = [self.rng.randint(self.frequency_min, self.frequency_max) for _ in range(self.n_cells)]\n\n        # Technology: str\n        technologies = [self.rng.choice(self.tech) for _ in range(self.n_cells)]\n\n        # # Valid start date, should be in the timestamp interval provided via config file\n        # span_seconds = (self.latest_valid_date_end - self.earliest_valid_date_start).total_seconds()\n\n        # # Start date will be some random nb of seconds after the earliest valid date start\n        # valid_date_start_dts = [\n        #     self.earliest_valid_date_start + datetime.timedelta(seconds=self.rng.uniform(0, span_seconds))\n        #     for _ in range(self.n_cells)\n        # ]\n\n        # # Remaining seconds from the valid date starts to the ending date\n        # remaining_seconds = [\n        #     (self.latest_valid_date_end - start_dt).total_seconds() for start_dt in valid_date_start_dts\n        # ]\n        # # Choose valid date ends ALWAYS after the valid date start\n        # # Minimum of two seconds, as valid date end is excluded from the time window,\n        # # so cell will be valid for at least 1 second\n        # valid_date_end_dts = [\n        #     (start_dt + datetime.timedelta(seconds=self.rng.uniform(2, rem_secs)))\n        #     for (start_dt, rem_secs) in zip(valid_date_start_dts, remaining_seconds)\n        # ]\n\n        valid_date_start_dts = [self.earliest_valid_date_start for _ in range(self.n_cells)]\n        valid_date_end_dts = [self.latest_valid_date_end for _ in range(self.n_cells)]\n\n        def check_operational(_curr_date: datetime.date, _end_datetime: datetime.datetime):\n            if _curr_date &lt; _end_datetime.date():  # still operational, return None\n                return None\n            else:\n                return _end_datetime.strftime(self.timestamp_format)\n\n        # Cell type: str in option list\n        cell_types = [self.rng.choice(self.cell_type_options) for _ in range(self.n_cells)]\n\n        for date in self.date_range:\n            for i in range(self.n_cells):\n                # Assume we do not have info about future cells\n                if valid_date_start_dts[i].date() &lt;= date:\n                    yield (\n                        cell_ids[i],\n                        latitudes[i],\n                        longitudes[i],\n                        altitudes[i],\n                        antenna_heights[i],\n                        directionalities[i],\n                        azimuth_angles[i],\n                        elevation_angles[i],\n                        hor_beam_widths[i],\n                        ver_beam_widths[i],\n                        powers[i],\n                        ranges[i],\n                        frequencies[i],\n                        technologies[i],\n                        valid_date_start_dts[i].strftime(self.timestamp_format),\n                        check_operational(date, valid_date_end_dts[i]),  # null if still operational today\n                        cell_types[i],\n                        date.year,\n                        date.month,\n                        date.day,\n                    )\n\n    def generate_errors(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Function handling the generation of out-of-bounds, null and erroneous values in different columns\n        according to the config-specified probabilities\n\n        Args:\n            df (DataFrame): clean DataFrame\n\n        Returns:\n            DataFrame: DataFrame after the generation of different invalid or null values\n        \"\"\"\n\n        if self.out_of_bounds_values_probability &gt; 0:\n            df = self.generate_out_of_bounds_values(df)\n\n        if self.mandatory_null_probability &gt; 0:\n            df = self.generate_nulls_in_mandatory_columns(df)\n\n        if self.erroneous_values_probability &gt; 0:\n            df = self.generate_erroneous_values(df)\n\n        return df\n\n    def generate_nulls_in_mandatory_columns(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Generates null values in the mandatory fields based on probabilities form config\n\n        Args:\n            df (DataFrame): synthetic dataframe\n\n        Returns:\n            DataFrame: synthetic dataframe with nulls in some mandatory fields\n        \"\"\"\n        # Use different seed for each column\n        for i, col_name in enumerate(BronzeNetworkDataObject.MANDATORY_COLUMNS):\n            df = df.withColumn(\n                col_name,\n                F.when(F.rand(seed=self.seed + i + 1) &lt; self.mandatory_null_probability, None).otherwise(\n                    F.col(col_name)\n                ),\n            )\n\n        return df\n\n    def generate_out_of_bounds_values(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Function that generates out-of-bounds values for the appropriate columns of the data object\n\n        Args:\n            df (DataFrame): cell dataframe with in-bound values\n\n        Returns:\n            DataFrame: cell dataframe with some out-of-bounds values\n        \"\"\"\n        # (2 * F.round(F.rand(seed=self.seed-1)) - 1) -&gt; random column of 1s and -1s\n        df = df.withColumn(\n            ColNames.latitude,\n            F.when(\n                F.rand(seed=self.seed - 1) &lt; self.out_of_bounds_values_probability,\n                F.col(ColNames.latitude) + (2 * F.round(F.rand(seed=self.seed - 1)) - 1) * 90,\n            )\n            .otherwise(F.col(ColNames.latitude))\n            .cast(FloatType()),  # the operation turns it into DoubleType, so cast it to Float\n        )\n\n        df = df.withColumn(\n            ColNames.longitude,\n            F.when(\n                F.rand(seed=self.seed - 2) &lt; self.out_of_bounds_values_probability,\n                F.col(ColNames.longitude) + (2 * F.round(F.rand(seed=self.seed - 2)) - 1) * 180,\n            )\n            .otherwise(F.col(ColNames.longitude))\n            .cast(FloatType()),\n        )\n\n        # antenna height, non positive\n        df = df.withColumn(\n            ColNames.antenna_height,\n            F.when(F.rand(seed=self.seed - 3) &lt; self.out_of_bounds_values_probability, -F.col(ColNames.antenna_height))\n            .otherwise(F.col(ColNames.antenna_height))\n            .cast(FloatType()),\n        )\n\n        # directionality: int different from 0 or 1. Just add a static 5 to the value\n        df = df.withColumn(\n            ColNames.directionality,\n            F.when(\n                F.rand(seed=self.seed - 4) &lt; self.out_of_bounds_values_probability, F.col(ColNames.directionality) + 5\n            )\n            .otherwise(F.col(ColNames.directionality))\n            .cast(IntegerType()),\n        )\n\n        # azimuth angle: outside of [0, 360]\n        df = df.withColumn(\n            ColNames.azimuth_angle,\n            F.when(\n                F.rand(seed=self.seed - 5) &lt; self.out_of_bounds_values_probability,\n                F.col(ColNames.azimuth_angle) + (2 * F.round(F.rand(seed=self.seed - 5)) - 1) * 360,\n            )\n            .otherwise(F.col(ColNames.azimuth_angle))\n            .cast(FloatType()),\n        )\n\n        # elevation_angle: outside of [-90, 90]\n        df = df.withColumn(\n            ColNames.elevation_angle,\n            F.when(\n                F.rand(seed=self.seed - 6) &lt; self.out_of_bounds_values_probability,\n                F.col(ColNames.elevation_angle) + (2 * F.round(F.rand(seed=self.seed - 6)) - 1) * 180,\n            )\n            .otherwise(F.col(ColNames.elevation_angle))\n            .cast(FloatType()),\n        )\n\n        # horizontal_beam_width: outside of [0, 360]\n        df = df.withColumn(\n            ColNames.horizontal_beam_width,\n            F.when(\n                F.rand(seed=self.seed - 7) &lt; self.out_of_bounds_values_probability,\n                F.col(ColNames.horizontal_beam_width) + (2 * F.round(F.rand(seed=self.seed - 7)) - 1) * 360,\n            )\n            .otherwise(F.col(ColNames.horizontal_beam_width))\n            .cast(FloatType()),\n        )\n\n        # vertical_beam_width: outside of [0, 360]\n        df = df.withColumn(\n            ColNames.vertical_beam_width,\n            F.when(\n                F.rand(seed=self.seed - 8) &lt; self.out_of_bounds_values_probability,\n                F.col(ColNames.vertical_beam_width) + (2 * F.round(F.rand(seed=self.seed - 8)) - 1) * 360,\n            )\n            .otherwise(F.col(ColNames.vertical_beam_width))\n            .cast(FloatType()),\n        )\n\n        # power: non positive value\n        df = df.withColumn(\n            ColNames.power,\n            F.when(F.rand(seed=self.seed - 9) &lt; self.out_of_bounds_values_probability, -F.col(ColNames.power))\n            .otherwise(F.col(ColNames.power))\n            .cast(FloatType()),\n        )\n\n        # range: non positive value\n        df = df.withColumn(\n            ColNames.range,\n            F.when(F.rand(seed=self.seed - 10) &lt; self.out_of_bounds_values_probability, -F.col(ColNames.range))\n            .otherwise(F.col(ColNames.range))\n            .cast(FloatType()),\n        )\n\n        # frequency: non positive vallue\n        df = df.withColumn(\n            ColNames.frequency,\n            F.when(F.rand(seed=self.seed - 11) &lt; self.out_of_bounds_values_probability, -F.col(ColNames.frequency))\n            .otherwise(F.col(ColNames.frequency))\n            .cast(IntegerType()),\n        )\n        return df\n\n    def generate_erroneous_values(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Function that generates erroneous values in the cell_id, valid_date_start and valid_date_end columns\n\n        Args:\n            df (DataFrame): DataFrame before the generation of erroneous values\n\n        Returns:\n            DataFrame: DataFrame with erroneous values\n        \"\"\"\n        # Erroneous cells: for now, a string not of 14 or 15 digits\n        df = df.withColumn(\n            ColNames.cell_id,\n            F.when(\n                F.rand(seed=self.seed * 1000) &lt; self.erroneous_values_probability,\n                (\n                    F.when(\n                        F.rand(seed=self.seed * 2000) &gt; 0.5,\n                        F.concat(\n                            F.col(ColNames.cell_id), F.substring(F.col(ColNames.cell_id), 3, 3)\n                        ),  # 17 or 18 digits\n                    ).otherwise(\n                        F.substring(F.col(ColNames.cell_id), 3, 10)  # 10 digits\n                    )\n                ),\n            )\n            .otherwise(F.col(ColNames.cell_id))\n            .cast(LongType())\n            .cast(StringType()),\n        )\n\n        # Dates\n        df_as_is, df_swap, df_wrong = df.randomSplit(\n            weights=[\n                1 - self.erroneous_values_probability,\n                self.erroneous_values_probability / 2,\n                self.erroneous_values_probability / 2,\n            ],\n            seed=self.seed,\n        )\n        # For some columns, swap valid_date_start and valid_date_end\n        df_swap = df_swap.withColumns(\n            {ColNames.valid_date_start: ColNames.valid_date_end, ColNames.valid_date_end: ColNames.valid_date_start}\n        )\n\n        chars_to_change = [\"0\", \"T\", \":\", \"/\", \"2\"]\n        changed_chars = [\"0\", \"T\", \":\", \"/\", \"2\"]\n        # Now, for dates as well, make the timestamp format incorrect\n\n        self.rng.shuffle(changed_chars)\n        df_wrong = df_wrong.withColumn(\n            ColNames.valid_date_start,\n            F.translate(F.col(ColNames.valid_date_start), \"\".join(chars_to_change), \"\".join(changed_chars)),\n        )\n\n        self.rng.shuffle(changed_chars)\n        df_wrong = df_wrong.withColumn(\n            ColNames.valid_date_end,\n            F.translate(F.col(ColNames.valid_date_end), \"\".join(chars_to_change), \"\".join(changed_chars)),\n        )\n\n        df = df_as_is.union(df_swap).union(df_wrong)\n\n        return df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.SyntheticNetwork.clean_cells_generator","title":"<code>clean_cells_generator()</code>","text":"<p>Method that generates valid and fully complete (mandatory and optional) cell physical attibutes.</p> <p>An underlying set of cells are created, covering the config-specified date interval. Then, for each cell and date, the valid_date_start and valid_date_end fields, marking the time interval in which the cell is operational, are comparted with the date:     a) If  date &lt; valid_date_start, the cell-date row will not appear.     b) If valid_date_start &lt;= date &lt; valid_date_end, the cell-date row will appear and         the valid_date_end will be null, as the cell was currently operational     c) If valid_date_end &lt;= date, the cell-date row will appear and         the valid_date_end will NOT be null, marking the past, now known, time interval of operation.</p> <p>Yields:</p> Type Description <p>( cell_id (str), latitude (float), longitude (float), altitudes (float), antenna_height (float), directionality (int), azimuth_angle (float | None), elevation_angle (float), hor_beam_width (float), ver_beam_width (float), power (float), range (float), frequency (int), technology (str), valid_date_start (str), valid_date_end (str | None) cell_type (str), year (int), month (int), day (int)</p> <p>)</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>def clean_cells_generator(self):\n    \"\"\"Method that generates valid and fully complete (mandatory and optional) cell physical attibutes.\n\n    An underlying set of cells are created, covering the config-specified date interval.\n    Then, for each cell and date,\n    the valid_date_start and valid_date_end fields, marking the time interval in which the cell is operational, are\n    comparted with the date:\n        a) If  date &lt; valid_date_start, the cell-date row will not appear.\n        b) If valid_date_start &lt;= date &lt; valid_date_end, the cell-date row will appear and\n            the valid_date_end will be null, as the cell was currently operational\n        c) If valid_date_end &lt;= date, the cell-date row will appear and\n            the valid_date_end will NOT be null, marking the past, now known, time interval of operation.\n\n    Yields:\n        (\n            cell_id (str),\n            latitude (float),\n            longitude (float),\n            altitudes (float),\n            antenna_height (float),\n            directionality (int),\n            azimuth_angle (float | None),\n            elevation_angle (float),\n            hor_beam_width (float),\n            ver_beam_width (float),\n            power (float),\n            range (float),\n            frequency (int),\n            technology (str),\n            valid_date_start (str),\n            valid_date_end (str | None)\n            cell_type (str),\n            year (int),\n            month (int),\n            day (int)\n        )\n    \"\"\"\n    # MANDATORY FIELDS\n    cell_ids = self.cell_id_generator.generate_cell_ids(self.n_cells)\n\n    latitudes = [self.rng.uniform(self.latitude_min, self.latitude_max) for _ in range(self.n_cells)]\n    longitudes = [self.rng.uniform(self.longitude_min, self.longitude_max) for _ in range(self.n_cells)]\n\n    # OPTIONAL FIELDS\n    altitudes = [self.rng.uniform(self.altitude_min, self.altitude_max) for _ in range(self.n_cells)]\n\n    # antenna height always positive\n    antenna_heights = [self.rng.uniform(0, self.antenna_height_max) for _ in range(self.n_cells)]\n\n    # Directionality: 0 or 1\n    directionalities = [self.rng.choice([0, 1]) for _ in range(self.n_cells)]\n\n    def random_azimuth_angle(directionality):\n        if directionality == 0:\n            return None\n        else:\n            return self.rng.uniform(0, 360)\n\n    # Azimuth angle: None if directionality is 0, float in [0, 360] if directionality is 1\n    azimuth_angles = [random_azimuth_angle(direc) for direc in directionalities]\n\n    # Eleveation angle: in [-90, 90]\n    elevation_angles = [self.rng.uniform(-90, 90) for _ in range(self.n_cells)]\n\n    # Horizontal/Vertical beam width: float in [0, 360]\n    hor_beam_widths = [self.rng.uniform(0, 360) for _ in range(self.n_cells)]\n    ver_beam_widths = [self.rng.uniform(0, 360) for _ in range(self.n_cells)]\n\n    # Power, float in specified range (unit: watts, W)\n    powers = [self.rng.uniform(self.power_min, self.power_max) for _ in range(self.n_cells)]\n\n    # Range, float in specified range (unit: metres, m)\n    ranges = [self.rng.uniform(self.range_min, self.range_max) for _ in range(self.n_cells)]\n\n    # Frequency: int in specifed range (unit: MHz)\n    frequencies = [self.rng.randint(self.frequency_min, self.frequency_max) for _ in range(self.n_cells)]\n\n    # Technology: str\n    technologies = [self.rng.choice(self.tech) for _ in range(self.n_cells)]\n\n    # # Valid start date, should be in the timestamp interval provided via config file\n    # span_seconds = (self.latest_valid_date_end - self.earliest_valid_date_start).total_seconds()\n\n    # # Start date will be some random nb of seconds after the earliest valid date start\n    # valid_date_start_dts = [\n    #     self.earliest_valid_date_start + datetime.timedelta(seconds=self.rng.uniform(0, span_seconds))\n    #     for _ in range(self.n_cells)\n    # ]\n\n    # # Remaining seconds from the valid date starts to the ending date\n    # remaining_seconds = [\n    #     (self.latest_valid_date_end - start_dt).total_seconds() for start_dt in valid_date_start_dts\n    # ]\n    # # Choose valid date ends ALWAYS after the valid date start\n    # # Minimum of two seconds, as valid date end is excluded from the time window,\n    # # so cell will be valid for at least 1 second\n    # valid_date_end_dts = [\n    #     (start_dt + datetime.timedelta(seconds=self.rng.uniform(2, rem_secs)))\n    #     for (start_dt, rem_secs) in zip(valid_date_start_dts, remaining_seconds)\n    # ]\n\n    valid_date_start_dts = [self.earliest_valid_date_start for _ in range(self.n_cells)]\n    valid_date_end_dts = [self.latest_valid_date_end for _ in range(self.n_cells)]\n\n    def check_operational(_curr_date: datetime.date, _end_datetime: datetime.datetime):\n        if _curr_date &lt; _end_datetime.date():  # still operational, return None\n            return None\n        else:\n            return _end_datetime.strftime(self.timestamp_format)\n\n    # Cell type: str in option list\n    cell_types = [self.rng.choice(self.cell_type_options) for _ in range(self.n_cells)]\n\n    for date in self.date_range:\n        for i in range(self.n_cells):\n            # Assume we do not have info about future cells\n            if valid_date_start_dts[i].date() &lt;= date:\n                yield (\n                    cell_ids[i],\n                    latitudes[i],\n                    longitudes[i],\n                    altitudes[i],\n                    antenna_heights[i],\n                    directionalities[i],\n                    azimuth_angles[i],\n                    elevation_angles[i],\n                    hor_beam_widths[i],\n                    ver_beam_widths[i],\n                    powers[i],\n                    ranges[i],\n                    frequencies[i],\n                    technologies[i],\n                    valid_date_start_dts[i].strftime(self.timestamp_format),\n                    check_operational(date, valid_date_end_dts[i]),  # null if still operational today\n                    cell_types[i],\n                    date.year,\n                    date.month,\n                    date.day,\n                )\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.SyntheticNetwork.generate_erroneous_values","title":"<code>generate_erroneous_values(df)</code>","text":"<p>Function that generates erroneous values in the cell_id, valid_date_start and valid_date_end columns</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame before the generation of erroneous values</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with erroneous values</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>def generate_erroneous_values(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Function that generates erroneous values in the cell_id, valid_date_start and valid_date_end columns\n\n    Args:\n        df (DataFrame): DataFrame before the generation of erroneous values\n\n    Returns:\n        DataFrame: DataFrame with erroneous values\n    \"\"\"\n    # Erroneous cells: for now, a string not of 14 or 15 digits\n    df = df.withColumn(\n        ColNames.cell_id,\n        F.when(\n            F.rand(seed=self.seed * 1000) &lt; self.erroneous_values_probability,\n            (\n                F.when(\n                    F.rand(seed=self.seed * 2000) &gt; 0.5,\n                    F.concat(\n                        F.col(ColNames.cell_id), F.substring(F.col(ColNames.cell_id), 3, 3)\n                    ),  # 17 or 18 digits\n                ).otherwise(\n                    F.substring(F.col(ColNames.cell_id), 3, 10)  # 10 digits\n                )\n            ),\n        )\n        .otherwise(F.col(ColNames.cell_id))\n        .cast(LongType())\n        .cast(StringType()),\n    )\n\n    # Dates\n    df_as_is, df_swap, df_wrong = df.randomSplit(\n        weights=[\n            1 - self.erroneous_values_probability,\n            self.erroneous_values_probability / 2,\n            self.erroneous_values_probability / 2,\n        ],\n        seed=self.seed,\n    )\n    # For some columns, swap valid_date_start and valid_date_end\n    df_swap = df_swap.withColumns(\n        {ColNames.valid_date_start: ColNames.valid_date_end, ColNames.valid_date_end: ColNames.valid_date_start}\n    )\n\n    chars_to_change = [\"0\", \"T\", \":\", \"/\", \"2\"]\n    changed_chars = [\"0\", \"T\", \":\", \"/\", \"2\"]\n    # Now, for dates as well, make the timestamp format incorrect\n\n    self.rng.shuffle(changed_chars)\n    df_wrong = df_wrong.withColumn(\n        ColNames.valid_date_start,\n        F.translate(F.col(ColNames.valid_date_start), \"\".join(chars_to_change), \"\".join(changed_chars)),\n    )\n\n    self.rng.shuffle(changed_chars)\n    df_wrong = df_wrong.withColumn(\n        ColNames.valid_date_end,\n        F.translate(F.col(ColNames.valid_date_end), \"\".join(chars_to_change), \"\".join(changed_chars)),\n    )\n\n    df = df_as_is.union(df_swap).union(df_wrong)\n\n    return df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.SyntheticNetwork.generate_errors","title":"<code>generate_errors(df)</code>","text":"<p>Function handling the generation of out-of-bounds, null and erroneous values in different columns according to the config-specified probabilities</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>clean DataFrame</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame after the generation of different invalid or null values</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>def generate_errors(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Function handling the generation of out-of-bounds, null and erroneous values in different columns\n    according to the config-specified probabilities\n\n    Args:\n        df (DataFrame): clean DataFrame\n\n    Returns:\n        DataFrame: DataFrame after the generation of different invalid or null values\n    \"\"\"\n\n    if self.out_of_bounds_values_probability &gt; 0:\n        df = self.generate_out_of_bounds_values(df)\n\n    if self.mandatory_null_probability &gt; 0:\n        df = self.generate_nulls_in_mandatory_columns(df)\n\n    if self.erroneous_values_probability &gt; 0:\n        df = self.generate_erroneous_values(df)\n\n    return df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.SyntheticNetwork.generate_nulls_in_mandatory_columns","title":"<code>generate_nulls_in_mandatory_columns(df)</code>","text":"<p>Generates null values in the mandatory fields based on probabilities form config</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>synthetic dataframe</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>synthetic dataframe with nulls in some mandatory fields</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>def generate_nulls_in_mandatory_columns(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Generates null values in the mandatory fields based on probabilities form config\n\n    Args:\n        df (DataFrame): synthetic dataframe\n\n    Returns:\n        DataFrame: synthetic dataframe with nulls in some mandatory fields\n    \"\"\"\n    # Use different seed for each column\n    for i, col_name in enumerate(BronzeNetworkDataObject.MANDATORY_COLUMNS):\n        df = df.withColumn(\n            col_name,\n            F.when(F.rand(seed=self.seed + i + 1) &lt; self.mandatory_null_probability, None).otherwise(\n                F.col(col_name)\n            ),\n        )\n\n    return df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.SyntheticNetwork.generate_out_of_bounds_values","title":"<code>generate_out_of_bounds_values(df)</code>","text":"<p>Function that generates out-of-bounds values for the appropriate columns of the data object</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>cell dataframe with in-bound values</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>cell dataframe with some out-of-bounds values</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>def generate_out_of_bounds_values(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Function that generates out-of-bounds values for the appropriate columns of the data object\n\n    Args:\n        df (DataFrame): cell dataframe with in-bound values\n\n    Returns:\n        DataFrame: cell dataframe with some out-of-bounds values\n    \"\"\"\n    # (2 * F.round(F.rand(seed=self.seed-1)) - 1) -&gt; random column of 1s and -1s\n    df = df.withColumn(\n        ColNames.latitude,\n        F.when(\n            F.rand(seed=self.seed - 1) &lt; self.out_of_bounds_values_probability,\n            F.col(ColNames.latitude) + (2 * F.round(F.rand(seed=self.seed - 1)) - 1) * 90,\n        )\n        .otherwise(F.col(ColNames.latitude))\n        .cast(FloatType()),  # the operation turns it into DoubleType, so cast it to Float\n    )\n\n    df = df.withColumn(\n        ColNames.longitude,\n        F.when(\n            F.rand(seed=self.seed - 2) &lt; self.out_of_bounds_values_probability,\n            F.col(ColNames.longitude) + (2 * F.round(F.rand(seed=self.seed - 2)) - 1) * 180,\n        )\n        .otherwise(F.col(ColNames.longitude))\n        .cast(FloatType()),\n    )\n\n    # antenna height, non positive\n    df = df.withColumn(\n        ColNames.antenna_height,\n        F.when(F.rand(seed=self.seed - 3) &lt; self.out_of_bounds_values_probability, -F.col(ColNames.antenna_height))\n        .otherwise(F.col(ColNames.antenna_height))\n        .cast(FloatType()),\n    )\n\n    # directionality: int different from 0 or 1. Just add a static 5 to the value\n    df = df.withColumn(\n        ColNames.directionality,\n        F.when(\n            F.rand(seed=self.seed - 4) &lt; self.out_of_bounds_values_probability, F.col(ColNames.directionality) + 5\n        )\n        .otherwise(F.col(ColNames.directionality))\n        .cast(IntegerType()),\n    )\n\n    # azimuth angle: outside of [0, 360]\n    df = df.withColumn(\n        ColNames.azimuth_angle,\n        F.when(\n            F.rand(seed=self.seed - 5) &lt; self.out_of_bounds_values_probability,\n            F.col(ColNames.azimuth_angle) + (2 * F.round(F.rand(seed=self.seed - 5)) - 1) * 360,\n        )\n        .otherwise(F.col(ColNames.azimuth_angle))\n        .cast(FloatType()),\n    )\n\n    # elevation_angle: outside of [-90, 90]\n    df = df.withColumn(\n        ColNames.elevation_angle,\n        F.when(\n            F.rand(seed=self.seed - 6) &lt; self.out_of_bounds_values_probability,\n            F.col(ColNames.elevation_angle) + (2 * F.round(F.rand(seed=self.seed - 6)) - 1) * 180,\n        )\n        .otherwise(F.col(ColNames.elevation_angle))\n        .cast(FloatType()),\n    )\n\n    # horizontal_beam_width: outside of [0, 360]\n    df = df.withColumn(\n        ColNames.horizontal_beam_width,\n        F.when(\n            F.rand(seed=self.seed - 7) &lt; self.out_of_bounds_values_probability,\n            F.col(ColNames.horizontal_beam_width) + (2 * F.round(F.rand(seed=self.seed - 7)) - 1) * 360,\n        )\n        .otherwise(F.col(ColNames.horizontal_beam_width))\n        .cast(FloatType()),\n    )\n\n    # vertical_beam_width: outside of [0, 360]\n    df = df.withColumn(\n        ColNames.vertical_beam_width,\n        F.when(\n            F.rand(seed=self.seed - 8) &lt; self.out_of_bounds_values_probability,\n            F.col(ColNames.vertical_beam_width) + (2 * F.round(F.rand(seed=self.seed - 8)) - 1) * 360,\n        )\n        .otherwise(F.col(ColNames.vertical_beam_width))\n        .cast(FloatType()),\n    )\n\n    # power: non positive value\n    df = df.withColumn(\n        ColNames.power,\n        F.when(F.rand(seed=self.seed - 9) &lt; self.out_of_bounds_values_probability, -F.col(ColNames.power))\n        .otherwise(F.col(ColNames.power))\n        .cast(FloatType()),\n    )\n\n    # range: non positive value\n    df = df.withColumn(\n        ColNames.range,\n        F.when(F.rand(seed=self.seed - 10) &lt; self.out_of_bounds_values_probability, -F.col(ColNames.range))\n        .otherwise(F.col(ColNames.range))\n        .cast(FloatType()),\n    )\n\n    # frequency: non positive vallue\n    df = df.withColumn(\n        ColNames.frequency,\n        F.when(F.rand(seed=self.seed - 11) &lt; self.out_of_bounds_values_probability, -F.col(ColNames.frequency))\n        .otherwise(F.col(ColNames.frequency))\n        .cast(IntegerType()),\n    )\n    return df\n</code></pre>"},{"location":"reference/components/quality/","title":"quality","text":""},{"location":"reference/components/quality/event_quality_warnings/","title":"event_quality_warnings","text":""},{"location":"reference/components/quality/event_quality_warnings/event_quality_warnings/","title":"event_quality_warnings","text":""},{"location":"reference/components/quality/event_quality_warnings/event_quality_warnings/#components.quality.event_quality_warnings.event_quality_warnings.EventQualityWarnings","title":"<code>EventQualityWarnings</code>","text":"<p>               Bases: <code>Component</code></p> <p>Component that calculates Quality Warnings from the Quality Metrics generated by the Event Cleaning component</p> Source code in <code>multimno/components/quality/event_quality_warnings/event_quality_warnings.py</code> <pre><code>class EventQualityWarnings(Component):\n    \"\"\"\n    Component that calculates Quality Warnings from the Quality Metrics generated by the Event Cleaning component\n    \"\"\"\n\n    COMPONENT_ID = \"EventQualityWarnings\"\n\n    dict_convert_to_num_days = {\"week\": 7, \"month\": 30}\n    # dict to store info regarding error type\n    # first element - corresponding encoding of ErrorTypes class\n    # second element - naming constants for coresponding measure definitions, conditions, and warning texts\n    dict_error_type_info = {\n        \"missing_value\": [ErrorTypes.missing_value, \"Missing value rate\"],\n        \"not_right_syntactic_format\": [\n            ErrorTypes.not_right_syntactic_format,\n            \"Wrong type/format rate\",\n        ],\n        \"out_of_admissible_values\": [\n            ErrorTypes.out_of_admissible_values,\n            \"Out of range rate\",\n        ],\n        \"no_location\": [ErrorTypes.no_location, \"No location error rate\"],\n        \"no_domain\": [ErrorTypes.no_location, \"No domain error rate\"],\n        \"out_of_bounding_box\": [\n            ErrorTypes.out_of_bounding_box,\n            \"Out of bounding box error rate\",\n        ],\n        \"same_location_duplicate\": [\n            ErrorTypes.same_location_duplicate,\n            \"Deduplication same locations rate\",\n        ],\n    }\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        print(self.config)\n        self.lookback_period = self.config.get(EventQualityWarnings.COMPONENT_ID, \"lookback_period\")\n        self.lookback_period_in_days = self.dict_convert_to_num_days[self.lookback_period]\n\n        self.data_period_start = self.config.get(EventQualityWarnings.COMPONENT_ID, \"data_period_start\")\n        self.data_period_end = self.config.get(EventQualityWarnings.COMPONENT_ID, \"data_period_end\")\n\n        self.qw_dfs_log = []\n        self.qw_dfs_plots = []\n\n        # FOR SYNTACTIC QUALITY WARNINGS\n        self.do_size_raw_data_qw = self.config.getboolean(\n            EventQualityWarnings.COMPONENT_ID, \"do_size_raw_data_qw\", fallback=False\n        )\n        self.do_size_clean_data_qw = self.config.getboolean(\n            EventQualityWarnings.COMPONENT_ID, \"do_size_clean_data_qw\", fallback=False\n        )\n\n        self.data_size_tresholds = self.config.geteval(\n            EventQualityWarnings.COMPONENT_ID, \"data_size_tresholds\", fallback=None\n        )\n\n        self.do_error_rate_by_date_qw = self.config.getboolean(\n            EventQualityWarnings.COMPONENT_ID,\n            \"do_error_rate_by_date_qw\",\n            fallback=False,\n        )\n\n        self.do_error_rate_by_date_and_cell_qw = self.config.getboolean(\n            EventQualityWarnings.COMPONENT_ID,\n            \"do_error_rate_by_date_and_cell_qw\",\n            fallback=False,\n        )\n\n        self.do_error_rate_by_date_and_user_qw = self.config.getboolean(\n            EventQualityWarnings.COMPONENT_ID,\n            \"do_error_rate_by_date_and_user_qw\",\n            fallback=False,\n        )\n\n        self.do_error_rate_by_date_and_cell_user_qw = self.config.getboolean(\n            EventQualityWarnings.COMPONENT_ID,\n            \"do_error_rate_by_date_and_cell_user_qw\",\n            fallback=False,\n        )\n\n        self.error_rate_tresholds = self.config.geteval(\n            EventQualityWarnings.COMPONENT_ID, \"error_rate_tresholds\", fallback=None\n        )\n\n        self.error_type_qw_checks = self.config.geteval(\n            EventQualityWarnings.COMPONENT_ID, \"error_type_qw_checks\", fallback=None\n        )\n\n        self.missing_value_thresholds = self.config.geteval(\n            EventQualityWarnings.COMPONENT_ID, \"missing_value_thresholds\", fallback=None\n        )\n\n        self.out_of_admissible_values_thresholds = self.config.geteval(\n            EventQualityWarnings.COMPONENT_ID,\n            \"out_of_admissible_values_thresholds\",\n            fallback=None,\n        )\n\n        self.not_right_syntactic_format_thresholds = self.config.geteval(\n            EventQualityWarnings.COMPONENT_ID,\n            \"not_right_syntactic_format_thresholds\",\n            fallback=None,\n        )\n\n        self.no_location_thresholds = self.config.geteval(\n            EventQualityWarnings.COMPONENT_ID, \"no_location_thresholds\", fallback=None\n        )\n\n        self.no_domain_thresholds = self.config.geteval(\n            EventQualityWarnings.COMPONENT_ID, \"no_domain_thresholds\", fallback=None\n        )\n\n        self.out_of_bounding_box_thresholds = self.config.geteval(\n            EventQualityWarnings.COMPONENT_ID,\n            \"out_of_bounding_box_thresholds\",\n            fallback=None,\n        )\n        # FOR DEDUPLICATION QUALITY WARNINGS\n        self.deduplication_same_location_thresholds = self.config.geteval(\n            EventQualityWarnings.COMPONENT_ID,\n            \"deduplication_same_location_thresholds\",\n            fallback=None,\n        )\n\n    def initalize_data_objects(self):\n        self.input_qm_data_objects = {}\n        self.output_qw_data_objects = {}\n        self.clear_destination_directory = self.config.getboolean(\n            EventQualityWarnings.COMPONENT_ID, \"clear_destination_directory\"\n        )\n\n        self.input_qm_by_column_path_key = self.config.get(\n            EventQualityWarnings.COMPONENT_ID, \"input_qm_by_column_path_key\"\n        )\n        self.input_qm_freq_distr_path_key = self.config.get(\n            EventQualityWarnings.COMPONENT_ID, \"input_qm_freq_distr_path_key\"\n        )\n        self.output_qw_log_table_path_key = self.config.get(\n            EventQualityWarnings.COMPONENT_ID, \"output_qw_log_table_path_key\"\n        )\n        self.output_qw_for_plots_path_key = self.config.get(\n            EventQualityWarnings.COMPONENT_ID,\n            \"output_qw_for_plots_path_key\",\n            fallback=None,\n        )\n\n        self.input_qm_by_column_path = self.config.get(CONFIG_SILVER_PATHS_KEY, self.input_qm_by_column_path_key)\n        if check_if_data_path_exists(self.spark, self.input_qm_by_column_path):\n            self.input_qm_data_objects[SilverEventDataSyntacticQualityMetricsByColumn.ID] = (\n                SilverEventDataSyntacticQualityMetricsByColumn(self.spark, self.input_qm_by_column_path)\n            )\n        else:\n            self.logger.warning(\"Wrong path for Quality Metrics By Column, terminating component execution\")\n            raise ValueError(\"Invalid path for Quality Metrics By Column\")\n\n        self.input_qm_freq_distr_path = self.config.get(CONFIG_SILVER_PATHS_KEY, self.input_qm_freq_distr_path_key)\n        if check_if_data_path_exists(self.spark, self.input_qm_freq_distr_path):\n            self.input_qm_data_objects[SilverEventDataSyntacticQualityMetricsFrequencyDistribution.ID] = (\n                SilverEventDataSyntacticQualityMetricsFrequencyDistribution(self.spark, self.input_qm_freq_distr_path)\n            )\n        else:\n            self.logger.warning(\n                \"Wrong path for Quality Metrics Frequency Distribution, terminating component execution\"\n            )\n            raise ValueError(\"Invalid path for Quality Metrics Frequency Distribution\")\n\n        self.output_qw_log_table_path = self.config.get(CONFIG_SILVER_PATHS_KEY, self.output_qw_log_table_path_key)\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, self.output_qw_log_table_path)\n        check_or_create_data_path(self.spark, self.output_qw_log_table_path)\n        self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsLogTable.ID] = (\n            SilverEventDataSyntacticQualityWarningsLogTable(self.spark, self.output_qw_log_table_path)\n        )\n        # no plots information is intended for EventDeduplicationQualityWarnings\n        if self.output_qw_for_plots_path_key is not None:\n            self.output_qw_for_plots_path = self.config.get(CONFIG_SILVER_PATHS_KEY, self.output_qw_for_plots_path_key)\n            if self.clear_destination_directory:\n                delete_file_or_folder(self.spark, self.output_qw_for_plots_path)\n            check_or_create_data_path(self.spark, self.output_qw_for_plots_path)\n            self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsForPlots.ID] = (\n                SilverEventDataSyntacticQualityWarningsForPlots(self.spark, self.output_qw_for_plots_path)\n            )\n\n    def read(self):\n        self.input_qm_data_objects[SilverEventDataSyntacticQualityMetricsByColumn.ID].read()\n        self.input_qm_data_objects[SilverEventDataSyntacticQualityMetricsFrequencyDistribution.ID].read()\n\n    def write(self):\n        self.save_quality_warnings_log_table(self.qw_dfs_log)\n        if self.output_qw_for_plots_path_key is not None:\n            self.save_quality_warnings_for_plots(self.qw_dfs_plots)\n\n    def execute(self):\n        self.logger.info(f\"Starting {EventQualityWarnings.COMPONENT_ID}...\")\n        self.read()\n        self.transform()  # Transforms the input_df\n        self.write()\n        self.logger.info(f\"Finished {EventQualityWarnings.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {EventQualityWarnings.COMPONENT_ID}\")\n        # Read QA Metrics of EventCleaning Component, the period of intrest is\n        #  [data_period_start-lookback_period_in_days, data_period_end]\n        # Since QualityWarnings are calculated based on prior data\n        # TODO: deal with cases when df_qa_by_column, df_qa_freq_distribution do not have data for\n        # whole defined period\n        # TODO: dynamically define/check the possible research period of QW  based on data period\n        #  of df_qa_by_column and df_qa_freq_distribution\n        # TODO: implement min_period conf param which is minimal amount of days with previous data to\n        #  have in order to calculate QW (the case for first days in reaserch period)\n        sdate = pd.to_datetime(self.data_period_start) - pd.Timedelta(days=self.lookback_period_in_days)\n        edate = pd.to_datetime(self.data_period_end)\n\n        df_qa_by_column = self.input_qm_data_objects[SilverEventDataSyntacticQualityMetricsByColumn.ID].df.where(\n            psf.col(ColNames.date).between(sdate, edate)\n        )\n\n        df_qa_freq_distribution = self.input_qm_data_objects[\n            SilverEventDataSyntacticQualityMetricsFrequencyDistribution.ID\n        ].df.where(psf.col(ColNames.date).between(sdate, edate))\n\n        df_qa_by_column = df_qa_by_column.cache()\n        # TODO: maybe makes sense to first sum init and final freq, cache and parse this aggregation\n        # to further QW functions\n        df_qa_freq_distribution = df_qa_freq_distribution.cache()\n\n        if self.do_size_raw_data_qw:\n            # for raw data size QW compute warnings and also retrive data to plot distribution of initial frequency\n            df_raw_data_qw, df_raw_plots = self.data_size_qw(\n                df_qa_freq_distribution,\n                self.lookback_period_in_days,\n                self.data_size_tresholds[\"SIZE_RAW_DATA_BYDATE_VARIABILITY\"],\n                self.data_size_tresholds[\"SIZE_RAW_DATA_BYDATE_ABS_VALUE_LOWER_LIMIT\"],\n                self.data_size_tresholds[\"SIZE_RAW_DATA_BYDATE_ABS_VALUE_UPPER_LIMIT\"],\n                type_of_data=\"raw\",\n            )\n            self.qw_dfs_log.append(df_raw_data_qw)\n            self.qw_dfs_plots.append(df_raw_plots)\n\n        if self.do_size_clean_data_qw:\n            # for clean data size QW compute warnings and also retrive data to plot distribution of total frequency\n            df_clean_data_qw, df_clean_plots = self.data_size_qw(\n                df_qa_freq_distribution,\n                self.lookback_period_in_days,\n                self.data_size_tresholds[\"SIZE_CLEAN_DATA_BYDATE_VARIABILITY\"],\n                self.data_size_tresholds[\"SIZE_CLEAN_DATA_BYDATE_ABS_VALUE_LOWER_LIMIT\"],\n                self.data_size_tresholds[\"SIZE_CLEAN_DATA_BYDATE_ABS_VALUE_UPPER_LIMIT\"],\n                type_of_data=\"clean\",\n            )\n            self.qw_dfs_log.append(df_clean_data_qw)\n            self.qw_dfs_plots.append(df_clean_plots)\n\n        if self.do_error_rate_by_date_qw:\n            # for error rate by date QW compute warnings and also retrive data to\n            # plot distribution of error rate by date\n            df_error_rate_by_date_qw, df_error_rate_plots = self.error_rate_qw(\n                df_qa_freq_distribution,\n                self.lookback_period_in_days,\n                [ColNames.date],\n                self.error_rate_tresholds[\"TOTAL_ERROR_RATE_BYDATE_OVER_AVERAGE\"],\n                self.error_rate_tresholds[\"TOTAL_ERROR_RATE_BYDATE_VARIABILITY\"],\n                self.error_rate_tresholds[\"TOTAL_ERROR_RATE_BYDATE_ABS_VALUE_UPPER_LIMIT\"],\n                save_data_for_plots=True,\n            )\n\n            self.qw_dfs_log.append(df_error_rate_by_date_qw)\n            self.qw_dfs_plots.append(df_error_rate_plots)\n        # The current aggrement is that for next error rates (more granular ones) do not store any data for plots\n        # although it could be done with save_data_for_plots=True\n        # TODO: should we consider error rate of null user_id or/and null cell_id in QW computation\n        if self.do_error_rate_by_date_and_cell_qw:\n            df_error_rate_by_date_and_cell_qw, _ = self.error_rate_qw(\n                df_qa_freq_distribution,\n                self.lookback_period_in_days,\n                [ColNames.date, ColNames.cell_id],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYCELL_OVER_AVERAGE\"],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYCELL_VARIABILITY\"],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYCELL_ABS_VALUE_UPPER_LIMIT\"],\n            )\n\n            self.qw_dfs_log.append(df_error_rate_by_date_and_cell_qw)\n\n        if self.do_error_rate_by_date_and_user_qw:\n            df_error_rate_by_date_and_user_qw, _ = self.error_rate_qw(\n                df_qa_freq_distribution,\n                self.lookback_period_in_days,\n                [ColNames.date, ColNames.user_id],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYUSER_OVER_AVERAGE\"],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYUSER_VARIABILITY\"],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYUSER_ABS_VALUE_UPPER_LIMIT\"],\n            )\n\n            self.qw_dfs_log.append(df_error_rate_by_date_and_user_qw)\n\n        if self.do_error_rate_by_date_and_cell_user_qw:\n            df_error_rate_by_date_and_cell_user_qw, _ = self.error_rate_qw(\n                df_qa_freq_distribution,\n                self.lookback_period_in_days,\n                [ColNames.date, ColNames.cell_id, ColNames.user_id],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYCELL_USER_OVER_AVERAGE\"],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYCELL_USER_VARIABILITY\"],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYCELL_USER_ABS_VALUE_UPPER_LIMIT\"],\n            )\n\n            self.qw_dfs_log.append(df_error_rate_by_date_and_cell_user_qw)\n\n        # Two previous types of QW were using df_qa_freq_distribution only\n        # Now calculate error rate for different error types like missing_value, wrong type\n        # based on two QA metrics - df_qa_by_column and df_qa_freq_distribution\n        # error_type_qw_checks - dict('error_type':[relevant columns])\n        for error_type, field_names in self.error_type_qw_checks.items():\n            if field_names == []:\n                self.logger.info(f\"No field name(s) were specified for error type: {error_type}\")\n            else:\n                # if you have a new error_type and thus new error_type_thresholds entry in config\n                # make sure to add it to class atributes and to this block with elif statement\n                if error_type == \"missing_value\":\n                    error_type_thresholds = self.missing_value_thresholds\n                elif error_type == \"out_of_admissible_values\":\n                    error_type_thresholds = self.out_of_admissible_values_thresholds\n                elif error_type == \"not_right_syntactic_format\":\n                    error_type_thresholds = self.not_right_syntactic_format_thresholds\n                elif error_type == \"no_location\":\n                    error_type_thresholds = self.no_location_thresholds\n                elif error_type == \"no_domain\":\n                    error_type_thresholds = self.no_domain_thresholds\n                elif error_type == \"out_of_bounding_box\":\n                    error_type_thresholds = self.out_of_bounding_box_thresholds\n                elif error_type == \"same_location_duplicate\":\n                    error_type_thresholds = self.deduplication_same_location_thresholds\n                else:\n                    self.logger.warning(\n                        f\"Unexpected error type in error_type_qw_checks config param\"\n                        f\": {error_type}, skipping calculation for this qw\"\n                    )\n                    continue\n\n            for field_name in field_names:\n                if field_name in error_type_thresholds.keys():\n                    error_type_qw, _ = self.error_type_rate_qw(\n                        df_qa_by_column,\n                        df_qa_freq_distribution,\n                        field_name,\n                        error_type,\n                        self.lookback_period_in_days,\n                        *list(error_type_thresholds[field_name].values()),\n                    )\n                    self.qw_dfs_log.append(error_type_qw)\n                else:\n                    self.logger.warning(\n                        f\"No thresholds were specified for field {field_name} of {error_type} error_type\"\n                    )\n\n        self.spark.catalog.clearCache()\n\n    def data_size_qw(\n        self,\n        df_freq_distribution: DataFrame,\n        lookback_period_in_days: int,\n        variablility: int | float,\n        lower_limit: int | float,\n        upper_limit: int | float,\n        type_of_data: str,\n        measure_definition_canva: str = f\"{MeasureDefinitions.size_data}\",\n        cond_warn_variability_canva: str = f\"{Conditions.size_data_variability}-{Warnings.size_data_variability}\",\n        cond_warn_upper_lower_canva: str = f\"{Conditions.size_data_upper_lower}-{Warnings.size_data_upper_lower}\",\n    ) -&gt; tuple[DataFrame]:\n        \"\"\"\n        A unified function to check both raw and clean data sizes, calculates four types of QWs:\n        LOWER_VARIABILITY - for each row using calculated mean and std compute lower variability limit\n            which is mean - SD*variability, check if  daily_value is lower tan limit\n        UPPER_VARIABILITY - for each row using calculated mean and std compute upper variability limit\n             which is mean + SD*variability, check if  daily_value exceeds limit\n        ABS_LOWER_LIMIT - check if daily_value is lower than absolute number lower_limit\n        ABS_UPPER_LIMIT - check if daily_value exceeds absolute number upper_limit\n        All four QWs depend on thresholds, in case if statement condition is met -&gt; store cond-warn-condition_value\n            information in array column,\n        some rows may have several QWs. In the end array column is exploded and cond-warn-consition_value information\n            is split into three corresponding columns.\n        The function returns almost ready dfs for SilverEventDataSyntacticQualityWarningsLogTable\n             and SilverEventDataSyntacticQualityWarningsForPlots DOs\n\n        Args:\n            df_freq_distribution (DataFrame): df with frequency data\n            lookback_period_in_days (int): lenght of lookback period in days\n            variablility (int | float): config param, the number of SD to define the upper and lower varibaility\n                limits: mean_size \u00b1 SD*variability, which daily_value should not exceed/be lower\n            lower_limit (int | float): absolute number which daily_value should not be lower\n            upper_limit (int | float): absolute number which daily_value can not exceed\n            type_of_data (str): which type of data raw or clean to check for QWs\n            measure_definition_canva (str): canva text to use for measure_definition column (see measure_definition.py)\n            cond_warn_variability_canva (str): canva text to use for lower_upper_variability cases\n                of data_size QWs (see conditions.py and warnings.py)\n            cond_warn_upper_lower_canva (str): canva text to use for lower_upper_limit cases\n                of data_size QWs (see conditions.py and warnings.py)\n\n        Returns:\n            tuple(DataFrame, DataFrame): a tuple, where first df\n                is used for warning log table, and the second df - for plots\n        \"\"\"\n        # based on type_of_data calculate either total daily initial freqeuncy or total daily final frequency\n        if type_of_data == \"raw\":\n            sum_column = ColNames.initial_frequency\n        else:\n            sum_column = ColNames.final_frequency\n        # fill in string canvases\n        measure_definition = measure_definition_canva.format(type_of_data=type_of_data)\n        cond_warn_variability = cond_warn_variability_canva.format(SD=variablility, type_of_data=type_of_data)\n        cond_warn_upper_lower = cond_warn_upper_lower_canva.format(\n            X=lower_limit, Y=upper_limit, type_of_data=type_of_data\n        )\n        # define lookback period\n        window = Window.orderBy(ColNames.date).rowsBetween(-lookback_period_in_days, -1)\n        # prepare data\n        # calculate mean and std over window, and based on them UCL - for UPPER_VARIABILITY check and LCL\n        #   - for LOWER_VARIABILITY check\n        # create empty array cond_warn_condition_value column to store information about qws\n        df_prep = (\n            df_freq_distribution.groupBy(ColNames.date)\n            .agg(psf.sum(sum_column).alias(ColNames.daily_value))\n            .withColumns(\n                {\n                    ColNames.average: psf.avg(ColNames.daily_value).over(window),\n                    \"std_freq\": psf.stddev(ColNames.daily_value).over(window),\n                    ColNames.UCL: psf.col(ColNames.average) + variablility * psf.col(\"std_freq\"),\n                    ColNames.LCL: psf.col(ColNames.average) - variablility * psf.col(\"std_freq\"),\n                    \"cond_warn_condition_value\": psf.array(),\n                }\n            )\n        )\n\n        df_prep = df_prep.cache()\n        # continue with QWs checks\n        # first filter data by period from [data_period_start-lookback_period_in_days, data_period_end]\n        # to [data_period_start, data_period_end]\n        # - a specified research period of QW\n        df_qw = df_prep.filter(psf.col(ColNames.date) &gt;= self.data_period_start)\n        # perform LOWER_VARIABILITY, UPPER_VARIABILITY, ABS_LOWER_LIMIT and ABS_UPPER_LIMIT checks\n        # if condition is met append information about condition-warning_text-condition_value as a string\n        # into array column 'cond_warn_condition_value'\n        df_qw = (\n            df_qw.withColumn(\n                \"cond_warn_condition_value\",\n                psf.when(\n                    (psf.col(ColNames.daily_value) &lt; psf.col(ColNames.LCL)),\n                    psf.array_append(\n                        psf.col(\"cond_warn_condition_value\"),\n                        psf.concat(\n                            psf.lit(f\"{cond_warn_variability}-\"),\n                            psf.col(ColNames.LCL).cast(\"string\"),\n                        ),\n                    ),\n                ).otherwise(psf.col(\"cond_warn_condition_value\")),\n            )\n            .withColumn(\n                \"cond_warn_condition_value\",\n                psf.when(\n                    (psf.col(ColNames.daily_value) &gt; psf.col(ColNames.UCL)),\n                    psf.array_append(\n                        psf.col(\"cond_warn_condition_value\"),\n                        psf.concat(\n                            psf.lit(f\"{cond_warn_variability}-\"),\n                            psf.col(ColNames.UCL).cast(\"string\"),\n                        ),\n                    ),\n                ).otherwise(psf.col(\"cond_warn_condition_value\")),\n            )\n            .withColumn(\n                \"cond_warn_condition_value\",\n                psf.when(\n                    (psf.col(ColNames.daily_value) &lt; psf.lit(lower_limit)),\n                    psf.array_append(\n                        psf.col(\"cond_warn_condition_value\"),\n                        psf.lit(f\"{cond_warn_upper_lower}-{str(lower_limit)}\"),\n                    ),\n                ).otherwise(psf.col(\"cond_warn_condition_value\")),\n            )\n            .withColumn(\n                \"cond_warn_condition_value\",\n                psf.when(\n                    (psf.col(ColNames.daily_value) &gt; psf.lit(upper_limit)),\n                    psf.array_append(\n                        psf.col(\"cond_warn_condition_value\"),\n                        psf.lit(f\"{cond_warn_upper_lower}-{str(upper_limit)}\"),\n                    ),\n                ).otherwise(psf.col(\"cond_warn_condition_value\")),\n            )\n        )\n        # explode array column 'cond_warn_condition_value'\n        df_qw = df_qw.withColumn(\n            \"cond_warn_condition_value\",\n            psf.explode(psf.col(\"cond_warn_condition_value\")),\n        )\n        # add some column constants\n        # split \"cond_warn_condition_value\" column into three: condition, wanring_text, conditon_value to\n        # match SilverEventDataSyntacticQualityWarningsLogTable.SCHEMA\n        # select only needed columns\n        df_qw = df_qw.withColumns(\n            {\n                ColNames.lookback_period: psf.lit(self.lookback_period),\n                ColNames.measure_definition: psf.lit(measure_definition),\n                ColNames.condition: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(0),\n                ColNames.warning_text: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(1),\n                ColNames.condition_value: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(2).cast(\"float\"),\n            }\n        ).select(self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsLogTable.ID].SCHEMA.fieldNames())\n\n        # save data for plots\n        # no filter by date because we need previous data of first days for plots\n        df_plots = df_prep.withColumns(\n            {\n                ColNames.lookback_period: psf.lit(self.lookback_period),\n                ColNames.type_of_qw: psf.lit(f\"{type_of_data}_data_size\"),\n            }\n        ).select(self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsForPlots.ID].SCHEMA.fieldNames())\n\n        return df_qw, df_plots\n\n    def error_rate_qw(\n        self,\n        df_freq_distribution: DataFrame,\n        lookback_period_in_days: int,\n        variables: list[str],\n        error_rate_over_average: int | float,\n        error_rate_upper_variability: int | float,\n        error_rate_upper_limit: int | float,\n        error_rate_measure_definition_canva: str = f\"{MeasureDefinitions.error_rate}\",\n        error_rate_cond_warn_over_average_canva: str = f\"{Conditions.error_rate_over_average}-{Warnings.error_rate_over_average}\",\n        error_rate_cond_warn_upper_variability_canva: str = f\"{Conditions.error_rate_upper_variability}-{Warnings.error_rate_upper_variability}\",\n        error_rate_cond_warn_upper_limit_canva: str = f\"{Conditions.error_rate_upper_limit}-{Warnings.error_rate_upper_limit}\",\n        save_data_for_plots: bool = False,\n    ) -&gt; tuple[DataFrame | None]:\n        \"\"\"\n        Prepare data for error rate calculation. First fill in different string canvas,\n            then define window of aggregation, and calculate error_rate over the window on follwoing formula:\n            (Total initial frequency - Total final frequency) / Total initial frequency*100.\n            Parse preprocessed input to self.rate_common_qw function which calculates three types\n                of QWs: OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT\n\n        Args:\n            df_freq_distribution (DataFrame): df with frequency data.\n            lookback_period_in_days (int): number of days prior to date of interest.\n            variables (list[str]): list of column names by which error rate is calculated, kind of granularity level\n            error_rate_over_average (int | float): config param, specifies the upper limit which a daily value\n                can not exceed its corresponding mean error rate\n            error_rate_upper_variability (int | float): config param, the number of SD to define the upper\n                varibaility limit: mean_rate + SD*error_rate_upper_variability, which error rate can't exceed\n            error_rate_upper_limit (int | float): absolute number which error rate can not exceed\n            error_rate_measure_definition_canva (str): canva text to use for measure_definition\n                column (see measure_definition.py)\n            error_rate_cond_warn_over_average_canva (str): canva text to use for over_average cases of\n                error_rate QWs (see conditions.py and warnings.py)\n            error_rate_cond_warn_upper_variability_canva (str): canva text to use for upper_variability cases of\n                error_rate QWs (see conditions.py and warnings.py)\n            error_rate_cond_warn_upper_limit_canva (str): canva text to use for upper_limit cases of error_rate\n                QWs (see conditions.py and warnings.py)\n            save_data_for_plots (bool): boolean, decide whether to store error rate and its corresponding average\n                and upper variability limit for plots, default False\n        Returns:\n            tuple(DataFrame | None): a tuple, where first df is used for warning log table,\n                and the second df - for plots (could be also None)\n        \"\"\"\n        # fill in all string comnstants with relevant information\n        # based on variables, error_rate_over_average, error_rate_upper_variability, error_rate_upper_limit args\n        error_rate_measure_definition = error_rate_measure_definition_canva.format(variables=\"&amp;\".join(variables))\n\n        error_rate_cond_warn_over_average = error_rate_cond_warn_over_average_canva.format(\n            variables=\"&amp;\".join(variables), X=error_rate_over_average\n        )\n        error_rate_cond_warn_upper_variability = error_rate_cond_warn_upper_variability_canva.format(\n            variables=\"&amp;\".join(variables), SD=error_rate_upper_variability\n        )\n        error_rate_cond_warn_upper_limit = error_rate_cond_warn_upper_limit_canva.format(\n            variables=\"&amp;\".join(variables), X=error_rate_upper_limit\n        )\n        # qws of error rate by date is calculated based on previous days\n        if variables == [ColNames.date]:\n            window = Window.orderBy(ColNames.date).rowsBetween(-lookback_period_in_days, -1)\n        else:\n            # qws of error rate by date and other colum(s) is calculated over all \"data points\" of the same date\n            window = Window.partitionBy(ColNames.date)\n        # calculate error rate, a.k.a daily_value\n        df_qw = (\n            df_freq_distribution.groupBy(*variables)\n            .agg(\n                psf.sum(ColNames.initial_frequency).alias(\"sum_init_freq\"),\n                psf.sum(ColNames.final_frequency).alias(\"sum_final_freq\"),\n            )\n            .withColumn(\n                ColNames.daily_value,\n                (psf.col(\"sum_init_freq\") - psf.col(\"sum_final_freq\")) / psf.col(\"sum_init_freq\") * 100,\n            )\n        )\n        # using self.rate_common_qw funciton calculate three types of QWs\n        #   -&gt; OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT\n        # return a tuple of (df_log, df_plots | None)\n        qw_result = self.rate_common_qw(\n            df_qw,\n            window,\n            error_rate_upper_variability,\n            error_rate_over_average,\n            error_rate_upper_limit,\n            error_rate_measure_definition,\n            error_rate_cond_warn_upper_variability,\n            error_rate_cond_warn_over_average,\n            error_rate_cond_warn_upper_limit,\n            save_data_for_plots,\n        )\n\n        return qw_result\n\n    def error_type_rate_qw(\n        self,\n        df_qa_by_column: DataFrame,\n        df_freq_distribution: DataFrame,\n        field_name: str | None,\n        error_type: str,\n        lookback_period_in_days: int,\n        error_type_rate_over_average: int | float,\n        error_type_rate_upper_variability: int | float,\n        error_type_rate_upper_limit: int | float,\n        error_type_rate_measure_definition_canva: str = f\"{MeasureDefinitions.error_type_rate}\",\n        error_type_rate_cond_warn_over_average_canva: str = f\"{Conditions.error_type_rate_over_average}-{Warnings.error_type_rate_over_average}\",\n        error_type_rate_cond_warn_upper_variability_canva: str = f\"{Conditions.error_type_rate_upper_variability}-{Warnings.error_type_rate_upper_variability}\",\n        error_type_rate_cond_warn_upper_limit_canva: str = f\"{Conditions.error_type_rate_upper_limit}-{Warnings.error_type_rate_upper_limit}\",\n    ) -&gt; tuple[DataFrame | None]:\n        \"\"\"\n        Prepare data for error type rate calculation. First fill in different string canvas, then based\n            on field name and error type calculate their corresponding error rate using formula:\n            number of errors of this error_type&amp;field_name combo / Total initial frequency *100 (BY DATE).\n            Parse preprocessed input along with window (which is a lookback period)\n            to self.rate_common_qw function which calculates three types of QWs:\n            OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT\n\n\n        Args:\n            df_qa_by_column (DataFrame): df with qa by column data.\n            df_freq_distribution (DataFrame): df with frequency data.\n            field_name (str | None): config param, the name of column of which to check error_type.\n            error_type (str): config param, the name of error type.\n            lookback_period_in_days (int): number of days prior to date of intrest.\n            error_type_rate_over_average (int | float): config param, specifies the upper limit over which daily\n                value can not exceed its corresponding mean error rate.\n            error_type_rate_upper_variability (int | float): config param, the number of SD to define the upper\n                varibaility limit: mean_rate + SD*error_type_rate_upper_variability, which daily value can not exceed\n            error_type_rate_upper_limit (int | float): absolute number which daily value can not exceed\n            error_type_rate_measure_definition_canva (str): canva text to use for measure_definition\n                column (see measure_definition.py)\n            error_type_rate_cond_warn_over_average_canva (str): canva text to use for over_average cases of\n                error_type_rate QWs (see conditions.py and warnings.py)\n            error_type_rate_cond_warn_upper_variability_canva (str): canva text to use for upper_variability\n                cases of error_type_rate QWs (see conditions.py and warnings.py)\n            error_type_rate_cond_warn_upper_limit_canva (str): canva text to use for upper_limit cases of\n                error_type_rate QWs (see conditions.py and warnings.py)\n        Returns:\n            tuple(DataFrame | None): a tuple, where first df is used for warning log table,\n                and the second df - for plots, but since save_data_for_plots always False, output=None\n        \"\"\"\n        # based on error_type, error_type_rate_over_average, error_type_rate_upper_variability,\n        #  error_type_rate_upper_limit\n        # fill in string canvases\n        colname_error_type, error_type_qw_name = self.dict_error_type_info[error_type]\n\n        error_type_rate_measure_definition = error_type_rate_measure_definition_canva.format(\n            error_type_name=error_type_qw_name, field_name=str(field_name)\n        )\n\n        error_type_rate_cond_warn_over_average = error_type_rate_cond_warn_over_average_canva.format(\n            error_type_name=error_type_qw_name,\n            field_name=str(field_name),\n            X=error_type_rate_over_average,\n        )\n        error_type_rate_cond_warn_upper_variability = error_type_rate_cond_warn_upper_variability_canva.format(\n            error_type_name=error_type_qw_name,\n            field_name=str(field_name),\n            SD=error_type_rate_upper_variability,\n        )\n        error_type_rate_cond_warn_upper_limit = error_type_rate_cond_warn_upper_limit_canva.format(\n            error_type_name=error_type_qw_name,\n            field_name=str(field_name),\n            X=error_type_rate_upper_limit,\n        )\n        # for error_type that have more then one or applicable columns\n        # filter df_qa_by_column by field_name and error_type\n        if field_name is not None:\n            df_qa_by_column = df_qa_by_column.filter(\n                (psf.col(ColNames.variable) == field_name) &amp; (psf.col(ColNames.type_of_error) == colname_error_type)\n            ).select(ColNames.date, ColNames.value)\n        else:\n            # for error_types which technically do not belong specifically to one of event\n            # columns filter only by error_type (e.g. no_location error_type)\n            df_qa_by_column = df_qa_by_column.filter(psf.col(ColNames.type_of_error) == colname_error_type).select(\n                ColNames.date, ColNames.value\n            )\n        # calculate total daily initial frequency\n        df_freq_distribution = (\n            df_freq_distribution.groupby(ColNames.date)\n            .agg(psf.sum(ColNames.initial_frequency).alias(\"sum_init_freq\"))\n            .select(ColNames.date, \"sum_init_freq\")\n        )\n        # for each date combine two type of information number of errors and total daily initial frequency\n        df_combined = df_qa_by_column.join(df_freq_distribution, on=ColNames.date, how=\"inner\")\n        # for each date calculate error_type_rate, a.k.a daily_value\n        df_temp = df_combined.withColumn(\n            ColNames.daily_value,\n            (psf.col(ColNames.value) / psf.col(\"sum_init_freq\")) * 100,\n        )\n\n        # qws will be caluclated based on previous days\n        window = Window.orderBy(ColNames.date).rowsBetween(-lookback_period_in_days, -1)\n        # using self.rate_common_qw funciton calculate three types of QWs -&gt;\n        # OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT\n        # return a tuple of (df_log, None)\n        qw_result = self.rate_common_qw(\n            df_temp,\n            window,\n            error_type_rate_upper_variability,\n            error_type_rate_over_average,\n            error_type_rate_upper_limit,\n            error_type_rate_measure_definition,\n            error_type_rate_cond_warn_upper_variability,\n            error_type_rate_cond_warn_over_average,\n            error_type_rate_cond_warn_upper_limit,\n        )\n        return qw_result\n\n    def rate_common_qw(\n        self,\n        df_temp: DataFrame,\n        window: Window,\n        rate_upper_variability: int | float,\n        rate_over_average: int | float,\n        rate_upper_limit: int | float,\n        measure_definition: str,\n        cond_warn_upper_variability: str,\n        cond_warn_over_average: str,\n        cond_warn_upper_limit: str,\n        save_data_for_plots: bool = False,\n    ) -&gt; tuple[DataFrame | None]:\n        \"\"\"\n        Take input df with \"daily_value\" column, and calculates three types of QWs:\n        OVER_AVERAGE - for each row first based on specified window take mean of values, and then check if\n            daily_value exceeds mean by more than rate_over_average\n        UPPER_VARIABILITY - for each row using already calculated mean compute upper variability limit which is\n            mean + SD*rate_upper_variability, check if  daily_value exceeds it\n        ABS_UPPER_LIMIT - check if daily_value exceeds absolute number rate_upper_limit\n        All three QWs depend on specified thresholds, if daily_value exceeds one of calculated values it will\n            store cond-warn-condition_value information in array column,\n        some rows may have several QWs. In the end array column is exploded and cond-warn-condition_value\n            information is split into three corresponding columns.\n        The function returns almost ready df for SilverEventDataSyntacticQualityWarningsLogTable DO, and based\n            on save_data_for_plots arg returns either almost ready data for plots or None\n\n        Args:\n            df_temp (DataFrame): temprory data that must have daily_value column to\n                be used in further QW calculations\n            window (Window): a window within which perform aggregation\n            rate_upper_variability (int|float): config param, specifies the upper limit over which daily value\n                 can not exceed its corresponding mean error rate\n            rate_over_average (int|float): config param, the number of SD to define the upper varibaility\n                 limit: mean_rate + SD*error_rate_upper_variability, which daily value can not exceed\n            rate_upper_limit (int|float): absolute number which daily value can not exceed\n            measure_definition (str): canva text to use for measure_definition column (see measure_definition.py)\n            cond_warn_over_average (str): canva text to use for over_average cases (see conditions.py and warnings.py)\n            cond_warn_upper_variability (str): canva text to use for\n                upper_variability cases (see conditions.py and warnings.py)\n            cond_warn_upper_limit (str): canva text to use for upper_limit cases (see conditions.py and warnings.py)\n            save_data_for_plots (bool): boolean, decide whether to store daily_value and its corresponding average\n                and upper variability limit for plots. Defaults to False.\n        Returns:\n             tuple(DataFrame | None): a tuple, where first df is used for\n                warning log table, and the second df - for plots\n        \"\"\"\n        # prepare data\n        # calculate mean and std over window, and based on them UCL - for UPPER_VARIABILITY check\n        # ratio_perc - for OVER_AVERAGE check\n        # create empty array cond_warn_condition_value column to store inromation about qws\n        df_prep = df_temp.withColumns(\n            {\n                ColNames.average: psf.avg(ColNames.daily_value).over(window),\n                \"std_rate\": psf.stddev(ColNames.daily_value).over(window),\n                \"ratio_perc\": (psf.col(ColNames.daily_value) / psf.col(ColNames.average)) * 100,\n                ColNames.UCL: psf.col(ColNames.average) + rate_upper_variability * psf.col(\"std_rate\"),\n                \"cond_warn_condition_value\": psf.array(),\n            }\n        )\n        # if save_data_for_plots=True, add some new columns with constant values\n        # and select only applicable to SilverEventDataSyntacticQualityWarningsForPlots.SCHEMA\n        # else - return None\n        if save_data_for_plots:\n            df_prep = df_prep.cache()\n            df_plots = df_prep.withColumns(\n                {\n                    ColNames.lookback_period: psf.lit(self.lookback_period),\n                    ColNames.type_of_qw: psf.lit(\"error_rate\"),\n                    ColNames.LCL: psf.lit(None).cast(\"float\"),\n                }\n            ).select(\n                self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsForPlots.ID].SCHEMA.fieldNames()\n            )\n        else:\n            df_plots = None\n\n        # continue with QWs checks\n        # first filter temp data by period from [data_period_start-lookback_period_in_days, data_period_end]\n        #  to [data_period_start, data_period_end]\n        # filter is aaplied after plot block because the first days of research period needs previous data to plot\n        df_qw = df_prep.filter(psf.col(ColNames.date) &gt;= self.data_period_start)\n        # perform UPPER_VARIABILITY, OVER_AVERAGE, and ABS_UPPER_LIMIT checks\n        # if condition is met store information about condition-warning_text-condition_value as a string into\n        # array column 'cond_warn_condition_value'\n        df_qw = (\n            df_qw.withColumn(\n                \"cond_warn_condition_value\",\n                psf.when(\n                    psf.col(ColNames.daily_value) &gt; psf.col(ColNames.UCL),\n                    psf.array_append(\n                        psf.col(\"cond_warn_condition_value\"),\n                        psf.concat(\n                            psf.lit(f\"{cond_warn_upper_variability}-\"),\n                            psf.col(ColNames.UCL).cast(\"string\"),\n                        ),\n                    ),\n                ).otherwise(psf.col(\"cond_warn_condition_value\")),\n            )\n            .withColumn(\n                \"cond_warn_condition_value\",\n                psf.when(\n                    psf.col(\"ratio_perc\") &gt; psf.lit(100 + rate_over_average),\n                    psf.array_append(\n                        psf.col(\"cond_warn_condition_value\"),\n                        psf.lit(f\"{cond_warn_over_average}-{str(rate_over_average)}\"),\n                    ),\n                ).otherwise(psf.col(\"cond_warn_condition_value\")),\n            )\n            .withColumn(\n                \"cond_warn_condition_value\",\n                psf.when(\n                    psf.col(ColNames.daily_value) &gt; psf.lit(rate_upper_limit),\n                    psf.array_append(\n                        psf.col(\"cond_warn_condition_value\"),\n                        psf.lit(f\"{cond_warn_upper_limit}-{str(rate_upper_limit)}\"),\n                    ),\n                ).otherwise(psf.col(\"cond_warn_condition_value\")),\n            )\n        )\n        # explode array column 'cond_warn_condition_value'\n        df_qw = df_qw.withColumn(\n            \"cond_warn_condition_value\",\n            psf.explode(psf.col(\"cond_warn_condition_value\")),\n        )\n        # add some column constants\n        # split \"cond_warn_condition_value\" column into three: condition, wanring_text, conditon_value to match\n        # SilverEventDataSyntacticQualityWarningsLogTable.SCHEMA\n        # select only needed columns\n        df_qw = df_qw.withColumns(\n            {\n                ColNames.lookback_period: psf.lit(self.lookback_period),\n                ColNames.measure_definition: psf.lit(measure_definition),\n                ColNames.condition: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(0),\n                ColNames.warning_text: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(1),\n                ColNames.condition_value: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(2).cast(\"float\"),\n            }\n        ).select(self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsLogTable.ID].SCHEMA.fieldNames())\n\n        return (df_qw, df_plots)\n\n    def save_quality_warnings_output(\n        self,\n        dfs_qw: list[DataFrame | None],\n        output_do: SilverEventDataSyntacticQualityWarningsLogTable | SilverEventDataSyntacticQualityWarningsForPlots,\n    ):\n        \"\"\"\n        Concatenates all elements in dfs_qw list, adjustes to schema of output_do, and using write\n            method of output_do stores the result\n\n        Args:\n            dfs_qw (list): _description_\n            output_do (SilverEventDataSyntacticQualityWarningsLogTable | \\\n                SilverEventDataSyntacticQualityWarningsForPlots): _description_\n        \"\"\"\n\n        output_do.df = reduce(lambda x, y: x.union(y), dfs_qw)\n\n        output_do.df = self.spark.createDataFrame(output_do.df.rdd, output_do.SCHEMA)\n\n        output_do.write()\n\n    def save_quality_warnings_log_table(self, dfs_qw):\n\n        self.save_quality_warnings_output(\n            dfs_qw,\n            self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsLogTable.ID],\n        )\n\n    def save_quality_warnings_for_plots(self, dfs_qw):\n\n        self.save_quality_warnings_output(\n            dfs_qw,\n            self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsForPlots.ID],\n        )\n</code></pre>"},{"location":"reference/components/quality/event_quality_warnings/event_quality_warnings/#components.quality.event_quality_warnings.event_quality_warnings.EventQualityWarnings.data_size_qw","title":"<code>data_size_qw(df_freq_distribution, lookback_period_in_days, variablility, lower_limit, upper_limit, type_of_data, measure_definition_canva=f'{MeasureDefinitions.size_data}', cond_warn_variability_canva=f'{Conditions.size_data_variability}-{Warnings.size_data_variability}', cond_warn_upper_lower_canva=f'{Conditions.size_data_upper_lower}-{Warnings.size_data_upper_lower}')</code>","text":"<p>A unified function to check both raw and clean data sizes, calculates four types of QWs: LOWER_VARIABILITY - for each row using calculated mean and std compute lower variability limit     which is mean - SDvariability, check if  daily_value is lower tan limit UPPER_VARIABILITY - for each row using calculated mean and std compute upper variability limit      which is mean + SDvariability, check if  daily_value exceeds limit ABS_LOWER_LIMIT - check if daily_value is lower than absolute number lower_limit ABS_UPPER_LIMIT - check if daily_value exceeds absolute number upper_limit All four QWs depend on thresholds, in case if statement condition is met -&gt; store cond-warn-condition_value     information in array column, some rows may have several QWs. In the end array column is exploded and cond-warn-consition_value information     is split into three corresponding columns. The function returns almost ready dfs for SilverEventDataSyntacticQualityWarningsLogTable      and SilverEventDataSyntacticQualityWarningsForPlots DOs</p> <p>Parameters:</p> Name Type Description Default <code>df_freq_distribution</code> <code>DataFrame</code> <p>df with frequency data</p> required <code>lookback_period_in_days</code> <code>int</code> <p>lenght of lookback period in days</p> required <code>variablility</code> <code>int | float</code> <p>config param, the number of SD to define the upper and lower varibaility limits: mean_size \u00b1 SD*variability, which daily_value should not exceed/be lower</p> required <code>lower_limit</code> <code>int | float</code> <p>absolute number which daily_value should not be lower</p> required <code>upper_limit</code> <code>int | float</code> <p>absolute number which daily_value can not exceed</p> required <code>type_of_data</code> <code>str</code> <p>which type of data raw or clean to check for QWs</p> required <code>measure_definition_canva</code> <code>str</code> <p>canva text to use for measure_definition column (see measure_definition.py)</p> <code>f'{size_data}'</code> <code>cond_warn_variability_canva</code> <code>str</code> <p>canva text to use for lower_upper_variability cases of data_size QWs (see conditions.py and warnings.py)</p> <code>f'{size_data_variability}-{size_data_variability}'</code> <code>cond_warn_upper_lower_canva</code> <code>str</code> <p>canva text to use for lower_upper_limit cases of data_size QWs (see conditions.py and warnings.py)</p> <code>f'{size_data_upper_lower}-{size_data_upper_lower}'</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>(DataFrame, DataFrame)</code> <p>a tuple, where first df is used for warning log table, and the second df - for plots</p> Source code in <code>multimno/components/quality/event_quality_warnings/event_quality_warnings.py</code> <pre><code>def data_size_qw(\n    self,\n    df_freq_distribution: DataFrame,\n    lookback_period_in_days: int,\n    variablility: int | float,\n    lower_limit: int | float,\n    upper_limit: int | float,\n    type_of_data: str,\n    measure_definition_canva: str = f\"{MeasureDefinitions.size_data}\",\n    cond_warn_variability_canva: str = f\"{Conditions.size_data_variability}-{Warnings.size_data_variability}\",\n    cond_warn_upper_lower_canva: str = f\"{Conditions.size_data_upper_lower}-{Warnings.size_data_upper_lower}\",\n) -&gt; tuple[DataFrame]:\n    \"\"\"\n    A unified function to check both raw and clean data sizes, calculates four types of QWs:\n    LOWER_VARIABILITY - for each row using calculated mean and std compute lower variability limit\n        which is mean - SD*variability, check if  daily_value is lower tan limit\n    UPPER_VARIABILITY - for each row using calculated mean and std compute upper variability limit\n         which is mean + SD*variability, check if  daily_value exceeds limit\n    ABS_LOWER_LIMIT - check if daily_value is lower than absolute number lower_limit\n    ABS_UPPER_LIMIT - check if daily_value exceeds absolute number upper_limit\n    All four QWs depend on thresholds, in case if statement condition is met -&gt; store cond-warn-condition_value\n        information in array column,\n    some rows may have several QWs. In the end array column is exploded and cond-warn-consition_value information\n        is split into three corresponding columns.\n    The function returns almost ready dfs for SilverEventDataSyntacticQualityWarningsLogTable\n         and SilverEventDataSyntacticQualityWarningsForPlots DOs\n\n    Args:\n        df_freq_distribution (DataFrame): df with frequency data\n        lookback_period_in_days (int): lenght of lookback period in days\n        variablility (int | float): config param, the number of SD to define the upper and lower varibaility\n            limits: mean_size \u00b1 SD*variability, which daily_value should not exceed/be lower\n        lower_limit (int | float): absolute number which daily_value should not be lower\n        upper_limit (int | float): absolute number which daily_value can not exceed\n        type_of_data (str): which type of data raw or clean to check for QWs\n        measure_definition_canva (str): canva text to use for measure_definition column (see measure_definition.py)\n        cond_warn_variability_canva (str): canva text to use for lower_upper_variability cases\n            of data_size QWs (see conditions.py and warnings.py)\n        cond_warn_upper_lower_canva (str): canva text to use for lower_upper_limit cases\n            of data_size QWs (see conditions.py and warnings.py)\n\n    Returns:\n        tuple(DataFrame, DataFrame): a tuple, where first df\n            is used for warning log table, and the second df - for plots\n    \"\"\"\n    # based on type_of_data calculate either total daily initial freqeuncy or total daily final frequency\n    if type_of_data == \"raw\":\n        sum_column = ColNames.initial_frequency\n    else:\n        sum_column = ColNames.final_frequency\n    # fill in string canvases\n    measure_definition = measure_definition_canva.format(type_of_data=type_of_data)\n    cond_warn_variability = cond_warn_variability_canva.format(SD=variablility, type_of_data=type_of_data)\n    cond_warn_upper_lower = cond_warn_upper_lower_canva.format(\n        X=lower_limit, Y=upper_limit, type_of_data=type_of_data\n    )\n    # define lookback period\n    window = Window.orderBy(ColNames.date).rowsBetween(-lookback_period_in_days, -1)\n    # prepare data\n    # calculate mean and std over window, and based on them UCL - for UPPER_VARIABILITY check and LCL\n    #   - for LOWER_VARIABILITY check\n    # create empty array cond_warn_condition_value column to store information about qws\n    df_prep = (\n        df_freq_distribution.groupBy(ColNames.date)\n        .agg(psf.sum(sum_column).alias(ColNames.daily_value))\n        .withColumns(\n            {\n                ColNames.average: psf.avg(ColNames.daily_value).over(window),\n                \"std_freq\": psf.stddev(ColNames.daily_value).over(window),\n                ColNames.UCL: psf.col(ColNames.average) + variablility * psf.col(\"std_freq\"),\n                ColNames.LCL: psf.col(ColNames.average) - variablility * psf.col(\"std_freq\"),\n                \"cond_warn_condition_value\": psf.array(),\n            }\n        )\n    )\n\n    df_prep = df_prep.cache()\n    # continue with QWs checks\n    # first filter data by period from [data_period_start-lookback_period_in_days, data_period_end]\n    # to [data_period_start, data_period_end]\n    # - a specified research period of QW\n    df_qw = df_prep.filter(psf.col(ColNames.date) &gt;= self.data_period_start)\n    # perform LOWER_VARIABILITY, UPPER_VARIABILITY, ABS_LOWER_LIMIT and ABS_UPPER_LIMIT checks\n    # if condition is met append information about condition-warning_text-condition_value as a string\n    # into array column 'cond_warn_condition_value'\n    df_qw = (\n        df_qw.withColumn(\n            \"cond_warn_condition_value\",\n            psf.when(\n                (psf.col(ColNames.daily_value) &lt; psf.col(ColNames.LCL)),\n                psf.array_append(\n                    psf.col(\"cond_warn_condition_value\"),\n                    psf.concat(\n                        psf.lit(f\"{cond_warn_variability}-\"),\n                        psf.col(ColNames.LCL).cast(\"string\"),\n                    ),\n                ),\n            ).otherwise(psf.col(\"cond_warn_condition_value\")),\n        )\n        .withColumn(\n            \"cond_warn_condition_value\",\n            psf.when(\n                (psf.col(ColNames.daily_value) &gt; psf.col(ColNames.UCL)),\n                psf.array_append(\n                    psf.col(\"cond_warn_condition_value\"),\n                    psf.concat(\n                        psf.lit(f\"{cond_warn_variability}-\"),\n                        psf.col(ColNames.UCL).cast(\"string\"),\n                    ),\n                ),\n            ).otherwise(psf.col(\"cond_warn_condition_value\")),\n        )\n        .withColumn(\n            \"cond_warn_condition_value\",\n            psf.when(\n                (psf.col(ColNames.daily_value) &lt; psf.lit(lower_limit)),\n                psf.array_append(\n                    psf.col(\"cond_warn_condition_value\"),\n                    psf.lit(f\"{cond_warn_upper_lower}-{str(lower_limit)}\"),\n                ),\n            ).otherwise(psf.col(\"cond_warn_condition_value\")),\n        )\n        .withColumn(\n            \"cond_warn_condition_value\",\n            psf.when(\n                (psf.col(ColNames.daily_value) &gt; psf.lit(upper_limit)),\n                psf.array_append(\n                    psf.col(\"cond_warn_condition_value\"),\n                    psf.lit(f\"{cond_warn_upper_lower}-{str(upper_limit)}\"),\n                ),\n            ).otherwise(psf.col(\"cond_warn_condition_value\")),\n        )\n    )\n    # explode array column 'cond_warn_condition_value'\n    df_qw = df_qw.withColumn(\n        \"cond_warn_condition_value\",\n        psf.explode(psf.col(\"cond_warn_condition_value\")),\n    )\n    # add some column constants\n    # split \"cond_warn_condition_value\" column into three: condition, wanring_text, conditon_value to\n    # match SilverEventDataSyntacticQualityWarningsLogTable.SCHEMA\n    # select only needed columns\n    df_qw = df_qw.withColumns(\n        {\n            ColNames.lookback_period: psf.lit(self.lookback_period),\n            ColNames.measure_definition: psf.lit(measure_definition),\n            ColNames.condition: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(0),\n            ColNames.warning_text: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(1),\n            ColNames.condition_value: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(2).cast(\"float\"),\n        }\n    ).select(self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsLogTable.ID].SCHEMA.fieldNames())\n\n    # save data for plots\n    # no filter by date because we need previous data of first days for plots\n    df_plots = df_prep.withColumns(\n        {\n            ColNames.lookback_period: psf.lit(self.lookback_period),\n            ColNames.type_of_qw: psf.lit(f\"{type_of_data}_data_size\"),\n        }\n    ).select(self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsForPlots.ID].SCHEMA.fieldNames())\n\n    return df_qw, df_plots\n</code></pre>"},{"location":"reference/components/quality/event_quality_warnings/event_quality_warnings/#components.quality.event_quality_warnings.event_quality_warnings.EventQualityWarnings.error_rate_qw","title":"<code>error_rate_qw(df_freq_distribution, lookback_period_in_days, variables, error_rate_over_average, error_rate_upper_variability, error_rate_upper_limit, error_rate_measure_definition_canva=f'{MeasureDefinitions.error_rate}', error_rate_cond_warn_over_average_canva=f'{Conditions.error_rate_over_average}-{Warnings.error_rate_over_average}', error_rate_cond_warn_upper_variability_canva=f'{Conditions.error_rate_upper_variability}-{Warnings.error_rate_upper_variability}', error_rate_cond_warn_upper_limit_canva=f'{Conditions.error_rate_upper_limit}-{Warnings.error_rate_upper_limit}', save_data_for_plots=False)</code>","text":"<p>Prepare data for error rate calculation. First fill in different string canvas,     then define window of aggregation, and calculate error_rate over the window on follwoing formula:     (Total initial frequency - Total final frequency) / Total initial frequency*100.     Parse preprocessed input to self.rate_common_qw function which calculates three types         of QWs: OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT</p> <p>Parameters:</p> Name Type Description Default <code>df_freq_distribution</code> <code>DataFrame</code> <p>df with frequency data.</p> required <code>lookback_period_in_days</code> <code>int</code> <p>number of days prior to date of interest.</p> required <code>variables</code> <code>list[str]</code> <p>list of column names by which error rate is calculated, kind of granularity level</p> required <code>error_rate_over_average</code> <code>int | float</code> <p>config param, specifies the upper limit which a daily value can not exceed its corresponding mean error rate</p> required <code>error_rate_upper_variability</code> <code>int | float</code> <p>config param, the number of SD to define the upper varibaility limit: mean_rate + SD*error_rate_upper_variability, which error rate can't exceed</p> required <code>error_rate_upper_limit</code> <code>int | float</code> <p>absolute number which error rate can not exceed</p> required <code>error_rate_measure_definition_canva</code> <code>str</code> <p>canva text to use for measure_definition column (see measure_definition.py)</p> <code>f'{error_rate}'</code> <code>error_rate_cond_warn_over_average_canva</code> <code>str</code> <p>canva text to use for over_average cases of error_rate QWs (see conditions.py and warnings.py)</p> <code>f'{error_rate_over_average}-{error_rate_over_average}'</code> <code>error_rate_cond_warn_upper_variability_canva</code> <code>str</code> <p>canva text to use for upper_variability cases of error_rate QWs (see conditions.py and warnings.py)</p> <code>f'{error_rate_upper_variability}-{error_rate_upper_variability}'</code> <code>error_rate_cond_warn_upper_limit_canva</code> <code>str</code> <p>canva text to use for upper_limit cases of error_rate QWs (see conditions.py and warnings.py)</p> <code>f'{error_rate_upper_limit}-{error_rate_upper_limit}'</code> <code>save_data_for_plots</code> <code>bool</code> <p>boolean, decide whether to store error rate and its corresponding average and upper variability limit for plots, default False</p> <code>False</code> <p>Returns:     tuple(DataFrame | None): a tuple, where first df is used for warning log table,         and the second df - for plots (could be also None)</p> Source code in <code>multimno/components/quality/event_quality_warnings/event_quality_warnings.py</code> <pre><code>def error_rate_qw(\n    self,\n    df_freq_distribution: DataFrame,\n    lookback_period_in_days: int,\n    variables: list[str],\n    error_rate_over_average: int | float,\n    error_rate_upper_variability: int | float,\n    error_rate_upper_limit: int | float,\n    error_rate_measure_definition_canva: str = f\"{MeasureDefinitions.error_rate}\",\n    error_rate_cond_warn_over_average_canva: str = f\"{Conditions.error_rate_over_average}-{Warnings.error_rate_over_average}\",\n    error_rate_cond_warn_upper_variability_canva: str = f\"{Conditions.error_rate_upper_variability}-{Warnings.error_rate_upper_variability}\",\n    error_rate_cond_warn_upper_limit_canva: str = f\"{Conditions.error_rate_upper_limit}-{Warnings.error_rate_upper_limit}\",\n    save_data_for_plots: bool = False,\n) -&gt; tuple[DataFrame | None]:\n    \"\"\"\n    Prepare data for error rate calculation. First fill in different string canvas,\n        then define window of aggregation, and calculate error_rate over the window on follwoing formula:\n        (Total initial frequency - Total final frequency) / Total initial frequency*100.\n        Parse preprocessed input to self.rate_common_qw function which calculates three types\n            of QWs: OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT\n\n    Args:\n        df_freq_distribution (DataFrame): df with frequency data.\n        lookback_period_in_days (int): number of days prior to date of interest.\n        variables (list[str]): list of column names by which error rate is calculated, kind of granularity level\n        error_rate_over_average (int | float): config param, specifies the upper limit which a daily value\n            can not exceed its corresponding mean error rate\n        error_rate_upper_variability (int | float): config param, the number of SD to define the upper\n            varibaility limit: mean_rate + SD*error_rate_upper_variability, which error rate can't exceed\n        error_rate_upper_limit (int | float): absolute number which error rate can not exceed\n        error_rate_measure_definition_canva (str): canva text to use for measure_definition\n            column (see measure_definition.py)\n        error_rate_cond_warn_over_average_canva (str): canva text to use for over_average cases of\n            error_rate QWs (see conditions.py and warnings.py)\n        error_rate_cond_warn_upper_variability_canva (str): canva text to use for upper_variability cases of\n            error_rate QWs (see conditions.py and warnings.py)\n        error_rate_cond_warn_upper_limit_canva (str): canva text to use for upper_limit cases of error_rate\n            QWs (see conditions.py and warnings.py)\n        save_data_for_plots (bool): boolean, decide whether to store error rate and its corresponding average\n            and upper variability limit for plots, default False\n    Returns:\n        tuple(DataFrame | None): a tuple, where first df is used for warning log table,\n            and the second df - for plots (could be also None)\n    \"\"\"\n    # fill in all string comnstants with relevant information\n    # based on variables, error_rate_over_average, error_rate_upper_variability, error_rate_upper_limit args\n    error_rate_measure_definition = error_rate_measure_definition_canva.format(variables=\"&amp;\".join(variables))\n\n    error_rate_cond_warn_over_average = error_rate_cond_warn_over_average_canva.format(\n        variables=\"&amp;\".join(variables), X=error_rate_over_average\n    )\n    error_rate_cond_warn_upper_variability = error_rate_cond_warn_upper_variability_canva.format(\n        variables=\"&amp;\".join(variables), SD=error_rate_upper_variability\n    )\n    error_rate_cond_warn_upper_limit = error_rate_cond_warn_upper_limit_canva.format(\n        variables=\"&amp;\".join(variables), X=error_rate_upper_limit\n    )\n    # qws of error rate by date is calculated based on previous days\n    if variables == [ColNames.date]:\n        window = Window.orderBy(ColNames.date).rowsBetween(-lookback_period_in_days, -1)\n    else:\n        # qws of error rate by date and other colum(s) is calculated over all \"data points\" of the same date\n        window = Window.partitionBy(ColNames.date)\n    # calculate error rate, a.k.a daily_value\n    df_qw = (\n        df_freq_distribution.groupBy(*variables)\n        .agg(\n            psf.sum(ColNames.initial_frequency).alias(\"sum_init_freq\"),\n            psf.sum(ColNames.final_frequency).alias(\"sum_final_freq\"),\n        )\n        .withColumn(\n            ColNames.daily_value,\n            (psf.col(\"sum_init_freq\") - psf.col(\"sum_final_freq\")) / psf.col(\"sum_init_freq\") * 100,\n        )\n    )\n    # using self.rate_common_qw funciton calculate three types of QWs\n    #   -&gt; OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT\n    # return a tuple of (df_log, df_plots | None)\n    qw_result = self.rate_common_qw(\n        df_qw,\n        window,\n        error_rate_upper_variability,\n        error_rate_over_average,\n        error_rate_upper_limit,\n        error_rate_measure_definition,\n        error_rate_cond_warn_upper_variability,\n        error_rate_cond_warn_over_average,\n        error_rate_cond_warn_upper_limit,\n        save_data_for_plots,\n    )\n\n    return qw_result\n</code></pre>"},{"location":"reference/components/quality/event_quality_warnings/event_quality_warnings/#components.quality.event_quality_warnings.event_quality_warnings.EventQualityWarnings.error_type_rate_qw","title":"<code>error_type_rate_qw(df_qa_by_column, df_freq_distribution, field_name, error_type, lookback_period_in_days, error_type_rate_over_average, error_type_rate_upper_variability, error_type_rate_upper_limit, error_type_rate_measure_definition_canva=f'{MeasureDefinitions.error_type_rate}', error_type_rate_cond_warn_over_average_canva=f'{Conditions.error_type_rate_over_average}-{Warnings.error_type_rate_over_average}', error_type_rate_cond_warn_upper_variability_canva=f'{Conditions.error_type_rate_upper_variability}-{Warnings.error_type_rate_upper_variability}', error_type_rate_cond_warn_upper_limit_canva=f'{Conditions.error_type_rate_upper_limit}-{Warnings.error_type_rate_upper_limit}')</code>","text":"<p>Prepare data for error type rate calculation. First fill in different string canvas, then based     on field name and error type calculate their corresponding error rate using formula:     number of errors of this error_type&amp;field_name combo / Total initial frequency *100 (BY DATE).     Parse preprocessed input along with window (which is a lookback period)     to self.rate_common_qw function which calculates three types of QWs:     OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT</p> <p>Parameters:</p> Name Type Description Default <code>df_qa_by_column</code> <code>DataFrame</code> <p>df with qa by column data.</p> required <code>df_freq_distribution</code> <code>DataFrame</code> <p>df with frequency data.</p> required <code>field_name</code> <code>str | None</code> <p>config param, the name of column of which to check error_type.</p> required <code>error_type</code> <code>str</code> <p>config param, the name of error type.</p> required <code>lookback_period_in_days</code> <code>int</code> <p>number of days prior to date of intrest.</p> required <code>error_type_rate_over_average</code> <code>int | float</code> <p>config param, specifies the upper limit over which daily value can not exceed its corresponding mean error rate.</p> required <code>error_type_rate_upper_variability</code> <code>int | float</code> <p>config param, the number of SD to define the upper varibaility limit: mean_rate + SD*error_type_rate_upper_variability, which daily value can not exceed</p> required <code>error_type_rate_upper_limit</code> <code>int | float</code> <p>absolute number which daily value can not exceed</p> required <code>error_type_rate_measure_definition_canva</code> <code>str</code> <p>canva text to use for measure_definition column (see measure_definition.py)</p> <code>f'{error_type_rate}'</code> <code>error_type_rate_cond_warn_over_average_canva</code> <code>str</code> <p>canva text to use for over_average cases of error_type_rate QWs (see conditions.py and warnings.py)</p> <code>f'{error_type_rate_over_average}-{error_type_rate_over_average}'</code> <code>error_type_rate_cond_warn_upper_variability_canva</code> <code>str</code> <p>canva text to use for upper_variability cases of error_type_rate QWs (see conditions.py and warnings.py)</p> <code>f'{error_type_rate_upper_variability}-{error_type_rate_upper_variability}'</code> <code>error_type_rate_cond_warn_upper_limit_canva</code> <code>str</code> <p>canva text to use for upper_limit cases of error_type_rate QWs (see conditions.py and warnings.py)</p> <code>f'{error_type_rate_upper_limit}-{error_type_rate_upper_limit}'</code> <p>Returns:     tuple(DataFrame | None): a tuple, where first df is used for warning log table,         and the second df - for plots, but since save_data_for_plots always False, output=None</p> Source code in <code>multimno/components/quality/event_quality_warnings/event_quality_warnings.py</code> <pre><code>def error_type_rate_qw(\n    self,\n    df_qa_by_column: DataFrame,\n    df_freq_distribution: DataFrame,\n    field_name: str | None,\n    error_type: str,\n    lookback_period_in_days: int,\n    error_type_rate_over_average: int | float,\n    error_type_rate_upper_variability: int | float,\n    error_type_rate_upper_limit: int | float,\n    error_type_rate_measure_definition_canva: str = f\"{MeasureDefinitions.error_type_rate}\",\n    error_type_rate_cond_warn_over_average_canva: str = f\"{Conditions.error_type_rate_over_average}-{Warnings.error_type_rate_over_average}\",\n    error_type_rate_cond_warn_upper_variability_canva: str = f\"{Conditions.error_type_rate_upper_variability}-{Warnings.error_type_rate_upper_variability}\",\n    error_type_rate_cond_warn_upper_limit_canva: str = f\"{Conditions.error_type_rate_upper_limit}-{Warnings.error_type_rate_upper_limit}\",\n) -&gt; tuple[DataFrame | None]:\n    \"\"\"\n    Prepare data for error type rate calculation. First fill in different string canvas, then based\n        on field name and error type calculate their corresponding error rate using formula:\n        number of errors of this error_type&amp;field_name combo / Total initial frequency *100 (BY DATE).\n        Parse preprocessed input along with window (which is a lookback period)\n        to self.rate_common_qw function which calculates three types of QWs:\n        OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT\n\n\n    Args:\n        df_qa_by_column (DataFrame): df with qa by column data.\n        df_freq_distribution (DataFrame): df with frequency data.\n        field_name (str | None): config param, the name of column of which to check error_type.\n        error_type (str): config param, the name of error type.\n        lookback_period_in_days (int): number of days prior to date of intrest.\n        error_type_rate_over_average (int | float): config param, specifies the upper limit over which daily\n            value can not exceed its corresponding mean error rate.\n        error_type_rate_upper_variability (int | float): config param, the number of SD to define the upper\n            varibaility limit: mean_rate + SD*error_type_rate_upper_variability, which daily value can not exceed\n        error_type_rate_upper_limit (int | float): absolute number which daily value can not exceed\n        error_type_rate_measure_definition_canva (str): canva text to use for measure_definition\n            column (see measure_definition.py)\n        error_type_rate_cond_warn_over_average_canva (str): canva text to use for over_average cases of\n            error_type_rate QWs (see conditions.py and warnings.py)\n        error_type_rate_cond_warn_upper_variability_canva (str): canva text to use for upper_variability\n            cases of error_type_rate QWs (see conditions.py and warnings.py)\n        error_type_rate_cond_warn_upper_limit_canva (str): canva text to use for upper_limit cases of\n            error_type_rate QWs (see conditions.py and warnings.py)\n    Returns:\n        tuple(DataFrame | None): a tuple, where first df is used for warning log table,\n            and the second df - for plots, but since save_data_for_plots always False, output=None\n    \"\"\"\n    # based on error_type, error_type_rate_over_average, error_type_rate_upper_variability,\n    #  error_type_rate_upper_limit\n    # fill in string canvases\n    colname_error_type, error_type_qw_name = self.dict_error_type_info[error_type]\n\n    error_type_rate_measure_definition = error_type_rate_measure_definition_canva.format(\n        error_type_name=error_type_qw_name, field_name=str(field_name)\n    )\n\n    error_type_rate_cond_warn_over_average = error_type_rate_cond_warn_over_average_canva.format(\n        error_type_name=error_type_qw_name,\n        field_name=str(field_name),\n        X=error_type_rate_over_average,\n    )\n    error_type_rate_cond_warn_upper_variability = error_type_rate_cond_warn_upper_variability_canva.format(\n        error_type_name=error_type_qw_name,\n        field_name=str(field_name),\n        SD=error_type_rate_upper_variability,\n    )\n    error_type_rate_cond_warn_upper_limit = error_type_rate_cond_warn_upper_limit_canva.format(\n        error_type_name=error_type_qw_name,\n        field_name=str(field_name),\n        X=error_type_rate_upper_limit,\n    )\n    # for error_type that have more then one or applicable columns\n    # filter df_qa_by_column by field_name and error_type\n    if field_name is not None:\n        df_qa_by_column = df_qa_by_column.filter(\n            (psf.col(ColNames.variable) == field_name) &amp; (psf.col(ColNames.type_of_error) == colname_error_type)\n        ).select(ColNames.date, ColNames.value)\n    else:\n        # for error_types which technically do not belong specifically to one of event\n        # columns filter only by error_type (e.g. no_location error_type)\n        df_qa_by_column = df_qa_by_column.filter(psf.col(ColNames.type_of_error) == colname_error_type).select(\n            ColNames.date, ColNames.value\n        )\n    # calculate total daily initial frequency\n    df_freq_distribution = (\n        df_freq_distribution.groupby(ColNames.date)\n        .agg(psf.sum(ColNames.initial_frequency).alias(\"sum_init_freq\"))\n        .select(ColNames.date, \"sum_init_freq\")\n    )\n    # for each date combine two type of information number of errors and total daily initial frequency\n    df_combined = df_qa_by_column.join(df_freq_distribution, on=ColNames.date, how=\"inner\")\n    # for each date calculate error_type_rate, a.k.a daily_value\n    df_temp = df_combined.withColumn(\n        ColNames.daily_value,\n        (psf.col(ColNames.value) / psf.col(\"sum_init_freq\")) * 100,\n    )\n\n    # qws will be caluclated based on previous days\n    window = Window.orderBy(ColNames.date).rowsBetween(-lookback_period_in_days, -1)\n    # using self.rate_common_qw funciton calculate three types of QWs -&gt;\n    # OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT\n    # return a tuple of (df_log, None)\n    qw_result = self.rate_common_qw(\n        df_temp,\n        window,\n        error_type_rate_upper_variability,\n        error_type_rate_over_average,\n        error_type_rate_upper_limit,\n        error_type_rate_measure_definition,\n        error_type_rate_cond_warn_upper_variability,\n        error_type_rate_cond_warn_over_average,\n        error_type_rate_cond_warn_upper_limit,\n    )\n    return qw_result\n</code></pre>"},{"location":"reference/components/quality/event_quality_warnings/event_quality_warnings/#components.quality.event_quality_warnings.event_quality_warnings.EventQualityWarnings.rate_common_qw","title":"<code>rate_common_qw(df_temp, window, rate_upper_variability, rate_over_average, rate_upper_limit, measure_definition, cond_warn_upper_variability, cond_warn_over_average, cond_warn_upper_limit, save_data_for_plots=False)</code>","text":"<p>Take input df with \"daily_value\" column, and calculates three types of QWs: OVER_AVERAGE - for each row first based on specified window take mean of values, and then check if     daily_value exceeds mean by more than rate_over_average UPPER_VARIABILITY - for each row using already calculated mean compute upper variability limit which is     mean + SD*rate_upper_variability, check if  daily_value exceeds it ABS_UPPER_LIMIT - check if daily_value exceeds absolute number rate_upper_limit All three QWs depend on specified thresholds, if daily_value exceeds one of calculated values it will     store cond-warn-condition_value information in array column, some rows may have several QWs. In the end array column is exploded and cond-warn-condition_value     information is split into three corresponding columns. The function returns almost ready df for SilverEventDataSyntacticQualityWarningsLogTable DO, and based     on save_data_for_plots arg returns either almost ready data for plots or None</p> <p>Parameters:</p> Name Type Description Default <code>df_temp</code> <code>DataFrame</code> <p>temprory data that must have daily_value column to be used in further QW calculations</p> required <code>window</code> <code>Window</code> <p>a window within which perform aggregation</p> required <code>rate_upper_variability</code> <code>int | float</code> <p>config param, specifies the upper limit over which daily value  can not exceed its corresponding mean error rate</p> required <code>rate_over_average</code> <code>int | float</code> <p>config param, the number of SD to define the upper varibaility  limit: mean_rate + SD*error_rate_upper_variability, which daily value can not exceed</p> required <code>rate_upper_limit</code> <code>int | float</code> <p>absolute number which daily value can not exceed</p> required <code>measure_definition</code> <code>str</code> <p>canva text to use for measure_definition column (see measure_definition.py)</p> required <code>cond_warn_over_average</code> <code>str</code> <p>canva text to use for over_average cases (see conditions.py and warnings.py)</p> required <code>cond_warn_upper_variability</code> <code>str</code> <p>canva text to use for upper_variability cases (see conditions.py and warnings.py)</p> required <code>cond_warn_upper_limit</code> <code>str</code> <p>canva text to use for upper_limit cases (see conditions.py and warnings.py)</p> required <code>save_data_for_plots</code> <code>bool</code> <p>boolean, decide whether to store daily_value and its corresponding average and upper variability limit for plots. Defaults to False.</p> <code>False</code> <p>Returns:      tuple(DataFrame | None): a tuple, where first df is used for         warning log table, and the second df - for plots</p> Source code in <code>multimno/components/quality/event_quality_warnings/event_quality_warnings.py</code> <pre><code>def rate_common_qw(\n    self,\n    df_temp: DataFrame,\n    window: Window,\n    rate_upper_variability: int | float,\n    rate_over_average: int | float,\n    rate_upper_limit: int | float,\n    measure_definition: str,\n    cond_warn_upper_variability: str,\n    cond_warn_over_average: str,\n    cond_warn_upper_limit: str,\n    save_data_for_plots: bool = False,\n) -&gt; tuple[DataFrame | None]:\n    \"\"\"\n    Take input df with \"daily_value\" column, and calculates three types of QWs:\n    OVER_AVERAGE - for each row first based on specified window take mean of values, and then check if\n        daily_value exceeds mean by more than rate_over_average\n    UPPER_VARIABILITY - for each row using already calculated mean compute upper variability limit which is\n        mean + SD*rate_upper_variability, check if  daily_value exceeds it\n    ABS_UPPER_LIMIT - check if daily_value exceeds absolute number rate_upper_limit\n    All three QWs depend on specified thresholds, if daily_value exceeds one of calculated values it will\n        store cond-warn-condition_value information in array column,\n    some rows may have several QWs. In the end array column is exploded and cond-warn-condition_value\n        information is split into three corresponding columns.\n    The function returns almost ready df for SilverEventDataSyntacticQualityWarningsLogTable DO, and based\n        on save_data_for_plots arg returns either almost ready data for plots or None\n\n    Args:\n        df_temp (DataFrame): temprory data that must have daily_value column to\n            be used in further QW calculations\n        window (Window): a window within which perform aggregation\n        rate_upper_variability (int|float): config param, specifies the upper limit over which daily value\n             can not exceed its corresponding mean error rate\n        rate_over_average (int|float): config param, the number of SD to define the upper varibaility\n             limit: mean_rate + SD*error_rate_upper_variability, which daily value can not exceed\n        rate_upper_limit (int|float): absolute number which daily value can not exceed\n        measure_definition (str): canva text to use for measure_definition column (see measure_definition.py)\n        cond_warn_over_average (str): canva text to use for over_average cases (see conditions.py and warnings.py)\n        cond_warn_upper_variability (str): canva text to use for\n            upper_variability cases (see conditions.py and warnings.py)\n        cond_warn_upper_limit (str): canva text to use for upper_limit cases (see conditions.py and warnings.py)\n        save_data_for_plots (bool): boolean, decide whether to store daily_value and its corresponding average\n            and upper variability limit for plots. Defaults to False.\n    Returns:\n         tuple(DataFrame | None): a tuple, where first df is used for\n            warning log table, and the second df - for plots\n    \"\"\"\n    # prepare data\n    # calculate mean and std over window, and based on them UCL - for UPPER_VARIABILITY check\n    # ratio_perc - for OVER_AVERAGE check\n    # create empty array cond_warn_condition_value column to store inromation about qws\n    df_prep = df_temp.withColumns(\n        {\n            ColNames.average: psf.avg(ColNames.daily_value).over(window),\n            \"std_rate\": psf.stddev(ColNames.daily_value).over(window),\n            \"ratio_perc\": (psf.col(ColNames.daily_value) / psf.col(ColNames.average)) * 100,\n            ColNames.UCL: psf.col(ColNames.average) + rate_upper_variability * psf.col(\"std_rate\"),\n            \"cond_warn_condition_value\": psf.array(),\n        }\n    )\n    # if save_data_for_plots=True, add some new columns with constant values\n    # and select only applicable to SilverEventDataSyntacticQualityWarningsForPlots.SCHEMA\n    # else - return None\n    if save_data_for_plots:\n        df_prep = df_prep.cache()\n        df_plots = df_prep.withColumns(\n            {\n                ColNames.lookback_period: psf.lit(self.lookback_period),\n                ColNames.type_of_qw: psf.lit(\"error_rate\"),\n                ColNames.LCL: psf.lit(None).cast(\"float\"),\n            }\n        ).select(\n            self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsForPlots.ID].SCHEMA.fieldNames()\n        )\n    else:\n        df_plots = None\n\n    # continue with QWs checks\n    # first filter temp data by period from [data_period_start-lookback_period_in_days, data_period_end]\n    #  to [data_period_start, data_period_end]\n    # filter is aaplied after plot block because the first days of research period needs previous data to plot\n    df_qw = df_prep.filter(psf.col(ColNames.date) &gt;= self.data_period_start)\n    # perform UPPER_VARIABILITY, OVER_AVERAGE, and ABS_UPPER_LIMIT checks\n    # if condition is met store information about condition-warning_text-condition_value as a string into\n    # array column 'cond_warn_condition_value'\n    df_qw = (\n        df_qw.withColumn(\n            \"cond_warn_condition_value\",\n            psf.when(\n                psf.col(ColNames.daily_value) &gt; psf.col(ColNames.UCL),\n                psf.array_append(\n                    psf.col(\"cond_warn_condition_value\"),\n                    psf.concat(\n                        psf.lit(f\"{cond_warn_upper_variability}-\"),\n                        psf.col(ColNames.UCL).cast(\"string\"),\n                    ),\n                ),\n            ).otherwise(psf.col(\"cond_warn_condition_value\")),\n        )\n        .withColumn(\n            \"cond_warn_condition_value\",\n            psf.when(\n                psf.col(\"ratio_perc\") &gt; psf.lit(100 + rate_over_average),\n                psf.array_append(\n                    psf.col(\"cond_warn_condition_value\"),\n                    psf.lit(f\"{cond_warn_over_average}-{str(rate_over_average)}\"),\n                ),\n            ).otherwise(psf.col(\"cond_warn_condition_value\")),\n        )\n        .withColumn(\n            \"cond_warn_condition_value\",\n            psf.when(\n                psf.col(ColNames.daily_value) &gt; psf.lit(rate_upper_limit),\n                psf.array_append(\n                    psf.col(\"cond_warn_condition_value\"),\n                    psf.lit(f\"{cond_warn_upper_limit}-{str(rate_upper_limit)}\"),\n                ),\n            ).otherwise(psf.col(\"cond_warn_condition_value\")),\n        )\n    )\n    # explode array column 'cond_warn_condition_value'\n    df_qw = df_qw.withColumn(\n        \"cond_warn_condition_value\",\n        psf.explode(psf.col(\"cond_warn_condition_value\")),\n    )\n    # add some column constants\n    # split \"cond_warn_condition_value\" column into three: condition, wanring_text, conditon_value to match\n    # SilverEventDataSyntacticQualityWarningsLogTable.SCHEMA\n    # select only needed columns\n    df_qw = df_qw.withColumns(\n        {\n            ColNames.lookback_period: psf.lit(self.lookback_period),\n            ColNames.measure_definition: psf.lit(measure_definition),\n            ColNames.condition: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(0),\n            ColNames.warning_text: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(1),\n            ColNames.condition_value: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(2).cast(\"float\"),\n        }\n    ).select(self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsLogTable.ID].SCHEMA.fieldNames())\n\n    return (df_qw, df_plots)\n</code></pre>"},{"location":"reference/components/quality/event_quality_warnings/event_quality_warnings/#components.quality.event_quality_warnings.event_quality_warnings.EventQualityWarnings.save_quality_warnings_output","title":"<code>save_quality_warnings_output(dfs_qw, output_do)</code>","text":"<p>Concatenates all elements in dfs_qw list, adjustes to schema of output_do, and using write     method of output_do stores the result</p> <p>Parameters:</p> Name Type Description Default <code>dfs_qw</code> <code>list</code> <p>description</p> required <code>output_do</code> <code>SilverEventDataSyntacticQualityWarningsLogTable | SilverEventDataSyntacticQualityWarningsForPlots</code> <p>description</p> required Source code in <code>multimno/components/quality/event_quality_warnings/event_quality_warnings.py</code> <pre><code>def save_quality_warnings_output(\n    self,\n    dfs_qw: list[DataFrame | None],\n    output_do: SilverEventDataSyntacticQualityWarningsLogTable | SilverEventDataSyntacticQualityWarningsForPlots,\n):\n    \"\"\"\n    Concatenates all elements in dfs_qw list, adjustes to schema of output_do, and using write\n        method of output_do stores the result\n\n    Args:\n        dfs_qw (list): _description_\n        output_do (SilverEventDataSyntacticQualityWarningsLogTable | \\\n            SilverEventDataSyntacticQualityWarningsForPlots): _description_\n    \"\"\"\n\n    output_do.df = reduce(lambda x, y: x.union(y), dfs_qw)\n\n    output_do.df = self.spark.createDataFrame(output_do.df.rdd, output_do.SCHEMA)\n\n    output_do.write()\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/","title":"network_quality_warnings","text":""},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/","title":"network_quality_warnings","text":"<p>Module that generates the quality warnings associated to the syntactic checks/cleaning of the raw MNO Network Topology Data.</p>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings","title":"<code>NetworkQualityWarnings</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class that produces the log tables and data required for plotting associated to Network Topology Data cleaning/syntactic checks.</p> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>class NetworkQualityWarnings(Component):\n    \"\"\"\n    Class that produces the log tables and data required for plotting associated to Network Topology Data\n    cleaning/syntactic checks.\n    \"\"\"\n\n    COMPONENT_ID = \"NetworkQualityWarnings\"\n\n    PERIOD_DURATION = {\"week\": 7, \"month\": 30, \"quarter\": 90}\n\n    TITLE = \"MNO Network Topology Data Quality Warnings\"\n\n    MEASURE_DEFINITION = {\n        \"SIZE_RAW_DATA\": \"Value of the size of the raw data object\",\n        \"SIZE_CLEAN_DATA\": \"Value of the size of the clean data object\",\n        \"TOTAL_ERROR_RATE\": \"Error rate\",\n        \"Missing_value_RATE\": \"Missing rate value of {field_name}\".format,\n        \"Out_of_range_RATE\": \"Out of range rate of {field_name}\".format,\n        \"Parsing_error_RATE\": \"Parsing error rate of {field_name}\".format,\n    }\n\n    ERROR_TYPE = {\n        \"Missing_value_RATE\": NetworkErrorType.NULL_VALUE,\n        \"Out_of_range_RATE\": NetworkErrorType.OUT_OF_RANGE,\n        \"Parsing_error_RATE\": NetworkErrorType.CANNOT_PARSE,\n    }\n\n    CONDITION = {\n        \"Missing_value_RATE\": {\n            \"AVERAGE\": \"Missing value rate of {field_name} is over the previous period average by more than {value} %\".format,\n            \"VARIABILITY\": (\n                \"Missing value rate of {field_name} is over the upper control limit calculated on the basis of average and standard deviation \"\n                + \"of the distribution of the missing value rate of {field_name} in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\"\n            ).format,\n            \"ABS_VALUE_UPPER_LIMIT\": \"Missing value rate of {field_name} is over the value {value}\".format,\n        },\n        \"Out_of_range_RATE\": {\n            \"AVERAGE\": \"Out of range rate of {field_name} is over the previous period average by more than {value} %\".format,\n            \"VARIABILITY\": (\n                \"Out of range rate of {field_name} is over the upper control limit calculated on the basis of average and standard deviation \"\n                + \"of the distribution of the out of range rate of {field_name} in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\"\n            ).format,\n            \"ABS_VALUE_UPPER_LIMIT\": \"Out of range rate of {field_name} is over the value {value} %\".format,\n        },\n        \"Parsing_error_RATE\": {\n            \"AVERAGE\": \"Parsing error rate of {field_name} is over the previous period average by more than {value} %\".format,\n            \"VARIABILITY\": (\n                \"Parsing error rate of {field_name} is over the upper control limit calculated on the basis of average and standard deviation \"\n                + \"of the distribution of the parsing error rate of {field_name} in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\"\n            ).format,\n            \"ABS_VALUE_UPPER_LIMIT\": \"Parsing error rate of {field_name} is over the value {value}\".format,\n        },\n    }\n\n    WARNING_MESSAGE = {\n        \"Missing_value_RATE\": {\n            \"AVERAGE\": \"The missing value rate of {field_name} after the syntactic check procedure is unexpectedly high with respect to the previous period.\".format,\n            \"VARIABILITY\": (\n                \"The missing value rate of {field_name} after the syntactic check procedure is unexpectedly high \"\n                + \"with respect to previous period taking into account its usual variability.\"\n            ).format,\n            \"ABS_VALUE_UPPER_LIMIT\": \"The missing value rate of {field_name} is over the threshold\".format,\n        },\n        \"Out_of_range_RATE\": {\n            \"AVERAGE\": \"The out of range rate of {field_name} after the syntactic check procedure is unexpectedly high with respect to the previous period.\".format,\n            \"VARIABILITY\": (\n                \"The out of range of {field_name} after the syntactic check procedure is unexpectedly high \"\n                + \"with respect to previous period taking into account its usual variability.\"\n            ).format,\n            \"ABS_VALUE_UPPER_LIMIT\": \"The out of range rate of {field_name} is over the threshold\".format,\n        },\n        \"Parsing_error_RATE\": {\n            \"AVERAGE\": \"The parsing error rate of {field_name} after the syntactic check procedure is unexpectedly high with respect to the previous period.\".format,\n            \"VARIABILITY\": (\n                \"The parsing error of {field_name} after the syntactic check procedure is unexpectedly high \"\n                + \"with respect to previous period taking into account its usual variability.\"\n            ).format,\n            \"ABS_VALUE_UPPER_LIMIT\": \"The parsing error rate of {field_name} is over the threshold\".format,\n        },\n    }\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.timestamp = datetime.datetime.now()\n\n        # Read date of study\n        self.date_format = self.config.get(self.COMPONENT_ID, \"date_format\")\n        date_of_study = self.config.get(self.COMPONENT_ID, \"date_of_study\")\n\n        try:\n            self.date_of_study = datetime.datetime.strptime(date_of_study, self.date_format).date()\n        except ValueError as e:\n            self.logger.error(str(e), exc_info=True)\n            raise e\n\n        # Read lookback period\n        self.lookback_period = self.config.get(self.COMPONENT_ID, \"lookback_period\")\n\n        if self.lookback_period not in [\"week\", \"month\", \"quarter\"]:\n            error_msg = (\n                \"Configuration parameter `lookback_period` must be one of `week`, `month`, or `quarter`, \"\n                f\"but {self.lookback_period} was passed\"\n            )\n\n            self.logger.error(error_msg)\n            raise ValueError(error_msg)\n\n        self.lookback_dates = [\n            self.date_of_study - datetime.timedelta(days=d)\n            for d in range(1, self.PERIOD_DURATION[self.lookback_period] + 1)\n        ]\n\n        self.lookback_period_start = min(self.lookback_dates)\n        self.lookback_period_end = max(self.lookback_dates)\n\n        # Read thresholds and use read values instead of default ones when appropriate\n        self.thresholds = self.get_thresholds()\n\n        self.warnings = []\n\n        self.plots_data = dict()\n\n    def get_thresholds(self) -&gt; dict:\n        \"\"\"\n        Method that reads the threshold-related parameters, contained in the config file,\n        and saves them in memory to use them instead of the ones specified by default.\n        Raises:\n            ValueError: non-numerical value that cannot be parsed to float has been used in\n                the config file\n            ValueError: Negative value for a given parameter has been given, when only\n                non-negative values make sense and are thus allowed.\n\n        Returns:\n            thresholds: dict containing the different thresholds used for computing the\n                quality warnings, with the same structure as\n                multimno.core.constants.network_default_thresholds.NETWORK_DEFAULT_THRESHOLDS\n        \"\"\"\n        config_thresholds = self.config.geteval(self.COMPONENT_ID, \"thresholds\")\n        thresholds = NETWORK_DEFAULT_THRESHOLDS\n\n        for error_key in config_thresholds.keys():\n            if error_key not in NETWORK_DEFAULT_THRESHOLDS:\n                self.logger.info(f\"Parameter key {error_key} unknown -- ignored\")\n                continue\n\n            if error_key in [\"SIZE_RAW_DATA\", \"SIZE_CLEAN_DATA\", \"TOTAL_ERROR_RATE\"]:\n                for param_key, val in config_thresholds[error_key].items():\n                    if param_key not in NETWORK_DEFAULT_THRESHOLDS[error_key]:\n                        self.logger.info(f\"Unknown parameter {param_key} under {error_key} -- ignored\")\n                        continue\n\n                    try:\n                        val = float(val)\n                    except ValueError as e:\n                        error_msg = (\n                            f\"Expected numeric value for parameter {param_key} under {error_key} - got {val} instead.\"\n                        )\n                        self.logger.error(error_msg)\n                        raise e\n\n                    if val &lt; 0:\n                        error_msg = f\"Parameter {param_key} under {error_key} must be non-negative - got {val}\"\n                        self.logger.error(error_msg)\n                        raise ValueError(error_msg)\n\n                    thresholds[error_key][param_key] = val\n\n            else:  # then is one of 'Missing_value_RATE', 'Out_of_range_RATE', 'Parsing_error_RATE'\n                for field_name in config_thresholds[error_key]:\n                    if field_name not in NETWORK_DEFAULT_THRESHOLDS[error_key]:\n                        self.logger.info(f\"Unknown field name {field_name} under {error_key} -- ignored\")\n                        continue\n\n                    for param_key, val in config_thresholds[error_key][field_name].items():\n                        if param_key not in NETWORK_DEFAULT_THRESHOLDS[error_key][field_name]:\n                            self.logger.info(\n                                f'Unknown parameter {param_key} under {error_key}[\"{field_name}\"] -- ignored'\n                            )\n                            continue\n\n                        try:\n                            val = float(val)\n                        except ValueError as e:\n                            error_msg = f'Expected numeric value for parameter {param_key} under {error_key}[\"{field_name}\"] - got {val} instead.'\n                            self.logger.error(error_msg)\n                            raise e\n\n                        if val &lt; 0:\n                            error_msg = f'Parameter {param_key} under {error_key}[\"{field_name}\"] must be non-negative - got {val}'\n                            self.logger.error(error_msg)\n                            raise ValueError(error_msg)\n\n                        thresholds[error_key][field_name][param_key] = val\n\n        return thresholds\n\n    def initalize_data_objects(self):\n        input_silver_quality_metrics_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"network_syntactic_quality_metrics_by_column\"\n        )\n        output_silver_log_table_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"network_syntactic_quality_warnings_log_table\"\n        )\n\n        output_silver_line_plot_data_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"network_syntactic_quality_warnings_line_plot_data\"\n        )\n\n        output_silver_pie_plot_data_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"network_syntactic_quality_warnings_pie_plot_data\"\n        )\n\n        silver_quality_metrics = SilverNetworkDataQualityMetricsByColumn(\n            self.spark,\n            input_silver_quality_metrics_path,\n            partition_columns=[ColNames.year, ColNames.month, ColNames.day],\n        )\n\n        silver_log_table = SilverNetworkDataSyntacticQualityWarningsLogTable(\n            self.spark, output_silver_log_table_path, partition_columns=[ColNames.year, ColNames.month, ColNames.day]\n        )\n\n        silver_line_plot_data = SilverNetworkSyntacticQualityWarningsLinePlotData(\n            self.spark, output_silver_line_plot_data_path\n        )\n\n        silver_pie_plot_data = SilverNetworkSyntacticQualityWarningsPiePlotData(\n            self.spark, output_silver_pie_plot_data_path\n        )\n\n        self.input_data_objects = {silver_quality_metrics.ID: silver_quality_metrics}\n        self.output_data_objects = {\n            silver_log_table.ID: silver_log_table,\n            silver_line_plot_data.ID: silver_line_plot_data,\n            silver_pie_plot_data.ID: silver_pie_plot_data,\n        }\n\n    def transform(self):\n\n        # Check if both the date of study and the specified lookback period dates are in file\n        self.check_needed_dates()\n\n        lookback_stats, lookback_initial_rows, lookback_final_rows = self.get_lookback_period_statistics()\n\n        today_values = self.get_study_date_values()\n\n        raw_average, raw_UCL, raw_LCL = self.raw_size_warnings(lookback_stats, today_values)\n\n        clean_average, clean_UCL, clean_LCL = self.clean_size_warnings(lookback_stats, today_values)\n\n        error_rate, error_rate_avg, error_rate_UCL = self.error_rate_warnings(\n            lookback_initial_rows, lookback_final_rows, today_values\n        )\n\n        self.all_specific_error_warnings(lookback_stats, today_values)\n\n        self.output_data_objects[SilverNetworkDataSyntacticQualityWarningsLogTable.ID].df = self.spark.createDataFrame(\n            self.warnings, SilverNetworkDataSyntacticQualityWarningsLogTable.SCHEMA\n        )\n\n        lookback_initial_rows[self.date_of_study] = today_values[None][NetworkErrorType.INITIAL_ROWS]\n        lookback_final_rows[self.date_of_study] = today_values[None][NetworkErrorType.FINAL_ROWS]\n\n        self.create_plots_data(\n            lookback_initial_rows=lookback_initial_rows,\n            lookback_final_rows=lookback_final_rows,\n            today_values=today_values,\n            error_rate=error_rate,\n            raw_average=raw_average,\n            clean_average=clean_average,\n            error_rate_avg=error_rate_avg,\n            raw_UCL=raw_UCL,\n            clean_UCL=clean_UCL,\n            error_rate_UCL=error_rate_UCL,\n            raw_LCL=raw_LCL,\n            clean_LCL=clean_LCL,\n        )\n\n    def check_needed_dates(self) -&gt; None:\n        \"\"\"\n        Method that checks if both the date of study and the dates necessary to generate\n        the quality warnings, specified through the lookback_period parameter, are present\n        in the input data.\n        \"\"\"\n\n        # Collect all distinct dates in the input quality metrics within the needed range\n        metrics = self.input_data_objects[SilverNetworkDataQualityMetricsByColumn.ID].df\n        dates = (\n            metrics.filter(\n                F.col(\"date\")\n                # left- and right- inclusive\n                .between(\n                    self.date_of_study - datetime.timedelta(days=self.PERIOD_DURATION[self.lookback_period]),\n                    self.date_of_study,\n                )\n            )\n            .select(F.col(ColNames.date))\n            .distinct()\n            .collect()\n        )\n\n        dates = [row[ColNames.date] for row in dates]\n\n        if self.date_of_study not in dates:\n            raise ValueError(\n                f\"The date of study, {self.date_of_study}, is not present in the input Quality Metrics data\"\n            )\n\n        if set(self.lookback_dates).intersection(set(dates)) != set(self.lookback_dates):\n            error_msg = f\"\"\"\n                The following dates from the lookback period are not present in the\n                input Quality Metrics data:\n                {\n                    sorted(\n                        map(\n                            lambda x: x.strftime(self.date_format),\n                            set(self.lookback_dates).difference(set(dates))\n                        )\n                    )\n                }\"\"\"\n\n            self.logger.error(error_msg)\n            raise ValueError(error_msg)\n\n    def get_lookback_period_statistics(self) -&gt; dict:\n        \"\"\"Method that computes the necessary statistics (average and standard deviation) from the\n        Quality Metrics of the lookback period.\n\n        Returns:\n            statistics (dict): dictionary containing said necessary statistics, with the following\n                structure:\n                {\n                    field_name1: {\n                        type_error1: {\n                            'average': 12.2,\n                            'stddev': 17.5\n                        },\n                        type_error2 : {\n                            'average': 4.5,\n                            'stddev': 10.1\n                        }\n                    },\n                    ...\n                }\n            initial_rows (dict): dictionary indicating the number of rows before syntactic checks for the previous period, {date -&gt; size_raw_data}\n            final_rows (dict): dictionary indicating the number of rows after syntactic checks for the previous period, {date -&gt; size_raw_data}\n        \"\"\"\n        intermediate_df = self.input_data_objects[SilverNetworkDataQualityMetricsByColumn.ID].df.filter(\n            F.col(ColNames.date).between(self.lookback_period_start, self.lookback_period_end)\n        )\n\n        intermediate_df.cache()\n\n        lookback_stats = (\n            intermediate_df.groupBy([ColNames.field_name, ColNames.type_code])\n            .agg(F.mean(F.col(ColNames.value)).alias(\"average\"), F.stddev_samp(F.col(ColNames.value)).alias(\"stddev\"))\n            .collect()\n        )\n\n        error_rate_data = (\n            intermediate_df.select([ColNames.type_code, ColNames.value, ColNames.date])\n            .filter(F.col(ColNames.type_code).isin([NetworkErrorType.INITIAL_ROWS, NetworkErrorType.FINAL_ROWS]))\n            .collect()\n        )\n\n        intermediate_df.unpersist()\n\n        initial_rows = {}\n        final_rows = {}\n\n        for row in error_rate_data:\n            if row[ColNames.type_code] == NetworkErrorType.INITIAL_ROWS:\n                initial_rows[row[ColNames.date]] = row[\"value\"]\n            else:\n                final_rows[row[ColNames.date]] = row[\"value\"]\n\n        statistics = dict()\n        for row in lookback_stats:\n            field_name, type_code, average, stddev = (\n                row[ColNames.field_name],\n                row[ColNames.type_code],\n                row[\"average\"],\n                row[\"stddev\"],\n            )\n            if field_name not in statistics:\n                statistics[field_name] = dict()\n            statistics[field_name][type_code] = {\"average\": average, \"stddev\": stddev}\n\n        return statistics, initial_rows, final_rows\n\n    def get_study_date_values(self) -&gt; dict:\n        \"\"\"Method that reads and returns the quality metrics of the date of study.\n\n        Returns:\n            today_values (dict): dictionary containing said values, with the following\n                structure:\n                {\n                    field_name1: {\n                        type_error1: 123,\n                        type_error2: 23,\n                        type_error3: 0\n                    },\n                    field_name2: {\n                        type_error1: 0,\n                        type_error2: 0,\n                        type_error3: 300\n                    },\n                }\n        \"\"\"\n        today_metrics = (\n            self.input_data_objects[SilverNetworkDataQualityMetricsByColumn.ID]\n            .df.filter(F.col(ColNames.date) == F.lit(self.date_of_study))\n            .select([ColNames.field_name, ColNames.type_code, ColNames.value])\n        ).collect()\n\n        today_values = {}\n\n        for row in today_metrics:\n            field_name, type_code, value = row[ColNames.field_name], row[ColNames.type_code], row[ColNames.value]\n\n            if field_name not in today_values:\n                today_values[field_name] = {}\n\n            today_values[field_name][type_code] = value\n\n        return today_values\n\n    def register_warning(\n        self, measure_definition: str, daily_value: float, condition: str, condition_value: float, warning_text: str\n    ) -&gt; None:\n        \"\"\"Method that abstracts away the creation in the correct format and data type, each of the quality warnings\n        that will be recorded in the log table.\n\n        Args:\n            measure_definition (str): measure that raised the warning (e.g. Error rate)\n            daily_value (float): measure's value in the date of study that raised the warning\n            condition (str): test that was checked in order to raise the warning\n            condition_value (float): value against which the date of study's daily_value was compared\n            warning_text (str): verbose explanation of the condition being satisfied and the warning\n                being raised\n        \"\"\"\n        warning = {\n            ColNames.title: self.TITLE,\n            ColNames.date: self.date_of_study,\n            ColNames.timestamp: self.timestamp,\n            ColNames.measure_definition: measure_definition,\n            ColNames.daily_value: float(daily_value),\n            ColNames.condition: condition,\n            ColNames.lookback_period: self.lookback_period,\n            ColNames.condition_value: float(condition_value),\n            ColNames.warning_text: warning_text,\n            ColNames.year: self.date_of_study.year,\n            ColNames.month: self.date_of_study.month,\n            ColNames.day: self.date_of_study.day,\n        }\n\n        self.warnings.append(Row(**warning))\n\n    def raw_size_warnings(self, lookback_stats, today_values):\n        \"\"\"Method that generates the quality warnings, that will be recorded in the output log table,\n        for the metric regarding the initial number of rows of the raw input network topology data prior\n        to the syntactic check procedure.\n\n        A total of six warnings might be generated:\n            - The study date's number of rows is greater than the average number of rows over the previous period\n                by more than a threshold percentage.\n            - The study date's number of rows is smaller than the average number of rows over the previous period\n                by more than a threshold percentage.\n            - The study date's number of rows is greater than the average number of rows over the previous period\n                by a config-specified number of standard deviations.\n            - The study date's number of rows is smaller than the average number of rows over the previous period\n                by a config-specified number of standard deviations.\n            - The study date's number of rows is greater than a config-specified threshold.\n            - The study date's number of rows is smaller than a config-specified threshold.\n        \"\"\"\n        current_val = today_values[None][NetworkErrorType.INITIAL_ROWS]\n        previous_avg = lookback_stats[None][NetworkErrorType.INITIAL_ROWS][\"average\"]\n        previous_std = lookback_stats[None][NetworkErrorType.INITIAL_ROWS][\"stddev\"]\n\n        measure_definition = self.MEASURE_DEFINITION[\"SIZE_RAW_DATA\"]\n\n        over_average = self.thresholds[\"SIZE_RAW_DATA\"][\"OVER_AVERAGE\"]\n        under_average = self.thresholds[\"SIZE_RAW_DATA\"][\"UNDER_AVERAGE\"]\n        variability = self.thresholds[\"SIZE_RAW_DATA\"][\"VARIABILITY\"]\n        absolute_upper_control_limit = self.thresholds[\"SIZE_RAW_DATA\"][\"ABS_VALUE_UPPER_LIMIT\"]\n        absolute_lower_control_limit = self.thresholds[\"SIZE_RAW_DATA\"][\"ABS_VALUE_LOWER_LIMIT\"]\n\n        if previous_avg == 0:\n            self.logger.warning(\n                f\"\"\"\n                The average of the number of rows in the raw input network topology data is 0\n                for the lookback period especified ({self.lookback_period_start} to {self.lookback_period_end},\n                 both included)\n                \"\"\"\n            )\n        else:\n            pct_difference = 100 * current_val / previous_avg - 100\n            if pct_difference &gt; over_average:\n                self.register_warning(\n                    measure_definition,\n                    pct_difference,\n                    f\"Size is greater than the previous period's average by more than {over_average} %\",\n                    over_average,\n                    \"The number of cells is unexpectedly high with respect to the previous period, please check if there have been issues in the network\",\n                )\n\n            if pct_difference &lt; -under_average:\n                self.register_warning(\n                    measure_definition,\n                    pct_difference,\n                    f\"Size is smaller than the previous period's average by more than {under_average} %\",\n                    under_average,\n                    \"The number of cells is unexpectedly low with respect to the previous period, please check if there have been issues in the network.\",\n                )\n\n        upper_control_limit = previous_avg + variability * previous_std\n        lower_control_limit = previous_avg - variability * previous_std\n\n        if current_val &gt; upper_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                \"Size is out of the upper control limit calculated on the basis of average and standard deviation of the distribution \"\n                f\"of the size in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\",\n                upper_control_limit,\n                \"The number of cells is unexpectedly high with respect to the previous period, taking into account the usual \"\n                \"variability of the cell numbers, please check if there have been issues in the network.\",\n            )\n        if current_val &lt; lower_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                \"Size is out of the lower control limit calculated on the basis of average and standard deviation of the distribution \"\n                f\"of the size in the previous period. Lower control limit = (average - {variability}\u00b7stddev)\",\n                lower_control_limit,\n                \"The number of cells is unexpectedly low with respect to the previous period, taking into account the usual \"\n                \"variability of the cell numbers, please check if there have been issues in the network.\",\n            )\n\n        # Use of UCL and LCL as default values for the absolute limits\n        if absolute_upper_control_limit is None:\n            absolute_upper_control_limit = upper_control_limit\n        if absolute_lower_control_limit is None:\n            absolute_lower_control_limit = lower_control_limit\n\n        if current_val &gt; absolute_upper_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                f\"The size is over the threshold {absolute_upper_control_limit}\",\n                absolute_upper_control_limit,\n                \"The number of cells is over the threshold, please check if there have been changes in the network.\",\n            )\n        if current_val &lt; absolute_lower_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                f\"The size is under the threshold {absolute_lower_control_limit}\",\n                absolute_upper_control_limit,\n                \"The number of cells is under the threshold, please check if there have been changes in the network.\",\n            )\n\n        return previous_avg, upper_control_limit, lower_control_limit\n\n    def clean_size_warnings(self, lookback_stats, today_values):\n        \"\"\"Method that generates quality warnings, that will be recorded in the output log table,\n        for the metric regarding the final number of rows of the raw input network topology data prior\n        to the syntactic check procedure.\n\n        A total of six warnings might be generated:\n            - The study date's number of rows is greater than the average number of rows over the previous period\n                by more than a threshold percentage.\n            - The study date's number of rows is smaller than the average number of rows over the previous period\n                by more than a threshold percentage.\n            - The study date's number of rows is greater than the average number of rows over the previous period\n                by a config-specified number of standard deviations.\n            - The study date's number of rows is smaller than the average number of rows over the previous period\n                by a config-specified number of standard deviations.\n            - The study date's number of rows is greater than a config-specified threshold.\n            - The study date's number of rows is smaller than a config-specified threshold.\n        \"\"\"\n        current_val = today_values[None][NetworkErrorType.FINAL_ROWS]\n        previous_avg = lookback_stats[None][NetworkErrorType.FINAL_ROWS][\"average\"]\n        previous_std = lookback_stats[None][NetworkErrorType.FINAL_ROWS][\"stddev\"]\n\n        measure_definition = self.MEASURE_DEFINITION[\"SIZE_CLEAN_DATA\"]\n\n        over_average = self.thresholds[\"SIZE_CLEAN_DATA\"][\"OVER_AVERAGE\"]\n        under_average = self.thresholds[\"SIZE_CLEAN_DATA\"][\"UNDER_AVERAGE\"]\n        variability = self.thresholds[\"SIZE_CLEAN_DATA\"][\"VARIABILITY\"]\n        absolute_upper_control_limit = self.thresholds[\"SIZE_CLEAN_DATA\"][\"ABS_VALUE_UPPER_LIMIT\"]\n        absolute_lower_control_limit = self.thresholds[\"SIZE_CLEAN_DATA\"][\"ABS_VALUE_LOWER_LIMIT\"]\n\n        if previous_avg == 0:\n            self.logger.warning(\n                f\"\"\"\n                The average of the number of rows in the clean input network topology data after syntactic checks is 0\n                for the lookback period especified ({self.lookback_period_start} to {self.lookback_period_end}, \n                both included)\n                \"\"\"\n            )\n        else:\n            pct_difference = 100 * current_val / previous_avg - 100\n            if pct_difference &gt; over_average:\n                self.register_warning(\n                    measure_definition,\n                    pct_difference,\n                    f\"Size is greater than the previous period's average by {over_average} %\",\n                    over_average,\n                    \"The number of cells after the syntactic checks procedure is unexpectedly high with respect to the previous period.\",\n                )\n\n            if pct_difference &lt; -under_average:\n                self.register_warning(\n                    measure_definition,\n                    pct_difference,\n                    f\"Size is smaller than the previous period's average by {under_average} %\",\n                    under_average,\n                    \"The number of cells after the syntactic checks procedure is unexpectedly low with respect to the previous period.\",\n                )\n\n        upper_control_limit = previous_avg + variability * previous_std\n        lower_control_limit = previous_avg - variability * previous_std\n\n        if current_val &gt; upper_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                \"Size is out of the upper control limit calculated on the basis of average and standard deviation of the distribution \"\n                f\"of the size in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\",\n                upper_control_limit,\n                \"The number of cells is unexpectedly high with respect to the previous period, taking into account the usual \"\n                \"variability of the cell numbers, please check if there have been issues in the network.\",\n            )\n        if current_val &lt; lower_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                \"Size is out of the lower control limit calculated on the basis of average and standard deviation of the distribution \"\n                f\"of the size in the previous period. Lower control limit = (average - {variability}\u00b7stddev)\",\n                lower_control_limit,\n                \"The number of cells is unexpectedly low with respect to the previous period, taking into account the usual \"\n                \"variability of the cell numbers, please check if there have been issues in the network.\",\n            )\n\n        # Use of UCL and LCL as default values for the absolute limits\n        if absolute_upper_control_limit is None:\n            absolute_upper_control_limit = upper_control_limit\n        if absolute_lower_control_limit is None:\n            absolute_lower_control_limit = lower_control_limit\n\n        if current_val &gt; absolute_upper_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                f\"The size is over the threshold {absolute_upper_control_limit}\",\n                absolute_upper_control_limit,\n                \"The number of cells after the syntactic checks procedure is over the threshold.\",\n            )\n        if current_val &lt; absolute_lower_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                f\"The size is under the threshold {absolute_lower_control_limit}\",\n                absolute_upper_control_limit,\n                \"The number of cells after the syntactic checks procedure is under the threshold.\",\n            )\n\n        return previous_avg, upper_control_limit, lower_control_limit\n\n    def error_rate_warnings(self, initial_rows, final_rows, today_values):\n        \"\"\"Method that generates quality warnings, that will be recorded in the output log table,\n        for the metric regarding the error rate observed in the syntactic check procedure.\n\n        A total of three warnings might be generated:\n            - The study date's error rate is greater than the average number of rows over the previous period\n                by more than a threshold percentage.\n            - The study date's error rate is greater than the average number of rows over the previous period\n                by a config-specified number of standard deviations.\n            - The study date's error rate is greater than a config-specified threshold.\n        \"\"\"\n        if len(initial_rows) != len(final_rows):\n            raise ValueError(\n                \"Input Quality Metrics do not have information on the number of rows \"\n                \"before and after syntactic checks on all dates considered!\"\n            )\n\n        error_rate = {\n            date: 100 * (initial_rows[date] - final_rows[date]) / initial_rows[date] for date in initial_rows.keys()\n        }\n\n        current_val = (\n            100\n            * (today_values[None][NetworkErrorType.INITIAL_ROWS] - today_values[None][NetworkErrorType.FINAL_ROWS])\n            / today_values[None][NetworkErrorType.INITIAL_ROWS]\n        )\n        previous_avg = sum(error_rate[date] for date in self.lookback_dates) / len(self.lookback_dates)\n        previous_std = math.sqrt(\n            sum((error_rate[date] - previous_avg) ** 2 for date in self.lookback_dates) / (len(self.lookback_dates) - 1)\n        )\n\n        measure_definition = self.MEASURE_DEFINITION[\"TOTAL_ERROR_RATE\"]\n\n        over_average = self.thresholds[\"TOTAL_ERROR_RATE\"][\"OVER_AVERAGE\"]\n        variability = self.thresholds[\"TOTAL_ERROR_RATE\"][\"VARIABILITY\"]\n        absolute_upper_control_limit = self.thresholds[\"TOTAL_ERROR_RATE\"][\"ABS_VALUE_UPPER_LIMIT\"]\n\n        if previous_avg == 0:\n            self.logger.warning(\n                f\"\"\"\n                The average error rate in the input network topology data 0\n                for the lookback period especified ({self.lookback_period_start} to {self.lookback_period_end}, \n                both included)\n                \"\"\"\n            )\n        else:\n            pct_difference = 100 * current_val / previous_avg - 100\n            if pct_difference &gt; over_average:\n                self.register_warning(\n                    measure_definition,\n                    pct_difference,\n                    f\"Error rate is greater than the previous period's average by more than {over_average} %\",\n                    over_average,\n                    \"The error rate after the syntactic checks procedure is unexpectedly high with respect to the previous period.\",\n                )\n        upper_control_limit = previous_avg + variability * previous_std\n\n        if current_val &gt; upper_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                \"Error rate is over the upper control limit calculated on the basis of average and standard deviation of the distribution \"\n                f\"of the size in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\",\n                upper_control_limit,\n                \"The error rate after the syntactic checks procedure is unexpectedly high with respect to the previous period, taking into account the usual \"\n                \"variability.\",\n            )\n\n        if current_val &gt; absolute_upper_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                f\"The error rate is over the value {absolute_upper_control_limit}\",\n                absolute_upper_control_limit,\n                \"The error rate after the syntactic checks procedure is over the threshold.\",\n            )\n        error_rate[self.date_of_study] = current_val\n        return error_rate, previous_avg, upper_control_limit\n\n    def all_specific_error_warnings(self, lookback_stats, today_values):\n        \"\"\"Parent method for the creation of warnings for each type of error rate\n\n        lookback_stats (dict): contains error information of each date of the lookback period\n        today_values (dict): contains error information of the date of study\n        \"\"\"\n        error_rate_types = [\"Missing_value_RATE\", \"Out_of_range_RATE\", \"Parsing_error_RATE\"]\n\n        for error_rate_type in error_rate_types:\n            for field_name in self.thresholds[error_rate_type]:\n                self.specific_error_warnings(error_rate_type, field_name, lookback_stats, today_values)\n\n    def specific_error_warnings(self, error_rate_type, field_name, lookback_stats, today_values):\n        \"\"\"Method that generates the quality warnings, that will be recorded in the output log table,\n        for the metric regarding a specific error type considered in the network syntactic checks.\n\n        A total of three warnings might be generated:\n            - The study date's error rate for this error and this field is greater than the average number of rows over the previous period\n                by more than a threshold percentage.\n            - The study date's error rate for this error and this field is greater than the average number of rows over the previous period\n                by a config-specified number of standard deviations.\n            - The study date's error rate for this error and this field is greater than a config-specified threshold.\n        \"\"\"\n        if error_rate_type not in self.ERROR_TYPE:\n            raise ValueError(f\"Unknown error type for quality warning `{error_rate_type}`\")\n\n        network_error_type = self.ERROR_TYPE[error_rate_type]\n\n        over_average = self.thresholds[error_rate_type][field_name][\"AVERAGE\"]\n        variability = self.thresholds[error_rate_type][field_name][\"VARIABILITY\"]\n        absolute_upper_control_limit = self.thresholds[error_rate_type][field_name][\"ABS_VALUE_UPPER_LIMIT\"]\n\n        if network_error_type == NetworkErrorType.OUT_OF_RANGE and field_name is None:\n            field_name = \"dates\"\n        current_val = today_values[field_name][network_error_type]\n        previous_avg = lookback_stats[field_name][network_error_type][\"average\"]\n        previous_std = lookback_stats[field_name][network_error_type][\"stddev\"]\n\n        if previous_avg == 0:\n            self.logger.warning(\n                f\"\"\"\n                The average {error_rate_type} for field {field_name} in the input network topology data is 0 for the lookback period\n                especified ({self.lookback_period_start} to {self.lookback_period_end}, both included)\n                \"\"\"\n            )\n        else:\n            pct_difference = 100 * current_val / previous_avg - 100\n            if pct_difference &gt; over_average:\n                self.register_warning(\n                    self.MEASURE_DEFINITION[error_rate_type](field_name=field_name),\n                    pct_difference,\n                    self.CONDITION[error_rate_type][\"AVERAGE\"](field_name=field_name, value=over_average),\n                    over_average,\n                    self.WARNING_MESSAGE[error_rate_type][\"AVERAGE\"](field_name=field_name),\n                )\n        upper_control_limit = previous_avg + variability * previous_std\n\n        if current_val &gt; upper_control_limit:\n            self.register_warning(\n                self.MEASURE_DEFINITION[error_rate_type](field_name=field_name),\n                current_val,\n                self.CONDITION[error_rate_type][\"VARIABILITY\"](field_name=field_name, variability=variability),\n                variability,\n                self.WARNING_MESSAGE[error_rate_type][\"VARIABILITY\"](field_name=field_name),\n            )\n\n        if current_val &gt; absolute_upper_control_limit:\n            self.register_warning(\n                self.MEASURE_DEFINITION[error_rate_type](field_name=field_name),\n                current_val,\n                self.CONDITION[error_rate_type][\"ABS_VALUE_UPPER_LIMIT\"](\n                    field_name=field_name, value=absolute_upper_control_limit\n                ),\n                absolute_upper_control_limit,\n                self.WARNING_MESSAGE[error_rate_type][\"ABS_VALUE_UPPER_LIMIT\"](field_name=field_name),\n            )\n\n    def create_plots_data(\n        self,\n        lookback_initial_rows,\n        lookback_final_rows,\n        today_values,\n        error_rate,\n        raw_average,\n        clean_average,\n        error_rate_avg,\n        raw_UCL,\n        clean_UCL,\n        error_rate_UCL,\n        raw_LCL,\n        clean_LCL,\n    ):\n        \"\"\"Method that takes the data needed to generate the required plots, and formats it for\n        easier plotting and saving later on.\n\n        Args:\n            lookback_initial_rows (dict): contains data on the lookback period dates' number of rows before syntactic checks\n            lookback_final_rows (dict): contains data on the lookback period dates' number of rows after syntactic checks\n            today_values (dict): contains data on the date of study error counts and rows before and after the syntactic checks\n            error_rate (dict): cotains data on the error rates for all lookback dates and date of study.\n            raw_average (float): average of rows in the raw data before syntactic checks\n            clean_average (float): average of rows in the clean data after syntactic checks\n            error_rate_avg (float): average of the error rate observed in the syntactic checks\n            raw_UCL (float): upper control limit for the rows in the raw data before syntactic checks\n            clean_UCL (float): upper control limit for the rows in the clean data after syntactic checks\n            error_rate_UCL (float): upper control limit for the error rate\n            raw_LCL (float): lower control limit for the rows in the raw data before syntactic checks\n            clean_LCL (float): lower control limit for the rows in the raw data after syntactic checks\n        \"\"\"\n        # self.plots_data = {\"rows_before_syntactic_check\": [], \"rows_after_syntactic_check\": [], \"error_rate\": []}\n\n        plots_data = {\"line_plot\": [], \"pie_plot\": []}\n\n        for date in sorted(self.lookback_dates) + [self.date_of_study]:\n            plots_data[\"line_plot\"].extend(\n                [\n                    Row(\n                        **{\n                            ColNames.date: date,\n                            ColNames.daily_value: float(lookback_initial_rows[date]),\n                            ColNames.average: float(raw_average),\n                            ColNames.UCL: float(raw_UCL),\n                            ColNames.LCL: float(raw_LCL),\n                            ColNames.variable: \"rows_before_syntactic_check\",\n                            ColNames.year: self.date_of_study.year,\n                            ColNames.month: self.date_of_study.month,\n                            ColNames.day: self.date_of_study.day,\n                            ColNames.timestamp: self.timestamp,\n                        }\n                    )\n                ]\n            )\n\n            plots_data[\"line_plot\"].extend(\n                [\n                    Row(\n                        **{\n                            ColNames.date: date,\n                            ColNames.daily_value: float(lookback_final_rows[date]),\n                            ColNames.average: float(clean_average),\n                            ColNames.UCL: float(clean_UCL),\n                            ColNames.LCL: float(clean_LCL),\n                            ColNames.variable: \"rows_after_syntactic_check\",\n                            ColNames.year: self.date_of_study.year,\n                            ColNames.month: self.date_of_study.month,\n                            ColNames.day: self.date_of_study.day,\n                            ColNames.timestamp: self.timestamp,\n                        }\n                    )\n                ]\n            )\n\n            plots_data[\"line_plot\"].extend(\n                [\n                    Row(\n                        **{\n                            ColNames.date: date,\n                            ColNames.daily_value: float(error_rate[date]),\n                            ColNames.average: float(error_rate_avg),\n                            ColNames.UCL: float(error_rate_UCL),\n                            ColNames.LCL: None,\n                            ColNames.variable: \"error_rate\",\n                            ColNames.year: self.date_of_study.year,\n                            ColNames.month: self.date_of_study.month,\n                            ColNames.day: self.date_of_study.day,\n                            ColNames.timestamp: self.timestamp,\n                        }\n                    )\n                ]\n            )\n\n        # Now, the data for the pie charts\n        # Ugly way to get the relation error_code -&gt; error attribute name\n        error_types = {getattr(NetworkErrorType, att): att for att in dir(NetworkErrorType) if not att.startswith(\"__\")}\n\n        for field_name, error_counts in today_values.items():\n            if field_name in [None, \"dates\"]:\n                continue\n\n            # boolean check if this field had any errors or not\n            field_has_errors = False\n\n            for key in error_counts.keys():\n                if key != NetworkErrorType.NO_ERROR:\n                    if error_counts[key] &gt; 0:\n                        # self.plots_data[field_name].append(\n                        #     field_name, error_types[key], error_counts[key]\n                        # )\n                        field_has_errors = True\n                        plots_data[\"pie_plot\"].append(\n                            Row(\n                                **{\n                                    ColNames.type_code: error_types[key],\n                                    ColNames.value: error_counts[key],\n                                    ColNames.variable: field_name,\n                                    ColNames.year: self.date_of_study.year,\n                                    ColNames.month: self.date_of_study.month,\n                                    ColNames.day: self.date_of_study.day,\n                                    ColNames.timestamp: self.timestamp,\n                                }\n                            )\n                        )\n            if not field_has_errors:\n                self.logger.info(f\"Field `{field_name}` had no errors\")\n        self.output_data_objects[SilverNetworkSyntacticQualityWarningsLinePlotData.ID].df = self.spark.createDataFrame(\n            plots_data[\"line_plot\"], schema=SilverNetworkSyntacticQualityWarningsLinePlotData.SCHEMA\n        )\n\n        self.output_data_objects[SilverNetworkSyntacticQualityWarningsPiePlotData.ID].df = self.spark.createDataFrame(\n            plots_data[\"pie_plot\"], schema=SilverNetworkSyntacticQualityWarningsPiePlotData.SCHEMA\n        )\n\n    def write(self):\n        # Write regular data objects\n        self.logger.info(\"Writing data objects...\")\n        super().write()\n        self.logger.info(\"Data objects written.\")\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.all_specific_error_warnings","title":"<code>all_specific_error_warnings(lookback_stats, today_values)</code>","text":"<p>Parent method for the creation of warnings for each type of error rate</p> <p>lookback_stats (dict): contains error information of each date of the lookback period today_values (dict): contains error information of the date of study</p> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def all_specific_error_warnings(self, lookback_stats, today_values):\n    \"\"\"Parent method for the creation of warnings for each type of error rate\n\n    lookback_stats (dict): contains error information of each date of the lookback period\n    today_values (dict): contains error information of the date of study\n    \"\"\"\n    error_rate_types = [\"Missing_value_RATE\", \"Out_of_range_RATE\", \"Parsing_error_RATE\"]\n\n    for error_rate_type in error_rate_types:\n        for field_name in self.thresholds[error_rate_type]:\n            self.specific_error_warnings(error_rate_type, field_name, lookback_stats, today_values)\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.check_needed_dates","title":"<code>check_needed_dates()</code>","text":"<p>Method that checks if both the date of study and the dates necessary to generate the quality warnings, specified through the lookback_period parameter, are present in the input data.</p> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def check_needed_dates(self) -&gt; None:\n    \"\"\"\n    Method that checks if both the date of study and the dates necessary to generate\n    the quality warnings, specified through the lookback_period parameter, are present\n    in the input data.\n    \"\"\"\n\n    # Collect all distinct dates in the input quality metrics within the needed range\n    metrics = self.input_data_objects[SilverNetworkDataQualityMetricsByColumn.ID].df\n    dates = (\n        metrics.filter(\n            F.col(\"date\")\n            # left- and right- inclusive\n            .between(\n                self.date_of_study - datetime.timedelta(days=self.PERIOD_DURATION[self.lookback_period]),\n                self.date_of_study,\n            )\n        )\n        .select(F.col(ColNames.date))\n        .distinct()\n        .collect()\n    )\n\n    dates = [row[ColNames.date] for row in dates]\n\n    if self.date_of_study not in dates:\n        raise ValueError(\n            f\"The date of study, {self.date_of_study}, is not present in the input Quality Metrics data\"\n        )\n\n    if set(self.lookback_dates).intersection(set(dates)) != set(self.lookback_dates):\n        error_msg = f\"\"\"\n            The following dates from the lookback period are not present in the\n            input Quality Metrics data:\n            {\n                sorted(\n                    map(\n                        lambda x: x.strftime(self.date_format),\n                        set(self.lookback_dates).difference(set(dates))\n                    )\n                )\n            }\"\"\"\n\n        self.logger.error(error_msg)\n        raise ValueError(error_msg)\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.clean_size_warnings","title":"<code>clean_size_warnings(lookback_stats, today_values)</code>","text":"<p>Method that generates quality warnings, that will be recorded in the output log table, for the metric regarding the final number of rows of the raw input network topology data prior to the syntactic check procedure.</p> A total of six warnings might be generated <ul> <li>The study date's number of rows is greater than the average number of rows over the previous period     by more than a threshold percentage.</li> <li>The study date's number of rows is smaller than the average number of rows over the previous period     by more than a threshold percentage.</li> <li>The study date's number of rows is greater than the average number of rows over the previous period     by a config-specified number of standard deviations.</li> <li>The study date's number of rows is smaller than the average number of rows over the previous period     by a config-specified number of standard deviations.</li> <li>The study date's number of rows is greater than a config-specified threshold.</li> <li>The study date's number of rows is smaller than a config-specified threshold.</li> </ul> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def clean_size_warnings(self, lookback_stats, today_values):\n    \"\"\"Method that generates quality warnings, that will be recorded in the output log table,\n    for the metric regarding the final number of rows of the raw input network topology data prior\n    to the syntactic check procedure.\n\n    A total of six warnings might be generated:\n        - The study date's number of rows is greater than the average number of rows over the previous period\n            by more than a threshold percentage.\n        - The study date's number of rows is smaller than the average number of rows over the previous period\n            by more than a threshold percentage.\n        - The study date's number of rows is greater than the average number of rows over the previous period\n            by a config-specified number of standard deviations.\n        - The study date's number of rows is smaller than the average number of rows over the previous period\n            by a config-specified number of standard deviations.\n        - The study date's number of rows is greater than a config-specified threshold.\n        - The study date's number of rows is smaller than a config-specified threshold.\n    \"\"\"\n    current_val = today_values[None][NetworkErrorType.FINAL_ROWS]\n    previous_avg = lookback_stats[None][NetworkErrorType.FINAL_ROWS][\"average\"]\n    previous_std = lookback_stats[None][NetworkErrorType.FINAL_ROWS][\"stddev\"]\n\n    measure_definition = self.MEASURE_DEFINITION[\"SIZE_CLEAN_DATA\"]\n\n    over_average = self.thresholds[\"SIZE_CLEAN_DATA\"][\"OVER_AVERAGE\"]\n    under_average = self.thresholds[\"SIZE_CLEAN_DATA\"][\"UNDER_AVERAGE\"]\n    variability = self.thresholds[\"SIZE_CLEAN_DATA\"][\"VARIABILITY\"]\n    absolute_upper_control_limit = self.thresholds[\"SIZE_CLEAN_DATA\"][\"ABS_VALUE_UPPER_LIMIT\"]\n    absolute_lower_control_limit = self.thresholds[\"SIZE_CLEAN_DATA\"][\"ABS_VALUE_LOWER_LIMIT\"]\n\n    if previous_avg == 0:\n        self.logger.warning(\n            f\"\"\"\n            The average of the number of rows in the clean input network topology data after syntactic checks is 0\n            for the lookback period especified ({self.lookback_period_start} to {self.lookback_period_end}, \n            both included)\n            \"\"\"\n        )\n    else:\n        pct_difference = 100 * current_val / previous_avg - 100\n        if pct_difference &gt; over_average:\n            self.register_warning(\n                measure_definition,\n                pct_difference,\n                f\"Size is greater than the previous period's average by {over_average} %\",\n                over_average,\n                \"The number of cells after the syntactic checks procedure is unexpectedly high with respect to the previous period.\",\n            )\n\n        if pct_difference &lt; -under_average:\n            self.register_warning(\n                measure_definition,\n                pct_difference,\n                f\"Size is smaller than the previous period's average by {under_average} %\",\n                under_average,\n                \"The number of cells after the syntactic checks procedure is unexpectedly low with respect to the previous period.\",\n            )\n\n    upper_control_limit = previous_avg + variability * previous_std\n    lower_control_limit = previous_avg - variability * previous_std\n\n    if current_val &gt; upper_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            \"Size is out of the upper control limit calculated on the basis of average and standard deviation of the distribution \"\n            f\"of the size in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\",\n            upper_control_limit,\n            \"The number of cells is unexpectedly high with respect to the previous period, taking into account the usual \"\n            \"variability of the cell numbers, please check if there have been issues in the network.\",\n        )\n    if current_val &lt; lower_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            \"Size is out of the lower control limit calculated on the basis of average and standard deviation of the distribution \"\n            f\"of the size in the previous period. Lower control limit = (average - {variability}\u00b7stddev)\",\n            lower_control_limit,\n            \"The number of cells is unexpectedly low with respect to the previous period, taking into account the usual \"\n            \"variability of the cell numbers, please check if there have been issues in the network.\",\n        )\n\n    # Use of UCL and LCL as default values for the absolute limits\n    if absolute_upper_control_limit is None:\n        absolute_upper_control_limit = upper_control_limit\n    if absolute_lower_control_limit is None:\n        absolute_lower_control_limit = lower_control_limit\n\n    if current_val &gt; absolute_upper_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            f\"The size is over the threshold {absolute_upper_control_limit}\",\n            absolute_upper_control_limit,\n            \"The number of cells after the syntactic checks procedure is over the threshold.\",\n        )\n    if current_val &lt; absolute_lower_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            f\"The size is under the threshold {absolute_lower_control_limit}\",\n            absolute_upper_control_limit,\n            \"The number of cells after the syntactic checks procedure is under the threshold.\",\n        )\n\n    return previous_avg, upper_control_limit, lower_control_limit\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.create_plots_data","title":"<code>create_plots_data(lookback_initial_rows, lookback_final_rows, today_values, error_rate, raw_average, clean_average, error_rate_avg, raw_UCL, clean_UCL, error_rate_UCL, raw_LCL, clean_LCL)</code>","text":"<p>Method that takes the data needed to generate the required plots, and formats it for easier plotting and saving later on.</p> <p>Parameters:</p> Name Type Description Default <code>lookback_initial_rows</code> <code>dict</code> <p>contains data on the lookback period dates' number of rows before syntactic checks</p> required <code>lookback_final_rows</code> <code>dict</code> <p>contains data on the lookback period dates' number of rows after syntactic checks</p> required <code>today_values</code> <code>dict</code> <p>contains data on the date of study error counts and rows before and after the syntactic checks</p> required <code>error_rate</code> <code>dict</code> <p>cotains data on the error rates for all lookback dates and date of study.</p> required <code>raw_average</code> <code>float</code> <p>average of rows in the raw data before syntactic checks</p> required <code>clean_average</code> <code>float</code> <p>average of rows in the clean data after syntactic checks</p> required <code>error_rate_avg</code> <code>float</code> <p>average of the error rate observed in the syntactic checks</p> required <code>raw_UCL</code> <code>float</code> <p>upper control limit for the rows in the raw data before syntactic checks</p> required <code>clean_UCL</code> <code>float</code> <p>upper control limit for the rows in the clean data after syntactic checks</p> required <code>error_rate_UCL</code> <code>float</code> <p>upper control limit for the error rate</p> required <code>raw_LCL</code> <code>float</code> <p>lower control limit for the rows in the raw data before syntactic checks</p> required <code>clean_LCL</code> <code>float</code> <p>lower control limit for the rows in the raw data after syntactic checks</p> required Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def create_plots_data(\n    self,\n    lookback_initial_rows,\n    lookback_final_rows,\n    today_values,\n    error_rate,\n    raw_average,\n    clean_average,\n    error_rate_avg,\n    raw_UCL,\n    clean_UCL,\n    error_rate_UCL,\n    raw_LCL,\n    clean_LCL,\n):\n    \"\"\"Method that takes the data needed to generate the required plots, and formats it for\n    easier plotting and saving later on.\n\n    Args:\n        lookback_initial_rows (dict): contains data on the lookback period dates' number of rows before syntactic checks\n        lookback_final_rows (dict): contains data on the lookback period dates' number of rows after syntactic checks\n        today_values (dict): contains data on the date of study error counts and rows before and after the syntactic checks\n        error_rate (dict): cotains data on the error rates for all lookback dates and date of study.\n        raw_average (float): average of rows in the raw data before syntactic checks\n        clean_average (float): average of rows in the clean data after syntactic checks\n        error_rate_avg (float): average of the error rate observed in the syntactic checks\n        raw_UCL (float): upper control limit for the rows in the raw data before syntactic checks\n        clean_UCL (float): upper control limit for the rows in the clean data after syntactic checks\n        error_rate_UCL (float): upper control limit for the error rate\n        raw_LCL (float): lower control limit for the rows in the raw data before syntactic checks\n        clean_LCL (float): lower control limit for the rows in the raw data after syntactic checks\n    \"\"\"\n    # self.plots_data = {\"rows_before_syntactic_check\": [], \"rows_after_syntactic_check\": [], \"error_rate\": []}\n\n    plots_data = {\"line_plot\": [], \"pie_plot\": []}\n\n    for date in sorted(self.lookback_dates) + [self.date_of_study]:\n        plots_data[\"line_plot\"].extend(\n            [\n                Row(\n                    **{\n                        ColNames.date: date,\n                        ColNames.daily_value: float(lookback_initial_rows[date]),\n                        ColNames.average: float(raw_average),\n                        ColNames.UCL: float(raw_UCL),\n                        ColNames.LCL: float(raw_LCL),\n                        ColNames.variable: \"rows_before_syntactic_check\",\n                        ColNames.year: self.date_of_study.year,\n                        ColNames.month: self.date_of_study.month,\n                        ColNames.day: self.date_of_study.day,\n                        ColNames.timestamp: self.timestamp,\n                    }\n                )\n            ]\n        )\n\n        plots_data[\"line_plot\"].extend(\n            [\n                Row(\n                    **{\n                        ColNames.date: date,\n                        ColNames.daily_value: float(lookback_final_rows[date]),\n                        ColNames.average: float(clean_average),\n                        ColNames.UCL: float(clean_UCL),\n                        ColNames.LCL: float(clean_LCL),\n                        ColNames.variable: \"rows_after_syntactic_check\",\n                        ColNames.year: self.date_of_study.year,\n                        ColNames.month: self.date_of_study.month,\n                        ColNames.day: self.date_of_study.day,\n                        ColNames.timestamp: self.timestamp,\n                    }\n                )\n            ]\n        )\n\n        plots_data[\"line_plot\"].extend(\n            [\n                Row(\n                    **{\n                        ColNames.date: date,\n                        ColNames.daily_value: float(error_rate[date]),\n                        ColNames.average: float(error_rate_avg),\n                        ColNames.UCL: float(error_rate_UCL),\n                        ColNames.LCL: None,\n                        ColNames.variable: \"error_rate\",\n                        ColNames.year: self.date_of_study.year,\n                        ColNames.month: self.date_of_study.month,\n                        ColNames.day: self.date_of_study.day,\n                        ColNames.timestamp: self.timestamp,\n                    }\n                )\n            ]\n        )\n\n    # Now, the data for the pie charts\n    # Ugly way to get the relation error_code -&gt; error attribute name\n    error_types = {getattr(NetworkErrorType, att): att for att in dir(NetworkErrorType) if not att.startswith(\"__\")}\n\n    for field_name, error_counts in today_values.items():\n        if field_name in [None, \"dates\"]:\n            continue\n\n        # boolean check if this field had any errors or not\n        field_has_errors = False\n\n        for key in error_counts.keys():\n            if key != NetworkErrorType.NO_ERROR:\n                if error_counts[key] &gt; 0:\n                    # self.plots_data[field_name].append(\n                    #     field_name, error_types[key], error_counts[key]\n                    # )\n                    field_has_errors = True\n                    plots_data[\"pie_plot\"].append(\n                        Row(\n                            **{\n                                ColNames.type_code: error_types[key],\n                                ColNames.value: error_counts[key],\n                                ColNames.variable: field_name,\n                                ColNames.year: self.date_of_study.year,\n                                ColNames.month: self.date_of_study.month,\n                                ColNames.day: self.date_of_study.day,\n                                ColNames.timestamp: self.timestamp,\n                            }\n                        )\n                    )\n        if not field_has_errors:\n            self.logger.info(f\"Field `{field_name}` had no errors\")\n    self.output_data_objects[SilverNetworkSyntacticQualityWarningsLinePlotData.ID].df = self.spark.createDataFrame(\n        plots_data[\"line_plot\"], schema=SilverNetworkSyntacticQualityWarningsLinePlotData.SCHEMA\n    )\n\n    self.output_data_objects[SilverNetworkSyntacticQualityWarningsPiePlotData.ID].df = self.spark.createDataFrame(\n        plots_data[\"pie_plot\"], schema=SilverNetworkSyntacticQualityWarningsPiePlotData.SCHEMA\n    )\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.error_rate_warnings","title":"<code>error_rate_warnings(initial_rows, final_rows, today_values)</code>","text":"<p>Method that generates quality warnings, that will be recorded in the output log table, for the metric regarding the error rate observed in the syntactic check procedure.</p> A total of three warnings might be generated <ul> <li>The study date's error rate is greater than the average number of rows over the previous period     by more than a threshold percentage.</li> <li>The study date's error rate is greater than the average number of rows over the previous period     by a config-specified number of standard deviations.</li> <li>The study date's error rate is greater than a config-specified threshold.</li> </ul> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def error_rate_warnings(self, initial_rows, final_rows, today_values):\n    \"\"\"Method that generates quality warnings, that will be recorded in the output log table,\n    for the metric regarding the error rate observed in the syntactic check procedure.\n\n    A total of three warnings might be generated:\n        - The study date's error rate is greater than the average number of rows over the previous period\n            by more than a threshold percentage.\n        - The study date's error rate is greater than the average number of rows over the previous period\n            by a config-specified number of standard deviations.\n        - The study date's error rate is greater than a config-specified threshold.\n    \"\"\"\n    if len(initial_rows) != len(final_rows):\n        raise ValueError(\n            \"Input Quality Metrics do not have information on the number of rows \"\n            \"before and after syntactic checks on all dates considered!\"\n        )\n\n    error_rate = {\n        date: 100 * (initial_rows[date] - final_rows[date]) / initial_rows[date] for date in initial_rows.keys()\n    }\n\n    current_val = (\n        100\n        * (today_values[None][NetworkErrorType.INITIAL_ROWS] - today_values[None][NetworkErrorType.FINAL_ROWS])\n        / today_values[None][NetworkErrorType.INITIAL_ROWS]\n    )\n    previous_avg = sum(error_rate[date] for date in self.lookback_dates) / len(self.lookback_dates)\n    previous_std = math.sqrt(\n        sum((error_rate[date] - previous_avg) ** 2 for date in self.lookback_dates) / (len(self.lookback_dates) - 1)\n    )\n\n    measure_definition = self.MEASURE_DEFINITION[\"TOTAL_ERROR_RATE\"]\n\n    over_average = self.thresholds[\"TOTAL_ERROR_RATE\"][\"OVER_AVERAGE\"]\n    variability = self.thresholds[\"TOTAL_ERROR_RATE\"][\"VARIABILITY\"]\n    absolute_upper_control_limit = self.thresholds[\"TOTAL_ERROR_RATE\"][\"ABS_VALUE_UPPER_LIMIT\"]\n\n    if previous_avg == 0:\n        self.logger.warning(\n            f\"\"\"\n            The average error rate in the input network topology data 0\n            for the lookback period especified ({self.lookback_period_start} to {self.lookback_period_end}, \n            both included)\n            \"\"\"\n        )\n    else:\n        pct_difference = 100 * current_val / previous_avg - 100\n        if pct_difference &gt; over_average:\n            self.register_warning(\n                measure_definition,\n                pct_difference,\n                f\"Error rate is greater than the previous period's average by more than {over_average} %\",\n                over_average,\n                \"The error rate after the syntactic checks procedure is unexpectedly high with respect to the previous period.\",\n            )\n    upper_control_limit = previous_avg + variability * previous_std\n\n    if current_val &gt; upper_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            \"Error rate is over the upper control limit calculated on the basis of average and standard deviation of the distribution \"\n            f\"of the size in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\",\n            upper_control_limit,\n            \"The error rate after the syntactic checks procedure is unexpectedly high with respect to the previous period, taking into account the usual \"\n            \"variability.\",\n        )\n\n    if current_val &gt; absolute_upper_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            f\"The error rate is over the value {absolute_upper_control_limit}\",\n            absolute_upper_control_limit,\n            \"The error rate after the syntactic checks procedure is over the threshold.\",\n        )\n    error_rate[self.date_of_study] = current_val\n    return error_rate, previous_avg, upper_control_limit\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.get_lookback_period_statistics","title":"<code>get_lookback_period_statistics()</code>","text":"<p>Method that computes the necessary statistics (average and standard deviation) from the Quality Metrics of the lookback period.</p> <p>Returns:</p> Name Type Description <code>statistics</code> <code>dict</code> <p>dictionary containing said necessary statistics, with the following structure: {     field_name1: {         type_error1: {             'average': 12.2,             'stddev': 17.5         },         type_error2 : {             'average': 4.5,             'stddev': 10.1         }     },     ... }</p> <code>initial_rows</code> <code>dict</code> <p>dictionary indicating the number of rows before syntactic checks for the previous period, {date -&gt; size_raw_data}</p> <code>final_rows</code> <code>dict</code> <p>dictionary indicating the number of rows after syntactic checks for the previous period, {date -&gt; size_raw_data}</p> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def get_lookback_period_statistics(self) -&gt; dict:\n    \"\"\"Method that computes the necessary statistics (average and standard deviation) from the\n    Quality Metrics of the lookback period.\n\n    Returns:\n        statistics (dict): dictionary containing said necessary statistics, with the following\n            structure:\n            {\n                field_name1: {\n                    type_error1: {\n                        'average': 12.2,\n                        'stddev': 17.5\n                    },\n                    type_error2 : {\n                        'average': 4.5,\n                        'stddev': 10.1\n                    }\n                },\n                ...\n            }\n        initial_rows (dict): dictionary indicating the number of rows before syntactic checks for the previous period, {date -&gt; size_raw_data}\n        final_rows (dict): dictionary indicating the number of rows after syntactic checks for the previous period, {date -&gt; size_raw_data}\n    \"\"\"\n    intermediate_df = self.input_data_objects[SilverNetworkDataQualityMetricsByColumn.ID].df.filter(\n        F.col(ColNames.date).between(self.lookback_period_start, self.lookback_period_end)\n    )\n\n    intermediate_df.cache()\n\n    lookback_stats = (\n        intermediate_df.groupBy([ColNames.field_name, ColNames.type_code])\n        .agg(F.mean(F.col(ColNames.value)).alias(\"average\"), F.stddev_samp(F.col(ColNames.value)).alias(\"stddev\"))\n        .collect()\n    )\n\n    error_rate_data = (\n        intermediate_df.select([ColNames.type_code, ColNames.value, ColNames.date])\n        .filter(F.col(ColNames.type_code).isin([NetworkErrorType.INITIAL_ROWS, NetworkErrorType.FINAL_ROWS]))\n        .collect()\n    )\n\n    intermediate_df.unpersist()\n\n    initial_rows = {}\n    final_rows = {}\n\n    for row in error_rate_data:\n        if row[ColNames.type_code] == NetworkErrorType.INITIAL_ROWS:\n            initial_rows[row[ColNames.date]] = row[\"value\"]\n        else:\n            final_rows[row[ColNames.date]] = row[\"value\"]\n\n    statistics = dict()\n    for row in lookback_stats:\n        field_name, type_code, average, stddev = (\n            row[ColNames.field_name],\n            row[ColNames.type_code],\n            row[\"average\"],\n            row[\"stddev\"],\n        )\n        if field_name not in statistics:\n            statistics[field_name] = dict()\n        statistics[field_name][type_code] = {\"average\": average, \"stddev\": stddev}\n\n    return statistics, initial_rows, final_rows\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.get_study_date_values","title":"<code>get_study_date_values()</code>","text":"<p>Method that reads and returns the quality metrics of the date of study.</p> <p>Returns:</p> Name Type Description <code>today_values</code> <code>dict</code> <p>dictionary containing said values, with the following structure: {     field_name1: {         type_error1: 123,         type_error2: 23,         type_error3: 0     },     field_name2: {         type_error1: 0,         type_error2: 0,         type_error3: 300     }, }</p> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def get_study_date_values(self) -&gt; dict:\n    \"\"\"Method that reads and returns the quality metrics of the date of study.\n\n    Returns:\n        today_values (dict): dictionary containing said values, with the following\n            structure:\n            {\n                field_name1: {\n                    type_error1: 123,\n                    type_error2: 23,\n                    type_error3: 0\n                },\n                field_name2: {\n                    type_error1: 0,\n                    type_error2: 0,\n                    type_error3: 300\n                },\n            }\n    \"\"\"\n    today_metrics = (\n        self.input_data_objects[SilverNetworkDataQualityMetricsByColumn.ID]\n        .df.filter(F.col(ColNames.date) == F.lit(self.date_of_study))\n        .select([ColNames.field_name, ColNames.type_code, ColNames.value])\n    ).collect()\n\n    today_values = {}\n\n    for row in today_metrics:\n        field_name, type_code, value = row[ColNames.field_name], row[ColNames.type_code], row[ColNames.value]\n\n        if field_name not in today_values:\n            today_values[field_name] = {}\n\n        today_values[field_name][type_code] = value\n\n    return today_values\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.get_thresholds","title":"<code>get_thresholds()</code>","text":"<p>Method that reads the threshold-related parameters, contained in the config file, and saves them in memory to use them instead of the ones specified by default. Raises:     ValueError: non-numerical value that cannot be parsed to float has been used in         the config file     ValueError: Negative value for a given parameter has been given, when only         non-negative values make sense and are thus allowed.</p> <p>Returns:</p> Name Type Description <code>thresholds</code> <code>dict</code> <p>dict containing the different thresholds used for computing the quality warnings, with the same structure as multimno.core.constants.network_default_thresholds.NETWORK_DEFAULT_THRESHOLDS</p> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def get_thresholds(self) -&gt; dict:\n    \"\"\"\n    Method that reads the threshold-related parameters, contained in the config file,\n    and saves them in memory to use them instead of the ones specified by default.\n    Raises:\n        ValueError: non-numerical value that cannot be parsed to float has been used in\n            the config file\n        ValueError: Negative value for a given parameter has been given, when only\n            non-negative values make sense and are thus allowed.\n\n    Returns:\n        thresholds: dict containing the different thresholds used for computing the\n            quality warnings, with the same structure as\n            multimno.core.constants.network_default_thresholds.NETWORK_DEFAULT_THRESHOLDS\n    \"\"\"\n    config_thresholds = self.config.geteval(self.COMPONENT_ID, \"thresholds\")\n    thresholds = NETWORK_DEFAULT_THRESHOLDS\n\n    for error_key in config_thresholds.keys():\n        if error_key not in NETWORK_DEFAULT_THRESHOLDS:\n            self.logger.info(f\"Parameter key {error_key} unknown -- ignored\")\n            continue\n\n        if error_key in [\"SIZE_RAW_DATA\", \"SIZE_CLEAN_DATA\", \"TOTAL_ERROR_RATE\"]:\n            for param_key, val in config_thresholds[error_key].items():\n                if param_key not in NETWORK_DEFAULT_THRESHOLDS[error_key]:\n                    self.logger.info(f\"Unknown parameter {param_key} under {error_key} -- ignored\")\n                    continue\n\n                try:\n                    val = float(val)\n                except ValueError as e:\n                    error_msg = (\n                        f\"Expected numeric value for parameter {param_key} under {error_key} - got {val} instead.\"\n                    )\n                    self.logger.error(error_msg)\n                    raise e\n\n                if val &lt; 0:\n                    error_msg = f\"Parameter {param_key} under {error_key} must be non-negative - got {val}\"\n                    self.logger.error(error_msg)\n                    raise ValueError(error_msg)\n\n                thresholds[error_key][param_key] = val\n\n        else:  # then is one of 'Missing_value_RATE', 'Out_of_range_RATE', 'Parsing_error_RATE'\n            for field_name in config_thresholds[error_key]:\n                if field_name not in NETWORK_DEFAULT_THRESHOLDS[error_key]:\n                    self.logger.info(f\"Unknown field name {field_name} under {error_key} -- ignored\")\n                    continue\n\n                for param_key, val in config_thresholds[error_key][field_name].items():\n                    if param_key not in NETWORK_DEFAULT_THRESHOLDS[error_key][field_name]:\n                        self.logger.info(\n                            f'Unknown parameter {param_key} under {error_key}[\"{field_name}\"] -- ignored'\n                        )\n                        continue\n\n                    try:\n                        val = float(val)\n                    except ValueError as e:\n                        error_msg = f'Expected numeric value for parameter {param_key} under {error_key}[\"{field_name}\"] - got {val} instead.'\n                        self.logger.error(error_msg)\n                        raise e\n\n                    if val &lt; 0:\n                        error_msg = f'Parameter {param_key} under {error_key}[\"{field_name}\"] must be non-negative - got {val}'\n                        self.logger.error(error_msg)\n                        raise ValueError(error_msg)\n\n                    thresholds[error_key][field_name][param_key] = val\n\n    return thresholds\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.raw_size_warnings","title":"<code>raw_size_warnings(lookback_stats, today_values)</code>","text":"<p>Method that generates the quality warnings, that will be recorded in the output log table, for the metric regarding the initial number of rows of the raw input network topology data prior to the syntactic check procedure.</p> A total of six warnings might be generated <ul> <li>The study date's number of rows is greater than the average number of rows over the previous period     by more than a threshold percentage.</li> <li>The study date's number of rows is smaller than the average number of rows over the previous period     by more than a threshold percentage.</li> <li>The study date's number of rows is greater than the average number of rows over the previous period     by a config-specified number of standard deviations.</li> <li>The study date's number of rows is smaller than the average number of rows over the previous period     by a config-specified number of standard deviations.</li> <li>The study date's number of rows is greater than a config-specified threshold.</li> <li>The study date's number of rows is smaller than a config-specified threshold.</li> </ul> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def raw_size_warnings(self, lookback_stats, today_values):\n    \"\"\"Method that generates the quality warnings, that will be recorded in the output log table,\n    for the metric regarding the initial number of rows of the raw input network topology data prior\n    to the syntactic check procedure.\n\n    A total of six warnings might be generated:\n        - The study date's number of rows is greater than the average number of rows over the previous period\n            by more than a threshold percentage.\n        - The study date's number of rows is smaller than the average number of rows over the previous period\n            by more than a threshold percentage.\n        - The study date's number of rows is greater than the average number of rows over the previous period\n            by a config-specified number of standard deviations.\n        - The study date's number of rows is smaller than the average number of rows over the previous period\n            by a config-specified number of standard deviations.\n        - The study date's number of rows is greater than a config-specified threshold.\n        - The study date's number of rows is smaller than a config-specified threshold.\n    \"\"\"\n    current_val = today_values[None][NetworkErrorType.INITIAL_ROWS]\n    previous_avg = lookback_stats[None][NetworkErrorType.INITIAL_ROWS][\"average\"]\n    previous_std = lookback_stats[None][NetworkErrorType.INITIAL_ROWS][\"stddev\"]\n\n    measure_definition = self.MEASURE_DEFINITION[\"SIZE_RAW_DATA\"]\n\n    over_average = self.thresholds[\"SIZE_RAW_DATA\"][\"OVER_AVERAGE\"]\n    under_average = self.thresholds[\"SIZE_RAW_DATA\"][\"UNDER_AVERAGE\"]\n    variability = self.thresholds[\"SIZE_RAW_DATA\"][\"VARIABILITY\"]\n    absolute_upper_control_limit = self.thresholds[\"SIZE_RAW_DATA\"][\"ABS_VALUE_UPPER_LIMIT\"]\n    absolute_lower_control_limit = self.thresholds[\"SIZE_RAW_DATA\"][\"ABS_VALUE_LOWER_LIMIT\"]\n\n    if previous_avg == 0:\n        self.logger.warning(\n            f\"\"\"\n            The average of the number of rows in the raw input network topology data is 0\n            for the lookback period especified ({self.lookback_period_start} to {self.lookback_period_end},\n             both included)\n            \"\"\"\n        )\n    else:\n        pct_difference = 100 * current_val / previous_avg - 100\n        if pct_difference &gt; over_average:\n            self.register_warning(\n                measure_definition,\n                pct_difference,\n                f\"Size is greater than the previous period's average by more than {over_average} %\",\n                over_average,\n                \"The number of cells is unexpectedly high with respect to the previous period, please check if there have been issues in the network\",\n            )\n\n        if pct_difference &lt; -under_average:\n            self.register_warning(\n                measure_definition,\n                pct_difference,\n                f\"Size is smaller than the previous period's average by more than {under_average} %\",\n                under_average,\n                \"The number of cells is unexpectedly low with respect to the previous period, please check if there have been issues in the network.\",\n            )\n\n    upper_control_limit = previous_avg + variability * previous_std\n    lower_control_limit = previous_avg - variability * previous_std\n\n    if current_val &gt; upper_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            \"Size is out of the upper control limit calculated on the basis of average and standard deviation of the distribution \"\n            f\"of the size in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\",\n            upper_control_limit,\n            \"The number of cells is unexpectedly high with respect to the previous period, taking into account the usual \"\n            \"variability of the cell numbers, please check if there have been issues in the network.\",\n        )\n    if current_val &lt; lower_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            \"Size is out of the lower control limit calculated on the basis of average and standard deviation of the distribution \"\n            f\"of the size in the previous period. Lower control limit = (average - {variability}\u00b7stddev)\",\n            lower_control_limit,\n            \"The number of cells is unexpectedly low with respect to the previous period, taking into account the usual \"\n            \"variability of the cell numbers, please check if there have been issues in the network.\",\n        )\n\n    # Use of UCL and LCL as default values for the absolute limits\n    if absolute_upper_control_limit is None:\n        absolute_upper_control_limit = upper_control_limit\n    if absolute_lower_control_limit is None:\n        absolute_lower_control_limit = lower_control_limit\n\n    if current_val &gt; absolute_upper_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            f\"The size is over the threshold {absolute_upper_control_limit}\",\n            absolute_upper_control_limit,\n            \"The number of cells is over the threshold, please check if there have been changes in the network.\",\n        )\n    if current_val &lt; absolute_lower_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            f\"The size is under the threshold {absolute_lower_control_limit}\",\n            absolute_upper_control_limit,\n            \"The number of cells is under the threshold, please check if there have been changes in the network.\",\n        )\n\n    return previous_avg, upper_control_limit, lower_control_limit\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.register_warning","title":"<code>register_warning(measure_definition, daily_value, condition, condition_value, warning_text)</code>","text":"<p>Method that abstracts away the creation in the correct format and data type, each of the quality warnings that will be recorded in the log table.</p> <p>Parameters:</p> Name Type Description Default <code>measure_definition</code> <code>str</code> <p>measure that raised the warning (e.g. Error rate)</p> required <code>daily_value</code> <code>float</code> <p>measure's value in the date of study that raised the warning</p> required <code>condition</code> <code>str</code> <p>test that was checked in order to raise the warning</p> required <code>condition_value</code> <code>float</code> <p>value against which the date of study's daily_value was compared</p> required <code>warning_text</code> <code>str</code> <p>verbose explanation of the condition being satisfied and the warning being raised</p> required Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def register_warning(\n    self, measure_definition: str, daily_value: float, condition: str, condition_value: float, warning_text: str\n) -&gt; None:\n    \"\"\"Method that abstracts away the creation in the correct format and data type, each of the quality warnings\n    that will be recorded in the log table.\n\n    Args:\n        measure_definition (str): measure that raised the warning (e.g. Error rate)\n        daily_value (float): measure's value in the date of study that raised the warning\n        condition (str): test that was checked in order to raise the warning\n        condition_value (float): value against which the date of study's daily_value was compared\n        warning_text (str): verbose explanation of the condition being satisfied and the warning\n            being raised\n    \"\"\"\n    warning = {\n        ColNames.title: self.TITLE,\n        ColNames.date: self.date_of_study,\n        ColNames.timestamp: self.timestamp,\n        ColNames.measure_definition: measure_definition,\n        ColNames.daily_value: float(daily_value),\n        ColNames.condition: condition,\n        ColNames.lookback_period: self.lookback_period,\n        ColNames.condition_value: float(condition_value),\n        ColNames.warning_text: warning_text,\n        ColNames.year: self.date_of_study.year,\n        ColNames.month: self.date_of_study.month,\n        ColNames.day: self.date_of_study.day,\n    }\n\n    self.warnings.append(Row(**warning))\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.specific_error_warnings","title":"<code>specific_error_warnings(error_rate_type, field_name, lookback_stats, today_values)</code>","text":"<p>Method that generates the quality warnings, that will be recorded in the output log table, for the metric regarding a specific error type considered in the network syntactic checks.</p> A total of three warnings might be generated <ul> <li>The study date's error rate for this error and this field is greater than the average number of rows over the previous period     by more than a threshold percentage.</li> <li>The study date's error rate for this error and this field is greater than the average number of rows over the previous period     by a config-specified number of standard deviations.</li> <li>The study date's error rate for this error and this field is greater than a config-specified threshold.</li> </ul> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def specific_error_warnings(self, error_rate_type, field_name, lookback_stats, today_values):\n    \"\"\"Method that generates the quality warnings, that will be recorded in the output log table,\n    for the metric regarding a specific error type considered in the network syntactic checks.\n\n    A total of three warnings might be generated:\n        - The study date's error rate for this error and this field is greater than the average number of rows over the previous period\n            by more than a threshold percentage.\n        - The study date's error rate for this error and this field is greater than the average number of rows over the previous period\n            by a config-specified number of standard deviations.\n        - The study date's error rate for this error and this field is greater than a config-specified threshold.\n    \"\"\"\n    if error_rate_type not in self.ERROR_TYPE:\n        raise ValueError(f\"Unknown error type for quality warning `{error_rate_type}`\")\n\n    network_error_type = self.ERROR_TYPE[error_rate_type]\n\n    over_average = self.thresholds[error_rate_type][field_name][\"AVERAGE\"]\n    variability = self.thresholds[error_rate_type][field_name][\"VARIABILITY\"]\n    absolute_upper_control_limit = self.thresholds[error_rate_type][field_name][\"ABS_VALUE_UPPER_LIMIT\"]\n\n    if network_error_type == NetworkErrorType.OUT_OF_RANGE and field_name is None:\n        field_name = \"dates\"\n    current_val = today_values[field_name][network_error_type]\n    previous_avg = lookback_stats[field_name][network_error_type][\"average\"]\n    previous_std = lookback_stats[field_name][network_error_type][\"stddev\"]\n\n    if previous_avg == 0:\n        self.logger.warning(\n            f\"\"\"\n            The average {error_rate_type} for field {field_name} in the input network topology data is 0 for the lookback period\n            especified ({self.lookback_period_start} to {self.lookback_period_end}, both included)\n            \"\"\"\n        )\n    else:\n        pct_difference = 100 * current_val / previous_avg - 100\n        if pct_difference &gt; over_average:\n            self.register_warning(\n                self.MEASURE_DEFINITION[error_rate_type](field_name=field_name),\n                pct_difference,\n                self.CONDITION[error_rate_type][\"AVERAGE\"](field_name=field_name, value=over_average),\n                over_average,\n                self.WARNING_MESSAGE[error_rate_type][\"AVERAGE\"](field_name=field_name),\n            )\n    upper_control_limit = previous_avg + variability * previous_std\n\n    if current_val &gt; upper_control_limit:\n        self.register_warning(\n            self.MEASURE_DEFINITION[error_rate_type](field_name=field_name),\n            current_val,\n            self.CONDITION[error_rate_type][\"VARIABILITY\"](field_name=field_name, variability=variability),\n            variability,\n            self.WARNING_MESSAGE[error_rate_type][\"VARIABILITY\"](field_name=field_name),\n        )\n\n    if current_val &gt; absolute_upper_control_limit:\n        self.register_warning(\n            self.MEASURE_DEFINITION[error_rate_type](field_name=field_name),\n            current_val,\n            self.CONDITION[error_rate_type][\"ABS_VALUE_UPPER_LIMIT\"](\n                field_name=field_name, value=absolute_upper_control_limit\n            ),\n            absolute_upper_control_limit,\n            self.WARNING_MESSAGE[error_rate_type][\"ABS_VALUE_UPPER_LIMIT\"](field_name=field_name),\n        )\n</code></pre>"},{"location":"reference/components/quality/semantic_quality_warnings/","title":"semantic_quality_warnings","text":""},{"location":"reference/components/quality/semantic_quality_warnings/semantic_quality_warnings/","title":"semantic_quality_warnings","text":""},{"location":"reference/components/quality/semantic_quality_warnings/semantic_quality_warnings/#components.quality.semantic_quality_warnings.semantic_quality_warnings.SemanticQualityWarnings","title":"<code>SemanticQualityWarnings</code>","text":"<p>               Bases: <code>Component</code></p> Source code in <code>multimno/components/quality/semantic_quality_warnings/semantic_quality_warnings.py</code> <pre><code>class SemanticQualityWarnings(Component):\n    \"\"\" \"\"\"\n\n    COMPONENT_ID = \"SemanticQualityWarnings\"\n\n    MINIMUM_STD_LOOKBACK_DAYS = 3\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.timestamp = datetime.datetime.now()\n\n        # Read date of study\n        self.date_format = self.config.get(self.COMPONENT_ID, \"date_format\")\n        date_of_study = self.config.get(self.COMPONENT_ID, \"date_of_study\")\n\n        try:\n            self.date_of_study = datetime.datetime.strptime(date_of_study, self.date_format).date()\n        except ValueError as e:\n            self.logger.error(str(e), exc_info=True)\n            raise e\n\n        self.thresholds = self.get_thresholds()\n\n        self.warning_long_format = []\n\n    def get_thresholds(self) -&gt; dict:\n        \"\"\"\n        Method that reads the threshold-related parameters, contained in the config file,\n        and saves them in memory to use them instead of the ones specified by default.\n\n        Raises:\n            ValueError: non-numerical value that cannot be parsed to float (or int) has been used in\n                the config file\n            ValueError: Negative value for a given parameter has been given, when only\n                non-negative values make sense and are thus allowed.\n\n        Returns:\n            thresholds: dict containing the different thresholds used for computing the\n                quality warnings, with the same structure as\n                multimno.core.constants.network_default_thresholds.ASEMANTIC_DEFAULT_THRESHOLDS\n        \"\"\"\n        config_thresholds = self.config.geteval(self.COMPONENT_ID, \"thresholds\")\n        thresholds = SEMANTIC_DEFAULT_THRESHOLDS\n\n        for error_key in config_thresholds.keys():\n            if error_key not in SEMANTIC_DEFAULT_THRESHOLDS:\n                self.logger.info(f\"Parameter key {error_key} unknown -- ignored\")\n                continue\n\n            for param_key, val in config_thresholds[error_key].items():\n                if param_key not in SEMANTIC_DEFAULT_THRESHOLDS[error_key]:\n                    self.logger.info(f\"Unknown parameter {param_key} under {error_key} -- ignored\")\n                    continue\n\n                if param_key == \"sd_lookback_days\":\n                    try:\n                        val = int(val)\n                    except ValueError as e:\n                        error_msg = (\n                            f\"Expected integer value for parameter {param_key} under {error_key} - got {val} instead.\"\n                        )\n                        self.logger.error(error_msg)\n                        raise e\n                else:\n                    try:\n                        val = float(val)\n                    except ValueError as e:\n                        error_msg = (\n                            f\"Expected numeric value for parameter {param_key} under {error_key} - got {val} instead.\"\n                        )\n                        self.logger.error(error_msg)\n                        raise e\n\n                if val &lt; 0:\n                    error_msg = f\"Parameter {param_key} under {error_key} must be non-negative - got {val}\"\n                    self.logger.error(error_msg)\n                    raise ValueError(error_msg)\n\n                thresholds[error_key][param_key] = val\n\n        return thresholds\n\n    def initalize_data_objects(self):\n        input_silver_quality_metrics_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"event_device_semantic_quality_metrics\"\n        )\n        output_silver_log_table_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"event_device_semantic_quality_warnings_log_table\"\n        )\n        output_silver_bar_plot_data_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"event_device_semantic_quality_warnings_bar_plot_data\"\n        )\n\n        silver_quality_metrics = SilverEventSemanticQualityMetrics(\n            self.spark,\n            input_silver_quality_metrics_path,\n            partition_columns=[ColNames.year, ColNames.month, ColNames.day],\n        )\n\n        silver_log_table = SilverEventSemanticQualityWarningsLogTable(\n            self.spark, output_silver_log_table_path, partition_columns=[ColNames.year, ColNames.month, ColNames.day]\n        )\n\n        silver_bar_plot_data = SilverEventSemanticQualityWarningsBarPlotData(\n            self.spark, output_silver_bar_plot_data_path\n        )\n\n        self.input_data_objects = {silver_quality_metrics.ID: silver_quality_metrics}\n        self.output_data_objects = {\n            silver_log_table.ID: silver_log_table,\n            silver_bar_plot_data.ID: silver_bar_plot_data,\n        }\n\n    def transform(self):\n        # Pushup filter, select only dates needed\n        # Since currently each QW has a different lookback period, we filter up to the\n        # furthest day in the past needed\n\n        metrics_df = self.input_data_objects[SilverEventSemanticQualityMetrics.ID].df\n\n        furthest_lookback = max(self.thresholds[key][\"sd_lookback_days\"] for key in self.thresholds.keys())\n\n        metrics_df = metrics_df.withColumn(\n            \"date\", F.make_date(year=F.col(ColNames.year), month=F.col(ColNames.month), day=F.col(ColNames.day))\n        ).filter(\n            F.col(\"date\").between(self.date_of_study - datetime.timedelta(days=furthest_lookback), self.date_of_study)\n        )\n\n        # Get all necessary metrics\n        error_counts = metrics_df.select([\"date\", ColNames.type_of_error, ColNames.value]).collect()\n\n        error_counts = [row.asDict() for row in error_counts]\n\n        error_stats = dict()\n        for count in error_counts:\n            date = count[\"date\"]\n            if date not in error_stats:\n                error_stats[date] = dict()\n\n            error_stats[date][count[ColNames.type_of_error]] = count[ColNames.value]\n\n        # If study date not present in the data, throw an exception\n        if self.date_of_study not in error_stats.keys():\n            raise ValueError(\n                f\"The date of study, {self.date_of_study.strftime(self.date_format)}, has no semantic checks metrics!\"\n            )\n\n        for key in error_stats.keys():\n            error_stats[key] = {\"count\": error_stats[key]}\n            error_stats[key][\"total\"] = sum(error_stats[key][\"count\"].values())\n            error_stats[key][\"percentage\"] = {\n                type_of_error: 100 * val / error_stats[key][\"total\"]\n                for type_of_error, val in error_stats[key][\"count\"].items()\n            }\n\n        for error_name in self.thresholds.keys():\n            self.quality_warnings_by_error(error_name, error_stats)\n\n        self.set_output_log_table()\n\n        self.create_plots_data(error_stats)\n\n    def quality_warnings_by_error(self, error_name: str, error_stats: dict):\n        \"\"\"Method that generates the quality warnings that will be recorded in the output log table,\n        for each type of error.\n\n        In the case that the data needed for a specific error's lookback period is not present, only the current date's\n        error percentage is computed and no warning is raised.\n\n        Args:\n            error_name (str): name of the error, an attribute of multimno.core.constants.error_types.SemanticErrorType\n            error_stats (dict): contains different values concerning each type of error, its counts, percentages, etc.\n        \"\"\"\n        # Get the code of the error given its name\n        error_code = getattr(SemanticErrorType, error_name)\n\n        # lookback days for this error\n        lookback_span = self.thresholds[error_name][\"sd_lookback_days\"]\n        lookback_dates = [self.date_of_study - datetime.timedelta(days=dd) for dd in range(1, lookback_span + 1)]\n\n        if not all(lookback_date in error_stats.keys() for lookback_date in lookback_dates):\n            # cannot compute lookback mean and average, so only showing this date's percentages\n            self.register_warning(\n                date=self.date_of_study,\n                error_code=error_code,\n                value=error_stats[self.date_of_study][\"percentage\"][error_code],\n                upper_control_limit=None,\n                display_warning=False,\n            )\n        else:\n            if lookback_span &lt; self.MINIMUM_STD_LOOKBACK_DAYS:\n                upper_control_limit = self.thresholds[error_name][\"min_percentage\"]\n                self.logger.info(\n                    f\"Lookback days for {error_name} lower than {self.MINIMUM_STD_LOOKBACK_DAYS} - using fixed \"\n                    f\"threshold {upper_control_limit}% instead of using average and standard deviation\"\n                )\n            else:\n                previous_avg = sum(error_stats[dd][\"percentage\"][error_code] for dd in lookback_dates) / lookback_span\n                previous_std = math.sqrt(\n                    sum((error_stats[dd][\"percentage\"][error_code] - previous_avg) ** 2 for dd in lookback_dates)\n                    / (lookback_span - 1)\n                )\n\n                upper_control_limit = previous_avg + self.thresholds[error_name][\"min_sd\"] * previous_std\n\n            # Now compare with todays value\n            if error_stats[self.date_of_study][\"percentage\"][error_code] &gt; upper_control_limit:\n                display_warning = True\n            else:\n                display_warning = False\n\n            self.register_warning(\n                date=self.date_of_study,\n                error_code=error_code,\n                value=error_stats[self.date_of_study][\"percentage\"][error_code],\n                upper_control_limit=upper_control_limit,\n                display_warning=display_warning,\n            )\n\n    def register_warning(\n        self, date: datetime.date, error_code: int, value: float, upper_control_limit: float, display_warning: bool\n    ):\n        \"\"\"Method that abstracts away the creation in the correct format and data type, each of the quality\n        warnings that will be recorded in the log table.\n\n        Args:\n            date (datetime.date): study date, for which the warnings are being calculated\n            error_code (int): code of the error\n            value (float): observed percentage of this specific error for the study date\n            upper_control_limit (float): upper control limit, used as threshold for the warning\n            display_warning (bool): whether the warning should be raised or not. It is currently independent of\n                the arguments values, but in theory it should be equal to (value &gt; control_limit)\n        \"\"\"\n        self.warning_long_format.append((date, error_code, value, upper_control_limit, display_warning))\n\n    def set_output_log_table(self):\n        \"\"\"\n        Method that formats the warnings into the expected table format\n        \"\"\"\n        warning_logs = pd.DataFrame(\n            self.warning_long_format, columns=[ColNames.date, ColNames.type_of_error, \"value\", \"UCL\", \"display\"]\n        ).pivot(index=ColNames.date, columns=[ColNames.type_of_error])\n        column_names = []\n        for name, code in warning_logs.columns:\n            if name == \"value\":\n                column_names.append(f\"Error {code}\")\n            elif name == \"UCL\":\n                column_names.append(f\"Error {code} upper control limit\")\n            elif name == \"display\":\n                column_names.append(f\"Error {code} display warning\")\n        warning_logs.columns = column_names\n        warning_logs = warning_logs.assign(execution_id=self.timestamp)\n        warning_logs = warning_logs.reset_index().assign(\n            **{\n                ColNames.year: lambda df_: df_[\"date\"].apply(lambda x: x.year),\n                ColNames.month: lambda df_: df_[\"date\"].apply(lambda x: x.month),\n                ColNames.day: lambda df_: df_[\"date\"].apply(lambda x: x.day),\n            }\n        )\n\n        # Force expected order of columns\n        warning_logs = warning_logs[SilverEventSemanticQualityWarningsLogTable.SCHEMA.names]\n\n        log_table_df = self.spark.createDataFrame(\n            warning_logs, schema=SilverEventSemanticQualityWarningsLogTable.SCHEMA\n        )\n\n        self.output_data_objects[SilverEventSemanticQualityWarningsLogTable.ID].df = log_table_df\n\n    def create_plots_data(self, error_stats):\n        \"\"\"\n        Method that takes the data needed to generate the required plots, and formats it for\n        easier plotting and saving later on.\n        \"\"\"\n        plot_data = []\n\n        def format_error_code(code):\n            if code == SemanticErrorType.NO_ERROR:\n                return \"No Error\"\n\n            return f\"Error {code}\"\n\n        for date in error_stats:\n            for error_code in error_stats[date][\"count\"]:\n                plot_data.append(\n                    Row(\n                        **{\n                            ColNames.date: date,\n                            ColNames.type_of_error: format_error_code(error_code),\n                            ColNames.value: float(error_stats[date][\"count\"][error_code]),\n                            ColNames.variable: \"Number of occurrences\",\n                            ColNames.year: self.date_of_study.year,\n                            ColNames.month: self.date_of_study.month,\n                            ColNames.day: self.date_of_study.day,\n                            ColNames.timestamp: self.timestamp,\n                        }\n                    )\n                )\n            for error_code in error_stats[date][\"percentage\"]:\n                plot_data.append(\n                    Row(\n                        **{\n                            ColNames.date: date,\n                            ColNames.type_of_error: format_error_code(error_code),\n                            ColNames.value: float(error_stats[date][\"percentage\"][error_code]),\n                            ColNames.variable: \"Percentage\",\n                            ColNames.year: self.date_of_study.year,\n                            ColNames.month: self.date_of_study.month,\n                            ColNames.day: self.date_of_study.day,\n                            ColNames.timestamp: self.timestamp,\n                        }\n                    )\n                )\n        self.output_data_objects[SilverEventSemanticQualityWarningsBarPlotData.ID].df = self.spark.createDataFrame(\n            plot_data, schema=SilverEventSemanticQualityWarningsBarPlotData.SCHEMA\n        )\n\n    def write(self):\n        # Write regular data objects\n        self.logger.info(\"Writing data objects...\")\n        super().write()\n        self.logger.info(\"Data objects written.\")\n</code></pre>"},{"location":"reference/components/quality/semantic_quality_warnings/semantic_quality_warnings/#components.quality.semantic_quality_warnings.semantic_quality_warnings.SemanticQualityWarnings.create_plots_data","title":"<code>create_plots_data(error_stats)</code>","text":"<p>Method that takes the data needed to generate the required plots, and formats it for easier plotting and saving later on.</p> Source code in <code>multimno/components/quality/semantic_quality_warnings/semantic_quality_warnings.py</code> <pre><code>def create_plots_data(self, error_stats):\n    \"\"\"\n    Method that takes the data needed to generate the required plots, and formats it for\n    easier plotting and saving later on.\n    \"\"\"\n    plot_data = []\n\n    def format_error_code(code):\n        if code == SemanticErrorType.NO_ERROR:\n            return \"No Error\"\n\n        return f\"Error {code}\"\n\n    for date in error_stats:\n        for error_code in error_stats[date][\"count\"]:\n            plot_data.append(\n                Row(\n                    **{\n                        ColNames.date: date,\n                        ColNames.type_of_error: format_error_code(error_code),\n                        ColNames.value: float(error_stats[date][\"count\"][error_code]),\n                        ColNames.variable: \"Number of occurrences\",\n                        ColNames.year: self.date_of_study.year,\n                        ColNames.month: self.date_of_study.month,\n                        ColNames.day: self.date_of_study.day,\n                        ColNames.timestamp: self.timestamp,\n                    }\n                )\n            )\n        for error_code in error_stats[date][\"percentage\"]:\n            plot_data.append(\n                Row(\n                    **{\n                        ColNames.date: date,\n                        ColNames.type_of_error: format_error_code(error_code),\n                        ColNames.value: float(error_stats[date][\"percentage\"][error_code]),\n                        ColNames.variable: \"Percentage\",\n                        ColNames.year: self.date_of_study.year,\n                        ColNames.month: self.date_of_study.month,\n                        ColNames.day: self.date_of_study.day,\n                        ColNames.timestamp: self.timestamp,\n                    }\n                )\n            )\n    self.output_data_objects[SilverEventSemanticQualityWarningsBarPlotData.ID].df = self.spark.createDataFrame(\n        plot_data, schema=SilverEventSemanticQualityWarningsBarPlotData.SCHEMA\n    )\n</code></pre>"},{"location":"reference/components/quality/semantic_quality_warnings/semantic_quality_warnings/#components.quality.semantic_quality_warnings.semantic_quality_warnings.SemanticQualityWarnings.get_thresholds","title":"<code>get_thresholds()</code>","text":"<p>Method that reads the threshold-related parameters, contained in the config file, and saves them in memory to use them instead of the ones specified by default.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>non-numerical value that cannot be parsed to float (or int) has been used in the config file</p> <code>ValueError</code> <p>Negative value for a given parameter has been given, when only non-negative values make sense and are thus allowed.</p> <p>Returns:</p> Name Type Description <code>thresholds</code> <code>dict</code> <p>dict containing the different thresholds used for computing the quality warnings, with the same structure as multimno.core.constants.network_default_thresholds.ASEMANTIC_DEFAULT_THRESHOLDS</p> Source code in <code>multimno/components/quality/semantic_quality_warnings/semantic_quality_warnings.py</code> <pre><code>def get_thresholds(self) -&gt; dict:\n    \"\"\"\n    Method that reads the threshold-related parameters, contained in the config file,\n    and saves them in memory to use them instead of the ones specified by default.\n\n    Raises:\n        ValueError: non-numerical value that cannot be parsed to float (or int) has been used in\n            the config file\n        ValueError: Negative value for a given parameter has been given, when only\n            non-negative values make sense and are thus allowed.\n\n    Returns:\n        thresholds: dict containing the different thresholds used for computing the\n            quality warnings, with the same structure as\n            multimno.core.constants.network_default_thresholds.ASEMANTIC_DEFAULT_THRESHOLDS\n    \"\"\"\n    config_thresholds = self.config.geteval(self.COMPONENT_ID, \"thresholds\")\n    thresholds = SEMANTIC_DEFAULT_THRESHOLDS\n\n    for error_key in config_thresholds.keys():\n        if error_key not in SEMANTIC_DEFAULT_THRESHOLDS:\n            self.logger.info(f\"Parameter key {error_key} unknown -- ignored\")\n            continue\n\n        for param_key, val in config_thresholds[error_key].items():\n            if param_key not in SEMANTIC_DEFAULT_THRESHOLDS[error_key]:\n                self.logger.info(f\"Unknown parameter {param_key} under {error_key} -- ignored\")\n                continue\n\n            if param_key == \"sd_lookback_days\":\n                try:\n                    val = int(val)\n                except ValueError as e:\n                    error_msg = (\n                        f\"Expected integer value for parameter {param_key} under {error_key} - got {val} instead.\"\n                    )\n                    self.logger.error(error_msg)\n                    raise e\n            else:\n                try:\n                    val = float(val)\n                except ValueError as e:\n                    error_msg = (\n                        f\"Expected numeric value for parameter {param_key} under {error_key} - got {val} instead.\"\n                    )\n                    self.logger.error(error_msg)\n                    raise e\n\n            if val &lt; 0:\n                error_msg = f\"Parameter {param_key} under {error_key} must be non-negative - got {val}\"\n                self.logger.error(error_msg)\n                raise ValueError(error_msg)\n\n            thresholds[error_key][param_key] = val\n\n    return thresholds\n</code></pre>"},{"location":"reference/components/quality/semantic_quality_warnings/semantic_quality_warnings/#components.quality.semantic_quality_warnings.semantic_quality_warnings.SemanticQualityWarnings.quality_warnings_by_error","title":"<code>quality_warnings_by_error(error_name, error_stats)</code>","text":"<p>Method that generates the quality warnings that will be recorded in the output log table, for each type of error.</p> <p>In the case that the data needed for a specific error's lookback period is not present, only the current date's error percentage is computed and no warning is raised.</p> <p>Parameters:</p> Name Type Description Default <code>error_name</code> <code>str</code> <p>name of the error, an attribute of multimno.core.constants.error_types.SemanticErrorType</p> required <code>error_stats</code> <code>dict</code> <p>contains different values concerning each type of error, its counts, percentages, etc.</p> required Source code in <code>multimno/components/quality/semantic_quality_warnings/semantic_quality_warnings.py</code> <pre><code>def quality_warnings_by_error(self, error_name: str, error_stats: dict):\n    \"\"\"Method that generates the quality warnings that will be recorded in the output log table,\n    for each type of error.\n\n    In the case that the data needed for a specific error's lookback period is not present, only the current date's\n    error percentage is computed and no warning is raised.\n\n    Args:\n        error_name (str): name of the error, an attribute of multimno.core.constants.error_types.SemanticErrorType\n        error_stats (dict): contains different values concerning each type of error, its counts, percentages, etc.\n    \"\"\"\n    # Get the code of the error given its name\n    error_code = getattr(SemanticErrorType, error_name)\n\n    # lookback days for this error\n    lookback_span = self.thresholds[error_name][\"sd_lookback_days\"]\n    lookback_dates = [self.date_of_study - datetime.timedelta(days=dd) for dd in range(1, lookback_span + 1)]\n\n    if not all(lookback_date in error_stats.keys() for lookback_date in lookback_dates):\n        # cannot compute lookback mean and average, so only showing this date's percentages\n        self.register_warning(\n            date=self.date_of_study,\n            error_code=error_code,\n            value=error_stats[self.date_of_study][\"percentage\"][error_code],\n            upper_control_limit=None,\n            display_warning=False,\n        )\n    else:\n        if lookback_span &lt; self.MINIMUM_STD_LOOKBACK_DAYS:\n            upper_control_limit = self.thresholds[error_name][\"min_percentage\"]\n            self.logger.info(\n                f\"Lookback days for {error_name} lower than {self.MINIMUM_STD_LOOKBACK_DAYS} - using fixed \"\n                f\"threshold {upper_control_limit}% instead of using average and standard deviation\"\n            )\n        else:\n            previous_avg = sum(error_stats[dd][\"percentage\"][error_code] for dd in lookback_dates) / lookback_span\n            previous_std = math.sqrt(\n                sum((error_stats[dd][\"percentage\"][error_code] - previous_avg) ** 2 for dd in lookback_dates)\n                / (lookback_span - 1)\n            )\n\n            upper_control_limit = previous_avg + self.thresholds[error_name][\"min_sd\"] * previous_std\n\n        # Now compare with todays value\n        if error_stats[self.date_of_study][\"percentage\"][error_code] &gt; upper_control_limit:\n            display_warning = True\n        else:\n            display_warning = False\n\n        self.register_warning(\n            date=self.date_of_study,\n            error_code=error_code,\n            value=error_stats[self.date_of_study][\"percentage\"][error_code],\n            upper_control_limit=upper_control_limit,\n            display_warning=display_warning,\n        )\n</code></pre>"},{"location":"reference/components/quality/semantic_quality_warnings/semantic_quality_warnings/#components.quality.semantic_quality_warnings.semantic_quality_warnings.SemanticQualityWarnings.register_warning","title":"<code>register_warning(date, error_code, value, upper_control_limit, display_warning)</code>","text":"<p>Method that abstracts away the creation in the correct format and data type, each of the quality warnings that will be recorded in the log table.</p> <p>Parameters:</p> Name Type Description Default <code>date</code> <code>date</code> <p>study date, for which the warnings are being calculated</p> required <code>error_code</code> <code>int</code> <p>code of the error</p> required <code>value</code> <code>float</code> <p>observed percentage of this specific error for the study date</p> required <code>upper_control_limit</code> <code>float</code> <p>upper control limit, used as threshold for the warning</p> required <code>display_warning</code> <code>bool</code> <p>whether the warning should be raised or not. It is currently independent of the arguments values, but in theory it should be equal to (value &gt; control_limit)</p> required Source code in <code>multimno/components/quality/semantic_quality_warnings/semantic_quality_warnings.py</code> <pre><code>def register_warning(\n    self, date: datetime.date, error_code: int, value: float, upper_control_limit: float, display_warning: bool\n):\n    \"\"\"Method that abstracts away the creation in the correct format and data type, each of the quality\n    warnings that will be recorded in the log table.\n\n    Args:\n        date (datetime.date): study date, for which the warnings are being calculated\n        error_code (int): code of the error\n        value (float): observed percentage of this specific error for the study date\n        upper_control_limit (float): upper control limit, used as threshold for the warning\n        display_warning (bool): whether the warning should be raised or not. It is currently independent of\n            the arguments values, but in theory it should be equal to (value &gt; control_limit)\n    \"\"\"\n    self.warning_long_format.append((date, error_code, value, upper_control_limit, display_warning))\n</code></pre>"},{"location":"reference/components/quality/semantic_quality_warnings/semantic_quality_warnings/#components.quality.semantic_quality_warnings.semantic_quality_warnings.SemanticQualityWarnings.set_output_log_table","title":"<code>set_output_log_table()</code>","text":"<p>Method that formats the warnings into the expected table format</p> Source code in <code>multimno/components/quality/semantic_quality_warnings/semantic_quality_warnings.py</code> <pre><code>def set_output_log_table(self):\n    \"\"\"\n    Method that formats the warnings into the expected table format\n    \"\"\"\n    warning_logs = pd.DataFrame(\n        self.warning_long_format, columns=[ColNames.date, ColNames.type_of_error, \"value\", \"UCL\", \"display\"]\n    ).pivot(index=ColNames.date, columns=[ColNames.type_of_error])\n    column_names = []\n    for name, code in warning_logs.columns:\n        if name == \"value\":\n            column_names.append(f\"Error {code}\")\n        elif name == \"UCL\":\n            column_names.append(f\"Error {code} upper control limit\")\n        elif name == \"display\":\n            column_names.append(f\"Error {code} display warning\")\n    warning_logs.columns = column_names\n    warning_logs = warning_logs.assign(execution_id=self.timestamp)\n    warning_logs = warning_logs.reset_index().assign(\n        **{\n            ColNames.year: lambda df_: df_[\"date\"].apply(lambda x: x.year),\n            ColNames.month: lambda df_: df_[\"date\"].apply(lambda x: x.month),\n            ColNames.day: lambda df_: df_[\"date\"].apply(lambda x: x.day),\n        }\n    )\n\n    # Force expected order of columns\n    warning_logs = warning_logs[SilverEventSemanticQualityWarningsLogTable.SCHEMA.names]\n\n    log_table_df = self.spark.createDataFrame(\n        warning_logs, schema=SilverEventSemanticQualityWarningsLogTable.SCHEMA\n    )\n\n    self.output_data_objects[SilverEventSemanticQualityWarningsLogTable.ID].df = log_table_df\n</code></pre>"},{"location":"reference/core/","title":"core","text":""},{"location":"reference/core/component/","title":"component","text":"<p>Module that defines the abstract pipeline component class</p>"},{"location":"reference/core/component/#core.component.Component","title":"<code>Component</code>","text":"<p>Class that models a pipeline component.</p> Source code in <code>multimno/core/component.py</code> <pre><code>class Component(metaclass=ABCMeta):\n    \"\"\"\n    Class that models a pipeline component.\n    \"\"\"\n\n    COMPONENT_ID: str = None\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        self.input_data_objects: Dict[str, DataObject] = None\n        self.output_data_objects: Dict[str, DataObject] = None\n\n        # Initialize variables\n        self.config: ConfigParser = parse_configuration(general_config_path, component_config_path)\n        self.logger: Logger = generate_logger(self.config, self.COMPONENT_ID)\n        self.spark: SparkSession = generate_spark_session(self.config)\n        self.initalize_data_objects()\n\n        # Log configuration\n        self.log_config()\n\n    @abstractmethod\n    def initalize_data_objects(self):\n        \"\"\"\n        Method that initializes the data objects associated with the component.\n        \"\"\"\n\n    def read(self):\n        \"\"\"\n        Method that performs the read operation of the input data objects of the component.\n        \"\"\"\n        for data_object in self.input_data_objects.values():\n            data_object.read()\n\n    @abstractmethod\n    def transform(self):\n        \"\"\"\n        Method that performs the data transformations needed to set the dataframes of the output\n         data objects from the input data objects.\n        \"\"\"\n\n    def write(self):\n        \"\"\"\n        Method that performs the write operation of the output data objects.\n        \"\"\"\n        for data_object in self.output_data_objects.values():\n            data_object.write()\n\n    def execute(self):\n        \"\"\"\n        Method that performs the read, transform and write methods of the component.\n        \"\"\"\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n        self.transform()\n        self.write()\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def log_config(self):\n        \"\"\"\n        Method that logs all sections and key-value pairs of a ConfigParser object.\n        \"\"\"\n        # Validation\n        if self.config is None or self.logger is None:\n            return\n\n        # Log each section in order\n        for section in self.config.sections():\n            self.logger.info(f\"[{section}]\")\n            for key, value in self.config.items(section):\n                self.logger.info(f\"{key}: {value}\")\n            # Break line for each section\n            self.logger.info(\"\")\n</code></pre>"},{"location":"reference/core/component/#core.component.Component.execute","title":"<code>execute()</code>","text":"<p>Method that performs the read, transform and write methods of the component.</p> Source code in <code>multimno/core/component.py</code> <pre><code>def execute(self):\n    \"\"\"\n    Method that performs the read, transform and write methods of the component.\n    \"\"\"\n    self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n    self.read()\n    self.transform()\n    self.write()\n    self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n</code></pre>"},{"location":"reference/core/component/#core.component.Component.initalize_data_objects","title":"<code>initalize_data_objects()</code>  <code>abstractmethod</code>","text":"<p>Method that initializes the data objects associated with the component.</p> Source code in <code>multimno/core/component.py</code> <pre><code>@abstractmethod\ndef initalize_data_objects(self):\n    \"\"\"\n    Method that initializes the data objects associated with the component.\n    \"\"\"\n</code></pre>"},{"location":"reference/core/component/#core.component.Component.log_config","title":"<code>log_config()</code>","text":"<p>Method that logs all sections and key-value pairs of a ConfigParser object.</p> Source code in <code>multimno/core/component.py</code> <pre><code>def log_config(self):\n    \"\"\"\n    Method that logs all sections and key-value pairs of a ConfigParser object.\n    \"\"\"\n    # Validation\n    if self.config is None or self.logger is None:\n        return\n\n    # Log each section in order\n    for section in self.config.sections():\n        self.logger.info(f\"[{section}]\")\n        for key, value in self.config.items(section):\n            self.logger.info(f\"{key}: {value}\")\n        # Break line for each section\n        self.logger.info(\"\")\n</code></pre>"},{"location":"reference/core/component/#core.component.Component.read","title":"<code>read()</code>","text":"<p>Method that performs the read operation of the input data objects of the component.</p> Source code in <code>multimno/core/component.py</code> <pre><code>def read(self):\n    \"\"\"\n    Method that performs the read operation of the input data objects of the component.\n    \"\"\"\n    for data_object in self.input_data_objects.values():\n        data_object.read()\n</code></pre>"},{"location":"reference/core/component/#core.component.Component.transform","title":"<code>transform()</code>  <code>abstractmethod</code>","text":"<p>Method that performs the data transformations needed to set the dataframes of the output  data objects from the input data objects.</p> Source code in <code>multimno/core/component.py</code> <pre><code>@abstractmethod\ndef transform(self):\n    \"\"\"\n    Method that performs the data transformations needed to set the dataframes of the output\n     data objects from the input data objects.\n    \"\"\"\n</code></pre>"},{"location":"reference/core/component/#core.component.Component.write","title":"<code>write()</code>","text":"<p>Method that performs the write operation of the output data objects.</p> Source code in <code>multimno/core/component.py</code> <pre><code>def write(self):\n    \"\"\"\n    Method that performs the write operation of the output data objects.\n    \"\"\"\n    for data_object in self.output_data_objects.values():\n        data_object.write()\n</code></pre>"},{"location":"reference/core/configuration/","title":"configuration","text":"<p>Module that manages the application configuration.</p>"},{"location":"reference/core/configuration/#core.configuration.parse_configuration","title":"<code>parse_configuration(general_config_path, component_config_path='')</code>","text":"<p>Function that parses a list of configurations in a single ConfigParser object. It expects the first element of the list to be the path to general configuration path. It will override values of the general configuration file with component configuration data.</p> <p>Parameters:</p> Name Type Description Default <code>general_config_path</code> <code>list</code> <p>Path to the general configuration file.</p> required <code>component_config_path</code> <code>str</code> <p>Path to the component configuration file.</p> <code>''</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the general configuration path is doesn't exist</p> <p>Returns:</p> Name Type Description <code>config</code> <code>ConfigParser</code> <p>ConfigParser object with the configuration data.</p> Source code in <code>multimno/core/configuration.py</code> <pre><code>def parse_configuration(general_config_path: str, component_config_path: str = \"\") -&gt; ConfigParser:\n    \"\"\"Function that parses a list of configurations in a single ConfigParser object. It expects\n    the first element of the list to be the path to general configuration path. It will override\n    values of the general configuration file with component configuration data.\n\n    Args:\n        general_config_path (list): Path to the general configuration file.\n        component_config_path (str): Path to the component configuration file.\n\n    Raises:\n        FileNotFoundError: If the general configuration path is doesn't exist\n\n    Returns:\n        config: ConfigParser object with the configuration data.\n    \"\"\"\n\n    # Check general configuration file\n    if not os.path.exists(general_config_path):\n        raise FileNotFoundError(f\"General Config file Not found: {general_config_path}\")\n\n    config_paths = [general_config_path, component_config_path]\n\n    converters = {\n        \"list\": lambda val: [i.strip() for i in val.strip().split(\"\\n\")],\n        \"eval\": eval,\n    }\n\n    parser: ConfigParser = ConfigParser(\n        converters=converters, interpolation=ExtendedInterpolation(), inline_comment_prefixes=\"#\"\n    )\n    parser.optionxform = str\n    parser.read(config_paths)\n\n    return parser\n</code></pre>"},{"location":"reference/core/grid/","title":"grid","text":"<p>This module provides functionality for generating a grid based on the INSPIRE grid system specification.</p>"},{"location":"reference/core/grid/#core.grid.GridGenerator","title":"<code>GridGenerator</code>","text":"<p>Abstract class that provides functionality for generating a grid.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>class GridGenerator(metaclass=ABCMeta):\n    \"\"\"\n    Abstract class that provides functionality for generating a grid.\n    \"\"\"\n\n    def __init__(self, spark: SparkSession) -&gt; None:\n        self.spark: SparkSession = spark\n\n    @abstractmethod\n    def cover_extent_with_grid_ids(self, extent: tuple) -&gt; DataFrame:\n        \"\"\"Cover given extent with grid_ids on given resolution.\"\"\"\n\n    @abstractmethod\n    def cover_polygon_with_grid_ids(self, polygon_sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"Cover given polygon with grid_ids on given resolution.\"\"\"\n\n    @abstractmethod\n    def grid_ids_to_centroids(self, sdf: DataFrame, to_crs: int) -&gt; DataFrame:\n        \"\"\"Get geometry centroids from grid_ids with given coordinate system.\"\"\"\n\n    @abstractmethod\n    def grid_ids_to_tiles(self, sdf: DataFrame, to_crs: int) -&gt; DataFrame:\n        \"\"\"Get grid polygons from grid_ids with given coordinate system.\"\"\"\n\n    @abstractmethod\n    def get_parent_grid_id(self, sdf: DataFrame, resolution: int) -&gt; DataFrame:\n        \"\"\"Get parent grid_id on given resolution.\"\"\"\n\n    @abstractmethod\n    def get_children_grid_ids(self, sdf: DataFrame, resolution: int) -&gt; DataFrame:\n        \"\"\"Get children grid_ids on given resolution.\"\"\"\n</code></pre>"},{"location":"reference/core/grid/#core.grid.GridGenerator.cover_extent_with_grid_ids","title":"<code>cover_extent_with_grid_ids(extent)</code>  <code>abstractmethod</code>","text":"<p>Cover given extent with grid_ids on given resolution.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>@abstractmethod\ndef cover_extent_with_grid_ids(self, extent: tuple) -&gt; DataFrame:\n    \"\"\"Cover given extent with grid_ids on given resolution.\"\"\"\n</code></pre>"},{"location":"reference/core/grid/#core.grid.GridGenerator.cover_polygon_with_grid_ids","title":"<code>cover_polygon_with_grid_ids(polygon_sdf)</code>  <code>abstractmethod</code>","text":"<p>Cover given polygon with grid_ids on given resolution.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>@abstractmethod\ndef cover_polygon_with_grid_ids(self, polygon_sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"Cover given polygon with grid_ids on given resolution.\"\"\"\n</code></pre>"},{"location":"reference/core/grid/#core.grid.GridGenerator.get_children_grid_ids","title":"<code>get_children_grid_ids(sdf, resolution)</code>  <code>abstractmethod</code>","text":"<p>Get children grid_ids on given resolution.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>@abstractmethod\ndef get_children_grid_ids(self, sdf: DataFrame, resolution: int) -&gt; DataFrame:\n    \"\"\"Get children grid_ids on given resolution.\"\"\"\n</code></pre>"},{"location":"reference/core/grid/#core.grid.GridGenerator.get_parent_grid_id","title":"<code>get_parent_grid_id(sdf, resolution)</code>  <code>abstractmethod</code>","text":"<p>Get parent grid_id on given resolution.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>@abstractmethod\ndef get_parent_grid_id(self, sdf: DataFrame, resolution: int) -&gt; DataFrame:\n    \"\"\"Get parent grid_id on given resolution.\"\"\"\n</code></pre>"},{"location":"reference/core/grid/#core.grid.GridGenerator.grid_ids_to_centroids","title":"<code>grid_ids_to_centroids(sdf, to_crs)</code>  <code>abstractmethod</code>","text":"<p>Get geometry centroids from grid_ids with given coordinate system.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>@abstractmethod\ndef grid_ids_to_centroids(self, sdf: DataFrame, to_crs: int) -&gt; DataFrame:\n    \"\"\"Get geometry centroids from grid_ids with given coordinate system.\"\"\"\n</code></pre>"},{"location":"reference/core/grid/#core.grid.GridGenerator.grid_ids_to_tiles","title":"<code>grid_ids_to_tiles(sdf, to_crs)</code>  <code>abstractmethod</code>","text":"<p>Get grid polygons from grid_ids with given coordinate system.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>@abstractmethod\ndef grid_ids_to_tiles(self, sdf: DataFrame, to_crs: int) -&gt; DataFrame:\n    \"\"\"Get grid polygons from grid_ids with given coordinate system.\"\"\"\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator","title":"<code>InspireGridGenerator</code>","text":"<p>               Bases: <code>GridGenerator</code></p> <p>A class used to generate a grid based on the INSPIRE grid system specification.</p> <p>Attributes:</p> Name Type Description <code>GRID_CRS_EPSG_CODE</code> <code>int</code> <p>The EPSG code for the grid's CRS.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>class InspireGridGenerator(GridGenerator):\n    \"\"\"A class used to generate a grid based on the INSPIRE grid system specification.\n\n    Attributes:\n        GRID_CRS_EPSG_CODE (int): The EPSG code for the grid's CRS.\n    \"\"\"\n\n    GRID_CRS_EPSG_CODE = 3035\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        resolution=100,\n        geometry_col_name: str = \"geometry\",\n        grid_id_col_name: str = \"grid_id\",\n        grid_partition_size: int = 2000,\n    ) -&gt; None:\n        \"\"\"Initializes the InspireGridGenerator with the given parameters.\n\n        Args:\n            spark (SparkSession): The SparkSession to use.\n            resolution (int, optional): The resolution of the grid. Defaults to 100. Has to be divisible by 100.\n            geometry_col_name (str, optional): The name of the geometry column. Defaults to 'geometry'.\n            grid_id_col_name (str, optional): The name of the grid ID column. Defaults to 'grid_id'.\n            grid_partition_size (int, optional): The size of the grid partitions, defined as number of tiles\n                in x and y dimensions of subdivisions of the intital grid. Defaults to 2000.\n\n        Raises:\n            ValueError: If the resolution is not divisible by 100.\n        \"\"\"\n        if resolution % 100 != 0:\n            raise ValueError(\"Resolution must be divisible by 100\")\n\n        super().__init__(spark)\n        self.geometry_col_name = geometry_col_name\n        self.grid_id_col_name = grid_id_col_name\n        self.resolution = resolution\n        self.grid_partition_size = grid_partition_size\n        self.resolution_str = self._format_distance(resolution)\n\n    @staticmethod\n    def _format_distance(value: int) -&gt; str:\n        \"\"\"Formats the given distance value to string.\n\n        Args:\n            value (int): The distance value to format.\n\n        Returns:\n            str: The formatted distance value.\n        \"\"\"\n        if value &lt;= 1000:\n            return f\"{value}m\"\n        else:\n            return f\"{value/1000}km\"\n\n    def _project_latlon_extent(self, extent: List[float]) -&gt; Union[List[float], List[float]]:\n        \"\"\"Projects the given extent from lat/lon to the grid's CRS.\n\n        Args:\n            extent (List[float]): The extent to project. Order: [lon_min, lat_min, lon_max, lat_max]\n\n        Returns:\n            List[float]: The projected extent.\n        \"\"\"\n        transformer = Transformer.from_crs(\"EPSG:4326\", f\"EPSG:{self.GRID_CRS_EPSG_CODE}\")\n        # EPSG4326: xx -&gt; lat, yy -&gt; lon\n        # EPSG3035: xx -&gt; northing, yy -&gt; easting\n        xx_bottomleft, yy_bottomleft = transformer.transform(extent[1], extent[0])  # bottom-left corner\n        xx_topright, yy_topright = transformer.transform(extent[3], extent[2])  # top-right corner\n        xx_bottomright, yy_bottomright = transformer.transform(extent[1], extent[2])  # bottom-right corner\n        xx_topleft, yy_topleft = transformer.transform(extent[3], extent[0])\n\n        return (\n            [xx_bottomleft, yy_bottomleft, xx_topright, yy_topright],\n            [xx_bottomright, yy_bottomright, xx_topleft, yy_topleft],\n        )\n\n    @staticmethod\n    def _project_bounding_box(extent: List[float], auxiliar_coords: List[float]) -&gt; (List[float], List[float]):\n        \"\"\"Returns the bottom-left and top-right coordinates of the rectangular bounding box in the projected CRS\n        that covers the bounding box defined from the bottom-left and top-right corners in lat/lon.\n\n        Args:\n            extent (List[float]): Coordinates in the projected CRS that are the transformation of the minimum and\n                maximum latitude and longitude, in [x_bottomleft, y_bottomleft, x_topright, y_topright] order.\n            auxiliar_coords (List[float]): Auxiliar coordinates in the prohected CRS that are the transformation\n                of the other two cornes of the lat/lon rectangular bounding box, in\n                [x_bottomright, y_bottomright, x_topleft, y_topleft] order\n\n        Returns:\n            List[float]: The projected extent, in [x_bottomleft, y_bottomleft, x_topright, y_topright] order.\n            List[float]: Raster cover bounds, in [x_topleft, y_topleft, x_bottomright, y_bottomright] order.\n        \"\"\"\n        cover_x_bottomleft = min(extent[0], auxiliar_coords[0])  # min lat\n        cover_y_bottomleft = min(extent[1], auxiliar_coords[3])  # min lon\n\n        cover_x_topright = max(extent[2], auxiliar_coords[2])  # max lat\n        cover_y_topright = max(extent[3], auxiliar_coords[1])  # max lon\n\n        cover_x_topleft = max(extent[2], auxiliar_coords[2])  # max lat\n        cover_y_topleft = min(extent[1], auxiliar_coords[3])  # min lon\n\n        cover_x_bottomright = min(extent[0], auxiliar_coords[0])  # min lat\n        cover_y_bottomright = max(extent[3], auxiliar_coords[1])  # max lon\n\n        return (\n            [cover_y_bottomleft, cover_x_bottomleft, cover_y_topright, cover_x_topright],\n            [cover_x_topleft, cover_y_topleft, cover_x_bottomright, cover_y_bottomright],\n        )\n\n    def _snap_extent_to_grid(self, extent: List[float]) -&gt; List[float]:\n        \"\"\"Snaps the given extent to the grid.\n\n        Args:\n            extent (List[float]): The extent to snap.\n\n        Returns:\n            List[float]: The snapped extent.\n        \"\"\"\n        return [round(coord / self.resolution) * self.resolution for coord in extent]\n\n    def _extend_grid_extent(self, extent: List[float], extension_factor: int = 5) -&gt; List[float]:\n        \"\"\"Extends the given extent by the specified factor in all directions.\n\n        Args:\n            extent (List[float]): The extent to extend.\n            extension_factor (int, optional): The factor by which to extend the extent. Defaults to 5.\n\n        Returns:\n            List[float]: The extended extent.\n        \"\"\"\n        extension_size = self.resolution * extension_factor\n        return [\n            extent[0] - extension_size,\n            extent[1] - extension_size,\n            extent[2] + extension_size,\n            extent[3] + extension_size,\n        ]\n\n    def _extend_grid_raster_bounds(self, raster_bounds: List[float], extension_factor: int = 5) -&gt; List[float]:\n        \"\"\"Extends the given extent by the specified factor in all directions.\n\n        Args:.\n            extent (List[float]): Raster cover bounds, in [x_topleft, y_topleft, x_bottomright, y_bottomright] order.\n\n            extension_factor (int, optional): The factor by which to extend the extent. Defaults to 5.\n\n        Returns:\n            List[float]: The extended extent.\n        \"\"\"\n        extension_size = self.resolution * extension_factor\n        return [\n            raster_bounds[0] + extension_size,  # x topleft\n            raster_bounds[1] - extension_size,  # y topleft\n            raster_bounds[2] - extension_size,  # x bottomright\n            raster_bounds[3] + extension_size,  # y bottomright\n        ]\n\n    def _get_grid_height(self, raster_bounds: List[float]) -&gt; int:\n        \"\"\"Calculates the height of the grid for the given extent.\n\n        Args:\n            raster_bounds (List[float]): The raster_bounds for which to calculate the grid height.\n\n        Returns:\n            int: The grid height.\n        \"\"\"\n        return int((raster_bounds[0] - raster_bounds[2]) / self.resolution)\n\n    def _get_grid_width(self, raster_bounds: List[float]) -&gt; int:\n        \"\"\"Calculates the width of the grid for the given extent.\n\n        Args:\n            raster_bounds (List[float]): The raster_bounds for which to calculate the grid width.\n\n        Returns:\n            int: The grid width.\n        \"\"\"\n        return int((raster_bounds[3] - raster_bounds[1]) / self.resolution)\n\n    def _get_grid_blueprint(self, extent: List[float]) -&gt; DataFrame:\n        \"\"\"Generates a blueprint for the grid for the given extent as a raster of grid resolution.\n        Splits initial raster into smaller rasters of size grid_partition_size x grid_partition_size.\n\n        Args:\n            extent (List[float]): The extent for which to generate the grid blueprint.\n\n        Returns:\n            DataFrame: The grid blueprint.\n        \"\"\"\n        extent, auxiliar_coords = self._project_latlon_extent(extent)\n        extent, raster_bounds = self._project_bounding_box(extent, auxiliar_coords)\n\n        extent = self._snap_extent_to_grid(extent)\n        raster_bounds = self._snap_extent_to_grid(raster_bounds)\n\n        extent = self._extend_grid_extent(extent)\n        raster_bounds = self._extend_grid_raster_bounds(raster_bounds)\n\n        grid_height = self._get_grid_height(raster_bounds)\n        grid_width = self._get_grid_width(raster_bounds)\n\n        # ONLY FOR EPSG:3035!!!! which has (northing, easting) order, BUT (Y, X) axis names in its EPSG (not in code)\n        # raster_bounds[1]: easting of the top left corner. \"X\" axis\n        # raster_bounds[0]: northing of the top left corner. \"Y\" axis\n\n        sdf = self.spark.sql(\n            f\"\"\"SELECT RS_MakeEmptyRaster(1, \"B\", {grid_width}, \n                                {grid_height}, \n                                {raster_bounds[1]},\n                                {raster_bounds[0]}, \n                                {self.resolution}, \n                               -{self.resolution}, 0.0, 0.0, {self.GRID_CRS_EPSG_CODE}) as raster\"\"\"\n        )\n\n        sdf = sdf.selectExpr(f\"RS_TileExplode(raster,{self.grid_partition_size}, {self.grid_partition_size})\")\n        return sdf.repartition(sdf.count())\n\n    @staticmethod\n    def _get_polygon_sdf_extent(polygon_sdf: DataFrame) -&gt; List[float]:\n        \"\"\"Gets the extent of the given polygon DataFrame.\n\n        Args:\n            polygon_sdf (DataFrame): The polygon DataFrame.\n\n        Returns:\n            List[float]: The extent of the polygon DataFrame.\n        \"\"\"\n        polygon_sdf = polygon_sdf.withColumn(\"bbox\", STF.ST_Envelope(polygon_sdf[\"geometry\"]))\n        polygon_sdf = (\n            polygon_sdf.withColumn(\"x_min\", STF.ST_XMin(polygon_sdf[\"bbox\"]))\n            .withColumn(\"y_min\", STF.ST_YMin(polygon_sdf[\"bbox\"]))\n            .withColumn(\"x_max\", STF.ST_XMax(polygon_sdf[\"bbox\"]))\n            .withColumn(\"y_max\", STF.ST_YMax(polygon_sdf[\"bbox\"]))\n        )\n\n        return polygon_sdf.select(\"x_min\", \"y_min\", \"x_max\", \"y_max\").collect()[0][0:]\n\n    def _get_grid_intersection_with_mask(self, sdf: DataFrame, polygon_sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"Gets the intersection of the grid with the given mask.\n\n        Args:\n            sdf (DataFrame): The DataFrame representing the grid.\n            polygon_sdf (DataFrame): The DataFrame representing the mask.\n\n        Returns:\n            DataFrame: The DataFrame representing the intersection of the grid with the mask.\n        \"\"\"\n        polygon_sdf = polygon_sdf.withColumn(\n            \"geometry\",\n            STF.ST_Transform(polygon_sdf[\"geometry\"], F.lit(\"EPSG:4326\"), F.lit(f\"EPSG:{self.GRID_CRS_EPSG_CODE}\")),\n        )\n\n        sdf = sdf.join(\n            polygon_sdf, STP.ST_Intersects(sdf[self.geometry_col_name], polygon_sdf[\"geometry\"]), \"inner\"\n        ).drop(polygon_sdf[\"geometry\"])\n\n        return sdf\n\n    def _get_grid_intersection_with_mask(self, sdf: DataFrame, polygon_sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"Covers the given extent with grid centroids.\n\n        Args:\n            extent (List[float]): The extent to cover.\n\n        Returns:\n            DataFrame: The DataFrame representing the grid centroids covering the extent.\n        \"\"\"\n        polygon_sdf = polygon_sdf.withColumn(\n            \"geometry\",\n            STF.ST_Transform(polygon_sdf[\"geometry\"], F.lit(\"EPSG:4326\"), F.lit(f\"EPSG:{self.GRID_CRS_EPSG_CODE}\")),\n        )\n\n        sdf = sdf.join(\n            polygon_sdf, STP.ST_Intersects(sdf[self.geometry_col_name], polygon_sdf[\"geometry\"]), \"inner\"\n        ).drop(polygon_sdf[\"geometry\"])\n\n        return sdf\n\n    def cover_extent_with_grid_centroids(self, extent: List[float]) -&gt; DataFrame:\n        \"\"\"Covers the given polygon with grid centroids.\n\n        Args:\n            extent (List[float]): The extent to cover.\n\n        Returns:\n            DataFrame: The DataFrame representing the grid centroids covering the polygon.\n        \"\"\"\n        sdf = self._get_grid_blueprint(extent)\n\n        sdf = sdf.selectExpr(\"explode(RS_PixelAsCentroids(tile, 1)) as exploded\").selectExpr(\n            f\"exploded.geom as {self.geometry_col_name}\"\n        )\n\n        sdf = sdf.withColumn(\n            self.grid_id_col_name,\n            F.concat(\n                F.lit(self.resolution_str),\n                F.lit(\"N\"),\n                (STF.ST_Y(sdf[\"geometry\"]) - self.resolution / 2).cast(IntegerType()),\n                F.lit(\"E\"),\n                (STF.ST_X(sdf[\"geometry\"]) - self.resolution / 2).cast(IntegerType()),\n            ),\n        )\n\n        return sdf\n\n    def cover_polygon_with_grid_centroids(self, polygon_sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"Covers the given extent with grid IDs.\n\n        Args:\n            polygon_sdf (DataFrame): The DataFrame representing the polygon.\n\n        Returns:\n            DataFrame: The DataFrame representing the grid IDs covering the extent.\n        \"\"\"\n        extent = self._get_polygon_sdf_extent(polygon_sdf)\n\n        sdf = self.cover_extent_with_grid_centroids(extent)\n\n        sdf = self._get_grid_intersection_with_mask(sdf, polygon_sdf)\n\n        return sdf\n\n    def cover_extent_with_grid_ids(self, extent: List[float]) -&gt; DataFrame:\n        \"\"\"Covers the given polygon with grid IDs.\n\n        Args:\n            extent (List[float]): The extent to cover.\n\n        Returns:\n            DataFrame: The DataFrame representing the grid IDs covering the polygon.\n        \"\"\"\n        sdf = self.cover_extent_with_grid_centroids(extent)\n\n        sdf = sdf.drop(self.geometry_col_name)\n\n        return sdf\n\n    def cover_polygon_with_grid_ids(self, polygon_sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"Covers the given polygon with grid IDs.\n\n        Args:\n            polygon_sdf (DataFrame): The DataFrame representing the polygon.\n\n        Returns:\n            DataFrame: The DataFrame representing the grid IDs covering the polygon.\n        \"\"\"\n        extent = self._get_polygon_sdf_extent(polygon_sdf)\n\n        sdf = self.cover_extent_with_grid_centroids(extent)\n\n        sdf = self._get_grid_intersection_with_mask(sdf, polygon_sdf)\n\n        sdf = sdf.drop(\"geometry\")\n\n        return sdf\n\n    def cover_extent_with_grid_tiles(self, extent: List[float]) -&gt; DataFrame:\n        \"\"\"Covers the given extent with grid tiles.\n\n        Args:\n            extent (List[float]): The extent to cover.\n\n        Returns:\n            DataFrame: The DataFrame representing the grid tiles covering the extent.\n        \"\"\"\n        sdf = self._get_grid_blueprint(extent)\n\n        sdf = sdf.selectExpr(\"explode(RS_PixelAsPolygons(tile, 1)) as exploded\").selectExpr(\n            f\"exploded.geom as {self.geometry_col_name}\"\n        )\n\n        sdf = sdf.withColumn(\n            self.grid_id_col_name,\n            F.concat(\n                F.lit(self.resolution_str),\n                F.lit(\"N\"),\n                STF.ST_XMin(sdf[\"geometry\"]).cast(IntegerType()),\n                F.lit(\"E\"),\n                STF.ST_YMin(sdf[\"geometry\"]).cast(IntegerType()),\n            ),\n        )\n\n        return sdf\n\n    def cover_polygon_with_grid_tiles(self, polygon_sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"Covers the given polygon with grid tiles.\n\n        Args:\n            polygon_sdf (DataFrame): The DataFrame representing the polygon.\n\n        Returns:\n            DataFrame: The DataFrame representing the grid tiles covering the polygon.\n        \"\"\"\n        extent = self._get_polygon_sdf_extent(polygon_sdf)\n\n        sdf = self.cover_extent_with_grid_tiles(extent)\n\n        sdf = self._get_grid_intersection_with_mask(sdf, polygon_sdf)\n\n        return sdf\n\n    def grid_ids_to_centroids(self, sdf: DataFrame, to_crs: int = None) -&gt; DataFrame:\n        \"\"\"Converts grid IDs to centroids.\n\n        Args:\n            sdf (DataFrame): The DataFrame containing the grid IDs.\n            to_crs (int, optional): The CRS to which to convert the centroids. If not provided,\n                the centroids will be in default grid crs.\n\n        Returns:\n            DataFrame: The DataFrame containing the centroids.\n        \"\"\"\n        sdf = sdf.withColumn(\n            self.geometry_col_name,\n            STC.ST_Point(\n                F.regexp_extract(sdf[self.grid_id_col_name], r\"E(\\d+)\", 1) + self.resolution / 2,\n                F.regexp_extract(sdf[self.grid_id_col_name], r\"N(\\d+)\", 1) + self.resolution / 2,\n            ),\n        )\n\n        sdf = sdf.withColumn(\n            self.geometry_col_name, STF.ST_SetSRID(sdf[self.geometry_col_name], self.GRID_CRS_EPSG_CODE)\n        )\n\n        if to_crs:\n            sdf = sdf.withColumn(\n                self.geometry_col_name, STF.ST_Transform(sdf[self.geometry_col_name], F.lit(f\"EPSG:{to_crs}\"))\n            )\n\n        return sdf\n\n    def grid_ids_to_tiles(self, sdf: DataFrame, to_crs: int = None) -&gt; DataFrame:\n        \"\"\"Converts grid IDs to tiles.\n\n        Args:\n            sdf (DataFrame): The DataFrame containing the grid IDs.\n            to_crs (int, optional): The CRS to which to convert the tiles. If not provided, the centroids\n                will be in default grid crs.\n        Returns:\n            DataFrame: The DataFrame containing the tiles.\n        \"\"\"\n        sdf = sdf.withColumn(\n            self.geometry_col_name,\n            STC.ST_PolygonFromEnvelope(\n                F.regexp_extract(sdf[self.grid_id_col_name], r\"E(\\d+)\", 1),\n                F.regexp_extract(sdf[self.grid_id_col_name], r\"N(\\d+)\", 1),\n                F.regexp_extract(sdf[self.grid_id_col_name], r\"E(\\d+)\", 1) + self.resolution,\n                F.regexp_extract(sdf[self.grid_id_col_name], r\"N(\\d+)\", 1) + self.resolution,\n            ),\n        )\n\n        sdf = sdf.withColumn(\n            self.geometry_col_name, STF.ST_SetSRID(sdf[self.geometry_col_name], self.GRID_CRS_EPSG_CODE)\n        )\n\n        if to_crs:\n            sdf = sdf.withColumn(\n                self.geometry_col_name, STF.ST_Transform(sdf[self.geometry_col_name], F.lit(f\"EPSG:{to_crs}\"))\n            )\n\n        return sdf\n\n    def get_children_grid_ids(self, grid_id, resolution):\n        # TODO: Implement this method\n        pass\n\n    def get_parent_grid_id(self, grid_id, resolution):\n        # TODO: Implement this method\n        pass\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.__init__","title":"<code>__init__(spark, resolution=100, geometry_col_name='geometry', grid_id_col_name='grid_id', grid_partition_size=2000)</code>","text":"<p>Initializes the InspireGridGenerator with the given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>The SparkSession to use.</p> required <code>resolution</code> <code>int</code> <p>The resolution of the grid. Defaults to 100. Has to be divisible by 100.</p> <code>100</code> <code>geometry_col_name</code> <code>str</code> <p>The name of the geometry column. Defaults to 'geometry'.</p> <code>'geometry'</code> <code>grid_id_col_name</code> <code>str</code> <p>The name of the grid ID column. Defaults to 'grid_id'.</p> <code>'grid_id'</code> <code>grid_partition_size</code> <code>int</code> <p>The size of the grid partitions, defined as number of tiles in x and y dimensions of subdivisions of the intital grid. Defaults to 2000.</p> <code>2000</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the resolution is not divisible by 100.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def __init__(\n    self,\n    spark: SparkSession,\n    resolution=100,\n    geometry_col_name: str = \"geometry\",\n    grid_id_col_name: str = \"grid_id\",\n    grid_partition_size: int = 2000,\n) -&gt; None:\n    \"\"\"Initializes the InspireGridGenerator with the given parameters.\n\n    Args:\n        spark (SparkSession): The SparkSession to use.\n        resolution (int, optional): The resolution of the grid. Defaults to 100. Has to be divisible by 100.\n        geometry_col_name (str, optional): The name of the geometry column. Defaults to 'geometry'.\n        grid_id_col_name (str, optional): The name of the grid ID column. Defaults to 'grid_id'.\n        grid_partition_size (int, optional): The size of the grid partitions, defined as number of tiles\n            in x and y dimensions of subdivisions of the intital grid. Defaults to 2000.\n\n    Raises:\n        ValueError: If the resolution is not divisible by 100.\n    \"\"\"\n    if resolution % 100 != 0:\n        raise ValueError(\"Resolution must be divisible by 100\")\n\n    super().__init__(spark)\n    self.geometry_col_name = geometry_col_name\n    self.grid_id_col_name = grid_id_col_name\n    self.resolution = resolution\n    self.grid_partition_size = grid_partition_size\n    self.resolution_str = self._format_distance(resolution)\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.cover_extent_with_grid_centroids","title":"<code>cover_extent_with_grid_centroids(extent)</code>","text":"<p>Covers the given polygon with grid centroids.</p> <p>Parameters:</p> Name Type Description Default <code>extent</code> <code>List[float]</code> <p>The extent to cover.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The DataFrame representing the grid centroids covering the polygon.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def cover_extent_with_grid_centroids(self, extent: List[float]) -&gt; DataFrame:\n    \"\"\"Covers the given polygon with grid centroids.\n\n    Args:\n        extent (List[float]): The extent to cover.\n\n    Returns:\n        DataFrame: The DataFrame representing the grid centroids covering the polygon.\n    \"\"\"\n    sdf = self._get_grid_blueprint(extent)\n\n    sdf = sdf.selectExpr(\"explode(RS_PixelAsCentroids(tile, 1)) as exploded\").selectExpr(\n        f\"exploded.geom as {self.geometry_col_name}\"\n    )\n\n    sdf = sdf.withColumn(\n        self.grid_id_col_name,\n        F.concat(\n            F.lit(self.resolution_str),\n            F.lit(\"N\"),\n            (STF.ST_Y(sdf[\"geometry\"]) - self.resolution / 2).cast(IntegerType()),\n            F.lit(\"E\"),\n            (STF.ST_X(sdf[\"geometry\"]) - self.resolution / 2).cast(IntegerType()),\n        ),\n    )\n\n    return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.cover_extent_with_grid_ids","title":"<code>cover_extent_with_grid_ids(extent)</code>","text":"<p>Covers the given polygon with grid IDs.</p> <p>Parameters:</p> Name Type Description Default <code>extent</code> <code>List[float]</code> <p>The extent to cover.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The DataFrame representing the grid IDs covering the polygon.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def cover_extent_with_grid_ids(self, extent: List[float]) -&gt; DataFrame:\n    \"\"\"Covers the given polygon with grid IDs.\n\n    Args:\n        extent (List[float]): The extent to cover.\n\n    Returns:\n        DataFrame: The DataFrame representing the grid IDs covering the polygon.\n    \"\"\"\n    sdf = self.cover_extent_with_grid_centroids(extent)\n\n    sdf = sdf.drop(self.geometry_col_name)\n\n    return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.cover_extent_with_grid_tiles","title":"<code>cover_extent_with_grid_tiles(extent)</code>","text":"<p>Covers the given extent with grid tiles.</p> <p>Parameters:</p> Name Type Description Default <code>extent</code> <code>List[float]</code> <p>The extent to cover.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The DataFrame representing the grid tiles covering the extent.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def cover_extent_with_grid_tiles(self, extent: List[float]) -&gt; DataFrame:\n    \"\"\"Covers the given extent with grid tiles.\n\n    Args:\n        extent (List[float]): The extent to cover.\n\n    Returns:\n        DataFrame: The DataFrame representing the grid tiles covering the extent.\n    \"\"\"\n    sdf = self._get_grid_blueprint(extent)\n\n    sdf = sdf.selectExpr(\"explode(RS_PixelAsPolygons(tile, 1)) as exploded\").selectExpr(\n        f\"exploded.geom as {self.geometry_col_name}\"\n    )\n\n    sdf = sdf.withColumn(\n        self.grid_id_col_name,\n        F.concat(\n            F.lit(self.resolution_str),\n            F.lit(\"N\"),\n            STF.ST_XMin(sdf[\"geometry\"]).cast(IntegerType()),\n            F.lit(\"E\"),\n            STF.ST_YMin(sdf[\"geometry\"]).cast(IntegerType()),\n        ),\n    )\n\n    return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.cover_polygon_with_grid_centroids","title":"<code>cover_polygon_with_grid_centroids(polygon_sdf)</code>","text":"<p>Covers the given extent with grid IDs.</p> <p>Parameters:</p> Name Type Description Default <code>polygon_sdf</code> <code>DataFrame</code> <p>The DataFrame representing the polygon.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The DataFrame representing the grid IDs covering the extent.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def cover_polygon_with_grid_centroids(self, polygon_sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"Covers the given extent with grid IDs.\n\n    Args:\n        polygon_sdf (DataFrame): The DataFrame representing the polygon.\n\n    Returns:\n        DataFrame: The DataFrame representing the grid IDs covering the extent.\n    \"\"\"\n    extent = self._get_polygon_sdf_extent(polygon_sdf)\n\n    sdf = self.cover_extent_with_grid_centroids(extent)\n\n    sdf = self._get_grid_intersection_with_mask(sdf, polygon_sdf)\n\n    return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.cover_polygon_with_grid_ids","title":"<code>cover_polygon_with_grid_ids(polygon_sdf)</code>","text":"<p>Covers the given polygon with grid IDs.</p> <p>Parameters:</p> Name Type Description Default <code>polygon_sdf</code> <code>DataFrame</code> <p>The DataFrame representing the polygon.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The DataFrame representing the grid IDs covering the polygon.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def cover_polygon_with_grid_ids(self, polygon_sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"Covers the given polygon with grid IDs.\n\n    Args:\n        polygon_sdf (DataFrame): The DataFrame representing the polygon.\n\n    Returns:\n        DataFrame: The DataFrame representing the grid IDs covering the polygon.\n    \"\"\"\n    extent = self._get_polygon_sdf_extent(polygon_sdf)\n\n    sdf = self.cover_extent_with_grid_centroids(extent)\n\n    sdf = self._get_grid_intersection_with_mask(sdf, polygon_sdf)\n\n    sdf = sdf.drop(\"geometry\")\n\n    return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.cover_polygon_with_grid_tiles","title":"<code>cover_polygon_with_grid_tiles(polygon_sdf)</code>","text":"<p>Covers the given polygon with grid tiles.</p> <p>Parameters:</p> Name Type Description Default <code>polygon_sdf</code> <code>DataFrame</code> <p>The DataFrame representing the polygon.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The DataFrame representing the grid tiles covering the polygon.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def cover_polygon_with_grid_tiles(self, polygon_sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"Covers the given polygon with grid tiles.\n\n    Args:\n        polygon_sdf (DataFrame): The DataFrame representing the polygon.\n\n    Returns:\n        DataFrame: The DataFrame representing the grid tiles covering the polygon.\n    \"\"\"\n    extent = self._get_polygon_sdf_extent(polygon_sdf)\n\n    sdf = self.cover_extent_with_grid_tiles(extent)\n\n    sdf = self._get_grid_intersection_with_mask(sdf, polygon_sdf)\n\n    return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.grid_ids_to_centroids","title":"<code>grid_ids_to_centroids(sdf, to_crs=None)</code>","text":"<p>Converts grid IDs to centroids.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The DataFrame containing the grid IDs.</p> required <code>to_crs</code> <code>int</code> <p>The CRS to which to convert the centroids. If not provided, the centroids will be in default grid crs.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The DataFrame containing the centroids.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def grid_ids_to_centroids(self, sdf: DataFrame, to_crs: int = None) -&gt; DataFrame:\n    \"\"\"Converts grid IDs to centroids.\n\n    Args:\n        sdf (DataFrame): The DataFrame containing the grid IDs.\n        to_crs (int, optional): The CRS to which to convert the centroids. If not provided,\n            the centroids will be in default grid crs.\n\n    Returns:\n        DataFrame: The DataFrame containing the centroids.\n    \"\"\"\n    sdf = sdf.withColumn(\n        self.geometry_col_name,\n        STC.ST_Point(\n            F.regexp_extract(sdf[self.grid_id_col_name], r\"E(\\d+)\", 1) + self.resolution / 2,\n            F.regexp_extract(sdf[self.grid_id_col_name], r\"N(\\d+)\", 1) + self.resolution / 2,\n        ),\n    )\n\n    sdf = sdf.withColumn(\n        self.geometry_col_name, STF.ST_SetSRID(sdf[self.geometry_col_name], self.GRID_CRS_EPSG_CODE)\n    )\n\n    if to_crs:\n        sdf = sdf.withColumn(\n            self.geometry_col_name, STF.ST_Transform(sdf[self.geometry_col_name], F.lit(f\"EPSG:{to_crs}\"))\n        )\n\n    return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.grid_ids_to_tiles","title":"<code>grid_ids_to_tiles(sdf, to_crs=None)</code>","text":"<p>Converts grid IDs to tiles.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The DataFrame containing the grid IDs.</p> required <code>to_crs</code> <code>int</code> <p>The CRS to which to convert the tiles. If not provided, the centroids will be in default grid crs.</p> <code>None</code> <p>Returns:     DataFrame: The DataFrame containing the tiles.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def grid_ids_to_tiles(self, sdf: DataFrame, to_crs: int = None) -&gt; DataFrame:\n    \"\"\"Converts grid IDs to tiles.\n\n    Args:\n        sdf (DataFrame): The DataFrame containing the grid IDs.\n        to_crs (int, optional): The CRS to which to convert the tiles. If not provided, the centroids\n            will be in default grid crs.\n    Returns:\n        DataFrame: The DataFrame containing the tiles.\n    \"\"\"\n    sdf = sdf.withColumn(\n        self.geometry_col_name,\n        STC.ST_PolygonFromEnvelope(\n            F.regexp_extract(sdf[self.grid_id_col_name], r\"E(\\d+)\", 1),\n            F.regexp_extract(sdf[self.grid_id_col_name], r\"N(\\d+)\", 1),\n            F.regexp_extract(sdf[self.grid_id_col_name], r\"E(\\d+)\", 1) + self.resolution,\n            F.regexp_extract(sdf[self.grid_id_col_name], r\"N(\\d+)\", 1) + self.resolution,\n        ),\n    )\n\n    sdf = sdf.withColumn(\n        self.geometry_col_name, STF.ST_SetSRID(sdf[self.geometry_col_name], self.GRID_CRS_EPSG_CODE)\n    )\n\n    if to_crs:\n        sdf = sdf.withColumn(\n            self.geometry_col_name, STF.ST_Transform(sdf[self.geometry_col_name], F.lit(f\"EPSG:{to_crs}\"))\n        )\n\n    return sdf\n</code></pre>"},{"location":"reference/core/io_interface/","title":"io_interface","text":"<p>Module that implements classes for reading data from different data sources into a Spark DataFrames.</p>"},{"location":"reference/core/io_interface/#core.io_interface.CsvInterface","title":"<code>CsvInterface</code>","text":"<p>               Bases: <code>PathInterface</code></p> <p>Class that implements the PathInterface abstract class for reading/writing data from a csv data source.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class CsvInterface(PathInterface):\n    \"\"\"Class that implements the PathInterface abstract class for reading/writing data from a csv data source.\"\"\"\n\n    FILE_FORMAT = \"csv\"\n\n    def read_from_interface(\n        self,\n        spark: SparkSession,\n        path: str,\n        schema: StructType,\n        header: bool = True,\n        sep: str = \",\",\n    ) -&gt; DataFrame:\n        \"\"\"Method that reads data from a csv type data source as a Spark DataFrame.\n\n        Args:\n            spark (SparkSession): Spark session.\n            path (str): Path to the data.\n            schema (StructType, optional): Schema of the data. Defaults to None.\n\n        Returns:\n            df: Spark dataframe.\n        \"\"\"\n        return spark.read.csv(path, schema=schema, header=header, sep=sep)\n\n    def write_from_interface(\n        self,\n        df: DataFrame,\n        path: str,\n        partition_columns: list[str] = None,\n        header: bool = True,\n        sep: str = \",\",\n    ):\n        \"\"\"Method that writes data from a Spark DataFrame to a csv data source.\n\n        Args:\n            df (DataFrame): Spark DataFrame.\n            path (str): Path to the data.\n            partition_columns (list[str], optional): columns used for a partition write.\n        Raises:\n            NotImplementedError: csv files should not be written in this architecture.\n        \"\"\"\n        if partition_columns is None:\n            partition_columns = []\n        df.write.option(\"header\", header).option(\"sep\", sep).mode(\"overwrite\").format(\"csv\").save(path)\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.CsvInterface.read_from_interface","title":"<code>read_from_interface(spark, path, schema, header=True, sep=',')</code>","text":"<p>Method that reads data from a csv type data source as a Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session.</p> required <code>path</code> <code>str</code> <p>Path to the data.</p> required <code>schema</code> <code>StructType</code> <p>Schema of the data. Defaults to None.</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>Spark dataframe.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def read_from_interface(\n    self,\n    spark: SparkSession,\n    path: str,\n    schema: StructType,\n    header: bool = True,\n    sep: str = \",\",\n) -&gt; DataFrame:\n    \"\"\"Method that reads data from a csv type data source as a Spark DataFrame.\n\n    Args:\n        spark (SparkSession): Spark session.\n        path (str): Path to the data.\n        schema (StructType, optional): Schema of the data. Defaults to None.\n\n    Returns:\n        df: Spark dataframe.\n    \"\"\"\n    return spark.read.csv(path, schema=schema, header=header, sep=sep)\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.CsvInterface.write_from_interface","title":"<code>write_from_interface(df, path, partition_columns=None, header=True, sep=',')</code>","text":"<p>Method that writes data from a Spark DataFrame to a csv data source.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Spark DataFrame.</p> required <code>path</code> <code>str</code> <p>Path to the data.</p> required <code>partition_columns</code> <code>list[str]</code> <p>columns used for a partition write.</p> <code>None</code> <p>Raises:     NotImplementedError: csv files should not be written in this architecture.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def write_from_interface(\n    self,\n    df: DataFrame,\n    path: str,\n    partition_columns: list[str] = None,\n    header: bool = True,\n    sep: str = \",\",\n):\n    \"\"\"Method that writes data from a Spark DataFrame to a csv data source.\n\n    Args:\n        df (DataFrame): Spark DataFrame.\n        path (str): Path to the data.\n        partition_columns (list[str], optional): columns used for a partition write.\n    Raises:\n        NotImplementedError: csv files should not be written in this architecture.\n    \"\"\"\n    if partition_columns is None:\n        partition_columns = []\n    df.write.option(\"header\", header).option(\"sep\", sep).mode(\"overwrite\").format(\"csv\").save(path)\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.GeoParquetInterface","title":"<code>GeoParquetInterface</code>","text":"<p>               Bases: <code>PathInterface</code></p> <p>Class that implements the PathInterface abstract class for reading/writing data from a geoparquet data source.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class GeoParquetInterface(PathInterface):\n    \"\"\"Class that implements the PathInterface abstract class for reading/writing data from a geoparquet data source.\"\"\"\n\n    FILE_FORMAT = \"geoparquet\"\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.HttpGeoJsonInterface","title":"<code>HttpGeoJsonInterface</code>","text":"<p>               Bases: <code>IOInterface</code></p> <p>Class that implements the IO interface abstract class for reading GeoJSON data from an HTTP source.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class HttpGeoJsonInterface(IOInterface):\n    \"\"\"Class that implements the IO interface abstract class for reading GeoJSON data from an HTTP source.\"\"\"\n\n    def read_from_interface(self, spark: SparkSession, url: str, timeout: int = 60, max_retries: int = 5) -&gt; DataFrame:\n        \"\"\"Method that reads GeoJSON data from an HTTP source and converts it to a Spark DataFrame.\n\n        Args:\n            url (str): URL of the GeoJSON data.\n            timeout (int): Timeout for the GET request in seconds. Default is 60.\n            max_retries (int): Maximum number of retries for the GET request. Default is 5.\n\n        Returns:\n            df: Spark DataFrame.\n        \"\"\"\n        session = requests.Session()\n        retry = Retry(total=max_retries, backoff_factor=0.1, status_forcelist=[500, 502, 503, 504])\n        adapter = HTTPAdapter(max_retries=retry)\n        session.mount(\"http://\", adapter)\n        session.mount(\"https://\", adapter)\n\n        try:\n            response = session.get(url, timeout=timeout)\n        except requests.exceptions.RequestException as e:\n            print(e)\n            raise Exception(\"Maximum number of retries exceeded.\")\n\n        if response.status_code != 200:\n            raise Exception(\"GET request not successful.\")\n\n        # Read the GeoJSON data into a GeoDataFrame\n        gdf = gpd.read_file(StringIO(response.text))\n\n        # Convert the GeoDataFrame to a Spark DataFrame\n        df = spark.createDataFrame(gdf)\n\n        return df\n\n    def write_from_interface(self, df: DataFrame, url: str, timeout: int = 60, max_retries: int = 5):\n        \"\"\"Method that writes a DataFrame to an HTTP source as GeoJSON data.\n\n        Args:\n            df (DataFrame): DataFrame to write.\n            url (str): URL of the HTTP source.\n            timeout (int): Timeout for the POST request in seconds. Default is 60.\n            max_retries (int): Maximum number of retries for the POST request. Default is 5.\n        \"\"\"\n        raise NotImplementedError(\"This method is not implemented.\")\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.HttpGeoJsonInterface.read_from_interface","title":"<code>read_from_interface(spark, url, timeout=60, max_retries=5)</code>","text":"<p>Method that reads GeoJSON data from an HTTP source and converts it to a Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL of the GeoJSON data.</p> required <code>timeout</code> <code>int</code> <p>Timeout for the GET request in seconds. Default is 60.</p> <code>60</code> <code>max_retries</code> <code>int</code> <p>Maximum number of retries for the GET request. Default is 5.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>Spark DataFrame.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def read_from_interface(self, spark: SparkSession, url: str, timeout: int = 60, max_retries: int = 5) -&gt; DataFrame:\n    \"\"\"Method that reads GeoJSON data from an HTTP source and converts it to a Spark DataFrame.\n\n    Args:\n        url (str): URL of the GeoJSON data.\n        timeout (int): Timeout for the GET request in seconds. Default is 60.\n        max_retries (int): Maximum number of retries for the GET request. Default is 5.\n\n    Returns:\n        df: Spark DataFrame.\n    \"\"\"\n    session = requests.Session()\n    retry = Retry(total=max_retries, backoff_factor=0.1, status_forcelist=[500, 502, 503, 504])\n    adapter = HTTPAdapter(max_retries=retry)\n    session.mount(\"http://\", adapter)\n    session.mount(\"https://\", adapter)\n\n    try:\n        response = session.get(url, timeout=timeout)\n    except requests.exceptions.RequestException as e:\n        print(e)\n        raise Exception(\"Maximum number of retries exceeded.\")\n\n    if response.status_code != 200:\n        raise Exception(\"GET request not successful.\")\n\n    # Read the GeoJSON data into a GeoDataFrame\n    gdf = gpd.read_file(StringIO(response.text))\n\n    # Convert the GeoDataFrame to a Spark DataFrame\n    df = spark.createDataFrame(gdf)\n\n    return df\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.HttpGeoJsonInterface.write_from_interface","title":"<code>write_from_interface(df, url, timeout=60, max_retries=5)</code>","text":"<p>Method that writes a DataFrame to an HTTP source as GeoJSON data.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to write.</p> required <code>url</code> <code>str</code> <p>URL of the HTTP source.</p> required <code>timeout</code> <code>int</code> <p>Timeout for the POST request in seconds. Default is 60.</p> <code>60</code> <code>max_retries</code> <code>int</code> <p>Maximum number of retries for the POST request. Default is 5.</p> <code>5</code> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def write_from_interface(self, df: DataFrame, url: str, timeout: int = 60, max_retries: int = 5):\n    \"\"\"Method that writes a DataFrame to an HTTP source as GeoJSON data.\n\n    Args:\n        df (DataFrame): DataFrame to write.\n        url (str): URL of the HTTP source.\n        timeout (int): Timeout for the POST request in seconds. Default is 60.\n        max_retries (int): Maximum number of retries for the POST request. Default is 5.\n    \"\"\"\n    raise NotImplementedError(\"This method is not implemented.\")\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.IOInterface","title":"<code>IOInterface</code>","text":"<p>Abstract interface that provides functionality for reading and writing data</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class IOInterface(metaclass=ABCMeta):\n    \"\"\"Abstract interface that provides functionality for reading and writing data\"\"\"\n\n    @classmethod\n    def __subclasshook__(cls, subclass: type) -&gt; bool:\n        if cls is IOInterface:\n            attrs: list[str] = []\n            callables: list[str] = [\"read_from_interface\", \"write_from_interface\"]\n            ret: bool = True\n            for attr in attrs:\n                ret = ret and (hasattr(subclass, attr) and isinstance(getattr(subclass, attr), property))\n            for call in callables:\n                ret = ret and (hasattr(subclass, call) and callable(getattr(subclass, call)))\n            return ret\n        else:\n            return NotImplemented\n\n    @abstractmethod\n    def read_from_interface(self, *args, **kwargs) -&gt; DataFrame:\n        pass\n\n    @abstractmethod\n    def write_from_interface(self, df: DataFrame, *args, **kwargs):\n        pass\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.JsonInterface","title":"<code>JsonInterface</code>","text":"<p>               Bases: <code>PathInterface</code></p> <p>Class that implements the PathInterface abstract class for reading/writing data from a json data source.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class JsonInterface(PathInterface):\n    \"\"\"Class that implements the PathInterface abstract class for reading/writing data from a json data source.\"\"\"\n\n    FILE_FORMAT = \"json\"\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.ParquetInterface","title":"<code>ParquetInterface</code>","text":"<p>               Bases: <code>PathInterface</code></p> <p>Class that implements the PathInterface abstract class for reading/writing data from a parquet data source.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class ParquetInterface(PathInterface):\n    \"\"\"Class that implements the PathInterface abstract class for reading/writing data from a parquet data source.\"\"\"\n\n    FILE_FORMAT = \"parquet\"\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.PathInterface","title":"<code>PathInterface</code>","text":"<p>               Bases: <code>IOInterface</code></p> <p>Abstract interface for reading/writing data from a file type data source.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class PathInterface(IOInterface, metaclass=ABCMeta):\n    \"\"\"Abstract interface for reading/writing data from a file type data source.\"\"\"\n\n    FILE_FORMAT = \"\"\n\n    def read_from_interface(self, spark: SparkSession, path: str, schema: StructType = None) -&gt; DataFrame:\n        \"\"\"Method that reads data from a file type data source as a Spark DataFrame.\n\n        Args:\n            spark (SparkSession): Spark session.\n            path (str): Path to the data.\n            schema (StructType, optional): Schema of the data. Defaults to None.\n\n        Returns:\n            df: Spark dataframe.\n        \"\"\"\n        if schema is None:\n            return spark.read.format(self.FILE_FORMAT).load(path)\n        else:\n            return (\n                spark.read.schema(schema).format(self.FILE_FORMAT).load(path)\n            )  # Read schema  # File format  # Load path\n\n    def write_from_interface(self, df: DataFrame, path: str, partition_columns: list[str] = None):\n        \"\"\"Method that writes data from a Spark DataFrame to a file type data source.\n\n        Args:\n            df (DataFrame): Spark DataFrame.\n            path (str): Path to the data.\n            partition_columns (list[str], optional): columns used for a partition write.\n        \"\"\"\n        # Args check\n        if partition_columns is None:\n            partition_columns = []\n\n        df.write.format(\n            self.FILE_FORMAT,  # File format\n        ).partitionBy(partition_columns).mode(\n            \"overwrite\"\n        ).save(path)\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.PathInterface.read_from_interface","title":"<code>read_from_interface(spark, path, schema=None)</code>","text":"<p>Method that reads data from a file type data source as a Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session.</p> required <code>path</code> <code>str</code> <p>Path to the data.</p> required <code>schema</code> <code>StructType</code> <p>Schema of the data. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>Spark dataframe.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def read_from_interface(self, spark: SparkSession, path: str, schema: StructType = None) -&gt; DataFrame:\n    \"\"\"Method that reads data from a file type data source as a Spark DataFrame.\n\n    Args:\n        spark (SparkSession): Spark session.\n        path (str): Path to the data.\n        schema (StructType, optional): Schema of the data. Defaults to None.\n\n    Returns:\n        df: Spark dataframe.\n    \"\"\"\n    if schema is None:\n        return spark.read.format(self.FILE_FORMAT).load(path)\n    else:\n        return (\n            spark.read.schema(schema).format(self.FILE_FORMAT).load(path)\n        )  # Read schema  # File format  # Load path\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.PathInterface.write_from_interface","title":"<code>write_from_interface(df, path, partition_columns=None)</code>","text":"<p>Method that writes data from a Spark DataFrame to a file type data source.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Spark DataFrame.</p> required <code>path</code> <code>str</code> <p>Path to the data.</p> required <code>partition_columns</code> <code>list[str]</code> <p>columns used for a partition write.</p> <code>None</code> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def write_from_interface(self, df: DataFrame, path: str, partition_columns: list[str] = None):\n    \"\"\"Method that writes data from a Spark DataFrame to a file type data source.\n\n    Args:\n        df (DataFrame): Spark DataFrame.\n        path (str): Path to the data.\n        partition_columns (list[str], optional): columns used for a partition write.\n    \"\"\"\n    # Args check\n    if partition_columns is None:\n        partition_columns = []\n\n    df.write.format(\n        self.FILE_FORMAT,  # File format\n    ).partitionBy(partition_columns).mode(\n        \"overwrite\"\n    ).save(path)\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.ShapefileInterface","title":"<code>ShapefileInterface</code>","text":"<p>               Bases: <code>PathInterface</code></p> <p>Class that implements the PathInterface abstract class for reading/writing data from a ShapeFile data source.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class ShapefileInterface(PathInterface):\n    \"\"\"Class that implements the PathInterface abstract class for reading/writing data from a ShapeFile data source.\"\"\"\n\n    def read_from_interface(self, spark: SparkSession, path: str, schema: StructType = None) -&gt; DataFrame:\n        \"\"\"Method that reads data from a ShapeFile type data source as a Spark DataFrame.\n\n        Args:\n            spark (SparkSession): Spark session.\n            path (str): Path to the data.\n            schema (StructType, optional): Schema of the data. Defaults to None.\n\n        Returns:\n            df: Spark dataframe.\n        \"\"\"\n        df = ShapefileReader.readToGeometryRDD(spark.sparkContext, path)\n        return Adapter.toDf(df, spark)\n\n    def write_from_interface(self, df: DataFrame, path: str, partition_columns: list = None):\n        \"\"\"Method that writes data from a Spark DataFrame to a ShapeFile data source.\n\n        Args:\n            df (DataFrame): Spark DataFrame.\n            path (str): Path to the data.\n            partition_columns (list[str], optional): columns used for a partition write.\n        Raises:\n            NotImplementedError: ShapeFile files should not be written in this architecture.\n        \"\"\"\n        raise NotImplementedError(\"Not implemented as Shapefiles shouldn't be written\")\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.ShapefileInterface.read_from_interface","title":"<code>read_from_interface(spark, path, schema=None)</code>","text":"<p>Method that reads data from a ShapeFile type data source as a Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session.</p> required <code>path</code> <code>str</code> <p>Path to the data.</p> required <code>schema</code> <code>StructType</code> <p>Schema of the data. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>Spark dataframe.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def read_from_interface(self, spark: SparkSession, path: str, schema: StructType = None) -&gt; DataFrame:\n    \"\"\"Method that reads data from a ShapeFile type data source as a Spark DataFrame.\n\n    Args:\n        spark (SparkSession): Spark session.\n        path (str): Path to the data.\n        schema (StructType, optional): Schema of the data. Defaults to None.\n\n    Returns:\n        df: Spark dataframe.\n    \"\"\"\n    df = ShapefileReader.readToGeometryRDD(spark.sparkContext, path)\n    return Adapter.toDf(df, spark)\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.ShapefileInterface.write_from_interface","title":"<code>write_from_interface(df, path, partition_columns=None)</code>","text":"<p>Method that writes data from a Spark DataFrame to a ShapeFile data source.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Spark DataFrame.</p> required <code>path</code> <code>str</code> <p>Path to the data.</p> required <code>partition_columns</code> <code>list[str]</code> <p>columns used for a partition write.</p> <code>None</code> <p>Raises:     NotImplementedError: ShapeFile files should not be written in this architecture.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def write_from_interface(self, df: DataFrame, path: str, partition_columns: list = None):\n    \"\"\"Method that writes data from a Spark DataFrame to a ShapeFile data source.\n\n    Args:\n        df (DataFrame): Spark DataFrame.\n        path (str): Path to the data.\n        partition_columns (list[str], optional): columns used for a partition write.\n    Raises:\n        NotImplementedError: ShapeFile files should not be written in this architecture.\n    \"\"\"\n    raise NotImplementedError(\"Not implemented as Shapefiles shouldn't be written\")\n</code></pre>"},{"location":"reference/core/log/","title":"log","text":"<p>Module that manages the logging functionality.</p>"},{"location":"reference/core/log/#core.log.generate_logger","title":"<code>generate_logger(config, component_id)</code>","text":"<p>Function that initializes a logger.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ConfigParser</code> <p>Object with the final configuration.</p> required <p>Returns:</p> Type Description <code>Logger</code> <p>Python logging object.</p> Source code in <code>multimno/core/log.py</code> <pre><code>def generate_logger(config: ConfigParser, component_id: str):\n    \"\"\"Function that initializes a logger.\n\n    Args:\n        config (ConfigParser): Object with the final configuration.\n\n    Returns:\n        (logging.Logger): Python logging object.\n    \"\"\"\n\n    notset_level = logging.getLevelName(logging.NOTSET)\n\n    # Parse config\n    console_log_level = config.get(LoggerKeys.LOG_CONFIG_KEY, LoggerKeys.CONSOLE_LOG_LEVEL, fallback=None)\n    file_log_level = config.get(LoggerKeys.LOG_CONFIG_KEY, LoggerKeys.FILE_LOG_LEVEL, fallback=None)\n    console_format = config.get(LoggerKeys.LOG_CONFIG_KEY, LoggerKeys.CONSOLE_FORMAT, fallback=None)\n    file_format = config.get(LoggerKeys.LOG_CONFIG_KEY, LoggerKeys.FILE_FORMAT, fallback=None)\n    datefmt = config.get(LoggerKeys.LOG_CONFIG_KEY, LoggerKeys.DATEFMT, fallback=None)\n    report_path = config.get(LoggerKeys.LOG_CONFIG_KEY, LoggerKeys.REPORT_PATH, fallback=None)\n\n    # Check if logger already exists\n    logger = logging.getLogger(component_id)\n    if len(logger.handlers) &gt; 0:\n        logger.warning(f\"Logger {component_id} already exists.\")\n        return logger\n\n    # Define a console logger\n    if console_log_level is not None and console_log_level != str(notset_level):\n        # Set console handler\n        console_h = logging.StreamHandler()\n        console_h.setLevel(console_log_level)\n        # Set console formatter\n        console_formatter = logging.Formatter(fmt=console_format, datefmt=datefmt)\n        console_h.setFormatter(console_formatter)\n        # Add console handler to logger\n        logger.addHandler(console_h)\n\n    # Define a file logger\n    if file_log_level is not None and file_log_level != str(notset_level):\n        # Verify required fields for file logger\n        if report_path is None:\n            raise ValueError(\"report_path is required to build a file logger.\")\n\n        # Get log path\n        today = datetime.now().strftime(\"%y%m%d\")\n        log_path = f\"{report_path}/{component_id}/{component_id}_{today}.log\"\n        # Make report path + log dir\n        os.makedirs(os.path.dirname(log_path), exist_ok=True)\n\n        # Set File handler\n        file_h = logging.FileHandler(log_path)\n        file_h.setLevel(file_log_level)\n        # Set file formatter\n        file_formatter = logging.Formatter(fmt=file_format, datefmt=datefmt)\n        file_h.setFormatter(file_formatter)\n        # Add file handler to logger\n        logger.addHandler(file_h)\n\n    # Set logger level\n    logger.setLevel(logging.DEBUG)\n    # Return logger\n    return logger\n</code></pre>"},{"location":"reference/core/settings/","title":"settings","text":"<p>Settings module</p>"},{"location":"reference/core/spark_session/","title":"spark_session","text":"<p>Module that manages the spark session.</p>"},{"location":"reference/core/spark_session/#core.spark_session.check_if_data_path_exists","title":"<code>check_if_data_path_exists(spark, data_path)</code>","text":"<p>Checks whether data path exists, returns True if it does, False if not</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>active SparkSession</p> required <code>data_path</code> <code>str</code> <p>path to check</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether the passed path exists</p> Source code in <code>multimno/core/spark_session.py</code> <pre><code>def check_if_data_path_exists(spark: SparkSession, data_path: str) -&gt; bool:\n    \"\"\"\n    Checks whether data path exists, returns True if it does, False if not\n\n    Args:\n        spark (SparkSession): active SparkSession\n        data_path (str): path to check\n\n    Returns:\n        bool: Whether the passed path exists\n    \"\"\"\n    conf = spark._jsc.hadoopConfiguration()\n    uri = spark._jvm.java.net.URI.create(data_path)\n    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(uri, conf)\n    return fs.exists(spark._jvm.org.apache.hadoop.fs.Path(data_path))\n</code></pre>"},{"location":"reference/core/spark_session/#core.spark_session.check_or_create_data_path","title":"<code>check_or_create_data_path(spark, data_path)</code>","text":"<p>Create the provided path on a file system. If path already exists, do nothing.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>active SparkSession</p> required <code>data_path</code> <code>str</code> <p>path to check</p> required Source code in <code>multimno/core/spark_session.py</code> <pre><code>def check_or_create_data_path(spark: SparkSession, data_path: str):\n    \"\"\"\n    Create the provided path on a file system. If path already exists, do nothing.\n\n    Args:\n        spark (SparkSession): active SparkSession\n        data_path (str): path to check\n    \"\"\"\n    conf = spark._jsc.hadoopConfiguration()\n    uri = spark._jvm.java.net.URI.create(data_path)\n    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(uri, conf)\n    path = spark._jvm.org.apache.hadoop.fs.Path(data_path)\n    if not fs.exists(path):\n        fs.mkdirs(path)\n</code></pre>"},{"location":"reference/core/spark_session/#core.spark_session.delete_file_or_folder","title":"<code>delete_file_or_folder(spark, data_path)</code>","text":"<p>Deletes file or folder with given path</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Currently active spark session</p> required <code>data_path</code> <code>str</code> <p>Path to remove</p> required Source code in <code>multimno/core/spark_session.py</code> <pre><code>def delete_file_or_folder(spark: SparkSession, data_path: str):\n    \"\"\"\n    Deletes file or folder with given path\n\n    Args:\n        spark (SparkSession): Currently active spark session\n        data_path (str): Path to remove\n    \"\"\"\n    conf = spark._jsc.hadoopConfiguration()\n    uri = spark._jvm.java.net.URI.create(data_path)\n    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(uri, conf)\n    path = spark._jvm.org.apache.hadoop.fs.Path(data_path)\n    fs.delete(path, True)\n</code></pre>"},{"location":"reference/core/spark_session/#core.spark_session.generate_spark_session","title":"<code>generate_spark_session(config)</code>","text":"<p>Function that generates a Spark Sedona session.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ConfigParser</code> <p>Object with the final configuration.</p> required <p>Returns:</p> Name Type Description <code>SparkSession</code> <code>SparkSession</code> <p>Session of spark.</p> Source code in <code>multimno/core/spark_session.py</code> <pre><code>def generate_spark_session(config: ConfigParser) -&gt; SparkSession:\n    \"\"\"Function that generates a Spark Sedona session.\n\n    Args:\n        config (ConfigParser): Object with the final configuration.\n\n    Returns:\n        SparkSession: Session of spark.\n    \"\"\"\n    conf_dict = dict(config[SPARK_CONFIG_KEY])\n    master = conf_dict.pop(\"spark.master\")\n    session_name = conf_dict.pop(\"session_name\")\n\n    builder = SedonaContext.builder().appName(f\"{session_name}\").master(master)\n\n    # Configuration file spark configs\n    for k, v in conf_dict.items():\n        builder = builder.config(k, v)\n\n    ##################\n    # SEDONA\n    ##################\n\n    # Set sedona session\n    spark = SedonaContext.create(builder.getOrCreate())\n    sc = spark.sparkContext\n    sc.setSystemProperty(\"sedona.global.charset\", \"utf8\")\n\n    # Set log\n    sc.setLogLevel(\"ERROR\")\n    log4j = sc._jvm.org.apache.log4j\n    log4j.LogManager.getRootLogger().setLevel(log4j.Level.ERROR)\n\n    return spark\n</code></pre>"},{"location":"reference/core/spark_session/#core.spark_session.list_all_files_helper","title":"<code>list_all_files_helper(path, fs, conf)</code>","text":"<p>This function is used by list_all_files_recursively. This should not be called elsewhere Recursively traverses the file tree from given spot saving all files to a list and returns it.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>py4j.java_gateway.JavaObject: Object from parent function</p> required <code>fs</code> <code>JavaClass</code> <p>Object from parent function</p> required <code>conf</code> <code>JavaObject</code> <p>Object from parent function</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list[str]</code> <p>List of all files this folder and subdirectories of this folder.</p> Source code in <code>multimno/core/spark_session.py</code> <pre><code>def list_all_files_helper(\n    path: py4j.java_gateway.JavaObject, fs: py4j.java_gateway.JavaClass, conf: py4j.java_gateway.JavaObject\n) -&gt; list[str]:\n    \"\"\"\n    This function is used by list_all_files_recursively. This should not be called elsewhere\n    Recursively traverses the file tree from given spot saving all files to a list and returns it.\n\n    Args:\n        path (str): py4j.java_gateway.JavaObject: Object from parent function\n        fs (py4j.java_gateway.JavaClass): Object from parent function\n        conf (py4j.java_gateway.JavaObject): Object from parent function\n\n    Returns:\n        list: List of all files this folder and subdirectories of this folder.\n    \"\"\"\n    files_list = []\n\n    for f in fs.listStatus(path):\n        if f.isDirectory():\n            files_list.extend(list_all_files_helper(f.getPath(), fs, conf))\n        else:\n            files_list.append(str(f.getPath()))\n\n    return files_list\n</code></pre>"},{"location":"reference/core/spark_session/#core.spark_session.list_all_files_recursively","title":"<code>list_all_files_recursively(spark, data_path)</code>","text":"<p>If path is a file, returns a singleton list with this path. If path is a folder, return a list of all files in this folder and any of its subfolders</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Currently active spark session</p> required <code>data_path</code> <code>str</code> <p>Path to list the files of</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of all files in that folder and its subfolders</p> Source code in <code>multimno/core/spark_session.py</code> <pre><code>def list_all_files_recursively(spark: SparkSession, data_path: str) -&gt; list[str]:\n    \"\"\"\n    If path is a file, returns a singleton list with this path.\n    If path is a folder, return a list of all files in this folder and any of its subfolders\n\n    Args:\n        spark (SparkSession): Currently active spark session\n        data_path (str): Path to list the files of\n\n    Returns:\n        list[str]: A list of all files in that folder and its subfolders\n    \"\"\"\n    conf = spark._jsc.hadoopConfiguration()\n    uri = spark._jvm.java.net.URI.create(data_path)\n    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(uri, conf)\n    path = spark._jvm.org.apache.hadoop.fs.Path(data_path)\n    return list_all_files_helper(path, fs, conf)\n</code></pre>"},{"location":"reference/core/spark_session/#core.spark_session.list_parquet_partition_col_values","title":"<code>list_parquet_partition_col_values(spark, data_path)</code>","text":"<p>Lists all partition column values given a partition parquet folder</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Currently active spark session</p> required <code>data_path</code> <code>str</code> <p>Path of parquet</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>str, list[str]: Name of partition column, List of partition col values</p> Source code in <code>multimno/core/spark_session.py</code> <pre><code>def list_parquet_partition_col_values(spark: SparkSession, data_path: str) -&gt; list[str]:\n    \"\"\"\n    Lists all partition column values given a partition parquet folder\n\n    Args:\n        spark (SparkSession): Currently active spark session\n        data_path (str): Path of parquet\n\n    Returns:\n        str, list[str]: Name of partition column, List of partition col values\n    \"\"\"\n\n    hadoop = spark._jvm.org.apache.hadoop\n    fs = hadoop.fs.FileSystem\n    conf = hadoop.conf.Configuration()\n    path = hadoop.fs.Path(data_path)\n\n    partitions = []\n    for f in fs.get(conf).listStatus(path):\n        if f.isDirectory():\n            partitions.append(str(f.getPath().getName()))\n\n    if len(partitions) == 0:\n        return None, None\n\n    partition_col = partitions[0].split(\"=\")[0]\n\n    partitions = [p.split(\"=\")[1] for p in partitions]\n    return partition_col, sorted(partitions)\n</code></pre>"},{"location":"reference/core/utils/","title":"utils","text":"<p>This module contains utility functions for the multimno package.</p>"},{"location":"reference/core/utils/#core.utils.apply_schema_casting","title":"<code>apply_schema_casting(sdf, schema)</code>","text":"<p>This function takes a DataFrame and a schema, and applies the schema to the DataFrame. It selects the columns in the DataFrame that are in the schema, and casts each column to the type specified in the schema.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The DataFrame to apply the schema to.</p> required <code>schema</code> <code>StructType</code> <p>The schema to apply to the DataFrame.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame that includes the same rows as the input DataFrame,</p> <code>DataFrame</code> <p>but with the columns cast to the types specified in the schema.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def apply_schema_casting(sdf: DataFrame, schema: StructType) -&gt; DataFrame:\n    \"\"\"\n    This function takes a DataFrame and a schema, and applies the schema to the DataFrame.\n    It selects the columns in the DataFrame that are in the schema, and casts each column to the type specified in the schema.\n\n    Args:\n        sdf (DataFrame): The DataFrame to apply the schema to.\n        schema (StructType): The schema to apply to the DataFrame.\n\n    Returns:\n        DataFrame: A new DataFrame that includes the same rows as the input DataFrame,\n        but with the columns cast to the types specified in the schema.\n    \"\"\"\n\n    sdf = sdf.select(*[F.col(field.name) for field in schema.fields])\n    for field in schema.fields:\n        sdf = sdf.withColumn(field.name, F.col(field.name).cast(field.dataType))\n\n    return sdf\n</code></pre>"},{"location":"reference/core/utils/#core.utils.assign_quadkey","title":"<code>assign_quadkey(sdf, crs_in, zoom_level)</code>","text":"<p>Assigns a quadkey to each row in a DataFrame based on the centroid of its geometry.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The DataFrame to assign quadkeys to. The DataFrame must contain a geometry column.</p> required <code>crs_in</code> <code>int</code> <p>The CRS of the dataframe to project to 4326 before assigning quadkeys.</p> required <code>zoom_level</code> <code>int</code> <p>The zoom level to use when assigning quadkeys.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing the same rows as the input DataFrame, but with an additional quadkey column.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def assign_quadkey(sdf: DataFrame, crs_in: int, zoom_level: int) -&gt; DataFrame:\n    \"\"\"\n    Assigns a quadkey to each row in a DataFrame based on the centroid of its geometry.\n\n    Args:\n        sdf (DataFrame): The DataFrame to assign quadkeys to. The DataFrame must contain a geometry column.\n        crs_in (int): The CRS of the dataframe to project to 4326 before assigning quadkeys.\n        zoom_level (int): The zoom level to use when assigning quadkeys.\n\n    Returns:\n        DataFrame: A DataFrame containing the same rows as the input DataFrame, but with an additional quadkey column.\n    \"\"\"\n\n    quadkey_udf = F.udf(latlon_to_quadkey, StringType())\n    sdf = sdf.withColumn(\"centroid\", STF.ST_Centroid(ColNames.geometry))\n\n    if crs_in != 4326:\n        sdf = project_to_crs(sdf, crs_in, 4326, \"centroid\")\n\n    sdf = sdf.withColumn(\n        \"quadkey\",\n        quadkey_udf(\n            STF.ST_Y(F.col(\"centroid\")),\n            STF.ST_X(F.col(\"centroid\")),\n            F.lit(zoom_level),\n        ),\n    ).drop(\"centroid\")\n\n    return sdf\n</code></pre>"},{"location":"reference/core/utils/#core.utils.calc_hashed_user_id","title":"<code>calc_hashed_user_id(df, user_column=ColNames.user_id)</code>","text":"<p>Calculates SHA2 hash of user id, takes the first 31 bits and converts them to a non-negative 32-bit integer.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Data of clean synthetic events with a user id column.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: Dataframe, where user_id column is transformered to a hashed value.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def calc_hashed_user_id(df: DataFrame, user_column: str = ColNames.user_id) -&gt; DataFrame:\n    \"\"\"\n    Calculates SHA2 hash of user id, takes the first 31 bits and converts them to a non-negative 32-bit integer.\n\n    Args:\n        df (pyspark.sql.DataFrame): Data of clean synthetic events with a user id column.\n\n    Returns:\n        pyspark.sql.DataFrame: Dataframe, where user_id column is transformered to a hashed value.\n\n    \"\"\"\n\n    df = df.withColumn(user_column, F.unhex(F.sha2(F.col(user_column).cast(\"string\"), 256)))\n    return df\n</code></pre>"},{"location":"reference/core/utils/#core.utils.cut_geodata_to_extent","title":"<code>cut_geodata_to_extent(sdf, extent, target_crs, geometry_column='geometry')</code>","text":"<p>Cuts geometries in a DataFrame to a specified extent.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The DataFrame to filter. The DataFrame must contain a geometry column.</p> required <code>extent</code> <code>tuple</code> <p>A tuple representing the extent. The tuple contains four elements: (west, south, east, north), which are the western, southern, eastern, and northern bounds of the WGS84 extent.</p> required <code>target_crs</code> <code>int</code> <p>The CRS of DataFrame to transform the extent to.</p> required <code>geometry_column</code> <code>str</code> <p>The name of the geometry column. Defaults to \"geometry\".</p> <code>'geometry'</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing the same rows as the input DataFrame, but with the geometries cut to the extent.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def cut_geodata_to_extent(\n    sdf: DataFrame,\n    extent: Tuple[float, float, float, float],\n    target_crs: int,\n    geometry_column: str = \"geometry\",\n) -&gt; DataFrame:\n    \"\"\"\n    Cuts geometries in a DataFrame to a specified extent.\n\n    Args:\n        sdf (DataFrame): The DataFrame to filter. The DataFrame must contain a geometry column.\n        extent (tuple): A tuple representing the extent. The tuple contains four elements:\n            (west, south, east, north), which are the western, southern, eastern, and northern bounds of the WGS84 extent.\n        target_crs (int): The CRS of DataFrame to transform the extent to.\n        geometry_column (str, optional): The name of the geometry column. Defaults to \"geometry\".\n\n    Returns:\n        DataFrame: A DataFrame containing the same rows as the input DataFrame, but with the geometries cut to the extent.\n    \"\"\"\n\n    extent = STC.ST_PolygonFromEnvelope(*extent)\n    if target_crs != 4326:\n        extent = STF.ST_Transform(extent, F.lit(\"EPSG:4326\"), F.lit(f\"EPSG:{target_crs}\"))\n\n    sdf = sdf.withColumn(geometry_column, STF.ST_Intersection(F.col(geometry_column), extent))\n\n    return sdf\n</code></pre>"},{"location":"reference/core/utils/#core.utils.cut_polygons_with_mask_polygons","title":"<code>cut_polygons_with_mask_polygons(input_sdf, mask_sdf, cols_to_keep, self_intersection=False, geometry_column='geometry')</code>","text":"<p>Cuts polygons in the input DataFrame with mask polygons from another DataFrame. This function takes two DataFrames: one with input polygons and another with mask polygons. It cuts the input polygons with the mask polygons, and returns a new DataFrame with the resulting polygons. Both dataframes have to have same coordinate system. Args:     input_sdf (DataFrame): A DataFrame containing the input polygons.     mask_sdf (DataFrame): A DataFrame containing the mask polygons.     cols_to_keep (list): A list of column names to keep from the input DataFrame.     geometry_column (str, optional): The name of the geometry column in the DataFrames.         Defaults to \"geometry\". Returns:     DataFrame: A DataFrame containing the resulting polygons after cutting the input polygons with the mask polygons.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def cut_polygons_with_mask_polygons(\n    input_sdf: DataFrame,\n    mask_sdf: DataFrame,\n    cols_to_keep: List[str],\n    self_intersection=False,\n    geometry_column: str = \"geometry\",\n) -&gt; DataFrame:\n    \"\"\"\n    Cuts polygons in the input DataFrame with mask polygons from another DataFrame.\n    This function takes two DataFrames: one with input polygons and another with mask polygons.\n    It cuts the input polygons with the mask polygons, and returns a new DataFrame with the resulting polygons.\n    Both dataframes have to have same coordinate system.\n    Args:\n        input_sdf (DataFrame): A DataFrame containing the input polygons.\n        mask_sdf (DataFrame): A DataFrame containing the mask polygons.\n        cols_to_keep (list): A list of column names to keep from the input DataFrame.\n        geometry_column (str, optional): The name of the geometry column in the DataFrames.\n            Defaults to \"geometry\".\n    Returns:\n        DataFrame: A DataFrame containing the resulting polygons after cutting the input polygons with the mask polygons.\n    \"\"\"\n    input_sdf = input_sdf.withColumn(\"id\", F.monotonically_increasing_id())\n    cols_to_keep = [f\"a.{col}\" for col in cols_to_keep]\n    if self_intersection:\n        input_sdf = input_sdf.withColumn(\"area\", STF.ST_Area(geometry_column))\n        intersection = input_sdf.alias(\"a\").join(\n            input_sdf.alias(\"b\"),\n            STP.ST_Intersects(\"a.geometry\", \"b.geometry\") &amp; (F.col(\"a.area\") &gt; F.col(\"b.area\")),\n        )\n        input_sdf = input_sdf.drop(\"area\")\n    else:\n        intersection = input_sdf.alias(\"a\").join(\n            mask_sdf.alias(\"b\"),\n            STP.ST_Intersects(\"a.geometry\", \"b.geometry\"),\n        )\n    intersection_cut = intersection.groupby(\"a.id\", *cols_to_keep).agg(\n        STA.ST_Union_Aggr(f\"b.{geometry_column}\").alias(\"cut_geometry\")\n    )\n    intersection_cut = fix_geometry(intersection_cut, 3, \"cut_geometry\")\n    intersection_cut = intersection_cut.withColumn(\n        geometry_column, STF.ST_Difference(f\"a.{geometry_column}\", \"cut_geometry\")\n    ).drop(\"cut_geometry\")\n\n    non_intersection = input_sdf.join(intersection_cut, [\"id\"], \"left_anti\")\n\n    return non_intersection.union(intersection_cut).drop(\"id\")\n</code></pre>"},{"location":"reference/core/utils/#core.utils.filter_geodata_to_extent","title":"<code>filter_geodata_to_extent(sdf, extent, target_crs, geometry_column='geometry')</code>","text":"<p>Filters a DataFrame to include only rows with geometries that intersect a specified extent.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The DataFrame to filter. The DataFrame must contain a geometry column.</p> required <code>extent</code> <code>tuple</code> <p>A tuple representing the extent. The tuple contains four elements: (west, south, east, north), which are the western, southern, eastern, and northern bounds of the WGS84 extent.</p> required <code>target_crs</code> <code>int</code> <p>The CRS of DataFrame to transform the extent to.</p> required <code>geometry_column</code> <code>str</code> <p>The name of the geometry column. Defaults to \"geometry\".</p> <code>'geometry'</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing only the rows from the input DataFrame where the geometry intersects the extent.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def filter_geodata_to_extent(\n    sdf: DataFrame,\n    extent: Tuple[float, float, float, float],\n    target_crs: int,\n    geometry_column: str = \"geometry\",\n) -&gt; DataFrame:\n    \"\"\"\n    Filters a DataFrame to include only rows with geometries that intersect a specified extent.\n\n    Args:\n        sdf (DataFrame): The DataFrame to filter. The DataFrame must contain a geometry column.\n        extent (tuple): A tuple representing the extent. The tuple contains four elements:\n            (west, south, east, north), which are the western, southern, eastern, and northern bounds of the WGS84 extent.\n        target_crs (int): The CRS of DataFrame to transform the extent to.\n        geometry_column (str, optional): The name of the geometry column. Defaults to \"geometry\".\n\n    Returns:\n        DataFrame: A DataFrame containing only the rows from the input DataFrame where the geometry intersects the extent.\n    \"\"\"\n\n    extent = STC.ST_PolygonFromEnvelope(*extent)\n    if target_crs != 4326:\n        extent = STF.ST_Transform(extent, F.lit(\"EPSG:4326\"), F.lit(f\"EPSG:{target_crs}\"))\n\n    sdf = sdf.filter(STP.ST_Intersects(extent, F.col(geometry_column)))\n\n    return sdf\n</code></pre>"},{"location":"reference/core/utils/#core.utils.fix_geometry","title":"<code>fix_geometry(sdf, geometry_type, geometry_column='geometry')</code>","text":"<p>Fixes the geometry of a given type in a DataFrame. This function applies several operations to the geometries in the specified geometry column of the DataFrame: 1. If a geometry is a collection of geometries, extracts only the geometries of the given type. 2. Filters out any geometries of type other than given. 3. Removes any invalid geometries. 4. Removes any empty geometries. Args:     sdf (DataFrame): The DataFrame containing the geometries to check.     geometry_column (str, optional): The name of the column containing the geometries. Defaults to \"geometry\". Returns:     DataFrame: The DataFrame with the fixed polygon geometries.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def fix_geometry(sdf: DataFrame, geometry_type: int, geometry_column: str = \"geometry\") -&gt; DataFrame:\n    \"\"\"\n    Fixes the geometry of a given type in a DataFrame.\n    This function applies several operations to the geometries in the specified geometry column of the DataFrame:\n    1. If a geometry is a collection of geometries, extracts only the geometries of the given type.\n    2. Filters out any geometries of type other than given.\n    3. Removes any invalid geometries.\n    4. Removes any empty geometries.\n    Args:\n        sdf (DataFrame): The DataFrame containing the geometries to check.\n        geometry_column (str, optional): The name of the column containing the geometries. Defaults to \"geometry\".\n    Returns:\n        DataFrame: The DataFrame with the fixed polygon geometries.\n    \"\"\"\n    geometry_name = \"Polygon\" if geometry_type == 3 else (\"Line\" if geometry_type == 2 else \"Point\")\n    if geometry_type == 3:\n        sdf = sdf.withColumn(geometry_column, STF.ST_ReducePrecision(F.col(geometry_column), F.lit(4)))\n    sdf = (\n        sdf.withColumn(\n            geometry_column,\n            F.when(\n                STF.ST_IsCollection(F.col(geometry_column)),\n                STF.ST_CollectionExtract(geometry_column, F.lit(geometry_type)),\n            ).otherwise(F.col(geometry_column)),\n        )\n        .filter(~STF.ST_IsEmpty(F.col(geometry_column)))\n        .filter(STF.ST_GeometryType(F.col(geometry_column)).like(f\"%{geometry_name}%\"))\n        .filter(STF.ST_IsValid(geometry_column))\n    )\n    return sdf\n</code></pre>"},{"location":"reference/core/utils/#core.utils.get_epsg_from_geometry_column","title":"<code>get_epsg_from_geometry_column(df)</code>","text":"<p>Get the EPSG code from the geometry column of a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with a geometry column.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the DataFrame contains multiple EPSG codes.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>EPSG code of the geometry column.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def get_epsg_from_geometry_column(df: DataFrame) -&gt; int:\n    \"\"\"\n    Get the EPSG code from the geometry column of a DataFrame.\n\n    Args:\n        df (DataFrame): DataFrame with a geometry column.\n\n    Raises:\n        ValueError: If the DataFrame contains multiple EPSG codes.\n\n    Returns:\n        int: EPSG code of the geometry column.\n    \"\"\"\n    # Get the EPSG code from the geometry column\n    temp = df.select(STF.ST_SRID(\"geometry\")).distinct().persist()\n    if temp.count() &gt; 1:\n        raise ValueError(\"Dataframe contains multiple EPSG codes\")\n\n    epsg = temp.collect()[0][0]\n    return epsg\n</code></pre>"},{"location":"reference/core/utils/#core.utils.get_quadkeys_for_bbox","title":"<code>get_quadkeys_for_bbox(extent, level_of_detail)</code>","text":"<p>Generates a list of quadkeys for a bounding box at a specific zoom level.</p> <p>This function takes a bounding box defined by its lon min, lat min, lon max, and lat max extents, and a zoom level, and generates a list of quadkeys that cover the bounding box at the specified zoom level. The quadkeys are strings of digits that represent specific tiles in a quadtree-based spatial index.</p> <p>Parameters:</p> Name Type Description Default <code>extent</code> <code>tuple</code> <p>A tuple representing the bounding box. The tuple contains four elements: (west, south, east, north), which are the western, southern, eastern, and northern extents of the bounding box, respectively. Each extent is a float representing a geographic coordinate in degrees.</p> required <code>level_of_detail</code> <code>int</code> <p>The zoom level.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>List[str]</code> <p>A list of quadkeys that cover the bounding box at the specified zoom level. Each quadkey is a string.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def get_quadkeys_for_bbox(extent: Tuple[float, float, float, float], level_of_detail: int) -&gt; List[str]:\n    \"\"\"\n    Generates a list of quadkeys for a bounding box at a specific zoom level.\n\n    This function takes a bounding box defined by its lon min, lat min, lon max, and lat max extents,\n    and a zoom level, and generates a list of quadkeys that cover the bounding box at the specified zoom level.\n    The quadkeys are strings of digits that represent specific tiles in a quadtree-based spatial index.\n\n    Args:\n        extent (tuple): A tuple representing the bounding box. The tuple contains four elements:\n            (west, south, east, north), which are the western, southern, eastern, and northern extents\n            of the bounding box, respectively. Each extent is a float representing a geographic coordinate in degrees.\n        level_of_detail (int): The zoom level.\n\n    Returns:\n        list: A list of quadkeys that cover the bounding box at the specified zoom level. Each quadkey is a string.\n    \"\"\"\n    west, south, east, north = extent\n    min_tile_x, min_tile_y = latlon_to_tilexy(north, west, level_of_detail)\n    max_tile_x, max_tile_y = latlon_to_tilexy(south, east, level_of_detail)\n    quadkeys = []\n    for x in range(min_tile_x, max_tile_x + 1):\n        for y in range(min_tile_y, max_tile_y + 1):\n            quadkeys.append(tilexy_to_quadkey(x, y, level_of_detail))\n    return quadkeys\n</code></pre>"},{"location":"reference/core/utils/#core.utils.latlon_to_quadkey","title":"<code>latlon_to_quadkey(latitude, longitude, level_of_detail)</code>","text":"<p>Converts a geographic coordinate to a quadkey at a specific zoom level.</p> <p>This function takes a latitude and longitude in degrees, and a zoom level, and converts them to a quadkey. The quadkey is a string of digits that represents a specific tile in a quadtree-based spatial index. The conversion process involves first converting the geographic coordinate to tile coordinates, and then converting the tile coordinates to a quadkey.</p> <p>Parameters:</p> Name Type Description Default <code>latitude</code> <code>float</code> <p>The latitude of the geographic coordinate, in degrees.</p> required <code>longitude</code> <code>float</code> <p>The longitude of the geographic coordinate, in degrees.</p> required <code>level_of_detail</code> <code>int</code> <p>The zoom level.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The quadkey representing the geographic coordinate at the specified zoom level.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def latlon_to_quadkey(latitude: float, longitude: float, level_of_detail: int) -&gt; str:\n    \"\"\"\n    Converts a geographic coordinate to a quadkey at a specific zoom level.\n\n    This function takes a latitude and longitude in degrees, and a zoom level, and converts them to a quadkey.\n    The quadkey is a string of digits that represents a specific tile in a quadtree-based spatial index.\n    The conversion process involves first converting the geographic coordinate to tile coordinates,\n    and then converting the tile coordinates to a quadkey.\n\n    Args:\n        latitude (float): The latitude of the geographic coordinate, in degrees.\n        longitude (float): The longitude of the geographic coordinate, in degrees.\n        level_of_detail (int): The zoom level.\n\n    Returns:\n        str: The quadkey representing the geographic coordinate at the specified zoom level.\n    \"\"\"\n    x, y = latlon_to_tilexy(latitude, longitude, level_of_detail)\n    return tilexy_to_quadkey(x, y, level_of_detail)\n</code></pre>"},{"location":"reference/core/utils/#core.utils.latlon_to_tilexy","title":"<code>latlon_to_tilexy(latitude, longitude, level_of_detail)</code>","text":"<p>Converts a geographic coordinate to tile coordinates at a specific zoom level.</p> <p>This function takes a latitude and longitude in degrees, and a zoom level, and converts them to tile coordinates (tile_x, tile_y) at the specified zoom level. The tile coordinates are in the tile system used by Bing Maps, OpenStreetMap, MapBox and other map providers.</p> <p>Parameters:</p> Name Type Description Default <code>latitude</code> <code>float</code> <p>The latitude of the geographic coordinate, in degrees.</p> required <code>longitude</code> <code>float</code> <p>The longitude of the geographic coordinate, in degrees.</p> required <code>level_of_detail</code> <code>int</code> <p>The zoom level.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>int</code> <p>A tuple representing the tile coordinates of the geographic coordinate at the specified</p> <code>int</code> <p>zoom level. The tuple contains two elements: (tile_x, tile_y).</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def latlon_to_tilexy(latitude: float, longitude: float, level_of_detail: int) -&gt; Tuple[int, int]:\n    \"\"\"\n    Converts a geographic coordinate to tile coordinates at a specific zoom level.\n\n    This function takes a latitude and longitude in degrees, and a zoom level, and converts them to\n    tile coordinates (tile_x, tile_y) at the specified zoom level. The tile coordinates are in the\n    tile system used by Bing Maps, OpenStreetMap, MapBox and other map providers.\n\n    Args:\n        latitude (float): The latitude of the geographic coordinate, in degrees.\n        longitude (float): The longitude of the geographic coordinate, in degrees.\n        level_of_detail (int): The zoom level.\n\n    Returns:\n        tuple: A tuple representing the tile coordinates of the geographic coordinate at the specified\n        zoom level. The tuple contains two elements: (tile_x, tile_y).\n    \"\"\"\n    if not -90 &lt;= latitude &lt;= 90:\n        raise ValueError(f\"Latitude must be in the range [-90, 90], got {latitude}\")\n    if not -180 &lt;= longitude &lt;= 180:\n        raise ValueError(f\"Longitude must be in the range [-180, 180], got {longitude}\")\n    latitude = math.radians(latitude)\n    longitude = math.radians(longitude)\n\n    sinLatitude = math.sin(latitude)\n    pixelX = ((longitude + math.pi) / (2 * math.pi)) * 256 * 2**level_of_detail\n    pixelY = (0.5 - math.log((1 + sinLatitude) / (1 - sinLatitude)) / (4 * math.pi)) * 256 * 2**level_of_detail\n    tileX = int(math.floor(pixelX / 256))\n    tileY = int(math.floor(pixelY / 256))\n    return tileX, tileY\n</code></pre>"},{"location":"reference/core/utils/#core.utils.project_to_crs","title":"<code>project_to_crs(sdf, crs_in, crs_out, geometry_column='geometry')</code>","text":"<p>Projects geometry to CRS.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>crs_in</code> <code>int</code> <p>Input CRS.</p> required <code>crs_out</code> <code>int</code> <p>Output CRS.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with geometry projected to cartesian CRS.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def project_to_crs(sdf: DataFrame, crs_in: int, crs_out: int, geometry_column=\"geometry\") -&gt; DataFrame:\n    \"\"\"\n    Projects geometry to CRS.\n\n    Args:\n        sdf (DataFrame): Input DataFrame.\n        crs_in (int): Input CRS.\n        crs_out (int): Output CRS.\n\n    Returns:\n        DataFrame: DataFrame with geometry projected to cartesian CRS.\n    \"\"\"\n    crs_in = f\"EPSG:{crs_in}\"\n    crs_out = f\"EPSG:{crs_out}\"\n\n    sdf = sdf.withColumn(\n        geometry_column,\n        STF.ST_Transform(sdf[geometry_column], F.lit(crs_in), F.lit(crs_out)),\n    )\n    return sdf\n</code></pre>"},{"location":"reference/core/utils/#core.utils.quadkey_to_extent","title":"<code>quadkey_to_extent(quadkey)</code>","text":"<p>Converts a quadkey to a geographic extent (bounding box).</p> <p>This function takes a quadkey and converts it to a geographic extent represented as a tuple of (longitude_min, latitude_min, longitude_max, latitude_max).</p> <p>Parameters:</p> Name Type Description Default <code>quadkey</code> <code>str</code> <p>The quadkey to convert. A quadkey is a string of digits that represents a</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple representing the geographic extent of the quadkey. The tuple contains four</p> <code>elements</code> <p>(longitude_min, latitude_min, longitude_max, latitude_max).</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def quadkey_to_extent(quadkey: str):\n    \"\"\"\n    Converts a quadkey to a geographic extent (bounding box).\n\n    This function takes a quadkey and converts it to a geographic extent represented as a tuple of\n    (longitude_min, latitude_min, longitude_max, latitude_max).\n\n    Args:\n        quadkey (str): The quadkey to convert. A quadkey is a string of digits that represents a\n        specific tile in a quadtree-based spatial index.\n\n    Returns:\n        tuple: A tuple representing the geographic extent of the quadkey. The tuple contains four\n        elements: (longitude_min, latitude_min, longitude_max, latitude_max).\n    \"\"\"\n    tile_x, tile_y, zoom_level = quadkey_to_tile(quadkey)\n    n = 2.0**zoom_level\n    lon_min = tile_x / n * 360.0 - 180.0\n    lat_min = math.degrees(math.atan(math.sinh(math.pi * (1 - 2 * (tile_y + 1) / n))))\n    lon_max = (tile_x + 1) / n * 360.0 - 180.0\n    lat_max = math.degrees(math.atan(math.sinh(math.pi * (1 - 2 * tile_y / n))))\n\n    return (lon_min, lat_min, lon_max, lat_max)\n</code></pre>"},{"location":"reference/core/utils/#core.utils.quadkey_to_tile","title":"<code>quadkey_to_tile(quadkey)</code>","text":"<p>Converts a quadkey to tile coordinates and zoom level.</p> <p>This function takes a quadkey and converts it to tile coordinates (tile_x, tile_y) and zoom level. A quadkey is a string of digits that represents a specific tile in a quadtree-based spatial index.</p> <p>Parameters:</p> Name Type Description Default <code>quadkey</code> <code>str</code> <p>The quadkey to convert.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>int</code> <p>A tuple representing the tile coordinates and zoom level of the quadkey. The tuple contains three</p> <code>elements</code> <code>int</code> <p>(tile_x, tile_y, zoom_level).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the quadkey contains an invalid character.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def quadkey_to_tile(quadkey: str) -&gt; Tuple[int, int, int]:\n    \"\"\"\n    Converts a quadkey to tile coordinates and zoom level.\n\n    This function takes a quadkey and converts it to tile coordinates (tile_x, tile_y) and zoom level.\n    A quadkey is a string of digits that represents a specific tile in a quadtree-based spatial index.\n\n    Args:\n        quadkey (str): The quadkey to convert.\n\n    Returns:\n        tuple: A tuple representing the tile coordinates and zoom level of the quadkey. The tuple contains three\n        elements: (tile_x, tile_y, zoom_level).\n\n    Raises:\n        ValueError: If the quadkey contains an invalid character.\n    \"\"\"\n    tile_x = tile_y = 0\n    zoom_level = len(quadkey)\n    for i in range(zoom_level):\n        bit = zoom_level - i - 1\n        mask = 1 &lt;&lt; bit\n        if quadkey[i] == \"0\":\n            pass\n        elif quadkey[i] == \"1\":\n            tile_x |= mask\n        elif quadkey[i] == \"2\":\n            tile_y |= mask\n        elif quadkey[i] == \"3\":\n            tile_x |= mask\n            tile_y |= mask\n        else:\n            raise ValueError(\"Invalid quadkey character.\")\n    return tile_x, tile_y, zoom_level\n</code></pre>"},{"location":"reference/core/utils/#core.utils.spark_to_geopandas","title":"<code>spark_to_geopandas(df, epsg=None)</code>","text":"<p>Convert a Spark DataFrame to a geopandas GeoDataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Spark DataFrame to convert.</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: GeoDataFrame with the same data as the input DataFrame.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def spark_to_geopandas(df: DataFrame, epsg: int = None) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Convert a Spark DataFrame to a geopandas GeoDataFrame.\n\n    Args:\n        df (DataFrame): Spark DataFrame to convert.\n\n    Returns:\n        gpd.GeoDataFrame: GeoDataFrame with the same data as the input DataFrame.\n    \"\"\"\n    # Convert the DataFrame to a GeoDataFrame\n    if epsg is None:\n        epsg = get_epsg_from_geometry_column(df)\n    gdf = gpd.GeoDataFrame(df.toPandas(), crs=f\"EPSG:{epsg}\")\n\n    return gdf\n</code></pre>"},{"location":"reference/core/utils/#core.utils.tilexy_to_quadkey","title":"<code>tilexy_to_quadkey(x, y, level_of_detail)</code>","text":"<p>Converts tile coordinates to a quadkey at a specific zoom level.</p> <p>This function takes tile coordinates (x, y) and a zoom level, and converts them to a quadkey. The quadkey is a string of digits that represents a specific tile in a quadtree-based spatial index. The conversion process involves bitwise operations on the tile coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>int</code> <p>The x-coordinate of the tile.</p> required <code>y</code> <code>int</code> <p>The y-coordinate of the tile.</p> required <code>level_of_detail</code> <code>int</code> <p>The zoom level.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The quadkey representing the tile at the specified zoom level.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def tilexy_to_quadkey(x: int, y: int, level_of_detail: int) -&gt; str:\n    \"\"\"\n    Converts tile coordinates to a quadkey at a specific zoom level.\n\n    This function takes tile coordinates (x, y) and a zoom level, and converts them to a quadkey.\n    The quadkey is a string of digits that represents a specific tile in a quadtree-based spatial index.\n    The conversion process involves bitwise operations on the tile coordinates.\n\n    Args:\n        x (int): The x-coordinate of the tile.\n        y (int): The y-coordinate of the tile.\n        level_of_detail (int): The zoom level.\n\n    Returns:\n        str: The quadkey representing the tile at the specified zoom level.\n    \"\"\"\n    quadkey = \"\"\n    for i in range(level_of_detail, 0, -1):\n        digit = 0\n        mask = 1 &lt;&lt; (i - 1)\n        if (x &amp; mask) != 0:\n            digit += 1\n        if (y &amp; mask) != 0:\n            digit += 2\n        quadkey += str(digit)\n    return quadkey\n</code></pre>"},{"location":"reference/core/constants/","title":"constants","text":""},{"location":"reference/core/constants/columns/","title":"columns","text":"<p>Reusable internal column names. Useful for referring to the the same column across multiple components.</p>"},{"location":"reference/core/constants/columns/#core.constants.columns.ColNames","title":"<code>ColNames</code>","text":"<p>Class that enumerates all the column names.</p> Source code in <code>multimno/core/constants/columns.py</code> <pre><code>class ColNames:\n    \"\"\"\n    Class that enumerates all the column names.\n    \"\"\"\n\n    user_id = \"user_id\"\n    partition_id = \"partition_id\"\n    timestamp = \"timestamp\"\n    mcc = \"mcc\"\n    mnc = \"mnc\"\n    plmn = \"plmn\"\n    cell_id = \"cell_id\"\n    latitude = \"latitude\"\n    longitude = \"longitude\"\n    error_flag = \"error_flag\"\n    domain = \"domain\"\n    # default values for domain col\n    domestic = \"domestic\"\n    inbound = \"inbound\"\n    outbound = \"outbound\"\n\n    altitude = \"altitude\"\n    antenna_height = \"antenna_height\"\n    directionality = \"directionality\"\n    azimuth_angle = \"azimuth_angle\"\n    elevation_angle = \"elevation_angle\"\n    horizontal_beam_width = \"horizontal_beam_width\"\n    vertical_beam_width = \"vertical_beam_width\"\n    power = \"power\"\n    range = \"range\"\n    frequency = \"frequency\"\n    technology = \"technology\"\n    valid_date_start = \"valid_date_start\"\n    valid_date_end = \"valid_date_end\"\n    cell_type = \"cell_type\"\n\n    loc_error = \"loc_error\"\n    event_id = \"event_id\"\n\n    year = \"year\"\n    month = \"month\"\n    day = \"day\"\n    user_id_modulo = \"user_id_modulo\"\n\n    # for QA by column\n    variable = \"variable\"\n    type_of_error = \"type_of_error\"\n    type_of_transformation = \"type_of_transformation\"\n    value = \"value\"\n    result_timestamp = \"result_timestamp\"\n    data_period_start = \"data_period_start\"\n    data_period_end = \"data_period_end\"\n    field_name = \"field_name\"\n    initial_frequency = \"initial_frequency\"\n    final_frequency = \"final_frequency\"\n    date = \"date\"\n\n    # warnings\n    # log table\n    measure_definition = \"measure_definition\"\n    lookback_period = \"lookback_period\"\n    daily_value = \"daily_value\"\n    condition_value = \"condition_value\"\n    condition = \"condition\"\n    warning_text = \"warning_text\"\n    # for plots\n    type_of_qw = \"type_of_qw\"\n    average = \"average\"\n    UCL = \"UCL\"\n    LCL = \"LCL\"\n    title = \"title\"\n\n    # top frequent errors\n    error_value = \"error_value\"\n    error_count = \"error_count\"\n    accumulated_percentage = \"accumulated_percentage\"\n\n    # for grid generation\n    geometry = \"geometry\"\n    grid_id = \"grid_id\"\n    elevation = \"elevation\"\n    land_use = \"land_use\"\n    type_code = \"type_code\"\n    prior_probability = \"prior_probability\"\n    ple_coefficient = \"environment_ple_coefficient\"\n    quadkey = \"quadkey\"\n\n    # device activity statistics\n    event_cnt = \"event_cnt\"\n    unique_cell_cnt = \"unique_cell_cnt\"\n    unique_location_cnt = \"unique_location_cnt\"\n    sum_distance_m = \"sum_distance_m\"\n    unique_hour_cnt = \"unique_hour_cnt\"\n    mean_time_gap = \"mean_time_gap\"\n    stdev_time_gap = \"stdev_time_gap\"\n\n    # signal\n    signal_strength = \"signal_strength\"\n    distance_to_cell = \"distance_to_cell\"\n    distance_to_cell_3D = \"distance_to_cell_3D\"\n    joined_geometry = \"joined_geometry\"\n    path_loss_exponent = \"path_loss_exponent\"\n    azimuth_signal_strength_back_loss = \"azimuth_signal_strength_back_loss\"\n    elevation_signal_strength_back_loss = \"elevation_signal_strength_back_loss\"\n\n    # for cell footprint\n    signal_dominance = \"signal_dominance\"\n    group_id = \"group_id\"\n    cells = \"cells\"\n    group_size = \"group_size\"\n\n    # time segments\n    time_segment_id = \"time_segment_id\"\n    start_timestamp = \"start_timestamp\"\n    end_timestamp = \"end_timestamp\"\n    state = \"state\"\n    is_last = \"is_last\"\n\n    # for cell connection probability\n    cell_connection_probability = \"cell_connection_probability\"\n    posterior_probability = \"posterior_probability\"\n\n    # dps (daily permanence score)\n    dps = \"dps\"\n    time_slot_initial_time = \"time_slot_initial_time\"\n    time_slot_end_time = \"time_slot_end_time\"\n    id_type = \"id_type\"\n    # time_slot_duration = \"time_slot_duration\"\n\n    # midterm permanence score\n    mps = \"mps\"\n    day_type = \"day_type\"\n    time_interval = \"time_interval\"\n    regularity_mean = \"regularity_mean\"\n    regularity_std = \"regularity_std\"\n\n    # longterm permanence score\n    lps = \"lps\"\n    total_frequency = \"total_frequency\"\n    frequency_mean = \"frequency_mean\"\n    frequency_std = \"frequency_std\"\n    start_date = \"start_date\"\n    end_date = \"end_date\"\n    season = \"season\"\n\n    # diaries\n    stay_type = \"stay_type\"\n    activity_type = \"activity_type\"\n    initial_timestamp = \"initial_timestamp\"\n    final_timestamp = \"final_timestamp\"\n\n    # present population\n    device_count = \"device_count\"\n    population = \"population\"\n\n    # zone to grid mapping\n    zone_id = \"zone_id\"\n    hierarchical_id = \"hierarchical_id\"\n    dataset_id = \"dataset_id\"\n\n    # for spatial data\n    category = \"category\"\n    zone_id = \"zone_id\"\n    level = \"level\"\n    parent_id = \"parent_id\"\n    iso2 = \"iso2\"\n    iso3 = \"iso3\"\n    name = \"name\"\n    dataset_id = \"dataset_id\"\n    hierarchical_id = \"hierarchical_id\"\n\n    # for usual environment labels\n    label = \"label\"\n    ue_label_rule = \"ue_label_rule\"\n    location_label_rule = \"location_label_rule\"\n\n    # for usual environment labeling quality metrics\n    labeling_quality_metric = \"metric\"\n    labeling_quality_count = \"count\"\n\n    # for usual environment aggregation\n    weighted_device_count = \"weighted_device_count\"\n    tile_weight = \"tile_weight\"\n    device_tile_weight = \"device_tile_weight\"\n</code></pre>"},{"location":"reference/core/constants/conditions/","title":"conditions","text":""},{"location":"reference/core/constants/error_types/","title":"error_types","text":"<p>Transformations Error types module.</p>"},{"location":"reference/core/constants/error_types/#core.constants.error_types.ErrorTypes","title":"<code>ErrorTypes</code>","text":"<p>Class that enumerates the multiple error types of data transformations.</p> Source code in <code>multimno/core/constants/error_types.py</code> <pre><code>class ErrorTypes:\n    \"\"\"\n    Class that enumerates the multiple error types of data transformations.\n    \"\"\"\n\n    missing_value = 1\n    not_right_syntactic_format = 2\n    out_of_admissible_values = 3\n    inconsistency_between_variables = 4\n    no_location = 5\n    out_of_bounding_box = 6\n    no_domain = 7\n    no_error = 9\n    same_location_duplicate = 10\n\n    # This shows the possible error types that can happen in syntactic event cleaning\n    # This is used for creating the quality metrics data object\n    event_syntactic_cleaning_possible_errors = [1, 2, 3, 4, 5, 6, 9, 10]\n</code></pre>"},{"location":"reference/core/constants/error_types/#core.constants.error_types.NetworkErrorType","title":"<code>NetworkErrorType</code>","text":"<p>Class that enumerates the multiple error types present in network topology data.</p> Source code in <code>multimno/core/constants/error_types.py</code> <pre><code>class NetworkErrorType:\n    \"\"\"\n    Class that enumerates the multiple error types present in network topology data.\n    \"\"\"\n\n    NO_ERROR = 0\n    NULL_VALUE = 1\n    OUT_OF_RANGE = 2\n    UNSUPPORTED_TYPE = 3\n    CANNOT_PARSE = 4\n    INITIAL_ROWS = 100\n    FINAL_ROWS = 101\n</code></pre>"},{"location":"reference/core/constants/error_types/#core.constants.error_types.SemanticErrorType","title":"<code>SemanticErrorType</code>","text":"<p>Class that enumerates the multiple error types associated to event semantic checks.</p> Source code in <code>multimno/core/constants/error_types.py</code> <pre><code>class SemanticErrorType:\n    \"\"\"\n    Class that enumerates the multiple error types associated to event semantic checks.\n    \"\"\"\n\n    NO_ERROR = 0\n    CELL_ID_NON_EXISTENT = 1\n    CELL_ID_NOT_VALID = 2\n    INCORRECT_EVENT_LOCATION = 3\n    SUSPICIOUS_EVENT_LOCATION = 4\n    DIFFERENT_LOCATION_DUPLICATE = 5\n</code></pre>"},{"location":"reference/core/constants/measure_definitions/","title":"measure_definitions","text":""},{"location":"reference/core/constants/network_default_thresholds/","title":"network_default_thresholds","text":"<p>Contains the default threshold values used by the Network Syntactic Quality Warnings</p>"},{"location":"reference/core/constants/period_names/","title":"period_names","text":"<p>List of names of the sub-daily periods/time intervals, sub-monthly periods/day types, and sub-yearly/seasons used in the Permanence Score components.</p>"},{"location":"reference/core/constants/semantic_qw_default_thresholds/","title":"semantic_qw_default_thresholds","text":"<p>Contains the default threshold values used by the Event Device Semantic Quality Warnings</p>"},{"location":"reference/core/constants/transformations/","title":"transformations","text":"<p>Data transformations types modukle</p>"},{"location":"reference/core/constants/transformations/#core.constants.transformations.Transformations","title":"<code>Transformations</code>","text":"<p>Class that enumerates the multiple data transformations types.</p> Source code in <code>multimno/core/constants/transformations.py</code> <pre><code>class Transformations:\n    \"\"\"\n    Class that enumerates the multiple data transformations types.\n    \"\"\"\n\n    converted_timestamp = 1\n    other_conversion = 2\n    no_transformation = 9\n\n    event_syntactic_cleaning_possible_transformations = [1, 2, 9]\n</code></pre>"},{"location":"reference/core/constants/warnings/","title":"warnings","text":""},{"location":"reference/core/data_objects/","title":"data_objects","text":""},{"location":"reference/core/data_objects/data_object/","title":"data_object","text":"<p>Module that defines the data object abstract classes</p>"},{"location":"reference/core/data_objects/data_object/#core.data_objects.data_object.DataObject","title":"<code>DataObject</code>","text":"<p>Abstract class that models a DataObject. It defines its data schema including the attributes that compose it.</p> Source code in <code>multimno/core/data_objects/data_object.py</code> <pre><code>class DataObject(metaclass=ABCMeta):\n    \"\"\"\n    Abstract class that models a DataObject. It defines its data schema including the attributes that compose it.\n    \"\"\"\n\n    ID: str = None\n    SCHEMA: StructType = None\n\n    def __init__(self, spark: SparkSession) -&gt; None:\n        self.df: DataFrame = None\n        self.spark: SparkSession = spark\n        self.interface: IOInterface = None\n\n    def read(self, *args, **kwargs):\n        \"\"\"\n        Method that performs the read operation of the data object dataframe through an IOInterface.\n        \"\"\"\n        self.df = self.interface.read_from_interface(*args, **kwargs)\n\n    def write(self, *args, **kwargs):\n        \"\"\"\n        Method that performs the write operation of the data object dataframe through an IOInterface.\n        \"\"\"\n        self.interface.write_from_interface(self.df, *args, **kwargs)\n</code></pre>"},{"location":"reference/core/data_objects/data_object/#core.data_objects.data_object.DataObject.read","title":"<code>read(*args, **kwargs)</code>","text":"<p>Method that performs the read operation of the data object dataframe through an IOInterface.</p> Source code in <code>multimno/core/data_objects/data_object.py</code> <pre><code>def read(self, *args, **kwargs):\n    \"\"\"\n    Method that performs the read operation of the data object dataframe through an IOInterface.\n    \"\"\"\n    self.df = self.interface.read_from_interface(*args, **kwargs)\n</code></pre>"},{"location":"reference/core/data_objects/data_object/#core.data_objects.data_object.DataObject.write","title":"<code>write(*args, **kwargs)</code>","text":"<p>Method that performs the write operation of the data object dataframe through an IOInterface.</p> Source code in <code>multimno/core/data_objects/data_object.py</code> <pre><code>def write(self, *args, **kwargs):\n    \"\"\"\n    Method that performs the write operation of the data object dataframe through an IOInterface.\n    \"\"\"\n    self.interface.write_from_interface(self.df, *args, **kwargs)\n</code></pre>"},{"location":"reference/core/data_objects/data_object/#core.data_objects.data_object.PathDataObject","title":"<code>PathDataObject</code>","text":"<p>               Bases: <code>DataObject</code></p> <p>Abstract Class that models DataObjects that will use a PathInterface for IO operations. It inherits the DataObject abstract class.</p> Source code in <code>multimno/core/data_objects/data_object.py</code> <pre><code>class PathDataObject(DataObject, metaclass=ABCMeta):\n    \"\"\"Abstract Class that models DataObjects that will use a PathInterface for IO operations.\n    It inherits the DataObject abstract class.\n    \"\"\"\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark)\n        self.interface: PathInterface = None\n        self.default_path: str = default_path\n\n    def read(self, *args, path: str = None, **kwargs):\n        if path is None:\n            path = self.default_path\n        self.df = self.interface.read_from_interface(self.spark, path, self.SCHEMA)\n\n    def write(self, *args, path: str = None, partition_columns: list[str] = None, **kwargs):\n        if path is None:\n            path = self.default_path\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/bronze/","title":"bronze","text":""},{"location":"reference/core/data_objects/bronze/bronze_admin_units_data_object/","title":"bronze_admin_units_data_object","text":""},{"location":"reference/core/data_objects/bronze/bronze_admin_units_data_object/#core.data_objects.bronze.bronze_admin_units_data_object.BronzeAdminUnitsDataObject","title":"<code>BronzeAdminUnitsDataObject</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models country polygons spatial data.</p> Source code in <code>multimno/core/data_objects/bronze/bronze_admin_units_data_object.py</code> <pre><code>class BronzeAdminUnitsDataObject(PathDataObject):\n    \"\"\"\n    Class that models country polygons spatial data.\n    \"\"\"\n\n    ID = \"AdminUnitsDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.zone_id, StringType(), nullable=False),\n            StructField(ColNames.name, StringType(), nullable=True),\n            StructField(ColNames.level, ShortType(), nullable=True),\n            StructField(ColNames.parent_id, StringType(), nullable=True),\n            StructField(ColNames.iso2, StringType(), nullable=False),\n            StructField(ColNames.geometry, GeometryType(), nullable=False),\n            # partition columns\n            StructField(ColNames.dataset_id, StringType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    def __init__(\n        self, spark: SparkSession, default_path: str, partition_columns: \"list[str]\" = None, default_crs: int = 3035\n    ) -&gt; None:\n\n        super().__init__(spark, default_path)\n        self.interface: GeoParquetInterface = GeoParquetInterface()\n        self.partition_columns = partition_columns\n\n        self.default_crs = default_crs\n\n    def read(self):\n\n        self.df = self.interface.read_from_interface(self.spark, self.default_path, self.SCHEMA)\n        self.df = self.df.withColumn(ColNames.geometry, STF.ST_SetSRID((ColNames.geometry), F.lit(self.default_crs)))\n\n    def write(self, path: str = None, partition_columns: \"list[str]\" = None):\n\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/bronze/bronze_countries_data_object/","title":"bronze_countries_data_object","text":""},{"location":"reference/core/data_objects/bronze/bronze_countries_data_object/#core.data_objects.bronze.bronze_countries_data_object.BronzeCountriesDataObject","title":"<code>BronzeCountriesDataObject</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models country polygons spatial data.</p> Source code in <code>multimno/core/data_objects/bronze/bronze_countries_data_object.py</code> <pre><code>class BronzeCountriesDataObject(PathDataObject):\n    \"\"\"\n    Class that models country polygons spatial data.\n    \"\"\"\n\n    ID = \"BronzeCountriesDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.iso2, StringType(), nullable=False),\n            StructField(ColNames.name, StringType(), nullable=False),\n            StructField(ColNames.geometry, GeometryType(), nullable=False),\n        ]\n    )\n\n    def __init__(\n        self, spark: SparkSession, default_path: str, partition_columns: \"list[str]\" = None, default_crs: int = 3035\n    ) -&gt; None:\n\n        super().__init__(spark, default_path)\n        self.interface: GeoParquetInterface = GeoParquetInterface()\n        self.partition_columns = partition_columns\n\n        self.default_crs = default_crs\n\n    def read(self):\n\n        self.df = self.interface.read_from_interface(self.spark, self.default_path, self.SCHEMA)\n        self.df = self.df.withColumn(ColNames.geometry, STF.ST_SetSRID((ColNames.geometry), F.lit(self.default_crs)))\n\n    def write(self, path: str = None, partition_columns: \"list[str]\" = None):\n\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/bronze/bronze_event_data_object/","title":"bronze_event_data_object","text":"<p>Bronze MNO Event data module</p>"},{"location":"reference/core/data_objects/bronze/bronze_event_data_object/#core.data_objects.bronze.bronze_event_data_object.BronzeEventDataObject","title":"<code>BronzeEventDataObject</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models the RAW MNO Event data.</p> Source code in <code>multimno/core/data_objects/bronze/bronze_event_data_object.py</code> <pre><code>class BronzeEventDataObject(PathDataObject):\n    \"\"\"\n    Class that models the RAW MNO Event data.\n    \"\"\"\n\n    ID = \"BronzeEventDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=True),\n            StructField(ColNames.timestamp, StringType(), nullable=True),\n            StructField(ColNames.mcc, IntegerType(), nullable=True),\n            StructField(ColNames.mnc, StringType(), nullable=True),\n            StructField(ColNames.plmn, IntegerType(), nullable=True),\n            StructField(ColNames.cell_id, StringType(), nullable=True),\n            StructField(ColNames.latitude, FloatType(), nullable=True),\n            StructField(ColNames.longitude, FloatType(), nullable=True),\n            StructField(ColNames.loc_error, FloatType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=True),\n            StructField(ColNames.month, ByteType(), nullable=True),\n            StructField(ColNames.day, ByteType(), nullable=True),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/bronze/bronze_geographic_zones_data_object/","title":"bronze_geographic_zones_data_object","text":""},{"location":"reference/core/data_objects/bronze/bronze_geographic_zones_data_object/#core.data_objects.bronze.bronze_geographic_zones_data_object.BronzeGeographicZonesDataObject","title":"<code>BronzeGeographicZonesDataObject</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models country polygons spatial data.</p> Source code in <code>multimno/core/data_objects/bronze/bronze_geographic_zones_data_object.py</code> <pre><code>class BronzeGeographicZonesDataObject(PathDataObject):\n    \"\"\"\n    Class that models country polygons spatial data.\n    \"\"\"\n\n    ID = \"GeographicZonesDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.zone_id, StringType(), nullable=False),\n            StructField(ColNames.name, StringType(), nullable=True),\n            StructField(ColNames.level, ShortType(), nullable=True),\n            StructField(ColNames.parent_id, StringType(), nullable=True),\n            StructField(ColNames.iso2, StringType(), nullable=False),\n            StructField(ColNames.geometry, GeometryType(), nullable=False),\n            # partition columns\n            StructField(ColNames.dataset_id, StringType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    def __init__(\n        self, spark: SparkSession, default_path: str, partition_columns: \"list[str]\" = None, default_crs: int = 3035\n    ) -&gt; None:\n\n        super().__init__(spark, default_path)\n        self.interface: GeoParquetInterface = GeoParquetInterface()\n        self.partition_columns = partition_columns\n\n        self.default_crs = default_crs\n\n    def read(self):\n\n        self.df = self.interface.read_from_interface(self.spark, self.default_path, self.SCHEMA)\n        self.df = self.df.withColumn(ColNames.geometry, STF.ST_SetSRID((ColNames.geometry), F.lit(self.default_crs)))\n\n    def write(self, path: str = None, partition_columns: \"list[str]\" = None):\n\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/bronze/bronze_holiday_calendar_data_object/","title":"bronze_holiday_calendar_data_object","text":"<p>Bronze Calendar Information Data Object Contains the national holidays of each country</p>"},{"location":"reference/core/data_objects/bronze/bronze_holiday_calendar_data_object/#core.data_objects.bronze.bronze_holiday_calendar_data_object.BronzeHolidayCalendarDataObject","title":"<code>BronzeHolidayCalendarDataObject</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models the Calendar information regarding national holidays and regular days.</p> Source code in <code>multimno/core/data_objects/bronze/bronze_holiday_calendar_data_object.py</code> <pre><code>class BronzeHolidayCalendarDataObject(PathDataObject):\n    \"\"\"\n    Class that models the Calendar information regarding national holidays\n    and regular days.\n    \"\"\"\n\n    ID = \"BronzeHolidayCalendarInfoDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.iso2, StringType(), nullable=False),\n            StructField(ColNames.date, DateType(), nullable=False),\n            StructField(ColNames.name, StringType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/bronze/bronze_landuse_data_object/","title":"bronze_landuse_data_object","text":""},{"location":"reference/core/data_objects/bronze/bronze_landuse_data_object/#core.data_objects.bronze.bronze_landuse_data_object.BronzeLanduseDataObject","title":"<code>BronzeLanduseDataObject</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models landuse spatial data.</p> Source code in <code>multimno/core/data_objects/bronze/bronze_landuse_data_object.py</code> <pre><code>class BronzeLanduseDataObject(PathDataObject):\n    \"\"\"\n    Class that models landuse spatial data.\n    \"\"\"\n\n    ID = \"BronzeLanduseDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.category, StringType(), nullable=False),\n            StructField(ColNames.geometry, GeometryType(), nullable=False),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.quadkey, StringType(), nullable=False),\n        ]\n    )\n\n    def __init__(\n        self, spark: SparkSession, default_path: str, partition_columns: \"list[str]\" = None, default_crs: int = 3035\n    ) -&gt; None:\n\n        super().__init__(spark, default_path)\n        self.interface: GeoParquetInterface = GeoParquetInterface()\n        self.partition_columns = partition_columns\n\n        self.default_crs = default_crs\n\n    def read(self):\n\n        self.df = self.interface.read_from_interface(self.spark, self.default_path, self.SCHEMA)\n        self.df = self.df.withColumn(ColNames.geometry, STF.ST_SetSRID((ColNames.geometry), F.lit(self.default_crs)))\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/bronze/bronze_network_physical_data_object/","title":"bronze_network_physical_data_object","text":"<p>Bronze MNO Network Topology Data module</p> <p>Currently, only considers the \"Cell Locations with Physical Properties\" type</p>"},{"location":"reference/core/data_objects/bronze/bronze_network_physical_data_object/#core.data_objects.bronze.bronze_network_physical_data_object.BronzeNetworkDataObject","title":"<code>BronzeNetworkDataObject</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models the RAW MNO Network Topology Data, based on the physical properties of the cells.</p> Source code in <code>multimno/core/data_objects/bronze/bronze_network_physical_data_object.py</code> <pre><code>class BronzeNetworkDataObject(PathDataObject):\n    \"\"\"\n    Class that models the RAW MNO Network Topology Data, based on the physical\n    properties of the cells.\n    \"\"\"\n\n    ID = \"BronzeNetworkDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.cell_id, StringType(), nullable=True),\n            StructField(ColNames.latitude, FloatType(), nullable=True),\n            StructField(ColNames.longitude, FloatType(), nullable=True),\n            StructField(ColNames.altitude, FloatType(), nullable=True),\n            StructField(ColNames.antenna_height, FloatType(), nullable=True),\n            StructField(ColNames.directionality, IntegerType(), nullable=True),\n            StructField(ColNames.azimuth_angle, FloatType(), nullable=True),\n            StructField(ColNames.elevation_angle, FloatType(), nullable=True),\n            StructField(ColNames.horizontal_beam_width, FloatType(), nullable=True),\n            StructField(ColNames.vertical_beam_width, FloatType(), nullable=True),\n            StructField(ColNames.power, FloatType(), nullable=True),\n            StructField(ColNames.range, FloatType(), nullable=True),\n            StructField(ColNames.frequency, IntegerType(), nullable=True),\n            StructField(ColNames.technology, StringType(), nullable=True),\n            StructField(ColNames.valid_date_start, StringType(), nullable=True),\n            StructField(ColNames.valid_date_end, StringType(), nullable=True),\n            StructField(ColNames.cell_type, StringType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=True),\n            StructField(ColNames.month, ByteType(), nullable=True),\n            StructField(ColNames.day, ByteType(), nullable=True),\n        ]\n    )\n    MANDATORY_COLUMNS = [ColNames.cell_id, ColNames.latitude, ColNames.longitude]\n\n    OPTIONAL_COLUMNS = [\n        ColNames.altitude,\n        ColNames.antenna_height,\n        ColNames.directionality,\n        ColNames.azimuth_angle,\n        ColNames.elevation_angle,\n        ColNames.horizontal_beam_width,\n        ColNames.vertical_beam_width,\n        ColNames.power,\n        ColNames.range,\n        ColNames.frequency,\n        ColNames.technology,\n        ColNames.valid_date_start,\n        ColNames.valid_date_end,\n        ColNames.cell_type,\n    ]\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/bronze/bronze_synthetic_diaries_data_object/","title":"bronze_synthetic_diaries_data_object","text":"<p>Bronze Synthetic Diaries Data module</p>"},{"location":"reference/core/data_objects/bronze/bronze_synthetic_diaries_data_object/#core.data_objects.bronze.bronze_synthetic_diaries_data_object.BronzeSyntheticDiariesDataObject","title":"<code>BronzeSyntheticDiariesDataObject</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models synthetically-generated agents activity-trip diaries.</p> <pre><code>    ''''''\n</code></pre> Source code in <code>multimno/core/data_objects/bronze/bronze_synthetic_diaries_data_object.py</code> <pre><code>class BronzeSyntheticDiariesDataObject(PathDataObject):\n    \"\"\"\n    Class that models synthetically-generated agents activity-trip diaries.\n\n            ''''''\n\n    \"\"\"\n\n    ID = \"BronzeSyntheticDiariesDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.activity_type, StringType(), nullable=True),\n            StructField(ColNames.stay_type, StringType(), nullable=True),\n            StructField(ColNames.longitude, FloatType(), nullable=True),\n            StructField(ColNames.latitude, FloatType(), nullable=True),\n            StructField(ColNames.initial_timestamp, TimestampType(), nullable=True),\n            StructField(ColNames.final_timestamp, TimestampType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/bronze/bronze_transportation_data_object/","title":"bronze_transportation_data_object","text":""},{"location":"reference/core/data_objects/bronze/bronze_transportation_data_object/#core.data_objects.bronze.bronze_transportation_data_object.BronzeTransportationDataObject","title":"<code>BronzeTransportationDataObject</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models the transportation network spatial data.</p> Source code in <code>multimno/core/data_objects/bronze/bronze_transportation_data_object.py</code> <pre><code>class BronzeTransportationDataObject(PathDataObject):\n    \"\"\"\n    Class that models the transportation network spatial data.\n    \"\"\"\n\n    ID = \"BronzeTransportationDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.category, StringType(), nullable=False),\n            StructField(ColNames.geometry, GeometryType(), nullable=False),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.quadkey, StringType(), nullable=False),\n        ]\n    )\n\n    def __init__(\n        self, spark: SparkSession, default_path: str, partition_columns: list[str] = None, default_crs: int = 3035\n    ) -&gt; None:\n\n        super().__init__(spark, default_path)\n        self.interface: GeoParquetInterface = GeoParquetInterface()\n        self.partition_columns = partition_columns\n\n        self.default_crs = default_crs\n\n    def read(self):\n\n        self.df = self.interface.read_from_interface(self.spark, self.default_path, self.SCHEMA)\n        self.df = self.df.withColumn(ColNames.geometry, STF.ST_SetSRID((ColNames.geometry), F.lit(self.default_crs)))\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/landing/","title":"landing","text":""},{"location":"reference/core/data_objects/landing/landing_geoparquet_data_object/","title":"landing_geoparquet_data_object","text":""},{"location":"reference/core/data_objects/landing/landing_geoparquet_data_object/#core.data_objects.landing.landing_geoparquet_data_object.LandingGeoParquetDataObject","title":"<code>LandingGeoParquetDataObject</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models input geospatial data.</p> Source code in <code>multimno/core/data_objects/landing/landing_geoparquet_data_object.py</code> <pre><code>class LandingGeoParquetDataObject(PathDataObject):\n    \"\"\"\n    Class that models input geospatial data.\n    \"\"\"\n\n    ID = \"LandingGeoParquetDO\"\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n\n        super().__init__(spark, default_path)\n        self.interface: GeoParquetInterface = GeoParquetInterface()\n        self.partition_columns = partition_columns\n\n    def read(self):\n\n        self.df = self.interface.read_from_interface(self.spark, self.default_path, self.SCHEMA)\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/landing/landing_http_geojson_data_object/","title":"landing_http_geojson_data_object","text":""},{"location":"reference/core/data_objects/landing/landing_http_geojson_data_object/#core.data_objects.landing.landing_http_geojson_data_object.LandingHttpGeoJsonDataObject","title":"<code>LandingHttpGeoJsonDataObject</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models input geospatial data in geojson format.</p> Source code in <code>multimno/core/data_objects/landing/landing_http_geojson_data_object.py</code> <pre><code>class LandingHttpGeoJsonDataObject(PathDataObject):\n    \"\"\"\n    Class that models input geospatial data in geojson format.\n    \"\"\"\n\n    ID = \"LandingGeoJsonDO\"\n\n    def __init__(self, spark: SparkSession, url: str, timeout: int, max_retries: int) -&gt; None:\n\n        super().__init__(spark, url)\n        self.interface: HttpGeoJsonInterface = HttpGeoJsonInterface()\n        self.default_path = url\n        self.timeout = timeout\n        self.max_retries = max_retries\n\n    def read(self):\n\n        self.df = self.interface.read_from_interface(self.spark, self.default_path, self.timeout, self.max_retries)\n</code></pre>"},{"location":"reference/core/data_objects/silver/","title":"silver","text":""},{"location":"reference/core/data_objects/silver/silver_aggregated_usual_environments_data_object/","title":"silver_aggregated_usual_environments_data_object","text":"<p>Silver Aggregated Usual Environments data object module</p>"},{"location":"reference/core/data_objects/silver/silver_aggregated_usual_environments_data_object/#core.data_objects.silver.silver_aggregated_usual_environments_data_object.SilverAggregatedUsualEnvironmentsDataObject","title":"<code>SilverAggregatedUsualEnvironmentsDataObject</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models the Aggregated Usual Environment data object.</p> Source code in <code>multimno/core/data_objects/silver/silver_aggregated_usual_environments_data_object.py</code> <pre><code>class SilverAggregatedUsualEnvironmentsDataObject(PathDataObject):\n    \"\"\"\n    Class that models the Aggregated Usual Environment data object.\n    \"\"\"\n\n    ID = \"SilverAggregatedUsualEnvironmentsDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.grid_id, StringType(), nullable=False),\n            StructField(ColNames.weighted_device_count, FloatType(), nullable=False),\n            # partition columns\n            StructField(ColNames.label, StringType(), nullable=False),\n            StructField(ColNames.start_date, DateType(), nullable=False),\n            StructField(ColNames.end_date, DateType(), nullable=False),\n            StructField(ColNames.season, StringType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface: ParquetInterface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_cell_connection_probabilities_data_object/","title":"silver_cell_connection_probabilities_data_object","text":"<p>Cell connection probabilities.</p>"},{"location":"reference/core/data_objects/silver/silver_cell_connection_probabilities_data_object/#core.data_objects.silver.silver_cell_connection_probabilities_data_object.SilverCellConnectionProbabilitiesDataObject","title":"<code>SilverCellConnectionProbabilitiesDataObject</code>","text":"<p>               Bases: <code>PathDataObject</code></p> Source code in <code>multimno/core/data_objects/silver/silver_cell_connection_probabilities_data_object.py</code> <pre><code>class SilverCellConnectionProbabilitiesDataObject(PathDataObject):\n    \"\"\" \"\"\"\n\n    ID = \"SilverCellConnectionProbabilitiesDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.cell_id, StringType(), nullable=True),\n            StructField(ColNames.grid_id, StringType(), nullable=True),\n            StructField(ColNames.valid_date_start, DateType(), nullable=True),\n            StructField(ColNames.valid_date_end, DateType(), nullable=True),\n            StructField(ColNames.cell_connection_probability, FloatType(), nullable=True),\n            StructField(ColNames.posterior_probability, FloatType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=True),\n            StructField(ColNames.month, ByteType(), nullable=True),\n            StructField(ColNames.day, ByteType(), nullable=True),\n        ]\n    )\n\n    MANDATORY_COLUMNS = [\n        ColNames.cell_id,\n        ColNames.grid_id,\n        ColNames.valid_date_start,\n        ColNames.valid_date_end,\n        ColNames.cell_connection_probability,\n        ColNames.posterior_probability,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    ]\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_cell_footprint_data_object/","title":"silver_cell_footprint_data_object","text":""},{"location":"reference/core/data_objects/silver/silver_cell_footprint_data_object/#core.data_objects.silver.silver_cell_footprint_data_object.SilverCellFootprintDataObject","title":"<code>SilverCellFootprintDataObject</code>","text":"<p>               Bases: <code>PathDataObject</code></p> Source code in <code>multimno/core/data_objects/silver/silver_cell_footprint_data_object.py</code> <pre><code>class SilverCellFootprintDataObject(PathDataObject):\n    \"\"\" \"\"\"\n\n    ID = \"SilverCellFootprintDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.cell_id, StringType(), nullable=True),\n            StructField(ColNames.grid_id, StringType(), nullable=True),\n            StructField(ColNames.valid_date_start, DateType(), nullable=True),\n            StructField(ColNames.valid_date_end, DateType(), nullable=True),\n            StructField(ColNames.signal_dominance, FloatType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=True),\n            StructField(ColNames.month, ByteType(), nullable=True),\n            StructField(ColNames.day, ByteType(), nullable=True),\n        ]\n    )\n\n    MANDATORY_COLUMNS = [\n        ColNames.cell_id,\n        ColNames.grid_id,\n        ColNames.valid_date_start,\n        ColNames.valid_date_end,\n        ColNames.signal_dominance,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    ]\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_cell_intersection_groups_data_object/","title":"silver_cell_intersection_groups_data_object","text":""},{"location":"reference/core/data_objects/silver/silver_cell_intersection_groups_data_object/#core.data_objects.silver.silver_cell_intersection_groups_data_object.SilverCellIntersectionGroupsDataObject","title":"<code>SilverCellIntersectionGroupsDataObject</code>","text":"<p>               Bases: <code>PathDataObject</code></p> Source code in <code>multimno/core/data_objects/silver/silver_cell_intersection_groups_data_object.py</code> <pre><code>class SilverCellIntersectionGroupsDataObject(PathDataObject):\n    \"\"\" \"\"\"\n\n    ID = \"SilverCellIntersectionGroupsDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.group_id, StringType(), nullable=True),\n            StructField(ColNames.cells, ArrayType(StringType()), nullable=True),\n            StructField(ColNames.group_size, IntegerType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=True),\n            StructField(ColNames.month, ByteType(), nullable=True),\n            StructField(ColNames.day, ByteType(), nullable=True),\n        ]\n    )\n\n    MANDATORY_COLUMNS = [\n        ColNames.group_id,\n        ColNames.cells,\n        ColNames.group_size,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    ]\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_daily_permanence_score_data_object/","title":"silver_daily_permanence_score_data_object","text":"<p>Silver Daily Permanence Score data module</p>"},{"location":"reference/core/data_objects/silver/silver_daily_permanence_score_data_object/#core.data_objects.silver.silver_daily_permanence_score_data_object.SilverDailyPermanenceScoreDataObject","title":"<code>SilverDailyPermanenceScoreDataObject</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models the Daily Permanence Score data.</p> Source code in <code>multimno/core/data_objects/silver/silver_daily_permanence_score_data_object.py</code> <pre><code>class SilverDailyPermanenceScoreDataObject(PathDataObject):\n    \"\"\"\n    Class that models the Daily Permanence Score data.\n    \"\"\"\n\n    ID = \"SilverDailyPermanenceScoreDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.grid_id, StringType(), nullable=False),\n            StructField(ColNames.time_slot_initial_time, TimestampType(), nullable=False),\n            StructField(ColNames.time_slot_end_time, TimestampType(), nullable=False),\n            StructField(ColNames.dps, ByteType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.user_id_modulo, IntegerType(), nullable=False),\n            StructField(ColNames.id_type, StringType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface: ParquetInterface = ParquetInterface()\n        self.partition_columns = [\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            ColNames.user_id_modulo,\n            ColNames.id_type,\n        ]\n\n        # Clear path\n        self.first_write = True\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        # If it is the first writing of this data object, clear the input directory, otherwise add\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_device_activity_statistics/","title":"silver_device_activity_statistics","text":"<p>Silver Device Activity Statistics module</p>"},{"location":"reference/core/data_objects/silver/silver_device_activity_statistics/#core.data_objects.silver.silver_device_activity_statistics.SilverDeviceActivityStatistics","title":"<code>SilverDeviceActivityStatistics</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models the cleaned MNO Event data.</p> Source code in <code>multimno/core/data_objects/silver/silver_device_activity_statistics.py</code> <pre><code>class SilverDeviceActivityStatistics(PathDataObject):\n    \"\"\"\n    Class that models the cleaned MNO Event data.\n    \"\"\"\n\n    ID = \"SilverDeviceActivityStatisticsDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.event_cnt, IntegerType(), nullable=False),\n            StructField(ColNames.unique_cell_cnt, ShortType(), nullable=False),\n            StructField(ColNames.unique_location_cnt, ShortType(), nullable=False),\n            StructField(ColNames.sum_distance_m, IntegerType(), nullable=True),\n            StructField(ColNames.unique_hour_cnt, ByteType(), nullable=False),\n            StructField(ColNames.mean_time_gap, IntegerType(), nullable=True),\n            StructField(ColNames.stdev_time_gap, FloatType(), nullable=True),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface: ParquetInterface = ParquetInterface()\n        self.partition_columns = [ColNames.year, ColNames.month, ColNames.day]\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        # If it is the first writing of this data object, clear the input directory, otherwise add\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n\n        self.df.write.format(\n            self.interface.FILE_FORMAT,  # File format\n        ).partitionBy(self.partition_columns).mode(\n            \"append\"\n        ).save(path)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_enriched_grid_data_object/","title":"silver_enriched_grid_data_object","text":""},{"location":"reference/core/data_objects/silver/silver_enriched_grid_data_object/#core.data_objects.silver.silver_enriched_grid_data_object.SilverEnrichedGridDataObject","title":"<code>SilverEnrichedGridDataObject</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models operational grid.</p> Source code in <code>multimno/core/data_objects/silver/silver_enriched_grid_data_object.py</code> <pre><code>class SilverEnrichedGridDataObject(PathDataObject):\n    \"\"\"\n    Class that models operational grid.\n    \"\"\"\n\n    ID = \"SilverEnrichedGridDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.geometry, GeometryType(), nullable=False),\n            StructField(ColNames.grid_id, StringType(), nullable=False),\n            StructField(ColNames.elevation, FloatType(), nullable=True),\n            StructField(ColNames.prior_probability, FloatType(), nullable=True),\n            StructField(ColNames.ple_coefficient, FloatType(), nullable=True),\n            StructField(ColNames.quadkey, StringType(), nullable=True),\n        ]\n    )\n\n    MANDATORY_COLUMNS = [ColNames.grid_id, ColNames.geometry]\n    OPTIONAL_COLUMNS = [ColNames.elevation, ColNames.ple_coefficient, ColNames.prior_probability]\n\n    def __init__(\n        self, spark: SparkSession, default_path: str, partition_columns: list[str] = None, default_crs: int = 3035\n    ) -&gt; None:\n\n        super().__init__(spark, default_path)\n        self.interface: GeoParquetInterface = GeoParquetInterface()\n        self.partition_columns = partition_columns\n\n        # Clear path\n        self.first_write = True\n        self.default_crs = default_crs\n\n    def read(self):\n\n        self.df = self.interface.read_from_interface(self.spark, self.default_path, self.SCHEMA)\n        self.df = self.df.withColumn(ColNames.geometry, STF.ST_SetSRID((ColNames.geometry), F.lit(self.default_crs)))\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_event_data_object/","title":"silver_event_data_object","text":"<p>Silver MNO Event data module</p>"},{"location":"reference/core/data_objects/silver/silver_event_data_object/#core.data_objects.silver.silver_event_data_object.SilverEventDataObject","title":"<code>SilverEventDataObject</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models the cleaned MNO Event data.</p> Source code in <code>multimno/core/data_objects/silver/silver_event_data_object.py</code> <pre><code>class SilverEventDataObject(PathDataObject):\n    \"\"\"\n    Class that models the cleaned MNO Event data.\n    \"\"\"\n\n    ID = \"SilverEventDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.mcc, IntegerType(), nullable=True),\n            StructField(ColNames.mnc, StringType(), nullable=True),\n            StructField(ColNames.plmn, IntegerType(), nullable=True),\n            StructField(ColNames.cell_id, StringType(), nullable=True),\n            StructField(ColNames.latitude, FloatType(), nullable=True),\n            StructField(ColNames.longitude, FloatType(), nullable=True),\n            StructField(ColNames.loc_error, FloatType(), nullable=True),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.user_id_modulo, IntegerType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface: ParquetInterface = ParquetInterface()\n        self.partition_columns = [\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            ColNames.user_id_modulo,\n        ]\n\n        # Clear path\n        self.first_write = True\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        # If it is the first writing of this data object, clear the input directory, otherwise add\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n\n        self.df.write.format(\n            self.interface.FILE_FORMAT,  # File format\n        ).partitionBy(self.partition_columns).mode(\n            \"append\"\n        ).save(path)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_metrics_by_column/","title":"silver_event_data_syntactic_quality_metrics_by_column","text":"<p>Silver Event Data quality metrics.</p>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_metrics_by_column/#core.data_objects.silver.silver_event_data_syntactic_quality_metrics_by_column.SilverEventDataSyntacticQualityMetricsByColumn","title":"<code>SilverEventDataSyntacticQualityMetricsByColumn</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models the Silver Event Data quality metrics DataObject.</p> Source code in <code>multimno/core/data_objects/silver/silver_event_data_syntactic_quality_metrics_by_column.py</code> <pre><code>class SilverEventDataSyntacticQualityMetricsByColumn(PathDataObject):\n    \"\"\"\n    Class that models the Silver Event Data quality metrics DataObject.\n    \"\"\"\n\n    ID = \"SilverEventDataSyntacticQualityMetricsByColumn\"\n    SCHEMA = StructType(\n        [\n            StructField(\"result_timestamp\", TimestampType(), nullable=False),\n            StructField(\"date\", DateType(), nullable=False),\n            StructField(\"variable\", StringType(), nullable=True),\n            StructField(\"type_of_error\", ShortType(), nullable=True),\n            StructField(\"type_of_transformation\", ShortType(), nullable=True),\n            StructField(\"value\", IntegerType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface: ParquetInterface = ParquetInterface()\n        self.partition_columns = [\"date\"]\n\n        # (variable, type_of_error, type_of_transformation) : value\n        self.error_and_transformation_counts = defaultdict(int)\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.df.write.format(\n            self.interface.FILE_FORMAT,  # File format\n        ).partitionBy(partition_columns).mode(\n            \"append\"\n        ).save(path)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_metrics_frequency_distribution/","title":"silver_event_data_syntactic_quality_metrics_frequency_distribution","text":"<p>Silver Event Data deduplication frequency quality metrics.</p>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_metrics_frequency_distribution/#core.data_objects.silver.silver_event_data_syntactic_quality_metrics_frequency_distribution.SilverEventDataSyntacticQualityMetricsFrequencyDistribution","title":"<code>SilverEventDataSyntacticQualityMetricsFrequencyDistribution</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models the Silver Event Data syntactic frequency quality metrics DataObject.</p> Source code in <code>multimno/core/data_objects/silver/silver_event_data_syntactic_quality_metrics_frequency_distribution.py</code> <pre><code>class SilverEventDataSyntacticQualityMetricsFrequencyDistribution(PathDataObject):\n    \"\"\"\n    Class that models the Silver Event Data syntactic\n    frequency quality metrics DataObject.\n    \"\"\"\n\n    ID = \"SilverEventDataSyntacticQualityMetricsFrequencyDistribution\"\n    SCHEMA = StructType(\n        [\n            StructField(\"cell_id\", StringType(), nullable=True),\n            StructField(\"user_id\", BinaryType(), nullable=True),\n            StructField(\"initial_frequency\", IntegerType(), nullable=False),\n            StructField(\"final_frequency\", IntegerType(), nullable=False),\n            StructField(\"date\", DateType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface: ParquetInterface = ParquetInterface()\n        self.partition_columns = [\"date\"]\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.df.write.format(\n            self.interface.FILE_FORMAT,  # File format\n        ).partitionBy(partition_columns).mode(\n            \"append\"\n        ).save(path)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_warnings_for_plots/","title":"silver_event_data_syntactic_quality_warnings_for_plots","text":"<p>Silver Event Data quality warning for plots table.</p>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_warnings_for_plots/#core.data_objects.silver.silver_event_data_syntactic_quality_warnings_for_plots.SilverEventDataSyntacticQualityWarningsForPlots","title":"<code>SilverEventDataSyntacticQualityWarningsForPlots</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that store data to plot raw, clean data sizez and error rate.</p> Source code in <code>multimno/core/data_objects/silver/silver_event_data_syntactic_quality_warnings_for_plots.py</code> <pre><code>class SilverEventDataSyntacticQualityWarningsForPlots(PathDataObject):\n    \"\"\"\n    Class that store data to plot raw, clean data sizez and error rate.\n    \"\"\"\n\n    ID = \"SilverEventDataSyntacticQualityWarningsForPlots\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.date, DateType(), nullable=False),\n            StructField(ColNames.type_of_qw, StringType(), nullable=False),\n            StructField(ColNames.lookback_period, StringType(), nullable=False),\n            StructField(ColNames.daily_value, FloatType(), nullable=False),\n            StructField(ColNames.average, FloatType(), nullable=True),\n            StructField(ColNames.LCL, FloatType(), nullable=True),\n            StructField(ColNames.UCL, FloatType(), nullable=True),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface: ParquetInterface = ParquetInterface()\n        self.partition_columns = [ColNames.date]\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.df.write.format(\n            self.interface.FILE_FORMAT,\n        ).partitionBy(partition_columns).mode(\n            \"append\"\n        ).save(path)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_warnings_log_table/","title":"silver_event_data_syntactic_quality_warnings_log_table","text":"<p>Silver Event Data Quality Warning log table.</p>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_warnings_log_table/#core.data_objects.silver.silver_event_data_syntactic_quality_warnings_log_table.SilverEventDataSyntacticQualityWarningsLogTable","title":"<code>SilverEventDataSyntacticQualityWarningsLogTable</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that stores information about Event Quallity Warnings</p> Source code in <code>multimno/core/data_objects/silver/silver_event_data_syntactic_quality_warnings_log_table.py</code> <pre><code>class SilverEventDataSyntacticQualityWarningsLogTable(PathDataObject):\n    \"\"\"\n    Class that stores information about Event Quallity Warnings\n    \"\"\"\n\n    ID = \"SilverEventDataSyntacticQualityWarningsLogTable\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.date, DateType(), nullable=False),\n            StructField(ColNames.measure_definition, StringType(), nullable=False),\n            StructField(ColNames.lookback_period, StringType(), nullable=False),\n            StructField(ColNames.daily_value, FloatType(), nullable=False),\n            StructField(ColNames.condition_value, FloatType(), nullable=True),\n            StructField(ColNames.condition, StringType(), nullable=False),\n            StructField(ColNames.warning_text, StringType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface: ParquetInterface = ParquetInterface()\n        self.partition_columns = [ColNames.date]\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.df.write.format(\n            self.interface.FILE_FORMAT,\n        ).partitionBy(partition_columns).mode(\n            \"append\"\n        ).save(path)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_event_flagged_data_object/","title":"silver_event_flagged_data_object","text":"<p>Silver MNO Event data module with flags computed in Semantic Checks</p>"},{"location":"reference/core/data_objects/silver/silver_event_flagged_data_object/#core.data_objects.silver.silver_event_flagged_data_object.SilverEventFlaggedDataObject","title":"<code>SilverEventFlaggedDataObject</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models the cleaned MNO Event data, with flags computed in the semantic checks module.</p> Source code in <code>multimno/core/data_objects/silver/silver_event_flagged_data_object.py</code> <pre><code>class SilverEventFlaggedDataObject(PathDataObject):\n    \"\"\"\n    Class that models the cleaned MNO Event data, with flags computed\n    in the semantic checks module.\n    \"\"\"\n\n    ID = \"SilverEventFlaggedDO\"\n\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.mcc, IntegerType(), nullable=True),\n            StructField(ColNames.mnc, StringType(), nullable=True),\n            StructField(ColNames.plmn, IntegerType(), nullable=True),\n            StructField(ColNames.cell_id, StringType(), nullable=True),\n            StructField(ColNames.latitude, FloatType(), nullable=True),\n            StructField(ColNames.longitude, FloatType(), nullable=True),\n            StructField(ColNames.loc_error, FloatType(), nullable=True),\n            StructField(ColNames.error_flag, IntegerType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.user_id_modulo, IntegerType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = [\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            ColNames.user_id_modulo,\n        ]\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_geozones_grid_map_data_object/","title":"silver_geozones_grid_map_data_object","text":""},{"location":"reference/core/data_objects/silver/silver_geozones_grid_map_data_object/#core.data_objects.silver.silver_geozones_grid_map_data_object.SilverGeozonesGridMapDataObject","title":"<code>SilverGeozonesGridMapDataObject</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models geographic and admin zones to grid mapping table.</p> Source code in <code>multimno/core/data_objects/silver/silver_geozones_grid_map_data_object.py</code> <pre><code>class SilverGeozonesGridMapDataObject(PathDataObject):\n    \"\"\"\n    Class that models geographic and admin zones to grid mapping table.\n    \"\"\"\n\n    ID = \"SilverGeozonesGridMapDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.grid_id, StringType(), nullable=False),\n            StructField(ColNames.zone_id, StringType(), nullable=False),\n            StructField(ColNames.hierarchical_id, StringType(), nullable=False),\n            # partition columns\n            StructField(ColNames.dataset_id, StringType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            # StructField(ColNames.quadkey, StringType(), nullable=False),\n        ]\n    )\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        default_path: str,\n        partition_columns: list[str] = None,\n    ) -&gt; None:\n\n        super().__init__(spark, default_path)\n        self.interface: ParquetInterface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n        # Clear path\n        self.first_write = True\n\n    def read(self):\n\n        self.df = self.interface.read_from_interface(self.spark, self.default_path, self.SCHEMA)\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_grid_data_object/","title":"silver_grid_data_object","text":""},{"location":"reference/core/data_objects/silver/silver_grid_data_object/#core.data_objects.silver.silver_grid_data_object.SilverGridDataObject","title":"<code>SilverGridDataObject</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models operational grid.</p> Source code in <code>multimno/core/data_objects/silver/silver_grid_data_object.py</code> <pre><code>class SilverGridDataObject(PathDataObject):\n    \"\"\"\n    Class that models operational grid.\n    \"\"\"\n\n    ID = \"SilverGridDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.geometry, GeometryType(), nullable=False),\n            StructField(ColNames.grid_id, StringType(), nullable=False),\n            StructField(ColNames.quadkey, StringType(), nullable=True),\n        ]\n    )\n\n    MANDATORY_COLUMNS = [ColNames.grid_id, ColNames.geometry]\n\n    def __init__(\n        self, spark: SparkSession, default_path: str, partition_columns: list[str] = None, default_crs: int = 3035\n    ) -&gt; None:\n\n        super().__init__(spark, default_path)\n        self.interface: GeoParquetInterface = GeoParquetInterface()\n        self.partition_columns = partition_columns\n\n        # Clear path\n        self.first_write = True\n        self.default_crs = default_crs\n\n    def read(self):\n\n        self.df = self.interface.read_from_interface(self.spark, self.default_path, self.SCHEMA)\n        self.df = self.df.withColumn(ColNames.geometry, STF.ST_SetSRID((ColNames.geometry), F.lit(self.default_crs)))\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_longterm_permanence_score_data_object/","title":"silver_longterm_permanence_score_data_object","text":"<p>Silver Mid Term Permanence Score data object module</p>"},{"location":"reference/core/data_objects/silver/silver_longterm_permanence_score_data_object/#core.data_objects.silver.silver_longterm_permanence_score_data_object.SilverLongtermPermanenceScoreDataObject","title":"<code>SilverLongtermPermanenceScoreDataObject</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models the Longterm Permanence Score data object.</p> Source code in <code>multimno/core/data_objects/silver/silver_longterm_permanence_score_data_object.py</code> <pre><code>class SilverLongtermPermanenceScoreDataObject(PathDataObject):\n    \"\"\"\n    Class that models the Longterm Permanence Score data object.\n    \"\"\"\n\n    ID = \"SilverLongtermPermanenceScoreDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.grid_id, StringType(), nullable=False),\n            StructField(ColNames.lps, IntegerType(), nullable=False),\n            StructField(ColNames.total_frequency, IntegerType(), nullable=False),\n            StructField(ColNames.frequency_mean, FloatType(), nullable=True),\n            StructField(ColNames.frequency_std, FloatType(), nullable=True),\n            StructField(ColNames.regularity_mean, FloatType(), nullable=True),\n            StructField(ColNames.regularity_std, FloatType(), nullable=True),\n            # partition columns\n            StructField(ColNames.start_date, DateType(), nullable=False),\n            StructField(ColNames.end_date, DateType(), nullable=False),\n            StructField(ColNames.season, StringType(), nullable=False),\n            StructField(ColNames.day_type, StringType(), nullable=False),\n            StructField(ColNames.time_interval, StringType(), nullable=False),\n            StructField(ColNames.id_type, StringType(), nullable=False),\n            StructField(ColNames.user_id_modulo, IntegerType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface: ParquetInterface = ParquetInterface()\n        self.partition_columns = [\n            ColNames.season,\n            ColNames.start_date,\n            ColNames.end_date,\n            ColNames.day_type,\n            ColNames.time_interval,\n            ColNames.id_type,\n            ColNames.user_id_modulo,\n        ]\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_midterm_permanence_score_data_object/","title":"silver_midterm_permanence_score_data_object","text":"<p>Silver Mid Term Permanence Score data object module</p>"},{"location":"reference/core/data_objects/silver/silver_midterm_permanence_score_data_object/#core.data_objects.silver.silver_midterm_permanence_score_data_object.SilverMidtermPermanenceScoreDataObject","title":"<code>SilverMidtermPermanenceScoreDataObject</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models the Midterm Permanence Score data object.</p> Source code in <code>multimno/core/data_objects/silver/silver_midterm_permanence_score_data_object.py</code> <pre><code>class SilverMidtermPermanenceScoreDataObject(PathDataObject):\n    \"\"\"\n    Class that models the Midterm Permanence Score data object.\n    \"\"\"\n\n    ID = \"SilverMidtermPermanenceScoreDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.grid_id, StringType(), nullable=False),\n            StructField(ColNames.mps, IntegerType(), nullable=False),\n            StructField(ColNames.frequency, IntegerType(), nullable=False),\n            StructField(ColNames.regularity_mean, FloatType(), nullable=True),\n            StructField(ColNames.regularity_std, FloatType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day_type, StringType(), nullable=False),\n            StructField(ColNames.time_interval, StringType(), nullable=False),\n            StructField(ColNames.id_type, StringType(), nullable=False),\n            StructField(ColNames.user_id_modulo, IntegerType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface: ParquetInterface = ParquetInterface()\n        self.partition_columns = [\n            ColNames.year,\n            ColNames.month,\n            ColNames.day_type,\n            ColNames.time_interval,\n            ColNames.id_type,\n            ColNames.user_id_modulo,\n        ]\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_network_data_object/","title":"silver_network_data_object","text":"<p>Silver MNO Network Topology Data module</p>"},{"location":"reference/core/data_objects/silver/silver_network_data_object/#core.data_objects.silver.silver_network_data_object.SilverNetworkDataObject","title":"<code>SilverNetworkDataObject</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models the clean MNO Network Topology Data, based on the physical properties of the cells.</p> Source code in <code>multimno/core/data_objects/silver/silver_network_data_object.py</code> <pre><code>class SilverNetworkDataObject(PathDataObject):\n    \"\"\"\n    Class that models the clean MNO Network Topology Data, based on the physical\n    properties of the cells.\n    \"\"\"\n\n    ID = \"SilverNetworkDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.cell_id, StringType(), nullable=False),\n            StructField(ColNames.latitude, FloatType(), nullable=False),\n            StructField(ColNames.longitude, FloatType(), nullable=False),\n            StructField(ColNames.altitude, FloatType(), nullable=True),\n            StructField(ColNames.antenna_height, FloatType(), nullable=True),\n            StructField(ColNames.directionality, IntegerType(), nullable=False),\n            StructField(ColNames.azimuth_angle, FloatType(), nullable=True),\n            StructField(ColNames.elevation_angle, FloatType(), nullable=True),\n            StructField(ColNames.horizontal_beam_width, FloatType(), nullable=True),\n            StructField(ColNames.vertical_beam_width, FloatType(), nullable=True),\n            StructField(ColNames.power, FloatType(), nullable=True),\n            StructField(ColNames.range, FloatType(), nullable=True),\n            StructField(ColNames.frequency, IntegerType(), nullable=True),\n            StructField(ColNames.technology, StringType(), nullable=True),\n            StructField(ColNames.valid_date_start, TimestampType(), nullable=True),\n            StructField(ColNames.valid_date_end, TimestampType(), nullable=True),\n            StructField(ColNames.cell_type, StringType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_network_data_syntactic_quality_metrics_by_column/","title":"silver_network_data_syntactic_quality_metrics_by_column","text":"<p>Silver Network topology quality metrics.</p>"},{"location":"reference/core/data_objects/silver/silver_network_data_syntactic_quality_metrics_by_column/#core.data_objects.silver.silver_network_data_syntactic_quality_metrics_by_column.SilverNetworkDataQualityMetricsByColumn","title":"<code>SilverNetworkDataQualityMetricsByColumn</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models the Silver Network Topology data quality metrics data object.</p> Source code in <code>multimno/core/data_objects/silver/silver_network_data_syntactic_quality_metrics_by_column.py</code> <pre><code>class SilverNetworkDataQualityMetricsByColumn(PathDataObject):\n    \"\"\"\n    Class that models the Silver Network Topology data quality metrics data object.\n    \"\"\"\n\n    ID = \"SilverNetworkDataQualityMetricsByColumn\"\n\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.result_timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.date, DateType(), nullable=False),\n            StructField(ColNames.field_name, StringType(), nullable=True),\n            StructField(ColNames.type_code, IntegerType(), nullable=False),\n            StructField(ColNames.value, IntegerType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_network_data_top_frequent_errors_data_object/","title":"silver_network_data_top_frequent_errors_data_object","text":"<p>Silver Network Data Top Frequent Errors.</p>"},{"location":"reference/core/data_objects/silver/silver_network_data_top_frequent_errors_data_object/#core.data_objects.silver.silver_network_data_top_frequent_errors_data_object.SilverNetworkDataTopFrequentErrors","title":"<code>SilverNetworkDataTopFrequentErrors</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models the Silver Network Topology Top Frequent Errors data object</p> Source code in <code>multimno/core/data_objects/silver/silver_network_data_top_frequent_errors_data_object.py</code> <pre><code>class SilverNetworkDataTopFrequentErrors(PathDataObject):\n    \"\"\"\n    Class that models the Silver Network Topology Top Frequent Errors data object\n    \"\"\"\n\n    ID = \"SilverNetworkDataTopFrequentErrorsDO\"\n\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.result_timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.field_name, StringType(), nullable=False),\n            StructField(ColNames.type_code, IntegerType(), nullable=False),\n            StructField(ColNames.error_value, StringType(), nullable=False),\n            StructField(ColNames.error_count, IntegerType(), nullable=False),\n            StructField(ColNames.accumulated_percentage, FloatType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_network_row_error_metrics/","title":"silver_network_row_error_metrics","text":"<p>Silver Network Data Row Error Metrics.</p>"},{"location":"reference/core/data_objects/silver/silver_network_row_error_metrics/#core.data_objects.silver.silver_network_row_error_metrics.SilverNetworkRowErrorMetrics","title":"<code>SilverNetworkRowErrorMetrics</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models the Silver Network Topology Row Error Metrics data object</p> Source code in <code>multimno/core/data_objects/silver/silver_network_row_error_metrics.py</code> <pre><code>class SilverNetworkRowErrorMetrics(PathDataObject):\n    \"\"\"\n    Class that models the Silver Network Topology Row Error Metrics data object\n    \"\"\"\n\n    ID = \"SilverNetworkRowErrorMetricsDO\"\n\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.result_timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.variable, StringType(), nullable=False),\n            StructField(ColNames.value, IntegerType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_network_syntactic_quality_warnings_log_table/","title":"silver_network_syntactic_quality_warnings_log_table","text":"<p>Silver MNO Network Topology Quality Warnings Log Table Data Object</p>"},{"location":"reference/core/data_objects/silver/silver_network_syntactic_quality_warnings_log_table/#core.data_objects.silver.silver_network_syntactic_quality_warnings_log_table.SilverNetworkDataSyntacticQualityWarningsLogTable","title":"<code>SilverNetworkDataSyntacticQualityWarningsLogTable</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models the log table keeping track of the quality warnings that may arise from the syntactic checks and cleaning of the MNO Network Topology Data.</p> Source code in <code>multimno/core/data_objects/silver/silver_network_syntactic_quality_warnings_log_table.py</code> <pre><code>class SilverNetworkDataSyntacticQualityWarningsLogTable(PathDataObject):\n    \"\"\"\n    Class that models the log table keeping track of the quality warnings that may arise from\n    the syntactic checks and cleaning of the MNO Network Topology Data.\n    \"\"\"\n\n    ID = \"SilverNetworkDataSyntacticQualityWarningsLogTable\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.title, StringType(), nullable=False),\n            StructField(ColNames.date, DateType(), nullable=False),  # date of study analysed\n            StructField(ColNames.timestamp, TimestampType(), nullable=False),  # moment when QW where generated\n            StructField(ColNames.measure_definition, StringType(), nullable=False),\n            StructField(ColNames.daily_value, FloatType(), nullable=False),\n            StructField(ColNames.condition, StringType(), nullable=False),\n            StructField(ColNames.lookback_period, StringType(), nullable=False),  # using same name as for events\n            StructField(ColNames.condition_value, FloatType(), nullable=False),\n            StructField(ColNames.warning_text, StringType(), nullable=False),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        # Always append\n        self.df.write.format(\n            self.interface.FILE_FORMAT,\n        ).partitionBy(partition_columns).mode(\n            \"append\"\n        ).save(path)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_network_syntactic_quality_warnings_plot_data/","title":"silver_network_syntactic_quality_warnings_plot_data","text":"<p>Silver MNO Network Topology Quality Warnings Data Object for the generation of plots</p>"},{"location":"reference/core/data_objects/silver/silver_network_syntactic_quality_warnings_plot_data/#core.data_objects.silver.silver_network_syntactic_quality_warnings_plot_data.SilverNetworkSyntacticQualityWarningsLinePlotData","title":"<code>SilverNetworkSyntacticQualityWarningsLinePlotData</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models the data required to produce line plots reflecting the daily evolution of the number of rows before and after the syntactic checks, as well as the overall error rate.</p> Source code in <code>multimno/core/data_objects/silver/silver_network_syntactic_quality_warnings_plot_data.py</code> <pre><code>class SilverNetworkSyntacticQualityWarningsLinePlotData(PathDataObject):\n    \"\"\"\n    Class that models the data required to produce line plots reflecting the daily evolution of the number\n    of rows before and after the syntactic checks, as well as the overall error rate.\n    \"\"\"\n\n    ID = \"SilverNetworkSyntacticQualityWarningsLinePlotData\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.date, DateType(), nullable=False),\n            StructField(ColNames.daily_value, FloatType(), nullable=False),\n            StructField(ColNames.average, FloatType(), nullable=False),\n            StructField(ColNames.LCL, FloatType(), nullable=False),\n            StructField(ColNames.UCL, FloatType(), nullable=True),\n            # partition columns\n            StructField(ColNames.variable, StringType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.timestamp, TimestampType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = [ColNames.variable, ColNames.year, ColNames.month, ColNames.day, ColNames.timestamp]\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_network_syntactic_quality_warnings_plot_data/#core.data_objects.silver.silver_network_syntactic_quality_warnings_plot_data.SilverNetworkSyntacticQualityWarningsPiePlotData","title":"<code>SilverNetworkSyntacticQualityWarningsPiePlotData</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models the data required to produce pie plots reflecting the percentage of each type of error for each field of the network topology data object.</p> Source code in <code>multimno/core/data_objects/silver/silver_network_syntactic_quality_warnings_plot_data.py</code> <pre><code>class SilverNetworkSyntacticQualityWarningsPiePlotData(PathDataObject):\n    \"\"\"\n    Class that models the data required to produce pie plots reflecting the percentage of each type of error\n    for each field of the network topology data object.\n    \"\"\"\n\n    ID = \"SilverNetworkSyntacticQualityWarningsPiePlotData\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.type_of_error, StringType(), nullable=False),\n            StructField(ColNames.value, IntegerType(), nullable=False),\n            # partition columns\n            StructField(ColNames.variable, StringType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.timestamp, TimestampType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = [ColNames.variable, ColNames.year, ColNames.month, ColNames.day, ColNames.timestamp]\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_present_population_data_object/","title":"silver_present_population_data_object","text":"<p>Silver present population estimatation per grid data object</p>"},{"location":"reference/core/data_objects/silver/silver_present_population_data_object/#core.data_objects.silver.silver_present_population_data_object.SilverPresentPopulationDataObject","title":"<code>SilverPresentPopulationDataObject</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Estimation of the population present at a given time at the grid tile level.</p> Source code in <code>multimno/core/data_objects/silver/silver_present_population_data_object.py</code> <pre><code>class SilverPresentPopulationDataObject(PathDataObject):\n    \"\"\"\n    Estimation of the population present at a given time at the grid tile level.\n    \"\"\"\n\n    ID = \"SilverPresentPopulationDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.grid_id, StringType(), nullable=False),\n            StructField(ColNames.population, FloatType(), nullable=False),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.timestamp, TimestampType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_present_population_zone_data_object/","title":"silver_present_population_zone_data_object","text":"<p>Silver present population estimatation per zone data object</p>"},{"location":"reference/core/data_objects/silver/silver_present_population_zone_data_object/#core.data_objects.silver.silver_present_population_zone_data_object.SilverPresentPopulationZoneDataObject","title":"<code>SilverPresentPopulationZoneDataObject</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Estimation of the population present at a given time at the level of some zoning system.</p> Source code in <code>multimno/core/data_objects/silver/silver_present_population_zone_data_object.py</code> <pre><code>class SilverPresentPopulationZoneDataObject(PathDataObject):\n    \"\"\"\n    Estimation of the population present at a given time at the level of some zoning system.\n    \"\"\"\n\n    ID = \"SilverPresentPopulationZoneDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.zone_id, StringType(), nullable=False),\n            StructField(ColNames.population, FloatType(), nullable=False),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.timestamp, TimestampType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_semantic_quality_metrics/","title":"silver_semantic_quality_metrics","text":""},{"location":"reference/core/data_objects/silver/silver_semantic_quality_metrics/#core.data_objects.silver.silver_semantic_quality_metrics.SilverEventSemanticQualityMetrics","title":"<code>SilverEventSemanticQualityMetrics</code>","text":"<p>               Bases: <code>PathDataObject</code></p> Source code in <code>multimno/core/data_objects/silver/silver_semantic_quality_metrics.py</code> <pre><code>class SilverEventSemanticQualityMetrics(PathDataObject):\n    \"\"\" \"\"\"\n\n    ID = \"SilverEventSemanticQualityMetrics\"\n\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.result_timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.variable, StringType(), nullable=False),\n            StructField(ColNames.type_of_error, IntegerType(), nullable=False),\n            StructField(ColNames.value, LongType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        # Always append\n        self.df.write.format(\n            self.interface.FILE_FORMAT,\n        ).partitionBy(partition_columns).mode(\n            \"overwrite\"\n        ).save(path)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_semantic_quality_warnings_log_table/","title":"silver_semantic_quality_warnings_log_table","text":"<p>Silver MNO Network Topology Quality Warnings Log Table Data Object</p>"},{"location":"reference/core/data_objects/silver/silver_semantic_quality_warnings_log_table/#core.data_objects.silver.silver_semantic_quality_warnings_log_table.SilverEventSemanticQualityWarningsLogTable","title":"<code>SilverEventSemanticQualityWarningsLogTable</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models the log table keeping track of the quality warnings that may arise from the semantic checks of the MNO Event data.</p> Source code in <code>multimno/core/data_objects/silver/silver_semantic_quality_warnings_log_table.py</code> <pre><code>class SilverEventSemanticQualityWarningsLogTable(PathDataObject):\n    \"\"\"\n    Class that models the log table keeping track of the quality warnings that may arise from\n    the semantic checks of the MNO Event data.\n    \"\"\"\n\n    ID = \"SilverEventSemanticQualityWarningsLogTable\"\n\n    SCHEMA = StructType(\n        [\n            StructField(\"date\", DateType(), nullable=False),\n            StructField(\"Error 1\", FloatType(), nullable=False),\n            StructField(\"Error 2\", FloatType(), nullable=False),\n            StructField(\"Error 3\", FloatType(), nullable=False),\n            StructField(\"Error 4\", FloatType(), nullable=False),\n            StructField(\"Error 5\", FloatType(), nullable=False),\n            StructField(\"Error 1 upper control limit\", FloatType(), nullable=True),\n            StructField(\"Error 2 upper control limit\", FloatType(), nullable=True),\n            StructField(\"Error 3 upper control limit\", FloatType(), nullable=True),\n            StructField(\"Error 4 upper control limit\", FloatType(), nullable=True),\n            StructField(\"Error 5 upper control limit\", FloatType(), nullable=True),\n            StructField(\"Error 1 display warning\", BooleanType(), nullable=False),\n            StructField(\"Error 2 display warning\", BooleanType(), nullable=False),\n            StructField(\"Error 3 display warning\", BooleanType(), nullable=False),\n            StructField(\"Error 4 display warning\", BooleanType(), nullable=False),\n            StructField(\"Error 5 display warning\", BooleanType(), nullable=False),\n            StructField(\"execution_id\", TimestampType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        default_path: str,\n        partition_columns: list[str] = None,\n    ) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        # Always append\n        self.df.write.format(\n            self.interface.FILE_FORMAT,\n        ).partitionBy(partition_columns).mode(\n            \"append\"\n        ).save(path)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_semantic_quality_warnings_plot_data/","title":"silver_semantic_quality_warnings_plot_data","text":"<p>Silver MNO Network Topology Quality Warnings Log Table Data Object</p>"},{"location":"reference/core/data_objects/silver/silver_semantic_quality_warnings_plot_data/#core.data_objects.silver.silver_semantic_quality_warnings_plot_data.SilverEventSemanticQualityWarningsBarPlotData","title":"<code>SilverEventSemanticQualityWarningsBarPlotData</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models the data required to produce line plots reflecting the daily evolution of the number of each type of error, as well as records without errors, as seen during the semantic checks of MNO Event data.</p> Source code in <code>multimno/core/data_objects/silver/silver_semantic_quality_warnings_plot_data.py</code> <pre><code>class SilverEventSemanticQualityWarningsBarPlotData(PathDataObject):\n    \"\"\"\n    Class that models the data required to produce line plots reflecting the daily evolution of the number\n    of each type of error, as well as records without errors, as seen during the semantic checks of MNO Event data.\n    \"\"\"\n\n    ID = \"SilverEventSemanticQualityWarningBarPlotData\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.date, DateType(), nullable=False),\n            StructField(ColNames.type_of_error, StringType(), nullable=False),\n            StructField(ColNames.value, FloatType(), nullable=False),\n            # partition columns\n            StructField(ColNames.variable, StringType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.timestamp, TimestampType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = [ColNames.variable, ColNames.year, ColNames.month, ColNames.day, ColNames.timestamp]\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_signal_strength_data_object/","title":"silver_signal_strength_data_object","text":""},{"location":"reference/core/data_objects/silver/silver_signal_strength_data_object/#core.data_objects.silver.silver_signal_strength_data_object.SilverSignalStrengthDataObject","title":"<code>SilverSignalStrengthDataObject</code>","text":"<p>               Bases: <code>PathDataObject</code></p> Source code in <code>multimno/core/data_objects/silver/silver_signal_strength_data_object.py</code> <pre><code>class SilverSignalStrengthDataObject(PathDataObject):\n    \"\"\" \"\"\"\n\n    ID = \"SilverSignalStrengthDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.cell_id, StringType(), nullable=True),\n            StructField(ColNames.grid_id, StringType(), nullable=True),\n            StructField(ColNames.valid_date_start, DateType(), nullable=True),\n            StructField(ColNames.valid_date_end, DateType(), nullable=True),\n            StructField(ColNames.signal_strength, FloatType(), nullable=True),\n            StructField(ColNames.distance_to_cell, FloatType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=True),\n            StructField(ColNames.month, ByteType(), nullable=True),\n            StructField(ColNames.day, ByteType(), nullable=True),\n        ]\n    )\n\n    MANDATORY_COLUMNS = [\n        ColNames.cell_id,\n        ColNames.grid_id,\n        ColNames.valid_date_start,\n        ColNames.valid_date_end,\n        ColNames.signal_strength,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    ]\n\n    OPTIONAL_COLUMNS = [ColNames.distance_to_cell]\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_time_segments_data_object/","title":"silver_time_segments_data_object","text":""},{"location":"reference/core/data_objects/silver/silver_time_segments_data_object/#core.data_objects.silver.silver_time_segments_data_object.SilverTimeSegmentsDataObject","title":"<code>SilverTimeSegmentsDataObject</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models the cleaned MNO Event data.</p> Source code in <code>multimno/core/data_objects/silver/silver_time_segments_data_object.py</code> <pre><code>class SilverTimeSegmentsDataObject(PathDataObject):\n    \"\"\"\n    Class that models the cleaned MNO Event data.\n    \"\"\"\n\n    ID = \"SilverTimeSegmentsDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.time_segment_id, IntegerType(), nullable=False),\n            StructField(ColNames.start_timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.end_timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.mcc, ShortType(), nullable=False),\n            StructField(ColNames.cells, ArrayType(StringType()), nullable=False),\n            StructField(ColNames.state, StringType(), nullable=False),\n            StructField(ColNames.is_last, BooleanType(), nullable=True),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.user_id_modulo, IntegerType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface: ParquetInterface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n        # Clear path\n        self.first_write = True\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        # If it is the first writing of this data object, clear the input directory, otherwise add\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_usual_environment_labeling_quality_metrics_data_object/","title":"silver_usual_environment_labeling_quality_metrics_data_object","text":"<p>Silver Usual Environment Labeling Quality Metrics data object module</p>"},{"location":"reference/core/data_objects/silver/silver_usual_environment_labeling_quality_metrics_data_object/#core.data_objects.silver.silver_usual_environment_labeling_quality_metrics_data_object.SilverUsualEnvironmentLabelingQualityMetricsDataObject","title":"<code>SilverUsualEnvironmentLabelingQualityMetricsDataObject</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models the Usual Environment Labeling Quality Metrics data object.</p> Source code in <code>multimno/core/data_objects/silver/silver_usual_environment_labeling_quality_metrics_data_object.py</code> <pre><code>class SilverUsualEnvironmentLabelingQualityMetricsDataObject(PathDataObject):\n    \"\"\"\n    Class that models the Usual Environment Labeling Quality Metrics data object.\n    \"\"\"\n\n    ID = \"SilverUsualEnvironmentLabelingQualityMetricsDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.labeling_quality_metric, StringType(), nullable=False),\n            StructField(ColNames.labeling_quality_count, IntegerType(), nullable=False),\n            # partition columns\n            StructField(ColNames.start_date, DateType(), nullable=False),\n            StructField(ColNames.end_date, DateType(), nullable=False),\n            StructField(ColNames.season, StringType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface: ParquetInterface = ParquetInterface()\n        self.partition_columns = [\n            ColNames.season,\n            ColNames.start_date,\n            ColNames.end_date,\n        ]\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_usual_environment_labels_data_object/","title":"silver_usual_environment_labels_data_object","text":"<p>Silver Usual Environment Labels data object module</p>"},{"location":"reference/core/data_objects/silver/silver_usual_environment_labels_data_object/#core.data_objects.silver.silver_usual_environment_labels_data_object.SilverUsualEnvironmentLabelsDataObject","title":"<code>SilverUsualEnvironmentLabelsDataObject</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models the Usual Environment Labels data object.</p> Source code in <code>multimno/core/data_objects/silver/silver_usual_environment_labels_data_object.py</code> <pre><code>class SilverUsualEnvironmentLabelsDataObject(PathDataObject):\n    \"\"\"\n    Class that models the Usual Environment Labels data object.\n    \"\"\"\n\n    ID = \"SilverUsualEnvironmentLabelsDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.grid_id, StringType(), nullable=False),\n            StructField(ColNames.label, StringType(), nullable=False),\n            StructField(ColNames.ue_label_rule, StringType(), nullable=False),\n            StructField(ColNames.location_label_rule, StringType(), nullable=False),\n            # partition columns\n            StructField(ColNames.start_date, DateType(), nullable=False),\n            StructField(ColNames.end_date, DateType(), nullable=False),\n            StructField(ColNames.season, StringType(), nullable=False),\n            StructField(ColNames.user_id_modulo, IntegerType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface: ParquetInterface = ParquetInterface()\n        self.partition_columns = [\n            ColNames.season,\n            ColNames.start_date,\n            ColNames.end_date,\n            ColNames.user_id_modulo,\n        ]\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"}]}