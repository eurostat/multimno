{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MultiMNO","text":"<p>This repository contains code that processes MNO Data to generate population and mobility insights.</p>"},{"location":"#sections","title":"Sections","text":"<ul> <li>User Manual: Guide on how to setup, configure and execute the software.</li> <li>Dev Guide: Guidelines and best practices for contributing to the repository and setting up a development environment.</li> <li>Pipeline: View of the data processing pipeline.</li> <li>Reference: Code documentation.</li> <li>System Requirements: Mandatory requirements to execute the software.</li> <li>License: Software license - EUROPEAN UNION PUBLIC LICENCE v. 1.2.</li> </ul>"},{"location":"LICENSE/","title":"License","text":"<p>EUROPEAN UNION PUBLIC LICENCE v. 1.2  EUPL \u00a9 the European Union 2007, 2016 </p> <p>This European Union Public Licence (the \u2018EUPL\u2019) applies to the Work (as defined below) which is provided under the  terms of this Licence. Any use of the Work, other than as authorised under this Licence is prohibited (to the extent such  use is covered by a right of the copyright holder of the Work).  The Work is provided under the terms of this Licence when the Licensor (as defined below) has placed the following  notice immediately following the copyright notice for the Work:                            Licensed under the EUPL  or has expressed by any other means his willingness to license under the EUPL. </p> <p>1.Definitions  In this Licence, the following terms have the following meaning:  \u2014 \u2018The Licence\u2019:this Licence.  \u2014 \u2018The Original Work\u2019:the work or software distributed or communicated by the Licensor under this Licence, available  as Source Code and also as Executable Code as the case may be.  \u2014 \u2018Derivative Works\u2019:the works or software that could be created by the Licensee, based upon the Original Work or  modifications thereof. This Licence does not define the extent of modification or dependence on the Original Work  required in order to classify a work as a Derivative Work; this extent is determined by copyright law applicable in  the country mentioned in Article 15.  \u2014 \u2018The Work\u2019:the Original Work or its Derivative Works.  \u2014 \u2018The Source Code\u2019:the human-readable form of the Work which is the most convenient for people to study and  modify.  \u2014 \u2018The Executable Code\u2019:any code which has generally been compiled and which is meant to be interpreted by  a computer as a program.  \u2014 \u2018The Licensor\u2019:the natural or legal person that distributes or communicates the Work under the Licence.  \u2014 \u2018Contributor(s)\u2019:any natural or legal person who modifies the Work under the Licence, or otherwise contributes to  the creation of a Derivative Work.  \u2014 \u2018The Licensee\u2019 or \u2018You\u2019:any natural or legal person who makes any usage of the Work under the terms of the  Licence.  \u2014 \u2018Distribution\u2019 or \u2018Communication\u2019:any act of selling, giving, lending, renting, distributing, communicating,  transmitting, or otherwise making available, online or offline, copies of the Work or providing access to its essential  functionalities at the disposal of any other natural or legal person. </p> <p>2.Scope of the rights granted by the Licence  The Licensor hereby grants You a worldwide, royalty-free, non-exclusive, sublicensable licence to do the following, for  the duration of copyright vested in the Original Work:  \u2014 use the Work in any circumstance and for all usage,  \u2014 reproduce the Work,  \u2014 modify the Work, and make Derivative Works based upon the Work,  \u2014 communicate to the public, including the right to make available or display the Work or copies thereof to the public  and perform publicly, as the case may be, the Work,  \u2014 distribute the Work or copies thereof,  \u2014 lend and rent the Work or copies thereof,  \u2014 sublicense rights in the Work or copies thereof.  Those rights can be exercised on any media, supports and formats, whether now known or later invented, as far as the  applicable law permits so.  In the countries where moral rights apply, the Licensor waives his right to exercise his moral right to the extent allowed  by law in order to make effective the licence of the economic rights here above listed.  The Licensor grants to the Licensee royalty-free, non-exclusive usage rights to any patents held by the Licensor, to the  extent necessary to make use of the rights granted on the Work under this Licence. </p> <p>3.Communication of the Source Code  The Licensor may provide the Work either in its Source Code form, or as Executable Code. If the Work is provided as  Executable Code, the Licensor provides in addition a machine-readable copy of the Source Code of the Work along with  each copy of the Work that the Licensor distributes or indicates, in a notice following the copyright notice attached to  the Work, a repository where the Source Code is easily and freely accessible for as long as the Licensor continues to  distribute or communicate the Work. </p> <p>4.Limitations on copyright  Nothing in this Licence is intended to deprive the Licensee of the benefits from any exception or limitation to the  exclusive rights of the rights owners in the Work, of the exhaustion of those rights or of other applicable limitations  thereto. </p> <p>5.Obligations of the Licensee  The grant of the rights mentioned above is subject to some restrictions and obligations imposed on the Licensee. Those  obligations are the following: </p> <p>Attribution right: The Licensee shall keep intact all copyright, patent or trademarks notices and all notices that refer to  the Licence and to the disclaimer of warranties. The Licensee must include a copy of such notices and a copy of the  Licence with every copy of the Work he/she distributes or communicates. The Licensee must cause any Derivative Work  to carry prominent notices stating that the Work has been modified and the date of modification. </p> <p>Copyleft clause: If the Licensee distributes or communicates copies of the Original Works or Derivative Works, this  Distribution or Communication will be done under the terms of this Licence or of a later version of this Licence unless  the Original Work is expressly distributed only under this version of the Licence \u2014 for example by communicating  \u2018EUPL v. 1.2 only\u2019. The Licensee (becoming Licensor) cannot offer or impose any additional terms or conditions on the  Work or Derivative Work that alter or restrict the terms of the Licence. </p> <p>Compatibility clause: If the Licensee Distributes or Communicates Derivative Works or copies thereof based upon both  the Work and another work licensed under a Compatible Licence, this Distribution or Communication can be done  under the terms of this Compatible Licence. For the sake of this clause, \u2018Compatible Licence\u2019 refers to the licences listed  in the appendix attached to this Licence. Should the Licensee's obligations under the Compatible Licence conflict with  his/her obligations under this Licence, the obligations of the Compatible Licence shall prevail. </p> <p>Provision of Source Code: When distributing or communicating copies of the Work, the Licensee will provide  a machine-readable copy of the Source Code or indicate a repository where this Source will be easily and freely available  for as long as the Licensee continues to distribute or communicate the Work.  Legal Protection: This Licence does not grant permission to use the trade names, trademarks, service marks, or names  of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and  reproducing the content of the copyright notice. </p> <p>6.Chain of Authorship  The original Licensor warrants that the copyright in the Original Work granted hereunder is owned by him/her or  licensed to him/her and that he/she has the power and authority to grant the Licence.  Each Contributor warrants that the copyright in the modifications he/she brings to the Work are owned by him/her or  licensed to him/her and that he/she has the power and authority to grant the Licence.  Each time You accept the Licence, the original Licensor and subsequent Contributors grant You a licence to their contributions  to the Work, under the terms of this Licence. </p> <p>7.Disclaimer of Warranty  The Work is a work in progress, which is continuously improved by numerous Contributors. It is not a finished work  and may therefore contain defects or \u2018bugs\u2019 inherent to this type of development.  For the above reason, the Work is provided under the Licence on an \u2018as is\u2019 basis and without warranties of any kind  concerning the Work, including without limitation merchantability, fitness for a particular purpose, absence of defects or  errors, accuracy, non-infringement of intellectual property rights other than copyright as stated in Article 6 of this  Licence.  This disclaimer of warranty is an essential part of the Licence and a condition for the grant of any rights to the Work. </p> <p>8.Disclaimer of Liability  Except in the cases of wilful misconduct or damages directly caused to natural persons, the Licensor will in no event be  liable for any direct or indirect, material or moral, damages of any kind, arising out of the Licence or of the use of the  Work, including without limitation, damages for loss of goodwill, work stoppage, computer failure or malfunction, loss  of data or any commercial damage, even if the Licensor has been advised of the possibility of such damage. However,  the Licensor will be liable under statutory product liability laws as far such laws apply to the Work. </p> <p>9.Additional agreements  While distributing the Work, You may choose to conclude an additional agreement, defining obligations or services  consistent with this Licence. However, if accepting obligations, You may act only on your own behalf and on your sole  responsibility, not on behalf of the original Licensor or any other Contributor, and only if You agree to indemnify,  defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against such Contributor by  the fact You have accepted any warranty or additional liability. </p> <p>10.Acceptance of the Licence  The provisions of this Licence can be accepted by clicking on an icon \u2018I agree\u2019 placed under the bottom of a window  displaying the text of this Licence or by affirming consent in any other similar way, in accordance with the rules of  applicable law. Clicking on that icon indicates your clear and irrevocable acceptance of this Licence and all of its terms  and conditions.  Similarly, you irrevocably accept this Licence and all of its terms and conditions by exercising any rights granted to You  by Article 2 of this Licence, such as the use of the Work, the creation by You of a Derivative Work or the Distribution  or Communication by You of the Work or copies thereof. </p> <p>11.Information to the public  In case of any Distribution or Communication of the Work by means of electronic communication by You (for example,  by offering to download the Work from a remote location) the distribution channel or media (for example, a website)  must at least provide to the public the information requested by the applicable law regarding the Licensor, the Licence  and the way it may be accessible, concluded, stored and reproduced by the Licensee. </p> <p>12.Termination of the Licence  The Licence and the rights granted hereunder will terminate automatically upon any breach by the Licensee of the terms  of the Licence.  Such a termination will not terminate the licences of any person who has received the Work from the Licensee under  the Licence, provided such persons remain in full compliance with the Licence. </p> <p>13.Miscellaneous  Without prejudice of Article 9 above, the Licence represents the complete agreement between the Parties as to the  Work.  If any provision of the Licence is invalid or unenforceable under applicable law, this will not affect the validity or  enforceability of the Licence as a whole. Such provision will be construed or reformed so as necessary to make it valid  and enforceable.  The European Commission may publish other linguistic versions or new versions of this Licence or updated versions of  the Appendix, so far this is required and reasonable, without reducing the scope of the rights granted by the Licence.  New versions of the Licence will be published with a unique version number.  All linguistic versions of this Licence, approved by the European Commission, have identical value. Parties can take  advantage of the linguistic version of their choice. </p> <p>14.Jurisdiction  Without prejudice to specific agreement between parties,  \u2014 any litigation resulting from the interpretation of this License, arising between the European Union institutions,  bodies, offices or agencies, as a Licensor, and any Licensee, will be subject to the jurisdiction of the Court of Justice  of the European Union, as laid down in article 272 of the Treaty on the Functioning of the European Union,  \u2014 any litigation arising between other parties and resulting from the interpretation of this License, will be subject to  the exclusive jurisdiction of the competent court where the Licensor resides or conducts its primary business. </p> <p>15.Applicable Law  Without prejudice to specific agreement between parties,  \u2014 this Licence shall be governed by the law of the European Union Member State where the Licensor has his seat,  resides or has his registered office,  \u2014 this licence shall be governed by Belgian law if the Licensor has no seat, residence or registered office inside  a European Union Member State. </p> <pre><code>                                                     Appendix\n</code></pre> <p>\u2018Compatible Licences\u2019 according to Article 5 EUPL are:  \u2014 GNU General Public License (GPL) v. 2, v. 3  \u2014 GNU Affero General Public License (AGPL) v. 3  \u2014 Open Software License (OSL) v. 2.1, v. 3.0  \u2014 Eclipse Public License (EPL) v. 1.0  \u2014 CeCILL v. 2.0, v. 2.1  \u2014 Mozilla Public Licence (MPL) v. 2  \u2014 GNU Lesser General Public Licence (LGPL) v. 2.1, v. 3  \u2014 Creative Commons Attribution-ShareAlike v. 3.0 Unported (CC BY-SA 3.0) for works other than software  \u2014 European Union Public Licence (EUPL) v. 1.1, v. 1.2  \u2014 Qu\u00e9bec Free and Open-Source Licence \u2014 Reciprocity (LiLiQ-R) or Strong Reciprocity (LiLiQ-R+).</p> <p>The European Commission may update this Appendix to later versions of the above licences without producing  a new version of the EUPL, as long as they provide the rights granted in Article 2 of this Licence and protect the  covered Source Code from exclusive appropriation.  All other changes or additions to this Appendix require the production of a new EUPL version. </p>"},{"location":"pipeline/","title":"Pipeline","text":"<p>The software can perform the following pipeline:</p> <pre><code>%%{\n  init: {\n    'theme': 'base',\n    'themeVariables': {\n      'primaryColor': '#BB2528',\n      'primaryTextColor': '#000000',\n      'primaryBorderColor': '#7C0000',\n      'lineColor': '#F8B229',\n      'secondaryColor': '#006100',\n      'tertiaryColor': '#fff'\n    }\n  }\n}%%\nflowchart TD;\n    %% --- Reference Data ---\n    %% Inspire grid generation\n    InspireGridGeneration--&gt;InspireGridData[(Inspire Grid\\n)];\n    %% --- MNO Data ---\n    %% -- NETWORK --\n    %% RAW Network cleaning\n    PhysicalNetworkRAWData[(MNO-Network\\nPhysical\\nRAW)]--&gt;NetworkCleaning--&gt;PhysicalNetworkData[(MNO-Network\\nPhysical)];\n    NetworkCleaning--&gt;NetworkQAData[(MNO-Network\\nQuality Checks)];\n    %% RAW Network QA\n    NetworkQAData--&gt;NetworkQualityWarnings--&gt;NetworkWarnings[(Network\\nQuality Warnings)];\n    NetworkQualityWarnings--&gt;NetworkReports{{Network\\nQA \\ngraph data\\ncsv}};\n    %% Signal Strength\n    InspireGridData--&gt;SignalStrengthModeling;\n    PhysicalNetworkData--&gt;SignalStrengthModeling--&gt;SignalStrengthData[(Signal Strength)];\n    %% Cell Footprint\n    SignalStrengthData--&gt;CellFootprintEstimation--&gt;CellFootprintData[(Cell Footprint\\nValues)];\n    CellFootprintEstimation--&gt;CellIntersectionGroupsData[(Cell Intersection Groups)];\n    %% Cell Connection Probability\n    CellFootprintData--&gt;CellConnectionProbabilityEstimation;\n    InspireGridData--&gt;CellConnectionProbabilityEstimation--&gt;CellConnectionProbabilityData[(Cell Connection\\nProbability)];\n    %% -- EVENTS --\n    %% RAW Events cleaning\n    EventsRAWData[(MNO-Event\\nRAW)]--&gt;EventCleaning--&gt;EventsData[(MNO-Event)];\n    EventCleaning--&gt;EventsQA[(MNO-Event\\nQuality Checks)]--&gt;EventQualityWarnings;\n    EventCleaning--&gt;EventsQAfreq[(MNO-Event\\nQuality Checks\\nfrequency)];\n    %% RAW Events Warnings\n    EventsQAfreq--&gt;EventQualityWarnings;\n    EventQualityWarnings--&gt;EventsWarnings[(Events\\nQuality Warnings)];\n    EventQualityWarnings--&gt;EventsReports{{Event QA \\ngraph data\\ncsv}};\n    %% Events deduplication\n    EventsData--&gt;EventDeduplication--&gt;EventsDeduplicated[(MNO-Event\\nDeduplicated)];\n    EventDeduplication--&gt;EventsDeduplicatedQA[(MNO-Event\\nDeduplicated\\nQuality Checks)];\n    EventDeduplication--&gt;EventsDeduplicatedQAfreq[(MNO-Event\\nDeduplicated\\nQuality Checks\\nfrequency)];\n    %% Events warnings\n    EventsDeduplicatedQA--&gt;EventQualityWarnings2;\n    EventsDeduplicatedQAfreq--&gt;EventQualityWarnings2--&gt;EventsDeduplicatedWarnings[(Events\\nDeduplicated\\nQuality Warnings)];\n    EventQualityWarnings2--&gt;EventsDeduplicatedReports{{Event Deduplicated\\nQA \\ngraph data\\ncsv}};\n    %% --- Combination ---\n    %% Event Semantic Checks\n    EventsDeduplicated--&gt;SemanticCleaning--&gt;EventsSemanticCleaned[(Events\\nSemantic\\nCleaned)];\n    PhysicalNetworkData--&gt;SemanticCleaning;\n    SemanticCleaning--&gt;DeviceSemanticQualityMetrics[(Device\\nSemantic\\nQuality\\nMetrics)];\n    %% Event Semantic Warnings\n    EventsSemanticCleaned--&gt;SemanticQualityWarnings--&gt;EventSemanticWarnings[(Event\\nSemantic\\nQuality\\nWarnings)];\n    DeviceSemanticQualityMetrics--&gt;SemanticQualityWarnings--&gt;EventSemanticReports{{Event Semantic QA \\ngraph data\\ncsv}};\n    %% Device activity Statistics\n    EventsSemanticCleaned--&gt;DeviceActivityStatistics--&gt;DeviceActivityStatisticsData[(Device\\nActivity\\nStatistics)];\n    PhysicalNetworkData--&gt;DeviceActivityStatistics;\n\n    %% --- Daily Processing module ---\n    %% Daily Permanence Score\n    EventsSemanticCleaned--&gt;DailyPermanenceScore--&gt;DPSdata[(Daily\\nPerformance\\nScore\\nData)];\n    CellFootprintData--&gt;DailyPermanenceScore;\n    %% Continuous Time segmentation\n    EventsSemanticCleaned--&gt;ContinuousTimeSegmentation--&gt;DailyCTSdata[(Daily\\nContinuous\\nTime\\nSegmentation)];\n    CellFootprintData--&gt;ContinuousTimeSegmentation;\n    CellIntersectionGroupsData--&gt;ContinuousTimeSegmentation;\n\n    classDef green fill:#229954,stroke:#333,stroke-width:2px;\n    classDef light_green fill:#AFE1AF,stroke:#333,stroke-width:1px;\n    classDef bronze fill:#CD7F32,stroke:#333,stroke-width:2px;\n    classDef silver fill:#adadad,stroke:#333,stroke-width:2px;\n    classDef light_silver fill:#dcdcdc,stroke:#333,stroke-width:2px;\n    classDef gold fill:#FFD700,stroke:#333,stroke-width:2px;\n\n    %% --- Reference Data ---\n    class InspireGridData light_silver\n    %% --- MNO Data ---\n    %% -- NETWORK --\n    class PhysicalNetworkRAWData bronze\n    class PhysicalNetworkData light_silver\n    class NetworkQAData,NetworkWarnings silver\n    class NetworkReports gold\n    class SignalStrengthData,CellFootprintData,CellConnectionProbabilityData,CellIntersectionGroupsData light_silver\n    %% -- EVENTS --\n    %% event cleaning\n    class EventsRAWData bronze\n    class EventsData light_silver\n    class EventsQA,EventsQAfreq,EventsWarnings silver\n    class EventsReports gold\n    %% event deduplicated\n    class EventsDeduplicated light_silver\n    class EventsDeduplicatedQA,EventsDeduplicatedQAfreq,EventsDeduplicatedWarnings silver\n    class EventsDeduplicatedReports gold\n    %% device activity statistics\n    class DeviceActivityStatisticsData light_silver\n    %% events semantic clean\n    class EventsSemanticCleaned light_silver\n    class DeviceSemanticQualityMetrics,EventSemanticWarnings silver\n    class EventSemanticReports gold\n    %% --- Daily Processing module ---\n    %% Daily Permanence Score\n    class DPSdata light_silver\n    %% Continuous Time segmentation\n    class DailyCTSdata light_silver\n\n    %% ---- Components ----\n    class InspireGridGeneration light_green\n    %% Net\n    class NetworkCleaning,SignalStrengthModeling,CellFootprintEstimation,CellConnectionProbabilityEstimation light_green\n    class NetworkQualityWarnings green\n    %% Events\n    class EventCleaning,EventDeduplication,SemanticCleaning light_green\n    class EventQualityWarnings,EventQualityWarnings2,SemanticQualityWarnings green\n    %% -&gt; Device Activity Statistics should start from semantic cleaned events\n    class DeviceActivityStatistics light_green\n    %% Daily\n    class DailyPermanenceScore,ContinuousTimeSegmentation light_green</code></pre>"},{"location":"system_requirements/","title":"System Requirements","text":"<p>Multimno is a python library which requires the installation of additional system &amp; python libraries.</p> <p>In this section the requirements for executing this software are defined. In the case of using docker for execution, the system only needs to comply with the Host and Docker requirements as the docker image will have all the software requirements.</p>"},{"location":"system_requirements/#host-requirements","title":"Host requirements","text":"<ul> <li>Cores: 4</li> <li>RAM: 16 Gb</li> <li>Disk: 32 Gb of free space</li> <li>OS: <ul> <li>Ubuntu 22.04 (Recommended)</li> <li>Mac 12.6</li> <li>Windows 11 + WSL2 with Ubuntu 22.04 </li> </ul> </li> </ul>"},{"location":"system_requirements/#docker-requirements","title":"Docker requirements","text":"<p>In the case of using the docker image provided the following requirements must be fulfilled:</p> <ul> <li>Docker-engine: 25.0.X</li> <li>Docker-compose: 2.24.X</li> <li>Internet connection to Ubuntu/Spark/Docker official repositories for building the docker image</li> </ul>"},{"location":"system_requirements/#software-requirements","title":"Software Requirements","text":"<p>In the case of setting up a system for launching the software, the following dependencies have to be installed.</p>"},{"location":"system_requirements/#os-libraries","title":"OS Libraries","text":"Library Version Python &gt; 3.10.8 Java JDK 17.0.9 Apache Spark 3.5.1 GDAL 3.6.2"},{"location":"system_requirements/#spark-libraries","title":"Spark Libraries","text":"Library Version Apache Sedona 1.5.1 Geotools wrapper 28.2"},{"location":"system_requirements/#python-libraries","title":"Python Libraries","text":"Library Version numpy 1.26.2 pandas 2.1.4 geopandas 0.11.1 shapely 1.8.4 pyarrow 14.0.1 requests 2.31.0 py4j 0.10.9.7 pydeck 0.8.0"},{"location":"DevGuide/","title":"Developer Guide","text":"<p>The developer guide contains two sections: - Contribute: Guidelines on how to contribute to the software. - Developer Guidelines: Guidelines and tips on how to develop and test the software.  </p>"},{"location":"DevGuide/1_contribute/","title":"Contribute","text":"<p>In this document the general rules and guidelines for contributing to the multimno repository are detailed.</p>"},{"location":"DevGuide/1_contribute/#source-control-strategy","title":"Source control strategy","text":"<p>This repository uses three principal branches for source control:</p> <ul> <li>main: Branch where the official releases are tagged. The HEAD of the branch corresponds to the    latest release of the software.  </li> <li>integration: Branch used for preproduction testing and validation from which a release to the main branch will be generated.</li> <li>development: Branch that centralizes the latest features developed in the repository. After enough features/bugs have been delivered to this branch, a snapshot will be created in the integration branch for testing before  generating a release.</li> </ul> <p>These three branches shall only accept changes by the repository administrator. Commits shall not be performed directly in these branches except for small hotfixes in the integration branch.</p> <p>All features and bug fixes will be developed in branches that origin from the development branch.</p>"},{"location":"DevGuide/1_contribute/#forking-the-repository","title":"Forking the repository","text":"<p>Developers that want to contribute to the multimno repository shall fork the repository with all its branches. This can  be done through the github website. After creating a fork of the repository, developers can clone the forked repo in  their computers.</p>"},{"location":"DevGuide/1_contribute/#create-an-issue-with-the-development-that-will-be-performed","title":"Create an Issue with the development that will be performed","text":"<p>First of all, Check if the issue you will develop already exists.  Then, create an issue in the multimno repository stating the objective of the development that will be performed. Templates for creating issues for features or fixes are provided in the repository.</p>"},{"location":"DevGuide/1_contribute/#creating-a-featurefix-branch","title":"Creating a feature/fix branch","text":"<p>Within the forked repository developers shall create a branch that originates from the development branch.  This branch shall have the following naming convention:</p> <ul> <li>feat_\\&lt;name&gt;: If it is a new feature.</li> <li>fix_\\&lt;name&gt;: If it is a bug solution.</li> </ul> <p>Please remember to keep the forked development branch up-to-date with the latest changes.</p> <p>Don't forget to look up the developer guide to check the code style, testing and development practices that shall be followed to develop new code for the multimno repository.</p>"},{"location":"DevGuide/1_contribute/#opening-a-pull-request","title":"Opening a pull request","text":"<p>Before opening a pull request in the multimno repository verify: * Latest development changes are merged into the branch that performs the pull request.  * All tests pass successfully. * All the new code is documented following the Google style docstrings. * New tests for the code developed are included and pass successfully.</p> <p>Use the github web to create a pull request. The pull request must deliver your developed branch to the development branch of the multimno repository. Associate the PR(pull request) to the previously created issue.</p>"},{"location":"DevGuide/1_contribute/#the-review-process-pull-request-closure","title":"The review process &amp; pull request closure","text":"<p>The repository administrators will review the pull request performed to the development branch. </p> <ul> <li>If the changes are accepted, they will be incorporated in the development branch and the pull request will be closed. </li> <li>If the changes are not accepted, the repository administrators may indicate as a comment in the pull request feedback  and modifications needed to accept the pull request. However, the pull request may be desestimated to which it  will be closed and changes will not be incorporated.</li> </ul>"},{"location":"DevGuide/2_dev_guidelines/","title":"Developer Guidelines","text":"<p>The repository contains a devcontainer configuration compatible with VsCode. This configuration will create a docker container with all the necessary libraries and configurations to develop and execute the source code. </p>"},{"location":"DevGuide/2_dev_guidelines/#development-environment-setup","title":"\ud83d\udee0\ufe0f Development Environment Setup","text":"<p>This repository provides a docker dev-container with a system completely configured to execute the source code as well as useful libraries and tools for the development of the multimno software.  Thanks to the dev-container, it is guaranteed that all developers are developing/executing code in the same environment.</p> <p>The dev-container specification is all stored inside the <code>.devcontainer</code> directory.</p>"},{"location":"DevGuide/2_dev_guidelines/#configuring-the-docker-container","title":"Configuring the docker container","text":"<p>Configuration parameters for building the docker image and for creating the docker container are specified in the configuration file <code>.devcontainer/.env</code> file. This file contains user specific container configuration (like the path in the host machine to the data). As this file will change  for each developer it is ignored for the git version control and must be created after cloning the repository.</p> <p>A <code>template file</code> is stored in this repostiory. You can use this file as a baseline copying it to the <code>.devcontainer</code> directory.</p> <pre><code>cp resources/templates/dev_container_template.env .devcontainer/.env\n</code></pre> <p>Please edit the <code>.devcontainer/.env</code> file <code>Docker run parameters</code> section  with your preferences:</p> <pre><code># ------------------- Docker Build parameters -------------------\nPYTHON_VERSION=3.11 # Python version.\nJDK_VERSION=17 # Java version.\nSPARK_VERSION=3.4.1 # Spark/Pyspark version.\nSCALA_VERSION=2.12 # Spark dependency.\nSEDONA_VERSION=1.5.1 # Sedona\nGEOTOOLS_WRAPPER_VERSION=28.2 # Sedona dependency\n\n# ------------------- Docker run parameters -------------------\nCONTAINER_NAME=multimno_dev_container # Container name.\nDATA_DIR=../sample_data # Path of the host machine to the data to be used within the container.\nSPARK_LOGS_DIR=../sample_data/logs # Path of the host machine to where the spark logs will be stored.\nJL_PORT=8888 # Port of the host machine to deploy a jupyterlab.\nJL_CPU=4 # CPU cores of the container.\nJL_MEM=16g # RAM of the container.\n</code></pre>"},{"location":"DevGuide/2_dev_guidelines/#starting-the-dev-environment","title":"Starting the dev environment","text":""},{"location":"DevGuide/2_dev_guidelines/#vscode","title":"VsCode","text":"<p>Prerequisite: Dev-Container/Docker extension</p> <p>In VsCode: F1 -&gt; Dev Containers: Rebuild and Reopen in container</p>"},{"location":"DevGuide/2_dev_guidelines/#manual","title":"Manual","text":""},{"location":"DevGuide/2_dev_guidelines/#docker-image-creation","title":"Docker image creation","text":"<p>Execute the following command: <pre><code>docker compose -f .devcontainer/docker-compose.yml --env-file=.devcontainer/.env build\n</code></pre></p>"},{"location":"DevGuide/2_dev_guidelines/#docker-container-creation","title":"Docker container creation","text":"<p>Create a container and start a shell session in it with the commands: <pre><code>docker compose -f .devcontainer/docker-compose.yml --env-file=.devcontainer/.env up -d\ndocker exec -it multimno_dev_container bash\n</code></pre></p>"},{"location":"DevGuide/2_dev_guidelines/#stopping-the-dev-environment","title":"Stopping the dev environment","text":""},{"location":"DevGuide/2_dev_guidelines/#vscode_1","title":"VsCode","text":"<p>Closing VsCode will automatically stop the devcontainer.</p>"},{"location":"DevGuide/2_dev_guidelines/#manual_1","title":"Manual","text":"<p>Exit the terminal with:</p> <p>Ctrl+D or writing <code>exit</code></p>"},{"location":"DevGuide/2_dev_guidelines/#deleting-the-dev-environment","title":"Deleting the dev environment","text":"<p>Delete the container created with: <pre><code>docker compose -f .devcontainer/docker-compose.yml --env-file=.devcontainer/.env down\n</code></pre></p>"},{"location":"DevGuide/2_dev_guidelines/#execution","title":"\ud83d\udc0e Execution","text":""},{"location":"DevGuide/2_dev_guidelines/#launching-a-single-component","title":"Launching a single component","text":"<p>In a terminal execute the command:  </p> <pre><code>spark-submit multimno/main.py &lt;component_id&gt; &lt;path_to_general_config&gt; &lt;path_to_component_config&gt; \n</code></pre> <p>Example:  </p> <pre><code>spark-submit multimno/main.py SyntheticEvents pipe_configs/configurations/general_config.ini pipe_configs/configurations/synthetic_events/synth_config.ini \n</code></pre>"},{"location":"DevGuide/2_dev_guidelines/#launching-a-pipeline","title":"Launching a pipeline","text":"<p>In a terminal execute the command:  </p> <pre><code>python multimno/orchestrator.py &lt;pipeline_json_path&gt;\n</code></pre> <p>Example:  </p> <pre><code>python multimno/orchestrator.py pipe_configs/pipelines/pipeline.json \n</code></pre>"},{"location":"DevGuide/2_dev_guidelines/#monitoringdebug","title":"\ud83d\udd0d Monitoring/Debug","text":""},{"location":"DevGuide/2_dev_guidelines/#launching-a-spark-history-server","title":"Launching a spark history server","text":"<p>The history server will access SparkUI logs stored at the path ${SPARK_LOGS_DIR} defined in the <code>.devcontainer/.env</code> file.</p> <p>Starting the history server <pre><code>start-history-server.sh \n</code></pre> Accesing the history server * Go to the address http://localhost:18080</p>"},{"location":"DevGuide/2_dev_guidelines/#style","title":"\ud83e\udeb6 Style","text":""},{"location":"DevGuide/2_dev_guidelines/#coding-style","title":"Coding style","text":"<p>The code shall follow the standard PEP 8 which is the coding style proposed for writing clean, readable, and maintainable Python code.  It was created to promote consistency in Python code and make it easier for developers to collaborate on projects. </p> <p>PEP 8 official guide</p>"},{"location":"DevGuide/2_dev_guidelines/#docstring-style","title":"Docstring style","text":"<p>The docstrings written in the code shall follow the Google Docstrings style. Adhering  to a unique docstring style  guarantees consistency within software development in a project. Google Docstrings are the most popular convention for  docstrings which facilitates readability and collaboration in open-source projects. </p> <p>Google Docstrings official guide</p>"},{"location":"DevGuide/2_dev_guidelines/#code-cleaning","title":"\ud83e\uddfc Code cleaning","text":"<p>Developed code shall be formatted and jupyter notebooks shall be cleaned of outputs to guarantee consistency and reduce  unnecessary differences between commits.</p>"},{"location":"DevGuide/2_dev_guidelines/#code-linting","title":"Code Linting","text":"<p>The python code generated shall be formatted with black. For formatting all source code execute the following command:</p> <pre><code>black -l 120 multimno tests/test_code/\n</code></pre>"},{"location":"DevGuide/2_dev_guidelines/#clean-jupyter-notebooks","title":"Clean jupyter notebooks","text":"<pre><code>find ./notebooks/ -type f -name \\*.ipynb | xargs jupyter nbconvert --clear-output --inplace\n</code></pre>"},{"location":"DevGuide/2_dev_guidelines/#code-security-scan","title":"Code Security Scan","text":"<pre><code>bandit -r multimno\n</code></pre>"},{"location":"DevGuide/2_dev_guidelines/#testing","title":"\ud83e\uddea Testing","text":""},{"location":"DevGuide/2_dev_guidelines/#launch-tests","title":"Launch Tests","text":""},{"location":"DevGuide/2_dev_guidelines/#manual_2","title":"Manual","text":"<pre><code>pytest tests/test_code/multimno\n</code></pre>"},{"location":"DevGuide/2_dev_guidelines/#vscode_2","title":"VsCode","text":"<p>1) Open the test view at the left panel.  2) Launch tests.</p>"},{"location":"DevGuide/2_dev_guidelines/#generate-test-coverage-reports","title":"Generate test &amp; coverage reports","text":"<p>Launch the command</p> <pre><code>pytest --cov-config=tests/.coveragerc \\\n    --cov-report=\"html:docs/autodoc/coverage\" \\\n    --cov=multimno --html=docs/autodoc/test_report.md \\\n    --self-contained-html tests/test_code/multimno\n</code></pre> <p>Test reports will be stored in the dir: <code>docs/autodoc</code></p>"},{"location":"DevGuide/2_dev_guidelines/#see-coverage-in-ide-vscode-extension","title":"See coverage in IDE (VsCode extension)","text":"<p>1) Launch tests with coverage to generate the coverage report (xml) <pre><code>pytest --cov-report=\"xml\" --cov=multimno tests/test_code/multimno\n</code></pre> 1) Install the extension: Coverage Gutters 2) Right click and select Coverage Gutters: Watch</p> <p>Note: You can see the coverage percentage at the bottom bar</p>"},{"location":"DevGuide/2_dev_guidelines/#code-documentation","title":"\ud83d\udcc4 Code Documentation","text":""},{"location":"DevGuide/2_dev_guidelines/#documentation-server-debugmkdocs","title":"Documentation server Debug(Mkdocs)","text":"<p>A code documentation can be deployed using mkdocs backend. </p> <p>1) Create documentation (This will launch all tests) <pre><code>./resources/scripts/generate_docs.sh\n</code></pre> 2) Launch doc server</p> <p><pre><code>mkdocs serve\n</code></pre> and navigate to the address: http://127.0.0.1:8000</p>"},{"location":"DevGuide/2_dev_guidelines/#documentation-deploy-mike","title":"Documentation deploy (mike)","text":"<p>Set <code>latest</code> as default version <pre><code>mike set-default --push latest\n</code></pre></p> <p>Deploy a version of the documentation with:</p> <pre><code>mike deploy --push --update-aliases &lt;version&gt; latest\n</code></pre> <p>Example:</p> <pre><code>mike deploy --push --update-aliases 0.2 latest\n</code></pre>"},{"location":"UserManual/","title":"User Manual","text":"<p>This document presents the user manual of the multimno software. Three sections are included: - Configuration: Section explaining the configuration values for all the components of the pipeline. - Setup: Section explaining how to install and deploy the software. - Execution: Section explaining how to execute the software.  </p>"},{"location":"UserManual/execution/","title":"Execution","text":"<p>The multimno software is a python application that launches a single component with a given configuration.  This atomic design allows the application to be integrated with multiple orchestration software. At the moment a python script called <code>orchestrator_multimno.py</code> is provided which will execute a pipeline of components sequentially using <code>spark-submit</code> commands.</p>"},{"location":"UserManual/execution/#single-component-execution","title":"Single component execution","text":"<p>The entrypoint of the application is a main.py which receives the following positional parameters: - component_id: Id of the component that will be launched. - general_config_path: Path to the general configuration file of the application. - component_config_path: Path to the component configuration file.</p> <pre><code>multimno/main.py &lt;component_id&gt; &lt;general_config_path&gt; &lt;component_config_path&gt;\n</code></pre> <p>Example:</p> <pre><code>multimno/main.py InspireGridGeneration pipe_configs/configurations/general_config.ini pipe_configs/configurations/grid/grid_generation.ini\n</code></pre>"},{"location":"UserManual/execution/#pipeline-execution","title":"Pipeline execution","text":"<p>For executing a pipeline the <code>orchestrator_multimno.py</code> script shall be used which takes a path to a json file as its only parameter. This json file, shall be defined as a file that glues all the configuration files  and defines the execution order of the components. The structure is as follows:</p> <ul> <li>general_config_path: Path to the general configuration file</li> <li>spark_submit_args: List containing arguments that will be passed to the spark-submit command. It can be empty.</li> <li>pipeline: List containing the order in which the components will be executed. Each item is composed of the values:<ul> <li>component_id: Id of the component to be executed.</li> <li>component_config_path: Path to the component configuration file.</li> </ul> </li> </ul> <p>Example: <pre><code>{\n    \"general_config_path\": \"/opt/dev/pipe_configs/configurations/general_config.ini\",\n    \"spark_submit_args\": [\n        \"--master=spark://spark-master:7077\",\n        \"--packages=org.apache.sedona:sedona-spark-3.5_2.12:1.5.1,org.datasyslab:geotools-wrapper:1.5.1-28.2\"\n    ],\n    \"pipeline\": [\n        {\n            \"component_id\": \"SyntheticEvents\",\n            \"component_config_path\": \"/opt/dev/pipe_configs/configurations/synthetic_events/synth_config.ini\"\n        },\n        {\n            \"component_id\": \"EventCleaning\",\n            \"component_config_path\": \"/opt/dev/pipe_configs/configurations/event/event_cleaning.ini\"\n        }\n    ]\n}\n</code></pre></p> <p>Configuration for executing a demo pipeline is given in the file: <code>pipe_configs/pipelines/pipeline.json</code> This file contains the order of the execution of the pipeline components and references to its configuration files.</p> <pre><code>./orchestrator_multimno.py pipe_configs/pipelines/pipeline.json\n</code></pre> <p>This demo will process MNO Event &amp; Network data cleaning it. At the same time it will generate quality metrics in both of these processes. Then, it will process the cleaned data until the generation of the continuous time  segmentation and daily permanence score indicators.</p> <p>If using the docker setup, all data will be stored under the path <code>/opt/data/lakehouse</code>. </p>"},{"location":"UserManual/setup_guide/","title":"Setup Guide","text":"<p>There are two ways of setting up a system for executing the source code:   1) Building the docker image provided. (Recommended for local executions)   2) Installing and setting up all required system libraries.  </p>"},{"location":"UserManual/setup_guide/#docker-setup","title":"Docker setup","text":"<p>A Dockerfile is provided to build a docker image with all necessary dependencies for the code execution.</p>"},{"location":"UserManual/setup_guide/#installing-docker-software","title":"Installing docker software","text":"<p>To use the docker image it is necessary to have the docker engine installed. Please follow the official docker  guide to set it up in your system: -  Official guide: Click here</p>"},{"location":"UserManual/setup_guide/#docker-image-creation","title":"Docker image creation","text":"<p>Execute the following command: <pre><code>docker build -t multimno:1.0-prod --target=multimno-prod .\n</code></pre></p>"},{"location":"UserManual/setup_guide/#docker-container-creation","title":"Docker container creation","text":""},{"location":"UserManual/setup_guide/#run-an-example-pipeline-within-a-container","title":"Run an example pipeline within a container","text":"<pre><code>docker run --rm --name=multimno-container -v \"${PWD}/sample_data:/opt/data\" -v \"${PWD}/pipe_configs:/opt/app/pipe_configs\" multimno:1.0-prod pipe_configs/pipelines/pipeline.json\n</code></pre>"},{"location":"UserManual/setup_guide/#run-a-container-in-interactive-mode","title":"Run a container in interactive mode","text":"<pre><code>docker run -it --name=multimno-container -v \"${PWD}/sample_data:/opt/data\" -v \"${PWD}/pipe_configs:/opt/app/pipe_configs\" --entrypoint=bash multimno:1.0-prod \n</code></pre> <p>After performing this command your shell(command-line) will be inside the container and you can perform  the execution steps to try out the code.</p>"},{"location":"UserManual/setup_guide/#clean-up","title":"Clean up","text":"<p>Exit the terminal with:</p> <p>Ctrl+D or writing <code>exit</code></p> <p>Delete the container created with: <pre><code>docker rm multimno-container\n</code></pre></p> <p>Delete the docker image with: <pre><code>docker rmi multimno:1.0-prod\n</code></pre></p>"},{"location":"UserManual/setup_guide/#docker-lite-version","title":"Docker Lite version","text":"<p>As the multimno software is a python application designed to be executed in a Spark cluster, a lightweight Dockerfile called <code>Dockerfile-lite</code> is given for execution of the software in existing Spark clusters.</p> <p>This docker image contains only minimum requirements to launch the application against an existing spark cluster. The image is a ubuntu:22.04 with python 3.10, jdk 17 and the required python dependencies.</p>"},{"location":"UserManual/setup_guide/#build-lite-image","title":"Build lite image","text":"<p>Execute the following command: <pre><code>docker build -t multimno_lite:1.0 -f ./Dockerfile-lite .\n</code></pre></p>"},{"location":"UserManual/setup_guide/#create-lite-container","title":"Create lite container","text":"<pre><code>docker run -it --name=multimno-lite-container -v \"${PWD}/pipe_configs:/opt/app/pipe_configs\" multimno_lite:1.0 bash\n</code></pre>"},{"location":"UserManual/setup_guide/#configuration","title":"Configuration","text":"<p>spark-submit args</p> <p>As explained in the execution guide, the entrypoint for the pipeline execution: <code>orchestrator_multimno.py</code>, performs <code>spark-submit</code> commands. To define <code>spark-submit</code> arguments edit the <code>spark_submit_args</code> variable in the pipeline.json.</p> <ul> <li>Spark submit documentation: https://spark.apache.org/docs/latest/submitting-applications.html</li> </ul> <p>Spark Configuration</p> <p>Edit the <code>[Spark]</code> section in the general_configuration file to define Spark session configuration parameters.</p> <ul> <li>Spark configuration documentation: https://spark.apache.org/docs/latest/configuration.html</li> </ul> <p>Python Version A requirement of a pyspark application is that the python version must be alligned for all the cluster. As the Dockerfile-Lite uses python3.10 the Spark cluster must have this python version alligned.</p> <ul> <li>Python package management: https://spark.apache.org/docs/latest/api/python/user_guide/python_packaging.html</li> </ul> <p>Python dependencies The application uses Apache Sedona and so it will need the Sedona jars as well as python dependencies installed in the Spark cluster. Please refer to the Apache Sedona official documentation: </p> <ul> <li> <p>Python setup: https://sedona.apache.org/1.5.1/setup/install-python/</p> </li> <li> <p>Spark Cluster: https://sedona.apache.org/1.5.1/setup/cluster/</p> </li> </ul> <p>Lite Configuration example An example of configuration of an execution with the lite image is given in the files: <code>pipe_configs/pipelines/test_production.json</code> and <code>pipe_configs/configurations/general_config_production.ini</code></p>"},{"location":"UserManual/setup_guide/#execution-lite","title":"Execution lite","text":"<p>Execute as defined in the execution guide.</p>"},{"location":"UserManual/setup_guide/#software-setup","title":"Software setup","text":"<p>The software is aimed to be executed in a Linux OS. It is recommended to use Ubuntu 22.04 LTS but these steps should also work in MAC OS 12.6(or superior) and in Windows 11 with WSL2 and setting up as the distro of WSL Ubuntu 22.04.</p>"},{"location":"UserManual/setup_guide/#install-system-libs","title":"Install system libs","text":"<pre><code>sudo apt-get update &amp;&amp; \\\n    sudo apt-get install -y --no-install-recommends \\\n      sudo \\\n      openjdk-17-jdk \\\n      build-essential \\\n      software-properties-common \\\n      openssh-client openssh-server \\\n      gdal-bin \\\n      libgdal-dev \\\n      ssh\n</code></pre>"},{"location":"UserManual/setup_guide/#download-spark-source-code","title":"Download Spark source code","text":"<pre><code>SPARK_VERSION=3.5.1\nexport SPARK_HOME=${SPARK_HOME:-\"/opt/spark\"}\nmkdir -p ${SPARK_HOME}\ncd ${SPARK_HOME}\nwget https://dlcdn.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.tgz \\\n  &amp;&amp; tar -xvzf spark-${SPARK_VERSION}-bin-hadoop3.tgz --directory /opt/spark --strip-components 1 \\\n  &amp;&amp; rm -rf spark-${SPARK_VERSION}-bin-hadoop3.tgz\nexport PATH=\"${PATH}:$SPARK_HOME/bin:$SPARK_HOME/sbin\"\n</code></pre>"},{"location":"UserManual/setup_guide/#install-python-requirements","title":"Install python requirements","text":"<pre><code>pip install --upgrade pip\npip install -r resources/requirements/requirements.in\npip install -r resources/requirements/dev_requirements.in\n</code></pre> <p>You can use a virtualenv for avoiding conflicts with other python libraries.</p>"},{"location":"UserManual/setup_guide/#install-spark-dependencies","title":"Install Spark dependencies","text":"<pre><code>SCALA_VERSION=2.12\nSEDONA_VERSION=1.5.1\nGEOTOOLS_WRAPPER_VERSION=28.2\nchmod +x ./resources/scripts/install_sedona_jars.sh\n./resources/scripts/install_sedona_jars.sh ${SPARK_VERSION} ${SCALA_VERSION} ${SEDONA_VERSION} ${GEOTOOLS_WRAPPER_VERSION} \n</code></pre>"},{"location":"UserManual/setup_guide/#setup-spark-configuration","title":"Setup spark configuration","text":"<pre><code>cp conf/spark-defaults.conf \"$SPARK_HOME/conf/spark-defaults.conf\"\ncp conf/log4j2.properties \"$SPARK_HOME/conf/log4j2.properties\"\n</code></pre>"},{"location":"UserManual/configuration/","title":"Configuration","text":"<p>The multimno application requires from multiple configuration files.  </p> <ul> <li>One general configuration file describing general parameters like file paths, logging, spark and  common values for all components in the pipeline.  </li> <li>A configuration file per each component in the pipeline with configuration parameters exclusive of the component. Values defined in these files can override values defined in the general configuration file.</li> </ul>"},{"location":"UserManual/configuration/0_general_configuration/","title":"General Configuration","text":"<p>The general configuration file contains transversal settings for all the pipeline. It is an INI file composed of four main sections:</p> <p>Logging: Section which contains the logger settings.</p> <p>Paths: Section containing the definition of all paths to be used.</p> <p>Spark: Apache Spark configuration values. Parameters defined in this section will be used to create the spark session. Values supported are based in: Configuration - Spark 3.5.1 Documentation (apache.org). As an exception the parameter session_name has been included which identifies the name of the spark session.</p> <p>General: Parameters that may be useful for all components in the pipeline. </p> <p>Example:</p> <pre><code>[Logging]\nlevel = INFO\nformat= %(asctime)-20s %(message)s\ndatefmt = %y-%m-%d %H:%M:%S\n\n[Paths]\n# Main paths\nhome_dir = /opt/data\nlakehouse_dir = ${Paths:home_dir}/lakehouse\n# Lakehouse\nlanding_dir = ${Paths:lakehouse_dir}/landing\nbronze_dir = ${Paths:lakehouse_dir}/bronze\nsilver_dir = ${Paths:lakehouse_dir}/silver\ngold_dir = ${Paths:lakehouse_dir}/gold\n\n[Spark]\n# parameters in this section are passed to Spark except session_name \nsession_name = MultiMNO\nspark.master = local[*]\nspark.driver.host = localhost\n\n[General]\nstart_date = 2023-01-01\nend_date = 2023-01-30\n</code></pre>"},{"location":"UserManual/configuration/10_continuous_time_segmentation/","title":"ContinuousTimeSegmentation Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>time_segments.ini</code>. In <code>general_config.ini</code> to execute the component specify all paths to its four corresponding data objects (input + output). Example: </p> <pre><code>[Paths.Silver]\nevent_data_silver_flagged = ${Paths:silver_dir}/mno_events_flagged\ncell_intersection_groups_data_silver = ${Paths:silver_dir}/cell_intersection_groups\ntime_segments_silver = ${Paths:silver_dir}/time_segments\n</code></pre> <p>In time_segments.ini parameters are as follows: </p> <ul> <li> <p>data_period_start - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-01), the date from which start Event Cleaning</p> </li> <li> <p>data_period_end - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-05), the date till which perform Event Cleaning</p> </li> <li> <p>is_first_run - boolean, if True, the component won't use previously calculated time segments. If False, the component will use last calculated time segment per device.</p> </li> <li> <p>event_error_flags_to_include - list of integers, the list of error flags that should be included in the time segments processing. Default value is [0], so only events with no errors are included.</p> </li> <li> <p>min_time_stay_s - integer, the minimum dwell time in seconds for a time segments to be considered as a \"stay\". Default value is 15 minutes.</p> </li> <li> <p>max_time_missing_stay_s - integer, maximum time difference between events to be considered a \u201cstay\u201d. If larger, the time segment will be marked \u201cunknown\u201d. Default value is 12 hours to support devices being offline at home or work addresses.</p> </li> <li> <p>max_time_missing_move_s - integer, maximum time difference between events to be considered a \u201cmove\u201d. If larger, the time segment will be marked \u201cunknown\u201d. Default value is 2 hours.</p> </li> <li> <p>pad_time_s - integer, half the size of an isolated time segment: between two \u201cunknowns\u201d time segments. It expands the isolated event in time, by \u201cpadding\u201d from the \u201cunknown\u201d time segments on both sides. Default value is 5 minutes.</p> </li> </ul>"},{"location":"UserManual/configuration/10_continuous_time_segmentation/#configuration-example","title":"Configuration example","text":"<pre><code>[ContinuousTimeSegmentation]\ndata_period_start = 2023-01-01\ndata_period_end = 2023-01-05\n\nis_first_run = true\nevent_error_flags_to_include = [0]\n\nmin_time_stay_s = 900\nmax_time_missing_stay_s = 43200\nmax_time_missing_move_s = 7200\npad_time_s = 300\n</code></pre>"},{"location":"UserManual/configuration/11_daily_permanence_score/","title":"DailyPermanenceScore Configuration","text":"<p>To initialise and run the component two configs are used -  <code>general_config.ini</code> and component\u2019s config <code>daily_permanence_score.ini</code>. In  <code>general_config.ini</code> all paths to the data objects used by DailyPermanenceScore component shall be specified. An example with the specified paths is shown below:</p> <pre><code>[Paths.Silver]\n...\nevent_data_silver_flagged = ${Paths:silver_dir}/mno_events_flagged\ncell_footprint_data_silver = ${Paths:silver_dir}/cell_footprint\ndaily_permanence_score_data_silver = ${Paths:silver_dir}/daily_permanence_score\n...\n</code></pre> <p>Below there is a description of the sub component\u2019s config  - <code>daily_permanence_score.ini</code>. </p> <p>Parameters are as follows:</p> <p>Under  <code>[DailyPermanenceScore]</code> config section: </p> <ul> <li> <p>data_period_start - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-08), the date from which start DailyPermanenceScore processing. Make sure events and cell footprint data are available for all the dates between data_period_start and data_period_end, as well as for the previous day to data_period_start and the posterior day to data_period_end. </p> </li> <li> <p>data_period_end - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-15), the date till which we will perform DailyPermanenceScore processing. Make sure events and cell footprint data are available for all the dates between data_period_start and data_period_end, as well as for the previous day to data_period_start and the posterior day to data_period_end. </p> </li> <li> <p>time_slot_number - integer, bigger than 0. Number of equal-length time slots in which to divide each date for the daily permanence score calculation. Recommended value: 24, so that the day is divided in 24 1-hour time slots.</p> </li> <li> <p>max_time_thresh - integer, in seconds. In case of 2 consecutive events taking place in different cells, if the time difference between the events is lower than this threshold, then the both the first event's assigned end time and the second event's assigned start time will be equal to the average between the first and second event's timestamps. If the time difference between the 2 consecutive events is higher than this threshold, then the assigned end time for the first event will be equal to the first event's timestamp plus half the value of this threshold; and the assigned start time for the second event will be equal to the second event's timestamp minus half the value of this threshold. For the case of 2 consecutive events taking place in different cells, if the time difference between the events is higher than the corresponding threshold (either <code>max_time_thresh_day</code> or <code>max_time_thresh_night</code>), then the event timestamps are also extended half this value of <code>max_time_thresh</code>. Recommended value: 900 seconds (15 minutes).</p> </li> <li> <p>max_time_thresh_day - integer, in seconds. In case of 2 consecutive events in the same cell, if the time of at least one of the events is included in the \"day time\", i.e. from 9:00 to 22:59, then <code>max_time_thresh_day</code> is the threshold to be applied. If the time difference between the events is lower than this threshold, then the both the first event's assigned end time and the second event's assigned start time will be equal to the average between the first and second event's timestamps. Otherwise, the assigned end time for the first event will be equal to the first event's timestamp plus half the value of <code>max_time_thresh</code>; and the assigned start time for the second event will be equal to the second event's timestamp minus half the value of <code>max_time_thresh</code>. Recommended value: 7200 seconds (2 hours).</p> </li> <li> <p>max_time_thresh_night - integer, in seconds. In case of 2 consecutive events in the same cell, if the time of both events is included in the \"night time\", i.e. from 23:00 to 8:59, then <code>max_time_thresh_night</code> is the threshold to be applied. If the time difference between the events is lower than this threshold, then the both the first event's assigned end time and the second event's assigned start time will be equal to the average between the first and second event's timestamps. Otherwise, the assigned end time for the first event will be equal to the first event's timestamp plus half the value of <code>max_time_thresh</code>; and the assigned start time for the second event will be equal to the second event's timestamp minus half the value of <code>max_time_thresh</code>. Recommended value: 28800 seconds (8 hours).</p> </li> <li> <p>max_vel_thresh - float, in metres per second (m/s). In order to evaluate if an event corresponds to a \"move\", the speed between the previous event and the next event is calculated. If the speed exceeds <code>max_vel_thresh</code>, then the event is tagged as a move and will be discarded for daily permanence score calculation. Recommended value: 13.889 m/s (50 km/h).</p> </li> <li> <p>score_interval - integer, bigger than 0. It indicates the number of integer values in which to discretise the stay duration in each tile and time slot for the daily permanence score calculation. E.g., if <code>score_interal</code> = 1, then only one value \"1\" will be assigned to all tiles in which a user has stayed during the time interval, regardless of the stay duration. If <code>score_interval</code> = 2, then 2 possible values will be assigned to each tile and time slot (\"1\" for cases in which the user has stayed for less than half the time slot in the corresponding tile, and \"2\" for cases in which the user has stayed for more than half the time slot in the corresponding tile). Recommended value: 4 (if <code>time_slot_number</code> = 24, then having <code>score_interval</code> = 4 would divide each hour in 15-min periods for the daily permanence score value calculation, allowing us to have dps=\"1\" if the user has stayed for between 0 and 15 min in the tile, dps=\"2\" for between 15 and 30 min, dps=\"3\" for between 30 and 45 min, and dps=\"4\" for between 45 and 60 min).</p> </li> <li> <p>event_error_flags_to_include - set of integers, e.g. \"{0}\". It indicates the values of the \"error_flag\" column of the input event data that will be kept. Rows with \"error_flag\" values not included in this set will be discarded and will not be considered for any step of the daily permanence score component. Recommended value: {0}.</p> </li> </ul>"},{"location":"UserManual/configuration/11_daily_permanence_score/#configuration-example","title":"Configuration example","text":"<pre><code>[Logging]\nlevel = DEBUG\n\n[Spark]\nsession_name = DailyPermanenceScore\n\n[DailyPermanenceScore]\ndata_period_start = 2023-01-02\ndata_period_end = 2023-01-02\n\ntime_slot_number = 24\n\nmax_time_thresh = 900  # 15 min\nmax_time_thresh_day = 7_200  # 2 h\nmax_time_thresh_night = 28_800  # 8 h\n\nmax_speed_thresh = 13.88888889  # 50 km/h\n\nscore_interval = 4\nevent_error_flags_to_include = {0}\n</code></pre>"},{"location":"UserManual/configuration/12_network_quality_warnings/","title":"NetworkQualityWarnings Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>network_quality_warnings.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Silver]\n...\nnetwork_syntactic_quality_metrics_by_column = ${Paths:silver_quality_metrics_dir}/network_syntactic_quality_metrics_by_column\nnetwork_syntactic_quality_warnings_log_table = ${Paths:silver_quality_warnings_dir}/network_syntactic_quality_warnings_log_table\nnetwork_syntactic_quality_warnings_line_plot_data = ${Paths:silver_quality_warnings_dir}/network_syntactic_quality_warnings_line_plot_data\nnetwork_syntactic_quality_warnings_pie_plot_data = ${Paths:silver_quality_warnings_dir}/network_syntactic_quality_warnings_pie_plot_data\n...\n</code></pre> <p>The expected parameters in <code>network_quality_warnings.ini</code> are as follows: - date_of_study: string, format should be the one specified <code>date_format</code> (e.g., <code>2023-01-01</code> for <code>%Y-%m-%d</code>), the first date for which data will be processed by the component. All dates between this one and the specified in <code>data_period_end</code> will be processed (both inclusive). - date_format: string, it indicates the format expected in <code>date_of_study</code>. For example, use <code>%Y-%m-%d</code> for the usual \"2023-01-01\" format separated by <code>-</code>. - lookback_period: string, it indicates the length of the lookback period used to compare the metrics of the date of study with past data volume and error rates. Three possible values are accepted: <code>week</code>, <code>month</code>, and <code>quarter</code>. - thresholds: dictionary, indicating the different thresholds to be used for raising warnings.</p>"},{"location":"UserManual/configuration/12_network_quality_warnings/#thresholds","title":"Thresholds","text":"<p>The thresholds parameter is a dictionary of dictionaries, used to indicate the different thresholds to be used for each type of warning. In the case that one of the thresholds is missing, a default value will be used instead. The default values are contained in <code>multimno.core.constants.network_default_thresholds.NETWORK_DEFAULT_THRESHOLDS</code>.</p> <p>Ihe dictionary structure is as follows: - <code>\"SIZE_RAW_DATA\"</code>: refers to the size of the input data.   - <code>\"OVER_AVERAGE\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the input raw file is greater than this threshold, in percentage different, when compared to the average over the lookback period. By default, the value is <code>30</code>.   - <code>\"UNDER_AVERAGE\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the input raw file is lower than this threshold, in percentage different, when compared to the average over the lookback period. By default, the value is <code>30</code>.   - <code>\"VARIABILITY\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the input raw file differs from its average over the lookback period by more than its standard deviation multiplied by this threshold. This is, this value is a threshold of the number of standard deviations than a value can differ from its average over the lookback period, either over or under the average. By default, the value is <code>2</code>.   - <code>\"ABS_VALUE_UPPER_LIMIT\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the input raw file is greater than this value. By default, it is equal to the threshold calculated from the <code>VARIABILITY</code> parameter over the average.   - <code>\"ABS_VALUE_LOWER_LIMIT\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the input raw file is greater than this value. By default, it is equal to the threshold calculated from the <code>VARIABILITY</code> parameter under the average.</p> <ul> <li><code>\"SIZE_CLEAN_DATA\"</code>: refers to the size of the output data.</li> <li><code>\"OVER_AVERAGE\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the output processed file is greater than this threshold, in percentage different, when compared to the average over the lookback period. By default, the value is <code>30</code>.</li> <li><code>\"UNDER_AVERAGE\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the output processed file is lower than this threshold, in percentage different, when compared to the average over the lookback period. By default, the value is <code>30</code>.</li> <li><code>\"VARIABILITY\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the output processed file differs from its average over the lookback period by more than its standard deviation multiplied by this threshold. This is, this value is a threshold of the number of standard deviations than a value can differ from its average over the lookback period, either over or under the average. By default, the value is <code>2</code>.</li> <li><code>\"ABS_VALUE_UPPER_LIMIT\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the output processed file is greater than this value. by default, it is equal to the threshold calculated from the <code>VARIABILITY</code> parameter over the average.</li> <li> <p><code>\"ABS_VALUE_LOWER_LIMIT\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the output processed file is greater than this value. By default, it is equal to the threshold calculated from the <code>VARIABILITY</code> parameter under the average.</p> </li> <li> <p><code>\"TOTAL_ERROR_RATE\"</code>: refers to the percentage of rows preserved from the input file, i.e., the rows that passed the cleaning/check procedure.</p> </li> <li><code>\"OVER_AVERAGE\"</code>: integer or float, indicates the threshold for a warning to be raised when the error rate is greater than this threshold, in percentage different, when compared to the average over the lookback period. By default, the value is <code>30</code>.</li> <li><code>\"VARIABILITY\"</code>: integer or float, indicates the threshold for a warning to be raised when the error rate differs from its average over the lookback period by more than its standard deviation multiplied by this threshold. This is, this value is a threshold of the number of standard deviations than a value can differ from its average over the lookback period, obly over the average. By default, the value is <code>2</code>.</li> <li> <p><code>\"ABS_VALUE_UPPER_LIMIT\"</code>: integer or float, indicates the threshold for a warning to be raised when the error rate is greater than this value. By default, the value is <code>20</code>.</p> </li> <li> <p><code>\"Missing_value_RATE\"</code>: refers to the percentage of missing/null values of a given field in the input data. The value of this dictionary key is a dictionary where the keys are field names, and the values are a dictionary containing the thresholds form this error type and field.</p> </li> <li>Admitted keys (i.e., fields) are: <code>cell_id</code>, <code>valid_date_start</code>, <code>latitude</code>, <code>longitude</code>, <code>altitude</code>, <code>antenna_height</code>, <code>directionality</code>, <code>azimuth_angle</code>, <code>elevation_angle</code>, <code>horizontal_beam_width</code>, <code>vertical_beam_width</code>, <code>power</code>, <code>frequency</code>, <code>technology</code>, and <code>cell_type</code>.</li> <li> <p>Each key (i.e., field) has the following thresholds:</p> <ul> <li><code>\"AVERAGE\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate is greater than this threshold, in percentage different, when compared to the average over the lookback period. By default, the value is <code>30</code> for <code>cell_id</code>, <code>latitude</code>, and <code>longitude</code>, and <code>60</code> otherwise.</li> <li><code>\"VARIABILITY\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate differs from its average over the lookback period by more than its standard deviation multiplied by this threshold. This is, this value is a threshold of the number of standard deviations than a value can differ from its average over the lookback period, obly over the average. By default, the value is <code>2</code> for <code>cell_id</code>, <code>latitude</code>, and <code>longitude</code>, and <code>3</code> otherwise.</li> <li><code>\"ABS_VALUE_UPPER_LIMIT\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate is greater than this value. By default, the value is <code>20</code> for <code>cell_id</code>, <code>latitude</code>, and <code>longitude</code>, and <code>50</code> otherwise.</li> </ul> </li> <li> <p><code>\"Out_of_range_RATE\"</code>: refers to the percentage of out of bounds, out of range or invalid values of a given field in the input data. The value of this dictionary key is a dictionary where the keys are field names, and the values are a dictionary containing the thresholds form this error type and field.</p> </li> <li>Admitted keys (i.e., fields) are: <code>\"cell_id\"</code>, <code>\"latitude\"</code>, <code>\"longitude\"</code>, <code>\"antenna_height\"</code>, <code>\"directionality\"</code>, <code>\"azimuth_angle\"</code>, <code>\"elevation_angle\"</code>, <code>\"horizontal_beam_width\"</code>, <code>\"vertical_beam_width\"</code>, <code>\"power\"</code>, <code>\"frequency\"</code>, <code>\"technology\"</code>, and <code>\"cell_type\"</code>. Exceptionally, the <code>None</code> value is also accepted, referring to the specific error where <code>valid_date_end</code> is a point int time earlier than <code>valid_date_start</code>.</li> <li> <p>Each key has the following thresholds:</p> <ul> <li><code>\"AVERAGE\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate is greater than this threshold, in percentage different, when compared to the average over the lookback period. By default, the value is <code>20</code> for <code>cell_id</code>, <code>latitude</code>, and <code>longitude</code>, and <code>60</code> otherwise.</li> <li><code>\"VARIABILITY\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate differs from its average over the lookback period by more than its standard deviation multiplied by this threshold. This is, this value is a threshold of the number of standard deviations than a value can differ from its average over the lookback period, obly over the average. By default, the value is <code>2</code> for <code>cell_id</code>, <code>latitude</code>, and <code>longitude</code>, and <code>3</code> otherwise.</li> <li><code>\"ABS_VALUE_UPPER_LIMIT\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate is greater than this value. By default, the value is <code>20</code> for <code>cell_id</code>, <code>latitude</code>, and <code>longitude</code>, and <code>50</code> otherwise.</li> </ul> </li> <li> <p><code>\"Parsing_error_RATE\"</code>: refers to values that could not be parsed.</p> </li> <li><code>\"valid_date_start\"</code>:<ul> <li><code>\"AVERAGE\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate is greater than this threshold, in percentage different, when compared to the average over the lookback period. By default, the value is <code>60</code>.</li> <li><code>\"VARIABILITY\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate differs from its average over the lookback period by more than its standard deviation multiplied by this threshold. This is, this value is a threshold of the number of standard deviations than a value can differ from its average over the lookback period, obly over the average. By default, the value is <code>3</code>.</li> <li><code>\"ABS_VALUE_UPPER_LIMIT\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate is greater than this value. By default, the value is <code>50</code>.</li> </ul> </li> </ul>"},{"location":"UserManual/configuration/12_network_quality_warnings/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = NetworkQualityWarnings\n\n[NetworkQualityWarnings]\ndate_format = %Y-%m-%d\ndate_of_study = 2023-01-08\nlookback_period = week\n\n# All values must be numeric\n# Missing parameter will take the default value\n# Incorrect value will throw an error\nthresholds = {\n    \"SIZE_RAW_DATA\": {\n        \"OVER_AVERAGE\": 30,\n        \"UNDER_AVERAGE\": 30,\n        \"VARIABILITY\": 2,\n        \"ABS_VALUE_UPPER_LIMIT\": None,\n        \"ABS_VALUE_LOWER_LIMIT\": None,\n    },\n    \"SIZE_CLEAN_DATA\": {\n        \"OVER_AVERAGE\": 30,\n        \"UNDER_AVERAGE\": 30,\n        \"VARIABILITY\": 2,\n        \"ABS_VALUE_UPPER_LIMIT\": None,\n        \"ABS_VALUE_LOWER_LIMIT\": None,\n    },\n    \"TOTAL_ERROR_RATE\": {\n        \"OVER_AVERAGE\": 30,\n        \"VARIABILITY\": 2,\n        \"ABS_VALUE_UPPER_LIMIT\": 20,\n    },\n    \"Missing_value_RATE\": {\n        \"cell_id\": {\n            \"AVERAGE\": 30,\n            \"VARIABILITY\": 2,\n            \"ABS_VALUE_UPPER_LIMIT\": 20,\n        },\n        \"valid_date_start\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"latitude\": {\n            \"AVERAGE\": 30,\n            \"VARIABILITY\": 2,\n            \"ABS_VALUE_UPPER_LIMIT\": 20,\n        },\n        \"longitude\": {\n            \"AVERAGE\": 30,\n            \"VARIABILITY\": 2,\n            \"ABS_VALUE_UPPER_LIMIT\": 20,\n        },\n        \"altitude\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"antenna_height\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"directionality\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"azimuth_angle\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"elevation_angle\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"horizontal_beam_width\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"vertical_beam_width\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"power\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"frequency\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"technology\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"cell_type\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n    },\n    \"Out_of_range_RATE\": {\n        \"cell_id\": {\n            \"AVERAGE\": 30,\n            \"VARIABILITY\": 2,\n            \"ABS_VALUE_UPPER_LIMIT\": 20,\n        },\n        \"latitude\": {\n            \"AVERAGE\": 30,\n            \"VARIABILITY\": 2,\n            \"ABS_VALUE_UPPER_LIMIT\": 20,\n        },\n        \"longitude\": {\n            \"AVERAGE\": 30,\n            \"VARIABILITY\": 2,\n            \"ABS_VALUE_UPPER_LIMIT\": 20,\n        },\n        \"antenna_height\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"directionality\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"azimuth_angle\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"elevation_angle\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"horizontal_beam_width\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"vertical_beam_width\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"power\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"frequency\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"technology\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"cell_type\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        None: {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n    },\n    \"Parsing_error_RATE\": {\n        \"valid_date_start\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"valid_date_end\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        }\n    }\n    }\n</code></pre>"},{"location":"UserManual/configuration/13_event_quality_warnings/","title":"EventQualityWarnings Configuration","text":"<p>To initialise and run the component two configs are used -  <code>general_config.ini</code> and component\u2019s config (either  <code>event_cleaning_quality_warnings.ini</code> or  <code>event_deduplication_quality_warnings.ini</code>). In  <code>general_config.ini</code> to execute Event Quality Warnings component specify all paths to its corresponding data objects. Example with specified paths for both cases:</p> <pre><code>[Paths.Silver]\n...\n# for Event Cleaning Quality Warnings\nevent_syntactic_quality_metrics_by_column = ${Paths:silver_dir}/event_syntactic_quality_metrics_by_column\nevent_syntactic_quality_metrics_frequency_distribution = ${Paths:silver_dir}/event_syntactic_quality_metrics_frequency_distribution\nevent_syntactic_quality_warnings_log_table = ${Paths:silver_dir}/event_syntactic_quality_warnings_log_table\nevent_syntactic_quality_warnings_for_plots = ${Paths:silver_dir}/event_syntactic_quality_warnings_for_plots\n# for Event Deduplication Quality Warnings\nevent_deduplicated_quality_metrics_by_column = ${Paths:silver_dir}/event_deduplicated_quality_metrics_by_column\nevent_deduplicated_quality_metrics_frequency_distribution = ${Paths:silver_dir}/event_deduplicated_quality_metrics_frequency_distribution\nevent_deduplicated_quality_warnings_log_table = ${Paths:silver_dir}/event_deduplicated_quality_warnings_log_table\n</code></pre> <p>Below there is a description of one of sub component\u2019s config  - <code>event_cleaning_quality_warnings.ini</code>. </p> <p>Parameters are as follows:</p> <p>Under  <code>[EventQualityWarnings]</code> config section: </p> <p>-SUB_COMPONENT_ID - string, default EventCleaningQualityWarnings, the config section's name should be identical to component id of EventQualityWarnings class, the value of SUB_COMPONENT_ID should be identical to the name of the following section, in this example - EventCleaningQualityWarnings</p> <p>Under <code>[EventCleaningQualityWarnings]</code> config section: </p> <ul> <li> <p>input_qm_by_column_path_key - string, key in Paths.Silver section in general config for a path to corresponding Quality Metrics By Column data</p> </li> <li> <p>input_qm_freq_distr_path_key - string, key in Paths.Silver section in general config for a path to corresponding Quality Metrics Frequency Distribution data</p> </li> <li> <p>output_qw_log_table_path_key - string, key in Paths.Silver section in general config for a path to write output Quality Warnings Log Table</p> </li> <li> <p>output_qw_for_plots_path_key - string, key in Paths.Silver section in general config for a path to write output Quality Warnings ForPLots</p> </li> <li> <p>data_period_start - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-08), the date from which start Event Quality Warnings, by now make sure the first day(s) of research period has enough previous data in in Quality Metrics Frequency Fistribution and Quality Metrics By Column </p> </li> <li> <p>data_period_end - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-15), the date till which perform Event Quality Warnings</p> </li> <li> <p>lookback_period - the length of lookback period, represented as string (could be either \u2018week' or 'month') which than will get its numeric representation in number of days</p> </li> <li> <p>do_size_raw_data_qw - boolean, whether perform QW checks on <code>initial_frequency</code> column in Quality Metrics Frequency Fistribution</p> </li> <li> <p>do_size_clean_data_qw - boolean, whether perform QW checks on <code>final_frequency</code> column in Quality Metrics Frequency Distribution</p> </li> <li> <p>data_size_tresholds - dictionary, with keys as thresholds names and values as corresponding thresholds values for  <code>do_size_raw_data_qw</code> and <code>do_size_clean_data_qw</code></p> </li> <li> <p>do_error_rate_by_date_qw - boolean, whether to perform QW checks on total error rate by <code>date</code></p> </li> <li> <p>do_error_rate_by_date_and_cell_qw -  boolean, whether to perform QW checks on total error rate by <code>date</code> and <code>cell_id</code></p> </li> <li> <p>do_error_rate_by_date_and_user_qw -  boolean, whether to perform QW checks on total error rate by <code>date</code> and <code>user_id</code></p> </li> <li> <p>do_error_rate_by_date_and_cell_user_qw - boolean, whether to perform QW checks on total error rate by <code>date</code>, <code>cell_id</code> and <code>user_id</code></p> </li> <li> <p>error_rate_tresholds - dictionary, with keys as thresholds names and values as corresponding thresholds values for  <code>do_error_rate_by_date_qw</code>, <code>do_error_rate_by_date_and_cell_qw</code>, <code>do_error_rate_by_date_and_user_qw</code>, and <code>do_error_rate_by_date_and_cell_user_qw</code></p> </li> <li> <p>error_type_qw_checks - dictionary, where the keys are names of error types (please see <code>multimno/core/constants/error_types.py</code> file) and values list of column names on which you want to perform QWs of the this error type. Example: during Event Cleaning three columns are checked for null values, if you want to check error rate of <code>missing_value</code> type for all mentioned columns specify them in the list. Some error types might have None for column names, which means that technically this kind or error do not belong to just one column but several (e.g. for <code>no_location</code> error three columns are used - cell_id, lat, lon): </p> </li> </ul> <p><pre><code>error_type_qw_checks = {\n    'missing_value': ['user_id', 'mcc', 'timestamp'],\n    'out_of_bounding_box':[None]\n    }\n</code></pre> If you do not intend to run QWs on some error type leave its corresponding list of columns empty.</p> <ul> <li>thresholds for each error_type &amp; column combination you want to compute QWs  - thresholds are combined in groups: each set of thresholds relevant to some error type is a separate config param of type dictionary, where keys are column names, values is another dictionary of structure: <code>threshold_name:threshold_value</code>. Example: </li> </ul> <p><pre><code>missing_value_thresholds = {\n    'user_id': {\"Missing_value_RATE_BYDATE_USER_AVERAGE\": 30,\n                \"Missing_value_RATE_BYDATE_USER_VARIABILITY\": 2,\n                \"Missing_value_RATE_BYDATE_USER_ABS_VALUE_UPPER_LIMIT\": 20\n               }, \n    'mcc': {\"Missing_value_RATE_BYDATE_MCC_AVERAGE\": 30,\n            \"Missing_value_RATE_BYDATE_MCC_VARIABILITY\": 2,\n            \"Missing_value_RATE_BYDATE_MCC_ABS_VALUE_UPPER_LIMIT\": 20\n           }, \n    'timestamp': {\"Missing_value_RATE_BYDATE_TIMESTAMP_AVERAGE\": 30,\n                  \"Missing_value_RATE_BYDATE_TIMESTAMP_VARIABILITY\": 2,\n                  \"Missing_value_RATE_BYDATE_TIMESTAMP_ABS_VALUE_UPPER_LIMIT\": 20\n                 }\n    }\nout_of_bounding_box_thresholds = {\n    None: {\"Out_of_bbox_RATE_BYDATE_AVERAGE\": 30,\n           \"Out_of_bbox_RATE_BYDATE_VARIABILITY\": 2,\n           \"Out_of_bbox_RATE_BYDATE_ABS_VALUE_UPPER_LIMIT\": 20\n          }\n    }\n</code></pre> Make sure, that for each column of interest specified in <code>error_type_qw_checks</code> there are corresponding thresholds. The order of thresholds is important and should be: <code>AVERAGE</code>, <code>VARIABILITY</code>, and <code>ABS_VALUE_UPPER_LIMIT</code> (at least by now all error type QWs follow the same logic and thus their computation is done within one function with ordered threshold arguments). Currently the code supports running QWs on following thresholds: </p> <p><pre><code># possible thresholds in event_cleaning_quality_warnings.ini\nmissing_value_thresholds\n\nout_of_admissible_values_thresholds\n\nnot_right_syntactic_format_thresholds\n\nno_location_thresholds\n\nout_of_bounding_box_thresholds\n\nthe last two are meant to be specified in event_deduplication_quality_warnings.ini\n\n# possible thresholds in event_deduplcaition_quality_warnings.ini\ndeduplication_diff_location_thresholds\n\ndeduplication_same_location_thresholds\n</code></pre> - clear_destination_directory - boolean, if True deletes all output of the Component in init stage</p>"},{"location":"UserManual/configuration/13_event_quality_warnings/#configuration-example","title":"Configuration example","text":"<pre><code># the name section should be identical to COMPONENT_ID of EventQualityWarnings Component\n# the value of SUB_COMPONENT_ID key shoudl be identical to the name of the following section\n[EventQualityWarnings]\nSUB_COMPONENT_ID = EventCleaningQualityWarnings\n\n[EventCleaningQualityWarnings]\n# keys in Paths.Silver section in general config\ninput_qm_by_column_path_key = event_syntactic_quality_metrics_by_column\ninput_qm_freq_distr_path_key = event_syntactic_quality_metrics_frequency_distribution\noutput_qw_log_table_path_key = event_syntactic_quality_warnings_log_table\noutput_qw_for_plots_path_key = event_syntactic_quality_warnings_for_plots\n# BY NOW make sure that the first day(s) of research period has enough previous data\n# of df_qa_by_column and df_qa_freq_distribution \n# (e.g. staring from 2023-01-01, if period is a week and start period is 2023-01-08)\ndata_period_start = 2023-01-08\n# you can exceed max(df_qa_by_column.date) \n# although you will still get QWs for dates till max(df_qa_by_column.date), including\ndata_period_end = 2023-01-15\n# should be either week or month\nlookback_period = week\n# SIZE QA\ndo_size_raw_data_qw = True\ndo_size_clean_data_qw = True\ndata_size_tresholds = {\n    \"SIZE_RAW_DATA_BYDATE_VARIABILITY\": 3,\n    \"SIZE_RAW_DATA_BYDATE_ABS_VALUE_UPPER_LIMIT\": 10000000,\n    \"SIZE_RAW_DATA_BYDATE_ABS_VALUE_LOWER_LIMIT\": 0,\n    \"SIZE_CLEAN_DATA_BYDATE_VARIABILITY\": 3,\n    \"SIZE_CLEAN_DATA_BYDATE_ABS_VALUE_UPPER_LIMIT\": 10000000,\n    \"SIZE_CLEAN_DATA_BYDATE_ABS_VALUE_LOWER_LIMIT\": 0,\n    }\n# ERROR RATE QW\ndo_error_rate_by_date_qw = True\ndo_error_rate_by_date_and_cell_qw = False\ndo_error_rate_by_date_and_user_qw = True\ndo_error_rate_by_date_and_cell_user_qw = True\nerror_rate_tresholds = {\n    \"TOTAL_ERROR_RATE_BYDATE_OVER_AVERAGE\": 30,\n    \"TOTAL_ERROR_RATE_BYDATE_VARIABILITY\": 2,\n    \"TOTAL_ERROR_RATE_BYDATE_ABS_VALUE_UPPER_LIMIT\": 20,\n    \"ERROR_RATE_BYDATE_BYCELL_OVER_AVERAGE\": 30,\n    \"ERROR_RATE_BYDATE_BYCELL_VARIABILITY\": 2,\n    \"ERROR_RATE_BYDATE_BYCELL_ABS_VALUE_UPPER_LIMIT\": 20,\n    \"ERROR_RATE_BYDATE_BYUSER_OVER_AVERAGE\": 30,\n    \"ERROR_RATE_BYDATE_BYUSER_VARIABILITY\": 2,\n    \"ERROR_RATE_BYDATE_BYUSER_ABS_VALUE_UPPER_LIMIT\": 20,\n    \"ERROR_RATE_BYDATE_BYCELL_USER_OVER_AVERAGE\": 30,\n    \"ERROR_RATE_BYDATE_BYCELL_USER_VARIABILITY\": 2,\n    \"ERROR_RATE_BYDATE_BYCELL_USER_ABS_VALUE_UPPER_LIMIT\": 20,\n    }\n# ERROR TYPE QW\n# for each type of error (key), specified the colums you want to check, naming of columns must be oidentical to ColNames\n# if you do not want to run qw on some error_type leave the list empty\n# None - for no_location and out_of_bounding_box because they do not have more than one column used for this error_type\n# for more clarity please check event_cleaning.py\nerror_type_qw_checks = {\n    'missing_value': ['user_id', 'mcc', 'timestamp'],\n    'out_of_admissible_values': ['cell_id', 'mcc', 'timestamp'],\n    'not_right_syntactic_format': ['timestamp'], \n    'no_location':[None], \n    'out_of_bounding_box':[None]\n    }\n# for each dict_error_type_thresholds make sure you specified all relevant columns\n# the order of thresholds is important, should be: AVERAGE, VARIABILITY, and ABS_VALUE_UPPER_LIMIT\nmissing_value_thresholds = {\n    'user_id': {\"Missing_value_RATE_BYDATE_USER_AVERAGE\": 30,\n                \"Missing_value_RATE_BYDATE_USER_VARIABILITY\": 2,\n                \"Missing_value_RATE_BYDATE_USER_ABS_VALUE_UPPER_LIMIT\": 20\n               }, \n    'mcc': {\"Missing_value_RATE_BYDATE_MCC_AVERAGE\": 30,\n            \"Missing_value_RATE_BYDATE_MCC_VARIABILITY\": 2,\n            \"Missing_value_RATE_BYDATE_MCC_ABS_VALUE_UPPER_LIMIT\": 20\n           }, \n    'timestamp': {\"Missing_value_RATE_BYDATE_TIMESTAMP_AVERAGE\": 30,\n                  \"Missing_value_RATE_BYDATE_TIMESTAMP_VARIABILITY\": 2,\n                  \"Missing_value_RATE_BYDATE_TIMESTAMP_ABS_VALUE_UPPER_LIMIT\": 20\n                 }\n    }\nout_of_admissible_values_thresholds = {\n    'cell_id': {\"Out_of_range_RATE_BYDATE_CELL_AVERAGE\": 30,\n                \"Out_of_range_RATE_BYDATE_CELL_VARIABILITY\": 2,\n                \"Out_of_range_RATE_BYDATE_CELL_ABS_VALUE_UPPER_LIMIT\": 20\n               }, \n    'mcc': {\"Out_of_range_RATE_BYDATE_MCC_AVERAGE\": 30,\n            \"Out_of_range_RATE_BYDATE_MCC_VARIABILITY\": 2,\n            \"Out_of_range_RATE_BYDATE_MCC_ABS_VALUE_UPPER_LIMIT\": 20\n           }, \n    'timestamp': {\"Out_of_range_RATE_BYDATE_TIMESTAMP_AVERAGE\": 30,\n                  \"Out_of_range_RATE_BYDATE_TIMESTAMP_VARIABILITY\": 2,\n                  \"Out_of_range_RATE_BYDATE_TIMESTAMP_ABS_VALUE_UPPER_LIMIT\": 20\n                 }\n    }\nnot_right_syntactic_format_thresholds = {\n    'timestamp': {\"Wrong_type_RATE_BYDATE_TIMESTAMP_AVERAGE\": 30,\n                  \"Wrong_type_RATE_BYDATE_TIMESTAMP_VARIABILITY\": 2,\n                  \"Wrong_type_RATE_BYDATE_TIMESTAMP_ABS_VALUE_UPPER_LIMIT\": 20\n                 }\n    }\nno_location_thresholds = {\n    None: {\"No_location_RATE_BYDATE_AVERAGE\": 30,\n           \"No_location_RATE_BYDATE_VARIABILITY\": 2,\n           \"No_location_RATE_BYDATE_ABS_VALUE_UPPER_LIMIT\": 20\n          }\n    }\nout_of_bounding_box_thresholds = {\n    None: {\"Out_of_bbox_RATE_BYDATE_AVERAGE\": 30,\n           \"Out_of_bbox_RATE_BYDATE_VARIABILITY\": 2,\n           \"Out_of_bbox_RATE_BYDATE_ABS_VALUE_UPPER_LIMIT\": 20\n          }\n    }\nclear_destination_directory = True\n</code></pre>"},{"location":"UserManual/configuration/14_semantic_quality_warnings/","title":"SemanticQualityWarnings Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>event_semantic_quality_warnings.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Silver]\n...\nevent_device_semantic_quality_metrics = ${Paths:silver_quality_metrics_dir}/semantic_quality_metrics\nevent_device_semantic_quality_warnings_log_table = ${Paths:silver_quality_warnings_dir}/semantic_quality_warnings_log_table\nevent_device_semantic_quality_warnings_bar_plot_data = ${Paths:silver_quality_warnings_dir}/semantic_quality_warnings_bar_plot_data\n...\n</code></pre> <p>The expected parameters in <code>event_semantic_quality_warnings.ini</code> are as follows: - date_of_study: string, format should be the one specified <code>date_format</code> (e.g., <code>2023-01-01</code> for <code>%Y-%m-%d</code>), the first date for which data will be processed by the component. All dates between this one and the specified in <code>data_period_end</code> will be processed (both inclusive). - date_format: string, it indicates the format expected in <code>date_of_study</code>. For example, use <code>%Y-%m-%d</code> for the usual \"2023-01-01\" format separated by <code>-</code>. - thresholds: dictionary, indicating the different thresholds to be used for raising warnings.</p>"},{"location":"UserManual/configuration/14_semantic_quality_warnings/#thresholds","title":"Thresholds","text":"<p>The thresholds parameter is a dictionary of dictionaries, used to indicate the different thresholds and lookback periods to be used for each type of warning. In the case than one of the thresholds is missing, a default value will be used instead. The default values are contained in <code>multimno.core.constants.semantic_qw_default_thresholds.SEMANTIC_DEFAULT_THRESHOLDS</code>.</p> <p>The dictionary structure is as follows:  - <code>\"CELL_ID_NON_EXISTENT\"</code>: refers to events that make reference to a non-existent cell ID.   - <code>\"sd_lookback_days\"</code>: integer, indicates the days to use as a lookback period to calculate the average and standard deviation over. By default, the value is 7.   - <code>\"min_sd\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate differs from the average over the lookback period by more than its standard deviation multiplied by this threshold. By default, the value is 2.   - <code>\"min_percentage\"</code>: integer or float, indicates the threshold for a warning to be raised for this error rate when the lookback date specified is strictly lower than 3. By default, the value is 0.  - <code>\"CELL_ID_NOT_VALID\"</code>: refers to events that make reference to an existent cell ID, but the cell is not operative.   - <code>\"sd_lookback_days\"</code>: integer, indicates the days to use as a lookback period to calculate the average and standard deviation over. By default, the value is 7.   - <code>\"min_sd\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate differs from the average over the lookback period by more than its standard deviation multiplied by this threshold. By default, the value is 2.   - <code>\"min_percentage\"</code>: integer or float, indicates the threshold for a warning to be raised for this error rate when the lookback date specified is strictly lower than 3. By default, the value is 0.  - <code>\"INCORRECT_EVENT_LOCATION\"</code>: refers to events that have been flagged as having an incorrect location.   - <code>\"sd_lookback_days\"</code>: integer, indicates the days to use as a lookback period to calculate the average and standard deviation over. By default, the value is 7.   - <code>\"min_sd\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate differs from the average over the lookback period by more than its standard deviation multiplied by this threshold. By default, the value is 2.   - <code>\"min_percentage\"</code>: integer or float, indicates the threshold for a warning to be raised for this error rate when the lookback date specified is strictly lower than 3. By default, the value is 0.  - <code>\"SUSPICIOUS_EVENT_LOCATION\"</code>: refers to events that have been flagged as having a suspicious location.   - <code>\"sd_lookback_days\"</code>: integer, indicates the days to use as a lookback period to calculate the average and standard deviation over. By default, the value is 7.   - <code>\"min_sd\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate differs from the average over the lookback period by more than its standard deviation multiplied by this threshold. By default, the value is 2.   - <code>\"min_percentage\"</code>: integer or float, indicates the threshold for a warning to be raised for this error rate when the lookback date specified is strictly lower than 3. By default, the value is 0.</p>"},{"location":"UserManual/configuration/14_semantic_quality_warnings/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = SemanticQualityWarnings\n\n[SemanticQualityWarnings]\n\ndate_format = %Y-%m-%d\ndate_of_study = 2023-01-08\n\nthresholds = {\n    \"CELL_ID_NON_EXISTENT\": {\n        \"sd_lookback_days\": 8,\n        \"min_sd\": 1.5,\n        \"min_percentage\": 5\n    },\n    \"CELL_ID_NOT_VALID\": {\n        \"sd_lookback_days\": 8,\n        \"min_sd\": 1.5,\n        \"min_percentage\": 5\n    },\n    \"INCORRECT_EVENT_LOCATION\": {\n        \"sd_lookback_days\": 8,\n        \"min_sd\": 1.5,\n        \"min_percentage\": 5\n    },\n    \"SUSPICIOUS_EVENT_LOCATION\": {\n        \"sd_lookback_days\": 8,\n        \"min_sd\": 1.5,\n        \"min_percentage\": 5\n    },\n    }\n</code></pre>"},{"location":"UserManual/configuration/1_inspire_grid_generation/","title":"InspireGridGeneration Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>grid_generation.ini</code>. In <code>general_config.ini</code> to execute the component specify all paths to its four corresponding data objects (input + output). Example: </p> <pre><code>[Paths.Silver]\ngrid_data_silver = ${Paths:silver_dir}/grid\n</code></pre> <p>In grid_generation.ini parameters are as follows: </p> <ul> <li> <p>grid_mask - string, the mask to be used for grid generation. It can be either 'extent' or 'polygon'. If 'extent' is chosen, the extent parameter should be provided. If 'polygon' is chosen, the polygon parameter should be provided. Only extent is currently implemented.</p> </li> <li> <p>extent - dictionary, the extent of the grid to be generated. It should contain the following keys: 'min_lat', 'max_lat', 'min_lon', 'max_lon'.</p> </li> <li> <p>do_landcover_enrichment - boolean, if True, the component will enrich the grid with landcover data. Default value is False. Currently not implemented.</p> </li> <li> <p>do_elevation_enrichment - boolean, if True, the component will enrich the grid with elevation data. Default value is False. Currently not implemented.</p> </li> <li> <p>grid_partition_size - integer, the size of the partition to be used for grid generation as a size of a side of grid subsquare. Default value is 500 grid tiles.</p> </li> </ul>"},{"location":"UserManual/configuration/1_inspire_grid_generation/#configuration-example","title":"Configuration example","text":"<pre><code>[InspireGridGeneration]\ngrid_mask = 'extent' # 'extent' or 'polygon'\nextent = {\n    'min_lat': -4.0,\n    'max_lat': -3.0,\n    'min_lon': 39.0,\n    'max_lon': 41.0\n    }\n\ndo_landcover_enrichment = False\ndo_elevation_enrichment = False\ngrid_partition_size = 500\n</code></pre>"},{"location":"UserManual/configuration/2_event_data_cleaning/","title":"EventCleaning Configuration","text":"<p>To initialise and run the component two configs are used - general_config.ini and event_cleaning.ini.  In general_config.ini to execute Event Cleaning component specify all paths to its four corresponding data objects (input + output). Example: </p> <pre><code>[Paths.Bronze]\nevent_data_bronze = ${Paths:bronze_dir}/mno_events\n\n[Paths.Silver]\nevent_data_silver = ${Paths:silver_dir}/mno_events\nevent_syntactic_quality_metrics_by_column = ${Paths:silver_dir}/event_syntactic_quality_metrics_by_column\nevent_syntactic_quality_metrics_frequency_distribution = ${Paths:silver_dir}/event_syntactic_quality_metrics_frequency_distribution\n</code></pre> <p>In event_cleaning.ini parameters are as follows: </p> <ul> <li> <p>data_period_start - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-01), the date from which start Event Cleaning</p> </li> <li> <p>data_period_end - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-05), the date till which perform Event Cleaning</p> </li> <li> <p>data_folder_date_format - string, to what string format convert dates so they match the naming of input data folders (it is expected that input data is divided into separate folders for each date of research period). Example: if you know that data for 2023-01-01 is stored in f\"{bronze_event_path}/20230101\", then the format to convert 2023-01-01 date to 20230101 string using strftimewill be %Y%m%d</p> </li> <li> <p>spark_data_folder_date_format - string, as for data_folder_date_format it depends on folder\u2019s naming pattern of input data but since datetime patterns in pyspark and strftime differ, it is a separate config param. Used to convert string to datetype when creating date column in frequency distribution table </p> </li> <li> <p>timestamp_format - str, expected string format of timestamp column when converting it to timestamp type</p> </li> <li> <p>input_timezone - str, timezone of data to use when converting to UTC, if you are sure that data was already changed to UTC or geographically in UTC, leave as \u201cUTC\u201c</p> </li> <li> <p>do_bounding_box_filtering- boolean, True/False, decides whether to apply bounding box filtering</p> </li> <li> <p>bounding_box - dictionary, with following keys 'min_lon', 'max_lon', 'min_lat', and 'max_lat' and integer/float values, to specify coordinates of bounding box, within which records should fall, make sure that records and bounding box are in the same src </p> </li> <li> <p>number_of_partitions - an integer, that determines the value of the modulo operator. This value will determine the number expected partitions as to the last partitioning column user_id_modulo. This value does not affect the number of folders in terms of other partitioning columns (day, month, year).</p> </li> </ul>"},{"location":"UserManual/configuration/2_event_data_cleaning/#configuration-example","title":"Configuration example","text":"<pre><code>[EventCleaning]\ndata_period_start = 2023-01-01\ndata_period_end = 2023-01-05\ndata_folder_date_format = %Y%m%d\nspark_data_folder_date_format = yyyyMMdd\ntimestamp_format = yyyy-MM-dd'T'HH:mm:ss\ninput_timezone = America/Los_Angeles\ndo_bounding_box_filtering = True\nbounding_box = {\n    'min_lon': -180,\n    'max_lon': 180,\n    'min_lat': -90,\n    'max_lat': 90\n    }\nnumber_of_partitions = 256\n</code></pre>"},{"location":"UserManual/configuration/3_network_cleaning/","title":"NetworkCleaning Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>network_cleaning.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Bronze]\n...\nnetwork_data_bronze = ${Paths:bronze_dir}/mno_network\n...\n\n[Paths.Silver]\n...\nnetwork_data_silver = ${Paths:silver_dir}/mno_network\nnetwork_syntactic_quality_metrics_by_column = ${Paths:silver_quality_metrics_dir}/network_syntactic_quality_metrics_by_column\n...\n</code></pre> <p>The expected parameters in <code>network_cleaning.ini</code> are as follows:</p> <ul> <li>latitude_min: float, minimum accepted latitude (WGS84) for the latitude of cells in the input data. Values lower than this will be treated as out of bounds/range.</li> <li>latitude_max: float, maximum accepted latitude (WGS84) for the latitude of cells in the input data. Values higher than this will be treated as out of bounds/range.</li> <li>longitude_min: float, minimum accepted longitude (WGS84) for the longitude of cells in the input data. Values lower than this will be treated as out of bounds/range.</li> <li>longitude_max: float, minimum accepted longitude (WGS84) for the longitude of cells in the input data. Values higher than this will be treated as out of bounds/range.</li> <li>cell_type_options: comma-separated list of strings, this parameter indicates the accepted values in the <code>cell_type_options</code> field. Other values will be treated as out of bounds/range. Example: <code>macrocell, microcell, picocell</code>.</li> <li>data_period_start: string, format should be the one specified <code>data_period_format</code> (e.g., <code>2023-01-01</code> for <code>%Y-%m-%d</code>), the first date for which data will be processed by the component. All dates between this one and the specified in <code>data_period_end</code> will be processed (both inclusive).</li> <li>data_period_end: string, format should be \"yyyy-MM-dd\" (e.g., <code>2023-01-09</code> for <code>%Y-%m-%d</code>), the last date for which data will be processed by the component. All dates between the specified in <code>data_period_start</code> and this one will be processed (both inclusive).</li> <li>data_period_format: string, it indicates the format expected in <code>data_period_start</code> and <code>data_period_end</code>. For example, use <code>%Y-%m-%d</code> for the usual \"2023-01-09\" format separated by <code>-</code>.</li> <li>valid_date_timestamp_format: string, the timestamp format that is expected to be in the input network data and that will be parsed with PySpark using thiis format. Example: <code>yyyy-MM-dd'T'HH:mm:ss</code></li> </ul>"},{"location":"UserManual/configuration/3_network_cleaning/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = NetworkCleaning\n\n[NetworkCleaning]\n# Bounding box\nlatitude_min = 40.352\nlatitude_max = 40.486\nlongitude_min = -3.751\nlongitude_max = -3.579\n\ncell_type_options = macrocell, microcell, picocell\n\n# Left- and right-inclusive date range for the data to be read\ndata_period_start = 2023-01-01\ndata_period_end = 2023-01-09\ndata_period_format = %Y-%m-%d\n\nvalid_date_timestamp_format = yyyy-MM-dd'T'HH:mm:ss\n</code></pre>"},{"location":"UserManual/configuration/4_signal_strength_modeling/","title":"SignalStrengthModeling Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>signal_strength_modeling.ini</code>. In <code>general_config.ini</code> to execute the component specify all paths to its corresponding data objects (input + output). Example:</p> <pre><code>[Paths.Silver]\nnetwork_data_silver = ${Paths:silver_dir}/mno_network\ngrid_data_silver = ${Paths:silver_dir}/grid\nsignal_strength_data_silver = ${Paths:silver_dir}/signal_strength\n</code></pre> <p>In signal_strength_modeling.ini parameters are as follows: </p> <ul> <li> <p>data_period_start - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-01), the date from which start Event Cleaning</p> </li> <li> <p>data_period_end - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-05), the date till which perform Event Cleaning</p> </li> <li> <p>use_elevation - boolean, if True, the elevation data will be used for signal strength modeling. If False, the elevation will be set to 0</p> </li> <li> <p>do_azimuth_angle_adjustments - boolean, if True, signal strengths values will be adjusted based on azimuth and antenna horizontal beam widths for directional cells. If False, no adjustments will be made.</p> </li> <li> <p>do_elevation_angle_adjustments - boolean, if True, signal strengths values will be adjusted based on tilt and antenna vertical beam widths for directional cells. If False, no adjustments will be made.</p> </li> <li> <p>cartesian_crs - integer, the coordinate reference system (CRS) to use for the cartesian coordinates. The default value is 3035.</p> </li> <li> <p>default_cell_physical_properties - dictionary, the default physical properties of the cell types. These properties will be assigned to cells of corresponding type if the properties are not found in the network topology data. If cell types are not peresent in network topology data, the default type properties will be assigned to all cells.</p> </li> </ul>"},{"location":"UserManual/configuration/4_signal_strength_modeling/#configuration-example","title":"Configuration example","text":"<pre><code>[SignalStrengthModeling]\ndata_period_start = 2023-01-01\ndata_period_end = 2023-01-15\nuse_elevation = False\ndo_azimuth_angle_adjustments = True\ndo_elevation_angle_adjustments = True\ncartesian_crs = 3035\n\ndefault_cell_physical_properties = {\n    'macrocell': {\n        'power': 10,\n        'range': 10000,\n        'path_loss_exponent': 3.75,\n        'antenna_height': 30,\n        'elevation_angle': 5,\n        'vertical_beam_width': 9,\n        'horizontal_beam_width': 65,\n        'azimuth_signal_strength_back_loss': -30,\n        'elevation_signal_strength_back_loss': -30,\n    },\n    'microcell': {\n        'power': 5,\n        'range': 1000,\n        'path_loss_exponent': 6.0,\n        'antenna_height': 8,\n        'elevation_angle': 5,\n        'vertical_beam_width': 9,\n        'horizontal_beam_width': 65,\n        'azimuth_signal_strength_back_loss': -30,\n        'elevation_signal_strength_back_loss': -30,\n    },\n    'default': {\n        'power': 5,\n        'range': 5000,\n        'path_loss_exponent': 3.75,\n        'antenna_height': 8,\n        'elevation_angle': 5,\n        'vertical_beam_width': 9,\n        'horizontal_beam_width': 65,\n        'azimuth_signal_strength_back_loss': -30,\n        'elevation_signal_strength_back_loss': -30,\n        }\n    }\n</code></pre>"},{"location":"UserManual/configuration/5_cell_footprint_estimation/","title":"CellFootprintEstimation Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>cell_footprint_estimation.ini</code>. In <code>general_config.ini</code> to execute the component specify all paths to its corresponding data objects (input + output). Example: </p> <pre><code>[Paths.Silver]\nsignal_strength_data_silver = ${Paths:silver_dir}/signal_strength\ncell_footprint_data_silver = ${Paths:silver_dir}/cell_footprint\ncell_intersection_groups_data_silver = ${Paths:silver_dir}/cell_intersection_groups\n</code></pre> <p>In cell_footprint_estimation.ini parameters are as follows: </p> <ul> <li> <p>data_period_start - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-01), the date from which start Event Cleaning</p> </li> <li> <p>data_period_end - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-05), the date till which perform Event Cleaning</p> </li> <li> <p>logistic_function_steepness - float, the steepness of the logistic function used to estimate the signal dominance (cell footprint) value from signal strength. The default value is 0.2.</p> </li> <li> <p>logistic_function_midpoint - float, the midpoint of the logistic function used to estimate the signal dominance (cell footprint) value from signal strength. The default value is -92.5.</p> </li> <li> <p>do_difference_from_best_sd_prunning - boolean, if True, the cells per grid tile with signal dominance values that are lower than the threshold percentage from the best signal dominance will be pruned. If False, no pruning of this method will be performed.</p> </li> <li> <p>difference_from_best_sd_treshold - float, the threshold percentage from the best signal dominance value. The default value is 90.</p> </li> <li> <p>do_max_cells_per_tile_prunning - boolean, if True, the maximum number of cells per grid tile will be kept, other cells will be pruned. If False, no pruning of this method will be performed.</p> </li> <li> <p>max_cells_per_grid_tile - integer, the maximum number of cells per grid tile. The default value is 10.</p> </li> <li> <p>do_sd_treshold_prunning - boolean, if True, the cells with signal dominance values that are lower than the threshold will be pruned. If False, no pruning of this method will be performed.</p> </li> <li> <p>signal_dominance_treshold - float, the threshold value for signal dominance. The default value is 0.01.</p> </li> <li> <p>do_cell_intersection_groups_calculation - boolean, if True, the cell intersection groups will be calculated and corresponding data object created. If False, no cell intersection groups will be calculated.</p> </li> </ul>"},{"location":"UserManual/configuration/5_cell_footprint_estimation/#configuration-example","title":"Configuration example","text":"<pre><code>[CellFootprintEstimation]\ndata_period_start = 2023-01-01\ndata_period_end = 2023-01-05\nlogistic_function_steepness = 0.2\nlogistic_function_midpoint = -92.5\n\ndo_difference_from_best_sd_prunning = True\ndifference_from_best_sd_treshold = 90 # percentage\n\ndo_max_cells_per_tile_prunning = False\nmax_cells_per_grid_tile = 10\n\ndo_sd_treshold_prunning = True\nsignal_dominance_treshold = 0.01\n\ndo_cell_intersection_groups_calculation = True\n</code></pre>"},{"location":"UserManual/configuration/6_cell_connection_probability/","title":"CellConnectionProbabilityEstimation Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>cell_connection_probability_estimation</code>.ini. In <code>general_config.ini</code> to execute the component specify all paths to its four corresponding data objects (input + output). Example: </p> <pre><code>[Paths.Bronze]\nevent_data_bronze = ${Paths:bronze_dir}/mno_events\n\n[Paths.Silver]\ncell_footprint_data_silver = ${Paths:silver_dir}/cell_footprint\ngrid_data_silver = ${Paths:silver_dir}/grid\ncell_connection_probabilities_data_silver = ${Paths:silver_dir}/cell_conn_probs\n</code></pre> <p>In cell_connection_probability_estimation.ini parameters are as follows: </p> <ul> <li> <p>data_period_start - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-01), the date from which start Event Cleaning</p> </li> <li> <p>data_period_end - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-05), the date till which perform Event Cleaning</p> </li> <li> <p>use_land_use_prior - boolean, if True, the land use prior will be used for cell connection posterior probability estimation. If False, the land use prior will not be used, only connection probability based on cell footprint will be estimated.</p> </li> </ul>"},{"location":"UserManual/configuration/6_cell_connection_probability/#configuration-example","title":"Configuration example","text":"<pre><code>[CellConnectionProbabilityEstimation]\ndata_period_start = 2023-01-01\ndata_period_end = 2023-01-15\nuse_land_use_prior = False\n</code></pre>"},{"location":"UserManual/configuration/7_event_data_deduplication/","title":"EventDeduplication Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>event_deduplication.ini</code>. In <code>general_config.ini</code> to execute the component specify all paths to its corresponding data objects (input + output). Example: </p> <pre><code>[Paths.Silver]\nevent_data_silver = ${Paths:silver_dir}/mno_events\nevent_data_silver_deduplicated = ${Paths:silver_dir}/mno_events_deduplicated\nevent_deduplicated_quality_metrics_by_column = ${Paths:silver_quality_metrics_dir}/event_deduplicated_quality_metrics_by_column\nevent_deduplicated_quality_metrics_frequency_distribution = ${Paths:silver_quality_metrics_dir}/event_deduplicated_quality_metrics_frequency_distribution\n</code></pre> <p>In event_deduplication.ini parameters are as follows: </p> <ul> <li> <p>data_period_start - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-01), the date from which start processing</p> </li> <li> <p>data_period_end - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-05), the date till which perform processing</p> </li> <li> <p>spark_data_folder_date_format - string, as for data_folder_date_format it depends on folder\u2019s naming pattern of input data but since datetime patterns in pyspark and strftime differ, it is a separate config param. Used to convert string to datetype when creating date column in frequency distribution table </p> </li> <li> <p>clear_destination_directory - boolean, if True, the destination directory will be cleared before writing new data to it</p> </li> </ul>"},{"location":"UserManual/configuration/7_event_data_deduplication/#configuration-example","title":"Configuration example","text":"<pre><code>[EventDeduplication]\ndata_period_start = 2023-01-01\ndata_period_end = 2023-01-15\nspark_data_folder_date_format = yyyyMMdd\nclear_destination_directory = True\n</code></pre>"},{"location":"UserManual/configuration/8_event_semantic_cleaning/","title":"SemanticCleaning Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>event_semantic_cleaning.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Silver]\n...\nevent_data_silver_deduplicated = ${Paths:silver_dir}/mno_events_deduplicated\nnetwork_data_silver = ${Paths:silver_dir}/mno_network\nevent_data_silver_flagged = ${Paths:silver_dir}/mno_events_flagged\nevent_device_semantic_quality_metrics = ${Paths:silver_quality_metrics_dir}/semantic_quality_metrics\n...\n</code></pre> <p>The expected parameters in <code>event_semantic_cleaning.ini</code> are as follows: - data_period_start: string, format should be the one specified <code>data_period_format</code> (e.g., <code>2023-01-01</code> for <code>%Y-%m-%d</code>), the first date for which data will be processed by the component. All dates between this one and the specified in <code>data_period_end</code> will be processed (both inclusive). - data_period_end: string, format should be \"yyyy-MM-dd\" (e.g., <code>2023-01-09</code> for <code>%Y-%m-%d</code>), the last date for which data will be processed by the component. All dates between the specified in <code>data_period_start</code> and this one will be processed (both inclusive). - data_period_format: string, it indicates the format expected in <code>data_period_start</code> and <code>data_period_end</code>. For example, use <code>%Y-%m-%d</code> for the usual \"2023-01-09\" format separated by <code>-</code>. - semantic_min_distance_m: float, minimum distance (in metres) between two consecutive events above which they will be considered for flagging as suspicious or incorrect location. Example: <code>10000</code>. - semantic_min_speed_m_s: float, minimum mean speed (in metres per second) between two consecutive events above whihc they will be considered for flagging as suspicious or incorrect location. Example: <code>55</code>.</p>"},{"location":"UserManual/configuration/8_event_semantic_cleaning/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = SemanticCleaning\n\n[SemanticCleaning]\ndata_period_start = 2023-01-01\ndata_period_end = 2023-01-09\ndate_format = %Y-%m-%d\n\nsemantic_min_distance_m = 10000\nsemantic_min_speed_m_s = 55\n</code></pre>"},{"location":"UserManual/configuration/9_device_activity_statistics/","title":"DeviceActivityStatistics Configuration","text":"<p>To initialise and run the component two configs are used - general_config.ini and device_activity_statistics.ini.  In general_config.ini to execute Device Activity Statistics component specify all paths to its three corresponding data objects (input + output). The local timezone must also be specified in the general config. Example: </p> <pre><code>[Timezone]\nlocal_timezone = UTC\n\n[Paths.Silver]\n# Data\nevent_data_silver_deduplicated = ${Paths:silver_dir}/mno_events_deduplicated\nnetwork_data_silver = ${Paths:silver_dir}/mno_network\n\ndevice_activity_statistics = ${Paths:silver_dir}/device_activity_statistics\n</code></pre> <p>In device_activity_statistics.ini parameters are as follows: </p> <ul> <li> <p>data_period_start - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-01), the date from which start Event Cleaning</p> </li> <li> <p>data_period_end - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-05), the date till which perform Event Cleaning</p> </li> <li> <p>clear_destination_directory - boolean, whether to empty the destination directory before running or not</p> </li> </ul>"},{"location":"UserManual/configuration/9_device_activity_statistics/#configuration-example","title":"Configuration example","text":"<pre><code>[DeviceActivityStatistics]\nclear_destination_directory = True\ndata_period_start = 2023-01-01\ndata_period_end = 2023-01-04\n</code></pre>"},{"location":"autodoc/test_report/","title":"Test Report","text":"test_report.md"},{"location":"autodoc/test_report/#title","title":"test_report.md","text":"<p>Report generated on 25-Mar-2024 at 15:10:22 by pytest-html         v4.1.1</p> Environment No results found. Check the filters. &lt; &gt; Summary <p>25 tests took 00:01:47.</p> <p>(Un)check the boxes to filter the results.</p> There are still tests running. Reload this page to get the latest results! 0 Failed, 25 Passed, 4 Skipped, 0 Expected failures, 0 Unexpected passes, 0 Errors, 0 Reruns Show all details\u00a0/\u00a0Hide all details Result Test Duration Links"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>components<ul> <li>execution<ul> <li>cell_connection_probability<ul> <li>cell_connection_probability</li> </ul> </li> <li>cell_footprint<ul> <li>cell_footprint_estimation</li> </ul> </li> <li>daily_permanence_score<ul> <li>daily_permanence_score</li> </ul> </li> <li>device_activity_statistics<ul> <li>device_activity_statistics</li> </ul> </li> <li>event_cleaning<ul> <li>event_cleaning</li> </ul> </li> <li>event_deduplication<ul> <li>event_deduplication</li> </ul> </li> <li>event_semantic_cleaning<ul> <li>event_semantic_cleaning</li> </ul> </li> <li>network_cleaning<ul> <li>network_cleaning</li> </ul> </li> <li>signal_strength<ul> <li>signal_stength_modeling</li> </ul> </li> <li>time_segments<ul> <li>continuous_time_segmentation</li> </ul> </li> </ul> </li> <li>ingestion<ul> <li>grid_generation<ul> <li>inspire_grid_generation</li> </ul> </li> <li>synthetic<ul> <li>synthetic_diaries</li> <li>synthetic_events</li> <li>synthetic_events_errors</li> <li>synthetic_network</li> </ul> </li> </ul> </li> <li>quality<ul> <li>event_quality_warnings<ul> <li>event_quality_warnings</li> </ul> </li> <li>network_quality_warnings<ul> <li>network_quality_warnings</li> </ul> </li> <li>semantic_quality_warnings<ul> <li>semantic_quality_warnings</li> </ul> </li> </ul> </li> </ul> </li> <li>core<ul> <li>component</li> <li>configuration</li> <li>constants<ul> <li>columns</li> <li>conditions</li> <li>error_types</li> <li>measure_definitions</li> <li>network_default_thresholds</li> <li>semantic_qw_default_thresholds</li> <li>transformations</li> <li>warnings</li> </ul> </li> <li>data_objects<ul> <li>bronze<ul> <li>bronze_event_data_object</li> <li>bronze_network_physical_data_object</li> <li>bronze_synthetic_diaries_data_object</li> </ul> </li> <li>data_object</li> <li>landing</li> <li>silver<ul> <li>silver_cell_connection_probabilities_data_object</li> <li>silver_cell_footprint_data_object</li> <li>silver_cell_intersection_groups_data_object</li> <li>silver_daily_permanence_score_data_object</li> <li>silver_device_activity_statistics</li> <li>silver_event_data_object</li> <li>silver_event_data_syntactic_quality_metrics_by_column</li> <li>silver_event_data_syntactic_quality_metrics_frequency_distribution</li> <li>silver_event_data_syntactic_quality_warnings_for_plots</li> <li>silver_event_data_syntactic_quality_warnings_log_table</li> <li>silver_event_flagged_data_object</li> <li>silver_grid_data_object</li> <li>silver_network_data_object</li> <li>silver_network_data_syntactic_quality_metrics_by_column</li> <li>silver_network_syntactic_quality_warnings_log_table</li> <li>silver_network_syntactic_quality_warnings_plot_data</li> <li>silver_semantic_quality_metrics</li> <li>silver_semantic_quality_warnings_log_table</li> <li>silver_semantic_quality_warnings_plot_data</li> <li>silver_signal_strength_data_object</li> <li>silver_time_segments_data_object</li> </ul> </li> </ul> </li> <li>grid</li> <li>io_interface</li> <li>log</li> <li>settings</li> <li>spark_session</li> <li>utils</li> </ul> </li> <li>main</li> </ul>"},{"location":"reference/main/","title":"main","text":"<p>Application entrypoint for launching a single component.</p> <p>Usage:</p> <pre><code>python multimno/main.py &lt;component_id&gt; &lt;general_config_path&gt; &lt;component_config_path&gt;\n</code></pre> <ul> <li>component_id: ID of the component to be executed.</li> <li>general_config_path: Path to a INI file with the general configuration of the execution.</li> <li>component_config_path: Path to a INI file with the specific configuration of the component.</li> </ul>"},{"location":"reference/main/#main.build","title":"<code>build(component_id, general_config_path, component_config_path)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>component_id</code> <code>str</code> <p>id of the component</p> required <code>general_config_path</code> <code>str</code> <p>general config path</p> required <code>component_config_path</code> <code>str</code> <p>component config path</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the component_id is not supported.</p> <p>Returns:</p> Type Description <code>Component</code> <p>Component constructor.</p> Source code in <code>multimno/main.py</code> <pre><code>def build(component_id: str, general_config_path: str, component_config_path: str):\n    \"\"\"\n\n\n    Args:\n        component_id (str): id of the component\n        general_config_path (str): general config path\n        component_config_path (str): component config path\n\n    Raises:\n        ValueError: If the component_id is not supported.\n\n    Returns:\n        (multimno.core.component.Component): Component constructor.\n    \"\"\"\n    try:\n        constructor = CONSTRUCTORS[component_id]\n    except KeyError as e:\n        raise ValueError(f\"Component {component_id} is not supported.\") from e\n\n    return constructor(general_config_path, component_config_path)\n</code></pre>"},{"location":"reference/components/","title":"components","text":""},{"location":"reference/components/execution/","title":"execution","text":""},{"location":"reference/components/execution/cell_connection_probability/","title":"cell_connection_probability","text":""},{"location":"reference/components/execution/cell_connection_probability/cell_connection_probability/","title":"cell_connection_probability","text":"<p>Module that calculates cell connection probabilities and posterior probabilities.</p>"},{"location":"reference/components/execution/cell_connection_probability/cell_connection_probability/#components.execution.cell_connection_probability.cell_connection_probability.CellConnectionProbabilityEstimation","title":"<code>CellConnectionProbabilityEstimation</code>","text":"<p>             Bases: <code>Component</code></p> <p>Estimates the cell connection probabilities and posterior probabilities for each grid tile. Cell connection probabilities are calculated based on footprint per grid. Posterior probabilities are calculated based on the cell connection probabilities and grid prior probabilities.</p> <p>This class reads in cell footprint estimation and the grid model wit prior probabilities. The output is a DataFrame that represents cell connection probabilities and  posterior probabilities for each cell and grid id combination for a given date.</p> Source code in <code>multimno/components/execution/cell_connection_probability/cell_connection_probability.py</code> <pre><code>class CellConnectionProbabilityEstimation(Component):\n    \"\"\"\n    Estimates the cell connection probabilities and posterior probabilities for each grid tile.\n    Cell connection probabilities are calculated based on footprint per grid.\n    Posterior probabilities are calculated based on the cell connection probabilities\n    and grid prior probabilities.\n\n    This class reads in cell footprint estimation and the grid model wit prior probabilities.\n    The output is a DataFrame that represents cell connection probabilities and\n     posterior probabilities for each cell and grid id combination for a given date.\n    \"\"\"\n\n    COMPONENT_ID = \"CellConnectionProbabilityEstimation\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.data_period_start = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n\n        self.data_period_end = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n\n        self.data_period_dates = [\n            self.data_period_start + datetime.timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n    def initalize_data_objects(self):\n\n        # Input\n        self.input_data_objects = {}\n        self.use_land_use_prior = self.config.getboolean(self.COMPONENT_ID, \"use_land_use_prior\")\n\n        inputs = {\n            \"cell_footprint_data_silver\": SilverCellFootprintDataObject,\n        }\n\n        if self.use_land_use_prior:\n            inputs[\"grid_data_silver\"] = SilverGridDataObject\n\n        for key, value in inputs.items():\n            path = self.config.get(CONFIG_SILVER_PATHS_KEY, key)\n            if check_if_data_path_exists(self.spark, path):\n                self.input_data_objects[value.ID] = value(self.spark, path)\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")\n                raise ValueError(f\"Invalid path for {value.ID} in component {self.COMPONENT_ID} initialization\")\n\n        # Output\n        self.output_data_objects = {}\n        self.silver_cell_footprint_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"cell_connection_probabilities_data_silver\"\n        )\n        self.output_data_objects[SilverCellConnectionProbabilitiesDataObject.ID] = (\n            SilverCellConnectionProbabilitiesDataObject(\n                self.spark,\n                self.silver_cell_footprint_path,\n                partition_columns=[ColNames.year, ColNames.month, ColNames.day],\n            )\n        )\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n\n        # TODO: We might need to iterate over dates and process the data for each date separately\n        # have to do tests with larger datasets.\n\n        cell_footprint_df = self.input_data_objects[SilverCellFootprintDataObject.ID].df.filter(\n            F.make_date(\"year\", \"month\", \"day\").isin(self.data_period_dates)\n        )\n\n        # Calculate the cell connection probabilities\n\n        grid_footprint_sums = cell_footprint_df.groupBy(\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            ColNames.valid_date_start,\n            ColNames.valid_date_end,\n            ColNames.grid_id,\n        ).agg(F.sum(ColNames.signal_dominance).alias(\"total_grid_footprint\"))\n\n        cell_footprint_df = cell_footprint_df.join(\n            grid_footprint_sums,\n            on=[\n                ColNames.year,\n                ColNames.month,\n                ColNames.day,\n                ColNames.valid_date_start,\n                ColNames.valid_date_end,\n                ColNames.grid_id,\n            ],\n            how=\"left\",\n        )\n\n        cell_conn_probs_df = cell_footprint_df.withColumn(\n            ColNames.cell_connection_probability, F.col(ColNames.signal_dominance) / F.col(\"total_grid_footprint\")\n        ).drop(\"total_grid_footprint\")\n\n        # Calculate the posterior probabilities\n\n        if self.use_land_use_prior:\n            grid_model_df = self.input_data_objects[SilverGridDataObject.ID].df\n\n            # TODO should default value be configurable? 1 would keep cell connection probability value\n            # Assign 0 to any missing prior values\n            grid_model_df = grid_model_df.fillna(1, subset=[ColNames.prior_probability])\n\n            cell_conn_probs_df = cell_conn_probs_df.join(grid_model_df, on=ColNames.grid_id, how=\"left\")\n\n            cell_conn_probs_df = cell_conn_probs_df.withColumn(\n                ColNames.posterior_probability,\n                F.col(ColNames.cell_connection_probability) * F.col(ColNames.prior_probability),\n            )\n\n        elif not self.use_land_use_prior:\n            cell_conn_probs_df = cell_conn_probs_df.withColumn(\n                ColNames.posterior_probability, F.col(ColNames.cell_connection_probability)\n            )\n\n        # Normalize the posterior probabilities\n\n        total_posterior_df = cell_conn_probs_df.groupBy(ColNames.cell_id).agg(\n            F.sum(ColNames.posterior_probability).alias(\"total_posterior_probability\")\n        )\n\n        cell_conn_probs_df = (\n            cell_conn_probs_df.join(total_posterior_df, on=ColNames.cell_id, how=\"left\")\n            .withColumn(\n                ColNames.posterior_probability,\n                F.col(ColNames.posterior_probability) / F.col(\"total_posterior_probability\"),\n            )\n            .drop(\"total_posterior_probability\")\n        )\n\n        cell_conn_probs_df = cell_conn_probs_df.select(\n            *[field.name for field in SilverCellConnectionProbabilitiesDataObject.SCHEMA.fields]\n        )\n        columns = {\n            field.name: F.col(field.name).cast(field.dataType)\n            for field in SilverCellConnectionProbabilitiesDataObject.SCHEMA.fields\n        }\n        cell_conn_probs_df = cell_conn_probs_df.withColumns(columns)\n\n        self.output_data_objects[SilverCellConnectionProbabilitiesDataObject.ID].df = cell_conn_probs_df\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/","title":"cell_footprint","text":""},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/","title":"cell_footprint_estimation","text":"<p>Module that cleans RAW MNO Event data.</p>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation","title":"<code>CellFootprintEstimation</code>","text":"<p>             Bases: <code>Component</code></p> <p>Estimates the footprint of each cell per grid tile in its coverage area from its signal strength.</p> <p>This class reads in signal strength data, calculates the signal dominance of each cell, and prunes the data based on configurable thresholds. The output is a DataFrame that represents the footprint of each cell.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>class CellFootprintEstimation(Component):\n    \"\"\"\n    Estimates the footprint of each cell per grid tile in its coverage area from its signal strength.\n\n    This class reads in signal strength data, calculates the signal dominance of each cell, and prunes the data based on\n    configurable thresholds. The output is a DataFrame that represents the footprint of each cell.\n    \"\"\"\n\n    COMPONENT_ID = \"CellFootprintEstimation\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.logistic_function_steepness = self.config.getfloat(self.COMPONENT_ID, \"logistic_function_steepness\")\n        self.logistic_function_midpoint = self.config.getfloat(self.COMPONENT_ID, \"logistic_function_midpoint\")\n        self.signal_dominance_treshold = self.config.getfloat(self.COMPONENT_ID, \"signal_dominance_treshold\")\n        self.max_cells_per_grid_tile = self.config.getfloat(self.COMPONENT_ID, \"max_cells_per_grid_tile\")\n        self.difference_from_best_sd_treshold = self.config.getfloat(\n            self.COMPONENT_ID, \"difference_from_best_sd_treshold\"\n        )\n        self.do_sd_treshold_prunning = self.config.getboolean(self.COMPONENT_ID, \"do_sd_treshold_prunning\")\n        self.do_max_cells_per_tile_prunning = self.config.getboolean(\n            self.COMPONENT_ID, \"do_max_cells_per_tile_prunning\"\n        )\n        self.do_difference_from_best_sd_prunning = self.config.getboolean(\n            self.COMPONENT_ID, \"do_difference_from_best_sd_prunning\"\n        )\n\n        self.data_period_start = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n\n        self.data_period_end = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n\n        self.data_period_dates = [\n            self.data_period_start + datetime.timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n    def initalize_data_objects(self):\n\n        self.do_do_cell_intersection_groups_calculation = self.config.getboolean(\n            self.COMPONENT_ID, \"do_cell_intersection_groups_calculation\"\n        )\n        # Input\n        self.input_data_objects = {}\n\n        signal_strength = self.config.get(CONFIG_SILVER_PATHS_KEY, \"signal_strength_data_silver\")\n        if check_if_data_path_exists(self.spark, signal_strength):\n            self.input_data_objects[SilverSignalStrengthDataObject.ID] = SilverSignalStrengthDataObject(\n                self.spark, signal_strength\n            )\n        else:\n            self.logger.warning(f\"Expected path {signal_strength} to exist but it does not\")\n            raise ValueError(f\"Invalid path for {signal_strength} {CellFootprintEstimation.COMPONENT_ID}\")\n\n        # Output\n        self.output_data_objects = {}\n        self.silver_cell_footprint_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"cell_footprint_data_silver\")\n        self.output_data_objects[SilverCellFootprintDataObject.ID] = SilverCellFootprintDataObject(\n            self.spark,\n            self.silver_cell_footprint_path,\n            partition_columns=[ColNames.year, ColNames.month, ColNames.day],\n        )\n\n        if self.do_do_cell_intersection_groups_calculation:\n            self.silver_cell_intersection_groups_path = self.config.get(\n                CONFIG_SILVER_PATHS_KEY, \"cell_intersection_groups_data_silver\"\n            )\n            self.output_data_objects[SilverCellIntersectionGroupsDataObject.ID] = (\n                SilverCellIntersectionGroupsDataObject(\n                    self.spark,\n                    self.silver_cell_intersection_groups_path,\n                    partition_columns=[ColNames.year, ColNames.month, ColNames.day],\n                )\n            )\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n\n        # TODO: We might need to iterate over dates and process the data for each date separately\n        # have to do tests with larger datasets.\n\n        current_signal_strength_sdf = self.input_data_objects[SilverSignalStrengthDataObject.ID].df.filter(\n            F.make_date(\"year\", \"month\", \"day\").isin(self.data_period_dates)\n        )\n\n        # Calculate the cell signal dominance\n        cell_footprint_sdf = self.signal_strength_to_signal_dominance(\n            current_signal_strength_sdf, self.logistic_function_steepness, self.logistic_function_midpoint\n        )\n\n        # Prune max cells per grid tile\n        if self.do_max_cells_per_tile_prunning:\n            cell_footprint_sdf = self.prune_max_cells_per_grid_tile(cell_footprint_sdf, self.max_cells_per_grid_tile)\n\n        # Prune signal dominance difference from best\n        if self.do_difference_from_best_sd_prunning:\n            cell_footprint_sdf = self.prune_signal_difference_from_best(\n                cell_footprint_sdf, self.difference_from_best_sd_treshold\n            )\n\n        # Prune small signal dominance values\n        if self.do_sd_treshold_prunning:\n            cell_footprint_sdf = self.prune_small_signal_dominance(cell_footprint_sdf, self.signal_dominance_treshold)\n\n        cell_footprint_sdf = cell_footprint_sdf.select(SilverCellFootprintDataObject.MANDATORY_COLUMNS)\n        columns = {\n            field.name: F.col(field.name).cast(field.dataType) for field in SilverCellFootprintDataObject.SCHEMA.fields\n        }\n\n        cell_footprint_sdf = cell_footprint_sdf.withColumns(columns)\n        self.output_data_objects[SilverCellFootprintDataObject.ID].df = cell_footprint_sdf\n\n        if self.do_do_cell_intersection_groups_calculation:\n\n            cell_intersection_groups_sdf = self.calculate_intersection_groups(cell_footprint_sdf)\n            cell_intersection_groups_sdf = self.calculate_all_intersection_combinations(cell_intersection_groups_sdf)\n\n            cell_intersection_groups_sdf = cell_intersection_groups_sdf.select(\n                SilverCellIntersectionGroupsDataObject.MANDATORY_COLUMNS\n            )\n            columns = {\n                field.name: F.col(field.name).cast(field.dataType)\n                for field in SilverCellIntersectionGroupsDataObject.SCHEMA.fields\n            }\n            cell_intersection_groups_sdf = cell_intersection_groups_sdf.withColumns(columns)\n            self.output_data_objects[SilverCellIntersectionGroupsDataObject.ID].df = cell_intersection_groups_sdf\n\n    @staticmethod\n    def signal_strength_to_signal_dominance(\n        sdf: DataFrame, logistic_function_steepness: float, logistic_function_midpoint: float\n    ) -&gt; DataFrame:\n        \"\"\"\n        Converts signal strength to signal dominance using a logistic function.\n        Methodology from A Bayesian approach to location estimation of mobile devices\n        from mobile network operator data. Tennekes and Gootzen (2022).\n\n        The logistic function is defined as 1 / (1 + exp(-scale)),\n        where scale is (signal_strength - logistic_function_midpoint) * logistic_function_steepness.\n\n        Parameters:\n        sdf (DataFrame): SignalStrenghtDataObject Spark DataFrame.\n        logistic_function_steepness (float): The steepness parameter for the logistic function.\n        logistic_function_midpoint (float): The midpoint parameter for the logistic function.\n\n        Returns:\n        DataFrame: A Spark DataFrame with the signal dominance added as a new column.\n        \"\"\"\n        sdf = sdf.withColumn(\n            \"scale\", (F.col(ColNames.signal_strength) - logistic_function_midpoint) * logistic_function_steepness\n        )\n        sdf = sdf.withColumn(ColNames.signal_dominance, 1 / (1 + F.exp(-F.col(\"scale\"))))\n        sdf = sdf.drop(\"scale\")\n\n        return sdf\n\n    @staticmethod\n    def prune_small_signal_dominance(sdf: DataFrame, signal_dominance_threshold: float) -&gt; DataFrame:\n        \"\"\"\n        Prunes rows from the DataFrame where the signal dominance is less than or equal to the provided threshold.\n\n        Parameters:\n        sdf (DataFrame): A Spark DataFrame containing the signal dominance data.\n        signal_dominance_threshold (float): The threshold for pruning small signal dominance values.\n\n        Returns:\n        DataFrame: A DataFrame with rows having signal dominance less than or equal to the threshold removed.\n        \"\"\"\n        sdf = sdf.filter(F.col(ColNames.signal_dominance) &gt; signal_dominance_threshold)\n        return sdf\n\n    @staticmethod\n    def prune_max_cells_per_grid_tile(sdf: DataFrame, max_cells_per_grid_tile: int) -&gt; DataFrame:\n        \"\"\"\n        Prunes rows from the DataFrame exceeding the maximum number of cells allowed per grid tile.\n\n        The rows are ordered by signal dominance in descending order,\n        and only the top 'max_cells_per_grid_tile' rows are kept for each grid tile.\n\n        Parameters:\n        sdf (DataFrame): A Spark DataFrame containing the signal dominance data.\n        max_cells_per_grid_tile (int): The maximum number of cells allowed per grid tile.\n\n        Returns:\n        DataFrame: A DataFrame with rows exceeding the maximum number of cells per grid tile removed.\n        \"\"\"\n        window = Window.partitionBy(ColNames.year, ColNames.month, ColNames.day, ColNames.grid_id).orderBy(\n            F.desc(ColNames.signal_dominance)\n        )\n\n        sdf = sdf.withColumn(\"row_number\", F.row_number().over(window))\n        sdf = sdf.filter(F.col(\"row_number\") &lt;= max_cells_per_grid_tile)\n        sdf = sdf.drop(\"row_number\")\n\n        return sdf\n\n    @staticmethod\n    def prune_signal_difference_from_best(sdf: DataFrame, difference_threshold: float) -&gt; DataFrame:\n        \"\"\"\n        Prunes rows from the DataFrame based on a threshold of signal dominance difference.\n\n        The rows are ordered by signal dominance in descending order, and only the rows where the difference\n        in signal dominance from the maximum is less than the threshold are kept for each grid tile.\n\n        Parameters:\n        sdf (DataFrame): A Spark DataFrame containing the signal dominance data.\n        threshold (float): The threshold for signal dominance difference in percentage.\n\n        Returns:\n        DataFrame: A DataFrame with rows pruned based on the signal dominance difference threshold.\n        \"\"\"\n        window = Window.partitionBy(ColNames.year, ColNames.month, ColNames.day, ColNames.grid_id).orderBy(\n            F.desc(ColNames.signal_dominance)\n        )\n\n        sdf = sdf.withColumn(\"row_number\", F.row_number().over(window))\n        # TODO: Check: Could use F.first instead of F.max as the window is sorted?\n        sdf = sdf.withColumn(\"max_signal_dominance\", F.max(ColNames.signal_dominance).over(window))\n        sdf = sdf.withColumn(\n            \"signal_dominance_diff_percentage\",\n            (sdf[\"max_signal_dominance\"] - sdf[ColNames.signal_dominance]) / sdf[\"max_signal_dominance\"] * 100,\n        )\n\n        sdf = sdf.filter(\n            (F.col(\"row_number\") == 1) | (F.col(\"signal_dominance_diff_percentage\") &lt;= difference_threshold)\n        )\n\n        sdf = sdf.drop(\"row_number\", \"max_signal_dominance\", \"signal_dominance_diff_percentage\")\n\n        return sdf\n\n    @staticmethod\n    def calculate_intersection_groups(sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Calculates the cell intersection groups based on cell footprints overlaps\n        over grid tiles.\n\n        Parameters:\n        sdf (DataFrame): A Spark DataFrame containing the signal dominance data.\n\n        Returns:\n        DataFrame: A DataFrame with the intersection groups.\n        \"\"\"\n        intersections_sdf = sdf.groupBy(\n            F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day), F.col(ColNames.grid_id)\n        ).agg(F.array_sort(F.collect_set(ColNames.cell_id)).alias(ColNames.cells))\n\n        intersections_sdf = intersections_sdf.withColumn(ColNames.group_size, F.size(F.col(ColNames.cells)))\n        intersections_sdf = intersections_sdf.filter(F.col(ColNames.group_size) &gt; 1)\n\n        return intersections_sdf\n\n    @staticmethod\n    def calculate_all_intersection_combinations(sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Calculates all possible combinations of intersection groups.\n        It is necessary to extract all overlap combinations from every\n        intersection group. If there is an intersection group ABC intersection\n        groups AB, AC, BC also has to be present.\n\n        Parameters:\n        sdf (DataFrame): A Spark DataFrame containing the intersection groups data.\n\n        Returns:\n        DataFrame: A DataFrame with all possible combinations of intersection groups for each grid tile.\n        \"\"\"\n        combinations_sdf = []\n        max_level = sdf.agg(F.max(ColNames.group_size).alias(\"max\")).collect()[0][\"max\"]\n\n        for level in range(2, max_level + 1):\n\n            combinations_udf = CellFootprintEstimation.generate_combinations(level)\n\n            combinations_sdf_level = (\n                sdf.filter(F.col(ColNames.group_size) &gt;= level)\n                .withColumn(ColNames.cells, F.explode(combinations_udf(F.col(ColNames.cells))))\n                .select(ColNames.cells, ColNames.year, ColNames.month, ColNames.day)\n            )\n\n            combinations_sdf_level = combinations_sdf_level.withColumn(\n                ColNames.cells, F.array_sort(F.col(ColNames.cells))\n            )\n            combinations_sdf_level = combinations_sdf_level.drop_duplicates([ColNames.cells])\n\n            combinations_sdf_level = combinations_sdf_level.withColumn(ColNames.group_size, F.lit(level))\n\n            window = Window.partitionBy(ColNames.year, ColNames.month, ColNames.day, ColNames.group_size).orderBy(\n                F.col(ColNames.group_size)\n            )\n\n            combinations_sdf_level = combinations_sdf_level.withColumn(\n                ColNames.group_id, F.concat(F.col(ColNames.group_size), F.lit(\"_\"), F.row_number().over(window))\n            )\n\n            combinations_sdf.append(combinations_sdf_level)\n\n        return reduce(DataFrame.unionAll, combinations_sdf)\n\n    @staticmethod\n    def generate_combinations(level):\n        def combinations_udf(arr):\n            return list(combinations(arr, level))\n\n        return F.udf(combinations_udf, ArrayType(ArrayType(StringType())))\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.calculate_all_intersection_combinations","title":"<code>calculate_all_intersection_combinations(sdf)</code>  <code>staticmethod</code>","text":"<p>Calculates all possible combinations of intersection groups. It is necessary to extract all overlap combinations from every intersection group. If there is an intersection group ABC intersection groups AB, AC, BC also has to be present.</p> <p>Parameters: sdf (DataFrame): A Spark DataFrame containing the intersection groups data.</p> <p>Returns: DataFrame: A DataFrame with all possible combinations of intersection groups for each grid tile.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef calculate_all_intersection_combinations(sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Calculates all possible combinations of intersection groups.\n    It is necessary to extract all overlap combinations from every\n    intersection group. If there is an intersection group ABC intersection\n    groups AB, AC, BC also has to be present.\n\n    Parameters:\n    sdf (DataFrame): A Spark DataFrame containing the intersection groups data.\n\n    Returns:\n    DataFrame: A DataFrame with all possible combinations of intersection groups for each grid tile.\n    \"\"\"\n    combinations_sdf = []\n    max_level = sdf.agg(F.max(ColNames.group_size).alias(\"max\")).collect()[0][\"max\"]\n\n    for level in range(2, max_level + 1):\n\n        combinations_udf = CellFootprintEstimation.generate_combinations(level)\n\n        combinations_sdf_level = (\n            sdf.filter(F.col(ColNames.group_size) &gt;= level)\n            .withColumn(ColNames.cells, F.explode(combinations_udf(F.col(ColNames.cells))))\n            .select(ColNames.cells, ColNames.year, ColNames.month, ColNames.day)\n        )\n\n        combinations_sdf_level = combinations_sdf_level.withColumn(\n            ColNames.cells, F.array_sort(F.col(ColNames.cells))\n        )\n        combinations_sdf_level = combinations_sdf_level.drop_duplicates([ColNames.cells])\n\n        combinations_sdf_level = combinations_sdf_level.withColumn(ColNames.group_size, F.lit(level))\n\n        window = Window.partitionBy(ColNames.year, ColNames.month, ColNames.day, ColNames.group_size).orderBy(\n            F.col(ColNames.group_size)\n        )\n\n        combinations_sdf_level = combinations_sdf_level.withColumn(\n            ColNames.group_id, F.concat(F.col(ColNames.group_size), F.lit(\"_\"), F.row_number().over(window))\n        )\n\n        combinations_sdf.append(combinations_sdf_level)\n\n    return reduce(DataFrame.unionAll, combinations_sdf)\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.calculate_intersection_groups","title":"<code>calculate_intersection_groups(sdf)</code>  <code>staticmethod</code>","text":"<p>Calculates the cell intersection groups based on cell footprints overlaps over grid tiles.</p> <p>Parameters: sdf (DataFrame): A Spark DataFrame containing the signal dominance data.</p> <p>Returns: DataFrame: A DataFrame with the intersection groups.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef calculate_intersection_groups(sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Calculates the cell intersection groups based on cell footprints overlaps\n    over grid tiles.\n\n    Parameters:\n    sdf (DataFrame): A Spark DataFrame containing the signal dominance data.\n\n    Returns:\n    DataFrame: A DataFrame with the intersection groups.\n    \"\"\"\n    intersections_sdf = sdf.groupBy(\n        F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day), F.col(ColNames.grid_id)\n    ).agg(F.array_sort(F.collect_set(ColNames.cell_id)).alias(ColNames.cells))\n\n    intersections_sdf = intersections_sdf.withColumn(ColNames.group_size, F.size(F.col(ColNames.cells)))\n    intersections_sdf = intersections_sdf.filter(F.col(ColNames.group_size) &gt; 1)\n\n    return intersections_sdf\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.prune_max_cells_per_grid_tile","title":"<code>prune_max_cells_per_grid_tile(sdf, max_cells_per_grid_tile)</code>  <code>staticmethod</code>","text":"<p>Prunes rows from the DataFrame exceeding the maximum number of cells allowed per grid tile.</p> <p>The rows are ordered by signal dominance in descending order, and only the top 'max_cells_per_grid_tile' rows are kept for each grid tile.</p> <p>Parameters: sdf (DataFrame): A Spark DataFrame containing the signal dominance data. max_cells_per_grid_tile (int): The maximum number of cells allowed per grid tile.</p> <p>Returns: DataFrame: A DataFrame with rows exceeding the maximum number of cells per grid tile removed.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef prune_max_cells_per_grid_tile(sdf: DataFrame, max_cells_per_grid_tile: int) -&gt; DataFrame:\n    \"\"\"\n    Prunes rows from the DataFrame exceeding the maximum number of cells allowed per grid tile.\n\n    The rows are ordered by signal dominance in descending order,\n    and only the top 'max_cells_per_grid_tile' rows are kept for each grid tile.\n\n    Parameters:\n    sdf (DataFrame): A Spark DataFrame containing the signal dominance data.\n    max_cells_per_grid_tile (int): The maximum number of cells allowed per grid tile.\n\n    Returns:\n    DataFrame: A DataFrame with rows exceeding the maximum number of cells per grid tile removed.\n    \"\"\"\n    window = Window.partitionBy(ColNames.year, ColNames.month, ColNames.day, ColNames.grid_id).orderBy(\n        F.desc(ColNames.signal_dominance)\n    )\n\n    sdf = sdf.withColumn(\"row_number\", F.row_number().over(window))\n    sdf = sdf.filter(F.col(\"row_number\") &lt;= max_cells_per_grid_tile)\n    sdf = sdf.drop(\"row_number\")\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.prune_signal_difference_from_best","title":"<code>prune_signal_difference_from_best(sdf, difference_threshold)</code>  <code>staticmethod</code>","text":"<p>Prunes rows from the DataFrame based on a threshold of signal dominance difference.</p> <p>The rows are ordered by signal dominance in descending order, and only the rows where the difference in signal dominance from the maximum is less than the threshold are kept for each grid tile.</p> <p>Parameters: sdf (DataFrame): A Spark DataFrame containing the signal dominance data. threshold (float): The threshold for signal dominance difference in percentage.</p> <p>Returns: DataFrame: A DataFrame with rows pruned based on the signal dominance difference threshold.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef prune_signal_difference_from_best(sdf: DataFrame, difference_threshold: float) -&gt; DataFrame:\n    \"\"\"\n    Prunes rows from the DataFrame based on a threshold of signal dominance difference.\n\n    The rows are ordered by signal dominance in descending order, and only the rows where the difference\n    in signal dominance from the maximum is less than the threshold are kept for each grid tile.\n\n    Parameters:\n    sdf (DataFrame): A Spark DataFrame containing the signal dominance data.\n    threshold (float): The threshold for signal dominance difference in percentage.\n\n    Returns:\n    DataFrame: A DataFrame with rows pruned based on the signal dominance difference threshold.\n    \"\"\"\n    window = Window.partitionBy(ColNames.year, ColNames.month, ColNames.day, ColNames.grid_id).orderBy(\n        F.desc(ColNames.signal_dominance)\n    )\n\n    sdf = sdf.withColumn(\"row_number\", F.row_number().over(window))\n    # TODO: Check: Could use F.first instead of F.max as the window is sorted?\n    sdf = sdf.withColumn(\"max_signal_dominance\", F.max(ColNames.signal_dominance).over(window))\n    sdf = sdf.withColumn(\n        \"signal_dominance_diff_percentage\",\n        (sdf[\"max_signal_dominance\"] - sdf[ColNames.signal_dominance]) / sdf[\"max_signal_dominance\"] * 100,\n    )\n\n    sdf = sdf.filter(\n        (F.col(\"row_number\") == 1) | (F.col(\"signal_dominance_diff_percentage\") &lt;= difference_threshold)\n    )\n\n    sdf = sdf.drop(\"row_number\", \"max_signal_dominance\", \"signal_dominance_diff_percentage\")\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.prune_small_signal_dominance","title":"<code>prune_small_signal_dominance(sdf, signal_dominance_threshold)</code>  <code>staticmethod</code>","text":"<p>Prunes rows from the DataFrame where the signal dominance is less than or equal to the provided threshold.</p> <p>Parameters: sdf (DataFrame): A Spark DataFrame containing the signal dominance data. signal_dominance_threshold (float): The threshold for pruning small signal dominance values.</p> <p>Returns: DataFrame: A DataFrame with rows having signal dominance less than or equal to the threshold removed.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef prune_small_signal_dominance(sdf: DataFrame, signal_dominance_threshold: float) -&gt; DataFrame:\n    \"\"\"\n    Prunes rows from the DataFrame where the signal dominance is less than or equal to the provided threshold.\n\n    Parameters:\n    sdf (DataFrame): A Spark DataFrame containing the signal dominance data.\n    signal_dominance_threshold (float): The threshold for pruning small signal dominance values.\n\n    Returns:\n    DataFrame: A DataFrame with rows having signal dominance less than or equal to the threshold removed.\n    \"\"\"\n    sdf = sdf.filter(F.col(ColNames.signal_dominance) &gt; signal_dominance_threshold)\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.signal_strength_to_signal_dominance","title":"<code>signal_strength_to_signal_dominance(sdf, logistic_function_steepness, logistic_function_midpoint)</code>  <code>staticmethod</code>","text":"<p>Converts signal strength to signal dominance using a logistic function. Methodology from A Bayesian approach to location estimation of mobile devices from mobile network operator data. Tennekes and Gootzen (2022).</p> <p>The logistic function is defined as 1 / (1 + exp(-scale)), where scale is (signal_strength - logistic_function_midpoint) * logistic_function_steepness.</p> <p>Parameters: sdf (DataFrame): SignalStrenghtDataObject Spark DataFrame. logistic_function_steepness (float): The steepness parameter for the logistic function. logistic_function_midpoint (float): The midpoint parameter for the logistic function.</p> <p>Returns: DataFrame: A Spark DataFrame with the signal dominance added as a new column.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef signal_strength_to_signal_dominance(\n    sdf: DataFrame, logistic_function_steepness: float, logistic_function_midpoint: float\n) -&gt; DataFrame:\n    \"\"\"\n    Converts signal strength to signal dominance using a logistic function.\n    Methodology from A Bayesian approach to location estimation of mobile devices\n    from mobile network operator data. Tennekes and Gootzen (2022).\n\n    The logistic function is defined as 1 / (1 + exp(-scale)),\n    where scale is (signal_strength - logistic_function_midpoint) * logistic_function_steepness.\n\n    Parameters:\n    sdf (DataFrame): SignalStrenghtDataObject Spark DataFrame.\n    logistic_function_steepness (float): The steepness parameter for the logistic function.\n    logistic_function_midpoint (float): The midpoint parameter for the logistic function.\n\n    Returns:\n    DataFrame: A Spark DataFrame with the signal dominance added as a new column.\n    \"\"\"\n    sdf = sdf.withColumn(\n        \"scale\", (F.col(ColNames.signal_strength) - logistic_function_midpoint) * logistic_function_steepness\n    )\n    sdf = sdf.withColumn(ColNames.signal_dominance, 1 / (1 + F.exp(-F.col(\"scale\"))))\n    sdf = sdf.drop(\"scale\")\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/","title":"daily_permanence_score","text":""},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/","title":"daily_permanence_score","text":"<p>Module that implements the Daily Permanence Score functionality</p>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore","title":"<code>DailyPermanenceScore</code>","text":"<p>             Bases: <code>Component</code></p> <p>A class to calculate the daily permanence score of each user per interval and grid tile.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>class DailyPermanenceScore(Component):\n    \"\"\"\n    A class to calculate the daily permanence score of each user per interval and grid tile.\n    \"\"\"\n\n    COMPONENT_ID = \"DailyPermanenceScore\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.data_period_start = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n        self.data_period_end = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n\n        self.time_slot_number = self.config.getint(self.COMPONENT_ID, \"time_slot_number\")\n\n        self.max_time_thresh = timedelta(seconds=self.config.getint(self.COMPONENT_ID, \"max_time_thresh\"))\n        self.max_time_thresh_day = timedelta(seconds=self.config.getint(self.COMPONENT_ID, \"max_time_thresh_day\"))\n        self.max_time_thresh_night = timedelta(seconds=self.config.getint(self.COMPONENT_ID, \"max_time_thresh_night\"))\n        self.max_speed_thresh = self.config.getfloat(self.COMPONENT_ID, \"max_speed_thresh\")\n\n        self.score_interval = self.config.getint(self.COMPONENT_ID, \"score_interval\")\n        self.event_error_flags_to_include = self.config.geteval(self.COMPONENT_ID, \"event_error_flags_to_include\")\n\n        self.data_period_dates = [\n            self.data_period_start + timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n        self.current_date = None\n        self.previous_date = None\n        self.next_date = None\n\n        self.current_events = None\n        self.previous_events = None\n        self.next_events = None\n\n        self.current_cell_footprint = None\n        self.previous_cell_footprint = None\n        self.next_cell_footprint = None\n\n    def initalize_data_objects(self):\n        input_events_silver_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"event_data_silver_flagged\")\n        input_cell_footprint_silver_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"cell_footprint_data_silver\")\n        output_dps_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"daily_permanence_score_data_silver\")\n\n        silver_events = SilverEventFlaggedDataObject(self.spark, input_events_silver_path)\n\n        silver_cell_footprint = SilverCellFootprintDataObject(\n            self.spark,\n            input_cell_footprint_silver_path,\n            partition_columns=[ColNames.year, ColNames.month, ColNames.day],\n        )\n\n        silver_dps = SilverDailyPermanenceScoreDataObject(\n            self.spark,\n            output_dps_path,\n        )\n\n        self.input_data_objects = {silver_events.ID: silver_events, silver_cell_footprint.ID: silver_cell_footprint}\n\n        self.output_data_objects = {silver_dps.ID: silver_dps}\n\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n        self.check_needed_dates()\n\n        for current_date in self.data_period_dates:\n\n            self.logger.info(f\"Processing events for {current_date.strftime('%Y-%m-%d')}\")\n\n            self.current_date = current_date\n            self.previous_date = current_date - timedelta(days=1)\n            self.next_date = current_date + timedelta(days=1)\n\n            self.current_events = self.filter_events(self.current_date)\n            self.previous_events = self.filter_events(self.previous_date)\n            self.next_events = self.filter_events(self.next_date)\n\n            self.current_cell_footprint = self.filter_cell_footprint(self.current_date)\n            self.previous_cell_footprint = self.filter_cell_footprint(self.previous_date)\n            self.next_cell_footprint = self.filter_cell_footprint(self.next_date)\n\n            self.transform()\n\n            self.write()\n\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def check_needed_dates(self):\n        \"\"\"\n        Method that checks if both the dates of study and the dates necessary to generate\n        the daily permanence scores are present in the input data (events + cell footprint).\n\n        Raises:\n            ValueError: If there is no data for one or more of the needed dates.\n        \"\"\"\n        # needed dates: for each date D, we also need D-1 and D+1\n        # this is built this way so it would also support definition of study\n        # dates that are not consecutive\n        needed_dates = (\n            {d + timedelta(days=1) for d in self.data_period_dates}\n            | set(self.data_period_dates)\n            | {d - timedelta(days=1) for d in self.data_period_dates}\n        )\n\n        # Assert needed dates in event data:\n        self.assert_needed_dates_data_object(SilverEventFlaggedDataObject.ID, needed_dates)\n\n        # Assert needed dates in cell footprint data:\n        self.assert_needed_dates_data_object(SilverCellFootprintDataObject.ID, needed_dates)\n\n    def assert_needed_dates_data_object(self, data_object_id: str, needed_dates: list[datetime]):\n        \"\"\"\n        Method that checks if data for a set of dates exists for a data object.\n\n        Args:\n            data_object_id (str): name of the data object to check.\n            needed_dates (list[datetime]): list of the dates for which data shall be available.\n\n        Raises:\n            ValueError: If there is no data for one or more of the needed dates.\n        \"\"\"\n        # Load data\n        df = self.input_data_objects[data_object_id].df\n\n        # Find dates that match the needed dates:\n        dates = (\n            df.withColumn(ColNames.date, F.make_date(ColNames.year, ColNames.month, ColNames.day))\n            .select(F.col(ColNames.date))\n            .filter(F.col(ColNames.date).isin(needed_dates))\n            .distinct()\n            .collect()\n        )\n        available_dates = {row[ColNames.date] for row in dates}\n\n        # If missing needed dates, raise error:\n        missing_dates = needed_dates.difference(available_dates)\n        if missing_dates:\n            error_msg = f\"Missing {data_object_id} data for dates {sorted(list(missing_dates))}\"\n            self.logger.error(error_msg)\n            raise ValueError(error_msg)\n\n    def filter_events(self, current_date: date) -&gt; DataFrame:\n        \"\"\"\n        Load events with no errors for a specific date.\n\n        Args:\n            current_date (date): current date.\n\n        Returns:\n            DataFrame: filtered events dataframe.\n        \"\"\"\n        return self.input_data_objects[SilverEventFlaggedDataObject.ID].df.filter(\n            (F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(current_date))\n            &amp; (F.col(ColNames.error_flag).isin(self.event_error_flags_to_include))\n        )\n\n    def filter_cell_footprint(self, current_date: date) -&gt; DataFrame:\n        \"\"\"\n        Load cell footprints for a specific date.\n\n        Args:\n            current_date (date): current date.\n\n        Returns:\n            DataFrame: filtered cell footprint dataframe.\n        \"\"\"\n        return self.input_data_objects[SilverCellFootprintDataObject.ID].df.filter(\n            (F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(current_date))\n        )\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n\n        # load users events (dates D-1, D, D+1):\n        events = self.build_events_table()\n\n        # load cell footprint (dates D-1, D, D+1):\n        cell_footprint = self.build_cell_footprint_table()\n\n        # build time slots dataframe (date D):\n        time_slots = self.build_time_slots_table()\n\n        # differentiate 'move' events:\n        events = self.detect_move_events(events, cell_footprint)\n\n        # Determine stay durations:\n        stays = self.determine_stay_durations(events)\n\n        # Assign stay time slot, assign duration to time slots and map to calculate DPS:\n        dps = self.calculate_dps(stays, time_slots)\n\n        self.output_data_objects[SilverDailyPermanenceScoreDataObject.ID].df = dps\n\n    def build_events_table(self) -&gt; DataFrame:\n        \"\"\"\n        Load events data for date D, also adding last event of each\n        user from date D-1 and first event of each user from D+1.\n\n        Returns:\n            DataFrame: events dataframe.\n        \"\"\"\n        # reach last event from previous day:\n        window = Window.partitionBy(ColNames.user_id).orderBy(F.desc(ColNames.timestamp))\n        self.previous_events = (\n            self.previous_events.withColumn(\"row_number\", F.row_number().over(window))\n            .filter(F.col(\"row_number\") == 1)\n            .drop(\"row_number\")\n        )\n\n        # reach first event from next day:\n        window = Window.partitionBy(ColNames.user_id).orderBy(ColNames.timestamp)\n        self.next_events = (\n            self.next_events.withColumn(\"row_number\", F.row_number().over(window))\n            .filter(F.col(\"row_number\") == 1)\n            .drop(\"row_number\")\n        )\n\n        # concat all events together (last of D-1 + all D + first of D+1):\n        events = (\n            self.previous_events.union(self.current_events)\n            .union(self.next_events)\n            .drop(ColNames.longitude, ColNames.latitude, ColNames.loc_error, ColNames.error_flag)\n        )\n\n        return events\n\n    def build_cell_footprint_table(self) -&gt; DataFrame:\n        \"\"\"\n        Load cell footprint data for dates D-1, D and D+1.\n\n        Returns:\n            DataFrame: cell footprint dataframe.\n        \"\"\"\n        cell_footprint = (\n            self.previous_cell_footprint.union(self.current_cell_footprint)\n            .union(self.next_cell_footprint)\n            .groupBy([ColNames.cell_id, ColNames.year, ColNames.month, ColNames.day])\n            .agg(F.collect_list(ColNames.grid_id).alias(\"grid_ids\"))\n        )\n\n        return cell_footprint\n\n    def build_time_slots_table(self) -&gt; DataFrame:\n        \"\"\"\n        Build a dataframe with the specified time slots for the current date.\n\n        Returns:\n            DataFrame: time slots dataframe.\n        \"\"\"\n        time_slot_length = timedelta(days=1) / self.time_slot_number\n\n        time_slots_list = []\n        previous_end_time = datetime(\n            year=self.current_date.year,\n            month=self.current_date.month,\n            day=self.current_date.day,\n            hour=0,\n            minute=0,\n            second=0,\n        )\n\n        while previous_end_time.date() == self.current_date:\n            init_time = previous_end_time\n            end_time = init_time + time_slot_length\n            time_slot = (init_time, end_time)\n            time_slots_list.append(time_slot)\n            previous_end_time = end_time\n\n        schema = StructType(\n            [\n                StructField(ColNames.time_slot_initial_time, TimestampType(), True),\n                StructField(\"time_slot_end_time\", TimestampType(), True),\n            ]\n        )\n\n        return self.spark.createDataFrame(time_slots_list, schema=schema)\n\n    def detect_move_events(self, events: DataFrame, cell_footprint: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Detect which of the events are associated to moves according to the\n        distances/times from previous to posterior event and a speed threshold.\n\n        Args:\n            events (DataFrame): events dataframe.\n            cell_footprint (DataFrame): cells footprint dataframe.\n\n        Returns:\n            DataFrame: events dataframe, with an additional 'is_move' boolean column.\n        \"\"\"\n        # left join -&gt; bring cell footprints to events data:\n        events = events.join(\n            cell_footprint,\n            (events[ColNames.cell_id] == cell_footprint[ColNames.cell_id])\n            &amp; (events[ColNames.year] == cell_footprint[ColNames.year])\n            &amp; (events[ColNames.month] == cell_footprint[ColNames.month])\n            &amp; (events[ColNames.day] == cell_footprint[ColNames.day]),\n            \"left\",\n        ).drop(\n            cell_footprint[ColNames.cell_id],\n            cell_footprint[ColNames.year],\n            cell_footprint[ColNames.month],\n            cell_footprint[ColNames.day],\n        )\n\n        # Add lags of timestamp, cell_id and grid_ids:\n        window = Window.partitionBy(ColNames.user_id).orderBy(ColNames.timestamp)\n        lag_fields = [ColNames.timestamp, ColNames.cell_id, \"grid_ids\"]\n        for lf in lag_fields:\n            events = events.withColumn(f\"{lf}_+1\", F.lag(lf, -1).over(window)).withColumn(\n                f\"{lf}_-1\", F.lag(lf, 1).over(window)\n            )\n\n        # Calculate distance between grid tiles associated to events -1, 0 and +1:\n        # Calculate speeds and determine which rows are moves:\n        events = (\n            events.withColumn(\"dist_0_+1\", self.grid_footprint_distance(F.col(\"grid_ids\"), F.col(\"grid_ids_+1\")))\n            .withColumn(\"dist_-1_0\", self.grid_footprint_distance(F.col(\"grid_ids_-1\"), F.col(\"grid_ids\")))\n            .withColumn(\"dist_-1_+1\", self.grid_footprint_distance(F.col(\"grid_ids_-1\"), F.col(\"grid_ids_+1\")))\n            .withColumn(\n                \"time_difference\",\n                F.unix_timestamp(events[f\"{ColNames.timestamp}_+1\"])\n                - F.unix_timestamp(events[f\"{ColNames.timestamp}_-1\"]),\n            )\n            .withColumn(\"max_dist\", F.greatest(F.col(\"dist_-1_0\") + F.col(\"dist_0_+1\"), F.col(\"dist_-1_+1\")))\n            .withColumn(\"speed\", F.col(\"max_dist\") / F.col(\"time_difference\"))\n            .withColumn(\"is_move\", F.when(F.col(\"speed\") &gt; self.max_speed_thresh, True).otherwise(False))\n            .drop(\n                \"dist_0_+1\",\n                \"dist_-1_0\",\n                \"dist_-1_+1\",\n                \"grid_ids_+1\",\n                \"grid_ids_-1\",\n                \"time_difference\",\n                \"max_dist\",\n                \"speed\",\n            )\n        )\n\n        return events\n\n    @staticmethod\n    def get_grid_id_size_meters(grid_id: str) -&gt; float:\n        \"\"\"\n        Reach the size of a grid tile from its identifier.\n\n        Args:\n            grid_id (str): ID of the corresponding grid tile.\n\n        Returns:\n            float: grid tile size, in meters.\n        \"\"\"\n        size_txt = re.search(r\"^.*m\", grid_id)[0]\n        if size_txt[-2:] == \"km\":\n            size = float(size_txt[:-2]) * 1000\n        else:\n            size = float(size_txt[:-1])\n        return size\n\n    @staticmethod\n    def get_grid_id_vertices(grid_id: str) -&gt; list[tuple[float]]:\n        \"\"\"\n        Obtain the coordinates of the vertices of a given grid tile.\n\n        Args:\n            grid_id (str): ID of the corresponding grid tile.\n\n        Returns:\n            list[tuple[float]]: list of tuples. Each tuple contains the x, y\n                coordinates of a vertex of the corresponding grid tile.\n        \"\"\"\n        size = DailyPermanenceScore.get_grid_id_size_meters(grid_id)\n        xleft = float(re.search(r\"E\\s*(\\d+)\", grid_id)[0][1:])\n        ybottom = float(re.search(r\"N\\s*(\\d+)\", grid_id)[0][1:])\n        xright = xleft + size\n        ytop = ybottom + size\n        xs = (xleft, xright)\n        ys = (ybottom, ytop)\n        vertices = list(itertools.product(xs, ys))\n        return vertices\n\n    @staticmethod\n    def calculate_min_distance_between_point_lists(points_i: set[tuple[float]], points_j: set[tuple[float]]) -&gt; float:\n        \"\"\"\n        Calculate minimum distance between the points in one list and the points\n        in another list.\n\n        Args:\n            points_i (set[tuple[float]]): set of tuples. Each tuple contains\n                the x, y coordinates of a point.\n            points_j (set[tuple[float]]): set of tuples. Each tuple contains\n                the x, y coordinates of a point.\n\n        Returns:\n            float: minimum distance between points in both lists.\n        \"\"\"\n        if points_i &amp; points_j:  # same point is included in both sets\n            return 0.0\n        min_distance = float(\"inf\")\n        for pi in points_i:\n            for pj in points_j:\n                distance = (pi[0] - pj[0]) ** 2 + (pi[1] - pj[1]) ** 2\n                if distance &lt; min_distance:\n                    min_distance = distance\n        return min_distance**0.5\n\n    @staticmethod\n    @F.udf(returnType=FloatType())\n    def grid_footprint_distance(grid_ids_i: list[str], grid_ids_j: list[str]) -&gt; float:\n        \"\"\"\n        Calculate minimum distance between the grid tiles in one list and the\n        grid tiles in another list, provided as grid tile IDs.\n\n        Args:\n            grid_ids_i (list[str]): IDs of the corresponding grid tiles.\n            grid_ids_j (list[str]): IDs of the corresponding grid tiles.\n\n        Returns:\n            float: minimum distance.\n        \"\"\"\n        # TODO: optimise\n        if not grid_ids_i or not grid_ids_j:\n            return None\n\n        if set(grid_ids_i) == set(grid_ids_j):\n            return 0.0\n\n        gi_vertices = set()\n        for gi in grid_ids_i:\n            gi_vertices |= set(DailyPermanenceScore.get_grid_id_vertices(gi))\n        gj_vertices = set()\n        for gj in grid_ids_j:\n            gj_vertices |= set(DailyPermanenceScore.get_grid_id_vertices(gj))\n\n        min_distance = DailyPermanenceScore.calculate_min_distance_between_point_lists(gi_vertices, gj_vertices)\n\n        return min_distance\n\n    def determine_stay_durations(self, events: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Determine the start time and end time for each stay event.\n\n        Args:\n            events (DataFrame): events dataframe.\n\n        Returns:\n            DataFrame: stays dataframe (filtering out moves).\n        \"\"\"\n        current_datetime = datetime(self.current_date.year, self.current_date.month, self.current_date.day)\n        night_start_time = current_datetime - timedelta(hours=1)\n        night_end_time = current_datetime + timedelta(hours=9)\n\n        stays = (\n            events\n            # Set applicable time thresholds:\n            .withColumn(\n                \"threshold_-1\",\n                F.when(\n                    (F.col(\"cell_id\") == F.col(\"cell_id_-1\"))\n                    &amp; (F.col(\"timestamp_-1\") &gt;= night_start_time)\n                    &amp; (F.col(\"timestamp\") &lt;= night_end_time),\n                    self.max_time_thresh_night,\n                ).otherwise(\n                    F.when(F.col(\"cell_id\") == F.col(\"cell_id_-1\"), self.max_time_thresh_day).otherwise(\n                        self.max_time_thresh\n                    )\n                ),\n            )\n            .withColumn(\n                \"threshold_+1\",\n                F.when(\n                    (F.col(\"cell_id\") == F.col(\"cell_id_+1\"))\n                    &amp; (F.col(\"timestamp\") &gt;= night_start_time)\n                    &amp; (F.col(\"timestamp_+1\") &lt;= night_end_time),\n                    self.max_time_thresh_night,\n                ).otherwise(\n                    F.when(F.col(\"cell_id\") == F.col(\"cell_id_+1\"), self.max_time_thresh_day).otherwise(\n                        self.max_time_thresh\n                    )\n                ),\n            )\n            # Calculate init_time and end_time according to thresholds and time differences between events:\n            .withColumn(\n                \"init_time\",\n                F.when(\n                    F.col(ColNames.timestamp) - F.col(f\"{ColNames.timestamp}_-1\") &lt;= F.col(\"threshold_-1\"),\n                    F.col(ColNames.timestamp) - (F.col(ColNames.timestamp) - F.col(f\"{ColNames.timestamp}_-1\")) / 2,\n                ).otherwise(F.col(ColNames.timestamp) - self.max_time_thresh / 2),\n            )\n            .withColumn(\n                \"end_time\",\n                F.when(\n                    F.col(f\"{ColNames.timestamp}_+1\") - F.col(ColNames.timestamp) &lt;= F.col(\"threshold_+1\"),\n                    F.col(f\"{ColNames.timestamp}_+1\")\n                    - (F.col(f\"{ColNames.timestamp}_+1\") - F.col(ColNames.timestamp)) / 2,\n                ).otherwise(F.col(ColNames.timestamp) + self.max_time_thresh / 2),\n            )\n            # Filter out 'move' events (keep only stays), and also drop unneeded columns:\n            .filter(F.col(\"is_move\") == False)\n            .drop(\n                ColNames.cell_id,\n                f\"{ColNames.cell_id}_-1\",\n                f\"{ColNames.cell_id}_+1\",\n                ColNames.timestamp,\n                f\"{ColNames.timestamp}_-1\",\n                f\"{ColNames.timestamp}_+1\",\n                ColNames.mcc,\n                \"is_move\",\n                \"threshold_-1\",\n                \"threshold_+1\",\n            )\n        )\n\n        return stays\n\n    def calculate_dps(self, stays: DataFrame, time_slots: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Temporally intersect each stay interval with the specified time slots. Then\n        calculate the number of seconds that each user stays at each grid tile within\n        each of these time slots according to the stay intervals and the grid tiles\n        associated to each stay.\n\n        Args:\n            stays (DataFrame): stays dataframe.\n            time_slots (DataFrame): time slots dataframe.\n\n        Returns:\n            DataFrame: daily permanence score dataframe.\n        \"\"\"\n        dps = (\n            stays.crossJoin(time_slots)\n            .withColumn(\"int_init_time\", F.greatest(F.col(\"init_time\"), F.col(ColNames.time_slot_initial_time)))\n            .withColumn(\"int_end_time\", F.least(F.col(\"end_time\"), F.col(\"time_slot_end_time\")))\n            .withColumn(\n                \"int_duration\",\n                F.when(\n                    F.col(\"int_init_time\") &lt; F.col(\"int_end_time\"),\n                    F.unix_timestamp(F.col(\"int_end_time\")) - F.unix_timestamp(F.col(\"int_init_time\")),\n                ).otherwise(0.0),\n            )\n            .drop(\"int_init_time\", \"int_end_time\", \"init_time\", \"end_time\")\n            .filter(F.col(\"int_duration\") &gt; 0.0)\n            .withColumn(ColNames.grid_id, F.explode(\"grid_ids\"))\n            .drop(\"grid_ids\")\n            .groupby(\n                ColNames.user_id,\n                ColNames.year,\n                ColNames.month,\n                ColNames.day,\n                ColNames.user_id_modulo,\n                ColNames.grid_id,\n                ColNames.time_slot_initial_time,\n                \"time_slot_end_time\",\n            )\n            .agg(F.sum(\"int_duration\").alias(\"int_duration\"))\n            .withColumn(\n                ColNames.time_slot_duration,\n                F.lit((timedelta(days=1) / self.time_slot_number).total_seconds()).cast(IntegerType()),\n            )\n            .withColumn(\n                \"dps\",\n                F.ceil(F.lit(self.score_interval) * F.col(\"int_duration\") / F.col(ColNames.time_slot_duration)).cast(\n                    IntegerType()\n                ),\n            )\n            .drop(\"int_duration\", \"time_slot_end_time\")\n            # since some stays may be from events in previous date, fix and always set current date:\n            .withColumn(ColNames.year, F.lit(self.current_date.year).cast(ShortType()))\n            .withColumn(ColNames.month, F.lit(self.current_date.month).cast(ByteType()))\n            .withColumn(ColNames.day, F.lit(self.current_date.day).cast(ByteType()))\n        )\n\n        return dps\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.assert_needed_dates_data_object","title":"<code>assert_needed_dates_data_object(data_object_id, needed_dates)</code>","text":"<p>Method that checks if data for a set of dates exists for a data object.</p> <p>Parameters:</p> Name Type Description Default <code>data_object_id</code> <code>str</code> <p>name of the data object to check.</p> required <code>needed_dates</code> <code>list[datetime]</code> <p>list of the dates for which data shall be available.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If there is no data for one or more of the needed dates.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def assert_needed_dates_data_object(self, data_object_id: str, needed_dates: list[datetime]):\n    \"\"\"\n    Method that checks if data for a set of dates exists for a data object.\n\n    Args:\n        data_object_id (str): name of the data object to check.\n        needed_dates (list[datetime]): list of the dates for which data shall be available.\n\n    Raises:\n        ValueError: If there is no data for one or more of the needed dates.\n    \"\"\"\n    # Load data\n    df = self.input_data_objects[data_object_id].df\n\n    # Find dates that match the needed dates:\n    dates = (\n        df.withColumn(ColNames.date, F.make_date(ColNames.year, ColNames.month, ColNames.day))\n        .select(F.col(ColNames.date))\n        .filter(F.col(ColNames.date).isin(needed_dates))\n        .distinct()\n        .collect()\n    )\n    available_dates = {row[ColNames.date] for row in dates}\n\n    # If missing needed dates, raise error:\n    missing_dates = needed_dates.difference(available_dates)\n    if missing_dates:\n        error_msg = f\"Missing {data_object_id} data for dates {sorted(list(missing_dates))}\"\n        self.logger.error(error_msg)\n        raise ValueError(error_msg)\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.build_cell_footprint_table","title":"<code>build_cell_footprint_table()</code>","text":"<p>Load cell footprint data for dates D-1, D and D+1.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>cell footprint dataframe.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def build_cell_footprint_table(self) -&gt; DataFrame:\n    \"\"\"\n    Load cell footprint data for dates D-1, D and D+1.\n\n    Returns:\n        DataFrame: cell footprint dataframe.\n    \"\"\"\n    cell_footprint = (\n        self.previous_cell_footprint.union(self.current_cell_footprint)\n        .union(self.next_cell_footprint)\n        .groupBy([ColNames.cell_id, ColNames.year, ColNames.month, ColNames.day])\n        .agg(F.collect_list(ColNames.grid_id).alias(\"grid_ids\"))\n    )\n\n    return cell_footprint\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.build_events_table","title":"<code>build_events_table()</code>","text":"<p>Load events data for date D, also adding last event of each user from date D-1 and first event of each user from D+1.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>events dataframe.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def build_events_table(self) -&gt; DataFrame:\n    \"\"\"\n    Load events data for date D, also adding last event of each\n    user from date D-1 and first event of each user from D+1.\n\n    Returns:\n        DataFrame: events dataframe.\n    \"\"\"\n    # reach last event from previous day:\n    window = Window.partitionBy(ColNames.user_id).orderBy(F.desc(ColNames.timestamp))\n    self.previous_events = (\n        self.previous_events.withColumn(\"row_number\", F.row_number().over(window))\n        .filter(F.col(\"row_number\") == 1)\n        .drop(\"row_number\")\n    )\n\n    # reach first event from next day:\n    window = Window.partitionBy(ColNames.user_id).orderBy(ColNames.timestamp)\n    self.next_events = (\n        self.next_events.withColumn(\"row_number\", F.row_number().over(window))\n        .filter(F.col(\"row_number\") == 1)\n        .drop(\"row_number\")\n    )\n\n    # concat all events together (last of D-1 + all D + first of D+1):\n    events = (\n        self.previous_events.union(self.current_events)\n        .union(self.next_events)\n        .drop(ColNames.longitude, ColNames.latitude, ColNames.loc_error, ColNames.error_flag)\n    )\n\n    return events\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.build_time_slots_table","title":"<code>build_time_slots_table()</code>","text":"<p>Build a dataframe with the specified time slots for the current date.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>time slots dataframe.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def build_time_slots_table(self) -&gt; DataFrame:\n    \"\"\"\n    Build a dataframe with the specified time slots for the current date.\n\n    Returns:\n        DataFrame: time slots dataframe.\n    \"\"\"\n    time_slot_length = timedelta(days=1) / self.time_slot_number\n\n    time_slots_list = []\n    previous_end_time = datetime(\n        year=self.current_date.year,\n        month=self.current_date.month,\n        day=self.current_date.day,\n        hour=0,\n        minute=0,\n        second=0,\n    )\n\n    while previous_end_time.date() == self.current_date:\n        init_time = previous_end_time\n        end_time = init_time + time_slot_length\n        time_slot = (init_time, end_time)\n        time_slots_list.append(time_slot)\n        previous_end_time = end_time\n\n    schema = StructType(\n        [\n            StructField(ColNames.time_slot_initial_time, TimestampType(), True),\n            StructField(\"time_slot_end_time\", TimestampType(), True),\n        ]\n    )\n\n    return self.spark.createDataFrame(time_slots_list, schema=schema)\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.calculate_dps","title":"<code>calculate_dps(stays, time_slots)</code>","text":"<p>Temporally intersect each stay interval with the specified time slots. Then calculate the number of seconds that each user stays at each grid tile within each of these time slots according to the stay intervals and the grid tiles associated to each stay.</p> <p>Parameters:</p> Name Type Description Default <code>stays</code> <code>DataFrame</code> <p>stays dataframe.</p> required <code>time_slots</code> <code>DataFrame</code> <p>time slots dataframe.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>daily permanence score dataframe.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def calculate_dps(self, stays: DataFrame, time_slots: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Temporally intersect each stay interval with the specified time slots. Then\n    calculate the number of seconds that each user stays at each grid tile within\n    each of these time slots according to the stay intervals and the grid tiles\n    associated to each stay.\n\n    Args:\n        stays (DataFrame): stays dataframe.\n        time_slots (DataFrame): time slots dataframe.\n\n    Returns:\n        DataFrame: daily permanence score dataframe.\n    \"\"\"\n    dps = (\n        stays.crossJoin(time_slots)\n        .withColumn(\"int_init_time\", F.greatest(F.col(\"init_time\"), F.col(ColNames.time_slot_initial_time)))\n        .withColumn(\"int_end_time\", F.least(F.col(\"end_time\"), F.col(\"time_slot_end_time\")))\n        .withColumn(\n            \"int_duration\",\n            F.when(\n                F.col(\"int_init_time\") &lt; F.col(\"int_end_time\"),\n                F.unix_timestamp(F.col(\"int_end_time\")) - F.unix_timestamp(F.col(\"int_init_time\")),\n            ).otherwise(0.0),\n        )\n        .drop(\"int_init_time\", \"int_end_time\", \"init_time\", \"end_time\")\n        .filter(F.col(\"int_duration\") &gt; 0.0)\n        .withColumn(ColNames.grid_id, F.explode(\"grid_ids\"))\n        .drop(\"grid_ids\")\n        .groupby(\n            ColNames.user_id,\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            ColNames.user_id_modulo,\n            ColNames.grid_id,\n            ColNames.time_slot_initial_time,\n            \"time_slot_end_time\",\n        )\n        .agg(F.sum(\"int_duration\").alias(\"int_duration\"))\n        .withColumn(\n            ColNames.time_slot_duration,\n            F.lit((timedelta(days=1) / self.time_slot_number).total_seconds()).cast(IntegerType()),\n        )\n        .withColumn(\n            \"dps\",\n            F.ceil(F.lit(self.score_interval) * F.col(\"int_duration\") / F.col(ColNames.time_slot_duration)).cast(\n                IntegerType()\n            ),\n        )\n        .drop(\"int_duration\", \"time_slot_end_time\")\n        # since some stays may be from events in previous date, fix and always set current date:\n        .withColumn(ColNames.year, F.lit(self.current_date.year).cast(ShortType()))\n        .withColumn(ColNames.month, F.lit(self.current_date.month).cast(ByteType()))\n        .withColumn(ColNames.day, F.lit(self.current_date.day).cast(ByteType()))\n    )\n\n    return dps\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.calculate_min_distance_between_point_lists","title":"<code>calculate_min_distance_between_point_lists(points_i, points_j)</code>  <code>staticmethod</code>","text":"<p>Calculate minimum distance between the points in one list and the points in another list.</p> <p>Parameters:</p> Name Type Description Default <code>points_i</code> <code>set[tuple[float]]</code> <p>set of tuples. Each tuple contains the x, y coordinates of a point.</p> required <code>points_j</code> <code>set[tuple[float]]</code> <p>set of tuples. Each tuple contains the x, y coordinates of a point.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>minimum distance between points in both lists.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>@staticmethod\ndef calculate_min_distance_between_point_lists(points_i: set[tuple[float]], points_j: set[tuple[float]]) -&gt; float:\n    \"\"\"\n    Calculate minimum distance between the points in one list and the points\n    in another list.\n\n    Args:\n        points_i (set[tuple[float]]): set of tuples. Each tuple contains\n            the x, y coordinates of a point.\n        points_j (set[tuple[float]]): set of tuples. Each tuple contains\n            the x, y coordinates of a point.\n\n    Returns:\n        float: minimum distance between points in both lists.\n    \"\"\"\n    if points_i &amp; points_j:  # same point is included in both sets\n        return 0.0\n    min_distance = float(\"inf\")\n    for pi in points_i:\n        for pj in points_j:\n            distance = (pi[0] - pj[0]) ** 2 + (pi[1] - pj[1]) ** 2\n            if distance &lt; min_distance:\n                min_distance = distance\n    return min_distance**0.5\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.check_needed_dates","title":"<code>check_needed_dates()</code>","text":"<p>Method that checks if both the dates of study and the dates necessary to generate the daily permanence scores are present in the input data (events + cell footprint).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there is no data for one or more of the needed dates.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def check_needed_dates(self):\n    \"\"\"\n    Method that checks if both the dates of study and the dates necessary to generate\n    the daily permanence scores are present in the input data (events + cell footprint).\n\n    Raises:\n        ValueError: If there is no data for one or more of the needed dates.\n    \"\"\"\n    # needed dates: for each date D, we also need D-1 and D+1\n    # this is built this way so it would also support definition of study\n    # dates that are not consecutive\n    needed_dates = (\n        {d + timedelta(days=1) for d in self.data_period_dates}\n        | set(self.data_period_dates)\n        | {d - timedelta(days=1) for d in self.data_period_dates}\n    )\n\n    # Assert needed dates in event data:\n    self.assert_needed_dates_data_object(SilverEventFlaggedDataObject.ID, needed_dates)\n\n    # Assert needed dates in cell footprint data:\n    self.assert_needed_dates_data_object(SilverCellFootprintDataObject.ID, needed_dates)\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.detect_move_events","title":"<code>detect_move_events(events, cell_footprint)</code>","text":"<p>Detect which of the events are associated to moves according to the distances/times from previous to posterior event and a speed threshold.</p> <p>Parameters:</p> Name Type Description Default <code>events</code> <code>DataFrame</code> <p>events dataframe.</p> required <code>cell_footprint</code> <code>DataFrame</code> <p>cells footprint dataframe.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>events dataframe, with an additional 'is_move' boolean column.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def detect_move_events(self, events: DataFrame, cell_footprint: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Detect which of the events are associated to moves according to the\n    distances/times from previous to posterior event and a speed threshold.\n\n    Args:\n        events (DataFrame): events dataframe.\n        cell_footprint (DataFrame): cells footprint dataframe.\n\n    Returns:\n        DataFrame: events dataframe, with an additional 'is_move' boolean column.\n    \"\"\"\n    # left join -&gt; bring cell footprints to events data:\n    events = events.join(\n        cell_footprint,\n        (events[ColNames.cell_id] == cell_footprint[ColNames.cell_id])\n        &amp; (events[ColNames.year] == cell_footprint[ColNames.year])\n        &amp; (events[ColNames.month] == cell_footprint[ColNames.month])\n        &amp; (events[ColNames.day] == cell_footprint[ColNames.day]),\n        \"left\",\n    ).drop(\n        cell_footprint[ColNames.cell_id],\n        cell_footprint[ColNames.year],\n        cell_footprint[ColNames.month],\n        cell_footprint[ColNames.day],\n    )\n\n    # Add lags of timestamp, cell_id and grid_ids:\n    window = Window.partitionBy(ColNames.user_id).orderBy(ColNames.timestamp)\n    lag_fields = [ColNames.timestamp, ColNames.cell_id, \"grid_ids\"]\n    for lf in lag_fields:\n        events = events.withColumn(f\"{lf}_+1\", F.lag(lf, -1).over(window)).withColumn(\n            f\"{lf}_-1\", F.lag(lf, 1).over(window)\n        )\n\n    # Calculate distance between grid tiles associated to events -1, 0 and +1:\n    # Calculate speeds and determine which rows are moves:\n    events = (\n        events.withColumn(\"dist_0_+1\", self.grid_footprint_distance(F.col(\"grid_ids\"), F.col(\"grid_ids_+1\")))\n        .withColumn(\"dist_-1_0\", self.grid_footprint_distance(F.col(\"grid_ids_-1\"), F.col(\"grid_ids\")))\n        .withColumn(\"dist_-1_+1\", self.grid_footprint_distance(F.col(\"grid_ids_-1\"), F.col(\"grid_ids_+1\")))\n        .withColumn(\n            \"time_difference\",\n            F.unix_timestamp(events[f\"{ColNames.timestamp}_+1\"])\n            - F.unix_timestamp(events[f\"{ColNames.timestamp}_-1\"]),\n        )\n        .withColumn(\"max_dist\", F.greatest(F.col(\"dist_-1_0\") + F.col(\"dist_0_+1\"), F.col(\"dist_-1_+1\")))\n        .withColumn(\"speed\", F.col(\"max_dist\") / F.col(\"time_difference\"))\n        .withColumn(\"is_move\", F.when(F.col(\"speed\") &gt; self.max_speed_thresh, True).otherwise(False))\n        .drop(\n            \"dist_0_+1\",\n            \"dist_-1_0\",\n            \"dist_-1_+1\",\n            \"grid_ids_+1\",\n            \"grid_ids_-1\",\n            \"time_difference\",\n            \"max_dist\",\n            \"speed\",\n        )\n    )\n\n    return events\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.determine_stay_durations","title":"<code>determine_stay_durations(events)</code>","text":"<p>Determine the start time and end time for each stay event.</p> <p>Parameters:</p> Name Type Description Default <code>events</code> <code>DataFrame</code> <p>events dataframe.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>stays dataframe (filtering out moves).</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def determine_stay_durations(self, events: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Determine the start time and end time for each stay event.\n\n    Args:\n        events (DataFrame): events dataframe.\n\n    Returns:\n        DataFrame: stays dataframe (filtering out moves).\n    \"\"\"\n    current_datetime = datetime(self.current_date.year, self.current_date.month, self.current_date.day)\n    night_start_time = current_datetime - timedelta(hours=1)\n    night_end_time = current_datetime + timedelta(hours=9)\n\n    stays = (\n        events\n        # Set applicable time thresholds:\n        .withColumn(\n            \"threshold_-1\",\n            F.when(\n                (F.col(\"cell_id\") == F.col(\"cell_id_-1\"))\n                &amp; (F.col(\"timestamp_-1\") &gt;= night_start_time)\n                &amp; (F.col(\"timestamp\") &lt;= night_end_time),\n                self.max_time_thresh_night,\n            ).otherwise(\n                F.when(F.col(\"cell_id\") == F.col(\"cell_id_-1\"), self.max_time_thresh_day).otherwise(\n                    self.max_time_thresh\n                )\n            ),\n        )\n        .withColumn(\n            \"threshold_+1\",\n            F.when(\n                (F.col(\"cell_id\") == F.col(\"cell_id_+1\"))\n                &amp; (F.col(\"timestamp\") &gt;= night_start_time)\n                &amp; (F.col(\"timestamp_+1\") &lt;= night_end_time),\n                self.max_time_thresh_night,\n            ).otherwise(\n                F.when(F.col(\"cell_id\") == F.col(\"cell_id_+1\"), self.max_time_thresh_day).otherwise(\n                    self.max_time_thresh\n                )\n            ),\n        )\n        # Calculate init_time and end_time according to thresholds and time differences between events:\n        .withColumn(\n            \"init_time\",\n            F.when(\n                F.col(ColNames.timestamp) - F.col(f\"{ColNames.timestamp}_-1\") &lt;= F.col(\"threshold_-1\"),\n                F.col(ColNames.timestamp) - (F.col(ColNames.timestamp) - F.col(f\"{ColNames.timestamp}_-1\")) / 2,\n            ).otherwise(F.col(ColNames.timestamp) - self.max_time_thresh / 2),\n        )\n        .withColumn(\n            \"end_time\",\n            F.when(\n                F.col(f\"{ColNames.timestamp}_+1\") - F.col(ColNames.timestamp) &lt;= F.col(\"threshold_+1\"),\n                F.col(f\"{ColNames.timestamp}_+1\")\n                - (F.col(f\"{ColNames.timestamp}_+1\") - F.col(ColNames.timestamp)) / 2,\n            ).otherwise(F.col(ColNames.timestamp) + self.max_time_thresh / 2),\n        )\n        # Filter out 'move' events (keep only stays), and also drop unneeded columns:\n        .filter(F.col(\"is_move\") == False)\n        .drop(\n            ColNames.cell_id,\n            f\"{ColNames.cell_id}_-1\",\n            f\"{ColNames.cell_id}_+1\",\n            ColNames.timestamp,\n            f\"{ColNames.timestamp}_-1\",\n            f\"{ColNames.timestamp}_+1\",\n            ColNames.mcc,\n            \"is_move\",\n            \"threshold_-1\",\n            \"threshold_+1\",\n        )\n    )\n\n    return stays\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.filter_cell_footprint","title":"<code>filter_cell_footprint(current_date)</code>","text":"<p>Load cell footprints for a specific date.</p> <p>Parameters:</p> Name Type Description Default <code>current_date</code> <code>date</code> <p>current date.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>filtered cell footprint dataframe.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def filter_cell_footprint(self, current_date: date) -&gt; DataFrame:\n    \"\"\"\n    Load cell footprints for a specific date.\n\n    Args:\n        current_date (date): current date.\n\n    Returns:\n        DataFrame: filtered cell footprint dataframe.\n    \"\"\"\n    return self.input_data_objects[SilverCellFootprintDataObject.ID].df.filter(\n        (F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(current_date))\n    )\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.filter_events","title":"<code>filter_events(current_date)</code>","text":"<p>Load events with no errors for a specific date.</p> <p>Parameters:</p> Name Type Description Default <code>current_date</code> <code>date</code> <p>current date.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>filtered events dataframe.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def filter_events(self, current_date: date) -&gt; DataFrame:\n    \"\"\"\n    Load events with no errors for a specific date.\n\n    Args:\n        current_date (date): current date.\n\n    Returns:\n        DataFrame: filtered events dataframe.\n    \"\"\"\n    return self.input_data_objects[SilverEventFlaggedDataObject.ID].df.filter(\n        (F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(current_date))\n        &amp; (F.col(ColNames.error_flag).isin(self.event_error_flags_to_include))\n    )\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.get_grid_id_size_meters","title":"<code>get_grid_id_size_meters(grid_id)</code>  <code>staticmethod</code>","text":"<p>Reach the size of a grid tile from its identifier.</p> <p>Parameters:</p> Name Type Description Default <code>grid_id</code> <code>str</code> <p>ID of the corresponding grid tile.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>grid tile size, in meters.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>@staticmethod\ndef get_grid_id_size_meters(grid_id: str) -&gt; float:\n    \"\"\"\n    Reach the size of a grid tile from its identifier.\n\n    Args:\n        grid_id (str): ID of the corresponding grid tile.\n\n    Returns:\n        float: grid tile size, in meters.\n    \"\"\"\n    size_txt = re.search(r\"^.*m\", grid_id)[0]\n    if size_txt[-2:] == \"km\":\n        size = float(size_txt[:-2]) * 1000\n    else:\n        size = float(size_txt[:-1])\n    return size\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.get_grid_id_vertices","title":"<code>get_grid_id_vertices(grid_id)</code>  <code>staticmethod</code>","text":"<p>Obtain the coordinates of the vertices of a given grid tile.</p> <p>Parameters:</p> Name Type Description Default <code>grid_id</code> <code>str</code> <p>ID of the corresponding grid tile.</p> required <p>Returns:</p> Type Description <code>list[tuple[float]]</code> <p>list[tuple[float]]: list of tuples. Each tuple contains the x, y coordinates of a vertex of the corresponding grid tile.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>@staticmethod\ndef get_grid_id_vertices(grid_id: str) -&gt; list[tuple[float]]:\n    \"\"\"\n    Obtain the coordinates of the vertices of a given grid tile.\n\n    Args:\n        grid_id (str): ID of the corresponding grid tile.\n\n    Returns:\n        list[tuple[float]]: list of tuples. Each tuple contains the x, y\n            coordinates of a vertex of the corresponding grid tile.\n    \"\"\"\n    size = DailyPermanenceScore.get_grid_id_size_meters(grid_id)\n    xleft = float(re.search(r\"E\\s*(\\d+)\", grid_id)[0][1:])\n    ybottom = float(re.search(r\"N\\s*(\\d+)\", grid_id)[0][1:])\n    xright = xleft + size\n    ytop = ybottom + size\n    xs = (xleft, xright)\n    ys = (ybottom, ytop)\n    vertices = list(itertools.product(xs, ys))\n    return vertices\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.grid_footprint_distance","title":"<code>grid_footprint_distance(grid_ids_i, grid_ids_j)</code>  <code>staticmethod</code>","text":"<p>Calculate minimum distance between the grid tiles in one list and the grid tiles in another list, provided as grid tile IDs.</p> <p>Parameters:</p> Name Type Description Default <code>grid_ids_i</code> <code>list[str]</code> <p>IDs of the corresponding grid tiles.</p> required <code>grid_ids_j</code> <code>list[str]</code> <p>IDs of the corresponding grid tiles.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>minimum distance.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>@staticmethod\n@F.udf(returnType=FloatType())\ndef grid_footprint_distance(grid_ids_i: list[str], grid_ids_j: list[str]) -&gt; float:\n    \"\"\"\n    Calculate minimum distance between the grid tiles in one list and the\n    grid tiles in another list, provided as grid tile IDs.\n\n    Args:\n        grid_ids_i (list[str]): IDs of the corresponding grid tiles.\n        grid_ids_j (list[str]): IDs of the corresponding grid tiles.\n\n    Returns:\n        float: minimum distance.\n    \"\"\"\n    # TODO: optimise\n    if not grid_ids_i or not grid_ids_j:\n        return None\n\n    if set(grid_ids_i) == set(grid_ids_j):\n        return 0.0\n\n    gi_vertices = set()\n    for gi in grid_ids_i:\n        gi_vertices |= set(DailyPermanenceScore.get_grid_id_vertices(gi))\n    gj_vertices = set()\n    for gj in grid_ids_j:\n        gj_vertices |= set(DailyPermanenceScore.get_grid_id_vertices(gj))\n\n    min_distance = DailyPermanenceScore.calculate_min_distance_between_point_lists(gi_vertices, gj_vertices)\n\n    return min_distance\n</code></pre>"},{"location":"reference/components/execution/device_activity_statistics/","title":"device_activity_statistics","text":""},{"location":"reference/components/execution/device_activity_statistics/device_activity_statistics/","title":"device_activity_statistics","text":"<p>Module that cleans RAW MNO Event data.</p>"},{"location":"reference/components/execution/device_activity_statistics/device_activity_statistics/#components.execution.device_activity_statistics.device_activity_statistics.DeviceActivityStatistics","title":"<code>DeviceActivityStatistics</code>","text":"<p>             Bases: <code>Component</code></p> <p>Class that removes duplicates from clean MNO Event data</p> Source code in <code>multimno/components/execution/device_activity_statistics/device_activity_statistics.py</code> <pre><code>class DeviceActivityStatistics(Component):\n    \"\"\"\n    Class that removes duplicates from clean MNO Event data\n    \"\"\"\n\n    COMPONENT_ID = \"DeviceActivityStatistics\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n        self.current_date = None\n        self.current_input_events = None\n        self.current_input_network = None\n        self.statistics_df = None\n\n    def initalize_data_objects(self):\n        # Input\n        # TODO: update this to semantically cleaned files after merge\n        self.input_events_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"event_data_silver_deduplicated\")\n        # TODO: Figure out how this would work with coverage areas. We don't have location of cells in those cases\n        self.input_topology_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"network_data_silver\")\n        self.output_statistics_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"device_activity_statistics\")\n\n        self.data_period_start = self.config.get(DeviceActivityStatistics.COMPONENT_ID, \"data_period_start\")\n        self.data_period_end = self.config.get(DeviceActivityStatistics.COMPONENT_ID, \"data_period_end\")\n        self.clear_destination_directory = self.config.get(\n            DeviceActivityStatistics.COMPONENT_ID, \"clear_destination_directory\"\n        )\n        self.local_timezone_str = self.config.get(TIMEZONE_CONFIG_KEY, \"local_timezone\")\n\n        # Create all possible dates between start and end\n        sdate = pd.to_datetime(self.data_period_start)\n        edate = pd.to_datetime(self.data_period_end)\n        self.to_process_dates = list(pd.date_range(sdate, edate, freq=\"d\"))\n\n        # Create all input data objects\n        self.input_data_objects = {SilverEventDataObject.ID: None}\n        if check_if_data_path_exists(self.spark, self.input_events_path):\n            # Single input option, with a filter follwed by it\n            self.input_data_objects[SilverEventDataObject.ID] = SilverEventDataObject(\n                self.spark, self.input_events_path\n            )\n        else:\n            self.logger.warning(f\"Expected path {self.input_events_path} to exist but it does not\")\n\n        self.input_data_objects[SilverNetworkDataObject.ID] = None\n\n        if check_if_data_path_exists(self.spark, self.input_topology_path):\n            # Single input option, with a filter follwed by it\n            self.input_data_objects[SilverNetworkDataObject.ID] = SilverNetworkDataObject(\n                self.spark, self.input_topology_path\n            )\n        else:\n            self.logger.warning(f\"Expected path {self.input_topology_path} to exist but it does not\")\n\n        # Output data objects dictionary\n        self.output_data_objects = {}\n        self.output_data_objects[SilverDeviceActivityStatistics.ID] = SilverDeviceActivityStatistics(\n            self.spark, self.output_statistics_path\n        )\n\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, self.output_statistics_path)\n\n        # Create timezones for transformations\n        self.local_tz = pytz.timezone(self.local_timezone_str)\n        self.utc_tz = pytz.timezone(\"UTC\")\n\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n\n        self.read()\n\n        for current_date in self.to_process_dates:\n            self.current_date = current_date\n\n            start = datetime(year=current_date.year, month=current_date.month, day=current_date.day)\n            start_utc = self.local_tz.localize(start).astimezone(self.utc_tz)\n            end_utc = start_utc + timedelta(days=1) - timedelta(seconds=1)\n\n            # TODO: should this selection also be based on user_id_modulo?\n            self.current_input_events = self.input_data_objects[SilverEventDataObject.ID].df.filter(\n                (F.col(ColNames.timestamp).between(start_utc, end_utc))\n            )\n\n            self.current_input_network = self.input_data_objects[SilverNetworkDataObject.ID].df.filter(\n                (\n                    (F.col(ColNames.year) == current_date.year)\n                    &amp; (F.col(ColNames.month) == current_date.month)\n                    &amp; (F.col(ColNames.day) == current_date.day)\n                )\n            )\n\n            self.transform()\n\n            self.write()\n\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n        # Prepare everything for metrics calculations\n        df_events = self.preprocess_events(self.current_input_events, self.current_input_network)\n\n        # Calculate count of events per user\n        self.statistics_df = df_events.groupby(ColNames.user_id).count().withColumnRenamed(\"count\", ColNames.event_cnt)\n\n        # Calculate count of unique cells per user\n        unique_cell_counts = (\n            df_events.groupBy(ColNames.user_id)\n            .agg(F.countDistinct(ColNames.cell_id))\n            .withColumnRenamed(f\"count(DISTINCT {ColNames.cell_id})\", ColNames.unique_cell_cnt)\n        )\n        self.statistics_df = self.statistics_df.join(unique_cell_counts, on=\"user_id\")\n        # Calculate count of unique locations per user\n        unique_location_counts = df_events.groupBy(ColNames.user_id).agg(\n            F.countDistinct(ColNames.latitude, ColNames.longitude).alias(ColNames.unique_location_cnt)\n        )\n        self.statistics_df = self.statistics_df.join(unique_location_counts, on=\"user_id\")\n\n        # Calculate sum of distances between cells\n        distance_per_user = df_events.groupBy(ColNames.user_id).agg(\n            F.sum(\"distance\").cast(IntegerType()).alias(ColNames.sum_distance_m)\n        )\n        self.statistics_df = self.statistics_df.join(distance_per_user, on=\"user_id\")\n\n        # Calculate number of unique hours in data per user\n        hourly_events_df = df_events.withColumn(\n            ColNames.timestamp, F.date_trunc(\"hour\", F.col(ColNames.timestamp))\n        ).select([ColNames.user_id, ColNames.timestamp])\n        hourly_counts = hourly_events_df.groupBy(ColNames.user_id).agg(\n            F.countDistinct(ColNames.timestamp).alias(ColNames.unique_hour_cnt)\n        )\n        self.statistics_df = self.statistics_df.join(hourly_counts, on=\"user_id\")\n\n        # mean_time_gap\n        mean_time_gaps = df_events.groupBy(ColNames.user_id).agg(\n            F.mean(\"time_gap_s\").cast(IntegerType()).alias(ColNames.mean_time_gap)\n        )\n        self.statistics_df = self.statistics_df.join(mean_time_gaps, on=\"user_id\")\n\n        # stdev_time_gap\n        stddev_time_gaps = df_events.groupBy(ColNames.user_id).agg(\n            F.stddev(\"time_gap_s\").alias(ColNames.stdev_time_gap)\n        )\n        self.statistics_df = self.statistics_df.join(stddev_time_gaps, on=\"user_id\")\n        # Add date to statistics\n        self.statistics_df = self.statistics_df.withColumns(\n            {\n                ColNames.year: F.lit(self.current_date.year).cast(\"smallint\"),\n                ColNames.month: F.lit(self.current_date.month).cast(\"tinyint\"),\n                ColNames.day: F.lit(self.current_date.day).cast(\"tinyint\"),\n            }\n        )\n        # Reorder rows\n        self.statistics_df = self.statistics_df.select([col.name for col in SilverDeviceActivityStatistics.SCHEMA])\n        for col in SilverDeviceActivityStatistics.SCHEMA:\n            self.statistics_df = self.statistics_df.withColumn(\n                col.name, self.statistics_df[col.name].cast(col.dataType)\n            )\n        self.output_data_objects[SilverDeviceActivityStatistics.ID].df = self.statistics_df\n\n        # after each chunk processing clear all Cache to free memory and disk\n        self.spark.catalog.clearCache()\n\n    def preprocess_events(\n        self, df_events: pyspark.sql.dataframe.DataFrame, df_network: pyspark.sql.dataframe.DataFrame\n    ) -&gt; DataFrame:\n        \"\"\"\n        Preprocesses events dataframe to be able to calculate all metrics. Steps:\n        1. Converts timestamp from UTC to local\n        2. Fills latitude and longitude columns from the location of the cell\n        3. Gets location of next event for each event\n        4. Gets timestamp of next event for each event\n        5. Calculates time gap to next event\n        6. Calculates distance to next event\n\n        Args:\n            df_events (pyspark.sql.dataframe.DataFrame): Events relating to the day that is currently being processed\n            df_network (pyspark.sql.dataframe.DataFrame): Network relating to the day that is currently being processed\n\n        Returns:\n            df_events: Events relating to the day that is currently being processed with\n                extra columns for metric calculation\n        \"\"\"\n\n        # Convert timestamp to local\n        df_events.withColumn(\n            ColNames.timestamp,\n            F.to_utc_timestamp(F.from_utc_timestamp(ColNames.timestamp, self.local_timezone_str), \"UTC\"),\n        )\n        df_events = df_events.withColumns(\n            {\n                ColNames.year: F.year(ColNames.timestamp).cast(\"smallint\"),\n                ColNames.month: F.month(ColNames.timestamp).cast(\"tinyint\"),\n                ColNames.day: F.dayofmonth(ColNames.timestamp).cast(\"tinyint\"),\n            }\n        )\n\n        # Join events with topology data to enable checking unique locations and travelled distances\n        df_events = df_events.join(\n            df_network.select(\n                [\n                    F.col(ColNames.cell_id),\n                    F.col(ColNames.latitude).alias(\"cell_lat\"),\n                    F.col(ColNames.longitude).alias(\"cell_lon\"),\n                ]\n            ),\n            on=ColNames.cell_id,\n            how=\"left\",\n        )\n\n        # Use latitude and longitude if they exist, otherwise use cells location\n        df_events = df_events.withColumn(ColNames.latitude, F.coalesce(ColNames.latitude, \"cell_lat\"))\n        df_events = df_events.withColumn(ColNames.longitude, F.coalesce(ColNames.longitude, \"cell_lon\"))\n\n        # Add timestamp and location of next record\n        window = Window.partitionBy(*[ColNames.user_id_modulo, ColNames.user_id]).orderBy(ColNames.timestamp)\n        df_events = df_events.withColumn(\n            f\"next_{ColNames.timestamp}\", F.lead(F.col(ColNames.timestamp), 1).over(window)\n        )\n        df_events = df_events.withColumn(f\"next_{ColNames.latitude}\", F.lead(F.col(ColNames.latitude), 1).over(window))\n        df_events = df_events.withColumn(\n            f\"next_{ColNames.longitude}\", F.lead(F.col(ColNames.longitude), 1).over(window)\n        )\n\n        # Calculate time gap to next event\n        df_events = df_events.withColumn(\n            \"time_gap_s\", F.unix_timestamp(f\"next_{ColNames.timestamp}\") - F.unix_timestamp(ColNames.timestamp)\n        )\n\n        # Calculate the distance between current and next event\n        # TODO: check if this is the correct way to calculate, got varying results\n        # There are many ways to calculate distance between points and all of them give different results\n        df_events = df_events.withColumn(\n            \"source_geom\",\n            STF.ST_SetSRID(STC.ST_Point(df_events[ColNames.latitude], df_events[ColNames.longitude]), 4326),\n        )\n        df_events = df_events.withColumn(\n            \"destination_geom\",\n            STF.ST_SetSRID(\n                STC.ST_Point(df_events[f\"next_{ColNames.latitude}\"], df_events[f\"next_{ColNames.longitude}\"]), 4326\n            ),\n        )\n        df_events = df_events.withColumn(\n            \"distance\", STF.ST_DistanceSphere(df_events[\"source_geom\"], df_events[\"destination_geom\"])\n        )\n\n        return df_events\n</code></pre>"},{"location":"reference/components/execution/device_activity_statistics/device_activity_statistics/#components.execution.device_activity_statistics.device_activity_statistics.DeviceActivityStatistics.preprocess_events","title":"<code>preprocess_events(df_events, df_network)</code>","text":"<p>Preprocesses events dataframe to be able to calculate all metrics. Steps: 1. Converts timestamp from UTC to local 2. Fills latitude and longitude columns from the location of the cell 3. Gets location of next event for each event 4. Gets timestamp of next event for each event 5. Calculates time gap to next event 6. Calculates distance to next event</p> <p>Parameters:</p> Name Type Description Default <code>df_events</code> <code>DataFrame</code> <p>Events relating to the day that is currently being processed</p> required <code>df_network</code> <code>DataFrame</code> <p>Network relating to the day that is currently being processed</p> required <p>Returns:</p> Name Type Description <code>df_events</code> <code>DataFrame</code> <p>Events relating to the day that is currently being processed with extra columns for metric calculation</p> Source code in <code>multimno/components/execution/device_activity_statistics/device_activity_statistics.py</code> <pre><code>def preprocess_events(\n    self, df_events: pyspark.sql.dataframe.DataFrame, df_network: pyspark.sql.dataframe.DataFrame\n) -&gt; DataFrame:\n    \"\"\"\n    Preprocesses events dataframe to be able to calculate all metrics. Steps:\n    1. Converts timestamp from UTC to local\n    2. Fills latitude and longitude columns from the location of the cell\n    3. Gets location of next event for each event\n    4. Gets timestamp of next event for each event\n    5. Calculates time gap to next event\n    6. Calculates distance to next event\n\n    Args:\n        df_events (pyspark.sql.dataframe.DataFrame): Events relating to the day that is currently being processed\n        df_network (pyspark.sql.dataframe.DataFrame): Network relating to the day that is currently being processed\n\n    Returns:\n        df_events: Events relating to the day that is currently being processed with\n            extra columns for metric calculation\n    \"\"\"\n\n    # Convert timestamp to local\n    df_events.withColumn(\n        ColNames.timestamp,\n        F.to_utc_timestamp(F.from_utc_timestamp(ColNames.timestamp, self.local_timezone_str), \"UTC\"),\n    )\n    df_events = df_events.withColumns(\n        {\n            ColNames.year: F.year(ColNames.timestamp).cast(\"smallint\"),\n            ColNames.month: F.month(ColNames.timestamp).cast(\"tinyint\"),\n            ColNames.day: F.dayofmonth(ColNames.timestamp).cast(\"tinyint\"),\n        }\n    )\n\n    # Join events with topology data to enable checking unique locations and travelled distances\n    df_events = df_events.join(\n        df_network.select(\n            [\n                F.col(ColNames.cell_id),\n                F.col(ColNames.latitude).alias(\"cell_lat\"),\n                F.col(ColNames.longitude).alias(\"cell_lon\"),\n            ]\n        ),\n        on=ColNames.cell_id,\n        how=\"left\",\n    )\n\n    # Use latitude and longitude if they exist, otherwise use cells location\n    df_events = df_events.withColumn(ColNames.latitude, F.coalesce(ColNames.latitude, \"cell_lat\"))\n    df_events = df_events.withColumn(ColNames.longitude, F.coalesce(ColNames.longitude, \"cell_lon\"))\n\n    # Add timestamp and location of next record\n    window = Window.partitionBy(*[ColNames.user_id_modulo, ColNames.user_id]).orderBy(ColNames.timestamp)\n    df_events = df_events.withColumn(\n        f\"next_{ColNames.timestamp}\", F.lead(F.col(ColNames.timestamp), 1).over(window)\n    )\n    df_events = df_events.withColumn(f\"next_{ColNames.latitude}\", F.lead(F.col(ColNames.latitude), 1).over(window))\n    df_events = df_events.withColumn(\n        f\"next_{ColNames.longitude}\", F.lead(F.col(ColNames.longitude), 1).over(window)\n    )\n\n    # Calculate time gap to next event\n    df_events = df_events.withColumn(\n        \"time_gap_s\", F.unix_timestamp(f\"next_{ColNames.timestamp}\") - F.unix_timestamp(ColNames.timestamp)\n    )\n\n    # Calculate the distance between current and next event\n    # TODO: check if this is the correct way to calculate, got varying results\n    # There are many ways to calculate distance between points and all of them give different results\n    df_events = df_events.withColumn(\n        \"source_geom\",\n        STF.ST_SetSRID(STC.ST_Point(df_events[ColNames.latitude], df_events[ColNames.longitude]), 4326),\n    )\n    df_events = df_events.withColumn(\n        \"destination_geom\",\n        STF.ST_SetSRID(\n            STC.ST_Point(df_events[f\"next_{ColNames.latitude}\"], df_events[f\"next_{ColNames.longitude}\"]), 4326\n        ),\n    )\n    df_events = df_events.withColumn(\n        \"distance\", STF.ST_DistanceSphere(df_events[\"source_geom\"], df_events[\"destination_geom\"])\n    )\n\n    return df_events\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/","title":"event_cleaning","text":""},{"location":"reference/components/execution/event_cleaning/event_cleaning/","title":"event_cleaning","text":"<p>Module that cleans RAW MNO Event data.</p>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning","title":"<code>EventCleaning</code>","text":"<p>             Bases: <code>Component</code></p> <p>Class that cleans MNO Event data</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>class EventCleaning(Component):\n    \"\"\"\n    Class that cleans MNO Event data\n    \"\"\"\n\n    COMPONENT_ID = \"EventCleaning\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.timestamp_format = self.config.get(EventCleaning.COMPONENT_ID, \"timestamp_format\")\n        self.input_timezone = self.config.get(EventCleaning.COMPONENT_ID, \"input_timezone\")\n\n        self.do_bounding_box_filtering = self.config.getboolean(\n            EventCleaning.COMPONENT_ID, \"do_bounding_box_filtering\", fallback=False\n        )\n        self.bounding_box = self.config.geteval(EventCleaning.COMPONENT_ID, \"bounding_box\")\n\n        self.spark_data_folder_date_format = self.config.get(\n            EventCleaning.COMPONENT_ID, \"spark_data_folder_date_format\"\n        )\n\n    def initalize_data_objects(self):\n        # Input\n        self.bronze_event_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"event_data_bronze\")\n\n        self.data_period_start = self.config.get(EventCleaning.COMPONENT_ID, \"data_period_start\")\n        self.data_period_end = self.config.get(EventCleaning.COMPONENT_ID, \"data_period_end\")\n        self.data_folder_date_format = self.config.get(EventCleaning.COMPONENT_ID, \"data_folder_date_format\")\n        self.clear_destination_directory = self.config.get(EventCleaning.COMPONENT_ID, \"clear_destination_directory\")\n        self.number_of_partitions = self.config.get(EventCleaning.COMPONENT_ID, \"number_of_partitions\")\n\n        # Create all possible dates between start and end\n        # It is suggested that data is already separated in date folders\n        # with names following self.data_folder_date_format (e.g. 20230101)\n        sdate = pd.to_datetime(self.data_period_start)\n        edate = pd.to_datetime(self.data_period_end)\n        self.to_process_dates = list(pd.date_range(sdate, edate, freq=\"d\"))\n\n        # Create all input data objects\n        self.input_event_data_objects = []\n        self.dates_to_process = []\n        for date in self.to_process_dates:\n            path = f\"{self.bronze_event_path}/year={date.year}/month={date.month}/day={date.day}\"\n            if check_if_data_path_exists(self.spark, path):\n                self.dates_to_process.append(date)\n                self.input_event_data_objects.append(BronzeEventDataObject(self.spark, path))\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")\n        # Output\n        self.output_data_objects = {}\n\n        silver_event_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"event_data_silver\")\n        silver_event_do = SilverEventDataObject(self.spark, silver_event_path)\n        self.output_data_objects[SilverEventDataObject.ID] = silver_event_do\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, silver_event_do.default_path)\n\n        event_syntactic_quality_metrics_by_column_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"event_syntactic_quality_metrics_by_column\"\n        )\n        event_syntactic_quality_metrics_by_column = SilverEventDataSyntacticQualityMetricsByColumn(\n            self.spark, event_syntactic_quality_metrics_by_column_path\n        )\n        self.output_data_objects[SilverEventDataSyntacticQualityMetricsByColumn.ID] = (\n            event_syntactic_quality_metrics_by_column\n        )\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, event_syntactic_quality_metrics_by_column.default_path)\n\n        self.output_qa_by_column = event_syntactic_quality_metrics_by_column\n\n        event_syntactic_quality_metrics_frequency_distribution_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"event_syntactic_quality_metrics_frequency_distribution\"\n        )\n        event_syntactic_quality_metrics_frequency_distribution = (\n            SilverEventDataSyntacticQualityMetricsFrequencyDistribution(\n                self.spark, event_syntactic_quality_metrics_frequency_distribution_path\n            )\n        )\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, event_syntactic_quality_metrics_frequency_distribution.default_path)\n\n        self.output_data_objects[SilverEventDataSyntacticQualityMetricsFrequencyDistribution.ID] = (\n            event_syntactic_quality_metrics_frequency_distribution\n        )\n        # this instance of SilverEventDataSyntacticQualityMetricsFrequencyDistribution class\n        # will be used to write frequency distrobution of each preprocessing date (chunk)\n        # the path argument will be changed dynamically\n        self.output_qa_freq_distribution = event_syntactic_quality_metrics_frequency_distribution\n\n    def read(self):\n        self.current_input_do.read()\n\n    def write(self):\n        self.output_data_objects[SilverEventDataObject.ID].write()\n        self.save_syntactic_quality_metrics_frequency_distribution()\n        self.save_syntactic_quality_metrics_by_column()\n\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        for input_do, current_date in zip(self.input_event_data_objects, self.dates_to_process):\n            self.current_date = current_date\n            self.logger.info(f\"Reading from path {input_do.default_path}\")\n            self.current_input_do = input_do\n            self.read()\n            self.transform()  # Transforms the input_df\n            self.write()\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n        df_events = self.current_input_do.df\n        # cache before each filter function because we appply action count()\n        df_events = df_events.cache()\n\n        self.quality_metrics_distribution_before = df_events.groupBy(ColNames.cell_id, ColNames.user_id).agg(\n            F.count(\"*\").alias(ColNames.initial_frequency)\n        )\n\n        df_events = self.filter_nulls_and_update_qa(\n            df_events,\n            [ColNames.user_id, ColNames.timestamp, ColNames.mcc],\n            self.output_qa_by_column.error_and_transformation_counts,\n        )\n\n        # MCC correct format verification: 3 digit value\n        df_events = self.filter_invalid_mcc_and_update_qa(\n            df_events, self.output_qa_by_column.error_and_transformation_counts\n        )\n\n        # already cached in previous function\n        df_events = self.filter_null_locations_and_update_qa(\n            df_events, self.output_qa_by_column.error_and_transformation_counts\n        )\n\n        df_events = df_events.cache()\n\n        # Remove rows with invalid cell_ids\n        df_events = self.filter_invalid_cell_id_and_update_qa(\n            df_events, self.output_qa_by_column.error_and_transformation_counts\n        )\n\n        df_events = df_events.cache()\n\n        df_events = self.convert_time_column_to_timestamp_and_update_qa(\n            df_events,\n            self.timestamp_format,\n            self.input_timezone,\n            self.output_qa_by_column.error_and_transformation_counts,\n        )\n\n        df_events = df_events.cache()\n        # TODO: discuss is this step even needed (did since it was in Method description)\n        df_events = self.data_period_filter_and_update_qa(\n            df_events,\n            self.data_period_start,\n            self.data_period_end,\n            self.output_qa_by_column.error_and_transformation_counts,\n        )\n\n        if self.do_bounding_box_filtering:\n            df_events = df_events.cache()\n            df_events = self.bounding_box_filtering_and_update_qa(\n                df_events, self.bounding_box, self.output_qa_by_column.error_and_transformation_counts\n            )\n\n        self.quality_metrics_distribution_after = df_events.groupBy(ColNames.cell_id, ColNames.user_id).agg(\n            F.count(\"*\").alias(ColNames.final_frequency)\n        )\n\n        # TODO: discuss\n        # if we impose the rule on input data that data in a folder\n        # is of date specified in folder name - maybe better to use F.lit()\n        df_events = df_events.withColumns(\n            {\n                ColNames.year: F.year(ColNames.timestamp).cast(\"smallint\"),\n                ColNames.month: F.month(ColNames.timestamp).cast(\"tinyint\"),\n                ColNames.day: F.dayofmonth(ColNames.timestamp).cast(\"tinyint\"),\n            }\n        )\n\n        # The concept of device demultiplex is implemented here\n        # 1) Creates a modulo column, 2) repartitions according to it 3) sorts data within partitions\n\n        df_events = self.calculate_user_id_modulo(df_events, self.number_of_partitions)\n        df_events = df_events.repartition(ColNames.user_id_modulo)\n        df_events = df_events.sortWithinPartitions(ColNames.user_id, ColNames.timestamp)\n\n        self.output_data_objects[SilverEventDataObject.ID].df = self.spark.createDataFrame(\n            df_events.rdd, SilverEventDataObject.SCHEMA\n        )\n        # after each chunk processing clear all Cache to free memory and disk\n        self.spark.catalog.clearCache()\n\n    def save_syntactic_quality_metrics_frequency_distribution(self):\n        \"\"\"\n        Join frequency distribution tables before and after,\n        from after table take only final_frequency and replace nulls with 0.\n        Create additional column date in DateType(),\n        match the schema of SilverEventDataSyntacticQualityMetricsByColumn class\n        Write chunk results in separate folders named by processing date\n        \"\"\"\n\n        # Using outer join and groupby after because Spark can not handle joining with null values in columns\n        self.output_qa_freq_distribution.df = self.quality_metrics_distribution_before.join(\n            self.quality_metrics_distribution_after, [ColNames.cell_id, ColNames.user_id], \"outer\"\n        ).select(self.quality_metrics_distribution_before.columns + [ColNames.final_frequency])\n\n        self.output_qa_freq_distribution.df = self.output_qa_freq_distribution.df.groupBy(\n            ColNames.cell_id, ColNames.user_id\n        ).sum(ColNames.initial_frequency, ColNames.final_frequency)\n        self.output_qa_freq_distribution.df = self.output_qa_freq_distribution.df.withColumnRenamed(\n            f\"sum({ColNames.initial_frequency})\", ColNames.initial_frequency\n        )\n        self.output_qa_freq_distribution.df = self.output_qa_freq_distribution.df.withColumnRenamed(\n            f\"sum({ColNames.final_frequency})\", ColNames.final_frequency\n        )\n\n        self.output_qa_freq_distribution.df = self.output_qa_freq_distribution.df.fillna(0, ColNames.final_frequency)\n\n        self.output_qa_freq_distribution.df = self.output_qa_freq_distribution.df.withColumn(\n            ColNames.date, F.to_date(F.lit(self.current_date), self.spark_data_folder_date_format)\n        )\n\n        self.output_qa_freq_distribution.df = self.spark.createDataFrame(\n            self.output_qa_freq_distribution.df.rdd, self.output_qa_freq_distribution.SCHEMA\n        )\n\n        self.output_qa_freq_distribution.write()\n\n    def save_syntactic_quality_metrics_by_column(self):\n        \"\"\"\n        Convert output_qa_by_column.error_and_transformation_counts dictionary into spark df.\n        Add additional columns, match schema of SilverEventDataSyntacticQualityMetricsByColumn class.\n        Write results in default path of this class\n        \"\"\"\n        # self.output_qa_by_column( variable, type_of_error, type_of_transformation) : value\n        # 3 Nones to match the expected schema\n        df_tuples = [\n            (variable, type_of_error, type_of_transformation, value)\n            for (\n                variable,\n                type_of_error,\n                type_of_transformation,\n            ), value in self.output_qa_by_column.error_and_transformation_counts.items()\n        ]\n        # clear dictionary after each preprocessed day\n        self.output_qa_by_column.error_and_transformation_counts.clear()\n\n        temp_schema = StructType(\n            [\n                StructField(ColNames.variable, StringType(), nullable=True),\n                StructField(ColNames.type_of_error, ShortType(), nullable=True),\n                StructField(ColNames.type_of_transformation, ShortType(), nullable=True),\n                StructField(ColNames.value, IntegerType(), nullable=False),\n            ]\n        )\n\n        self.output_qa_by_column.df = self.spark.createDataFrame(df_tuples, temp_schema)\n\n        self.output_qa_by_column.df = self.output_qa_by_column.df.withColumns(\n            {\n                ColNames.result_timestamp: F.lit(F.current_timestamp()),\n                ColNames.date: F.to_date(F.lit(self.current_date), self.spark_data_folder_date_format),\n            }\n        ).select(self.output_qa_by_column.SCHEMA.fieldNames())\n\n        self.output_qa_by_column.df = self.spark.createDataFrame(\n            self.output_qa_by_column.df.rdd, self.output_qa_by_column.SCHEMA\n        )\n\n        self.output_qa_by_column.write()\n\n    def filter_nulls_and_update_qa(\n        self,\n        df: pyspark.sql.dataframe.DataFrame,\n        filter_columns: list[str],\n        error_and_transformation_counts: dict[tuple:int],\n    ) -&gt; pyspark.sql.dataframe.DataFrame:\n        \"\"\"\n        Loop throuh filter columns (user_id, timestamp)\n        delete rows that have null in the corresponfing column.\n        Counts the number of filtered records for quality metrics,\n        for user_id also calculates the overall correct values,\n        since null check is the one and only filter for this column\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): df events with possible nulls values\n            filter_columns (list[str], optional): columns to check for nulls\n            error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n        Returns:\n            pyspark.sql.dataframe.DataFrame: df without null values in specified columns\n        \"\"\"\n\n        for filter_column in filter_columns:\n            filtered_df = df.na.drop(how=\"any\", subset=filter_column)\n            error_and_transformation_counts[(filter_column, ErrorTypes.missing_value, None)] += (\n                df.count() - filtered_df.count()\n            )\n            # because timestamp column is then also used in another filters,\n            # and no error count should be done in the last filter\n            if filter_column not in [ColNames.timestamp, ColNames.mcc]:\n                error_and_transformation_counts[(filter_column, ErrorTypes.no_error, None)] += filtered_df.count()\n\n            df = filtered_df.cache()\n\n        return df\n\n    def filter_invalid_mcc_and_update_qa(\n        self, df: pyspark.sql.dataframe.DataFrame, error_and_transformation_counts: dict[tuple:int]\n    ) -&gt; pyspark.sql.dataframe.DataFrame:\n        \"\"\"\n        Remove any rows where the value for mcc is not a 3 digit number (between 100 and 999)\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): dataframe of events\n            error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: Dataframe with erroneous values of mcc filtered out\n        \"\"\"\n\n        filtered_df = df.filter(F.col(ColNames.mcc).between(100, 999))\n        error_and_transformation_counts[(ColNames.mcc, ErrorTypes.out_of_admissible_values, None)] += (\n            df.count() - filtered_df.count()\n        )\n        # because timestamp column is then also used in another filters,\n        # and no error count should be done in the last filter\n        error_and_transformation_counts[(ColNames.mcc, ErrorTypes.no_error, None)] += filtered_df.count()\n\n        df = filtered_df.cache()\n\n        return df\n\n    def filter_invalid_cell_id_and_update_qa(\n        self, df: pyspark.sql.dataframe.DataFrame, error_and_transformation_counts: dict[tuple:int]\n    ) -&gt; pyspark.sql.dataframe.DataFrame:\n        \"\"\"\n        Remove any rows where the value for cell_id is not a 14 or 15 digit number\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): dataframe of events\n            error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: Dataframe with erroneous values of cell_id filtered out\n        \"\"\"\n\n        filtered_df = df.filter(\n            (\n                ((F.length(F.col(ColNames.cell_id)) == 14) | (F.length(F.col(ColNames.cell_id)) == 15))\n                &amp; (F.col(ColNames.cell_id).cast(\"long\").isNotNull())\n                | F.col(\"cell_id\").isNull()\n            )\n        )\n        error_and_transformation_counts[(ColNames.cell_id, ErrorTypes.out_of_admissible_values, None)] += (\n            df.count() - filtered_df.count()\n        )\n        # because timestamp column is then also used in another filters,\n        # and no error count should be done in the last filter\n        error_and_transformation_counts[(ColNames.cell_id, ErrorTypes.no_error, None)] += filtered_df.count()\n\n        df = filtered_df.cache()\n\n        return df\n\n    def filter_null_locations_and_update_qa(\n        self, df: pyspark.sql.dataframe.DataFrame, error_and_transformation_counts: dict[tuple:int]\n    ) -&gt; pyspark.sql.dataframe.DataFrame:\n        \"\"\"\n        Filter rows with inappropriate location: neither cell_id\n        nor latitude&amp;longitude are specified. Count the number of errors,\n        since this filter depends on several columns \"variable\" in quality metrics table will be None.\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): df with possible null location columns\n            error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: filtered df with correctly specified location\n        \"\"\"\n\n        filtered_df = df.filter(\n            (F.col(ColNames.cell_id).isNotNull())\n            | (F.col(ColNames.longitude).isNotNull() &amp; F.col(ColNames.latitude).isNotNull())\n        )\n\n        error_and_transformation_counts[(None, ErrorTypes.no_location, None)] += df.count() - filtered_df.count()\n\n        return filtered_df\n\n    def convert_time_column_to_timestamp_and_update_qa(\n        self,\n        df: pyspark.sql.dataframe.DataFrame,\n        timestampt_format: str,\n        input_timezone: str,\n        error_and_transformation_counts: dict[tuple:int],\n    ) -&gt; pyspark.sql.dataframe.DataFrame:\n        \"\"\"\n        Based on config params timestampt format and input timezone\n        convert timestampt column from string to timestampt type, if filter rows with failed conversion.\n        Count number of succesful timestampt transformations and number of errors.\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): df with timestampt column\n            timestampt_format (str): expected string format to use in time conversion\n            input_timezone (str): timezone of the input data\n            error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: df with time column parsed to timestamp\n        \"\"\"\n\n        # TODO: Check timestamp validation\n        # Validating timestamp of format '2023-01-03T03:12:00+00:00' raises Exception\n        # Error: Fail to parse '2023-01-03T03:12:00+00:00' in the new parser\n        # Can we use a conditional(F.when) to check if column can be casted?\n        filtered_df = df.withColumn(\n            ColNames.timestamp,\n            F.to_utc_timestamp(F.to_timestamp(ColNames.timestamp, timestampt_format), input_timezone),\n        ).filter(F.col(ColNames.timestamp).isNotNull())\n\n        error_and_transformation_counts[\n            (ColNames.timestamp, None, Transformations.converted_timestamp)\n        ] += filtered_df.count()\n        error_and_transformation_counts[(ColNames.timestamp, ErrorTypes.not_right_syntactic_format, None)] += (\n            df.count() - filtered_df.count()\n        )\n\n        return filtered_df\n\n    def data_period_filter_and_update_qa(\n        self,\n        df: pyspark.sql.dataframe.DataFrame,\n        data_period_start: str,\n        data_period_end: str,\n        error_and_transformation_counts: dict[tuple:int],\n    ) -&gt; pyspark.sql.dataframe.DataFrame:\n        \"\"\"\n        Filter rows which timestampt is not in specified date range.\n        Count the number of error rows for quality metrics,\n        and the number of complitely correct timestampt (that pass all corresponding filters)\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): df with timestamp column\n            data_period_start (str): start of date period\n            data_period_end (str): end of date period\n            error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n        Returns:\n            pyspark.sql.dataframe.DataFrame: df with records in specified date period\n        \"\"\"\n        data_period_start = pd.to_datetime(data_period_start)\n        # timedelta is needed to include records happened in data_period_end\n        data_period_end = pd.to_datetime(data_period_end) + pd.Timedelta(days=1)\n        filtered_df = df.filter(F.col(ColNames.timestamp).between(data_period_start, data_period_end))\n\n        error_and_transformation_counts[(ColNames.timestamp, ErrorTypes.out_of_admissible_values, None)] += (\n            df.count() - filtered_df.count()\n        )\n        # TODO: if we decide not to use data_period_filtering don't forget to put this in convert_time_column_to_timestamp\n        error_and_transformation_counts[(ColNames.timestamp, ErrorTypes.no_error, None)] += filtered_df.count()\n\n        return filtered_df\n\n    def bounding_box_filtering_and_update_qa(\n        self, df: pyspark.sql.dataframe.DataFrame, bounding_box: dict, error_and_transformation_counts: dict[tuple:int]\n    ) -&gt; pyspark.sql.dataframe.DataFrame:\n        \"\"\"\n        Filter rows which not null longitude &amp; latitude values are\n        within coordinates of bounding box. Count the number of errors,\n        since this filter depends on several columns \"variable\" in quality metrics table will be None\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): df with longitude &amp; latitude columns\n            bounding_box (dict): coordinates of bounding box in df_events crs\n            error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: filtered df with records within bbox\n        \"\"\"\n        # coordinates of bounding box should be of the same crs of mno data\n        lat_condition = (\n            F.col(ColNames.latitude).between(bounding_box[\"min_lat\"], bounding_box[\"max_lat\"])\n            | F.col(ColNames.latitude).isNull()\n        )\n        lon_condition = (\n            F.col(ColNames.longitude).between(bounding_box[\"min_lon\"], bounding_box[\"max_lon\"])\n            | F.col(ColNames.longitude).isNull()\n        )\n\n        filtered_df = df.filter(lat_condition &amp; lon_condition)\n        error_and_transformation_counts[(None, ErrorTypes.out_of_bounding_box, None)] += (\n            df.count() - filtered_df.count()\n        )\n\n        return filtered_df\n\n    def calculate_user_id_modulo(\n        self, df: pyspark.sql.dataframe.DataFrame, modulo_value: int, hex_truncation_end: int = 12\n    ) -&gt; pyspark.sql.dataframe.DataFrame:\n        \"\"\"Calculates the extra column user_id_modulo, as the result of the modulo function\n        applied on the binary user id column. The modulo value will affect the number of\n        partitions in the final output.\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): df with user_id column\n            modulo_value (int): modulo value to be used when dividing user id.\n            hex_truncation_end (int): to which character truncate the hex, before sending it to conv function\n                and then to modulo. Anything upward of 13 is likely to result in distributional issues,\n                as the modulo value might not correspond to the number of final partitions.\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: dataframe with user_id_modulo column.\n        \"\"\"\n\n        # TODO make hex truncation (substring parameters) as configurable by user?\n\n        df = df.withColumn(\n            ColNames.user_id_modulo,\n            F.conv(F.substring(F.hex(F.col(ColNames.user_id)), 1, hex_truncation_end), 16, 10).cast(\"long\")\n            % F.lit(modulo_value).cast(\"bigint\"),\n        )\n\n        return df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.bounding_box_filtering_and_update_qa","title":"<code>bounding_box_filtering_and_update_qa(df, bounding_box, error_and_transformation_counts)</code>","text":"<p>Filter rows which not null longitude &amp; latitude values are within coordinates of bounding box. Count the number of errors, since this filter depends on several columns \"variable\" in quality metrics table will be None</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>df with longitude &amp; latitude columns</p> required <code>bounding_box</code> <code>dict</code> <p>coordinates of bounding box in df_events crs</p> required <code>error_and_transformation_counts</code> <code>dict[tuple, int]</code> <p>dictionary to track qa</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: filtered df with records within bbox</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>def bounding_box_filtering_and_update_qa(\n    self, df: pyspark.sql.dataframe.DataFrame, bounding_box: dict, error_and_transformation_counts: dict[tuple:int]\n) -&gt; pyspark.sql.dataframe.DataFrame:\n    \"\"\"\n    Filter rows which not null longitude &amp; latitude values are\n    within coordinates of bounding box. Count the number of errors,\n    since this filter depends on several columns \"variable\" in quality metrics table will be None\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): df with longitude &amp; latitude columns\n        bounding_box (dict): coordinates of bounding box in df_events crs\n        error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: filtered df with records within bbox\n    \"\"\"\n    # coordinates of bounding box should be of the same crs of mno data\n    lat_condition = (\n        F.col(ColNames.latitude).between(bounding_box[\"min_lat\"], bounding_box[\"max_lat\"])\n        | F.col(ColNames.latitude).isNull()\n    )\n    lon_condition = (\n        F.col(ColNames.longitude).between(bounding_box[\"min_lon\"], bounding_box[\"max_lon\"])\n        | F.col(ColNames.longitude).isNull()\n    )\n\n    filtered_df = df.filter(lat_condition &amp; lon_condition)\n    error_and_transformation_counts[(None, ErrorTypes.out_of_bounding_box, None)] += (\n        df.count() - filtered_df.count()\n    )\n\n    return filtered_df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.calculate_user_id_modulo","title":"<code>calculate_user_id_modulo(df, modulo_value, hex_truncation_end=12)</code>","text":"<p>Calculates the extra column user_id_modulo, as the result of the modulo function applied on the binary user id column. The modulo value will affect the number of partitions in the final output.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>df with user_id column</p> required <code>modulo_value</code> <code>int</code> <p>modulo value to be used when dividing user id.</p> required <code>hex_truncation_end</code> <code>int</code> <p>to which character truncate the hex, before sending it to conv function and then to modulo. Anything upward of 13 is likely to result in distributional issues, as the modulo value might not correspond to the number of final partitions.</p> <code>12</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: dataframe with user_id_modulo column.</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>def calculate_user_id_modulo(\n    self, df: pyspark.sql.dataframe.DataFrame, modulo_value: int, hex_truncation_end: int = 12\n) -&gt; pyspark.sql.dataframe.DataFrame:\n    \"\"\"Calculates the extra column user_id_modulo, as the result of the modulo function\n    applied on the binary user id column. The modulo value will affect the number of\n    partitions in the final output.\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): df with user_id column\n        modulo_value (int): modulo value to be used when dividing user id.\n        hex_truncation_end (int): to which character truncate the hex, before sending it to conv function\n            and then to modulo. Anything upward of 13 is likely to result in distributional issues,\n            as the modulo value might not correspond to the number of final partitions.\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: dataframe with user_id_modulo column.\n    \"\"\"\n\n    # TODO make hex truncation (substring parameters) as configurable by user?\n\n    df = df.withColumn(\n        ColNames.user_id_modulo,\n        F.conv(F.substring(F.hex(F.col(ColNames.user_id)), 1, hex_truncation_end), 16, 10).cast(\"long\")\n        % F.lit(modulo_value).cast(\"bigint\"),\n    )\n\n    return df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.convert_time_column_to_timestamp_and_update_qa","title":"<code>convert_time_column_to_timestamp_and_update_qa(df, timestampt_format, input_timezone, error_and_transformation_counts)</code>","text":"<p>Based on config params timestampt format and input timezone convert timestampt column from string to timestampt type, if filter rows with failed conversion. Count number of succesful timestampt transformations and number of errors.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>df with timestampt column</p> required <code>timestampt_format</code> <code>str</code> <p>expected string format to use in time conversion</p> required <code>input_timezone</code> <code>str</code> <p>timezone of the input data</p> required <code>error_and_transformation_counts</code> <code>dict[tuple, int]</code> <p>dictionary to track qa</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: df with time column parsed to timestamp</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>def convert_time_column_to_timestamp_and_update_qa(\n    self,\n    df: pyspark.sql.dataframe.DataFrame,\n    timestampt_format: str,\n    input_timezone: str,\n    error_and_transformation_counts: dict[tuple:int],\n) -&gt; pyspark.sql.dataframe.DataFrame:\n    \"\"\"\n    Based on config params timestampt format and input timezone\n    convert timestampt column from string to timestampt type, if filter rows with failed conversion.\n    Count number of succesful timestampt transformations and number of errors.\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): df with timestampt column\n        timestampt_format (str): expected string format to use in time conversion\n        input_timezone (str): timezone of the input data\n        error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: df with time column parsed to timestamp\n    \"\"\"\n\n    # TODO: Check timestamp validation\n    # Validating timestamp of format '2023-01-03T03:12:00+00:00' raises Exception\n    # Error: Fail to parse '2023-01-03T03:12:00+00:00' in the new parser\n    # Can we use a conditional(F.when) to check if column can be casted?\n    filtered_df = df.withColumn(\n        ColNames.timestamp,\n        F.to_utc_timestamp(F.to_timestamp(ColNames.timestamp, timestampt_format), input_timezone),\n    ).filter(F.col(ColNames.timestamp).isNotNull())\n\n    error_and_transformation_counts[\n        (ColNames.timestamp, None, Transformations.converted_timestamp)\n    ] += filtered_df.count()\n    error_and_transformation_counts[(ColNames.timestamp, ErrorTypes.not_right_syntactic_format, None)] += (\n        df.count() - filtered_df.count()\n    )\n\n    return filtered_df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.data_period_filter_and_update_qa","title":"<code>data_period_filter_and_update_qa(df, data_period_start, data_period_end, error_and_transformation_counts)</code>","text":"<p>Filter rows which timestampt is not in specified date range. Count the number of error rows for quality metrics, and the number of complitely correct timestampt (that pass all corresponding filters)</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>df with timestamp column</p> required <code>data_period_start</code> <code>str</code> <p>start of date period</p> required <code>data_period_end</code> <code>str</code> <p>end of date period</p> required <code>error_and_transformation_counts</code> <code>dict[tuple, int]</code> <p>dictionary to track qa</p> required <p>Returns:     pyspark.sql.dataframe.DataFrame: df with records in specified date period</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>def data_period_filter_and_update_qa(\n    self,\n    df: pyspark.sql.dataframe.DataFrame,\n    data_period_start: str,\n    data_period_end: str,\n    error_and_transformation_counts: dict[tuple:int],\n) -&gt; pyspark.sql.dataframe.DataFrame:\n    \"\"\"\n    Filter rows which timestampt is not in specified date range.\n    Count the number of error rows for quality metrics,\n    and the number of complitely correct timestampt (that pass all corresponding filters)\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): df with timestamp column\n        data_period_start (str): start of date period\n        data_period_end (str): end of date period\n        error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n    Returns:\n        pyspark.sql.dataframe.DataFrame: df with records in specified date period\n    \"\"\"\n    data_period_start = pd.to_datetime(data_period_start)\n    # timedelta is needed to include records happened in data_period_end\n    data_period_end = pd.to_datetime(data_period_end) + pd.Timedelta(days=1)\n    filtered_df = df.filter(F.col(ColNames.timestamp).between(data_period_start, data_period_end))\n\n    error_and_transformation_counts[(ColNames.timestamp, ErrorTypes.out_of_admissible_values, None)] += (\n        df.count() - filtered_df.count()\n    )\n    # TODO: if we decide not to use data_period_filtering don't forget to put this in convert_time_column_to_timestamp\n    error_and_transformation_counts[(ColNames.timestamp, ErrorTypes.no_error, None)] += filtered_df.count()\n\n    return filtered_df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.filter_invalid_cell_id_and_update_qa","title":"<code>filter_invalid_cell_id_and_update_qa(df, error_and_transformation_counts)</code>","text":"<p>Remove any rows where the value for cell_id is not a 14 or 15 digit number</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe of events</p> required <code>error_and_transformation_counts</code> <code>dict[tuple, int]</code> <p>dictionary to track qa</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: Dataframe with erroneous values of cell_id filtered out</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>def filter_invalid_cell_id_and_update_qa(\n    self, df: pyspark.sql.dataframe.DataFrame, error_and_transformation_counts: dict[tuple:int]\n) -&gt; pyspark.sql.dataframe.DataFrame:\n    \"\"\"\n    Remove any rows where the value for cell_id is not a 14 or 15 digit number\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): dataframe of events\n        error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: Dataframe with erroneous values of cell_id filtered out\n    \"\"\"\n\n    filtered_df = df.filter(\n        (\n            ((F.length(F.col(ColNames.cell_id)) == 14) | (F.length(F.col(ColNames.cell_id)) == 15))\n            &amp; (F.col(ColNames.cell_id).cast(\"long\").isNotNull())\n            | F.col(\"cell_id\").isNull()\n        )\n    )\n    error_and_transformation_counts[(ColNames.cell_id, ErrorTypes.out_of_admissible_values, None)] += (\n        df.count() - filtered_df.count()\n    )\n    # because timestamp column is then also used in another filters,\n    # and no error count should be done in the last filter\n    error_and_transformation_counts[(ColNames.cell_id, ErrorTypes.no_error, None)] += filtered_df.count()\n\n    df = filtered_df.cache()\n\n    return df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.filter_invalid_mcc_and_update_qa","title":"<code>filter_invalid_mcc_and_update_qa(df, error_and_transformation_counts)</code>","text":"<p>Remove any rows where the value for mcc is not a 3 digit number (between 100 and 999)</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe of events</p> required <code>error_and_transformation_counts</code> <code>dict[tuple, int]</code> <p>dictionary to track qa</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: Dataframe with erroneous values of mcc filtered out</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>def filter_invalid_mcc_and_update_qa(\n    self, df: pyspark.sql.dataframe.DataFrame, error_and_transformation_counts: dict[tuple:int]\n) -&gt; pyspark.sql.dataframe.DataFrame:\n    \"\"\"\n    Remove any rows where the value for mcc is not a 3 digit number (between 100 and 999)\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): dataframe of events\n        error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: Dataframe with erroneous values of mcc filtered out\n    \"\"\"\n\n    filtered_df = df.filter(F.col(ColNames.mcc).between(100, 999))\n    error_and_transformation_counts[(ColNames.mcc, ErrorTypes.out_of_admissible_values, None)] += (\n        df.count() - filtered_df.count()\n    )\n    # because timestamp column is then also used in another filters,\n    # and no error count should be done in the last filter\n    error_and_transformation_counts[(ColNames.mcc, ErrorTypes.no_error, None)] += filtered_df.count()\n\n    df = filtered_df.cache()\n\n    return df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.filter_null_locations_and_update_qa","title":"<code>filter_null_locations_and_update_qa(df, error_and_transformation_counts)</code>","text":"<p>Filter rows with inappropriate location: neither cell_id nor latitude&amp;longitude are specified. Count the number of errors, since this filter depends on several columns \"variable\" in quality metrics table will be None.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>df with possible null location columns</p> required <code>error_and_transformation_counts</code> <code>dict[tuple, int]</code> <p>dictionary to track qa</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: filtered df with correctly specified location</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>def filter_null_locations_and_update_qa(\n    self, df: pyspark.sql.dataframe.DataFrame, error_and_transformation_counts: dict[tuple:int]\n) -&gt; pyspark.sql.dataframe.DataFrame:\n    \"\"\"\n    Filter rows with inappropriate location: neither cell_id\n    nor latitude&amp;longitude are specified. Count the number of errors,\n    since this filter depends on several columns \"variable\" in quality metrics table will be None.\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): df with possible null location columns\n        error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: filtered df with correctly specified location\n    \"\"\"\n\n    filtered_df = df.filter(\n        (F.col(ColNames.cell_id).isNotNull())\n        | (F.col(ColNames.longitude).isNotNull() &amp; F.col(ColNames.latitude).isNotNull())\n    )\n\n    error_and_transformation_counts[(None, ErrorTypes.no_location, None)] += df.count() - filtered_df.count()\n\n    return filtered_df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.filter_nulls_and_update_qa","title":"<code>filter_nulls_and_update_qa(df, filter_columns, error_and_transformation_counts)</code>","text":"<p>Loop throuh filter columns (user_id, timestamp) delete rows that have null in the corresponfing column. Counts the number of filtered records for quality metrics, for user_id also calculates the overall correct values, since null check is the one and only filter for this column</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>df events with possible nulls values</p> required <code>filter_columns</code> <code>list[str]</code> <p>columns to check for nulls</p> required <code>error_and_transformation_counts</code> <code>dict[tuple, int]</code> <p>dictionary to track qa</p> required <p>Returns:     pyspark.sql.dataframe.DataFrame: df without null values in specified columns</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>def filter_nulls_and_update_qa(\n    self,\n    df: pyspark.sql.dataframe.DataFrame,\n    filter_columns: list[str],\n    error_and_transformation_counts: dict[tuple:int],\n) -&gt; pyspark.sql.dataframe.DataFrame:\n    \"\"\"\n    Loop throuh filter columns (user_id, timestamp)\n    delete rows that have null in the corresponfing column.\n    Counts the number of filtered records for quality metrics,\n    for user_id also calculates the overall correct values,\n    since null check is the one and only filter for this column\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): df events with possible nulls values\n        filter_columns (list[str], optional): columns to check for nulls\n        error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n    Returns:\n        pyspark.sql.dataframe.DataFrame: df without null values in specified columns\n    \"\"\"\n\n    for filter_column in filter_columns:\n        filtered_df = df.na.drop(how=\"any\", subset=filter_column)\n        error_and_transformation_counts[(filter_column, ErrorTypes.missing_value, None)] += (\n            df.count() - filtered_df.count()\n        )\n        # because timestamp column is then also used in another filters,\n        # and no error count should be done in the last filter\n        if filter_column not in [ColNames.timestamp, ColNames.mcc]:\n            error_and_transformation_counts[(filter_column, ErrorTypes.no_error, None)] += filtered_df.count()\n\n        df = filtered_df.cache()\n\n    return df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.save_syntactic_quality_metrics_by_column","title":"<code>save_syntactic_quality_metrics_by_column()</code>","text":"<p>Convert output_qa_by_column.error_and_transformation_counts dictionary into spark df. Add additional columns, match schema of SilverEventDataSyntacticQualityMetricsByColumn class. Write results in default path of this class</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>def save_syntactic_quality_metrics_by_column(self):\n    \"\"\"\n    Convert output_qa_by_column.error_and_transformation_counts dictionary into spark df.\n    Add additional columns, match schema of SilverEventDataSyntacticQualityMetricsByColumn class.\n    Write results in default path of this class\n    \"\"\"\n    # self.output_qa_by_column( variable, type_of_error, type_of_transformation) : value\n    # 3 Nones to match the expected schema\n    df_tuples = [\n        (variable, type_of_error, type_of_transformation, value)\n        for (\n            variable,\n            type_of_error,\n            type_of_transformation,\n        ), value in self.output_qa_by_column.error_and_transformation_counts.items()\n    ]\n    # clear dictionary after each preprocessed day\n    self.output_qa_by_column.error_and_transformation_counts.clear()\n\n    temp_schema = StructType(\n        [\n            StructField(ColNames.variable, StringType(), nullable=True),\n            StructField(ColNames.type_of_error, ShortType(), nullable=True),\n            StructField(ColNames.type_of_transformation, ShortType(), nullable=True),\n            StructField(ColNames.value, IntegerType(), nullable=False),\n        ]\n    )\n\n    self.output_qa_by_column.df = self.spark.createDataFrame(df_tuples, temp_schema)\n\n    self.output_qa_by_column.df = self.output_qa_by_column.df.withColumns(\n        {\n            ColNames.result_timestamp: F.lit(F.current_timestamp()),\n            ColNames.date: F.to_date(F.lit(self.current_date), self.spark_data_folder_date_format),\n        }\n    ).select(self.output_qa_by_column.SCHEMA.fieldNames())\n\n    self.output_qa_by_column.df = self.spark.createDataFrame(\n        self.output_qa_by_column.df.rdd, self.output_qa_by_column.SCHEMA\n    )\n\n    self.output_qa_by_column.write()\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.save_syntactic_quality_metrics_frequency_distribution","title":"<code>save_syntactic_quality_metrics_frequency_distribution()</code>","text":"<p>Join frequency distribution tables before and after, from after table take only final_frequency and replace nulls with 0. Create additional column date in DateType(), match the schema of SilverEventDataSyntacticQualityMetricsByColumn class Write chunk results in separate folders named by processing date</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>def save_syntactic_quality_metrics_frequency_distribution(self):\n    \"\"\"\n    Join frequency distribution tables before and after,\n    from after table take only final_frequency and replace nulls with 0.\n    Create additional column date in DateType(),\n    match the schema of SilverEventDataSyntacticQualityMetricsByColumn class\n    Write chunk results in separate folders named by processing date\n    \"\"\"\n\n    # Using outer join and groupby after because Spark can not handle joining with null values in columns\n    self.output_qa_freq_distribution.df = self.quality_metrics_distribution_before.join(\n        self.quality_metrics_distribution_after, [ColNames.cell_id, ColNames.user_id], \"outer\"\n    ).select(self.quality_metrics_distribution_before.columns + [ColNames.final_frequency])\n\n    self.output_qa_freq_distribution.df = self.output_qa_freq_distribution.df.groupBy(\n        ColNames.cell_id, ColNames.user_id\n    ).sum(ColNames.initial_frequency, ColNames.final_frequency)\n    self.output_qa_freq_distribution.df = self.output_qa_freq_distribution.df.withColumnRenamed(\n        f\"sum({ColNames.initial_frequency})\", ColNames.initial_frequency\n    )\n    self.output_qa_freq_distribution.df = self.output_qa_freq_distribution.df.withColumnRenamed(\n        f\"sum({ColNames.final_frequency})\", ColNames.final_frequency\n    )\n\n    self.output_qa_freq_distribution.df = self.output_qa_freq_distribution.df.fillna(0, ColNames.final_frequency)\n\n    self.output_qa_freq_distribution.df = self.output_qa_freq_distribution.df.withColumn(\n        ColNames.date, F.to_date(F.lit(self.current_date), self.spark_data_folder_date_format)\n    )\n\n    self.output_qa_freq_distribution.df = self.spark.createDataFrame(\n        self.output_qa_freq_distribution.df.rdd, self.output_qa_freq_distribution.SCHEMA\n    )\n\n    self.output_qa_freq_distribution.write()\n</code></pre>"},{"location":"reference/components/execution/event_deduplication/","title":"event_deduplication","text":""},{"location":"reference/components/execution/event_deduplication/event_deduplication/","title":"event_deduplication","text":"<p>Module that cleans RAW MNO Event data.</p>"},{"location":"reference/components/execution/event_deduplication/event_deduplication/#components.execution.event_deduplication.event_deduplication.EventDeduplication","title":"<code>EventDeduplication</code>","text":"<p>             Bases: <code>Component</code></p> <p>Class that removes duplicates from clean MNO Event data</p> Source code in <code>multimno/components/execution/event_deduplication/event_deduplication.py</code> <pre><code>class EventDeduplication(Component):\n    \"\"\"\n    Class that removes duplicates from clean MNO Event data\n    \"\"\"\n\n    COMPONENT_ID = \"EventDeduplication\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.spark_data_folder_date_format = self.config.get(\n            EventDeduplication.COMPONENT_ID, \"spark_data_folder_date_format\"\n        )\n\n    def initalize_data_objects(self):\n        # Input\n        self.input_silver_events_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"event_data_silver\")\n\n        self.data_period_start = self.config.get(EventDeduplication.COMPONENT_ID, \"data_period_start\")\n        self.data_period_end = self.config.get(EventDeduplication.COMPONENT_ID, \"data_period_end\")\n        self.data_folder_date_format = self.config.get(EventDeduplication.COMPONENT_ID, \"data_folder_date_format\")\n        self.clear_destination_directory = self.config.get(\n            EventDeduplication.COMPONENT_ID, \"clear_destination_directory\"\n        )\n\n        # Create all possible dates between start and end\n        sdate = pd.to_datetime(self.data_period_start)\n        edate = pd.to_datetime(self.data_period_end)\n        self.to_process_dates = list(pd.date_range(sdate, edate, freq=\"d\"))\n\n        # Create all input data objects\n        self.input_event_data_objects = {SilverEventDataObject.ID: None}\n        path = self.input_silver_events_path\n\n        if check_if_data_path_exists(self.spark, path):\n            # Single input option, with a filter follwed by it\n            self.input_event_data_objects[SilverEventDataObject.ID] = SilverEventDataObject(\n                self.spark, self.input_silver_events_path\n            )\n\n        else:\n            self.logger.warning(f\"Expected path {path} to exist but it does not\")\n\n        # Output data objects dictionary\n        self.output_data_objects = {}\n\n        deduplicated_silver_events_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"event_data_silver_deduplicated\")\n        silver_event_do = SilverEventDataObject(self.spark, deduplicated_silver_events_path)\n        self.output_data_objects[SilverEventDataObject.ID] = silver_event_do\n\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, silver_event_do.default_path)\n\n        event_deduplicated_quality_metrics_by_column_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"event_deduplicated_quality_metrics_by_column\"\n        )\n        event_deduplicated_quality_metrics_by_column = SilverEventDataSyntacticQualityMetricsByColumn(\n            self.spark, event_deduplicated_quality_metrics_by_column_path\n        )\n        self.output_data_objects[SilverEventDataSyntacticQualityMetricsByColumn.ID] = (\n            event_deduplicated_quality_metrics_by_column\n        )\n\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, event_deduplicated_quality_metrics_by_column.default_path)\n\n        self.output_qa_by_column = event_deduplicated_quality_metrics_by_column\n\n        event_deduplicated_quality_metrics_frequency_distribution_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"event_deduplicated_quality_metrics_frequency_distribution\"\n        )\n        event_deduplicated_quality_metrics_frequency_distribution = (\n            SilverEventDataSyntacticQualityMetricsFrequencyDistribution(\n                self.spark, event_deduplicated_quality_metrics_frequency_distribution_path\n            )\n        )\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, event_deduplicated_quality_metrics_frequency_distribution.default_path)\n\n        self.output_data_objects[SilverEventDataSyntacticQualityMetricsFrequencyDistribution.ID] = (\n            event_deduplicated_quality_metrics_frequency_distribution\n        )\n\n        # this instance of SilverEventDataSyntacticQualityMetricsFrequencyDistribution class\n        # will be used to write frequency distrobution of each preprocessing date (chunk)\n        # the path argument will be changed dynamically\n\n        self.output_qa_freq_distribution = event_deduplicated_quality_metrics_frequency_distribution\n\n    def read(self):\n        self.input_event_data_objects[SilverEventDataObject.ID].read()\n        # self.read()\n\n    def write(self):\n        self.output_data_objects[SilverEventDataObject.ID].write()\n        self.save_deduplicated_quality_metrics_frequency_distribution()\n\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n\n        self.read()\n\n        # for current_date in self.dates_to_process:\n        for current_date in self.to_process_dates:\n            self.current_date = current_date\n\n            self.current_input_do = self.input_event_data_objects[SilverEventDataObject.ID].df.filter(\n                (\n                    (psf.col(ColNames.year) == current_date.year)\n                    &amp; (psf.col(ColNames.month) == current_date.month)\n                    &amp; (psf.col(ColNames.day) == current_date.day)\n                )\n            )\n\n            self.transform()\n\n            if self.output_qa_by_column.error_and_transformation_counts is not None:\n                self.save_deduplicated_quality_metrics_by_column()\n\n            self.write()\n\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n        df_events = self.current_input_do\n\n        # cache before each filter function because we appply action count()\n        df_events = df_events.cache()\n\n        self.quality_metrics_distribution_before = (\n            df_events.groupBy(ColNames.cell_id, ColNames.user_id, ColNames.user_id_modulo)\n            .agg(psf.count(\"*\").alias(ColNames.initial_frequency))\n            .drop(ColNames.user_id_modulo)\n        )\n\n        # Perform duplicate removal\n\n        df_events = self.remove_same_location_duplicates_and_update_qa(\n            df_events,\n            self.output_qa_by_column.error_and_transformation_counts,\n        )\n\n        df_events = self.remove_different_location_duplicates_and_update_qa(\n            df_events,\n            self.output_qa_by_column.error_and_transformation_counts,\n        )\n\n        df_events = df_events.withColumn(\n            ColNames.date, psf.to_date(psf.col(ColNames.timestamp), self.spark_data_folder_date_format)\n        )\n\n        self.quality_metrics_distribution_after = (\n            df_events.groupBy(ColNames.cell_id, ColNames.user_id, ColNames.user_id_modulo, ColNames.date)\n            .agg(psf.count(\"*\").alias(ColNames.final_frequency))\n            .drop(ColNames.user_id_modulo)\n        )\n\n        df_events = df_events.drop(ColNames.date)  # dropping internal date used for prior calculation of metrics\n\n        # TODO: currently results in several files per one partition when commented out\n        # Improve this part after performance tests\n\n        # df_events = df_events.repartition(ColNames.user_id_modulo)\n        # df_events = df_events.sortWithinPartitions(ColNames.user_id_modulo, ColNames.user_id, ColNames.timestamp)\n\n        self.output_data_objects[SilverEventDataObject.ID].df = self.spark.createDataFrame(\n            df_events.rdd, SilverEventDataObject.SCHEMA\n        )\n\n        # after each chunk processing clear all Cache to free memory and disk\n        self.spark.catalog.clearCache()\n\n    def save_deduplicated_quality_metrics_frequency_distribution(self):\n        \"\"\"\n        Join frequency distribution tables before and after,\n        from after table take only final_frequency and replace nulls with 0.\n        Create additional column date in DateType(),\n        match the schema of SilverEventDataDeduplicationQualityMetricsByColumn class\n        Write chunk results in separate folders named by processing date\n        \"\"\"\n\n        self.output_qa_freq_distribution.df = self.quality_metrics_distribution_before.join(\n            self.quality_metrics_distribution_after, [ColNames.cell_id, ColNames.user_id], \"left\"\n        ).select(self.quality_metrics_distribution_before.columns + [ColNames.final_frequency])\n\n        self.output_qa_freq_distribution.df = self.output_qa_freq_distribution.df.fillna(0, ColNames.final_frequency)\n\n        self.output_qa_freq_distribution.df = self.output_qa_freq_distribution.df.withColumn(\n            ColNames.date, psf.to_date(psf.lit(self.current_date), self.spark_data_folder_date_format)\n        )\n\n        self.output_qa_freq_distribution.df = self.spark.createDataFrame(\n            self.output_qa_freq_distribution.df.rdd, self.output_qa_freq_distribution.SCHEMA\n        )\n\n        self.output_qa_freq_distribution.write()\n\n    def save_deduplicated_quality_metrics_by_column(self):\n        \"\"\"\n        Convert output_qa_by_column.error_and_transformation_counts dictionary into spark df.\n        Add additional columns, match schema of SilverEventDataDeduplicationQualityMetricsByColumn class.\n        Write results in default path of this class\n        \"\"\"\n        # self.output_qa_by_column( variable, type_of_error, type_of_transformation) : value\n        # 3 Nones to match the expected schema\n        df_tuples = [\n            (variable, type_of_error, type_of_transformation, value)\n            for (\n                variable,\n                type_of_error,\n                type_of_transformation,\n            ), value in self.output_qa_by_column.error_and_transformation_counts.items()\n        ]\n\n        self.output_qa_by_column.error_and_transformation_counts.clear()\n\n        temp_schema = StructType(\n            [\n                StructField(ColNames.variable, StringType(), nullable=True),\n                StructField(ColNames.type_of_error, ShortType(), nullable=True),\n                StructField(ColNames.type_of_transformation, ShortType(), nullable=True),\n                StructField(ColNames.value, IntegerType(), nullable=False),\n            ]\n        )\n\n        self.output_qa_by_column.df = self.spark.createDataFrame(df_tuples, temp_schema)\n\n        self.output_qa_by_column.df = self.output_qa_by_column.df.withColumns(\n            {\n                ColNames.result_timestamp: psf.lit(psf.current_timestamp()),\n                ColNames.date: psf.to_date(psf.lit(self.current_date), self.spark_data_folder_date_format),\n            }\n        ).select(self.output_qa_by_column.SCHEMA.fieldNames())\n\n        self.output_qa_by_column.df = self.spark.createDataFrame(\n            self.output_qa_by_column.df.rdd, self.output_qa_by_column.SCHEMA\n        )\n\n        self.output_qa_by_column.write()\n\n    def remove_same_location_duplicates_and_update_qa(\n        self,\n        df: pyspark.sql.dataframe.DataFrame,\n        duplication_counts: dict[tuple:int],\n    ) -&gt; pyspark.sql.dataframe.DataFrame:\n        \"\"\"\n        Remove rows that have identical records for\n        timestamp, cell_id, longitude, latitude and user_id.\n        Counts the number of removed records for quality metrics.\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): df events with possible duplicates\n            duplication_counts (dict[tuple, int]): dictionary to track qa\n        Returns:\n            pyspark.sql.dataframe.DataFrame: df without duplicate rows\n            in terms of user_id, cell_id, latitutde, longitude\n            and timestamp combination\n        \"\"\"\n\n        # if lot and lan are null, they are null for all rows (presumably)\n        # the same goes for cell_id\n\n        deduplicated_df = df.drop_duplicates(\n            [ColNames.user_id, ColNames.cell_id, ColNames.latitude, ColNames.longitude, ColNames.timestamp]\n        )\n\n        duplication_counts[(None, ErrorTypes.same_location_duplicate, None)] += df.count() - deduplicated_df.count()\n\n        return deduplicated_df\n\n    def remove_different_location_duplicates_and_update_qa(\n        self,\n        df: pyspark.sql.dataframe.DataFrame,\n        duplication_counts: dict[tuple:int],\n    ) -&gt; pyspark.sql.dataframe.DataFrame:\n        \"\"\"\n        Remove rows that have the same timestamp for a user.\n        Requires input where same location duplicates have been removed.\n        In case a user has two or more identical records for a given timestamp,\n        all of such records are removed. The duplication check is performed\n        over two columns: timestamp and user_id.\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): df events with possible duplicates for\n                different locations, but no duplicates for same location\n                duplication_counts (dict[tuple, int]): dictionary to track qa\n        Returns:\n            pyspark.sql.dataframe.DataFrame: df without duplicate rows\n                in terms of user_id and timestamp combination\n        \"\"\"\n\n        window_dedupl = Window.partitionBy(*[ColNames.user_id_modulo, ColNames.user_id, ColNames.timestamp])\n\n        deduplicated_df = df.withColumn(\"count\", psf.count(\"*\").over(window_dedupl)).where(\"count=1\").drop(\"count\")\n\n        duplication_counts[(None, ErrorTypes.different_location_duplicate, None)] += (\n            df.count() - deduplicated_df.count()\n        )\n\n        return deduplicated_df\n</code></pre>"},{"location":"reference/components/execution/event_deduplication/event_deduplication/#components.execution.event_deduplication.event_deduplication.EventDeduplication.remove_different_location_duplicates_and_update_qa","title":"<code>remove_different_location_duplicates_and_update_qa(df, duplication_counts)</code>","text":"<p>Remove rows that have the same timestamp for a user. Requires input where same location duplicates have been removed. In case a user has two or more identical records for a given timestamp, all of such records are removed. The duplication check is performed over two columns: timestamp and user_id.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>df events with possible duplicates for different locations, but no duplicates for same location duplication_counts (dict[tuple, int]): dictionary to track qa</p> required <p>Returns:     pyspark.sql.dataframe.DataFrame: df without duplicate rows         in terms of user_id and timestamp combination</p> Source code in <code>multimno/components/execution/event_deduplication/event_deduplication.py</code> <pre><code>def remove_different_location_duplicates_and_update_qa(\n    self,\n    df: pyspark.sql.dataframe.DataFrame,\n    duplication_counts: dict[tuple:int],\n) -&gt; pyspark.sql.dataframe.DataFrame:\n    \"\"\"\n    Remove rows that have the same timestamp for a user.\n    Requires input where same location duplicates have been removed.\n    In case a user has two or more identical records for a given timestamp,\n    all of such records are removed. The duplication check is performed\n    over two columns: timestamp and user_id.\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): df events with possible duplicates for\n            different locations, but no duplicates for same location\n            duplication_counts (dict[tuple, int]): dictionary to track qa\n    Returns:\n        pyspark.sql.dataframe.DataFrame: df without duplicate rows\n            in terms of user_id and timestamp combination\n    \"\"\"\n\n    window_dedupl = Window.partitionBy(*[ColNames.user_id_modulo, ColNames.user_id, ColNames.timestamp])\n\n    deduplicated_df = df.withColumn(\"count\", psf.count(\"*\").over(window_dedupl)).where(\"count=1\").drop(\"count\")\n\n    duplication_counts[(None, ErrorTypes.different_location_duplicate, None)] += (\n        df.count() - deduplicated_df.count()\n    )\n\n    return deduplicated_df\n</code></pre>"},{"location":"reference/components/execution/event_deduplication/event_deduplication/#components.execution.event_deduplication.event_deduplication.EventDeduplication.remove_same_location_duplicates_and_update_qa","title":"<code>remove_same_location_duplicates_and_update_qa(df, duplication_counts)</code>","text":"<p>Remove rows that have identical records for timestamp, cell_id, longitude, latitude and user_id. Counts the number of removed records for quality metrics.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>df events with possible duplicates</p> required <code>duplication_counts</code> <code>dict[tuple, int]</code> <p>dictionary to track qa</p> required <p>Returns:     pyspark.sql.dataframe.DataFrame: df without duplicate rows     in terms of user_id, cell_id, latitutde, longitude     and timestamp combination</p> Source code in <code>multimno/components/execution/event_deduplication/event_deduplication.py</code> <pre><code>def remove_same_location_duplicates_and_update_qa(\n    self,\n    df: pyspark.sql.dataframe.DataFrame,\n    duplication_counts: dict[tuple:int],\n) -&gt; pyspark.sql.dataframe.DataFrame:\n    \"\"\"\n    Remove rows that have identical records for\n    timestamp, cell_id, longitude, latitude and user_id.\n    Counts the number of removed records for quality metrics.\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): df events with possible duplicates\n        duplication_counts (dict[tuple, int]): dictionary to track qa\n    Returns:\n        pyspark.sql.dataframe.DataFrame: df without duplicate rows\n        in terms of user_id, cell_id, latitutde, longitude\n        and timestamp combination\n    \"\"\"\n\n    # if lot and lan are null, they are null for all rows (presumably)\n    # the same goes for cell_id\n\n    deduplicated_df = df.drop_duplicates(\n        [ColNames.user_id, ColNames.cell_id, ColNames.latitude, ColNames.longitude, ColNames.timestamp]\n    )\n\n    duplication_counts[(None, ErrorTypes.same_location_duplicate, None)] += df.count() - deduplicated_df.count()\n\n    return deduplicated_df\n</code></pre>"},{"location":"reference/components/execution/event_deduplication/event_deduplication/#components.execution.event_deduplication.event_deduplication.EventDeduplication.save_deduplicated_quality_metrics_by_column","title":"<code>save_deduplicated_quality_metrics_by_column()</code>","text":"<p>Convert output_qa_by_column.error_and_transformation_counts dictionary into spark df. Add additional columns, match schema of SilverEventDataDeduplicationQualityMetricsByColumn class. Write results in default path of this class</p> Source code in <code>multimno/components/execution/event_deduplication/event_deduplication.py</code> <pre><code>def save_deduplicated_quality_metrics_by_column(self):\n    \"\"\"\n    Convert output_qa_by_column.error_and_transformation_counts dictionary into spark df.\n    Add additional columns, match schema of SilverEventDataDeduplicationQualityMetricsByColumn class.\n    Write results in default path of this class\n    \"\"\"\n    # self.output_qa_by_column( variable, type_of_error, type_of_transformation) : value\n    # 3 Nones to match the expected schema\n    df_tuples = [\n        (variable, type_of_error, type_of_transformation, value)\n        for (\n            variable,\n            type_of_error,\n            type_of_transformation,\n        ), value in self.output_qa_by_column.error_and_transformation_counts.items()\n    ]\n\n    self.output_qa_by_column.error_and_transformation_counts.clear()\n\n    temp_schema = StructType(\n        [\n            StructField(ColNames.variable, StringType(), nullable=True),\n            StructField(ColNames.type_of_error, ShortType(), nullable=True),\n            StructField(ColNames.type_of_transformation, ShortType(), nullable=True),\n            StructField(ColNames.value, IntegerType(), nullable=False),\n        ]\n    )\n\n    self.output_qa_by_column.df = self.spark.createDataFrame(df_tuples, temp_schema)\n\n    self.output_qa_by_column.df = self.output_qa_by_column.df.withColumns(\n        {\n            ColNames.result_timestamp: psf.lit(psf.current_timestamp()),\n            ColNames.date: psf.to_date(psf.lit(self.current_date), self.spark_data_folder_date_format),\n        }\n    ).select(self.output_qa_by_column.SCHEMA.fieldNames())\n\n    self.output_qa_by_column.df = self.spark.createDataFrame(\n        self.output_qa_by_column.df.rdd, self.output_qa_by_column.SCHEMA\n    )\n\n    self.output_qa_by_column.write()\n</code></pre>"},{"location":"reference/components/execution/event_deduplication/event_deduplication/#components.execution.event_deduplication.event_deduplication.EventDeduplication.save_deduplicated_quality_metrics_frequency_distribution","title":"<code>save_deduplicated_quality_metrics_frequency_distribution()</code>","text":"<p>Join frequency distribution tables before and after, from after table take only final_frequency and replace nulls with 0. Create additional column date in DateType(), match the schema of SilverEventDataDeduplicationQualityMetricsByColumn class Write chunk results in separate folders named by processing date</p> Source code in <code>multimno/components/execution/event_deduplication/event_deduplication.py</code> <pre><code>def save_deduplicated_quality_metrics_frequency_distribution(self):\n    \"\"\"\n    Join frequency distribution tables before and after,\n    from after table take only final_frequency and replace nulls with 0.\n    Create additional column date in DateType(),\n    match the schema of SilverEventDataDeduplicationQualityMetricsByColumn class\n    Write chunk results in separate folders named by processing date\n    \"\"\"\n\n    self.output_qa_freq_distribution.df = self.quality_metrics_distribution_before.join(\n        self.quality_metrics_distribution_after, [ColNames.cell_id, ColNames.user_id], \"left\"\n    ).select(self.quality_metrics_distribution_before.columns + [ColNames.final_frequency])\n\n    self.output_qa_freq_distribution.df = self.output_qa_freq_distribution.df.fillna(0, ColNames.final_frequency)\n\n    self.output_qa_freq_distribution.df = self.output_qa_freq_distribution.df.withColumn(\n        ColNames.date, psf.to_date(psf.lit(self.current_date), self.spark_data_folder_date_format)\n    )\n\n    self.output_qa_freq_distribution.df = self.spark.createDataFrame(\n        self.output_qa_freq_distribution.df.rdd, self.output_qa_freq_distribution.SCHEMA\n    )\n\n    self.output_qa_freq_distribution.write()\n</code></pre>"},{"location":"reference/components/execution/event_semantic_cleaning/","title":"event_semantic_cleaning","text":""},{"location":"reference/components/execution/event_semantic_cleaning/event_semantic_cleaning/","title":"event_semantic_cleaning","text":"<p>Module that computes semantic checks on event data and adds error flags</p>"},{"location":"reference/components/execution/event_semantic_cleaning/event_semantic_cleaning/#components.execution.event_semantic_cleaning.event_semantic_cleaning.SemanticCleaning","title":"<code>SemanticCleaning</code>","text":"<p>             Bases: <code>Component</code></p> <p>Class that performs semantic checks on event data and adds error flags</p> Source code in <code>multimno/components/execution/event_semantic_cleaning/event_semantic_cleaning.py</code> <pre><code>class SemanticCleaning(Component):\n    \"\"\"\n    Class that performs semantic checks on event data and adds error flags\n    \"\"\"\n\n    COMPONENT_ID = \"SemanticCleaning\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n        self.timestamp = datetime.datetime.now()\n\n        # Read date of study\n        self.date_format = self.config.get(self.COMPONENT_ID, \"date_format\")\n        data_period_start = self.config.get(self.COMPONENT_ID, \"data_period_start\")\n        data_period_end = self.config.get(self.COMPONENT_ID, \"data_period_end\")\n        self.date_of_study: datetime.date = None\n\n        try:\n            self.data_period_start = datetime.datetime.strptime(data_period_start, self.date_format).date()\n        except ValueError as e:\n            self.logger.error(str(e), exc_info=True)\n            raise e\n\n        try:\n            self.data_period_end = datetime.datetime.strptime(data_period_end, self.date_format).date()\n        except ValueError as e:\n            self.logger.error(str(e), exc_info=True)\n            raise e\n\n        self.data_period_dates = [\n            self.data_period_start + datetime.timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n        # Unit: metre\n        self.semantic_min_distance = self.config.getfloat(self.COMPONENT_ID, \"semantic_min_distance_m\")\n\n        # Unit: metre / second\n        self.semantic_min_speed = self.config.getfloat(self.COMPONENT_ID, \"semantic_min_speed_m_s\")\n\n    def initalize_data_objects(self):\n        input_silver_event_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"event_data_silver_deduplicated\")\n        input_silver_network_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"network_data_silver\")\n        output_silver_event_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"event_data_silver_flagged\")\n        output_silver_semantic_metrics_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"event_device_semantic_quality_metrics\"\n        )\n\n        input_silver_network = SilverNetworkDataObject(\n            self.spark, input_silver_network_path, partition_columns=[ColNames.year, ColNames.month, ColNames.day]\n        )\n        input_silver_event = SilverEventDataObject(self.spark, input_silver_event_path)\n        output_silver_event = SilverEventFlaggedDataObject(self.spark, output_silver_event_path)\n        output_silver_semantic_metrics = SilverEventSemanticQualityMetrics(\n            self.spark,\n            output_silver_semantic_metrics_path,\n            partition_columns=[ColNames.year, ColNames.month, ColNames.day],\n        )\n\n        self.input_data_objects = {\n            SilverEventDataObject.ID: input_silver_event,\n            SilverNetworkDataObject.ID: input_silver_network,\n        }\n        self.output_data_objects = {\n            SilverEventFlaggedDataObject.ID: output_silver_event,\n            SilverEventSemanticQualityMetrics.ID: output_silver_semantic_metrics,\n        }\n\n    def transform(self):\n        events_df = self.events_df\n        cells_df = self.cells_df\n\n        # Pushup filter\n        events_df = events_df.filter(\n            F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(self.date_of_study)\n        )\n\n        cells_df = (\n            cells_df.filter(\n                F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n                == F.lit(self.date_of_study)\n            )\n            # Point geometry auxiliar column\n            .withColumn(\"geometry\", STC.ST_Point(F.col(ColNames.latitude), F.col(ColNames.latitude))).select(\n                [\n                    F.col(ColNames.cell_id),\n                    F.col(ColNames.latitude).alias(\"cell_lat\"),\n                    F.col(ColNames.longitude).alias(\"cell_lon\"),\n                    F.col(ColNames.valid_date_start),\n                    F.col(ColNames.valid_date_end),\n                    F.col(\"geometry\"),\n                ]\n            )\n        )\n\n        # Perform a left join between events and cell IDs. Non-existent cell IDs will be matched\n        # with null values\n        df = events_df.join(cells_df, on=ColNames.cell_id, how=\"left\")\n\n        # Create error flag column alongside the first error flag: non existent column\n        df = self._flag_non_existent_cell_ids(df)\n\n        # Error flag: Check rows which have valid date start and/or valid date end, and flag when timestamp is incompatible\n        df = self._flag_invalid_cell_ids(df)\n\n        # Error flag: suspicious and incorrect events based on location change distance and speed\n        df = self._flag_by_event_location(df)\n\n        # Keep only the necessary columns and remove auxiliar ones\n        df = df.select(SilverEventFlaggedDataObject.SCHEMA.names)\n\n        df.cache()\n\n        # Semantic metrics\n        metrics_df = self._compute_semantic_metrics(df)\n\n        self.output_data_objects[SilverEventFlaggedDataObject.ID].df = df\n        self.output_data_objects[SilverEventSemanticQualityMetrics.ID].df = metrics_df\n\n    def _flag_non_existent_cell_ids(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Method that creates a new integer column with the name of ColNames.error_flag, and\n        sets the corresponding flags to events that refer to non-existent cell IDs. The rest of\n        the column's values are left as null.\n\n        Args:\n            df (DataFrame): PySpark DataFrame resulting from the left join of events and cells\n\n        Returns:\n            DataFrame: same DataFrame as the input with a new error flag column, containing\n                flags for the events with a non existent cell ID\n        \"\"\"\n        df = df.withColumn(\n            ColNames.error_flag, F.when(F.col(\"geometry\").isNull(), F.lit(SemanticErrorType.CELL_ID_NON_EXISTENT))\n        )\n        return df\n\n    def _flag_invalid_cell_ids(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Method that finds and flags events which refer to an existent cell ID, but that happened outside the\n        time interval during which the cell was operationals. This flag cannot occur at the same time as a\n        non-existent cell. The rest of values (i.e., those with no previous flags) in the error flag column are left\n        as null.\n        The auxiliar Point geometry column will be set to null for these flagged events.\n\n        Args:\n            df (DataFrame): DataFrame in which invalid cells will be flagged\n\n        Returns:\n            DataFrame: DataFrame with flagged invalid cells\n        \"\"\"\n        df = df.withColumn(\n            ColNames.error_flag,\n            # Leave already flagged rows as is\n            F.when(F.col(ColNames.error_flag).isNotNull(), F.col(ColNames.error_flag)).otherwise(\n                # event happened before the cell was operational, or after the cell was operational\n                F.when(\n                    (\n                        (\n                            F.col(ColNames.valid_date_start).isNotNull()\n                            &amp; (F.col(ColNames.timestamp) &lt; F.col(ColNames.valid_date_start))\n                        )\n                        | (\n                            F.col(ColNames.valid_date_end).isNotNull()\n                            &amp; (F.col(ColNames.timestamp) &gt; F.col(ColNames.valid_date_end))\n                        )\n                    ),\n                    F.lit(SemanticErrorType.CELL_ID_NOT_VALID),\n                )\n            ),\n        )\n\n        df = df.withColumn(\n            \"geometry\", F.when(F.col(ColNames.error_flag).isNotNull(), F.lit(None)).otherwise(F.col(\"geometry\"))\n        )\n        return df\n\n    def _flag_by_event_location(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Method that finds and flags events that are considered to be suspicious or incorrect in\n        terms of their timestamp and cell location with respect to their previous and/or following\n        events.\n        It is assumed that these are the last flags to be raised. Thus, non-flagged events are also\n        set to the no-error-flag value within this method.\n        Args:\n            df (DataFrame): DataFrame in which suspicious and/or incorrect events based on\n                location are to be found and flagged\n\n        Returns:\n            DataFrame: flagged DataFrame with suspicious and/or incorrect events\n        \"\"\"\n        # Windows that comprise all previous (following) rows ordered by time for each user.\n        # Partition pruning\n        # These windows have to be used, as all records have to be kept, and we skip them\n        forward_window = (\n            Window.partitionBy([ColNames.year, ColNames.month, ColNames.day, ColNames.user_id_modulo, ColNames.user_id])\n            .orderBy(ColNames.timestamp)\n            .rowsBetween(Window.currentRow + 1, Window.unboundedFollowing)\n        )\n        backward_window = (\n            Window.partitionBy([ColNames.year, ColNames.month, ColNames.day, ColNames.user_id_modulo, ColNames.user_id])\n            .orderBy(ColNames.timestamp)\n            .rowsBetween(Window.unboundedPreceding, Window.currentRow - 1)\n        )\n\n        # Columns to be evaluated for flags\n        # The order of the definition of these columns appears to affect the physical plan\n        # TODO: find best ordering\n        df = (\n            df\n            # auxiliar column containing timestamps of non-flagged events, and null for flagged events\n            .withColumn(\n                \"filtered_ts\", F.when(F.col(ColNames.error_flag).isNull(), F.col(ColNames.timestamp)).otherwise(None)\n            )\n            .withColumn(\n                \"next_timediff\",  # time b/w curr event and first following non-flagged event timestamp\n                F.when(\n                    F.col(ColNames.error_flag).isNull(),\n                    (\n                        F.first(F.col(\"filtered_ts\"), ignorenulls=True).over(forward_window).cast(LongType())\n                        - F.col(ColNames.timestamp).cast(LongType())\n                    ),\n                ).otherwise(F.lit(None)),\n            )\n            .withColumn(\n                \"prev_timediff\",  # time b/w curr event and last previous non-flagged event timestamp\n                F.when(\n                    F.col(ColNames.error_flag).isNull(),\n                    (\n                        F.col(ColNames.timestamp).cast(LongType())\n                        - F.last(F.col(\"filtered_ts\"), ignorenulls=True).over(backward_window).cast(LongType())\n                    ),\n                ).otherwise(F.lit(None)),\n            )\n            .withColumn(\n                \"next_distance\",  # distance b/w curr location and first following non-flagged event location\n                F.when(\n                    F.col(ColNames.error_flag).isNull(),  # rows not flagged (yet)\n                    STF.ST_DistanceSpheroid(\n                        F.col(\"geometry\"), F.first(F.col(\"geometry\"), ignorenulls=True).over(forward_window)\n                    ),\n                ).otherwise(F.lit(None)),\n            )\n            .withColumn(\n                \"prev_distance\",  # distance b/w curr location and last previous non-flagged event location\n                F.when(\n                    F.col(ColNames.error_flag).isNull(),  # rows not flagged (yet)\n                    STF.ST_DistanceSpheroid(\n                        F.col(\"geometry\"), F.last(F.col(\"geometry\"), ignorenulls=True).over(backward_window)\n                    ),\n                ).otherwise(F.lit(None)),\n            )\n            .withColumn(  # mean speed in m/s for euclidean displacemente b/w curr and next non-flagged events\n                \"next_speed\", F.col(\"next_distance\") / F.col(\"next_timediff\")\n            )\n            .withColumn(  # mean speed in m/s for euclidean displacemente b/w curr and previous non-flagged events\n                \"prev_speed\", F.col(\"prev_distance\") / F.col(\"prev_timediff\")\n            )\n        )\n\n        # Conditions that must occur for the two location related error flags\n        incorrect_location_cond = (\n            (F.col(\"prev_speed\") &gt; self.semantic_min_speed)\n            &amp; (F.col(\"prev_distance\") &gt; self.semantic_min_distance)\n            &amp; (F.col(\"next_speed\") &gt; self.semantic_min_speed)\n            &amp; (F.col(\"next_distance\") &gt; self.semantic_min_distance)\n        )\n\n        suspicious_location_cond = F.coalesce(\n            (F.col(\"prev_speed\") &gt; self.semantic_min_speed) &amp; (F.col(\"prev_distance\") &gt; self.semantic_min_distance),\n            F.lit(False),\n        ) | F.coalesce(\n            (F.col(\"next_speed\") &gt; self.semantic_min_speed) &amp; (F.col(\"next_distance\") &gt; self.semantic_min_distance),\n            F.lit(False),\n        )\n        # Set the error flags\n        # NOTE: it is assumed that this is the last flag to be computed. Thus, all non-flagged events\n        # will be set to the code corresponding to no error flags. If new flags are to be added, one might\n        # want to change this.\n        df = df.withColumn(\n            ColNames.error_flag,\n            F.when(\n                F.col(ColNames.error_flag).isNull(),  # for non-flagged events\n                F.when(incorrect_location_cond, F.lit(SemanticErrorType.INCORRECT_EVENT_LOCATION)).otherwise(\n                    F.when(suspicious_location_cond, F.lit(SemanticErrorType.SUSPICIOUS_EVENT_LOCATION)).otherwise(\n                        F.lit(SemanticErrorType.NO_ERROR)\n                    )\n                ),\n            ).otherwise(F.col(ColNames.error_flag)),\n        )\n\n        return df\n\n    def _compute_semantic_metrics(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Method that computes the semantic quality metrics of the semantic checks.\n        This amounts to counting the number of flagged events after the semantic checks.\n\n        Args:\n            df (DataFrame): Flagged event DataFrame\n\n        Returns:\n            DataFrame: semantic metrics DataFrame\n        \"\"\"\n        metrics_df = (\n            df.groupby(ColNames.error_flag)\n            .agg(F.count(F.col(ColNames.error_flag)).alias(ColNames.value))\n            .withColumnRenamed(ColNames.error_flag, ColNames.type_of_error)\n            .withColumns(\n                {\n                    ColNames.variable: F.lit(ColNames.cell_id),  # currently, only cell_id here\n                    ColNames.year: F.lit(self.date_of_study.year).cast(ShortType()),\n                    ColNames.month: F.lit(self.date_of_study.month).cast(ByteType()),\n                    ColNames.day: F.lit(self.date_of_study.day).cast(ByteType()),\n                }\n            )\n        )\n\n        all_error_codes = [error_name for error_name in dir(SemanticErrorType) if not error_name.startswith(\"__\")]\n        all_error_codes = [\n            Row(\n                **{\n                    ColNames.type_of_error: getattr(SemanticErrorType, error_name),\n                }\n            )\n            for error_name in all_error_codes\n        ]\n\n        all_errors_df = self.spark.createDataFrame(\n            all_error_codes, schema=StructType([SilverEventSemanticQualityMetrics.SCHEMA[ColNames.type_of_error]])\n        )\n\n        metrics_df = (\n            metrics_df.join(all_errors_df, on=ColNames.type_of_error, how=\"right\")\n            .fillna(\n                {\n                    ColNames.variable: ColNames.cell_id,\n                    ColNames.value: 0,\n                    ColNames.year: self.date_of_study.year,\n                    ColNames.month: self.date_of_study.month,\n                    ColNames.day: self.date_of_study.day,\n                }\n            )\n            .withColumn(ColNames.result_timestamp, F.lit(self.timestamp))\n        )\n\n        return metrics_df\n\n    def execute(self):\n        \"\"\"\n        Method that performs the read, transform and write methods of the component.\n        \"\"\"\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n\n        for date in self.data_period_dates:\n            self.date_of_study = date\n\n            self.events_df = self.input_data_objects[SilverEventDataObject.ID].df.filter(\n                F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n                == F.lit(self.date_of_study)\n            )\n            self.cells_df = self.input_data_objects[SilverNetworkDataObject.ID].df.filter(\n                F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n                == F.lit(self.date_of_study)\n            )\n\n            self.logger.info(f\"Processing data for {date}\")\n            self.transform()\n            self.write()\n            self.logger.info(f\"Finished {date}\")\n\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n</code></pre>"},{"location":"reference/components/execution/event_semantic_cleaning/event_semantic_cleaning/#components.execution.event_semantic_cleaning.event_semantic_cleaning.SemanticCleaning.execute","title":"<code>execute()</code>","text":"<p>Method that performs the read, transform and write methods of the component.</p> Source code in <code>multimno/components/execution/event_semantic_cleaning/event_semantic_cleaning.py</code> <pre><code>def execute(self):\n    \"\"\"\n    Method that performs the read, transform and write methods of the component.\n    \"\"\"\n    self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n    self.read()\n\n    for date in self.data_period_dates:\n        self.date_of_study = date\n\n        self.events_df = self.input_data_objects[SilverEventDataObject.ID].df.filter(\n            F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n            == F.lit(self.date_of_study)\n        )\n        self.cells_df = self.input_data_objects[SilverNetworkDataObject.ID].df.filter(\n            F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n            == F.lit(self.date_of_study)\n        )\n\n        self.logger.info(f\"Processing data for {date}\")\n        self.transform()\n        self.write()\n        self.logger.info(f\"Finished {date}\")\n\n    self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n</code></pre>"},{"location":"reference/components/execution/network_cleaning/","title":"network_cleaning","text":""},{"location":"reference/components/execution/network_cleaning/network_cleaning/","title":"network_cleaning","text":"<p>Module that cleans raw MNO Network Topology data.</p>"},{"location":"reference/components/execution/network_cleaning/network_cleaning/#components.execution.network_cleaning.network_cleaning.NetworkCleaning","title":"<code>NetworkCleaning</code>","text":"<p>             Bases: <code>Component</code></p> <p>Class that cleans MNO Network Topology Data (based on physical properties of the cell)</p> Source code in <code>multimno/components/execution/network_cleaning/network_cleaning.py</code> <pre><code>class NetworkCleaning(Component):\n    \"\"\"\n    Class that cleans MNO Network Topology Data (based on physical properties of the cell)\n    \"\"\"\n\n    COMPONENT_ID = \"NetworkCleaning\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n        self.latitude_max = self.config.getfloat(self.COMPONENT_ID, \"latitude_max\")\n        self.latitude_min = self.config.getfloat(self.COMPONENT_ID, \"latitude_min\")\n        self.longitude_max = self.config.getfloat(self.COMPONENT_ID, \"longitude_max\")\n        self.longitude_min = self.config.getfloat(self.COMPONENT_ID, \"longitude_min\")\n        self.tech = [\"5G\", \"LTE\", \"UMTS\", \"GSM\"]  # hardcoded\n\n        # list of possible cell types\n        self.cell_type_options = (\n            self.config.get(self.COMPONENT_ID, \"cell_type_options\").strip().replace(\" \", \"\").split(\",\")\n        )\n        self.data_period_format = self.config.get(self.COMPONENT_ID, \"data_period_format\")\n\n        self.data_period_start = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), self.data_period_format\n        ).date()\n        self.data_period_end = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), self.data_period_format\n        ).date()\n\n        # List of datetime.dates for which to perform cleaning of the raw network input data.\n        # Notice that date_period_end is included\n        self.data_period_dates = [\n            self.data_period_start + datetime.timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n        # Timestamp format that the function pyspark.sql.functions.to_timestamp expects.\n        # Format must follow guidelines in https://spark.apache.org/docs/3.4.2/sql-ref-datetime-pattern.html\n        self.valid_date_timestamp_format = self.config.get(self.COMPONENT_ID, \"valid_date_timestamp_format\")\n\n    def initalize_data_objects(self):\n        input_bronze_network_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"network_data_bronze\")\n        output_silver_network_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"network_data_silver\")\n        output_silver_network_syntactic_quality_metrics_by_column = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"network_syntactic_quality_metrics_by_column\"\n        )\n\n        bronze_network = BronzeNetworkDataObject(\n            self.spark, input_bronze_network_path, partition_columns=[ColNames.year, ColNames.month, ColNames.day]\n        )\n        silver_network = SilverNetworkDataObject(\n            self.spark, output_silver_network_path, partition_columns=[ColNames.year, ColNames.month, ColNames.day]\n        )\n\n        silver_network_quality_metrics_by_column = SilverNetworkDataQualityMetricsByColumn(\n            self.spark,\n            output_silver_network_syntactic_quality_metrics_by_column,\n            partition_columns=[ColNames.year, ColNames.month, ColNames.day],\n        )\n\n        self.input_data_objects = {bronze_network.ID: bronze_network}\n        self.output_data_objects = {\n            silver_network.ID: silver_network,\n            silver_network_quality_metrics_by_column.ID: silver_network_quality_metrics_by_column,\n        }\n\n    def transform(self):\n        # Raw/Bronze Network Topology DF\n        cells_df = self.input_data_objects[BronzeNetworkDataObject.ID].df\n\n        # Read only desired dates, specified via config\n        cells_df = cells_df.filter(F.make_date(\"year\", \"month\", \"day\").isin(self.data_period_dates))\n\n        # List that will contain all the columns created to keep track of every kind of error\n        auxiliar_columns = []\n\n        # Columns for which we will check for null values\n        # Notice that currently, valid_date_end can have null values by definition as long as for the current date the tower is\n        # still operational. Thus, it is not taken into account for the deletion of rows/records\n        check_for_null_columns = [\n            ColNames.cell_id,\n            ColNames.valid_date_start,\n            ColNames.valid_date_end,  # should not be counted for discarding rows!!\n            ColNames.latitude,\n            ColNames.longitude,\n            ColNames.altitude,\n            ColNames.antenna_height,\n            ColNames.directionality,\n            ColNames.elevation_angle,\n            ColNames.horizontal_beam_width,\n            ColNames.vertical_beam_width,\n            ColNames.power,\n            ColNames.frequency,\n            ColNames.technology,\n            ColNames.cell_type,\n        ]\n\n        # Add auxiliar columns to track instances where a row has a null value\n        # Note that currently valid_date_end has a permited null value, as well as\n        # azimith_angle when directionality is 0\n        cells_df = cells_df.withColumns(\n            {f\"{col}_{NetworkErrorType.NULL_VALUE}\": F.col(col).isNull() for col in check_for_null_columns}\n        ).withColumn(\n            f\"{ColNames.azimuth_angle}_{NetworkErrorType.NULL_VALUE}\",\n            F.when(F.col(ColNames.directionality) == F.lit(1), F.col(ColNames.azimuth_angle).isNull()).otherwise(\n                F.lit(False)\n            ),\n        )\n\n        auxiliar_columns.extend([f\"{col}_{NetworkErrorType.NULL_VALUE}\" for col in check_for_null_columns])\n        auxiliar_columns.append(f\"{ColNames.azimuth_angle}_{NetworkErrorType.NULL_VALUE}\")\n\n        # Now, we try to parse the valid_date_start and valid_date_end columns, from a string to a timestamp\n        cells_df = (\n            cells_df.withColumn(\n                f\"{ColNames.valid_date_start}_parsed\",\n                F.to_timestamp(F.col(ColNames.valid_date_start), self.valid_date_timestamp_format),\n            )\n            .withColumn(\n                f\"{ColNames.valid_date_end}_parsed\",\n                F.to_timestamp(F.col(ColNames.valid_date_end), self.valid_date_timestamp_format),\n            )\n            # Check when parsing failed, excluding the cases where the field was null to begi with\n            .withColumn(\n                f\"{ColNames.valid_date_start}_{NetworkErrorType.CANNOT_PARSE}\",\n                F.when(\n                    F.col(ColNames.valid_date_start).isNotNull()\n                    &amp; F.col(f\"{ColNames.valid_date_start}_parsed\").isNull(),\n                    F.lit(True),\n                ).otherwise(F.lit(False)),\n            )\n            .withColumn(\n                f\"{ColNames.valid_date_end}_{NetworkErrorType.CANNOT_PARSE}\",\n                F.when(\n                    F.col(ColNames.valid_date_end).isNotNull() &amp; F.col(f\"{ColNames.valid_date_end}_parsed\").isNull(),\n                    F.lit(True),\n                ).otherwise(F.lit(False)),\n            )\n        )\n\n        auxiliar_columns.extend(\n            [\n                f\"{ColNames.valid_date_start}_{NetworkErrorType.CANNOT_PARSE}\",\n                f\"{ColNames.valid_date_end}_{NetworkErrorType.CANNOT_PARSE}\",\n            ]\n        )\n\n        # Now, we check for incoherent dates (valid_date_end is earlier in time than valid_date_start)\n        cells_df = cells_df.withColumn(\n            f\"dates_{NetworkErrorType.OUT_OF_RANGE}\",\n            F.when(\n                F.col(f\"{ColNames.valid_date_start}_parsed\").isNotNull()\n                &amp; F.col(f\"{ColNames.valid_date_end}_parsed\").isNotNull(),\n                F.col(f\"{ColNames.valid_date_start}_parsed\") &gt; F.col(f\"{ColNames.valid_date_end}_parsed\"),\n            ).otherwise(F.lit(False)),\n        )\n\n        auxiliar_columns.append(f\"dates_{NetworkErrorType.OUT_OF_RANGE}\")\n\n        # Now we check for invalid values that are outside of the range defined for the data object.\n\n        # TODO: correct check for CGI in cell ids\n        cells_df = cells_df.withColumn(\n            f\"{ColNames.cell_id}_{NetworkErrorType.OUT_OF_RANGE}\",\n            (F.length(F.col(ColNames.cell_id)) != F.lit(14)) &amp; (F.length(F.col(ColNames.cell_id)) != F.lit(15)),\n        )\n\n        # TODO: cover case where bounding box crosses the -180/180 longitude\n        cells_df = (\n            cells_df.withColumn(\n                f\"{ColNames.latitude}_{NetworkErrorType.OUT_OF_RANGE}\",\n                (F.col(ColNames.latitude) &gt; F.lit(self.latitude_max))\n                | (F.lit(ColNames.latitude) &lt; F.lit(self.latitude_min)),\n            )\n            .withColumn(\n                f\"{ColNames.longitude}_{NetworkErrorType.OUT_OF_RANGE}\",\n                (F.col(ColNames.longitude) &gt; F.lit(self.longitude_max))\n                | (F.lit(ColNames.longitude) &lt; F.lit(self.longitude_min)),\n            )\n            # altitude: must only be float, no checks\n            # antenna height: must be positive\n            .withColumn(\n                f\"{ColNames.antenna_height}_{NetworkErrorType.OUT_OF_RANGE}\",\n                F.col(ColNames.antenna_height) &lt;= F.lit(0),\n            )\n            # directionality: 0 or 1\n            .withColumn(\n                f\"{ColNames.directionality}_{NetworkErrorType.OUT_OF_RANGE}\",\n                (F.col(ColNames.directionality) != F.lit(0)) &amp; (F.col(ColNames.directionality) != F.lit(1)),\n            )\n            .withColumn(\n                f\"{ColNames.azimuth_angle}_{NetworkErrorType.OUT_OF_RANGE}\",\n                F.when(\n                    (F.col(ColNames.directionality) == F.lit(1)),  # &amp; F.col(ColNames.azimuth_angle).isNotNull(),\n                    (F.col(ColNames.azimuth_angle) &lt; F.lit(0)) | (F.col(ColNames.azimuth_angle) &gt; F.lit(360)),\n                ).otherwise(F.lit(False)),\n            )\n            .withColumn(\n                f\"{ColNames.elevation_angle}_{NetworkErrorType.OUT_OF_RANGE}\",\n                (F.col(ColNames.elevation_angle) &lt; F.lit(-90)) | (F.col(ColNames.elevation_angle) &gt; F.lit(90)),\n            )\n            .withColumn(\n                f\"{ColNames.horizontal_beam_width}_{NetworkErrorType.OUT_OF_RANGE}\",\n                (F.col(ColNames.horizontal_beam_width) &lt; F.lit(0))\n                | (F.col(ColNames.horizontal_beam_width) &gt; F.lit(360)),\n            )\n            .withColumn(\n                f\"{ColNames.vertical_beam_width}_{NetworkErrorType.OUT_OF_RANGE}\",\n                (F.col(ColNames.vertical_beam_width) &lt; F.lit(0)) | (F.col(ColNames.vertical_beam_width) &gt; F.lit(360)),\n            )\n            .withColumn(\n                f\"{ColNames.power}_{NetworkErrorType.OUT_OF_RANGE}\",\n                F.col(ColNames.power) &lt; F.lit(0),\n            )\n            .withColumn(\n                f\"{ColNames.frequency}_{NetworkErrorType.OUT_OF_RANGE}\",\n                F.col(ColNames.frequency) &lt; F.lit(0),\n            )\n            .withColumn(\n                f\"{ColNames.technology}_{NetworkErrorType.OUT_OF_RANGE}\",\n                ~F.col(ColNames.technology).isin(self.tech),\n            )\n            .withColumn(\n                f\"{ColNames.cell_type}_{NetworkErrorType.OUT_OF_RANGE}\",\n                ~F.col(ColNames.cell_type).isin(self.cell_type_options),\n            )\n        )\n\n        # Null values will appear for the above checks when the raw data was null. Thus, for these columns\n        # we change null for False by using the .fillna() method\n        cells_df = cells_df.fillna(\n            False,\n            [\n                f\"{ColNames.cell_id}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.latitude}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.longitude}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.antenna_height}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.directionality}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.azimuth_angle}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.elevation_angle}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.horizontal_beam_width}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.vertical_beam_width}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.power}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.frequency}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.technology}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.cell_type}_{NetworkErrorType.OUT_OF_RANGE}\",\n            ],\n        )\n\n        auxiliar_columns.extend(\n            [\n                f\"{ColNames.cell_id}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.latitude}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.longitude}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.antenna_height}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.directionality}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.azimuth_angle}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.elevation_angle}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.horizontal_beam_width}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.vertical_beam_width}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.power}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.frequency}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.technology}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.cell_type}_{NetworkErrorType.OUT_OF_RANGE}\",\n            ]\n        )\n\n        # Auxiliar dict, relating the DO columns with its auxiliar columns\n        column_groups = dict()\n        for col in self.output_data_objects[\"SilverNetworkDO\"].SCHEMA.names:\n            # for each auxiliar column\n            column_groups[col] = []\n            for cc in auxiliar_columns:\n                # if it is related to the DO's column:\n                if col == \"_\".join(cc.split(\"_\")[:-1]):\n                    # Ignore nulls for valid date end\n                    if cc == f\"{ColNames.valid_date_end}_{NetworkErrorType.NULL_VALUE}\":\n                        continue\n                    column_groups[col].append(F.col(cc))\n\n        # For each column, create an abstract conditional whenever a value of that column has ANY type of error.\n        # Example: latitude can have two types of error: a) being null, or b) being out of range.\n        # The coniditonal for this column is then&gt; (isNull(latitude) OR isOutOfRange(latitude))\n        column_conditions = {\n            col: reduce(lambda a, b: a | b, column_groups[col]) for col in column_groups if len(column_groups[col]) &gt; 0\n        }\n\n        # Negate the conditionals above to get those records without ANY type of error\n        field_without_errors = {col: ~column_conditions[col] for col in column_conditions}\n\n        auxiliar_columns.extend([f\"{col}_{NetworkErrorType.NO_ERROR}\" for col in field_without_errors])\n\n        # Abstract conditional ,indicating those records that do not have any type of error in any column, i.e. all accepted\n        # values.\n        preserve_row = reduce(lambda a, b: a &amp; b, field_without_errors.values())\n\n        cells_df = cells_df.withColumns(\n            {f\"{col}_{NetworkErrorType.NO_ERROR}\": field_without_errors[col] for col in field_without_errors}\n        )\n\n        cells_df = cells_df.withColumn(\"to_preserve\", preserve_row)\n\n        cells_df.cache()\n\n        # Collect the number of True values in each auxiliar column, that counts the number of each error type, or\n        # any error, in each of the columns of the data object.\n        # TODO: possible improvement if pyspark.sql.GroupedData.pivot can be used instead.\n        metrics = (\n            cells_df.withColumns({col: F.col(col).cast(IntegerType()) for col in auxiliar_columns})\n            .groupBy([F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)])\n            .agg({col: \"sum\" for col in auxiliar_columns})\n            .withColumnsRenamed({f\"sum({col})\": col for col in auxiliar_columns})\n            .collect()\n        )\n\n        # Extract the collected values and reformat them into the shape of the metrics DO dataframe.\n        metrics_long_format = []\n\n        for row in metrics:\n            row = row.asDict()\n            year = row[ColNames.year]\n            month = row[ColNames.month]\n            day = row[ColNames.day]\n            date = datetime.date(year=year, month=month, day=day)\n            row_dict = dict()\n\n            for col in row:\n                if col in [ColNames.year, ColNames.month, ColNames.day]:\n                    continue\n\n                row_dict = {\n                    ColNames.field_name: \"_\".join(col.split(\"_\")[:-1]),\n                    ColNames.type_code: int(col.split(\"_\")[-1]),\n                    ColNames.value: row[col],\n                    ColNames.date: date,\n                    ColNames.year: year,\n                    ColNames.month: month,\n                    ColNames.day: day,\n                }\n\n                metrics_long_format.append(Row(**row_dict))\n\n        # Initial records (before cleaning)\n        initial_records = (\n            cells_df.groupBy([F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)]).count().collect()\n        )\n\n        for row in initial_records:\n            row = row.asDict()\n            year = row[ColNames.year]\n            month = row[ColNames.month]\n            day = row[ColNames.day]\n            date = datetime.date(year=year, month=month, day=day)\n\n            row_dict = {\n                ColNames.field_name: None,\n                ColNames.type_code: NetworkErrorType.INITIAL_ROWS,\n                ColNames.value: row[\"count\"],\n                ColNames.date: date,\n                ColNames.year: year,\n                ColNames.month: month,\n                ColNames.day: day,\n            }\n\n            metrics_long_format.append(Row(**row_dict))\n\n        # Final records (after cleaning)\n        final_records = (\n            cells_df.withColumn(\"to_preserve\", F.col(\"to_preserve\").cast(IntegerType()))\n            .groupBy([F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)])\n            .sum(\"to_preserve\")\n            .withColumnRenamed(\"sum(to_preserve)\", \"count\")\n            .collect()\n        )\n        for row in final_records:\n            row = row.asDict()\n            year = row[ColNames.year]\n            month = row[ColNames.month]\n            day = row[ColNames.day]\n            date = datetime.date(year=year, month=month, day=day)\n\n            row_dict = {\n                ColNames.field_name: None,\n                ColNames.type_code: NetworkErrorType.FINAL_ROWS,\n                ColNames.value: row[\"count\"],\n                ColNames.date: date,\n                ColNames.year: year,\n                ColNames.month: month,\n                ColNames.day: day,\n            }\n\n            metrics_long_format.append(Row(**row_dict))\n\n        # Final result\n        metrics_df = self.spark.createDataFrame(\n            metrics_long_format,\n            schema=StructType(\n                [\n                    StructField(ColNames.field_name, StringType(), nullable=True),\n                    StructField(ColNames.type_code, IntegerType(), nullable=False),\n                    StructField(ColNames.value, IntegerType(), nullable=False),\n                    StructField(ColNames.date, DateType(), nullable=False),\n                    StructField(ColNames.year, ShortType(), nullable=False),\n                    StructField(ColNames.month, ByteType(), nullable=False),\n                    StructField(ColNames.day, ByteType(), nullable=False),\n                ]\n            ),\n        )\n        metrics_df = metrics_df.withColumn(ColNames.result_timestamp, F.current_timestamp())\n\n        self.output_data_objects[SilverNetworkDataQualityMetricsByColumn.ID].df = metrics_df\n\n        silver_cells_df = (\n            cells_df.filter(F.col(\"to_preserve\"))\n            .withColumn(ColNames.valid_date_start, F.col(f\"{ColNames.valid_date_start}_parsed\"))\n            .withColumn(ColNames.valid_date_end, F.col(f\"{ColNames.valid_date_end}_parsed\"))\n            .select(SilverNetworkDataObject.SCHEMA.names)\n        )\n\n        self.output_data_objects[SilverNetworkDataObject.ID].df = silver_cells_df\n</code></pre>"},{"location":"reference/components/execution/signal_strength/","title":"signal_strength","text":""},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/","title":"signal_stength_modeling","text":"<p>Module that cleans RAW MNO Event data.</p>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling","title":"<code>SignalStrengthModeling</code>","text":"<p>             Bases: <code>Component</code></p> <p>This class is responsible for modeling the signal strength of a cellular network.</p> <p>It takes as input a configuration file and a set of data representing the network's cells and their properties. The class then calculates the signal strength at various points of a grid, taking into account factors such as the distance to the cell, the azimuth and elevation angles, and the directionality of the cell.</p> <p>The class provides methods for adjusting the signal strength based on the horizontal and vertical angles, imputing default cell properties.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>class SignalStrengthModeling(Component):\n    \"\"\"\n    This class is responsible for modeling the signal strength of a cellular network.\n\n    It takes as input a configuration file and a set of data representing the network's cells and their properties.\n    The class then calculates the signal strength at various points of a grid, taking into account factors such\n    as the distance to the cell, the azimuth and elevation angles, and the directionality of the cell.\n\n    The class provides methods for adjusting the signal strength based on the horizontal and vertical angles,\n    imputing default cell properties.\n    \"\"\"\n\n    COMPONENT_ID = \"SignalStrengthModeling\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.data_period_start = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n        self.data_period_end = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n        self.use_elevation = self.config.getboolean(SignalStrengthModeling.COMPONENT_ID, \"use_elevation\")\n        self.do_azimuth_angle_adjustments = self.config.getboolean(\n            SignalStrengthModeling.COMPONENT_ID, \"do_azimuth_angle_adjustments\"\n        )\n        self.do_elevation_angle_adjustments = self.config.getboolean(\n            SignalStrengthModeling.COMPONENT_ID, \"do_elevation_angle_adjustments\"\n        )\n        self.default_cell_properties = self.config.geteval(\n            SignalStrengthModeling.COMPONENT_ID, \"default_cell_physical_properties\"\n        )\n\n        self.data_period_dates = [\n            self.data_period_start + datetime.timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n    def initalize_data_objects(self):\n\n        # Input\n        self.input_data_objects = {}\n        self.cartesian_crs = self.config.get(SignalStrengthModeling.COMPONENT_ID, \"cartesian_crs\")\n\n        grid_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"grid_data_silver\")\n        if check_if_data_path_exists(self.spark, grid_path):\n            self.input_data_objects[SilverGridDataObject.ID] = SilverGridDataObject(\n                self.spark, grid_path, default_crs=self.cartesian_crs\n            )\n        else:\n            self.logger.warning(f\"Expected path {grid_path} to exist but it does not\")\n            raise ValueError(f\"Invalid path for {grid_path} {SignalStrengthModeling.COMPONENT_ID}\")\n\n        network_topology_cells_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"network_data_silver\")\n        if check_if_data_path_exists(self.spark, network_topology_cells_path):\n            self.input_data_objects[SilverNetworkDataObject.ID] = SilverNetworkDataObject(\n                self.spark, network_topology_cells_path\n            )\n        else:\n            self.logger.warning(f\"Expected path {network_topology_cells_path} to exist but it does not\")\n            raise ValueError(f\"Invalid path for {network_topology_cells_path} {SignalStrengthModeling.COMPONENT_ID}\")\n\n        # Output\n        self.output_data_objects = {}\n        self.silver_signal_strength_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"signal_strength_data_silver\")\n        self.output_data_objects[SilverSignalStrengthDataObject.ID] = SilverSignalStrengthDataObject(\n            self.spark,\n            self.silver_signal_strength_path,\n            partition_columns=[ColNames.year, ColNames.month, ColNames.day],\n        )\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n\n        grid_sdf = self.input_data_objects[SilverGridDataObject.ID].df\n        grid_sdf = self.add_z_to_point_geometry(grid_sdf, ColNames.geometry, self.use_elevation)\n\n        # TODO: We might need to iterate over dates and process the data for each date separately\n        # have to do tests with larger datasets.\n\n        # TODO: Potentially\n        current_cells_sdf = self.input_data_objects[SilverNetworkDataObject.ID].df.filter(\n            F.make_date(\"year\", \"month\", \"day\").isin(self.data_period_dates)\n        )\n\n        current_cells_sdf = self.impute_default_cell_properties(current_cells_sdf)\n\n        current_cells_sdf = self.watt_to_dbm(current_cells_sdf)\n\n        # TODO: Add Path Loss Exponent calculation based on grid landuse data\n\n        # Create geometries\n        current_cells_sdf = self.create_cell_point_geometry(current_cells_sdf, self.use_elevation)\n        current_cells_sdf = self.project_to_crs(current_cells_sdf, 4326, self.cartesian_crs)\n\n        # Spatial join\n        current_cell_grid_sdf = self.spatial_join_within_distance(current_cells_sdf, grid_sdf, ColNames.range)\n\n        # Calculate planar and 3D distances\n        current_cell_grid_sdf = self.calculate_cartesian_distances(current_cell_grid_sdf)\n\n        # Calculate initial signal strength without azimuth and tilt angles adjustments\n        current_cell_grid_sdf = self.calculate_distance_power_loss(current_cell_grid_sdf)\n\n        # next step is to calculate azimuth and tilt angles adjustments, if set in the configuration file\n        # otherwise all cells are assumed to be omnideirectional and there is no need for these adjustments\n\n        # first need to find all standard deviations in signal strength distributions for azimuth and elevation\n        # angles for each signal strength back loss. This is done only once for each unique combination of\n        # signal strength back loss\n        # and beam width in cells dataset\n\n        if self.do_azimuth_angle_adjustments:\n\n            # get standard deviation mapping table for azimuth beam width azimuth back loss pairs\n            sd_azimuth_mapping_sdf = self.get_angular_adjustments_sd_mapping(\n                current_cells_sdf,\n                ColNames.azimuth_angle,\n                ColNames.azimuth_signal_strength_back_loss,\n                \"azimuth\",\n            )\n\n            current_cell_grid_directional = current_cell_grid_sdf.where(F.col(ColNames.directionality) == 1)\n\n            current_cell_grid_directional = self.join_sd_mapping(\n                current_cell_grid_directional,\n                sd_azimuth_mapping_sdf,\n                ColNames.azimuth_angle,\n                ColNames.azimuth_signal_strength_back_loss,\n            )\n\n            current_cell_grid_directional = self.calculate_horizontal_angle_power_adjustment(\n                current_cell_grid_directional\n            )\n\n            current_cell_grid_sdf = current_cell_grid_directional.unionByName(\n                current_cell_grid_sdf.where(F.col(ColNames.directionality) == 0),\n                allowMissingColumns=True,\n            )\n\n        if self.do_elevation_angle_adjustments:\n            # get standard deviation mapping table for elevation beam width elevation back loss pairs\n            sd_elevation_mapping_sdf = self.get_angular_adjustments_sd_mapping(\n                current_cells_sdf,\n                ColNames.elevation_angle,\n                ColNames.elevation_signal_strength_back_loss,\n                \"elevation\",\n            )\n\n            current_cell_grid_directional = current_cell_grid_sdf.where(F.col(ColNames.directionality) == 1)\n\n            current_cell_grid_directional = self.join_sd_mapping(\n                current_cell_grid_directional,\n                sd_elevation_mapping_sdf,\n                ColNames.elevation_angle,\n                ColNames.elevation_signal_strength_back_loss,\n            )\n\n            current_cell_grid_directional = self.calculate_vertical_angle_power_adjustment(\n                current_cell_grid_directional\n            )\n\n            current_cell_grid_sdf = current_cell_grid_directional.unionByName(\n                current_cell_grid_sdf.where(F.col(ColNames.directionality) == 0),\n                allowMissingColumns=True,\n            )\n\n        # amend start and end valid dates\n        current_cell_grid_sdf = current_cell_grid_sdf.withColumn(\n            \"valid_date_start\", F.make_date(F.col(\"year\"), F.col(\"month\"), F.col(\"day\"))\n        )\n        current_cell_grid_sdf = current_cell_grid_sdf.withColumn(\n            \"valid_date_end\", F.date_add(F.col(\"valid_date_start\"), 1)\n        )\n\n        current_cell_grid_sdf = current_cell_grid_sdf.select(\n            SilverSignalStrengthDataObject.MANDATORY_COLUMNS + SilverSignalStrengthDataObject.OPTIONAL_COLUMNS\n        )\n        columns = {\n            field.name: F.col(field.name).cast(field.dataType) for field in SilverSignalStrengthDataObject.SCHEMA.fields\n        }\n        current_cell_grid_sdf = current_cell_grid_sdf.withColumns(columns)\n\n        self.output_data_objects[SilverSignalStrengthDataObject.ID].df = current_cell_grid_sdf\n\n    def impute_default_cell_properties(self, sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Imputes default cell properties for null values in the input DataFrame using\n        default properties for cell types from config.\n\n        Args:\n            sdf (DataFrame): Input DataFrame.\n\n        Returns:\n            DataFrame: DataFrame with imputed default cell properties.\n        \"\"\"\n        default_properties_df = self.create_default_properties_df()\n\n        # add default prefix to the columns of default_properties_df\n        default_properties_df = default_properties_df.select(\n            [F.col(col).alias(f\"default_{col}\") for col in default_properties_df.columns]\n        )\n\n        # assign default cell type to cell types not present in config\n        sdf = sdf.withColumn(\n            ColNames.cell_type,\n            F.when(~F.col(ColNames.cell_type).isin(list(self.default_cell_properties.keys())), \"default\").otherwise(\n                F.col(ColNames.cell_type)\n            ),\n        )\n\n        # all cell types which are absent from the default_properties_df will be assigned default values\n        sdf = sdf.join(\n            default_properties_df,\n            sdf[ColNames.cell_type] == default_properties_df[f\"default_{ColNames.cell_type}\"],\n            how=\"inner\",\n        )\n        # if orignal column is null, assign the default value\n        for col in default_properties_df.columns:\n            col = col.replace(\"default_\", \"\")\n            if col not in sdf.columns:\n                sdf = sdf.withColumn(col, F.lit(None))\n            sdf = sdf.withColumn(col, F.coalesce(F.col(col), F.col(f\"default_{col}\")))\n\n        return sdf\n\n    def create_default_properties_df(self) -&gt; DataFrame:\n        \"\"\"\n        Creates a DataFrame with default cell properties from config dict.\n\n        Returns:\n            DataFrame: A DataFrame with default cell properties.\n        \"\"\"\n\n        rows = [Row(cell_type=k, **v) for k, v in self.default_cell_properties.items()]\n        return self.spark.createDataFrame(rows).withColumnRenamed(\"cell_type\", ColNames.cell_type)\n\n    # create geometry for cells. Set Z values if elevation is taken into account from z column, otherwise to 0\n    @staticmethod\n    def create_cell_point_geometry(sdf: DataFrame, use_elevation: bool) -&gt; DataFrame:\n        \"\"\"\n        Creates cell point geometry.\n        If elevation is taken into account, set Z values from z column, otherwise to 0.\n\n        Args:\n            sdf (DataFrame): Input DataFrame.\n            use_elevation (bool): Whether to use elevation.\n\n        Returns:\n            DataFrame: DataFrame with cell point geometry.\n        \"\"\"\n        if use_elevation:\n            sdf = sdf.withColumn(\n                ColNames.geometry,\n                STC.ST_MakePoint(\n                    F.col(ColNames.longitude),\n                    F.col(ColNames.latitude),\n                    F.col(ColNames.altitude) + F.col(ColNames.antenna_height),\n                ),\n            )\n        else:\n            sdf = sdf.withColumn(\n                ColNames.geometry,\n                STC.ST_MakePoint(\n                    F.col(ColNames.longitude),\n                    F.col(ColNames.latitude),\n                    F.col(ColNames.antenna_height),\n                ),\n            )\n        # assign crs\n        sdf = sdf.withColumn(ColNames.geometry, STF.ST_SetSRID(F.col(ColNames.geometry), F.lit(4326)))\n\n        return sdf\n\n    # add z value to the grid geometry if elevation is taken into account from z column in the grid otherwise set to 0\n    @staticmethod\n    def add_z_to_point_geometry(sdf: DataFrame, geometry_col: str, use_elevation: bool) -&gt; DataFrame:\n        \"\"\"\n        Adds z value to the point geometry (grid centroids).\n        If elevation is taken into account, set Z values from z column, otherwise to 0.\n\n        Args:\n            sdf (DataFrame): Input DataFrame.\n            use_elevation (bool): Whether to use elevation.\n\n        Returns:\n            DataFrame: DataFrame with z value added to point geometry.\n        \"\"\"\n        if use_elevation:\n            sdf = sdf.withColumn(\n                geometry_col,\n                STC.ST_MakePoint(\n                    STF.ST_X(F.col(geometry_col)),\n                    STF.ST_Y(F.col(geometry_col)),\n                    F.col(ColNames.elevation),\n                ),\n            )\n        else:\n            sdf = sdf.withColumn(\n                geometry_col,\n                STC.ST_MakePoint(\n                    STF.ST_X(F.col(geometry_col)),\n                    STF.ST_Y(F.col(geometry_col)),\n                    F.lit(0.0),\n                ),\n            )\n        return sdf\n\n    # project geometry to cartesian crs\n    @staticmethod\n    def project_to_crs(sdf: DataFrame, crs_in: int, crs_out: int) -&gt; DataFrame:\n        \"\"\"\n        Projects geometry to cartesian CRS.\n\n        Args:\n            sdf (DataFrame): Input DataFrame.\n            crs_in (int): Input CRS.\n            crs_out (int): Output CRS.\n\n        Returns:\n            DataFrame: DataFrame with geometry projected to cartesian CRS.\n        \"\"\"\n        crs_in = f\"EPSG:{crs_in}\"\n        crs_out = f\"EPSG:{crs_out}\"\n\n        sdf = sdf.withColumn(\n            ColNames.geometry,\n            STF.ST_Transform(sdf[ColNames.geometry], F.lit(crs_in), F.lit(crs_out)),\n        )\n        return sdf\n\n    @staticmethod\n    def spatial_join_within_distance(sdf_from: DataFrame, sdf_to: DataFrame, within_distance_col: str) -&gt; DataFrame:\n        \"\"\"\n        Performs a spatial join within a specified distance.\n\n        Args:\n            sdf_from (DataFrame): Input DataFrame.\n            sdf_to (DataFrame): DataFrame to join with.\n            within_distance_col (str): Column name for the within distance.\n\n        Returns:\n            DataFrame: DataFrame after performing the spatial join.\n        \"\"\"\n        sdf_to = sdf_to.withColumnRenamed(ColNames.geometry, ColNames.joined_geometry)\n\n        sdf_merged = sdf_from.join(\n            sdf_to,\n            STP.ST_Intersects(\n                STF.ST_Buffer(sdf_from[ColNames.geometry], sdf_from[within_distance_col]),\n                sdf_to[ColNames.joined_geometry],\n            ),\n        )\n\n        return sdf_merged\n\n    @staticmethod\n    def calculate_cartesian_distances(sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Calculates cartesian distances.\n\n        Args:\n            sdf (DataFrame): Input DataFrame.\n\n        Returns:\n            DataFrame: DataFrame with calculated cartesian distances.\n        \"\"\"\n\n        sdf = sdf.withColumns(\n            {\n                ColNames.distance_to_cell_3D: STF.ST_3DDistance(\n                    F.col(ColNames.geometry), F.col(ColNames.joined_geometry)\n                ),\n                ColNames.distance_to_cell: STF.ST_Distance(F.col(ColNames.geometry), F.col(ColNames.joined_geometry)),\n            }\n        )\n\n        return sdf\n\n    @staticmethod\n    def watt_to_dbm(sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Converts power from watt to dBm.\n\n        Args:\n            sdf (DataFrame): Input DataFrame.\n\n        Returns:\n            DataFrame: DataFrame with power converted to dBm.\n        \"\"\"\n        return sdf.withColumn(ColNames.power, 10.0 * F.log10(F.col(ColNames.power)) + 30.0)\n\n    @staticmethod\n    def calculate_distance_power_loss(sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Calculates distance power loss caluclated as\n        power - path_loss_exponent * 10 * log10(distance_to_cell_3D).\n\n        Args:\n            sdf (DataFrame): Input DataFrame.\n\n        Returns:\n            DataFrame: DataFrame with calculated distance power loss.\n        \"\"\"\n        return sdf.withColumn(\n            ColNames.signal_strength,\n            F.col(ColNames.power)\n            - F.col(ColNames.path_loss_exponent) * 10 * F.log10(F.col(ColNames.distance_to_cell_3D)),\n        )\n\n    @staticmethod\n    def join_sd_mapping(\n        sdf: DataFrame,\n        sd_mapping_sdf: DataFrame,\n        beam_width_col: str,\n        signal_front_back_difference_col: str,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Joins DataFrame with standard deviation mapping.\n\n        Args:\n            sdf (DataFrame): Input DataFrame.\n            sd_mapping_sdf (DataFrame): DataFrame with standard deviation mapping.\n            beam_width_col (str): Column name for the beam width.\n            signal_front_back_difference_col (str): Column name for the signal front-back difference.\n\n        Returns:\n            DataFrame: DataFrame after joining with standard deviation mapping.\n        \"\"\"\n\n        join_condition = (sdf[beam_width_col] == sd_mapping_sdf[beam_width_col]) &amp; (\n            sdf[signal_front_back_difference_col] == sd_mapping_sdf[signal_front_back_difference_col]\n        )\n\n        sdf = sdf.join(sd_mapping_sdf, join_condition).drop(\n            sd_mapping_sdf[beam_width_col],\n            sd_mapping_sdf[signal_front_back_difference_col],\n        )\n\n        return sdf\n\n    @staticmethod\n    def get_sd_to_signal_back_loss_mappings(cells_sdf: DataFrame, signal_front_back_difference_col: str) -&gt; DataFrame:\n        \"\"\"\n        Generates a DataFrame with mapping of signal strength standard deviation for each\n            elevation/azimuth angle degree.\n\n        Parameters:\n        cells_sdf (DataFrame): A Spark DataFrame containing information about the cells.\n        signal_front_back_difference_col (str): The name of the column that contains the difference\n            in signal strength between\n        the front and back of the cell.\n\n        Returns:\n        DataFrame: A pandas DataFrame with standard deviation mappings.\n\n        \"\"\"\n        db_back_diffs = (\n            cells_sdf.select(F.col(signal_front_back_difference_col))\n            .where(F.col(ColNames.directionality) == 1)\n            .distinct()\n        )\n        db_back_diffs = [row.asDict()[signal_front_back_difference_col] for row in db_back_diffs.collect()]\n        mappings = [\n            SignalStrengthModeling.create_mapping(item, signal_front_back_difference_col) for item in db_back_diffs\n        ]\n\n        return pd.concat(mappings)\n\n    def get_angular_adjustments_sd_mapping(\n        self,\n        cells_sdf: DataFrame,\n        beam_width_col: str,\n        signal_front_back_difference_col: str,\n        angular_adjustment_type: str,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Calculates standard deviations in signal strength based on beam width and front-back cell signal difference.\n\n        Args:\n            cells_sdf (DataFrame): Input DataFrame.\n            beam_width_col (str): Column name for the beam width.\n            signal_front_back_difference_col (str): Column name for the signal front-back difference.\n            angular_adjustment_type (str): Type of angular adjustment.\n\n        Returns:\n            DataFrame: DataFrame with angular adjustments standard deviation mapping.\n        \"\"\"\n\n        sd_mappings = SignalStrengthModeling.get_sd_to_signal_back_loss_mappings(\n            cells_sdf, signal_front_back_difference_col\n        )\n        beam_widths_diff = (\n            cells_sdf.select(F.col(beam_width_col), F.col(signal_front_back_difference_col))\n            .where(F.col(ColNames.directionality) == 1)\n            .distinct()\n        )\n        beam_widths_diff = [row.asDict() for row in beam_widths_diff.collect()]\n\n        beam_sds = []\n        for item in beam_widths_diff:\n            item[f\"sd_{angular_adjustment_type}\"] = self.find_sd(\n                item[beam_width_col],\n                sd_mappings[sd_mappings[signal_front_back_difference_col] == item[signal_front_back_difference_col]],\n            )\n            beam_sds.append(item)\n\n        beam_sd_sdf = self.spark.createDataFrame(beam_sds)\n\n        return beam_sd_sdf\n\n    @staticmethod\n    def normalize_angle(a: float) -&gt; float:\n        \"\"\"\n        Adjusts the given angle to fall within the range of -180 to 180 degrees.\n\n        Args:\n            a (float): The angle in degrees to be normalized.\n\n        Returns:\n            float: The input angle adjusted to fall within the range of -180 to 180 degrees.\n        \"\"\"\n        return ((a + 180) % 360) - 180\n\n    @staticmethod\n    def normal_distribution(x: float, mean: float, sd: float, return_type: str) -&gt; Union[np.array, list]:\n        \"\"\"\n        Computes the value of the normal distribution with the given mean\n        and standard deviation at the given point.\n\n        Args:\n            x (float): The point at which to evaluate the normal distribution.\n            mean (float): The mean of the normal distribution.\n            sd (float): The standard deviation of the normal distribution.\n            return_type (str): The desired return type, either 'np_array' or 'list'.\n\n        Returns:\n            np.array or list: The value of the normal distribution at the given point,\n            returned as either a numpy array or a list.\n        \"\"\"\n        n_dist = (1.0 / (sqrt(2.0 * pi) * sd)) * np.exp(-0.5 * ((x - mean) / sd) ** 2)\n\n        if return_type == \"np_array\":\n            return n_dist\n\n        elif return_type == \"list\":\n            return n_dist.tolist()\n\n    @staticmethod\n    def norm_dBloss(a: float, sd: float, db_back: float) -&gt; float:\n        \"\"\"\n        Computes the loss in signal strength in dB as a function of\n        angle from the direction of maximum signal strength.\n\n        Args:\n            a (float): The angle from the direction of maximum signal strength.\n            sd (float): The standard deviation of the normal distribution modeling the signal strength.\n            db_back (float): The difference in signal strength in dB between the front and back of the signal.\n\n        Returns:\n            float: The loss in signal strength in dB at the given angle.\n        \"\"\"\n        a = SignalStrengthModeling.normalize_angle(a)\n        inflate = -db_back / (\n            SignalStrengthModeling.normal_distribution(0, 0, sd, \"np_array\")\n            - SignalStrengthModeling.normal_distribution(180, 0, sd, \"np_array\")\n        )\n        return (\n            SignalStrengthModeling.normal_distribution(a, 0, sd, \"np_array\")\n            - SignalStrengthModeling.normal_distribution(0, 0, sd, \"np_array\")\n        ) * inflate\n\n    @staticmethod\n    @F.udf(FloatType())\n    def norm_dBloss_udf(a, sd, db_back):\n        a = SignalStrengthModeling.normalize_angle(a)  # You need to define this function\n        inflate = -db_back / (\n            SignalStrengthModeling.normal_distribution(0, 0, sd, \"list\")\n            - SignalStrengthModeling.normal_distribution(180, 0, sd, \"list\")\n        )\n        return (\n            SignalStrengthModeling.normal_distribution(a, 0, sd, \"list\")\n            - SignalStrengthModeling.normal_distribution(0, 0, sd, \"list\")\n        ) * inflate\n\n    @staticmethod\n    def get_min3db(sd: float, db_back: float) -&gt; float:\n        \"\"\"\n        Finds the angle at which the signal strength falls to 3 dB below its maximum value.\n\n        Args:\n            sd (float): The standard deviation of the normal distribution modeling the signal strength.\n            db_back (float): The difference in signal strength in dB between the front and back of the signal.\n\n        Returns:\n            float: The angle at which the signal strength falls to 3 dB below its maximum value.\n        \"\"\"\n        df = pd.DataFrame({\"a\": np.linspace(0, 180, 720)})\n        df[\"dbLoss\"] = SignalStrengthModeling.norm_dBloss(df[\"a\"], db_back=db_back, sd=sd)\n        return df.loc[np.abs(-3 - df[\"dbLoss\"]).idxmin(), \"a\"]\n\n    @staticmethod\n    def create_mapping(db_back: float, signal_front_back_difference_col) -&gt; DataFrame:\n        \"\"\"\n        Creates a mapping between standard deviation and the angle\n        at which the signal strength falls to 3 dB below its maximum value.\n\n        Args:\n            db_back (float): The difference in signal strength in dB between the front and back of the signal.\n\n        Returns:\n            DataFrame: A DataFrame where each row corresponds to a\n            standard deviation and contains the corresponding angle.\n        \"\"\"\n        idf = pd.DataFrame({\"sd\": np.arange(180 / 1000, 180, 180 / 1000)})\n        idf[\"deg\"] = idf[\"sd\"].apply(SignalStrengthModeling.get_min3db, db_back=db_back)\n        df = pd.DataFrame({\"deg\": np.arange(1, 181)})\n        df[\"sd\"] = df[\"deg\"].apply(lambda dg: idf.loc[np.abs(idf[\"deg\"] - dg).idxmin(), \"sd\"])\n        df[signal_front_back_difference_col] = db_back\n        return df\n\n    @staticmethod\n    def find_sd(beam_width: float, mapping: DataFrame) -&gt; float:\n        \"\"\"\n        Finds the standard deviation corresponding to the given beam width using the provided mapping.\n\n        Args:\n            beam_width (float): The width of the beam in degrees.\n            mapping (DataFrame): A DataFrame where each row corresponds to a standard deviation\n                and contains the corresponding angle.\n\n        Returns:\n            float: The standard deviation corresponding to the given beam width.\n        \"\"\"\n        min_diff_index = (abs(mapping[\"deg\"] - beam_width / 2)).idxmin()\n        return float(mapping.loc[min_diff_index, \"sd\"])\n\n    @staticmethod\n    def calculate_horizontal_angle_power_adjustment(sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Adjusts the signal strength of each cell in the input DataFrame based on the horizontal angle.\n\n        This function calculates the azimuth angle between each cell and a reference point,\n        projects the data to the elevation plane, and adjusts the signal strength based on the\n        relative azimuth angle and the distance to the cell. The adjustment is calculated using\n        a normal distribution model of signal strength.\n\n        Based on https://github.com/MobilePhoneESSnetBigData/mobloc/blob/master/R/signal_strength.R\n\n        Args:\n            sdf (DataFrame): A Spark DataFrame\n\n        Returns:\n            DataFrame: The input DataFrame with the 'signal_strength' column adjusted and intermediate columns dropped.\n        \"\"\"\n\n        # TODO: simplify math in this function by using Sedona built in spatial methods\n        sdf = sdf.withColumn(\n            \"theta_azim\",\n            (\n                90\n                - F.degrees(\n                    (\n                        F.atan2(\n                            STF.ST_Y(F.col(ColNames.joined_geometry)) - STF.ST_Y(F.col(ColNames.geometry)),\n                            STF.ST_X(F.col(ColNames.joined_geometry)) - STF.ST_X(F.col(ColNames.geometry)),\n                        )\n                    )\n                )\n            ),\n        )\n        sdf = sdf.withColumn(\n            \"theta_azim\",\n            F.when(F.col(\"theta_azim\") &lt; 0, F.col(\"theta_azim\") + 360).otherwise(F.col(\"theta_azim\")),\n        )\n        sdf = sdf.withColumn(\"azim\", (F.col(\"theta_azim\") - F.col(ColNames.azimuth_angle)) % 360)\n        sdf = sdf.withColumn(\n            \"azim\",\n            F.when(F.col(\"azim\") &gt; 180, F.col(\"azim\") - 360).otherwise(\n                F.when(F.col(\"azim\") &lt; -180, F.col(\"azim\") + 360).otherwise(F.col(\"azim\"))\n            ),\n        )\n        sdf = sdf.withColumn(\"a\", F.sin(F.radians(F.col(\"azim\"))) * F.col(ColNames.distance_to_cell))\n\n        # project to elevation plane\n        sdf = sdf.withColumn(\"b\", F.cos(F.radians(F.col(\"azim\"))) * F.col(ColNames.distance_to_cell))\n        sdf = sdf.withColumn(\"c\", STF.ST_Z(ColNames.geometry) - STF.ST_Z(ColNames.joined_geometry))\n\n        sdf = sdf.withColumn(\"d\", F.sqrt(F.col(\"b\") ** 2 + F.col(\"c\") ** 2))\n        sdf = sdf.withColumn(\"lambda\", F.degrees(F.atan2(F.col(\"c\"), F.abs(F.col(\"b\")))))\n        sdf = sdf.withColumn(\n            \"cases\",\n            F.when(\n                F.col(\"b\") &gt; 0,\n                F.when(F.col(ColNames.elevation_angle) &lt; F.col(\"lambda\"), F.lit(1)).otherwise(F.lit(2)),\n            ).otherwise(F.when(F.col(\"lambda\") + F.col(ColNames.elevation_angle) &lt; 90, F.lit(3)).otherwise(F.lit(4))),\n        )\n\n        sdf = sdf.withColumn(\n            \"e\",\n            F.when(\n                F.col(\"cases\") == 1,\n                F.cos(F.radians(F.col(\"lambda\") - F.col(ColNames.elevation_angle))) * F.col(\"d\"),\n            )\n            .when(\n                F.col(\"cases\") == 2,\n                F.cos(F.radians(F.col(ColNames.elevation_angle) - F.col(\"lambda\"))) * F.col(\"d\"),\n            )\n            .when(\n                F.col(\"cases\") == 3,\n                -F.cos(F.radians(F.col(\"lambda\") + F.col(ColNames.elevation_angle))) * F.col(\"d\"),\n            )\n            .otherwise(F.cos(F.radians(180 - F.col(\"lambda\") - F.col(ColNames.elevation_angle))) * F.col(\"d\")),\n        )\n\n        sdf = sdf.withColumn(\"azim2\", F.degrees(F.atan2(F.col(\"a\"), F.col(\"e\"))))\n\n        # finally get power adjustments\n        sdf = sdf.withColumn(\n            ColNames.signal_strength,\n            F.col(ColNames.signal_strength)\n            + SignalStrengthModeling.norm_dBloss_udf(\n                F.col(\"azim2\"),\n                F.col(\"sd_azimuth\"),\n                F.col(ColNames.azimuth_signal_strength_back_loss),\n            ),\n        )\n\n        # cleanup\n        sdf = sdf.drop(\n            \"theta_azim\",\n            \"azim\",\n            \"a\",\n            \"b\",\n            \"c\",\n            \"d\",\n            \"_lambda\",\n            \"cases\",\n            \"e\",\n            \"azim2\",\n            \"sd_azimuth\",\n        )\n\n        return sdf\n\n    @staticmethod\n    def calculate_vertical_angle_power_adjustment(sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Adjusts the signal strength of each cell in the input DataFrame based on the vertical angle.\n\n        This function calculates the elevation angle between each cell and a reference point,\n        and adjusts the signal strength based on the relative elevation angle and the distance to the cell.\n        The adjustment is calculated using a normal distribution model of signal strength.\n\n        Based on https://github.com/MobilePhoneESSnetBigData/mobloc/blob/master/R/signal_strength.R\n\n        Args:\n            sdf (DataFrame): A Spark DataFrame\n\n        Returns:\n            DataFrame: The input DataFrame with the 'signal_strength' column adjusted and intermediate columns dropped.\n        \"\"\"\n\n        sdf = sdf.withColumn(\n            \"gamma_elev\",\n            F.degrees(\n                F.atan2(\n                    STF.ST_Z(ColNames.geometry) - STF.ST_Z(ColNames.joined_geometry),\n                    F.col(ColNames.distance_to_cell),\n                )\n            ),\n        )\n        sdf = sdf.withColumn(\"elev\", (F.col(\"gamma_elev\") - F.col(ColNames.elevation_angle)) % 360)\n\n        sdf = sdf.withColumn(\n            \"elev\",\n            F.when(F.col(\"elev\") &gt; 180, F.col(\"elev\") - 360).otherwise(\n                F.when(F.col(\"elev\") &lt; -180, F.col(\"elev\") + 360).otherwise(F.col(\"elev\"))\n            ),\n        )\n\n        # get power adjustments\n        sdf = sdf.withColumn(\n            ColNames.signal_strength,\n            F.col(ColNames.signal_strength)\n            + SignalStrengthModeling.norm_dBloss_udf(\n                F.col(\"elev\"),\n                F.col(\"sd_elevation\"),\n                F.col(ColNames.elevation_signal_strength_back_loss),\n            ),\n        )\n\n        # cleanup\n        sdf = sdf.drop(\"gamma_elev\", \"elev\", \"sd_elevation\")\n\n        return sdf\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.add_z_to_point_geometry","title":"<code>add_z_to_point_geometry(sdf, geometry_col, use_elevation)</code>  <code>staticmethod</code>","text":"<p>Adds z value to the point geometry (grid centroids). If elevation is taken into account, set Z values from z column, otherwise to 0.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>use_elevation</code> <code>bool</code> <p>Whether to use elevation.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with z value added to point geometry.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>@staticmethod\ndef add_z_to_point_geometry(sdf: DataFrame, geometry_col: str, use_elevation: bool) -&gt; DataFrame:\n    \"\"\"\n    Adds z value to the point geometry (grid centroids).\n    If elevation is taken into account, set Z values from z column, otherwise to 0.\n\n    Args:\n        sdf (DataFrame): Input DataFrame.\n        use_elevation (bool): Whether to use elevation.\n\n    Returns:\n        DataFrame: DataFrame with z value added to point geometry.\n    \"\"\"\n    if use_elevation:\n        sdf = sdf.withColumn(\n            geometry_col,\n            STC.ST_MakePoint(\n                STF.ST_X(F.col(geometry_col)),\n                STF.ST_Y(F.col(geometry_col)),\n                F.col(ColNames.elevation),\n            ),\n        )\n    else:\n        sdf = sdf.withColumn(\n            geometry_col,\n            STC.ST_MakePoint(\n                STF.ST_X(F.col(geometry_col)),\n                STF.ST_Y(F.col(geometry_col)),\n                F.lit(0.0),\n            ),\n        )\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.calculate_cartesian_distances","title":"<code>calculate_cartesian_distances(sdf)</code>  <code>staticmethod</code>","text":"<p>Calculates cartesian distances.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with calculated cartesian distances.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>@staticmethod\ndef calculate_cartesian_distances(sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Calculates cartesian distances.\n\n    Args:\n        sdf (DataFrame): Input DataFrame.\n\n    Returns:\n        DataFrame: DataFrame with calculated cartesian distances.\n    \"\"\"\n\n    sdf = sdf.withColumns(\n        {\n            ColNames.distance_to_cell_3D: STF.ST_3DDistance(\n                F.col(ColNames.geometry), F.col(ColNames.joined_geometry)\n            ),\n            ColNames.distance_to_cell: STF.ST_Distance(F.col(ColNames.geometry), F.col(ColNames.joined_geometry)),\n        }\n    )\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.calculate_distance_power_loss","title":"<code>calculate_distance_power_loss(sdf)</code>  <code>staticmethod</code>","text":"<p>Calculates distance power loss caluclated as power - path_loss_exponent * 10 * log10(distance_to_cell_3D).</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with calculated distance power loss.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>@staticmethod\ndef calculate_distance_power_loss(sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Calculates distance power loss caluclated as\n    power - path_loss_exponent * 10 * log10(distance_to_cell_3D).\n\n    Args:\n        sdf (DataFrame): Input DataFrame.\n\n    Returns:\n        DataFrame: DataFrame with calculated distance power loss.\n    \"\"\"\n    return sdf.withColumn(\n        ColNames.signal_strength,\n        F.col(ColNames.power)\n        - F.col(ColNames.path_loss_exponent) * 10 * F.log10(F.col(ColNames.distance_to_cell_3D)),\n    )\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.calculate_horizontal_angle_power_adjustment","title":"<code>calculate_horizontal_angle_power_adjustment(sdf)</code>  <code>staticmethod</code>","text":"<p>Adjusts the signal strength of each cell in the input DataFrame based on the horizontal angle.</p> <p>This function calculates the azimuth angle between each cell and a reference point, projects the data to the elevation plane, and adjusts the signal strength based on the relative azimuth angle and the distance to the cell. The adjustment is calculated using a normal distribution model of signal strength.</p> <p>Based on https://github.com/MobilePhoneESSnetBigData/mobloc/blob/master/R/signal_strength.R</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>A Spark DataFrame</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The input DataFrame with the 'signal_strength' column adjusted and intermediate columns dropped.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>@staticmethod\ndef calculate_horizontal_angle_power_adjustment(sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Adjusts the signal strength of each cell in the input DataFrame based on the horizontal angle.\n\n    This function calculates the azimuth angle between each cell and a reference point,\n    projects the data to the elevation plane, and adjusts the signal strength based on the\n    relative azimuth angle and the distance to the cell. The adjustment is calculated using\n    a normal distribution model of signal strength.\n\n    Based on https://github.com/MobilePhoneESSnetBigData/mobloc/blob/master/R/signal_strength.R\n\n    Args:\n        sdf (DataFrame): A Spark DataFrame\n\n    Returns:\n        DataFrame: The input DataFrame with the 'signal_strength' column adjusted and intermediate columns dropped.\n    \"\"\"\n\n    # TODO: simplify math in this function by using Sedona built in spatial methods\n    sdf = sdf.withColumn(\n        \"theta_azim\",\n        (\n            90\n            - F.degrees(\n                (\n                    F.atan2(\n                        STF.ST_Y(F.col(ColNames.joined_geometry)) - STF.ST_Y(F.col(ColNames.geometry)),\n                        STF.ST_X(F.col(ColNames.joined_geometry)) - STF.ST_X(F.col(ColNames.geometry)),\n                    )\n                )\n            )\n        ),\n    )\n    sdf = sdf.withColumn(\n        \"theta_azim\",\n        F.when(F.col(\"theta_azim\") &lt; 0, F.col(\"theta_azim\") + 360).otherwise(F.col(\"theta_azim\")),\n    )\n    sdf = sdf.withColumn(\"azim\", (F.col(\"theta_azim\") - F.col(ColNames.azimuth_angle)) % 360)\n    sdf = sdf.withColumn(\n        \"azim\",\n        F.when(F.col(\"azim\") &gt; 180, F.col(\"azim\") - 360).otherwise(\n            F.when(F.col(\"azim\") &lt; -180, F.col(\"azim\") + 360).otherwise(F.col(\"azim\"))\n        ),\n    )\n    sdf = sdf.withColumn(\"a\", F.sin(F.radians(F.col(\"azim\"))) * F.col(ColNames.distance_to_cell))\n\n    # project to elevation plane\n    sdf = sdf.withColumn(\"b\", F.cos(F.radians(F.col(\"azim\"))) * F.col(ColNames.distance_to_cell))\n    sdf = sdf.withColumn(\"c\", STF.ST_Z(ColNames.geometry) - STF.ST_Z(ColNames.joined_geometry))\n\n    sdf = sdf.withColumn(\"d\", F.sqrt(F.col(\"b\") ** 2 + F.col(\"c\") ** 2))\n    sdf = sdf.withColumn(\"lambda\", F.degrees(F.atan2(F.col(\"c\"), F.abs(F.col(\"b\")))))\n    sdf = sdf.withColumn(\n        \"cases\",\n        F.when(\n            F.col(\"b\") &gt; 0,\n            F.when(F.col(ColNames.elevation_angle) &lt; F.col(\"lambda\"), F.lit(1)).otherwise(F.lit(2)),\n        ).otherwise(F.when(F.col(\"lambda\") + F.col(ColNames.elevation_angle) &lt; 90, F.lit(3)).otherwise(F.lit(4))),\n    )\n\n    sdf = sdf.withColumn(\n        \"e\",\n        F.when(\n            F.col(\"cases\") == 1,\n            F.cos(F.radians(F.col(\"lambda\") - F.col(ColNames.elevation_angle))) * F.col(\"d\"),\n        )\n        .when(\n            F.col(\"cases\") == 2,\n            F.cos(F.radians(F.col(ColNames.elevation_angle) - F.col(\"lambda\"))) * F.col(\"d\"),\n        )\n        .when(\n            F.col(\"cases\") == 3,\n            -F.cos(F.radians(F.col(\"lambda\") + F.col(ColNames.elevation_angle))) * F.col(\"d\"),\n        )\n        .otherwise(F.cos(F.radians(180 - F.col(\"lambda\") - F.col(ColNames.elevation_angle))) * F.col(\"d\")),\n    )\n\n    sdf = sdf.withColumn(\"azim2\", F.degrees(F.atan2(F.col(\"a\"), F.col(\"e\"))))\n\n    # finally get power adjustments\n    sdf = sdf.withColumn(\n        ColNames.signal_strength,\n        F.col(ColNames.signal_strength)\n        + SignalStrengthModeling.norm_dBloss_udf(\n            F.col(\"azim2\"),\n            F.col(\"sd_azimuth\"),\n            F.col(ColNames.azimuth_signal_strength_back_loss),\n        ),\n    )\n\n    # cleanup\n    sdf = sdf.drop(\n        \"theta_azim\",\n        \"azim\",\n        \"a\",\n        \"b\",\n        \"c\",\n        \"d\",\n        \"_lambda\",\n        \"cases\",\n        \"e\",\n        \"azim2\",\n        \"sd_azimuth\",\n    )\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.calculate_vertical_angle_power_adjustment","title":"<code>calculate_vertical_angle_power_adjustment(sdf)</code>  <code>staticmethod</code>","text":"<p>Adjusts the signal strength of each cell in the input DataFrame based on the vertical angle.</p> <p>This function calculates the elevation angle between each cell and a reference point, and adjusts the signal strength based on the relative elevation angle and the distance to the cell. The adjustment is calculated using a normal distribution model of signal strength.</p> <p>Based on https://github.com/MobilePhoneESSnetBigData/mobloc/blob/master/R/signal_strength.R</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>A Spark DataFrame</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The input DataFrame with the 'signal_strength' column adjusted and intermediate columns dropped.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>@staticmethod\ndef calculate_vertical_angle_power_adjustment(sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Adjusts the signal strength of each cell in the input DataFrame based on the vertical angle.\n\n    This function calculates the elevation angle between each cell and a reference point,\n    and adjusts the signal strength based on the relative elevation angle and the distance to the cell.\n    The adjustment is calculated using a normal distribution model of signal strength.\n\n    Based on https://github.com/MobilePhoneESSnetBigData/mobloc/blob/master/R/signal_strength.R\n\n    Args:\n        sdf (DataFrame): A Spark DataFrame\n\n    Returns:\n        DataFrame: The input DataFrame with the 'signal_strength' column adjusted and intermediate columns dropped.\n    \"\"\"\n\n    sdf = sdf.withColumn(\n        \"gamma_elev\",\n        F.degrees(\n            F.atan2(\n                STF.ST_Z(ColNames.geometry) - STF.ST_Z(ColNames.joined_geometry),\n                F.col(ColNames.distance_to_cell),\n            )\n        ),\n    )\n    sdf = sdf.withColumn(\"elev\", (F.col(\"gamma_elev\") - F.col(ColNames.elevation_angle)) % 360)\n\n    sdf = sdf.withColumn(\n        \"elev\",\n        F.when(F.col(\"elev\") &gt; 180, F.col(\"elev\") - 360).otherwise(\n            F.when(F.col(\"elev\") &lt; -180, F.col(\"elev\") + 360).otherwise(F.col(\"elev\"))\n        ),\n    )\n\n    # get power adjustments\n    sdf = sdf.withColumn(\n        ColNames.signal_strength,\n        F.col(ColNames.signal_strength)\n        + SignalStrengthModeling.norm_dBloss_udf(\n            F.col(\"elev\"),\n            F.col(\"sd_elevation\"),\n            F.col(ColNames.elevation_signal_strength_back_loss),\n        ),\n    )\n\n    # cleanup\n    sdf = sdf.drop(\"gamma_elev\", \"elev\", \"sd_elevation\")\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.create_cell_point_geometry","title":"<code>create_cell_point_geometry(sdf, use_elevation)</code>  <code>staticmethod</code>","text":"<p>Creates cell point geometry. If elevation is taken into account, set Z values from z column, otherwise to 0.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>use_elevation</code> <code>bool</code> <p>Whether to use elevation.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with cell point geometry.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>@staticmethod\ndef create_cell_point_geometry(sdf: DataFrame, use_elevation: bool) -&gt; DataFrame:\n    \"\"\"\n    Creates cell point geometry.\n    If elevation is taken into account, set Z values from z column, otherwise to 0.\n\n    Args:\n        sdf (DataFrame): Input DataFrame.\n        use_elevation (bool): Whether to use elevation.\n\n    Returns:\n        DataFrame: DataFrame with cell point geometry.\n    \"\"\"\n    if use_elevation:\n        sdf = sdf.withColumn(\n            ColNames.geometry,\n            STC.ST_MakePoint(\n                F.col(ColNames.longitude),\n                F.col(ColNames.latitude),\n                F.col(ColNames.altitude) + F.col(ColNames.antenna_height),\n            ),\n        )\n    else:\n        sdf = sdf.withColumn(\n            ColNames.geometry,\n            STC.ST_MakePoint(\n                F.col(ColNames.longitude),\n                F.col(ColNames.latitude),\n                F.col(ColNames.antenna_height),\n            ),\n        )\n    # assign crs\n    sdf = sdf.withColumn(ColNames.geometry, STF.ST_SetSRID(F.col(ColNames.geometry), F.lit(4326)))\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.create_default_properties_df","title":"<code>create_default_properties_df()</code>","text":"<p>Creates a DataFrame with default cell properties from config dict.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame with default cell properties.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>def create_default_properties_df(self) -&gt; DataFrame:\n    \"\"\"\n    Creates a DataFrame with default cell properties from config dict.\n\n    Returns:\n        DataFrame: A DataFrame with default cell properties.\n    \"\"\"\n\n    rows = [Row(cell_type=k, **v) for k, v in self.default_cell_properties.items()]\n    return self.spark.createDataFrame(rows).withColumnRenamed(\"cell_type\", ColNames.cell_type)\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.create_mapping","title":"<code>create_mapping(db_back, signal_front_back_difference_col)</code>  <code>staticmethod</code>","text":"<p>Creates a mapping between standard deviation and the angle at which the signal strength falls to 3 dB below its maximum value.</p> <p>Parameters:</p> Name Type Description Default <code>db_back</code> <code>float</code> <p>The difference in signal strength in dB between the front and back of the signal.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame where each row corresponds to a</p> <code>DataFrame</code> <p>standard deviation and contains the corresponding angle.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>@staticmethod\ndef create_mapping(db_back: float, signal_front_back_difference_col) -&gt; DataFrame:\n    \"\"\"\n    Creates a mapping between standard deviation and the angle\n    at which the signal strength falls to 3 dB below its maximum value.\n\n    Args:\n        db_back (float): The difference in signal strength in dB between the front and back of the signal.\n\n    Returns:\n        DataFrame: A DataFrame where each row corresponds to a\n        standard deviation and contains the corresponding angle.\n    \"\"\"\n    idf = pd.DataFrame({\"sd\": np.arange(180 / 1000, 180, 180 / 1000)})\n    idf[\"deg\"] = idf[\"sd\"].apply(SignalStrengthModeling.get_min3db, db_back=db_back)\n    df = pd.DataFrame({\"deg\": np.arange(1, 181)})\n    df[\"sd\"] = df[\"deg\"].apply(lambda dg: idf.loc[np.abs(idf[\"deg\"] - dg).idxmin(), \"sd\"])\n    df[signal_front_back_difference_col] = db_back\n    return df\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.find_sd","title":"<code>find_sd(beam_width, mapping)</code>  <code>staticmethod</code>","text":"<p>Finds the standard deviation corresponding to the given beam width using the provided mapping.</p> <p>Parameters:</p> Name Type Description Default <code>beam_width</code> <code>float</code> <p>The width of the beam in degrees.</p> required <code>mapping</code> <code>DataFrame</code> <p>A DataFrame where each row corresponds to a standard deviation and contains the corresponding angle.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The standard deviation corresponding to the given beam width.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>@staticmethod\ndef find_sd(beam_width: float, mapping: DataFrame) -&gt; float:\n    \"\"\"\n    Finds the standard deviation corresponding to the given beam width using the provided mapping.\n\n    Args:\n        beam_width (float): The width of the beam in degrees.\n        mapping (DataFrame): A DataFrame where each row corresponds to a standard deviation\n            and contains the corresponding angle.\n\n    Returns:\n        float: The standard deviation corresponding to the given beam width.\n    \"\"\"\n    min_diff_index = (abs(mapping[\"deg\"] - beam_width / 2)).idxmin()\n    return float(mapping.loc[min_diff_index, \"sd\"])\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.get_angular_adjustments_sd_mapping","title":"<code>get_angular_adjustments_sd_mapping(cells_sdf, beam_width_col, signal_front_back_difference_col, angular_adjustment_type)</code>","text":"<p>Calculates standard deviations in signal strength based on beam width and front-back cell signal difference.</p> <p>Parameters:</p> Name Type Description Default <code>cells_sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>beam_width_col</code> <code>str</code> <p>Column name for the beam width.</p> required <code>signal_front_back_difference_col</code> <code>str</code> <p>Column name for the signal front-back difference.</p> required <code>angular_adjustment_type</code> <code>str</code> <p>Type of angular adjustment.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with angular adjustments standard deviation mapping.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>def get_angular_adjustments_sd_mapping(\n    self,\n    cells_sdf: DataFrame,\n    beam_width_col: str,\n    signal_front_back_difference_col: str,\n    angular_adjustment_type: str,\n) -&gt; DataFrame:\n    \"\"\"\n    Calculates standard deviations in signal strength based on beam width and front-back cell signal difference.\n\n    Args:\n        cells_sdf (DataFrame): Input DataFrame.\n        beam_width_col (str): Column name for the beam width.\n        signal_front_back_difference_col (str): Column name for the signal front-back difference.\n        angular_adjustment_type (str): Type of angular adjustment.\n\n    Returns:\n        DataFrame: DataFrame with angular adjustments standard deviation mapping.\n    \"\"\"\n\n    sd_mappings = SignalStrengthModeling.get_sd_to_signal_back_loss_mappings(\n        cells_sdf, signal_front_back_difference_col\n    )\n    beam_widths_diff = (\n        cells_sdf.select(F.col(beam_width_col), F.col(signal_front_back_difference_col))\n        .where(F.col(ColNames.directionality) == 1)\n        .distinct()\n    )\n    beam_widths_diff = [row.asDict() for row in beam_widths_diff.collect()]\n\n    beam_sds = []\n    for item in beam_widths_diff:\n        item[f\"sd_{angular_adjustment_type}\"] = self.find_sd(\n            item[beam_width_col],\n            sd_mappings[sd_mappings[signal_front_back_difference_col] == item[signal_front_back_difference_col]],\n        )\n        beam_sds.append(item)\n\n    beam_sd_sdf = self.spark.createDataFrame(beam_sds)\n\n    return beam_sd_sdf\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.get_min3db","title":"<code>get_min3db(sd, db_back)</code>  <code>staticmethod</code>","text":"<p>Finds the angle at which the signal strength falls to 3 dB below its maximum value.</p> <p>Parameters:</p> Name Type Description Default <code>sd</code> <code>float</code> <p>The standard deviation of the normal distribution modeling the signal strength.</p> required <code>db_back</code> <code>float</code> <p>The difference in signal strength in dB between the front and back of the signal.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The angle at which the signal strength falls to 3 dB below its maximum value.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>@staticmethod\ndef get_min3db(sd: float, db_back: float) -&gt; float:\n    \"\"\"\n    Finds the angle at which the signal strength falls to 3 dB below its maximum value.\n\n    Args:\n        sd (float): The standard deviation of the normal distribution modeling the signal strength.\n        db_back (float): The difference in signal strength in dB between the front and back of the signal.\n\n    Returns:\n        float: The angle at which the signal strength falls to 3 dB below its maximum value.\n    \"\"\"\n    df = pd.DataFrame({\"a\": np.linspace(0, 180, 720)})\n    df[\"dbLoss\"] = SignalStrengthModeling.norm_dBloss(df[\"a\"], db_back=db_back, sd=sd)\n    return df.loc[np.abs(-3 - df[\"dbLoss\"]).idxmin(), \"a\"]\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.get_sd_to_signal_back_loss_mappings","title":"<code>get_sd_to_signal_back_loss_mappings(cells_sdf, signal_front_back_difference_col)</code>  <code>staticmethod</code>","text":"<p>Generates a DataFrame with mapping of signal strength standard deviation for each     elevation/azimuth angle degree.</p> <p>Parameters: cells_sdf (DataFrame): A Spark DataFrame containing information about the cells. signal_front_back_difference_col (str): The name of the column that contains the difference     in signal strength between the front and back of the cell.</p> <p>Returns: DataFrame: A pandas DataFrame with standard deviation mappings.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>@staticmethod\ndef get_sd_to_signal_back_loss_mappings(cells_sdf: DataFrame, signal_front_back_difference_col: str) -&gt; DataFrame:\n    \"\"\"\n    Generates a DataFrame with mapping of signal strength standard deviation for each\n        elevation/azimuth angle degree.\n\n    Parameters:\n    cells_sdf (DataFrame): A Spark DataFrame containing information about the cells.\n    signal_front_back_difference_col (str): The name of the column that contains the difference\n        in signal strength between\n    the front and back of the cell.\n\n    Returns:\n    DataFrame: A pandas DataFrame with standard deviation mappings.\n\n    \"\"\"\n    db_back_diffs = (\n        cells_sdf.select(F.col(signal_front_back_difference_col))\n        .where(F.col(ColNames.directionality) == 1)\n        .distinct()\n    )\n    db_back_diffs = [row.asDict()[signal_front_back_difference_col] for row in db_back_diffs.collect()]\n    mappings = [\n        SignalStrengthModeling.create_mapping(item, signal_front_back_difference_col) for item in db_back_diffs\n    ]\n\n    return pd.concat(mappings)\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.impute_default_cell_properties","title":"<code>impute_default_cell_properties(sdf)</code>","text":"<p>Imputes default cell properties for null values in the input DataFrame using default properties for cell types from config.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with imputed default cell properties.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>def impute_default_cell_properties(self, sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Imputes default cell properties for null values in the input DataFrame using\n    default properties for cell types from config.\n\n    Args:\n        sdf (DataFrame): Input DataFrame.\n\n    Returns:\n        DataFrame: DataFrame with imputed default cell properties.\n    \"\"\"\n    default_properties_df = self.create_default_properties_df()\n\n    # add default prefix to the columns of default_properties_df\n    default_properties_df = default_properties_df.select(\n        [F.col(col).alias(f\"default_{col}\") for col in default_properties_df.columns]\n    )\n\n    # assign default cell type to cell types not present in config\n    sdf = sdf.withColumn(\n        ColNames.cell_type,\n        F.when(~F.col(ColNames.cell_type).isin(list(self.default_cell_properties.keys())), \"default\").otherwise(\n            F.col(ColNames.cell_type)\n        ),\n    )\n\n    # all cell types which are absent from the default_properties_df will be assigned default values\n    sdf = sdf.join(\n        default_properties_df,\n        sdf[ColNames.cell_type] == default_properties_df[f\"default_{ColNames.cell_type}\"],\n        how=\"inner\",\n    )\n    # if orignal column is null, assign the default value\n    for col in default_properties_df.columns:\n        col = col.replace(\"default_\", \"\")\n        if col not in sdf.columns:\n            sdf = sdf.withColumn(col, F.lit(None))\n        sdf = sdf.withColumn(col, F.coalesce(F.col(col), F.col(f\"default_{col}\")))\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.join_sd_mapping","title":"<code>join_sd_mapping(sdf, sd_mapping_sdf, beam_width_col, signal_front_back_difference_col)</code>  <code>staticmethod</code>","text":"<p>Joins DataFrame with standard deviation mapping.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>sd_mapping_sdf</code> <code>DataFrame</code> <p>DataFrame with standard deviation mapping.</p> required <code>beam_width_col</code> <code>str</code> <p>Column name for the beam width.</p> required <code>signal_front_back_difference_col</code> <code>str</code> <p>Column name for the signal front-back difference.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame after joining with standard deviation mapping.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>@staticmethod\ndef join_sd_mapping(\n    sdf: DataFrame,\n    sd_mapping_sdf: DataFrame,\n    beam_width_col: str,\n    signal_front_back_difference_col: str,\n) -&gt; DataFrame:\n    \"\"\"\n    Joins DataFrame with standard deviation mapping.\n\n    Args:\n        sdf (DataFrame): Input DataFrame.\n        sd_mapping_sdf (DataFrame): DataFrame with standard deviation mapping.\n        beam_width_col (str): Column name for the beam width.\n        signal_front_back_difference_col (str): Column name for the signal front-back difference.\n\n    Returns:\n        DataFrame: DataFrame after joining with standard deviation mapping.\n    \"\"\"\n\n    join_condition = (sdf[beam_width_col] == sd_mapping_sdf[beam_width_col]) &amp; (\n        sdf[signal_front_back_difference_col] == sd_mapping_sdf[signal_front_back_difference_col]\n    )\n\n    sdf = sdf.join(sd_mapping_sdf, join_condition).drop(\n        sd_mapping_sdf[beam_width_col],\n        sd_mapping_sdf[signal_front_back_difference_col],\n    )\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.norm_dBloss","title":"<code>norm_dBloss(a, sd, db_back)</code>  <code>staticmethod</code>","text":"<p>Computes the loss in signal strength in dB as a function of angle from the direction of maximum signal strength.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>The angle from the direction of maximum signal strength.</p> required <code>sd</code> <code>float</code> <p>The standard deviation of the normal distribution modeling the signal strength.</p> required <code>db_back</code> <code>float</code> <p>The difference in signal strength in dB between the front and back of the signal.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The loss in signal strength in dB at the given angle.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>@staticmethod\ndef norm_dBloss(a: float, sd: float, db_back: float) -&gt; float:\n    \"\"\"\n    Computes the loss in signal strength in dB as a function of\n    angle from the direction of maximum signal strength.\n\n    Args:\n        a (float): The angle from the direction of maximum signal strength.\n        sd (float): The standard deviation of the normal distribution modeling the signal strength.\n        db_back (float): The difference in signal strength in dB between the front and back of the signal.\n\n    Returns:\n        float: The loss in signal strength in dB at the given angle.\n    \"\"\"\n    a = SignalStrengthModeling.normalize_angle(a)\n    inflate = -db_back / (\n        SignalStrengthModeling.normal_distribution(0, 0, sd, \"np_array\")\n        - SignalStrengthModeling.normal_distribution(180, 0, sd, \"np_array\")\n    )\n    return (\n        SignalStrengthModeling.normal_distribution(a, 0, sd, \"np_array\")\n        - SignalStrengthModeling.normal_distribution(0, 0, sd, \"np_array\")\n    ) * inflate\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.normal_distribution","title":"<code>normal_distribution(x, mean, sd, return_type)</code>  <code>staticmethod</code>","text":"<p>Computes the value of the normal distribution with the given mean and standard deviation at the given point.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float</code> <p>The point at which to evaluate the normal distribution.</p> required <code>mean</code> <code>float</code> <p>The mean of the normal distribution.</p> required <code>sd</code> <code>float</code> <p>The standard deviation of the normal distribution.</p> required <code>return_type</code> <code>str</code> <p>The desired return type, either 'np_array' or 'list'.</p> required <p>Returns:</p> Type Description <code>Union[array, list]</code> <p>np.array or list: The value of the normal distribution at the given point,</p> <code>Union[array, list]</code> <p>returned as either a numpy array or a list.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>@staticmethod\ndef normal_distribution(x: float, mean: float, sd: float, return_type: str) -&gt; Union[np.array, list]:\n    \"\"\"\n    Computes the value of the normal distribution with the given mean\n    and standard deviation at the given point.\n\n    Args:\n        x (float): The point at which to evaluate the normal distribution.\n        mean (float): The mean of the normal distribution.\n        sd (float): The standard deviation of the normal distribution.\n        return_type (str): The desired return type, either 'np_array' or 'list'.\n\n    Returns:\n        np.array or list: The value of the normal distribution at the given point,\n        returned as either a numpy array or a list.\n    \"\"\"\n    n_dist = (1.0 / (sqrt(2.0 * pi) * sd)) * np.exp(-0.5 * ((x - mean) / sd) ** 2)\n\n    if return_type == \"np_array\":\n        return n_dist\n\n    elif return_type == \"list\":\n        return n_dist.tolist()\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.normalize_angle","title":"<code>normalize_angle(a)</code>  <code>staticmethod</code>","text":"<p>Adjusts the given angle to fall within the range of -180 to 180 degrees.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>The angle in degrees to be normalized.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The input angle adjusted to fall within the range of -180 to 180 degrees.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>@staticmethod\ndef normalize_angle(a: float) -&gt; float:\n    \"\"\"\n    Adjusts the given angle to fall within the range of -180 to 180 degrees.\n\n    Args:\n        a (float): The angle in degrees to be normalized.\n\n    Returns:\n        float: The input angle adjusted to fall within the range of -180 to 180 degrees.\n    \"\"\"\n    return ((a + 180) % 360) - 180\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.project_to_crs","title":"<code>project_to_crs(sdf, crs_in, crs_out)</code>  <code>staticmethod</code>","text":"<p>Projects geometry to cartesian CRS.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>crs_in</code> <code>int</code> <p>Input CRS.</p> required <code>crs_out</code> <code>int</code> <p>Output CRS.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with geometry projected to cartesian CRS.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>@staticmethod\ndef project_to_crs(sdf: DataFrame, crs_in: int, crs_out: int) -&gt; DataFrame:\n    \"\"\"\n    Projects geometry to cartesian CRS.\n\n    Args:\n        sdf (DataFrame): Input DataFrame.\n        crs_in (int): Input CRS.\n        crs_out (int): Output CRS.\n\n    Returns:\n        DataFrame: DataFrame with geometry projected to cartesian CRS.\n    \"\"\"\n    crs_in = f\"EPSG:{crs_in}\"\n    crs_out = f\"EPSG:{crs_out}\"\n\n    sdf = sdf.withColumn(\n        ColNames.geometry,\n        STF.ST_Transform(sdf[ColNames.geometry], F.lit(crs_in), F.lit(crs_out)),\n    )\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.spatial_join_within_distance","title":"<code>spatial_join_within_distance(sdf_from, sdf_to, within_distance_col)</code>  <code>staticmethod</code>","text":"<p>Performs a spatial join within a specified distance.</p> <p>Parameters:</p> Name Type Description Default <code>sdf_from</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>sdf_to</code> <code>DataFrame</code> <p>DataFrame to join with.</p> required <code>within_distance_col</code> <code>str</code> <p>Column name for the within distance.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame after performing the spatial join.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>@staticmethod\ndef spatial_join_within_distance(sdf_from: DataFrame, sdf_to: DataFrame, within_distance_col: str) -&gt; DataFrame:\n    \"\"\"\n    Performs a spatial join within a specified distance.\n\n    Args:\n        sdf_from (DataFrame): Input DataFrame.\n        sdf_to (DataFrame): DataFrame to join with.\n        within_distance_col (str): Column name for the within distance.\n\n    Returns:\n        DataFrame: DataFrame after performing the spatial join.\n    \"\"\"\n    sdf_to = sdf_to.withColumnRenamed(ColNames.geometry, ColNames.joined_geometry)\n\n    sdf_merged = sdf_from.join(\n        sdf_to,\n        STP.ST_Intersects(\n            STF.ST_Buffer(sdf_from[ColNames.geometry], sdf_from[within_distance_col]),\n            sdf_to[ColNames.joined_geometry],\n        ),\n    )\n\n    return sdf_merged\n</code></pre>"},{"location":"reference/components/execution/signal_strength/signal_stength_modeling/#components.execution.signal_strength.signal_stength_modeling.SignalStrengthModeling.watt_to_dbm","title":"<code>watt_to_dbm(sdf)</code>  <code>staticmethod</code>","text":"<p>Converts power from watt to dBm.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with power converted to dBm.</p> Source code in <code>multimno/components/execution/signal_strength/signal_stength_modeling.py</code> <pre><code>@staticmethod\ndef watt_to_dbm(sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Converts power from watt to dBm.\n\n    Args:\n        sdf (DataFrame): Input DataFrame.\n\n    Returns:\n        DataFrame: DataFrame with power converted to dBm.\n    \"\"\"\n    return sdf.withColumn(ColNames.power, 10.0 * F.log10(F.col(ColNames.power)) + 30.0)\n</code></pre>"},{"location":"reference/components/execution/time_segments/","title":"time_segments","text":""},{"location":"reference/components/execution/time_segments/continuous_time_segmentation/","title":"continuous_time_segmentation","text":"<p>Module that implements the Continuous Time Segmentations functionality</p>"},{"location":"reference/components/execution/time_segments/continuous_time_segmentation/#components.execution.time_segments.continuous_time_segmentation.ContinuousTimeSegmentation","title":"<code>ContinuousTimeSegmentation</code>","text":"<p>             Bases: <code>Component</code></p> <p>A class to aggregate events into time segments.</p> Source code in <code>multimno/components/execution/time_segments/continuous_time_segmentation.py</code> <pre><code>class ContinuousTimeSegmentation(Component):\n    \"\"\"\n    A class to aggregate events into time segments.\n    \"\"\"\n\n    COMPONENT_ID = \"ContinuousTimeSegmentation\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.data_period_start = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n        self.data_period_end = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n\n        self.min_time_stay = timedelta(seconds=self.config.getint(self.COMPONENT_ID, \"min_time_stay_s\"))\n\n        self.max_time_missing_stay = timedelta(seconds=self.config.getint(self.COMPONENT_ID, \"max_time_missing_stay_s\"))\n\n        self.max_time_missing_move = timedelta(seconds=self.config.getint(self.COMPONENT_ID, \"max_time_missing_move_s\"))\n\n        self.pad_time = timedelta(seconds=self.config.getint(self.COMPONENT_ID, \"pad_time_s\"))\n\n        self.event_error_flags_to_include = self.config.geteval(self.COMPONENT_ID, \"event_error_flags_to_include\")\n        # this is for UDF\n        self.segmentation_return_schema = StructType(\n            [\n                StructField(ColNames.start_timestamp, TimestampType()),\n                StructField(ColNames.end_timestamp, TimestampType()),\n                StructField(ColNames.cells, ArrayType(StringType())),\n                StructField(ColNames.state, StringType()),\n                StructField(ColNames.is_last, BooleanType()),\n                StructField(ColNames.time_segment_id, IntegerType()),\n                StructField(ColNames.user_id, StringType()),\n                StructField(ColNames.mcc, ShortType()),\n                StructField(ColNames.user_id_modulo, IntegerType()),\n            ]\n        )\n\n        self.data_period_dates = [\n            self.data_period_start + timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n    def initalize_data_objects(self):\n\n        # Input\n        self.input_data_objects = {}\n        self.is_first_run = self.config.getboolean(self.COMPONENT_ID, \"is_first_run\")\n\n        inputs = {\n            \"event_data_silver_flagged\": SilverEventFlaggedDataObject,\n            \"cell_intersection_groups_data_silver\": SilverCellIntersectionGroupsDataObject,\n        }\n        if not self.is_first_run:\n            inputs[\"time_segments_silver\"] = SilverTimeSegmentsDataObject\n\n        for key, value in inputs.items():\n            path = self.config.get(CONFIG_SILVER_PATHS_KEY, key)\n            if check_if_data_path_exists(self.spark, path):\n                self.input_data_objects[value.ID] = value(self.spark, path)\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")\n                raise ValueError(f\"Invalid path for {value.ID}: {path}\")\n\n        # Output\n        self.output_data_objects = {}\n        self.silver_signal_strength_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"time_segments_silver\")\n        self.output_data_objects[SilverTimeSegmentsDataObject.ID] = SilverTimeSegmentsDataObject(\n            self.spark,\n            self.silver_signal_strength_path,\n            partition_columns=[ColNames.year, ColNames.month, ColNames.day, ColNames.user_id_modulo],\n        )\n\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n\n        # If segements was already calculated and this is continuation of the previous run\n        # we need to get the last time segment for each user.\n        # If this is the first run, we will create an empty dataframe\n        if self.is_first_run:\n            self.intital_time_segment = self.spark.createDataFrame([], SilverTimeSegmentsDataObject.SCHEMA)\n        else:\n            previous_date = self.data_period_start - timedelta(days=1)\n            self.intital_time_segment = self.input_data_objects[SilverTimeSegmentsDataObject.ID].df.filter(\n                (F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(previous_date))\n                &amp; (F.col(ColNames.is_last) == True)\n            )\n\n            # this is needed to join the last time segement with the events of the current date\n            self.intital_time_segment = self.intital_time_segment.withColumn(\n                ColNames.start_timestamp, F.date_add(F.col(ColNames.start_timestamp), 1)\n            ).withColumns(\n                {\n                    ColNames.year: F.year(ColNames.start_timestamp).cast(\"smallint\"),\n                    ColNames.month: F.month(ColNames.start_timestamp).cast(\"tinyint\"),\n                    ColNames.day: F.dayofmonth(ColNames.start_timestamp).cast(\"tinyint\"),\n                }\n            )\n\n            self.intital_time_segment = self.intital_time_segment.withColumn(\n                ColNames.user_id, F.hex(F.col(ColNames.user_id))\n            )\n\n        # for every date in the data period, get the events and the intersection groups\n        # for that date and calculate the time segments\n        for current_date in self.data_period_dates:\n\n            self.logger.info(f\"Processing events for {current_date.strftime('%Y-%m-%d')}\")\n\n            self.current_date = current_date\n            self.current_input_events_sdf = self.input_data_objects[SilverEventFlaggedDataObject.ID].df.filter(\n                (F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(current_date))\n                &amp; (F.col(ColNames.error_flag).isin(self.event_error_flags_to_include))\n            )\n\n            self.current_interesection_groups_sdf = (\n                self.input_data_objects[SilverCellIntersectionGroupsDataObject.ID]\n                .df.filter(\n                    (\n                        F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n                        == F.lit(current_date)\n                    )\n                )\n                .select(ColNames.cells)\n            )\n\n            self.transform()\n            self.write()\n\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n\n        current_events = self.current_input_events_sdf\n        last_time_segment = self.intital_time_segment\n\n        # TODO: This conversion is needed for Pandas serialisation/deserialisation,\n        # to remove it when user_id will be stored as string, not as binary\n        current_events = current_events.withColumn(ColNames.user_id, F.hex(F.col(ColNames.user_id)))\n\n        # Conversion to string is needed for easier intersection groups lookup in the aggregation function\n        groups_sdf = self.current_interesection_groups_sdf.withColumn(\n            ColNames.cells, F.concat_ws(\",\", F.col(ColNames.cells))\n        )\n\n        # Broadcast the intersection groups to all the workers\n        # TODO: To test this approach with large datasets, might not be feasible\n        groups_sdf = self.spark.sparkContext.broadcast(groups_sdf.collect())\n\n        # Partial function to pass the current date and other parameters to the aggregation function\n        aggregate_stays_partial = partial(\n            self.aggregate_stays,\n            current_date=self.current_date,\n            min_time_stay=self.min_time_stay,\n            max_time_missing_stay=self.max_time_missing_stay,\n            max_time_missing_move=self.max_time_missing_move,\n            pad_time=self.pad_time,\n            groups_sdf=groups_sdf,\n        )\n\n        groupby_cols = self.input_data_objects[SilverEventFlaggedDataObject.ID].partition_columns + [ColNames.user_id]\n\n        # Using cogroup to join the current events with the last time segment.\n        # Handy to avoid joining last segments to every row of the current events\n        # Also helps to detect missing events for the user for the last day or for the current day\n        # TODO: To test this approach with large datasets, might not be feasible\n        current_segments_sdf = (\n            current_events.groupby(*groupby_cols)\n            .cogroup(last_time_segment.groupby(*groupby_cols))\n            .applyInPandas(aggregate_stays_partial, self.segmentation_return_schema)\n        )\n\n        current_segments_sdf = current_segments_sdf.withColumns(\n            {\n                ColNames.year: F.year(ColNames.start_timestamp).cast(\"smallint\"),\n                ColNames.month: F.month(ColNames.start_timestamp).cast(\"tinyint\"),\n                ColNames.day: F.dayofmonth(ColNames.start_timestamp).cast(\"tinyint\"),\n            }\n        )\n\n        # Need to keep last segment for the next date iteration\n        # Have to add one day to time columns to be able to cogroup with the next day\n        last_segments = current_segments_sdf.filter(F.col(ColNames.is_last) == True)\n        last_segments = last_segments.withColumn(\n            ColNames.start_timestamp, F.date_add(F.col(ColNames.start_timestamp), 1)\n        )\n\n        last_segments = last_segments.withColumns(\n            {\n                ColNames.year: F.year(ColNames.start_timestamp).cast(\"smallint\"),\n                ColNames.month: F.month(ColNames.start_timestamp).cast(\"tinyint\"),\n                ColNames.day: F.dayofmonth(ColNames.start_timestamp).cast(\"tinyint\"),\n            }\n        )\n        self.intital_time_segment = last_segments.cache()\n\n        # TODO: This conversion is needed to get back to binary after Pandas serialisation/deserialisation,\n        # to remove it when user_id will be stored as string, not as binary\n        current_segments_sdf = current_segments_sdf.withColumn(ColNames.user_id, F.unhex(F.col(ColNames.user_id)))\n\n        current_segments_sdf = current_segments_sdf.select(\n            *[field.name for field in SilverTimeSegmentsDataObject.SCHEMA.fields]\n        )\n        columns = {\n            field.name: F.col(field.name).cast(field.dataType) for field in SilverTimeSegmentsDataObject.SCHEMA.fields\n        }\n        current_segments_sdf = current_segments_sdf.withColumns(columns)\n\n        self.output_data_objects[SilverTimeSegmentsDataObject.ID].df = current_segments_sdf\n\n    @staticmethod\n    def aggregate_stays(\n        pdf: pdDataFrame,\n        last_segments_pdf: pdDataFrame,\n        current_date: date,\n        min_time_stay: timedelta,\n        max_time_missing_stay: timedelta,\n        max_time_missing_move: timedelta,\n        pad_time: timedelta,\n        groups_sdf: DataFrame,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Aggregates events into Time Segments for a given user.\n\n        This method processes a Pandas DataFrame of user events, and aggregates them into time segments based on\n        certain conditions. It handles the first event separately, then iterates over the remaining events. For each\n        event, it checks if there's an intersection of an event cell and the current time segment cells and if the time\n        gap is within anacceptable range. Depending on the state of the current time segment and the result of\n        the intersection check, it either updates the current time segment or creates a new one.\n\n        Input user event data is expected to be sorted by timestamp.\n\n        Parameters:\n        pdf (pdDataFrame): The input Pandas DataFrame containing user events to be processed.\n        last_segments_pdf (pdDataFrame): A Pandas DataFrame containing the last segments.\n        current_date (date): The current date.\n        min_time_stay (timedelta): The minimum time to consider a segment as a 'stay'.\n        max_time_missing_stay (timedelta): The maximum time gap to consider continuation of a 'stay'.\n        max_time_missing_move (timedelta): The maximum time gap to consider continuation of a 'move'.\n        pad_time (timedelta): The padding time to have between 'unknown' segment.\n        groups_sdf (DataFrame): A PySpark DataFrame containing the groups.\n\n        Returns:\n        DataFrame: A DataFrame containing the aggregated time segments.\n        \"\"\"\n        segments = []\n        is_first_ts = True\n\n        current_date_start = datetime.combine(current_date, time())\n        current_date_end = datetime.combine(current_date, time(23, 59, 59))\n        previous_date_start = current_date_start - timedelta(days=1)\n        previous_date_end = current_date_end - timedelta(days=1)\n\n        intersection_pdf = pd.DataFrame(groups_sdf.value)\n        pdf = pdf.sort_values(by=[ColNames.timestamp])\n\n        # Depending on the presence of events and last segments, initialize the user info and the last time segment\n        # If there are no events, but last segment exists use the last time segment to derrive the user info\n        # and create 'unknown' segment for current date\n        # If there are events, use the first event to derrive the user info\n        # if there are no last segments, assume last segment as 'unknown' for the whole previous date\n        # if there are last segments, use the last segment\n        user_id, user_mod, mcc, current_ts = ContinuousTimeSegmentation.initialize_user_and_ts(\n            pdf, last_segments_pdf, current_date_start, current_date_end, previous_date_start, previous_date_end\n        )\n        # We process events only if there are any\n        if not pdf.empty:\n            for event in pdf.itertuples(index=False):\n                next_ts = {}\n                ts_to_add = []\n                event_timestamp = event[1]\n                event_cell = event[3]\n\n                # For the first time segment to start, look at the previous day's last segment\n                # and create a new time segment with the same state starting from the day start till the first event\n                if is_first_ts:\n                    next_ts = ContinuousTimeSegmentation.handle_first_segment(\n                        current_ts,\n                        event_timestamp,\n                        current_date_start,\n                        max_time_missing_stay,\n                        max_time_missing_move,\n                        pad_time,\n                    )\n                    current_ts = next_ts\n                    current_ts[ColNames.time_segment_id] = 1\n                    is_first_ts = False\n\n                current_intersection = list(set(current_ts[ColNames.cells] + [event_cell]))\n                is_intersected = ContinuousTimeSegmentation.check_intersection(\n                    current_ts[ColNames.cells], current_intersection, intersection_pdf\n                )\n\n                if current_ts[ColNames.state] == \"unknown\":\n                    # If the current state is 'unknown' (from the previous day),\n                    # create a new 'undetermined' time segment with pad_time adjustment\n                    next_ts = ContinuousTimeSegmentation.create_time_segment(\n                        current_ts[ColNames.end_timestamp],\n                        event_timestamp,\n                        [event_cell],\n                        \"undetermined\",\n                        current_ts[ColNames.time_segment_id],\n                    )\n                    ts_to_add = [current_ts]\n                    current_ts = next_ts\n\n                elif is_intersected and event_timestamp - current_ts[ColNames.end_timestamp] &lt;= max_time_missing_stay:\n                    # If there's an intersection, check if we should update the current_ts or create a new one\n                    if current_ts[ColNames.state] in [\"undetermined\", \"stay\"]:\n                        # If the current state is 'undetermined' or 'stay' and the time gap is within\n                        # the acceptable range for a stay update the current time segment with the new cell\n                        # and the new end timestamp and set state to stay\n                        current_ts[ColNames.end_timestamp] = event_timestamp\n                        current_ts[ColNames.cells] = current_intersection\n                        if event_timestamp - current_ts[ColNames.start_timestamp] &gt; min_time_stay:\n                            current_ts[ColNames.state] = \"stay\"\n                    elif current_ts[ColNames.state] == \"move\":\n                        # If the current state is 'move' and the time gap is within the acceptable range\n                        # create new time segment with state 'undetermined' after move segment\n                        next_ts = ContinuousTimeSegmentation.create_time_segment(\n                            current_ts[ColNames.end_timestamp],\n                            event_timestamp,\n                            [event_cell],\n                            \"undetermined\",\n                            current_ts[ColNames.time_segment_id],\n                        )\n                        # if time gap is big enough to assume that its stay change the state to stay\n                        if next_ts[ColNames.end_timestamp] - next_ts[ColNames.start_timestamp] &gt; min_time_stay:\n                            next_ts[ColNames.state] = \"stay\"\n                        ts_to_add = [current_ts]\n                        current_ts = next_ts\n\n                elif (\n                    not is_intersected and event_timestamp - current_ts[ColNames.end_timestamp] &lt;= max_time_missing_move\n                ):\n                    # If there's no intersection and the time gap is within the acceptable range for a move\n                    mid_point = (\n                        current_ts[ColNames.end_timestamp] + (event_timestamp - current_ts[ColNames.end_timestamp]) / 2\n                    )\n\n                    next_ts_1 = ContinuousTimeSegmentation.create_time_segment(\n                        current_ts[ColNames.end_timestamp],\n                        mid_point,\n                        current_ts[ColNames.cells],\n                        \"move\",\n                        current_ts[ColNames.time_segment_id],\n                    )\n\n                    next_ts_2 = ContinuousTimeSegmentation.create_time_segment(\n                        mid_point, event_timestamp, [event_cell], \"move\", next_ts_1[ColNames.time_segment_id]\n                    )\n\n                    ts_to_add = [current_ts, next_ts_1]\n                    current_ts = next_ts_2\n\n                else:\n                    # If the time gap is too big, create 'unknown' segment for missing time with pad_time adjustment\n                    # create new time segment with state 'undetermined' with pad_time adjustment\n                    current_ts[ColNames.end_timestamp] = current_ts[ColNames.end_timestamp] + pad_time\n\n                    next_ts_1 = ContinuousTimeSegmentation.create_time_segment(\n                        current_ts[ColNames.end_timestamp],\n                        event_timestamp - pad_time,\n                        [],\n                        \"unknown\",\n                        current_ts[ColNames.time_segment_id],\n                    )\n\n                    next_ts_2 = ContinuousTimeSegmentation.create_time_segment(\n                        event_timestamp - pad_time,\n                        event_timestamp,\n                        [event_cell],\n                        \"undetermined\",\n                        next_ts_1[ColNames.time_segment_id],\n                    )\n\n                    ts_to_add = [current_ts, next_ts_1]\n                    current_ts = next_ts_2\n\n                segments.extend(ts_to_add)\n\n        # TODO: NOT IMPLEMENTED.\n        # Currently there is no methodological description on how to handle the time from the last event to the end of the day.\n        # Create extra time segment that covers the duration from the last event-based time segment until the end of the date.\n        # if (current_ts[ColNames.end_timestamp] &lt; current_date_end):\n        #     last_ts = ContinuousTimeSegmentation.create_time_segment(\n        #                     current_ts[ColNames.end_timestamp],\n        #                     current_date_end,\n        #                     [],\n        #                     \"unknown\",\n        #                     current_ts[ColNames.time_segment_id],\n        #                 )\n        #     last_ts[ColNames.is_last] = True\n        #     segments.append(last_ts)\n        # else:\n        #     # If no extra time segment was generated, the final event-generated time segment is the last of the date.\n        #     current_ts[ColNames.is_last] = True\n\n        # Add final event-generated time segment to output list.\n        current_ts[ColNames.is_last] = True\n        segments.append(current_ts)\n\n        # Prepare return columns\n        segments_df = pd.DataFrame(segments)\n        segments_df[ColNames.user_id] = user_id\n        segments_df[ColNames.mcc] = mcc\n        segments_df[ColNames.user_id_modulo] = user_mod\n\n        return segments_df.sort_values(ColNames.start_timestamp)\n\n    @staticmethod\n    def handle_first_segment(\n        current_ts: Dict,\n        event_timestamp: datetime,\n        current_date_start: datetime,\n        max_time_missing_stay: timedelta,\n        max_time_missing_move: timedelta,\n        pad_time: timedelta,\n    ) -&gt; dict:\n        \"\"\"\n        Handles the first segment for a user for a date based on a previous date last segment.\n\n        This method takes the last time segment of previous date and the timestamp of the first\n            event in the current date.\n        It checks the state of the current time segment and the time difference between the end of the current\n        time segment and the first event in the next date.\n\n        If the state is 'undetermined' or 'stay' and the time difference is within the maximum missing stay time,\n        or if the state is 'move' and the time difference is within the maximum missing move time, it creates a new\n        time segment with the same cells, state. The start timestamp\n        of the new time segment is the start of the current date, and the end timestamp\n            is the timestamp of the first event.\n\n        If neither of these conditions are met, it creates a new time segment with\n            an empty list of cells, state 'unknown'.\n        The start timestamp of the new time segment is the start of the next date,\n            and the end timestamp is the timestamp of the first event minus the padding time.\n\n        Parameters:\n        current_ts (Dict): The last time segment from previous date.\n        event_timestamp (datetime): The timestamp of the first event in the current date.\n        current_date_start (datetime): The start of the current date.\n        max_time_missing_stay (timedelta): The maximum time gap to consider continuation of a 'stay'.\n        max_time_missing_move (timedelta): The maximum time gap to consider continuation of a 'move'.\n        pad_time (timedelta): The padding time to have between 'unknown' segment.\n\n        Returns:\n        dict: The new first time segment for the current date.\n        \"\"\"\n\n        if (\n            current_ts[ColNames.state] in [\"undetermined\", \"stay\"]\n            and event_timestamp - current_ts[ColNames.end_timestamp] &lt;= max_time_missing_stay\n        ):\n            next_ts = ContinuousTimeSegmentation.create_time_segment(\n                current_date_start,\n                event_timestamp,\n                list(current_ts[ColNames.cells]),\n                current_ts[ColNames.state],\n                current_ts[ColNames.time_segment_id],\n            )\n        elif (\n            current_ts[ColNames.state] == \"move\"\n            and event_timestamp - current_ts[ColNames.end_timestamp] &lt;= max_time_missing_move\n        ):\n            next_ts = ContinuousTimeSegmentation.create_time_segment(\n                current_date_start,\n                event_timestamp,\n                list(current_ts[ColNames.cells]),\n                current_ts[ColNames.state],\n                current_ts[ColNames.time_segment_id],\n            )\n        else:\n            pad_time = timedelta(seconds=0) if event_timestamp - current_date_start &lt; pad_time else pad_time\n            next_ts = ContinuousTimeSegmentation.create_time_segment(\n                current_date_start, event_timestamp - pad_time, [], \"unknown\", current_ts[ColNames.time_segment_id]\n            )\n\n        return next_ts\n\n    @staticmethod\n    def create_time_segment(\n        start_timestamp: datetime,\n        end_timestamp: datetime,\n        cells: List[str],\n        state: str,\n        previous_segment_id: Optional[int] = None,\n    ) -&gt; Dict:\n        \"\"\"\n        Creates a new time segment.\n\n        It creates a new time segment with these values, incrementing the segment ID by 1\n        if a previous segment ID is provided, or setting it to 1 if not.\n\n        Parameters:\n        start_timestamp (datetime): The start timestamp of the time segment.\n        end_timestamp (datetime): The end timestamp of the time segment.\n        cells (List[str]): The cells of the time segment.\n        state (str): The state of the time segment.\n        previous_segment_id (Optional[int]): The ID of the previous time segment, if any.\n\n        Returns:\n        Dict: The new time segment.\n        \"\"\"\n\n        previous_segment_id = previous_segment_id if previous_segment_id else 0\n\n        return {\n            ColNames.time_segment_id: previous_segment_id + 1,\n            ColNames.start_timestamp: start_timestamp,\n            ColNames.end_timestamp: end_timestamp,\n            ColNames.cells: cells,\n            ColNames.state: state,\n            ColNames.is_last: False,\n        }\n\n    @staticmethod\n    def initialize_user_and_ts(\n        pdf: pdDataFrame,\n        last_segments_pdf: pdDataFrame,\n        current_date_start: datetime,\n        current_date_end: datetime,\n        previous_date_start: datetime,\n        previous_date_end: datetime,\n    ) -&gt; Tuple[str, int, str, Dict]:\n        \"\"\"\n        Initializes the user ID, user ID modulo, MCC, and last time segment for time segmenation.\n\n        If the events DataFrame for the current date is empty, it uses the user ID, user ID modulo,\n            and MCC from the last segments DataFrame and creates a new 'unknown' time segment for the current date.\n\n        If the events DataFrame for the current date is not empty, it uses the user ID, user ID modulo,\n        and MCC from this DataFrame. If the last segments DataFrame is not empty, it uses the first\n        time segment from this DataFrame as the last time segment. If the last segments DataFrame is empty,\n        it creates a new 'unknown' time segment for the previous date.\n\n        Parameters:\n        pdf (pdDataFrame): The input Pandas DataFrame for the current date.\n        last_segments_pdf (pdDataFrame): A Pandas DataFrame containing the last segments.\n        current_date_start (datetime): The start of the current date.\n        current_date_end (datetime): The end of the current date.\n        previous_date_start (datetime): The start of the previous date.\n        previous_date_end (datetime): The end of the previous date.\n\n        Returns:\n        Tuple[str, int, str, Dict]: The user ID, user ID modulo, MCC, and last time segment.\n        \"\"\"\n\n        if pdf.empty:\n            user_id = last_segments_pdf[ColNames.user_id][0]\n            user_id_mod = last_segments_pdf[ColNames.user_id_modulo][0]\n            mcc = last_segments_pdf[ColNames.mcc][0]\n            last_time_segment = ContinuousTimeSegmentation.create_time_segment(\n                current_date_start, current_date_end, [], \"unknown\"\n            )\n        else:\n            user_id = pdf[ColNames.user_id][0]\n            user_id_mod = pdf[ColNames.user_id_modulo][0]\n            mcc = pdf[ColNames.mcc][0]\n            if not last_segments_pdf.empty:\n                last_time_segment = last_segments_pdf.iloc[0][\n                    [\n                        ColNames.time_segment_id,\n                        ColNames.start_timestamp,\n                        ColNames.end_timestamp,\n                        ColNames.cells,\n                        ColNames.state,\n                    ]\n                ].to_dict()\n            else:\n                last_time_segment = ContinuousTimeSegmentation.create_time_segment(\n                    previous_date_start, previous_date_end, [], \"unknown\"\n                )\n\n        return user_id, user_id_mod, mcc, last_time_segment\n\n    @staticmethod\n    def check_intersection(\n        previous_ts_inersection: List[str], current_intersection: List[str], intersection_pd_df: pdDataFrame\n    ) -&gt; bool:\n        \"\"\"\n        Checks if there is an intersection between the current and previous time segments.\n\n        This method takes two lists of cells, one for the previous time segment and one for the current time segment,\n        and a Pandas DataFrame of intersections.\n\n        If the list for the previous time segment is empty, it returns False, indicating that there is no intersection.\n\n        If the list for the previous time segment is not empty, it checks if the sorted, comma-separated string of\n        cells for the current time segment is in the DataFrame of intersections. If the list for the current time\n        segment has more than one cell, it returns the result of this check. If the list for the current time segment\n        has one cell, it returns True, indicating that there is an intersection.\n\n        Parameters:\n        previous_ts_inersection (List[str]): The cells of the previous time segment.\n        current_intersection (List[str]): The cells of the current time segment.\n        intersection_pd_df (pdDataFrame): A Pandas DataFrame containing the intersections.\n\n        Returns:\n        bool: True if there is an intersection, False otherwise.\n        \"\"\"\n        if len(previous_ts_inersection) == 0:\n            is_intersected = False\n        else:\n            is_intersected = (\n                \",\".join(sorted(current_intersection)) in intersection_pd_df[0].values\n                if len(current_intersection) &gt; 1\n                else True\n            )\n        return is_intersected\n</code></pre>"},{"location":"reference/components/execution/time_segments/continuous_time_segmentation/#components.execution.time_segments.continuous_time_segmentation.ContinuousTimeSegmentation.aggregate_stays","title":"<code>aggregate_stays(pdf, last_segments_pdf, current_date, min_time_stay, max_time_missing_stay, max_time_missing_move, pad_time, groups_sdf)</code>  <code>staticmethod</code>","text":"<p>Aggregates events into Time Segments for a given user.</p> <p>This method processes a Pandas DataFrame of user events, and aggregates them into time segments based on certain conditions. It handles the first event separately, then iterates over the remaining events. For each event, it checks if there's an intersection of an event cell and the current time segment cells and if the time gap is within anacceptable range. Depending on the state of the current time segment and the result of the intersection check, it either updates the current time segment or creates a new one.</p> <p>Input user event data is expected to be sorted by timestamp.</p> <p>Parameters: pdf (pdDataFrame): The input Pandas DataFrame containing user events to be processed. last_segments_pdf (pdDataFrame): A Pandas DataFrame containing the last segments. current_date (date): The current date. min_time_stay (timedelta): The minimum time to consider a segment as a 'stay'. max_time_missing_stay (timedelta): The maximum time gap to consider continuation of a 'stay'. max_time_missing_move (timedelta): The maximum time gap to consider continuation of a 'move'. pad_time (timedelta): The padding time to have between 'unknown' segment. groups_sdf (DataFrame): A PySpark DataFrame containing the groups.</p> <p>Returns: DataFrame: A DataFrame containing the aggregated time segments.</p> Source code in <code>multimno/components/execution/time_segments/continuous_time_segmentation.py</code> <pre><code>@staticmethod\ndef aggregate_stays(\n    pdf: pdDataFrame,\n    last_segments_pdf: pdDataFrame,\n    current_date: date,\n    min_time_stay: timedelta,\n    max_time_missing_stay: timedelta,\n    max_time_missing_move: timedelta,\n    pad_time: timedelta,\n    groups_sdf: DataFrame,\n) -&gt; DataFrame:\n    \"\"\"\n    Aggregates events into Time Segments for a given user.\n\n    This method processes a Pandas DataFrame of user events, and aggregates them into time segments based on\n    certain conditions. It handles the first event separately, then iterates over the remaining events. For each\n    event, it checks if there's an intersection of an event cell and the current time segment cells and if the time\n    gap is within anacceptable range. Depending on the state of the current time segment and the result of\n    the intersection check, it either updates the current time segment or creates a new one.\n\n    Input user event data is expected to be sorted by timestamp.\n\n    Parameters:\n    pdf (pdDataFrame): The input Pandas DataFrame containing user events to be processed.\n    last_segments_pdf (pdDataFrame): A Pandas DataFrame containing the last segments.\n    current_date (date): The current date.\n    min_time_stay (timedelta): The minimum time to consider a segment as a 'stay'.\n    max_time_missing_stay (timedelta): The maximum time gap to consider continuation of a 'stay'.\n    max_time_missing_move (timedelta): The maximum time gap to consider continuation of a 'move'.\n    pad_time (timedelta): The padding time to have between 'unknown' segment.\n    groups_sdf (DataFrame): A PySpark DataFrame containing the groups.\n\n    Returns:\n    DataFrame: A DataFrame containing the aggregated time segments.\n    \"\"\"\n    segments = []\n    is_first_ts = True\n\n    current_date_start = datetime.combine(current_date, time())\n    current_date_end = datetime.combine(current_date, time(23, 59, 59))\n    previous_date_start = current_date_start - timedelta(days=1)\n    previous_date_end = current_date_end - timedelta(days=1)\n\n    intersection_pdf = pd.DataFrame(groups_sdf.value)\n    pdf = pdf.sort_values(by=[ColNames.timestamp])\n\n    # Depending on the presence of events and last segments, initialize the user info and the last time segment\n    # If there are no events, but last segment exists use the last time segment to derrive the user info\n    # and create 'unknown' segment for current date\n    # If there are events, use the first event to derrive the user info\n    # if there are no last segments, assume last segment as 'unknown' for the whole previous date\n    # if there are last segments, use the last segment\n    user_id, user_mod, mcc, current_ts = ContinuousTimeSegmentation.initialize_user_and_ts(\n        pdf, last_segments_pdf, current_date_start, current_date_end, previous_date_start, previous_date_end\n    )\n    # We process events only if there are any\n    if not pdf.empty:\n        for event in pdf.itertuples(index=False):\n            next_ts = {}\n            ts_to_add = []\n            event_timestamp = event[1]\n            event_cell = event[3]\n\n            # For the first time segment to start, look at the previous day's last segment\n            # and create a new time segment with the same state starting from the day start till the first event\n            if is_first_ts:\n                next_ts = ContinuousTimeSegmentation.handle_first_segment(\n                    current_ts,\n                    event_timestamp,\n                    current_date_start,\n                    max_time_missing_stay,\n                    max_time_missing_move,\n                    pad_time,\n                )\n                current_ts = next_ts\n                current_ts[ColNames.time_segment_id] = 1\n                is_first_ts = False\n\n            current_intersection = list(set(current_ts[ColNames.cells] + [event_cell]))\n            is_intersected = ContinuousTimeSegmentation.check_intersection(\n                current_ts[ColNames.cells], current_intersection, intersection_pdf\n            )\n\n            if current_ts[ColNames.state] == \"unknown\":\n                # If the current state is 'unknown' (from the previous day),\n                # create a new 'undetermined' time segment with pad_time adjustment\n                next_ts = ContinuousTimeSegmentation.create_time_segment(\n                    current_ts[ColNames.end_timestamp],\n                    event_timestamp,\n                    [event_cell],\n                    \"undetermined\",\n                    current_ts[ColNames.time_segment_id],\n                )\n                ts_to_add = [current_ts]\n                current_ts = next_ts\n\n            elif is_intersected and event_timestamp - current_ts[ColNames.end_timestamp] &lt;= max_time_missing_stay:\n                # If there's an intersection, check if we should update the current_ts or create a new one\n                if current_ts[ColNames.state] in [\"undetermined\", \"stay\"]:\n                    # If the current state is 'undetermined' or 'stay' and the time gap is within\n                    # the acceptable range for a stay update the current time segment with the new cell\n                    # and the new end timestamp and set state to stay\n                    current_ts[ColNames.end_timestamp] = event_timestamp\n                    current_ts[ColNames.cells] = current_intersection\n                    if event_timestamp - current_ts[ColNames.start_timestamp] &gt; min_time_stay:\n                        current_ts[ColNames.state] = \"stay\"\n                elif current_ts[ColNames.state] == \"move\":\n                    # If the current state is 'move' and the time gap is within the acceptable range\n                    # create new time segment with state 'undetermined' after move segment\n                    next_ts = ContinuousTimeSegmentation.create_time_segment(\n                        current_ts[ColNames.end_timestamp],\n                        event_timestamp,\n                        [event_cell],\n                        \"undetermined\",\n                        current_ts[ColNames.time_segment_id],\n                    )\n                    # if time gap is big enough to assume that its stay change the state to stay\n                    if next_ts[ColNames.end_timestamp] - next_ts[ColNames.start_timestamp] &gt; min_time_stay:\n                        next_ts[ColNames.state] = \"stay\"\n                    ts_to_add = [current_ts]\n                    current_ts = next_ts\n\n            elif (\n                not is_intersected and event_timestamp - current_ts[ColNames.end_timestamp] &lt;= max_time_missing_move\n            ):\n                # If there's no intersection and the time gap is within the acceptable range for a move\n                mid_point = (\n                    current_ts[ColNames.end_timestamp] + (event_timestamp - current_ts[ColNames.end_timestamp]) / 2\n                )\n\n                next_ts_1 = ContinuousTimeSegmentation.create_time_segment(\n                    current_ts[ColNames.end_timestamp],\n                    mid_point,\n                    current_ts[ColNames.cells],\n                    \"move\",\n                    current_ts[ColNames.time_segment_id],\n                )\n\n                next_ts_2 = ContinuousTimeSegmentation.create_time_segment(\n                    mid_point, event_timestamp, [event_cell], \"move\", next_ts_1[ColNames.time_segment_id]\n                )\n\n                ts_to_add = [current_ts, next_ts_1]\n                current_ts = next_ts_2\n\n            else:\n                # If the time gap is too big, create 'unknown' segment for missing time with pad_time adjustment\n                # create new time segment with state 'undetermined' with pad_time adjustment\n                current_ts[ColNames.end_timestamp] = current_ts[ColNames.end_timestamp] + pad_time\n\n                next_ts_1 = ContinuousTimeSegmentation.create_time_segment(\n                    current_ts[ColNames.end_timestamp],\n                    event_timestamp - pad_time,\n                    [],\n                    \"unknown\",\n                    current_ts[ColNames.time_segment_id],\n                )\n\n                next_ts_2 = ContinuousTimeSegmentation.create_time_segment(\n                    event_timestamp - pad_time,\n                    event_timestamp,\n                    [event_cell],\n                    \"undetermined\",\n                    next_ts_1[ColNames.time_segment_id],\n                )\n\n                ts_to_add = [current_ts, next_ts_1]\n                current_ts = next_ts_2\n\n            segments.extend(ts_to_add)\n\n    # TODO: NOT IMPLEMENTED.\n    # Currently there is no methodological description on how to handle the time from the last event to the end of the day.\n    # Create extra time segment that covers the duration from the last event-based time segment until the end of the date.\n    # if (current_ts[ColNames.end_timestamp] &lt; current_date_end):\n    #     last_ts = ContinuousTimeSegmentation.create_time_segment(\n    #                     current_ts[ColNames.end_timestamp],\n    #                     current_date_end,\n    #                     [],\n    #                     \"unknown\",\n    #                     current_ts[ColNames.time_segment_id],\n    #                 )\n    #     last_ts[ColNames.is_last] = True\n    #     segments.append(last_ts)\n    # else:\n    #     # If no extra time segment was generated, the final event-generated time segment is the last of the date.\n    #     current_ts[ColNames.is_last] = True\n\n    # Add final event-generated time segment to output list.\n    current_ts[ColNames.is_last] = True\n    segments.append(current_ts)\n\n    # Prepare return columns\n    segments_df = pd.DataFrame(segments)\n    segments_df[ColNames.user_id] = user_id\n    segments_df[ColNames.mcc] = mcc\n    segments_df[ColNames.user_id_modulo] = user_mod\n\n    return segments_df.sort_values(ColNames.start_timestamp)\n</code></pre>"},{"location":"reference/components/execution/time_segments/continuous_time_segmentation/#components.execution.time_segments.continuous_time_segmentation.ContinuousTimeSegmentation.check_intersection","title":"<code>check_intersection(previous_ts_inersection, current_intersection, intersection_pd_df)</code>  <code>staticmethod</code>","text":"<p>Checks if there is an intersection between the current and previous time segments.</p> <p>This method takes two lists of cells, one for the previous time segment and one for the current time segment, and a Pandas DataFrame of intersections.</p> <p>If the list for the previous time segment is empty, it returns False, indicating that there is no intersection.</p> <p>If the list for the previous time segment is not empty, it checks if the sorted, comma-separated string of cells for the current time segment is in the DataFrame of intersections. If the list for the current time segment has more than one cell, it returns the result of this check. If the list for the current time segment has one cell, it returns True, indicating that there is an intersection.</p> <p>Parameters: previous_ts_inersection (List[str]): The cells of the previous time segment. current_intersection (List[str]): The cells of the current time segment. intersection_pd_df (pdDataFrame): A Pandas DataFrame containing the intersections.</p> <p>Returns: bool: True if there is an intersection, False otherwise.</p> Source code in <code>multimno/components/execution/time_segments/continuous_time_segmentation.py</code> <pre><code>@staticmethod\ndef check_intersection(\n    previous_ts_inersection: List[str], current_intersection: List[str], intersection_pd_df: pdDataFrame\n) -&gt; bool:\n    \"\"\"\n    Checks if there is an intersection between the current and previous time segments.\n\n    This method takes two lists of cells, one for the previous time segment and one for the current time segment,\n    and a Pandas DataFrame of intersections.\n\n    If the list for the previous time segment is empty, it returns False, indicating that there is no intersection.\n\n    If the list for the previous time segment is not empty, it checks if the sorted, comma-separated string of\n    cells for the current time segment is in the DataFrame of intersections. If the list for the current time\n    segment has more than one cell, it returns the result of this check. If the list for the current time segment\n    has one cell, it returns True, indicating that there is an intersection.\n\n    Parameters:\n    previous_ts_inersection (List[str]): The cells of the previous time segment.\n    current_intersection (List[str]): The cells of the current time segment.\n    intersection_pd_df (pdDataFrame): A Pandas DataFrame containing the intersections.\n\n    Returns:\n    bool: True if there is an intersection, False otherwise.\n    \"\"\"\n    if len(previous_ts_inersection) == 0:\n        is_intersected = False\n    else:\n        is_intersected = (\n            \",\".join(sorted(current_intersection)) in intersection_pd_df[0].values\n            if len(current_intersection) &gt; 1\n            else True\n        )\n    return is_intersected\n</code></pre>"},{"location":"reference/components/execution/time_segments/continuous_time_segmentation/#components.execution.time_segments.continuous_time_segmentation.ContinuousTimeSegmentation.create_time_segment","title":"<code>create_time_segment(start_timestamp, end_timestamp, cells, state, previous_segment_id=None)</code>  <code>staticmethod</code>","text":"<p>Creates a new time segment.</p> <p>It creates a new time segment with these values, incrementing the segment ID by 1 if a previous segment ID is provided, or setting it to 1 if not.</p> <p>Parameters: start_timestamp (datetime): The start timestamp of the time segment. end_timestamp (datetime): The end timestamp of the time segment. cells (List[str]): The cells of the time segment. state (str): The state of the time segment. previous_segment_id (Optional[int]): The ID of the previous time segment, if any.</p> <p>Returns: Dict: The new time segment.</p> Source code in <code>multimno/components/execution/time_segments/continuous_time_segmentation.py</code> <pre><code>@staticmethod\ndef create_time_segment(\n    start_timestamp: datetime,\n    end_timestamp: datetime,\n    cells: List[str],\n    state: str,\n    previous_segment_id: Optional[int] = None,\n) -&gt; Dict:\n    \"\"\"\n    Creates a new time segment.\n\n    It creates a new time segment with these values, incrementing the segment ID by 1\n    if a previous segment ID is provided, or setting it to 1 if not.\n\n    Parameters:\n    start_timestamp (datetime): The start timestamp of the time segment.\n    end_timestamp (datetime): The end timestamp of the time segment.\n    cells (List[str]): The cells of the time segment.\n    state (str): The state of the time segment.\n    previous_segment_id (Optional[int]): The ID of the previous time segment, if any.\n\n    Returns:\n    Dict: The new time segment.\n    \"\"\"\n\n    previous_segment_id = previous_segment_id if previous_segment_id else 0\n\n    return {\n        ColNames.time_segment_id: previous_segment_id + 1,\n        ColNames.start_timestamp: start_timestamp,\n        ColNames.end_timestamp: end_timestamp,\n        ColNames.cells: cells,\n        ColNames.state: state,\n        ColNames.is_last: False,\n    }\n</code></pre>"},{"location":"reference/components/execution/time_segments/continuous_time_segmentation/#components.execution.time_segments.continuous_time_segmentation.ContinuousTimeSegmentation.handle_first_segment","title":"<code>handle_first_segment(current_ts, event_timestamp, current_date_start, max_time_missing_stay, max_time_missing_move, pad_time)</code>  <code>staticmethod</code>","text":"<p>Handles the first segment for a user for a date based on a previous date last segment.</p> <p>This method takes the last time segment of previous date and the timestamp of the first     event in the current date. It checks the state of the current time segment and the time difference between the end of the current time segment and the first event in the next date.</p> <p>If the state is 'undetermined' or 'stay' and the time difference is within the maximum missing stay time, or if the state is 'move' and the time difference is within the maximum missing move time, it creates a new time segment with the same cells, state. The start timestamp of the new time segment is the start of the current date, and the end timestamp     is the timestamp of the first event.</p> <p>If neither of these conditions are met, it creates a new time segment with     an empty list of cells, state 'unknown'. The start timestamp of the new time segment is the start of the next date,     and the end timestamp is the timestamp of the first event minus the padding time.</p> <p>Parameters: current_ts (Dict): The last time segment from previous date. event_timestamp (datetime): The timestamp of the first event in the current date. current_date_start (datetime): The start of the current date. max_time_missing_stay (timedelta): The maximum time gap to consider continuation of a 'stay'. max_time_missing_move (timedelta): The maximum time gap to consider continuation of a 'move'. pad_time (timedelta): The padding time to have between 'unknown' segment.</p> <p>Returns: dict: The new first time segment for the current date.</p> Source code in <code>multimno/components/execution/time_segments/continuous_time_segmentation.py</code> <pre><code>@staticmethod\ndef handle_first_segment(\n    current_ts: Dict,\n    event_timestamp: datetime,\n    current_date_start: datetime,\n    max_time_missing_stay: timedelta,\n    max_time_missing_move: timedelta,\n    pad_time: timedelta,\n) -&gt; dict:\n    \"\"\"\n    Handles the first segment for a user for a date based on a previous date last segment.\n\n    This method takes the last time segment of previous date and the timestamp of the first\n        event in the current date.\n    It checks the state of the current time segment and the time difference between the end of the current\n    time segment and the first event in the next date.\n\n    If the state is 'undetermined' or 'stay' and the time difference is within the maximum missing stay time,\n    or if the state is 'move' and the time difference is within the maximum missing move time, it creates a new\n    time segment with the same cells, state. The start timestamp\n    of the new time segment is the start of the current date, and the end timestamp\n        is the timestamp of the first event.\n\n    If neither of these conditions are met, it creates a new time segment with\n        an empty list of cells, state 'unknown'.\n    The start timestamp of the new time segment is the start of the next date,\n        and the end timestamp is the timestamp of the first event minus the padding time.\n\n    Parameters:\n    current_ts (Dict): The last time segment from previous date.\n    event_timestamp (datetime): The timestamp of the first event in the current date.\n    current_date_start (datetime): The start of the current date.\n    max_time_missing_stay (timedelta): The maximum time gap to consider continuation of a 'stay'.\n    max_time_missing_move (timedelta): The maximum time gap to consider continuation of a 'move'.\n    pad_time (timedelta): The padding time to have between 'unknown' segment.\n\n    Returns:\n    dict: The new first time segment for the current date.\n    \"\"\"\n\n    if (\n        current_ts[ColNames.state] in [\"undetermined\", \"stay\"]\n        and event_timestamp - current_ts[ColNames.end_timestamp] &lt;= max_time_missing_stay\n    ):\n        next_ts = ContinuousTimeSegmentation.create_time_segment(\n            current_date_start,\n            event_timestamp,\n            list(current_ts[ColNames.cells]),\n            current_ts[ColNames.state],\n            current_ts[ColNames.time_segment_id],\n        )\n    elif (\n        current_ts[ColNames.state] == \"move\"\n        and event_timestamp - current_ts[ColNames.end_timestamp] &lt;= max_time_missing_move\n    ):\n        next_ts = ContinuousTimeSegmentation.create_time_segment(\n            current_date_start,\n            event_timestamp,\n            list(current_ts[ColNames.cells]),\n            current_ts[ColNames.state],\n            current_ts[ColNames.time_segment_id],\n        )\n    else:\n        pad_time = timedelta(seconds=0) if event_timestamp - current_date_start &lt; pad_time else pad_time\n        next_ts = ContinuousTimeSegmentation.create_time_segment(\n            current_date_start, event_timestamp - pad_time, [], \"unknown\", current_ts[ColNames.time_segment_id]\n        )\n\n    return next_ts\n</code></pre>"},{"location":"reference/components/execution/time_segments/continuous_time_segmentation/#components.execution.time_segments.continuous_time_segmentation.ContinuousTimeSegmentation.initialize_user_and_ts","title":"<code>initialize_user_and_ts(pdf, last_segments_pdf, current_date_start, current_date_end, previous_date_start, previous_date_end)</code>  <code>staticmethod</code>","text":"<p>Initializes the user ID, user ID modulo, MCC, and last time segment for time segmenation.</p> <p>If the events DataFrame for the current date is empty, it uses the user ID, user ID modulo,     and MCC from the last segments DataFrame and creates a new 'unknown' time segment for the current date.</p> <p>If the events DataFrame for the current date is not empty, it uses the user ID, user ID modulo, and MCC from this DataFrame. If the last segments DataFrame is not empty, it uses the first time segment from this DataFrame as the last time segment. If the last segments DataFrame is empty, it creates a new 'unknown' time segment for the previous date.</p> <p>Parameters: pdf (pdDataFrame): The input Pandas DataFrame for the current date. last_segments_pdf (pdDataFrame): A Pandas DataFrame containing the last segments. current_date_start (datetime): The start of the current date. current_date_end (datetime): The end of the current date. previous_date_start (datetime): The start of the previous date. previous_date_end (datetime): The end of the previous date.</p> <p>Returns: Tuple[str, int, str, Dict]: The user ID, user ID modulo, MCC, and last time segment.</p> Source code in <code>multimno/components/execution/time_segments/continuous_time_segmentation.py</code> <pre><code>@staticmethod\ndef initialize_user_and_ts(\n    pdf: pdDataFrame,\n    last_segments_pdf: pdDataFrame,\n    current_date_start: datetime,\n    current_date_end: datetime,\n    previous_date_start: datetime,\n    previous_date_end: datetime,\n) -&gt; Tuple[str, int, str, Dict]:\n    \"\"\"\n    Initializes the user ID, user ID modulo, MCC, and last time segment for time segmenation.\n\n    If the events DataFrame for the current date is empty, it uses the user ID, user ID modulo,\n        and MCC from the last segments DataFrame and creates a new 'unknown' time segment for the current date.\n\n    If the events DataFrame for the current date is not empty, it uses the user ID, user ID modulo,\n    and MCC from this DataFrame. If the last segments DataFrame is not empty, it uses the first\n    time segment from this DataFrame as the last time segment. If the last segments DataFrame is empty,\n    it creates a new 'unknown' time segment for the previous date.\n\n    Parameters:\n    pdf (pdDataFrame): The input Pandas DataFrame for the current date.\n    last_segments_pdf (pdDataFrame): A Pandas DataFrame containing the last segments.\n    current_date_start (datetime): The start of the current date.\n    current_date_end (datetime): The end of the current date.\n    previous_date_start (datetime): The start of the previous date.\n    previous_date_end (datetime): The end of the previous date.\n\n    Returns:\n    Tuple[str, int, str, Dict]: The user ID, user ID modulo, MCC, and last time segment.\n    \"\"\"\n\n    if pdf.empty:\n        user_id = last_segments_pdf[ColNames.user_id][0]\n        user_id_mod = last_segments_pdf[ColNames.user_id_modulo][0]\n        mcc = last_segments_pdf[ColNames.mcc][0]\n        last_time_segment = ContinuousTimeSegmentation.create_time_segment(\n            current_date_start, current_date_end, [], \"unknown\"\n        )\n    else:\n        user_id = pdf[ColNames.user_id][0]\n        user_id_mod = pdf[ColNames.user_id_modulo][0]\n        mcc = pdf[ColNames.mcc][0]\n        if not last_segments_pdf.empty:\n            last_time_segment = last_segments_pdf.iloc[0][\n                [\n                    ColNames.time_segment_id,\n                    ColNames.start_timestamp,\n                    ColNames.end_timestamp,\n                    ColNames.cells,\n                    ColNames.state,\n                ]\n            ].to_dict()\n        else:\n            last_time_segment = ContinuousTimeSegmentation.create_time_segment(\n                previous_date_start, previous_date_end, [], \"unknown\"\n            )\n\n    return user_id, user_id_mod, mcc, last_time_segment\n</code></pre>"},{"location":"reference/components/ingestion/","title":"ingestion","text":""},{"location":"reference/components/ingestion/grid_generation/","title":"grid_generation","text":""},{"location":"reference/components/ingestion/grid_generation/inspire_grid_generation/","title":"inspire_grid_generation","text":"<p>This module contains the InspireGridGeneration class which is responsible for generating the INSPIRE grid  and enrich it with elevation and landuse data.</p>"},{"location":"reference/components/ingestion/grid_generation/inspire_grid_generation/#components.ingestion.grid_generation.inspire_grid_generation.InspireGridGeneration","title":"<code>InspireGridGeneration</code>","text":"<p>             Bases: <code>Component</code></p> <p>This class is responsible for generating the INSPIRE grid for given extent or polygon and enrich it with elevation and landuse data.</p> Source code in <code>multimno/components/ingestion/grid_generation/inspire_grid_generation.py</code> <pre><code>class InspireGridGeneration(Component):\n    \"\"\"\n    This class is responsible for generating the INSPIRE grid for given extent or polygon\n    and enrich it with elevation and landuse data.\n    \"\"\"\n\n    COMPONENT_ID = \"InspireGridGeneration\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.do_elevation_enrichment = self.config.get(InspireGridGeneration.COMPONENT_ID, \"do_elevation_enrichment\")\n        self.do_land_cover_enrichment = self.config.get(InspireGridGeneration.COMPONENT_ID, \"do_landcover_enrichment\")\n\n        exent_dict = self.config.geteval(InspireGridGeneration.COMPONENT_ID, \"extent\")\n\n        self.grid_extent = [exent_dict[\"min_lon\"], exent_dict[\"min_lat\"], exent_dict[\"max_lon\"], exent_dict[\"max_lat\"]]\n        self.grid_partition_size = self.config.get(InspireGridGeneration.COMPONENT_ID, \"grid_partition_size\")\n\n        # TODO: For now set to default 100, but can be dynamic from config in future\n        self.grid_resolution = 100\n\n    def initalize_data_objects(self):\n\n        grid_do_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"grid_data_silver\")\n\n        self.output_data_objects = {}\n        self.output_data_objects[SilverGridDataObject.ID] = SilverGridDataObject(self.spark, grid_do_path, [])\n\n    def read(self):\n        # TODO implement read method to read elevation and landuse rasters\n        pass\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n        grid_generator = InspireGridGenerator(\n            self.spark, self.grid_resolution, ColNames.geometry, ColNames.grid_id, self.grid_partition_size\n        )\n\n        grid_sdf = grid_generator.cover_extent_with_grid_centroids(self.grid_extent)\n\n        if self.do_elevation_enrichment:\n            self.add_elevation_to_grid()\n\n        if self.do_land_cover_enrichment:\n            self.add_landcover_to_grid()\n\n        # Cast column types to DO schema, add missing columns manually\n        df_columns = set(grid_sdf.columns)\n        schema_columns = set(field.name for field in SilverGridDataObject.SCHEMA.fields)\n        missing_columns = schema_columns - df_columns\n\n        for column in missing_columns:\n            grid_sdf = grid_sdf.withColumn(column, F.lit(None).cast(SilverGridDataObject.SCHEMA[column].dataType))\n\n        # Apply schema casting\n        for field in SilverGridDataObject.SCHEMA.fields:\n            grid_sdf = grid_sdf.withColumn(field.name, F.col(field.name).cast(field.dataType))\n\n        # TODO: figure out why createDataFrame from rdd is not working for geometry column and how to make it work\n\n        self.output_data_objects[SilverGridDataObject.ID].df = grid_sdf\n\n    def add_elevation_to_grid(self):\n        # TODO: implement elevation enrichment\n        pass\n\n    def add_landcover_to_grid(self):\n        # TODO: implement landcover enrichment\n        pass\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/","title":"synthetic","text":""},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/","title":"synthetic_diaries","text":"<p>Module that generates a MNO synthetic network.</p>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries","title":"<code>SyntheticDiaries</code>","text":"<p>             Bases: <code>Component</code></p> <p>Class that generates the synthetic activity-trip diaries data. It inherits from the Component abstract class.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>class SyntheticDiaries(Component):\n    \"\"\"\n    Class that generates the synthetic activity-trip diaries data.\n    It inherits from the Component abstract class.\n    \"\"\"\n\n    COMPONENT_ID = \"SyntheticDiaries\"\n\n    def __init__(self, general_config_path: str, component_config_path: str):\n        # keep super class init method:\n        super().__init__(general_config_path=general_config_path, component_config_path=component_config_path)\n\n        # and additionally:\n        # self.n_partitions = self.config.getint(self.COMPONENT_ID, \"n_partitions\")\n\n        self.number_of_users = self.config.getint(self.COMPONENT_ID, \"number_of_users\")\n\n        self.date_format = self.config.get(self.COMPONENT_ID, \"date_format\")\n        self.initial_date = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"initial_date\"), self.date_format\n        ).date()\n        self.number_of_dates = self.config.getint(self.COMPONENT_ID, \"number_of_dates\")\n        self.date_range = [(self.initial_date + datetime.timedelta(days=d)) for d in range(self.number_of_dates)]\n\n        self.longitude_min = self.config.getfloat(self.COMPONENT_ID, \"longitude_min\")\n        self.longitude_max = self.config.getfloat(self.COMPONENT_ID, \"longitude_max\")\n        self.latitude_min = self.config.getfloat(self.COMPONENT_ID, \"latitude_min\")\n        self.latitude_max = self.config.getfloat(self.COMPONENT_ID, \"latitude_max\")\n\n        self.home_work_distance_min = self.config.getfloat(self.COMPONENT_ID, \"home_work_distance_min\")\n        self.home_work_distance_max = self.config.getfloat(self.COMPONENT_ID, \"home_work_distance_max\")\n        self.other_distance_min = self.config.getfloat(self.COMPONENT_ID, \"other_distance_min\")\n        self.other_distance_max = self.config.getfloat(self.COMPONENT_ID, \"other_distance_max\")\n\n        self.home_duration_min = self.config.getfloat(self.COMPONENT_ID, \"home_duration_min\")\n        self.home_duration_max = self.config.getfloat(self.COMPONENT_ID, \"home_duration_max\")\n        self.work_duration_min = self.config.getfloat(self.COMPONENT_ID, \"work_duration_min\")\n        self.work_duration_max = self.config.getfloat(self.COMPONENT_ID, \"work_duration_max\")\n        self.other_duration_min = self.config.getfloat(self.COMPONENT_ID, \"other_duration_min\")\n        self.other_duration_max = self.config.getfloat(self.COMPONENT_ID, \"other_duration_max\")\n\n        self.displacement_speed = self.config.getfloat(self.COMPONENT_ID, \"displacement_speed\")\n\n        self.stay_sequence_superset = self.config.get(self.COMPONENT_ID, \"stay_sequence_superset\").split(\",\")\n        self.stay_sequence_probabilities = [\n            float(w)\n            for w in self.config.get(self.COMPONENT_ID, \"stay_sequence_probabilities\").split(\n                \",\"\n            )  # TODO: cambiar por stay_sequence\n        ]\n        assert len(self.stay_sequence_superset) == len(self.stay_sequence_probabilities)\n\n    def initalize_data_objects(self):\n        output_synthetic_diaries_data_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"diaries_data_bronze\")\n        bronze_synthetic_diaries = BronzeSyntheticDiariesDataObject(\n            self.spark,\n            output_synthetic_diaries_data_path,\n            partition_columns=[ColNames.year, ColNames.month, ColNames.day],\n        )\n        self.output_data_objects = {\"SyntheticDiaries\": bronze_synthetic_diaries}\n\n    def read(self):\n        pass  # No input datasets are used in this component\n\n    def transform(self):\n        spark = self.spark\n        activities_df = spark.createDataFrame(self.generate_activities())\n        activities_df = calc_hashed_user_id(activities_df)\n        columns = {\n            field.name: F.col(field.name).cast(field.dataType)\n            for field in BronzeSyntheticDiariesDataObject.SCHEMA.fields\n        }\n        activities_df = activities_df.withColumns(columns)\n        self.output_data_objects[\"SyntheticDiaries\"].df = activities_df\n\n    def haversine(self, lon1: float, lat1: float, lon2: float, lat2: float) -&gt; float:\n        \"\"\"\n        Calculate the haversine distance in meters between two points.\n\n        Args:\n            lon1 (float): longitude of first point, in decimal degrees.\n            lat1 (float): latitude of first point, in decimal degrees.\n            lon2 (float): longitude of second point, in decimal degrees.\n            lat2 (float): latitude of second point, in decimal degrees.\n\n        Returns:\n            float: distance between both points, in meters.\n        \"\"\"\n        r = 6_371_000  # Radius of earth in meters.\n\n        # convert decimal degrees to radians\n        lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n\n        # haversine formula\n        dlon = lon2 - lon1\n        dlat = lat2 - lat1\n        a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n        c = 2 * asin(sqrt(a))\n        return c * r\n\n    def random_seed_number_generator(\n        self, base_seed: int, agent_id: int = None, date: datetime.date = None, i: int = None\n    ) -&gt; int:\n        \"\"\"\n        Generate random seed integer based on provided arguments.\n\n        Args:\n            base_seed (int): base integer for operations.\n            agent_id (int, optional): agent identifier. Defaults to None.\n            date (datetime.date, optional): date. Defaults to None.\n            i (int, optional): position integer. Defaults to None.\n\n        Returns:\n            int: generated random seed integer.\n        \"\"\"\n        seed = base_seed\n        if agent_id is not None:\n            seed += int(agent_id) * 100\n        if date is not None:\n            start_datetime = datetime.datetime.combine(date, datetime.time(0))\n            seed += int(start_datetime.timestamp())\n        if i is not None:\n            seed += i\n        return seed\n\n    def calculate_trip_time(self, o_location: tuple[float, float], d_location: tuple[float, float]) -&gt; float:\n        \"\"\"\n        Calculate trip time given an origin location and a destination\n        location, according to the specified trip speed.\n\n        Args:\n            o_location (tuple[float,float]): lon, lat of 1st point,\n                in decimal degrees.\n            d_location (tuple[float,float]): lon, lat of 2nd point,\n                in decimal degrees.\n\n        Returns:\n            float: trip time, in seconds.\n        \"\"\"\n        trip_distance = self.haversine(o_location[0], o_location[1], d_location[0], d_location[1])  # m\n        trip_speed = self.displacement_speed  # m/s\n        trip_time = trip_distance / trip_speed  # s\n        return trip_time\n\n    def calculate_trip_final_time(\n        self,\n        origin_location: tuple[float, float],\n        destin_location: tuple[float, float],\n        origin_timestamp: datetime.datetime,\n    ) -&gt; datetime.datetime:\n        \"\"\"\n        Calculate end time of a trip given an origin time, an origin location,\n        a destination location and a speed.\n\n        Args:\n            origin_location (tuple[float,float]): lon, lat of 1st point,\n                in decimal degrees.\n            destin_location (tuple[float,float]): lon, lat of 2nd point,\n                in decimal degrees.\n            origin_timestamp (datetime.datetime): start time of trip.\n\n        Returns:\n            datetime.datetime: end time of trip.\n        \"\"\"\n\n        trip_time = self.calculate_trip_time(origin_location, destin_location)  # s\n        return origin_timestamp + datetime.timedelta(seconds=trip_time)\n\n    def generate_stay_location(\n        self,\n        stay_type: str,\n        home_location: tuple[float, float],\n        work_location: tuple[float, float],\n        previous_location: tuple[float, float],\n        user_id: int,\n        date: datetime.date,\n        i: int,\n    ) -&gt; tuple[float, float]:\n        \"\"\"\n        Generate a random activity location within the bounding box limits based\n        on the activity type and previous activity locations.\n\n        Args:\n            stay_type (str): type of stay (\"home\", \"work\" or \"other\").\n            home_location (tuple[float,float]): coordinates of home location.\n            work_location (tuple[float,float]): coordinates of work location.\n            previous_location (tuple[float,float]): coordinates of previous\n                activity location.\n            user_id (int): agent identifier, used for random seed generation.\n            date (datetime.date): date, used for random seed generation.\n            i (int): activity position, used for random seed generation.\n\n        Returns:\n            tuple[float,float]: randomly generated activity location coordinates.\n        \"\"\"\n        if stay_type == \"home\":\n            location = home_location\n        elif stay_type == \"work\":\n            location = work_location\n        else:\n            location = self.generate_other_location(user_id, date, i, home_location, previous_location)\n        return location\n\n    def create_agent_activities_min_duration(\n        self,\n        user_id: int,\n        agent_stay_type_sequence: list[str],\n        home_location: tuple[float, float],\n        work_location: tuple[float, float],\n        date: datetime.date,\n        start_of_date: datetime.datetime,\n        end_of_date: datetime.datetime,\n    ) -&gt; list[Row]:\n        \"\"\"\n        Generate activities of the minimum duration following the specified agent\n        activity sequence for this agent and date.\n\n        Args:\n            user_id (int): agent identifier.\n            agent_stay_type_sequence (list[str]): list of generated stay types,\n                each represented by a string indicating the stay type.\n            home_location (tuple[float,float]): coordinates of home location.\n            work_location (tuple[float,float]): coordinates of work location.\n            date (datetime.date): date for activity sequence generation, used for\n                timestamps and random seed generation.\n            start_of_date (datetime.datetime): timestamp of current date at 00:00:00.\n            end_of_date (datetime.datetime): timestamp of current date at 23:59:59.\n\n        Returns:\n            list[Row]: list of generated activities and trips, each represented by a\n                spark row object with all its information.\n        \"\"\"\n        date_activities = []\n        previous_location = None\n        for i, stay_type in enumerate(agent_stay_type_sequence):\n            # activity location:\n            location = self.generate_stay_location(\n                stay_type, home_location, work_location, previous_location, user_id, date, i\n            )\n            # previous move (unless first stay)\n            if i != 0:\n                # move timestamps:\n                trip_initial_timestamp = stay_final_timestamp\n                trip_final_timestamp = self.calculate_trip_final_time(\n                    previous_location, location, trip_initial_timestamp\n                )\n                # add move:\n                date_activities.append(\n                    Row(\n                        user_id=user_id,\n                        activity_type=\"move\",\n                        stay_type=\"move\",\n                        longitude=float(\"nan\"),\n                        latitude=float(\"nan\"),\n                        initial_timestamp=trip_initial_timestamp,\n                        final_timestamp=trip_final_timestamp,\n                        year=date.year,\n                        month=date.month,\n                        day=date.day,\n                    )\n                )\n            # stay timestamps:\n            stay_initial_timestamp = start_of_date if i == 0 else trip_final_timestamp\n            stay_duration = self.generate_min_stay_duration(stay_type)\n            stay_final_timestamp = stay_initial_timestamp + datetime.timedelta(hours=stay_duration)\n\n            # add stay:\n            date_activities.append(\n                Row(\n                    user_id=user_id,\n                    activity_type=\"stay\",\n                    stay_type=stay_type,\n                    longitude=location[0],\n                    latitude=location[1],\n                    initial_timestamp=stay_initial_timestamp,\n                    final_timestamp=stay_final_timestamp,\n                    year=date.year,\n                    month=date.month,\n                    day=date.day,\n                )\n            )\n\n            previous_location = location\n\n        # after the iterations:\n        if not date_activities:  # 0 stays\n            condition_for_full_home = True\n        elif stay_final_timestamp &gt; end_of_date:  # too many stays\n            condition_for_full_home = True\n        else:\n            condition_for_full_home = False\n\n        if condition_for_full_home:  # simple \"only home\" diary\n            return [\n                Row(\n                    user_id=user_id,\n                    activity_type=\"stay\",\n                    stay_type=\"home\",\n                    longitude=home_location[0],\n                    latitude=home_location[1],\n                    initial_timestamp=start_of_date,\n                    final_timestamp=end_of_date,\n                    year=date.year,\n                    month=date.month,\n                    day=date.day,\n                )\n            ]\n        else:\n            return date_activities  # actual generated diary\n\n    def update_spark_row(self, row: Row, column_name: str, new_value: Any) -&gt; Row:\n        \"\"\"\n        Return an updated spark row object, changing the value of a column.\n\n        Args:\n            row (Row): input spark row.\n            column_name (str): name of column to modify.\n            new_value (Any): new value to assign.\n\n        Returns:\n            Row: modified spark row\n        \"\"\"\n        return Row(**{**row.asDict(), **{column_name: new_value}})\n\n    def adjust_activity_times(\n        self,\n        date_activities: list[Row],\n        remaining_time: float,\n        user_id: int,\n        date: datetime.date,\n        start_of_date: datetime.datetime,\n        end_of_date: datetime.datetime,\n    ):\n        \"\"\"\n        Modifies the \"date_activities\" list, changing the initial and\n        final timestamps of both stays and moves probablilistically in order to\n        generate stay durations different from the minimum and adjust the\n        durations of the activities to the 24h of the day.\n\n        Args:\n            date_activities (list[Row]): list of generated activities (stays and\n                moves) of the agent for the specified date. Each activity/trip is a\n                spark row object.\n            user_id (int): agent identifier.\n            date (datetime.date): date for activity sequence generation, used for\n                timestamps and random seed generation.\n            start_of_date (datetime.datetime): timestamp of current date at 00:00:00.\n            end_of_date (datetime.datetime): timestamp of current date at 23:59:59.\n        \"\"\"\n        current_timestamp = start_of_date\n        for i, activity_row in enumerate(date_activities):\n            if activity_row.activity_type == \"stay\":  # stay:\n                stay_type = activity_row.stay_type\n                old_stay_duration = (\n                    activity_row.final_timestamp - activity_row.initial_timestamp\n                ).total_seconds() / 3600.0\n                new_initial_timestamp = current_timestamp\n                if i == len(date_activities) - 1:\n                    new_final_timestamp = end_of_date\n                    remaining_time = 0.0\n                else:\n                    new_stay_duration = self.generate_stay_duration(user_id, date, i, stay_type, remaining_time)\n                    new_duration_td = datetime.timedelta(seconds=new_stay_duration * 3600.0)\n                    new_final_timestamp = new_initial_timestamp + new_duration_td\n                    remaining_time -= new_stay_duration - old_stay_duration\n            else:  # move:\n                old_move_duration = activity_row.final_timestamp - activity_row.initial_timestamp\n                new_initial_timestamp = current_timestamp\n                new_final_timestamp = new_initial_timestamp + old_move_duration\n\n            # common for all activities (stays and moves):\n            activity_row = self.update_spark_row(activity_row, \"initial_timestamp\", new_initial_timestamp)\n            activity_row = self.update_spark_row(activity_row, \"final_timestamp\", new_final_timestamp)\n            date_activities[i] = activity_row\n            current_timestamp = new_final_timestamp\n\n    def add_agent_date_activities(\n        self,\n        activities: list[Row],\n        user_id: int,\n        agent_stay_type_sequence: list[str],\n        home_location: tuple[float, float],\n        work_location: tuple[float, float],\n        date: datetime.date,\n        start_of_date: datetime.datetime,\n        end_of_date: datetime.datetime,\n    ):\n        \"\"\"\n        For a specific date and user, generate a sequence of activities probabilistically\n        according to the specified activity superset and the activity probabilities.\n        Firstly, assign to each of these activities the minimum duration considered for\n        that activity type. Trip times are based on Pythagorean distance and a specified\n        average speed.\n        If the sum of all minimum duration of the activities and the duration of the trips\n        is higher than the 24h of the day, then assign just one \"home\" activity to the\n        agent from 00:00:00 to 23:59:59.\n        Else, there will be a remaining time. E.g., the diary of an agent, after adding\n        up all trip durations and minimum activity durations may end at 20:34:57. There is\n        a remaining time to complete the full diary (23:59:59 - 20:34:57).\n        Adjust activity times probabilistically according to the maximum activity duration\n        and this remaining time, making the diary end at exactly 23:59:59.\n\n        Args:\n            activities (list[Row]): list of generated activities (stays and moves) for\n                the agent for all of the specified dates. Each activity is a spark\n                row object.\n            user_id (int): agent identifier.\n            agent_stay_type_sequence (list[str]): list of generated stay types,\n                each represented by a string indicating the stay type.\n            home_location (tuple[float,float]): coordinates of home location.\n            work_location (tuple[float,float]): coordinates of work location.\n            date (datetime.date): date for activity sequence generation, used for\n                timestamps and random seed generation.\n            start_of_date (datetime.datetime): timestamp of current date at 00:00:00.\n            end_of_date (datetime.datetime): timestamp of current date at 23:59:59.\n        \"\"\"\n        date_activities = self.create_agent_activities_min_duration(\n            user_id, agent_stay_type_sequence, home_location, work_location, date, start_of_date, end_of_date\n        )\n        remaining_time = (end_of_date - date_activities[-1].final_timestamp).total_seconds() / 3600.0\n\n        if remaining_time != 0:\n            self.adjust_activity_times(\n                date_activities,\n                remaining_time,\n                user_id,\n                date,\n                start_of_date,\n                end_of_date,\n            )\n        activities += date_activities\n\n    def add_date_activities(self, date: datetime.date, activities: list[Row]):\n        \"\"\"\n        Generate activity (stays and moves) rows for a specific date according to\n        parameters.\n\n        Args:\n            date (datetime.date): date for activity sequence generation, used for\n                timestamps and random seed generation.\n            activities (list[Row]): list of generated activities (stays and moves) for\n                the agent for all of the specified dates. Each activity is a spark\n                row object.\n        \"\"\"\n        # Start of date, end of date: datetime object generation\n        start_of_date = datetime.datetime.combine(date, datetime.time(0, 0, 0))\n        end_of_date = datetime.datetime.combine(date, datetime.time(23, 59, 59))\n        for user_id in range(self.number_of_users):\n            # generate user information:\n            agent_stay_type_sequence = self.generate_stay_type_sequence(user_id, date)\n            home_location = self.generate_home_location(user_id)\n            work_location = self.generate_work_location(user_id, home_location)\n            self.add_agent_date_activities(\n                activities,\n                user_id,\n                agent_stay_type_sequence,\n                home_location,\n                work_location,\n                date,\n                start_of_date,\n                end_of_date,\n            )\n\n    def generate_activities(self) -&gt; list[Row]:\n        \"\"\"\n        Generate activity and trip rows according to parameters.\n\n        Returns:\n            list[Row]: list of generated activities and trips for the agent for all\n                of the specified dates. Each activity/trip is a spark row object.\n        \"\"\"\n        activities = []\n        for date in self.date_range:\n            self.add_date_activities(date, activities)\n        return activities\n\n    def generate_lonlat_at_distance(self, lon1: float, lat1: float, d: float, seed: int) -&gt; tuple[float, float]:\n        \"\"\"\n        Given a point (lon, lat) and a distance, in meters, calculate a new random\n        point that is exactly at the specified distance of the provided lon, lat.\n\n        Args:\n            lon1 (float): longitude of point, specified in decimal degrees.\n            lat1 (float): latitude of point, specified in decimal degrees.\n            d (float): distance, in meters.\n            seed (int): random seed integer.\n\n        Returns:\n            tuple[float, float]: coordinates of randomly generated point.\n        \"\"\"\n        r = 6_371_000  # Radius of earth in meters.\n\n        d_x = Random(seed).uniform(0, d)\n        d_y = sqrt(d**2 - d_x**2)\n\n        # firstly, convert lat to radians for later\n        lat1_radians = lat1 * pi / 180.0\n\n        # how many meters correspond to one degree of latitude?\n        deg_to_meters = r * pi / 180  # aprox. 111111 meters\n        # thus, the northwards displacement, in degrees of latitude is:\n        north_delta = d_y / deg_to_meters\n\n        # but one degree of longitude does not always correspond to the\n        # same distance... depends on the latitude at where you are!\n        parallel_radius = abs(r * cos(lat1_radians))\n        deg_to_meters = parallel_radius * pi / 180  # variable\n        # thus, the eastwards displacement, in degrees of longitude is:\n        east_delta = d_x / deg_to_meters\n\n        final_lon = lon1 + east_delta * Random(seed).choice([-1, 1])\n        final_lat = lat1 + north_delta * Random(seed).choice([-1, 1])\n\n        return (final_lon, final_lat)\n\n    def generate_home_location(self, agent_id: int) -&gt; tuple[float, float]:\n        \"\"\"\n        Generate random home location based on bounding box limits.\n\n        Args:\n            agent_id (int): identifier of agent, used for random seed generation.\n\n        Returns:\n            tuple[float,float]: coordinates of generated home location.\n        \"\"\"\n        seed_lon = self.random_seed_number_generator(1, agent_id)\n        seed_lat = self.random_seed_number_generator(2, agent_id)\n        hlon = Random(seed_lon).uniform(self.longitude_min, self.longitude_max)\n        hlat = Random(seed_lat).uniform(self.latitude_min, self.latitude_max)\n        return (hlon, hlat)\n\n    def generate_work_location(\n        self, agent_id: int, home_location: tuple[float, float], seed: int = 4\n    ) -&gt; tuple[float, float]:\n        \"\"\"\n        Generate random work location based on home location and maximum distance to\n        home. If the work location falls outside of bounding box limits, try again.\n\n        Args:\n            agent_id (int): identifier of agent, used for random seed generation.\n            home_location (tuple[float,float]): coordinates of home location.\n            seed (int, optional): random seed integer. Defaults to 4.\n\n        Returns:\n            tuple[float,float]: coordinates of generated work location.\n        \"\"\"\n        seed_distance = self.random_seed_number_generator(seed - 1, agent_id)\n        random_distance = Random(seed_distance).uniform(self.home_work_distance_min, self.home_work_distance_max)\n        hlon, hlat = home_location\n        seed_coords = self.random_seed_number_generator(seed, agent_id)\n        wlon, wlat = self.generate_lonlat_at_distance(hlon, hlat, random_distance, seed_coords)\n\n        if not (self.longitude_min &lt; wlon &lt; self.longitude_max) or not (\n            self.latitude_min &lt; wlat &lt; self.latitude_max\n        ):  # outside limits\n            seed += 1\n            wlon, wlat = self.generate_work_location(agent_id, home_location, seed=seed)\n\n        return (wlon, wlat)\n\n    def generate_other_location(\n        self,\n        agent_id: int,\n        date: datetime.date,\n        activity_number: int,\n        home_location: tuple[float, float],\n        previous_location: tuple[float, float],\n        seed: int = 6,\n    ) -&gt; tuple[float, float]:\n        \"\"\"\n        Generate other activity location based on previous location and maximum distance\n        to previous location. If there is no previous location (this is the first\n        activity of the day), then the home location is considered as previous location.\n        If the location falls outside of bounding box limits, try again.\n\n        Args:\n            agent_id (int): identifier of agent, used for random seed generation.\n            date (datetime.date): date, used for random seed generation.\n            activity_number (int): act position, used for random seed generation.\n            home_location (tuple[float,float]): coordinates of home location.\n            previous_location (tuple[float,float]): coordinates of previous location.\n            seed (int, optional): random seed integer. Defaults to 6.\n\n        Returns:\n            tuple[float,float]: coordinates of generated location.\n        \"\"\"\n        seed_distance = self.random_seed_number_generator(seed - 1, agent_id)\n        random_distance = Random(seed_distance).uniform(self.other_distance_min, self.other_distance_max)\n        if previous_location is None:\n            plon, plat = home_location\n        else:\n            plon, plat = previous_location\n\n        seed_coords = self.random_seed_number_generator(seed, agent_id)\n        olon, olat = self.generate_lonlat_at_distance(plon, plat, random_distance, seed_coords)\n        if not (self.longitude_min &lt; olon &lt; self.longitude_max) or not (\n            self.latitude_min &lt; olat &lt; self.latitude_max\n        ):  # outside limits\n            seed += 1\n            olon, olat = self.generate_other_location(\n                agent_id, date, activity_number, home_location, previous_location, seed=seed\n            )\n\n        return (olon, olat)\n\n    def generate_stay_duration(\n        self, agent_id: int, date: datetime.date, i: int, stay_type: str, remaining_time: float\n    ) -&gt; float:\n        \"\"\"\n        Generate stay duration probabilistically based on activity type\n        abd remaining time.\n\n        Args:\n            agent_id (int): identifier of agent, used for random seed generation.\n            date (datetime.date): date, used for random seed generation.\n            i (int): activity position, used for random seed generation.\n            stay_type (str): type of stay. Shall be \"home\", \"work\" or \"other\".\n            remaining_time (float): same units as durations.\n\n        Returns:\n            float: generated activity duration.\n        \"\"\"\n        if stay_type == \"home\":\n            min_duration = self.home_duration_min\n            max_duration = self.home_duration_max\n        elif stay_type == \"work\":\n            min_duration = self.work_duration_min\n            max_duration = self.work_duration_max\n        elif stay_type == \"other\":\n            min_duration = self.other_duration_min\n            max_duration = self.other_duration_max\n        else:\n            raise ValueError\n        seed = self.random_seed_number_generator(7, agent_id, date, i)\n        max_value = min(max_duration, min_duration + remaining_time)\n        return Random(seed).uniform(min_duration, max_value)\n\n    def generate_min_stay_duration(self, stay_type: str) -&gt; float:\n        \"\"\"\n        Generate minimum stay duration based on stay type specifications.\n\n        Args:\n            stay_type (str): type of stay. Shall be \"home\", \"work\" or \"other\".\n\n        Returns:\n            float: minimum stay duration.\n        \"\"\"\n        if stay_type == \"home\":\n            return self.home_duration_min\n        elif stay_type == \"work\":\n            return self.work_duration_min\n        elif stay_type == \"other\":\n            return self.other_duration_min\n        else:\n            raise ValueError\n\n    def remove_consecutive_stay_types(self, stay_sequence_list: list[str], stay_types_to_group: set[str]) -&gt; list[str]:\n        \"\"\"\n        Generate new list replacing consecutive stays of the same type by\n        a unique stay as long as the stay type is contained in the\n        \"stay_types_to_group\" list.\n\n        Args:\n            stay_sequence_list (list[str]): input stay type list.\n            stay_types_to_group (set[str]): stay types to group.\n\n        Returns:\n            list[str]: output stay sequence list.\n        \"\"\"\n        new_stay_sequence_list = []\n        previous_stay = None\n        for stay in stay_sequence_list:\n            if stay == previous_stay and stay in stay_types_to_group:\n                pass\n            else:\n                new_stay_sequence_list.append(stay)\n            previous_stay = stay\n        return new_stay_sequence_list\n\n    def generate_stay_type_sequence(self, agent_id: int, date: datetime.date) -&gt; list[str]:\n        \"\"\"\n        Generate the sequence of stay types for an agent for a specific date\n        probabilistically based on the superset sequence and specified\n        probabilities.\n        Replace 'home'-'home' and 'work'-'work' sequences by just 'home' or\n        'work'.\n\n        Args:\n            agent_id (int): identifier of agent, used for random seed generation.\n            date (datetime.date): date for activity sequence generation, used for\n                random seed generation.\n\n        Returns:\n            list[str]: list of generated stay types, each represented by a string\n                indicating the stay type (e.g. \"home\", \"work\", \"other\").\n        \"\"\"\n        stay_type_sequence = []\n        for i, stay_type in enumerate(self.stay_sequence_superset):\n            stay_weight = self.stay_sequence_probabilities[i]\n            seed = self.random_seed_number_generator(0, agent_id, date, i)\n            if Random(seed).random() &lt; stay_weight:\n                stay_type_sequence.append(stay_type)\n        stay_type_sequence = self.remove_consecutive_stay_types(stay_type_sequence, {\"home\", \"work\"})\n        return stay_type_sequence\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.add_agent_date_activities","title":"<code>add_agent_date_activities(activities, user_id, agent_stay_type_sequence, home_location, work_location, date, start_of_date, end_of_date)</code>","text":"<p>For a specific date and user, generate a sequence of activities probabilistically according to the specified activity superset and the activity probabilities. Firstly, assign to each of these activities the minimum duration considered for that activity type. Trip times are based on Pythagorean distance and a specified average speed. If the sum of all minimum duration of the activities and the duration of the trips is higher than the 24h of the day, then assign just one \"home\" activity to the agent from 00:00:00 to 23:59:59. Else, there will be a remaining time. E.g., the diary of an agent, after adding up all trip durations and minimum activity durations may end at 20:34:57. There is a remaining time to complete the full diary (23:59:59 - 20:34:57). Adjust activity times probabilistically according to the maximum activity duration and this remaining time, making the diary end at exactly 23:59:59.</p> <p>Parameters:</p> Name Type Description Default <code>activities</code> <code>list[Row]</code> <p>list of generated activities (stays and moves) for the agent for all of the specified dates. Each activity is a spark row object.</p> required <code>user_id</code> <code>int</code> <p>agent identifier.</p> required <code>agent_stay_type_sequence</code> <code>list[str]</code> <p>list of generated stay types, each represented by a string indicating the stay type.</p> required <code>home_location</code> <code>tuple[float, float]</code> <p>coordinates of home location.</p> required <code>work_location</code> <code>tuple[float, float]</code> <p>coordinates of work location.</p> required <code>date</code> <code>date</code> <p>date for activity sequence generation, used for timestamps and random seed generation.</p> required <code>start_of_date</code> <code>datetime</code> <p>timestamp of current date at 00:00:00.</p> required <code>end_of_date</code> <code>datetime</code> <p>timestamp of current date at 23:59:59.</p> required Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def add_agent_date_activities(\n    self,\n    activities: list[Row],\n    user_id: int,\n    agent_stay_type_sequence: list[str],\n    home_location: tuple[float, float],\n    work_location: tuple[float, float],\n    date: datetime.date,\n    start_of_date: datetime.datetime,\n    end_of_date: datetime.datetime,\n):\n    \"\"\"\n    For a specific date and user, generate a sequence of activities probabilistically\n    according to the specified activity superset and the activity probabilities.\n    Firstly, assign to each of these activities the minimum duration considered for\n    that activity type. Trip times are based on Pythagorean distance and a specified\n    average speed.\n    If the sum of all minimum duration of the activities and the duration of the trips\n    is higher than the 24h of the day, then assign just one \"home\" activity to the\n    agent from 00:00:00 to 23:59:59.\n    Else, there will be a remaining time. E.g., the diary of an agent, after adding\n    up all trip durations and minimum activity durations may end at 20:34:57. There is\n    a remaining time to complete the full diary (23:59:59 - 20:34:57).\n    Adjust activity times probabilistically according to the maximum activity duration\n    and this remaining time, making the diary end at exactly 23:59:59.\n\n    Args:\n        activities (list[Row]): list of generated activities (stays and moves) for\n            the agent for all of the specified dates. Each activity is a spark\n            row object.\n        user_id (int): agent identifier.\n        agent_stay_type_sequence (list[str]): list of generated stay types,\n            each represented by a string indicating the stay type.\n        home_location (tuple[float,float]): coordinates of home location.\n        work_location (tuple[float,float]): coordinates of work location.\n        date (datetime.date): date for activity sequence generation, used for\n            timestamps and random seed generation.\n        start_of_date (datetime.datetime): timestamp of current date at 00:00:00.\n        end_of_date (datetime.datetime): timestamp of current date at 23:59:59.\n    \"\"\"\n    date_activities = self.create_agent_activities_min_duration(\n        user_id, agent_stay_type_sequence, home_location, work_location, date, start_of_date, end_of_date\n    )\n    remaining_time = (end_of_date - date_activities[-1].final_timestamp).total_seconds() / 3600.0\n\n    if remaining_time != 0:\n        self.adjust_activity_times(\n            date_activities,\n            remaining_time,\n            user_id,\n            date,\n            start_of_date,\n            end_of_date,\n        )\n    activities += date_activities\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.add_date_activities","title":"<code>add_date_activities(date, activities)</code>","text":"<p>Generate activity (stays and moves) rows for a specific date according to parameters.</p> <p>Parameters:</p> Name Type Description Default <code>date</code> <code>date</code> <p>date for activity sequence generation, used for timestamps and random seed generation.</p> required <code>activities</code> <code>list[Row]</code> <p>list of generated activities (stays and moves) for the agent for all of the specified dates. Each activity is a spark row object.</p> required Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def add_date_activities(self, date: datetime.date, activities: list[Row]):\n    \"\"\"\n    Generate activity (stays and moves) rows for a specific date according to\n    parameters.\n\n    Args:\n        date (datetime.date): date for activity sequence generation, used for\n            timestamps and random seed generation.\n        activities (list[Row]): list of generated activities (stays and moves) for\n            the agent for all of the specified dates. Each activity is a spark\n            row object.\n    \"\"\"\n    # Start of date, end of date: datetime object generation\n    start_of_date = datetime.datetime.combine(date, datetime.time(0, 0, 0))\n    end_of_date = datetime.datetime.combine(date, datetime.time(23, 59, 59))\n    for user_id in range(self.number_of_users):\n        # generate user information:\n        agent_stay_type_sequence = self.generate_stay_type_sequence(user_id, date)\n        home_location = self.generate_home_location(user_id)\n        work_location = self.generate_work_location(user_id, home_location)\n        self.add_agent_date_activities(\n            activities,\n            user_id,\n            agent_stay_type_sequence,\n            home_location,\n            work_location,\n            date,\n            start_of_date,\n            end_of_date,\n        )\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.adjust_activity_times","title":"<code>adjust_activity_times(date_activities, remaining_time, user_id, date, start_of_date, end_of_date)</code>","text":"<p>Modifies the \"date_activities\" list, changing the initial and final timestamps of both stays and moves probablilistically in order to generate stay durations different from the minimum and adjust the durations of the activities to the 24h of the day.</p> <p>Parameters:</p> Name Type Description Default <code>date_activities</code> <code>list[Row]</code> <p>list of generated activities (stays and moves) of the agent for the specified date. Each activity/trip is a spark row object.</p> required <code>user_id</code> <code>int</code> <p>agent identifier.</p> required <code>date</code> <code>date</code> <p>date for activity sequence generation, used for timestamps and random seed generation.</p> required <code>start_of_date</code> <code>datetime</code> <p>timestamp of current date at 00:00:00.</p> required <code>end_of_date</code> <code>datetime</code> <p>timestamp of current date at 23:59:59.</p> required Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def adjust_activity_times(\n    self,\n    date_activities: list[Row],\n    remaining_time: float,\n    user_id: int,\n    date: datetime.date,\n    start_of_date: datetime.datetime,\n    end_of_date: datetime.datetime,\n):\n    \"\"\"\n    Modifies the \"date_activities\" list, changing the initial and\n    final timestamps of both stays and moves probablilistically in order to\n    generate stay durations different from the minimum and adjust the\n    durations of the activities to the 24h of the day.\n\n    Args:\n        date_activities (list[Row]): list of generated activities (stays and\n            moves) of the agent for the specified date. Each activity/trip is a\n            spark row object.\n        user_id (int): agent identifier.\n        date (datetime.date): date for activity sequence generation, used for\n            timestamps and random seed generation.\n        start_of_date (datetime.datetime): timestamp of current date at 00:00:00.\n        end_of_date (datetime.datetime): timestamp of current date at 23:59:59.\n    \"\"\"\n    current_timestamp = start_of_date\n    for i, activity_row in enumerate(date_activities):\n        if activity_row.activity_type == \"stay\":  # stay:\n            stay_type = activity_row.stay_type\n            old_stay_duration = (\n                activity_row.final_timestamp - activity_row.initial_timestamp\n            ).total_seconds() / 3600.0\n            new_initial_timestamp = current_timestamp\n            if i == len(date_activities) - 1:\n                new_final_timestamp = end_of_date\n                remaining_time = 0.0\n            else:\n                new_stay_duration = self.generate_stay_duration(user_id, date, i, stay_type, remaining_time)\n                new_duration_td = datetime.timedelta(seconds=new_stay_duration * 3600.0)\n                new_final_timestamp = new_initial_timestamp + new_duration_td\n                remaining_time -= new_stay_duration - old_stay_duration\n        else:  # move:\n            old_move_duration = activity_row.final_timestamp - activity_row.initial_timestamp\n            new_initial_timestamp = current_timestamp\n            new_final_timestamp = new_initial_timestamp + old_move_duration\n\n        # common for all activities (stays and moves):\n        activity_row = self.update_spark_row(activity_row, \"initial_timestamp\", new_initial_timestamp)\n        activity_row = self.update_spark_row(activity_row, \"final_timestamp\", new_final_timestamp)\n        date_activities[i] = activity_row\n        current_timestamp = new_final_timestamp\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.calculate_trip_final_time","title":"<code>calculate_trip_final_time(origin_location, destin_location, origin_timestamp)</code>","text":"<p>Calculate end time of a trip given an origin time, an origin location, a destination location and a speed.</p> <p>Parameters:</p> Name Type Description Default <code>origin_location</code> <code>tuple[float, float]</code> <p>lon, lat of 1st point, in decimal degrees.</p> required <code>destin_location</code> <code>tuple[float, float]</code> <p>lon, lat of 2nd point, in decimal degrees.</p> required <code>origin_timestamp</code> <code>datetime</code> <p>start time of trip.</p> required <p>Returns:</p> Type Description <code>datetime</code> <p>datetime.datetime: end time of trip.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def calculate_trip_final_time(\n    self,\n    origin_location: tuple[float, float],\n    destin_location: tuple[float, float],\n    origin_timestamp: datetime.datetime,\n) -&gt; datetime.datetime:\n    \"\"\"\n    Calculate end time of a trip given an origin time, an origin location,\n    a destination location and a speed.\n\n    Args:\n        origin_location (tuple[float,float]): lon, lat of 1st point,\n            in decimal degrees.\n        destin_location (tuple[float,float]): lon, lat of 2nd point,\n            in decimal degrees.\n        origin_timestamp (datetime.datetime): start time of trip.\n\n    Returns:\n        datetime.datetime: end time of trip.\n    \"\"\"\n\n    trip_time = self.calculate_trip_time(origin_location, destin_location)  # s\n    return origin_timestamp + datetime.timedelta(seconds=trip_time)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.calculate_trip_time","title":"<code>calculate_trip_time(o_location, d_location)</code>","text":"<p>Calculate trip time given an origin location and a destination location, according to the specified trip speed.</p> <p>Parameters:</p> Name Type Description Default <code>o_location</code> <code>tuple[float, float]</code> <p>lon, lat of 1st point, in decimal degrees.</p> required <code>d_location</code> <code>tuple[float, float]</code> <p>lon, lat of 2nd point, in decimal degrees.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>trip time, in seconds.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def calculate_trip_time(self, o_location: tuple[float, float], d_location: tuple[float, float]) -&gt; float:\n    \"\"\"\n    Calculate trip time given an origin location and a destination\n    location, according to the specified trip speed.\n\n    Args:\n        o_location (tuple[float,float]): lon, lat of 1st point,\n            in decimal degrees.\n        d_location (tuple[float,float]): lon, lat of 2nd point,\n            in decimal degrees.\n\n    Returns:\n        float: trip time, in seconds.\n    \"\"\"\n    trip_distance = self.haversine(o_location[0], o_location[1], d_location[0], d_location[1])  # m\n    trip_speed = self.displacement_speed  # m/s\n    trip_time = trip_distance / trip_speed  # s\n    return trip_time\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.create_agent_activities_min_duration","title":"<code>create_agent_activities_min_duration(user_id, agent_stay_type_sequence, home_location, work_location, date, start_of_date, end_of_date)</code>","text":"<p>Generate activities of the minimum duration following the specified agent activity sequence for this agent and date.</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>int</code> <p>agent identifier.</p> required <code>agent_stay_type_sequence</code> <code>list[str]</code> <p>list of generated stay types, each represented by a string indicating the stay type.</p> required <code>home_location</code> <code>tuple[float, float]</code> <p>coordinates of home location.</p> required <code>work_location</code> <code>tuple[float, float]</code> <p>coordinates of work location.</p> required <code>date</code> <code>date</code> <p>date for activity sequence generation, used for timestamps and random seed generation.</p> required <code>start_of_date</code> <code>datetime</code> <p>timestamp of current date at 00:00:00.</p> required <code>end_of_date</code> <code>datetime</code> <p>timestamp of current date at 23:59:59.</p> required <p>Returns:</p> Type Description <code>list[Row]</code> <p>list[Row]: list of generated activities and trips, each represented by a spark row object with all its information.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def create_agent_activities_min_duration(\n    self,\n    user_id: int,\n    agent_stay_type_sequence: list[str],\n    home_location: tuple[float, float],\n    work_location: tuple[float, float],\n    date: datetime.date,\n    start_of_date: datetime.datetime,\n    end_of_date: datetime.datetime,\n) -&gt; list[Row]:\n    \"\"\"\n    Generate activities of the minimum duration following the specified agent\n    activity sequence for this agent and date.\n\n    Args:\n        user_id (int): agent identifier.\n        agent_stay_type_sequence (list[str]): list of generated stay types,\n            each represented by a string indicating the stay type.\n        home_location (tuple[float,float]): coordinates of home location.\n        work_location (tuple[float,float]): coordinates of work location.\n        date (datetime.date): date for activity sequence generation, used for\n            timestamps and random seed generation.\n        start_of_date (datetime.datetime): timestamp of current date at 00:00:00.\n        end_of_date (datetime.datetime): timestamp of current date at 23:59:59.\n\n    Returns:\n        list[Row]: list of generated activities and trips, each represented by a\n            spark row object with all its information.\n    \"\"\"\n    date_activities = []\n    previous_location = None\n    for i, stay_type in enumerate(agent_stay_type_sequence):\n        # activity location:\n        location = self.generate_stay_location(\n            stay_type, home_location, work_location, previous_location, user_id, date, i\n        )\n        # previous move (unless first stay)\n        if i != 0:\n            # move timestamps:\n            trip_initial_timestamp = stay_final_timestamp\n            trip_final_timestamp = self.calculate_trip_final_time(\n                previous_location, location, trip_initial_timestamp\n            )\n            # add move:\n            date_activities.append(\n                Row(\n                    user_id=user_id,\n                    activity_type=\"move\",\n                    stay_type=\"move\",\n                    longitude=float(\"nan\"),\n                    latitude=float(\"nan\"),\n                    initial_timestamp=trip_initial_timestamp,\n                    final_timestamp=trip_final_timestamp,\n                    year=date.year,\n                    month=date.month,\n                    day=date.day,\n                )\n            )\n        # stay timestamps:\n        stay_initial_timestamp = start_of_date if i == 0 else trip_final_timestamp\n        stay_duration = self.generate_min_stay_duration(stay_type)\n        stay_final_timestamp = stay_initial_timestamp + datetime.timedelta(hours=stay_duration)\n\n        # add stay:\n        date_activities.append(\n            Row(\n                user_id=user_id,\n                activity_type=\"stay\",\n                stay_type=stay_type,\n                longitude=location[0],\n                latitude=location[1],\n                initial_timestamp=stay_initial_timestamp,\n                final_timestamp=stay_final_timestamp,\n                year=date.year,\n                month=date.month,\n                day=date.day,\n            )\n        )\n\n        previous_location = location\n\n    # after the iterations:\n    if not date_activities:  # 0 stays\n        condition_for_full_home = True\n    elif stay_final_timestamp &gt; end_of_date:  # too many stays\n        condition_for_full_home = True\n    else:\n        condition_for_full_home = False\n\n    if condition_for_full_home:  # simple \"only home\" diary\n        return [\n            Row(\n                user_id=user_id,\n                activity_type=\"stay\",\n                stay_type=\"home\",\n                longitude=home_location[0],\n                latitude=home_location[1],\n                initial_timestamp=start_of_date,\n                final_timestamp=end_of_date,\n                year=date.year,\n                month=date.month,\n                day=date.day,\n            )\n        ]\n    else:\n        return date_activities  # actual generated diary\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_activities","title":"<code>generate_activities()</code>","text":"<p>Generate activity and trip rows according to parameters.</p> <p>Returns:</p> Type Description <code>list[Row]</code> <p>list[Row]: list of generated activities and trips for the agent for all of the specified dates. Each activity/trip is a spark row object.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_activities(self) -&gt; list[Row]:\n    \"\"\"\n    Generate activity and trip rows according to parameters.\n\n    Returns:\n        list[Row]: list of generated activities and trips for the agent for all\n            of the specified dates. Each activity/trip is a spark row object.\n    \"\"\"\n    activities = []\n    for date in self.date_range:\n        self.add_date_activities(date, activities)\n    return activities\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_home_location","title":"<code>generate_home_location(agent_id)</code>","text":"<p>Generate random home location based on bounding box limits.</p> <p>Parameters:</p> Name Type Description Default <code>agent_id</code> <code>int</code> <p>identifier of agent, used for random seed generation.</p> required <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>tuple[float,float]: coordinates of generated home location.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_home_location(self, agent_id: int) -&gt; tuple[float, float]:\n    \"\"\"\n    Generate random home location based on bounding box limits.\n\n    Args:\n        agent_id (int): identifier of agent, used for random seed generation.\n\n    Returns:\n        tuple[float,float]: coordinates of generated home location.\n    \"\"\"\n    seed_lon = self.random_seed_number_generator(1, agent_id)\n    seed_lat = self.random_seed_number_generator(2, agent_id)\n    hlon = Random(seed_lon).uniform(self.longitude_min, self.longitude_max)\n    hlat = Random(seed_lat).uniform(self.latitude_min, self.latitude_max)\n    return (hlon, hlat)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_lonlat_at_distance","title":"<code>generate_lonlat_at_distance(lon1, lat1, d, seed)</code>","text":"<p>Given a point (lon, lat) and a distance, in meters, calculate a new random point that is exactly at the specified distance of the provided lon, lat.</p> <p>Parameters:</p> Name Type Description Default <code>lon1</code> <code>float</code> <p>longitude of point, specified in decimal degrees.</p> required <code>lat1</code> <code>float</code> <p>latitude of point, specified in decimal degrees.</p> required <code>d</code> <code>float</code> <p>distance, in meters.</p> required <code>seed</code> <code>int</code> <p>random seed integer.</p> required <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>tuple[float, float]: coordinates of randomly generated point.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_lonlat_at_distance(self, lon1: float, lat1: float, d: float, seed: int) -&gt; tuple[float, float]:\n    \"\"\"\n    Given a point (lon, lat) and a distance, in meters, calculate a new random\n    point that is exactly at the specified distance of the provided lon, lat.\n\n    Args:\n        lon1 (float): longitude of point, specified in decimal degrees.\n        lat1 (float): latitude of point, specified in decimal degrees.\n        d (float): distance, in meters.\n        seed (int): random seed integer.\n\n    Returns:\n        tuple[float, float]: coordinates of randomly generated point.\n    \"\"\"\n    r = 6_371_000  # Radius of earth in meters.\n\n    d_x = Random(seed).uniform(0, d)\n    d_y = sqrt(d**2 - d_x**2)\n\n    # firstly, convert lat to radians for later\n    lat1_radians = lat1 * pi / 180.0\n\n    # how many meters correspond to one degree of latitude?\n    deg_to_meters = r * pi / 180  # aprox. 111111 meters\n    # thus, the northwards displacement, in degrees of latitude is:\n    north_delta = d_y / deg_to_meters\n\n    # but one degree of longitude does not always correspond to the\n    # same distance... depends on the latitude at where you are!\n    parallel_radius = abs(r * cos(lat1_radians))\n    deg_to_meters = parallel_radius * pi / 180  # variable\n    # thus, the eastwards displacement, in degrees of longitude is:\n    east_delta = d_x / deg_to_meters\n\n    final_lon = lon1 + east_delta * Random(seed).choice([-1, 1])\n    final_lat = lat1 + north_delta * Random(seed).choice([-1, 1])\n\n    return (final_lon, final_lat)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_min_stay_duration","title":"<code>generate_min_stay_duration(stay_type)</code>","text":"<p>Generate minimum stay duration based on stay type specifications.</p> <p>Parameters:</p> Name Type Description Default <code>stay_type</code> <code>str</code> <p>type of stay. Shall be \"home\", \"work\" or \"other\".</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>minimum stay duration.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_min_stay_duration(self, stay_type: str) -&gt; float:\n    \"\"\"\n    Generate minimum stay duration based on stay type specifications.\n\n    Args:\n        stay_type (str): type of stay. Shall be \"home\", \"work\" or \"other\".\n\n    Returns:\n        float: minimum stay duration.\n    \"\"\"\n    if stay_type == \"home\":\n        return self.home_duration_min\n    elif stay_type == \"work\":\n        return self.work_duration_min\n    elif stay_type == \"other\":\n        return self.other_duration_min\n    else:\n        raise ValueError\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_other_location","title":"<code>generate_other_location(agent_id, date, activity_number, home_location, previous_location, seed=6)</code>","text":"<p>Generate other activity location based on previous location and maximum distance to previous location. If there is no previous location (this is the first activity of the day), then the home location is considered as previous location. If the location falls outside of bounding box limits, try again.</p> <p>Parameters:</p> Name Type Description Default <code>agent_id</code> <code>int</code> <p>identifier of agent, used for random seed generation.</p> required <code>date</code> <code>date</code> <p>date, used for random seed generation.</p> required <code>activity_number</code> <code>int</code> <p>act position, used for random seed generation.</p> required <code>home_location</code> <code>tuple[float, float]</code> <p>coordinates of home location.</p> required <code>previous_location</code> <code>tuple[float, float]</code> <p>coordinates of previous location.</p> required <code>seed</code> <code>int</code> <p>random seed integer. Defaults to 6.</p> <code>6</code> <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>tuple[float,float]: coordinates of generated location.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_other_location(\n    self,\n    agent_id: int,\n    date: datetime.date,\n    activity_number: int,\n    home_location: tuple[float, float],\n    previous_location: tuple[float, float],\n    seed: int = 6,\n) -&gt; tuple[float, float]:\n    \"\"\"\n    Generate other activity location based on previous location and maximum distance\n    to previous location. If there is no previous location (this is the first\n    activity of the day), then the home location is considered as previous location.\n    If the location falls outside of bounding box limits, try again.\n\n    Args:\n        agent_id (int): identifier of agent, used for random seed generation.\n        date (datetime.date): date, used for random seed generation.\n        activity_number (int): act position, used for random seed generation.\n        home_location (tuple[float,float]): coordinates of home location.\n        previous_location (tuple[float,float]): coordinates of previous location.\n        seed (int, optional): random seed integer. Defaults to 6.\n\n    Returns:\n        tuple[float,float]: coordinates of generated location.\n    \"\"\"\n    seed_distance = self.random_seed_number_generator(seed - 1, agent_id)\n    random_distance = Random(seed_distance).uniform(self.other_distance_min, self.other_distance_max)\n    if previous_location is None:\n        plon, plat = home_location\n    else:\n        plon, plat = previous_location\n\n    seed_coords = self.random_seed_number_generator(seed, agent_id)\n    olon, olat = self.generate_lonlat_at_distance(plon, plat, random_distance, seed_coords)\n    if not (self.longitude_min &lt; olon &lt; self.longitude_max) or not (\n        self.latitude_min &lt; olat &lt; self.latitude_max\n    ):  # outside limits\n        seed += 1\n        olon, olat = self.generate_other_location(\n            agent_id, date, activity_number, home_location, previous_location, seed=seed\n        )\n\n    return (olon, olat)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_stay_duration","title":"<code>generate_stay_duration(agent_id, date, i, stay_type, remaining_time)</code>","text":"<p>Generate stay duration probabilistically based on activity type abd remaining time.</p> <p>Parameters:</p> Name Type Description Default <code>agent_id</code> <code>int</code> <p>identifier of agent, used for random seed generation.</p> required <code>date</code> <code>date</code> <p>date, used for random seed generation.</p> required <code>i</code> <code>int</code> <p>activity position, used for random seed generation.</p> required <code>stay_type</code> <code>str</code> <p>type of stay. Shall be \"home\", \"work\" or \"other\".</p> required <code>remaining_time</code> <code>float</code> <p>same units as durations.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>generated activity duration.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_stay_duration(\n    self, agent_id: int, date: datetime.date, i: int, stay_type: str, remaining_time: float\n) -&gt; float:\n    \"\"\"\n    Generate stay duration probabilistically based on activity type\n    abd remaining time.\n\n    Args:\n        agent_id (int): identifier of agent, used for random seed generation.\n        date (datetime.date): date, used for random seed generation.\n        i (int): activity position, used for random seed generation.\n        stay_type (str): type of stay. Shall be \"home\", \"work\" or \"other\".\n        remaining_time (float): same units as durations.\n\n    Returns:\n        float: generated activity duration.\n    \"\"\"\n    if stay_type == \"home\":\n        min_duration = self.home_duration_min\n        max_duration = self.home_duration_max\n    elif stay_type == \"work\":\n        min_duration = self.work_duration_min\n        max_duration = self.work_duration_max\n    elif stay_type == \"other\":\n        min_duration = self.other_duration_min\n        max_duration = self.other_duration_max\n    else:\n        raise ValueError\n    seed = self.random_seed_number_generator(7, agent_id, date, i)\n    max_value = min(max_duration, min_duration + remaining_time)\n    return Random(seed).uniform(min_duration, max_value)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_stay_location","title":"<code>generate_stay_location(stay_type, home_location, work_location, previous_location, user_id, date, i)</code>","text":"<p>Generate a random activity location within the bounding box limits based on the activity type and previous activity locations.</p> <p>Parameters:</p> Name Type Description Default <code>stay_type</code> <code>str</code> <p>type of stay (\"home\", \"work\" or \"other\").</p> required <code>home_location</code> <code>tuple[float, float]</code> <p>coordinates of home location.</p> required <code>work_location</code> <code>tuple[float, float]</code> <p>coordinates of work location.</p> required <code>previous_location</code> <code>tuple[float, float]</code> <p>coordinates of previous activity location.</p> required <code>user_id</code> <code>int</code> <p>agent identifier, used for random seed generation.</p> required <code>date</code> <code>date</code> <p>date, used for random seed generation.</p> required <code>i</code> <code>int</code> <p>activity position, used for random seed generation.</p> required <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>tuple[float,float]: randomly generated activity location coordinates.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_stay_location(\n    self,\n    stay_type: str,\n    home_location: tuple[float, float],\n    work_location: tuple[float, float],\n    previous_location: tuple[float, float],\n    user_id: int,\n    date: datetime.date,\n    i: int,\n) -&gt; tuple[float, float]:\n    \"\"\"\n    Generate a random activity location within the bounding box limits based\n    on the activity type and previous activity locations.\n\n    Args:\n        stay_type (str): type of stay (\"home\", \"work\" or \"other\").\n        home_location (tuple[float,float]): coordinates of home location.\n        work_location (tuple[float,float]): coordinates of work location.\n        previous_location (tuple[float,float]): coordinates of previous\n            activity location.\n        user_id (int): agent identifier, used for random seed generation.\n        date (datetime.date): date, used for random seed generation.\n        i (int): activity position, used for random seed generation.\n\n    Returns:\n        tuple[float,float]: randomly generated activity location coordinates.\n    \"\"\"\n    if stay_type == \"home\":\n        location = home_location\n    elif stay_type == \"work\":\n        location = work_location\n    else:\n        location = self.generate_other_location(user_id, date, i, home_location, previous_location)\n    return location\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_stay_type_sequence","title":"<code>generate_stay_type_sequence(agent_id, date)</code>","text":"<p>Generate the sequence of stay types for an agent for a specific date probabilistically based on the superset sequence and specified probabilities. Replace 'home'-'home' and 'work'-'work' sequences by just 'home' or 'work'.</p> <p>Parameters:</p> Name Type Description Default <code>agent_id</code> <code>int</code> <p>identifier of agent, used for random seed generation.</p> required <code>date</code> <code>date</code> <p>date for activity sequence generation, used for random seed generation.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: list of generated stay types, each represented by a string indicating the stay type (e.g. \"home\", \"work\", \"other\").</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_stay_type_sequence(self, agent_id: int, date: datetime.date) -&gt; list[str]:\n    \"\"\"\n    Generate the sequence of stay types for an agent for a specific date\n    probabilistically based on the superset sequence and specified\n    probabilities.\n    Replace 'home'-'home' and 'work'-'work' sequences by just 'home' or\n    'work'.\n\n    Args:\n        agent_id (int): identifier of agent, used for random seed generation.\n        date (datetime.date): date for activity sequence generation, used for\n            random seed generation.\n\n    Returns:\n        list[str]: list of generated stay types, each represented by a string\n            indicating the stay type (e.g. \"home\", \"work\", \"other\").\n    \"\"\"\n    stay_type_sequence = []\n    for i, stay_type in enumerate(self.stay_sequence_superset):\n        stay_weight = self.stay_sequence_probabilities[i]\n        seed = self.random_seed_number_generator(0, agent_id, date, i)\n        if Random(seed).random() &lt; stay_weight:\n            stay_type_sequence.append(stay_type)\n    stay_type_sequence = self.remove_consecutive_stay_types(stay_type_sequence, {\"home\", \"work\"})\n    return stay_type_sequence\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_work_location","title":"<code>generate_work_location(agent_id, home_location, seed=4)</code>","text":"<p>Generate random work location based on home location and maximum distance to home. If the work location falls outside of bounding box limits, try again.</p> <p>Parameters:</p> Name Type Description Default <code>agent_id</code> <code>int</code> <p>identifier of agent, used for random seed generation.</p> required <code>home_location</code> <code>tuple[float, float]</code> <p>coordinates of home location.</p> required <code>seed</code> <code>int</code> <p>random seed integer. Defaults to 4.</p> <code>4</code> <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>tuple[float,float]: coordinates of generated work location.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_work_location(\n    self, agent_id: int, home_location: tuple[float, float], seed: int = 4\n) -&gt; tuple[float, float]:\n    \"\"\"\n    Generate random work location based on home location and maximum distance to\n    home. If the work location falls outside of bounding box limits, try again.\n\n    Args:\n        agent_id (int): identifier of agent, used for random seed generation.\n        home_location (tuple[float,float]): coordinates of home location.\n        seed (int, optional): random seed integer. Defaults to 4.\n\n    Returns:\n        tuple[float,float]: coordinates of generated work location.\n    \"\"\"\n    seed_distance = self.random_seed_number_generator(seed - 1, agent_id)\n    random_distance = Random(seed_distance).uniform(self.home_work_distance_min, self.home_work_distance_max)\n    hlon, hlat = home_location\n    seed_coords = self.random_seed_number_generator(seed, agent_id)\n    wlon, wlat = self.generate_lonlat_at_distance(hlon, hlat, random_distance, seed_coords)\n\n    if not (self.longitude_min &lt; wlon &lt; self.longitude_max) or not (\n        self.latitude_min &lt; wlat &lt; self.latitude_max\n    ):  # outside limits\n        seed += 1\n        wlon, wlat = self.generate_work_location(agent_id, home_location, seed=seed)\n\n    return (wlon, wlat)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.haversine","title":"<code>haversine(lon1, lat1, lon2, lat2)</code>","text":"<p>Calculate the haversine distance in meters between two points.</p> <p>Parameters:</p> Name Type Description Default <code>lon1</code> <code>float</code> <p>longitude of first point, in decimal degrees.</p> required <code>lat1</code> <code>float</code> <p>latitude of first point, in decimal degrees.</p> required <code>lon2</code> <code>float</code> <p>longitude of second point, in decimal degrees.</p> required <code>lat2</code> <code>float</code> <p>latitude of second point, in decimal degrees.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>distance between both points, in meters.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def haversine(self, lon1: float, lat1: float, lon2: float, lat2: float) -&gt; float:\n    \"\"\"\n    Calculate the haversine distance in meters between two points.\n\n    Args:\n        lon1 (float): longitude of first point, in decimal degrees.\n        lat1 (float): latitude of first point, in decimal degrees.\n        lon2 (float): longitude of second point, in decimal degrees.\n        lat2 (float): latitude of second point, in decimal degrees.\n\n    Returns:\n        float: distance between both points, in meters.\n    \"\"\"\n    r = 6_371_000  # Radius of earth in meters.\n\n    # convert decimal degrees to radians\n    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n\n    # haversine formula\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n    a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n    c = 2 * asin(sqrt(a))\n    return c * r\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.random_seed_number_generator","title":"<code>random_seed_number_generator(base_seed, agent_id=None, date=None, i=None)</code>","text":"<p>Generate random seed integer based on provided arguments.</p> <p>Parameters:</p> Name Type Description Default <code>base_seed</code> <code>int</code> <p>base integer for operations.</p> required <code>agent_id</code> <code>int</code> <p>agent identifier. Defaults to None.</p> <code>None</code> <code>date</code> <code>date</code> <p>date. Defaults to None.</p> <code>None</code> <code>i</code> <code>int</code> <p>position integer. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>generated random seed integer.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def random_seed_number_generator(\n    self, base_seed: int, agent_id: int = None, date: datetime.date = None, i: int = None\n) -&gt; int:\n    \"\"\"\n    Generate random seed integer based on provided arguments.\n\n    Args:\n        base_seed (int): base integer for operations.\n        agent_id (int, optional): agent identifier. Defaults to None.\n        date (datetime.date, optional): date. Defaults to None.\n        i (int, optional): position integer. Defaults to None.\n\n    Returns:\n        int: generated random seed integer.\n    \"\"\"\n    seed = base_seed\n    if agent_id is not None:\n        seed += int(agent_id) * 100\n    if date is not None:\n        start_datetime = datetime.datetime.combine(date, datetime.time(0))\n        seed += int(start_datetime.timestamp())\n    if i is not None:\n        seed += i\n    return seed\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.remove_consecutive_stay_types","title":"<code>remove_consecutive_stay_types(stay_sequence_list, stay_types_to_group)</code>","text":"<p>Generate new list replacing consecutive stays of the same type by a unique stay as long as the stay type is contained in the \"stay_types_to_group\" list.</p> <p>Parameters:</p> Name Type Description Default <code>stay_sequence_list</code> <code>list[str]</code> <p>input stay type list.</p> required <code>stay_types_to_group</code> <code>set[str]</code> <p>stay types to group.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: output stay sequence list.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def remove_consecutive_stay_types(self, stay_sequence_list: list[str], stay_types_to_group: set[str]) -&gt; list[str]:\n    \"\"\"\n    Generate new list replacing consecutive stays of the same type by\n    a unique stay as long as the stay type is contained in the\n    \"stay_types_to_group\" list.\n\n    Args:\n        stay_sequence_list (list[str]): input stay type list.\n        stay_types_to_group (set[str]): stay types to group.\n\n    Returns:\n        list[str]: output stay sequence list.\n    \"\"\"\n    new_stay_sequence_list = []\n    previous_stay = None\n    for stay in stay_sequence_list:\n        if stay == previous_stay and stay in stay_types_to_group:\n            pass\n        else:\n            new_stay_sequence_list.append(stay)\n        previous_stay = stay\n    return new_stay_sequence_list\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.update_spark_row","title":"<code>update_spark_row(row, column_name, new_value)</code>","text":"<p>Return an updated spark row object, changing the value of a column.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Row</code> <p>input spark row.</p> required <code>column_name</code> <code>str</code> <p>name of column to modify.</p> required <code>new_value</code> <code>Any</code> <p>new value to assign.</p> required <p>Returns:</p> Name Type Description <code>Row</code> <code>Row</code> <p>modified spark row</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def update_spark_row(self, row: Row, column_name: str, new_value: Any) -&gt; Row:\n    \"\"\"\n    Return an updated spark row object, changing the value of a column.\n\n    Args:\n        row (Row): input spark row.\n        column_name (str): name of column to modify.\n        new_value (Any): new value to assign.\n\n    Returns:\n        Row: modified spark row\n    \"\"\"\n    return Row(**{**row.asDict(), **{column_name: new_value}})\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/","title":"synthetic_events","text":"<p>This module contains the SyntheticEvents class, which is responsible for generating the synthetic event data.</p>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents","title":"<code>SyntheticEvents</code>","text":"<p>             Bases: <code>Component</code></p> <p>Class that generates the event synthetic data. It inherits from the Component abstract class.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>class SyntheticEvents(Component):\n    \"\"\"\n    Class that generates the event synthetic data. It inherits from the Component abstract class.\n    \"\"\"\n\n    COMPONENT_ID = \"SyntheticEvents\"\n\n    def __init__(self, general_config_path: str, component_config_path: str):\n        super().__init__(general_config_path=general_config_path, component_config_path=component_config_path)\n\n        self.seed = self.config.getint(self.COMPONENT_ID, \"seed\")\n        self.event_freq_stays = self.config.getint(self.COMPONENT_ID, \"event_freq_stays\")\n        self.event_freq_moves = self.config.getint(self.COMPONENT_ID, \"event_freq_moves\")\n        self.closest_cell_distance_max = self.config.getint(self.COMPONENT_ID, \"closest_cell_distance_max\")\n        self.closest_cell_distance_max_for_errors = self.config.getint(\n            self.COMPONENT_ID, \"closest_cell_distance_max_for_errors\"\n        )\n        self.error_location_probability = self.config.getfloat(self.COMPONENT_ID, \"error_location_probability\")\n        self.error_location_distance_min = self.config.getint(self.COMPONENT_ID, \"error_location_distance_min\")\n        self.error_location_distance_max = self.config.getint(self.COMPONENT_ID, \"error_location_distance_max\")\n        self.cartesian_crs = self.config.getint(self.COMPONENT_ID, \"cartesian_crs\")\n        self.error_cell_id_probability = self.config.getfloat(self.COMPONENT_ID, \"error_cell_id_probability\")\n        self.maximum_number_of_cells_for_event = self.config.getfloat(\n            self.COMPONENT_ID, \"maximum_number_of_cells_for_event\"\n        )\n\n        self.mcc = self.config.getint(self.COMPONENT_ID, \"mcc\")\n\n    def initalize_data_objects(self):\n\n        pop_diares_input_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"diaries_data_bronze\")\n        pop_diaries_bronze_event = BronzeSyntheticDiariesDataObject(\n            self.spark, pop_diares_input_path, partition_columns=[ColNames.year, ColNames.month, ColNames.day]\n        )\n\n        # Input for cell attributes\n        network_data_input_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"network_data_bronze\")\n\n        cell_locations_bronze = BronzeNetworkDataObject(\n            self.spark, network_data_input_path, partition_columns=[ColNames.year, ColNames.month, ColNames.day]\n        )\n\n        self.input_data_objects = {\n            BronzeSyntheticDiariesDataObject.ID: pop_diaries_bronze_event,\n            BronzeNetworkDataObject.ID: cell_locations_bronze,\n        }\n\n        output_records_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"event_data_bronze\")\n        bronze_event = BronzeEventDataObject(\n            self.spark, output_records_path, partition_columns=[ColNames.year, ColNames.month, ColNames.day]\n        )\n\n        self.output_data_objects = {BronzeEventDataObject.ID: bronze_event}\n\n    def transform(self):\n\n        pop_diaries_df = self.input_data_objects[BronzeSyntheticDiariesDataObject.ID].df\n        cells_df = self.input_data_objects[BronzeNetworkDataObject.ID].df.select(\n            ColNames.cell_id, ColNames.latitude, ColNames.longitude, ColNames.year, ColNames.month, ColNames.day\n        )\n\n        # Filtering stays to get the lat and lon of movement starting and end point\n        stays_df = pop_diaries_df.filter(F.col(ColNames.activity_type) == \"stay\")\n\n        move_events_df = self.generate_event_timestamps_for_moves(stays_df, self.event_freq_moves, self.cartesian_crs)\n        move_events_with_locations_df = self.generate_locations_for_moves(move_events_df, self.cartesian_crs)\n\n        stay_events_df = self.generate_event_timestamps_for_stays(stays_df, self.event_freq_stays, self.cartesian_crs)\n\n        generated_stays_and_moves = stay_events_df.union(move_events_with_locations_df)\n\n        # Add geometry column to cells\n        cells_df = cells_df.withColumn(\n            \"cell_geometry\",\n            STF.ST_Transform(\n                STC.ST_Point(F.col(ColNames.longitude), F.col(ColNames.latitude)),\n                F.lit(\"EPSG:4326\"),\n                F.lit(f\"EPSG:{self.cartesian_crs}\"),\n            ),\n        ).select(ColNames.cell_id, \"cell_geometry\")\n\n        # 1) From the clean records, sample records for location errors\n        sampled_records = generated_stays_and_moves.sample(self.error_location_probability, self.seed)\n        generated_stays_and_moves = generated_stays_and_moves.subtract(sampled_records)\n        generated_stays_and_moves = generated_stays_and_moves.withColumn(\n            ColNames.loc_error, F.lit(None).cast(FloatType())\n        )\n\n        records_with_location_errors = self.generate_location_errors(\n            sampled_records,\n            self.error_location_distance_max,\n            self.error_location_distance_min,\n            self.closest_cell_distance_max_for_errors,\n            self.cartesian_crs,\n            self.seed,\n        )\n\n        # 2) From the clean records, sample records for erroneous cell id creation\n        sampled_records = generated_stays_and_moves.sample(self.error_cell_id_probability, self.seed)\n        records_with_cell_id_errors = self.generate_records_with_non_existant_cell_ids(sampled_records, cells_df)\n\n        generated_stays_and_moves = generated_stays_and_moves.subtract(sampled_records)\n        generated_stays_and_moves = generated_stays_and_moves.withColumn(\n            \"closest_cell_distance_max\", F.lit(self.closest_cell_distance_max)\n        )\n\n        # 3) Link a cell id to each location\n        records_sdf = generated_stays_and_moves.union(records_with_location_errors)\n\n        records_sdf = self.add_cell_ids_to_locations(\n            records_sdf, cells_df, self.maximum_number_of_cells_for_event, self.seed\n        )\n\n        # 4) Continuing with the combined dataframe\n\n        records_sdf = records_sdf.union(records_with_cell_id_errors)\n\n        records_sdf = records_sdf.dropDuplicates([ColNames.user_id, ColNames.timestamp])\n\n        records_sdf = records_sdf.withColumn(\n            \"generated_geometry\",\n            STF.ST_Transform(\"generated_geometry\", F.lit(f\"EPSG:{self.cartesian_crs}\"), F.lit(\"EPSG:4326\")),\n        )\n\n        records_sdf = (\n            records_sdf.withColumn(ColNames.longitude, STF.ST_X(F.col(\"generated_geometry\")))\n            .withColumn(ColNames.latitude, STF.ST_Y(F.col(\"generated_geometry\")))\n            .drop(\"generated_geometry\")\n        )\n\n        # MCC and loc_error are added to the records\n        records_sdf = records_sdf.withColumn(ColNames.mcc, F.lit(self.mcc).cast(IntegerType()))\n\n        # records_sdf = self.calc_hashed_user_id(records_sdf)\n        records_sdf = records_sdf.withColumn(ColNames.year, F.year(F.col(ColNames.timestamp)))\n        records_sdf = records_sdf.withColumn(ColNames.month, F.month(F.col(ColNames.timestamp)))\n        records_sdf = records_sdf.withColumn(ColNames.day, F.dayofmonth(F.col(ColNames.timestamp)))\n\n        # Select bronze schema columns\n        columns = {field.name: F.col(field.name).cast(field.dataType) for field in BronzeEventDataObject.SCHEMA.fields}\n\n        records_sdf = records_sdf.withColumns(columns)\n\n        self.output_data_objects[BronzeEventDataObject.ID].df = records_sdf\n\n    @staticmethod\n    def generate_event_timestamps_for_moves(\n        stays_sdf: DataFrame, event_freq_moves: int, cartesian_crs: int\n    ) -&gt; DataFrame:\n        \"\"\"\n        Generates a DataFrame of event timestamps for movements between stays.\n\n        For each stay in the input DataFrame, this method generates a random number of timestamps equal to the time\n            difference between the end of the current stay and the start of the next stay, divided by the event\n            frequency for moves.\n\n        Args:\n            stays_sdf (pyspark.sql.DataFrame): A DataFrame of stays.\n            event_freq_moves (int): The frequency of events for movements.\n            cartesian_crs (int): The EPSG code of the Cartesian coordinate reference system to use for the geometries.\n\n        Returns:\n            pyspark.sql.DataFrame: A DataFrame of event timestamps for movements.\n        \"\"\"\n\n        # Since the rows with activity_type = movement don't have any locations in the population diaries,\n        # we select the stay points and start generating timestamps in between the start and end of the stay\n\n        stays_sdf = stays_sdf.withColumn(\n            ColNames.geometry,\n            STF.ST_Transform(\n                STC.ST_Point(F.col(ColNames.longitude), F.col(ColNames.latitude)),\n                F.lit(\"EPSG:4326\"),\n                F.lit(f\"EPSG:{cartesian_crs}\"),\n            ),\n        )\n\n        # Define the window specification\u00a0\u2022\n        window_spec = Window.partitionBy(ColNames.year, ColNames.month, ColNames.day, ColNames.user_id).orderBy(\n            ColNames.initial_timestamp\n        )\n\n        # Add columns for next stay's geometry and start timestamp using the lead function\n        stays_sdf = stays_sdf.withColumn(\n            \"next_stay_initial_timestamp\", F.lead(ColNames.initial_timestamp, 1).over(window_spec)\n        )\n\n        stays_sdf = stays_sdf.withColumn(\"next_stay_geometry\", F.lead(ColNames.geometry, 1).over(window_spec))\n\n        stays_sdf = stays_sdf.withColumn(\n            \"time_diff_seconds\",\n            F.unix_timestamp(\"next_stay_initial_timestamp\") - F.unix_timestamp(ColNames.final_timestamp),\n        )\n\n        # Calculate how many timestamps fit in the interval for the given frequency\n        stays_sdf = stays_sdf.withColumn(\n            \"timestamps_count\", (F.col(\"time_diff_seconds\") / event_freq_moves).cast(\"integer\")\n        )\n\n        # Generate random floats between 0 and 1 for, using timestamps_count\n        stays_sdf = stays_sdf.withColumn(\n            \"random_fraction_on_line\", F.expr(\"transform(sequence(1, timestamps_count), x -&gt; rand())\")\n        ).withColumn(\"random_fraction_on_line\", F.explode(F.col(\"random_fraction_on_line\")))\n\n        # Generate timestamps\n        stays_sdf = stays_sdf.withColumn(\n            \"offset_seconds\", F.col(\"random_fraction_on_line\") * F.col(\"time_diff_seconds\")\n        )\n        stays_sdf = stays_sdf.withColumn(\n            ColNames.timestamp, F.from_unixtime(F.unix_timestamp(ColNames.final_timestamp) + F.col(\"offset_seconds\"))\n        )\n\n        moves_sdf = stays_sdf.withColumn(ColNames.activity_type, F.lit(\"move\"))\n\n        # Keep only necessary columns\n        moves_sdf = moves_sdf.select(\n            ColNames.user_id,\n            ColNames.timestamp,\n            ColNames.geometry,\n            \"next_stay_geometry\",\n            \"random_fraction_on_line\",\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n        )\n\n        return moves_sdf\n\n    @staticmethod\n    def generate_event_timestamps_for_stays(\n        stays_df: DataFrame, event_freq_stays: int, cartesian_crs: int\n    ) -&gt; DataFrame:\n        \"\"\"\n        Generates a DataFrame of event timestamps for stays based on the event frequency for stays.\n\n        For each stay in the input DataFrame, this method calculates\n        the time difference between the initial and final timestamps of the stay.\n        It then generates a number of timestamps equal to this time difference divided by the event\n        frequency for stays. Each timestamp is associated with the location of the stay.\n\n        Args:\n            stays_df (pyspark.sql.DataFrame): A DataFrame of stays.\n            event_freq_stays (int): The frequency of events for stays.\n            cartesian_crs (int): The EPSG code of the Cartesian coordinate reference system to use for the geometries.\n        Returns:\n            pyspark.sql.DataFrame: A DataFrame of event timestamps for stays.\n        \"\"\"\n\n        stays_df = stays_df.withColumn(\n            \"time_diff_seconds\",\n            F.unix_timestamp(F.col(ColNames.final_timestamp)) - F.unix_timestamp(F.col(ColNames.initial_timestamp)),\n        )\n\n        stays_df = stays_df.withColumn(\n            \"timestamps_count\", (F.col(\"time_diff_seconds\") / event_freq_stays).cast(\"integer\")\n        )\n\n        stays_df = stays_df.withColumn(\n            \"random_fraction_between_timestamps\", F.expr(\"transform(sequence(1, timestamps_count), x -&gt; rand())\")\n        )\n        stays_df = stays_df.withColumn(\n            \"random_fraction_between_timestamps\", F.explode(F.col(\"random_fraction_between_timestamps\"))\n        )\n        stays_df = stays_df.withColumn(\n            \"offset_seconds\", F.col(\"random_fraction_between_timestamps\") * F.col(\"time_diff_seconds\")\n        )\n        stays_df = stays_df.withColumn(\n            ColNames.timestamp, F.from_unixtime(F.unix_timestamp(ColNames.initial_timestamp) + F.col(\"offset_seconds\"))\n        )\n\n        stays_df = stays_df.withColumn(\n            \"generated_geometry\",\n            STF.ST_Transform(\n                STC.ST_Point(F.col(ColNames.longitude), F.col(ColNames.latitude)),\n                F.lit(\"EPSG:4326\"),\n                F.lit(f\"EPSG:{cartesian_crs}\"),\n            ),\n        )\n\n        stays_df = stays_df.select(\n            ColNames.user_id, ColNames.timestamp, \"generated_geometry\", ColNames.year, ColNames.month, ColNames.day\n        )\n\n        return stays_df\n\n    @staticmethod\n    def generate_locations_for_moves(event_timestamps_df: DataFrame, cartesian_crs: int) -&gt; DataFrame:\n        \"\"\"\n        Generates locations for moves based on the event timestamps dataframe.\n        Returns a dataframe, where for each move in the event timestamps dataframe\n        a geometry column is added, representing the location of the move.\n\n        Performs interpolation along the line between the starting move point (previous stay point)\n        and next move point (next stay point) using the randomly generated values in the column random_fraction_on_line.\n\n        Args:\n            event_timestamps_df (pyspark.sql.DataFrame): The event timestamps dataframe.\n\n        Returns:\n            pyspark.sql.DataFrame: The event timestamps dataframe with the added geometry column.\n        \"\"\"\n\n        moves_with_geometry = event_timestamps_df.withColumn(\n            \"line\", STF.ST_MakeLine(F.col(ColNames.geometry), F.col(\"next_stay_geometry\"))\n        )\n        moves_with_geometry = moves_with_geometry.withColumn(\n            \"generated_geometry\",\n            STF.ST_SetSRID(STF.ST_LineInterpolatePoint(F.col(\"line\"), F.col(\"random_fraction_on_line\")), cartesian_crs),\n        )\n\n        moves_with_geometry = moves_with_geometry.select(\n            ColNames.user_id, ColNames.timestamp, \"generated_geometry\", ColNames.year, ColNames.month, ColNames.day\n        )\n\n        return moves_with_geometry\n\n    @staticmethod\n    def add_cell_ids_to_locations(\n        events_with_locations_df: DataFrame, cells_df: DataFrame, max_n_of_cells: int, seed: int\n    ) -&gt; DataFrame:\n        \"\"\"\n        Links cell IDs to locations in the events DataFrame.\n\n        This method performs a spatial join between the events and cells DataFrames to add cell IDs to each event.\n        It first creates a buffer around each event location and finds cells that intersect with this buffer.\n        It then calculates the distance from each event location to the cell and ranks the cells based on this distance.\n        It keeps only the top 'max_n_of_cells' closest cells for each event.\n\n        The method also adds a random index to each event-cell pair and filters to keep only one pair per event,\n        randomly selecting one of the closest cells for each event.\n\n        Args:\n            events_with_locations_df (pyspark.sql.DataFrame): A DataFrame of events\n            cells_df (pyspark.sql.DataFrame): A DataFrame of cells\n            max_n_of_cells (int): The maximum number of closest cells to consider for each event.\n\n        Returns:\n            pyspark.sql.DataFrame: A DataFrame of events with cell IDs added.\n        \"\"\"\n        events_with_cells_sdf = events_with_locations_df.join(\n            cells_df[[ColNames.cell_id, \"cell_geometry\"]],\n            (\n                STP.ST_Intersects(\n                    STF.ST_Buffer(events_with_locations_df[\"generated_geometry\"], F.col(\"closest_cell_distance_max\")),\n                    cells_df[\"cell_geometry\"],\n                )\n            ),\n        ).withColumn(\n            \"distance_to_cell\",\n            STF.ST_Distance(events_with_locations_df[\"generated_geometry\"], cells_df[\"cell_geometry\"]),\n        )\n\n        # Selection of different cell ids for a given timestamp is random\n        window_spec = Window.partitionBy(\n            ColNames.year, ColNames.month, ColNames.day, ColNames.user_id, ColNames.timestamp\n        ).orderBy(F.col(\"distance_to_cell\"))\n\n        events_with_cells_sdf = events_with_cells_sdf.withColumn(\n            \"closest_cells_index\", F.row_number().over(window_spec)\n        ).filter(F.col(\"closest_cells_index\") &lt;= max_n_of_cells)\n\n        window_spec_random = Window.partitionBy(\n            ColNames.year, ColNames.month, ColNames.day, ColNames.user_id, ColNames.timestamp\n        ).orderBy(F.rand(seed))\n        events_with_cells_sdf = events_with_cells_sdf.withColumn(\n            \"random_cell_index\", F.row_number().over(window_spec_random)\n        ).filter(F.col(\"random_cell_index\") == 1)\n\n        records_sdf = events_with_cells_sdf.select(\n            ColNames.user_id,\n            ColNames.timestamp,\n            \"generated_geometry\",\n            ColNames.cell_id,\n            ColNames.loc_error,\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n        )\n\n        return records_sdf\n\n    @staticmethod\n    def generate_location_errors(\n        records_sdf: DataFrame,\n        error_location_distance_max: float,\n        error_location_distance_min: float,\n        closest_cell_distance_max: float,\n        cartesian_crs: int,\n        seed: int,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Generates location errors for x and y coordinates of each record in the DataFrame.\n\n        This method adds a random location error to the x and y coordinates of each record in the input DataFrame.\n        The location error is a random value between error_location_distance_min and error_location_distance_max,\n        and is added or subtracted from the x and y coordinates based on a random sign.\n\n        Args:\n            records_sdf (DataFrame): A DataFrame of records\n            error_location_distance_max (float): The maximum location error distance.\n            error_location_distance_min (float): The minimum location error distance.\n            closest_cell_distance_max (float): The maximum distance to the closest cell.\n            cartesian_crs (int): The EPSG code of the Cartesian coordinate reference system to use for the geometries.\n            seed (int): The seed for the random number generator.\n\n        Returns:\n            DataFrame: A DataFrame of records with location errors added to the x and y coordinates.\n        \"\"\"\n\n        errors_df = (\n            records_sdf.withColumn(\"y\", STF.ST_Y(F.col(\"generated_geometry\")))\n            .withColumn(\"x\", STF.ST_X(F.col(\"generated_geometry\")))\n            .drop(\"generated_geometry\")\n        )\n\n        errors_df = errors_df.withColumn(\n            ColNames.loc_error,\n            (F.rand(seed) * (error_location_distance_max - error_location_distance_min)) + error_location_distance_min,\n        )\n\n        errors_df = (\n            errors_df.withColumn(\"sign\", F.when(F.rand(seed) &gt; 0.5, 1).otherwise(-1))\n            .withColumn(\"new_x\", F.col(\"x\") + (F.col(ColNames.loc_error) * F.col(\"sign\")))\n            .withColumn(\"new_y\", F.col(\"y\") + (F.col(ColNames.loc_error) * F.col(\"sign\")))\n        )\n\n        errors_df = errors_df.withColumn(\n            \"generated_geometry\", STF.ST_SetSRID(STC.ST_Point(F.col(\"new_x\"), F.col(\"new_y\")), cartesian_crs)\n        ).drop(\"new_x\", \"new_y\")\n\n        errors_df = errors_df.withColumn(\"closest_cell_distance_max\", F.lit(closest_cell_distance_max))\n\n        errors_df = errors_df.select(\n            ColNames.user_id,\n            ColNames.timestamp,\n            \"generated_geometry\",\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            ColNames.loc_error,\n            \"closest_cell_distance_max\",\n        )\n\n        return errors_df\n\n    @staticmethod\n    def generate_records_with_non_existant_cell_ids(records_sdf: DataFrame, cells_sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Adds the cell_id column so that it will contain cell_ids that\n        are not present in the cells_df dataframe, yet follow the format of a cell id.\n\n        Args:\n            records_sdf (DataFrame): generated records\n            cells_sdf (DataFrame): cells dataframe\n\n        Returns:\n            DataFrame: records with cell ids that are not present in the cells_df dataframe\n        \"\"\"\n\n        # Generates random cell ids for cells_df, and selects those\n        # Join to records is implemented with a monotonically increasing id\n        # So to limit that, this number of all unique cells is used\n        # TODO check how to make this more optimal\n\n        n_of_actual_cell_ids_as_base = cells_sdf.count()\n\n        cells_df_with_random_cell_ids = cells_sdf[[ColNames.cell_id]].withColumn(\n            \"random_cell_id\",\n            (F.rand() * (999_999_999_999_999 - 10_000_000_000_000) + 10_000_000_000_000).cast(\"bigint\"),\n        )\n\n        # Do an  left anti join to ensure that now generated cell ids are not among actual cell ids\n\n        cells_df_inner_joined = cells_df_with_random_cell_ids[[\"random_cell_id\"]].join(\n            cells_sdf[[ColNames.cell_id]], on=F.col(\"random_cell_id\") == F.col(ColNames.cell_id), how=\"leftanti\"\n        )\n\n        cells_df_inner_joined = cells_df_inner_joined.withColumn(\"row_number\", (F.monotonically_increasing_id()))\n        records_sdf = records_sdf.withColumn(\n            \"row_number\", (F.monotonically_increasing_id() % n_of_actual_cell_ids_as_base) + 1\n        )\n\n        cells_df_inner_joined = cells_df_inner_joined.select(\n            \"row_number\", F.col(\"random_cell_id\").alias(ColNames.cell_id)\n        )\n\n        records_with_random_cell_id = records_sdf.join(cells_df_inner_joined, on=\"row_number\", how=\"left\").drop(\n            \"row_number\"\n        )\n\n        records_with_random_cell_id = records_with_random_cell_id.select(\n            ColNames.user_id,\n            ColNames.timestamp,\n            \"generated_geometry\",\n            ColNames.cell_id,\n            ColNames.loc_error,\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n        )\n\n        return records_with_random_cell_id\n\n    @staticmethod\n    def calc_hashed_user_id(sdf) -&gt; DataFrame:\n        \"\"\"\n        Calculates SHA2 hash of user id, takes the first 31 bits and converts them to a non-negative 32-bit integer.\n\n        Args:\n            sdf (pyspark.sql.DataFrame): Data of clean synthetic events with a user id column.\n\n        Returns:\n            pyspark.sql.DataFrame: Dataframe, where user_id column is transformered to a hashed value.\n\n        \"\"\"\n\n        sdf = sdf.withColumn(\"hashed_user_id\", F.sha2(sdf[ColNames.user_id].cast(\"string\"), 256))\n        sdf = sdf.withColumn(ColNames.user_id, F.unhex(F.col(\"hashed_user_id\")))\n        sdf = sdf.drop(\"hashed_user_id\")\n\n        return sdf\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.add_cell_ids_to_locations","title":"<code>add_cell_ids_to_locations(events_with_locations_df, cells_df, max_n_of_cells, seed)</code>  <code>staticmethod</code>","text":"<p>Links cell IDs to locations in the events DataFrame.</p> <p>This method performs a spatial join between the events and cells DataFrames to add cell IDs to each event. It first creates a buffer around each event location and finds cells that intersect with this buffer. It then calculates the distance from each event location to the cell and ranks the cells based on this distance. It keeps only the top 'max_n_of_cells' closest cells for each event.</p> <p>The method also adds a random index to each event-cell pair and filters to keep only one pair per event, randomly selecting one of the closest cells for each event.</p> <p>Parameters:</p> Name Type Description Default <code>events_with_locations_df</code> <code>DataFrame</code> <p>A DataFrame of events</p> required <code>cells_df</code> <code>DataFrame</code> <p>A DataFrame of cells</p> required <code>max_n_of_cells</code> <code>int</code> <p>The maximum number of closest cells to consider for each event.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: A DataFrame of events with cell IDs added.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>@staticmethod\ndef add_cell_ids_to_locations(\n    events_with_locations_df: DataFrame, cells_df: DataFrame, max_n_of_cells: int, seed: int\n) -&gt; DataFrame:\n    \"\"\"\n    Links cell IDs to locations in the events DataFrame.\n\n    This method performs a spatial join between the events and cells DataFrames to add cell IDs to each event.\n    It first creates a buffer around each event location and finds cells that intersect with this buffer.\n    It then calculates the distance from each event location to the cell and ranks the cells based on this distance.\n    It keeps only the top 'max_n_of_cells' closest cells for each event.\n\n    The method also adds a random index to each event-cell pair and filters to keep only one pair per event,\n    randomly selecting one of the closest cells for each event.\n\n    Args:\n        events_with_locations_df (pyspark.sql.DataFrame): A DataFrame of events\n        cells_df (pyspark.sql.DataFrame): A DataFrame of cells\n        max_n_of_cells (int): The maximum number of closest cells to consider for each event.\n\n    Returns:\n        pyspark.sql.DataFrame: A DataFrame of events with cell IDs added.\n    \"\"\"\n    events_with_cells_sdf = events_with_locations_df.join(\n        cells_df[[ColNames.cell_id, \"cell_geometry\"]],\n        (\n            STP.ST_Intersects(\n                STF.ST_Buffer(events_with_locations_df[\"generated_geometry\"], F.col(\"closest_cell_distance_max\")),\n                cells_df[\"cell_geometry\"],\n            )\n        ),\n    ).withColumn(\n        \"distance_to_cell\",\n        STF.ST_Distance(events_with_locations_df[\"generated_geometry\"], cells_df[\"cell_geometry\"]),\n    )\n\n    # Selection of different cell ids for a given timestamp is random\n    window_spec = Window.partitionBy(\n        ColNames.year, ColNames.month, ColNames.day, ColNames.user_id, ColNames.timestamp\n    ).orderBy(F.col(\"distance_to_cell\"))\n\n    events_with_cells_sdf = events_with_cells_sdf.withColumn(\n        \"closest_cells_index\", F.row_number().over(window_spec)\n    ).filter(F.col(\"closest_cells_index\") &lt;= max_n_of_cells)\n\n    window_spec_random = Window.partitionBy(\n        ColNames.year, ColNames.month, ColNames.day, ColNames.user_id, ColNames.timestamp\n    ).orderBy(F.rand(seed))\n    events_with_cells_sdf = events_with_cells_sdf.withColumn(\n        \"random_cell_index\", F.row_number().over(window_spec_random)\n    ).filter(F.col(\"random_cell_index\") == 1)\n\n    records_sdf = events_with_cells_sdf.select(\n        ColNames.user_id,\n        ColNames.timestamp,\n        \"generated_geometry\",\n        ColNames.cell_id,\n        ColNames.loc_error,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    )\n\n    return records_sdf\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.calc_hashed_user_id","title":"<code>calc_hashed_user_id(sdf)</code>  <code>staticmethod</code>","text":"<p>Calculates SHA2 hash of user id, takes the first 31 bits and converts them to a non-negative 32-bit integer.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Data of clean synthetic events with a user id column.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: Dataframe, where user_id column is transformered to a hashed value.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>@staticmethod\ndef calc_hashed_user_id(sdf) -&gt; DataFrame:\n    \"\"\"\n    Calculates SHA2 hash of user id, takes the first 31 bits and converts them to a non-negative 32-bit integer.\n\n    Args:\n        sdf (pyspark.sql.DataFrame): Data of clean synthetic events with a user id column.\n\n    Returns:\n        pyspark.sql.DataFrame: Dataframe, where user_id column is transformered to a hashed value.\n\n    \"\"\"\n\n    sdf = sdf.withColumn(\"hashed_user_id\", F.sha2(sdf[ColNames.user_id].cast(\"string\"), 256))\n    sdf = sdf.withColumn(ColNames.user_id, F.unhex(F.col(\"hashed_user_id\")))\n    sdf = sdf.drop(\"hashed_user_id\")\n\n    return sdf\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_event_timestamps_for_moves","title":"<code>generate_event_timestamps_for_moves(stays_sdf, event_freq_moves, cartesian_crs)</code>  <code>staticmethod</code>","text":"<p>Generates a DataFrame of event timestamps for movements between stays.</p> <p>For each stay in the input DataFrame, this method generates a random number of timestamps equal to the time     difference between the end of the current stay and the start of the next stay, divided by the event     frequency for moves.</p> <p>Parameters:</p> Name Type Description Default <code>stays_sdf</code> <code>DataFrame</code> <p>A DataFrame of stays.</p> required <code>event_freq_moves</code> <code>int</code> <p>The frequency of events for movements.</p> required <code>cartesian_crs</code> <code>int</code> <p>The EPSG code of the Cartesian coordinate reference system to use for the geometries.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: A DataFrame of event timestamps for movements.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>@staticmethod\ndef generate_event_timestamps_for_moves(\n    stays_sdf: DataFrame, event_freq_moves: int, cartesian_crs: int\n) -&gt; DataFrame:\n    \"\"\"\n    Generates a DataFrame of event timestamps for movements between stays.\n\n    For each stay in the input DataFrame, this method generates a random number of timestamps equal to the time\n        difference between the end of the current stay and the start of the next stay, divided by the event\n        frequency for moves.\n\n    Args:\n        stays_sdf (pyspark.sql.DataFrame): A DataFrame of stays.\n        event_freq_moves (int): The frequency of events for movements.\n        cartesian_crs (int): The EPSG code of the Cartesian coordinate reference system to use for the geometries.\n\n    Returns:\n        pyspark.sql.DataFrame: A DataFrame of event timestamps for movements.\n    \"\"\"\n\n    # Since the rows with activity_type = movement don't have any locations in the population diaries,\n    # we select the stay points and start generating timestamps in between the start and end of the stay\n\n    stays_sdf = stays_sdf.withColumn(\n        ColNames.geometry,\n        STF.ST_Transform(\n            STC.ST_Point(F.col(ColNames.longitude), F.col(ColNames.latitude)),\n            F.lit(\"EPSG:4326\"),\n            F.lit(f\"EPSG:{cartesian_crs}\"),\n        ),\n    )\n\n    # Define the window specification\u00a0\u2022\n    window_spec = Window.partitionBy(ColNames.year, ColNames.month, ColNames.day, ColNames.user_id).orderBy(\n        ColNames.initial_timestamp\n    )\n\n    # Add columns for next stay's geometry and start timestamp using the lead function\n    stays_sdf = stays_sdf.withColumn(\n        \"next_stay_initial_timestamp\", F.lead(ColNames.initial_timestamp, 1).over(window_spec)\n    )\n\n    stays_sdf = stays_sdf.withColumn(\"next_stay_geometry\", F.lead(ColNames.geometry, 1).over(window_spec))\n\n    stays_sdf = stays_sdf.withColumn(\n        \"time_diff_seconds\",\n        F.unix_timestamp(\"next_stay_initial_timestamp\") - F.unix_timestamp(ColNames.final_timestamp),\n    )\n\n    # Calculate how many timestamps fit in the interval for the given frequency\n    stays_sdf = stays_sdf.withColumn(\n        \"timestamps_count\", (F.col(\"time_diff_seconds\") / event_freq_moves).cast(\"integer\")\n    )\n\n    # Generate random floats between 0 and 1 for, using timestamps_count\n    stays_sdf = stays_sdf.withColumn(\n        \"random_fraction_on_line\", F.expr(\"transform(sequence(1, timestamps_count), x -&gt; rand())\")\n    ).withColumn(\"random_fraction_on_line\", F.explode(F.col(\"random_fraction_on_line\")))\n\n    # Generate timestamps\n    stays_sdf = stays_sdf.withColumn(\n        \"offset_seconds\", F.col(\"random_fraction_on_line\") * F.col(\"time_diff_seconds\")\n    )\n    stays_sdf = stays_sdf.withColumn(\n        ColNames.timestamp, F.from_unixtime(F.unix_timestamp(ColNames.final_timestamp) + F.col(\"offset_seconds\"))\n    )\n\n    moves_sdf = stays_sdf.withColumn(ColNames.activity_type, F.lit(\"move\"))\n\n    # Keep only necessary columns\n    moves_sdf = moves_sdf.select(\n        ColNames.user_id,\n        ColNames.timestamp,\n        ColNames.geometry,\n        \"next_stay_geometry\",\n        \"random_fraction_on_line\",\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    )\n\n    return moves_sdf\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_event_timestamps_for_stays","title":"<code>generate_event_timestamps_for_stays(stays_df, event_freq_stays, cartesian_crs)</code>  <code>staticmethod</code>","text":"<p>Generates a DataFrame of event timestamps for stays based on the event frequency for stays.</p> <p>For each stay in the input DataFrame, this method calculates the time difference between the initial and final timestamps of the stay. It then generates a number of timestamps equal to this time difference divided by the event frequency for stays. Each timestamp is associated with the location of the stay.</p> <p>Parameters:</p> Name Type Description Default <code>stays_df</code> <code>DataFrame</code> <p>A DataFrame of stays.</p> required <code>event_freq_stays</code> <code>int</code> <p>The frequency of events for stays.</p> required <code>cartesian_crs</code> <code>int</code> <p>The EPSG code of the Cartesian coordinate reference system to use for the geometries.</p> required <p>Returns:     pyspark.sql.DataFrame: A DataFrame of event timestamps for stays.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>@staticmethod\ndef generate_event_timestamps_for_stays(\n    stays_df: DataFrame, event_freq_stays: int, cartesian_crs: int\n) -&gt; DataFrame:\n    \"\"\"\n    Generates a DataFrame of event timestamps for stays based on the event frequency for stays.\n\n    For each stay in the input DataFrame, this method calculates\n    the time difference between the initial and final timestamps of the stay.\n    It then generates a number of timestamps equal to this time difference divided by the event\n    frequency for stays. Each timestamp is associated with the location of the stay.\n\n    Args:\n        stays_df (pyspark.sql.DataFrame): A DataFrame of stays.\n        event_freq_stays (int): The frequency of events for stays.\n        cartesian_crs (int): The EPSG code of the Cartesian coordinate reference system to use for the geometries.\n    Returns:\n        pyspark.sql.DataFrame: A DataFrame of event timestamps for stays.\n    \"\"\"\n\n    stays_df = stays_df.withColumn(\n        \"time_diff_seconds\",\n        F.unix_timestamp(F.col(ColNames.final_timestamp)) - F.unix_timestamp(F.col(ColNames.initial_timestamp)),\n    )\n\n    stays_df = stays_df.withColumn(\n        \"timestamps_count\", (F.col(\"time_diff_seconds\") / event_freq_stays).cast(\"integer\")\n    )\n\n    stays_df = stays_df.withColumn(\n        \"random_fraction_between_timestamps\", F.expr(\"transform(sequence(1, timestamps_count), x -&gt; rand())\")\n    )\n    stays_df = stays_df.withColumn(\n        \"random_fraction_between_timestamps\", F.explode(F.col(\"random_fraction_between_timestamps\"))\n    )\n    stays_df = stays_df.withColumn(\n        \"offset_seconds\", F.col(\"random_fraction_between_timestamps\") * F.col(\"time_diff_seconds\")\n    )\n    stays_df = stays_df.withColumn(\n        ColNames.timestamp, F.from_unixtime(F.unix_timestamp(ColNames.initial_timestamp) + F.col(\"offset_seconds\"))\n    )\n\n    stays_df = stays_df.withColumn(\n        \"generated_geometry\",\n        STF.ST_Transform(\n            STC.ST_Point(F.col(ColNames.longitude), F.col(ColNames.latitude)),\n            F.lit(\"EPSG:4326\"),\n            F.lit(f\"EPSG:{cartesian_crs}\"),\n        ),\n    )\n\n    stays_df = stays_df.select(\n        ColNames.user_id, ColNames.timestamp, \"generated_geometry\", ColNames.year, ColNames.month, ColNames.day\n    )\n\n    return stays_df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_location_errors","title":"<code>generate_location_errors(records_sdf, error_location_distance_max, error_location_distance_min, closest_cell_distance_max, cartesian_crs, seed)</code>  <code>staticmethod</code>","text":"<p>Generates location errors for x and y coordinates of each record in the DataFrame.</p> <p>This method adds a random location error to the x and y coordinates of each record in the input DataFrame. The location error is a random value between error_location_distance_min and error_location_distance_max, and is added or subtracted from the x and y coordinates based on a random sign.</p> <p>Parameters:</p> Name Type Description Default <code>records_sdf</code> <code>DataFrame</code> <p>A DataFrame of records</p> required <code>error_location_distance_max</code> <code>float</code> <p>The maximum location error distance.</p> required <code>error_location_distance_min</code> <code>float</code> <p>The minimum location error distance.</p> required <code>closest_cell_distance_max</code> <code>float</code> <p>The maximum distance to the closest cell.</p> required <code>cartesian_crs</code> <code>int</code> <p>The EPSG code of the Cartesian coordinate reference system to use for the geometries.</p> required <code>seed</code> <code>int</code> <p>The seed for the random number generator.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame of records with location errors added to the x and y coordinates.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>@staticmethod\ndef generate_location_errors(\n    records_sdf: DataFrame,\n    error_location_distance_max: float,\n    error_location_distance_min: float,\n    closest_cell_distance_max: float,\n    cartesian_crs: int,\n    seed: int,\n) -&gt; DataFrame:\n    \"\"\"\n    Generates location errors for x and y coordinates of each record in the DataFrame.\n\n    This method adds a random location error to the x and y coordinates of each record in the input DataFrame.\n    The location error is a random value between error_location_distance_min and error_location_distance_max,\n    and is added or subtracted from the x and y coordinates based on a random sign.\n\n    Args:\n        records_sdf (DataFrame): A DataFrame of records\n        error_location_distance_max (float): The maximum location error distance.\n        error_location_distance_min (float): The minimum location error distance.\n        closest_cell_distance_max (float): The maximum distance to the closest cell.\n        cartesian_crs (int): The EPSG code of the Cartesian coordinate reference system to use for the geometries.\n        seed (int): The seed for the random number generator.\n\n    Returns:\n        DataFrame: A DataFrame of records with location errors added to the x and y coordinates.\n    \"\"\"\n\n    errors_df = (\n        records_sdf.withColumn(\"y\", STF.ST_Y(F.col(\"generated_geometry\")))\n        .withColumn(\"x\", STF.ST_X(F.col(\"generated_geometry\")))\n        .drop(\"generated_geometry\")\n    )\n\n    errors_df = errors_df.withColumn(\n        ColNames.loc_error,\n        (F.rand(seed) * (error_location_distance_max - error_location_distance_min)) + error_location_distance_min,\n    )\n\n    errors_df = (\n        errors_df.withColumn(\"sign\", F.when(F.rand(seed) &gt; 0.5, 1).otherwise(-1))\n        .withColumn(\"new_x\", F.col(\"x\") + (F.col(ColNames.loc_error) * F.col(\"sign\")))\n        .withColumn(\"new_y\", F.col(\"y\") + (F.col(ColNames.loc_error) * F.col(\"sign\")))\n    )\n\n    errors_df = errors_df.withColumn(\n        \"generated_geometry\", STF.ST_SetSRID(STC.ST_Point(F.col(\"new_x\"), F.col(\"new_y\")), cartesian_crs)\n    ).drop(\"new_x\", \"new_y\")\n\n    errors_df = errors_df.withColumn(\"closest_cell_distance_max\", F.lit(closest_cell_distance_max))\n\n    errors_df = errors_df.select(\n        ColNames.user_id,\n        ColNames.timestamp,\n        \"generated_geometry\",\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n        ColNames.loc_error,\n        \"closest_cell_distance_max\",\n    )\n\n    return errors_df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_locations_for_moves","title":"<code>generate_locations_for_moves(event_timestamps_df, cartesian_crs)</code>  <code>staticmethod</code>","text":"<p>Generates locations for moves based on the event timestamps dataframe. Returns a dataframe, where for each move in the event timestamps dataframe a geometry column is added, representing the location of the move.</p> <p>Performs interpolation along the line between the starting move point (previous stay point) and next move point (next stay point) using the randomly generated values in the column random_fraction_on_line.</p> <p>Parameters:</p> Name Type Description Default <code>event_timestamps_df</code> <code>DataFrame</code> <p>The event timestamps dataframe.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: The event timestamps dataframe with the added geometry column.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>@staticmethod\ndef generate_locations_for_moves(event_timestamps_df: DataFrame, cartesian_crs: int) -&gt; DataFrame:\n    \"\"\"\n    Generates locations for moves based on the event timestamps dataframe.\n    Returns a dataframe, where for each move in the event timestamps dataframe\n    a geometry column is added, representing the location of the move.\n\n    Performs interpolation along the line between the starting move point (previous stay point)\n    and next move point (next stay point) using the randomly generated values in the column random_fraction_on_line.\n\n    Args:\n        event_timestamps_df (pyspark.sql.DataFrame): The event timestamps dataframe.\n\n    Returns:\n        pyspark.sql.DataFrame: The event timestamps dataframe with the added geometry column.\n    \"\"\"\n\n    moves_with_geometry = event_timestamps_df.withColumn(\n        \"line\", STF.ST_MakeLine(F.col(ColNames.geometry), F.col(\"next_stay_geometry\"))\n    )\n    moves_with_geometry = moves_with_geometry.withColumn(\n        \"generated_geometry\",\n        STF.ST_SetSRID(STF.ST_LineInterpolatePoint(F.col(\"line\"), F.col(\"random_fraction_on_line\")), cartesian_crs),\n    )\n\n    moves_with_geometry = moves_with_geometry.select(\n        ColNames.user_id, ColNames.timestamp, \"generated_geometry\", ColNames.year, ColNames.month, ColNames.day\n    )\n\n    return moves_with_geometry\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_records_with_non_existant_cell_ids","title":"<code>generate_records_with_non_existant_cell_ids(records_sdf, cells_sdf)</code>  <code>staticmethod</code>","text":"<p>Adds the cell_id column so that it will contain cell_ids that are not present in the cells_df dataframe, yet follow the format of a cell id.</p> <p>Parameters:</p> Name Type Description Default <code>records_sdf</code> <code>DataFrame</code> <p>generated records</p> required <code>cells_sdf</code> <code>DataFrame</code> <p>cells dataframe</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>records with cell ids that are not present in the cells_df dataframe</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>@staticmethod\ndef generate_records_with_non_existant_cell_ids(records_sdf: DataFrame, cells_sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Adds the cell_id column so that it will contain cell_ids that\n    are not present in the cells_df dataframe, yet follow the format of a cell id.\n\n    Args:\n        records_sdf (DataFrame): generated records\n        cells_sdf (DataFrame): cells dataframe\n\n    Returns:\n        DataFrame: records with cell ids that are not present in the cells_df dataframe\n    \"\"\"\n\n    # Generates random cell ids for cells_df, and selects those\n    # Join to records is implemented with a monotonically increasing id\n    # So to limit that, this number of all unique cells is used\n    # TODO check how to make this more optimal\n\n    n_of_actual_cell_ids_as_base = cells_sdf.count()\n\n    cells_df_with_random_cell_ids = cells_sdf[[ColNames.cell_id]].withColumn(\n        \"random_cell_id\",\n        (F.rand() * (999_999_999_999_999 - 10_000_000_000_000) + 10_000_000_000_000).cast(\"bigint\"),\n    )\n\n    # Do an  left anti join to ensure that now generated cell ids are not among actual cell ids\n\n    cells_df_inner_joined = cells_df_with_random_cell_ids[[\"random_cell_id\"]].join(\n        cells_sdf[[ColNames.cell_id]], on=F.col(\"random_cell_id\") == F.col(ColNames.cell_id), how=\"leftanti\"\n    )\n\n    cells_df_inner_joined = cells_df_inner_joined.withColumn(\"row_number\", (F.monotonically_increasing_id()))\n    records_sdf = records_sdf.withColumn(\n        \"row_number\", (F.monotonically_increasing_id() % n_of_actual_cell_ids_as_base) + 1\n    )\n\n    cells_df_inner_joined = cells_df_inner_joined.select(\n        \"row_number\", F.col(\"random_cell_id\").alias(ColNames.cell_id)\n    )\n\n    records_with_random_cell_id = records_sdf.join(cells_df_inner_joined, on=\"row_number\", how=\"left\").drop(\n        \"row_number\"\n    )\n\n    records_with_random_cell_id = records_with_random_cell_id.select(\n        ColNames.user_id,\n        ColNames.timestamp,\n        \"generated_geometry\",\n        ColNames.cell_id,\n        ColNames.loc_error,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    )\n\n    return records_with_random_cell_id\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events_errors/","title":"synthetic_events_errors","text":"<p>Module that adds errors to synthetic MNO event data.</p>"},{"location":"reference/components/ingestion/synthetic/synthetic_events_errors/#components.ingestion.synthetic.synthetic_events_errors.SyntheticEventsErrors","title":"<code>SyntheticEventsErrors</code>","text":"<p>             Bases: <code>Component</code></p> <p>Class that handles adding errors to synthetic data. It inherits from the Component abstract class.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events_errors.py</code> <pre><code>class SyntheticEventsErrors(Component):\n    \"\"\"\n    Class that handles adding errors to synthetic data. It inherits from the Component abstract class.\n    \"\"\"\n\n    COMPONENT_ID = \"SyntheticEventsErrors\"\n\n    def __init__(self, general_config_path: str, component_config_path: str):\n        super().__init__(general_config_path=general_config_path, component_config_path=component_config_path)\n\n        self.seed = self.config.getint(self.COMPONENT_ID, \"seed\")\n        self.timestamp_format = self.config.get(self.COMPONENT_ID, \"timestamp_format\")\n\n        # Error generation parameters.\n        self.do_error_generation = self.config.getboolean(self.COMPONENT_ID, \"do_error_generation\")\n        self.max_ratio_of_mandatory_columns = self.config.getfloat(\n            self.COMPONENT_ID, \"max_ratio_of_mandatory_columns_to_generate_as_null\"\n        )\n        self.null_row_prob = self.config.getfloat(self.COMPONENT_ID, \"null_row_probability\")\n        self.error_prob = self.config.getfloat(self.COMPONENT_ID, \"data_type_error_probability\")\n        self.out_of_bounds_prob = self.config.getfloat(self.COMPONENT_ID, \"out_of_bounds_probability\")\n        self.starting_timestamp = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"starting_timestamp\"), self.timestamp_format\n        )\n        self.ending_timestamp = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"ending_timestamp\"), self.timestamp_format\n        )\n        self.mandatory_columns = [i.name for i in BronzeEventDataObject.SCHEMA]\n        # Do not allow error generation on year-month-day columns to avoid partitioning errors\n        self.error_generation_allowed_columns = set(self.mandatory_columns) - set(\n            [ColNames.year, ColNames.month, ColNames.day]\n        )\n\n        # self.sort_output = self.config.getboolean(self.COMPONENT_ID, \"sort_output\")\n\n    def initalize_data_objects(self):\n        # Input (synthetic_events results)\n        input_events_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"event_data_bronze\")\n        input_bronze_event = BronzeEventDataObject(\n            self.spark, input_events_path, partition_columns=[ColNames.year, ColNames.month, ColNames.day]\n        )\n\n        self.input_data_objects = {BronzeEventDataObject.ID: input_bronze_event}\n\n        # Output\n        output_records_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"event_with_err_data_bronze\")\n        output_bronze_event = BronzeEventDataObject(\n            self.spark, output_records_path, partition_columns=[ColNames.year, ColNames.month, ColNames.day]\n        )\n        self.output_data_objects = {\"SyntheticErrors\": output_bronze_event}\n\n    def transform(self):\n        # Get input events df\n        records_df = self.input_data_objects[BronzeEventDataObject.ID].df\n\n        # Create a copy of original MSID column for final sorting and joining\n        records_df = records_df.withColumn(\"user_id_copy\", F.col(ColNames.user_id))\n\n        bronze_columns = [i.name for i in BronzeEventDataObject.SCHEMA]\n\n        records_df = records_df.select(bronze_columns + [\"user_id_copy\"])\n\n        # If error generation is enabled, replace clean dataset with errorful dataset.\n        if self.do_error_generation:\n            records_df = self.generate_errors(records_df)\n\n        records_df = records_df.drop(F.col(\"user_id_copy\"))\n\n        # records_df = records_df.select(bronze_columns)\n\n        # Assign output data object dataframe\n        self.output_data_objects[\"SyntheticErrors\"].df = records_df\n\n    def calc_hashed_user_id(self, df) -&gt; DataFrame:\n        \"\"\"\n        Calculates SHA2 hash of user id, takes the first 31 bits and converts them to a non-negative 32-bit integer.\n\n        Args:\n            df (pyspark.sql.DataFrame): Data of clean synthetic events with a user id column.\n\n        Returns:\n            pyspark.sql.DataFrame: Dataframe, where user_id column is transformered to a hashed value.\n\n        \"\"\"\n        # TODO: is this method used?\n        df = df.withColumn(\"hashed_user_id\", F.sha2(df[ColNames.user_id].cast(\"string\"), 256))\n        df = df.withColumn(ColNames.user_id, F.unhex(F.col(\"hashed_user_id\")))\n        df = df.drop(\"hashed_user_id\")\n\n        return df\n\n    def generate_errors(self, synth_df_raw: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Transforms a dataframe with clean synthetic records, by calculating year, month and day columns,\n        creating an event_id column, and generating all types of erronous records.\n        Calls all error generation functions.\n\n        Args:\n            synth_df_raw (pyspark.sql.DataFrame): Data of raw and clean synthetic events.\n\n        Returns:\n            pyspark.sql.DataFrame: Dataframe, with erroneous records, according to probabilities defined in the configuration.\n        \"\"\"\n\n        synth_df_raw = synth_df_raw.withColumn(ColNames.year, F.year(F.col(ColNames.timestamp)))\n        synth_df_raw = synth_df_raw.withColumn(ColNames.month, F.month(F.col(ColNames.timestamp)))\n        synth_df_raw = synth_df_raw.withColumn(ColNames.day, F.dayofmonth(F.col(ColNames.timestamp)))\n        synth_df_raw = synth_df_raw.withColumn(\"is_modified\", F.lit(False))\n\n        synth_df = synth_df_raw.cache()\n\n        synth_df = self.generate_nulls_in_mandatory_fields(synth_df)\n        synth_df = self.generate_out_of_bounds_dates(synth_df)\n        synth_df = self.generate_erroneous_type_values(synth_df)\n\n        synth_df = synth_df.drop(\"is_modified\")\n        return synth_df\n\n    def generate_nulls_in_mandatory_fields(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Generates null values in some fields of some rows based on configuration parameters.\n\n        Args:\n            df (pyspark.sql.DataFrame): clean synthetic data\n\n        Returns:\n            pyspark.sql.DataFrame: synthetic records dataframe with nulls in some columns of some rows\n        \"\"\"\n\n        # Two probability parameters from config apply:\n        # First one sets how many rows (as fraction of all rows) are selected for possible value nulling.\n        # Second one sets the likelyhood for each column to be set to null.\n        # If 1.0, then all mandatory columns of each selected row will be nulled.\n\n        if self.null_row_prob == 0.0:\n            # TODO logging\n            return df\n\n        # Split input dataframe to unchanged and changed portions\n        df = df.cache()\n        error_row_prob = self.null_row_prob\n        unchanged_row_prob = 1.0 - error_row_prob\n        unchanged_rows_df, error_rows_df = df.randomSplit([unchanged_row_prob, error_row_prob], seed=self.seed)\n\n        # Randomly select one column to be set as null to ensure at least one column is nulled.\n        error_rows_df = error_rows_df.withColumn(\n            \"rand_selection\", F.floor(F.rand() * (1 + len(self.error_generation_allowed_columns)))\n        )\n        # Randomly set some column values of rows to null, likelyhood based on ratio confing param.\n        for i, column in enumerate(self.error_generation_allowed_columns):\n            error_rows_df = error_rows_df.withColumn(\n                column,\n                F.when(\n                    (F.rand(seed=self.seed) &lt; self.max_ratio_of_mandatory_columns) | (F.col(\"rand_selection\") == i),\n                    F.lit(None),\n                ).otherwise(F.col(column)),\n            )\n        error_rows_df = error_rows_df.drop(\"rand_selection\")\n        error_rows_df = error_rows_df.withColumn(\"is_modified\", lit(True))\n\n        # Re-combine unchanged and changed rows of the dataframe.\n        return unchanged_rows_df.union(error_rows_df)\n\n    def generate_out_of_bounds_dates(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Transforms the timestamp column values to be out of bound of the selected period,\n        based on probabilities from configuration.\n        Only rows with non-null timestamp values can become altered here.\n\n        Args:\n            df (pyspark.sql.DataFrame): Dataframe of clean synthetic events.\n\n        Returns:\n            pyspark.sql.DataFrame: Data where some timestamp column values are out of bounds as per config.\n        \"\"\"\n\n        if self.out_of_bounds_prob == 0.0:\n            # TODO logging\n            return df\n\n        # Calculate approximate span in months from config parameters.\n        # The parameters should approximately cover the range of \"clean\" dates, so that the erroneous values can be generated outside the range.\n        events_span_in_months = max(\n            1, (pd.Timestamp(self.ending_timestamp) - pd.Timestamp(self.starting_timestamp)).days / 30\n        )\n\n        # Split rows by null/non-null timestamp.\n        df = df.cache()\n        null_timestamp_df = df.where(F.col(ColNames.timestamp).isNull())\n        nonnull_timestamp_df = df.where(F.col(ColNames.timestamp).isNotNull())\n        df.unpersist()\n\n        # From non-null timestamp rows, select a subset for adding errors to, depending on config parameter.\n        nonnull_timestamp_df = nonnull_timestamp_df.cache()\n        error_row_prob = self.out_of_bounds_prob\n        unchanged_row_prob = 1.0 - error_row_prob\n        unchanged_rows_df, error_rows_df = nonnull_timestamp_df.randomSplit(\n            [unchanged_row_prob, error_row_prob], seed=self.seed\n        )\n        # Combine null timestamp rows and not-modified non-null timestamp rows.\n        unchanged_rows_df = unchanged_rows_df.union(null_timestamp_df)\n\n        # Add months offset to error rows to make their timestamp values become outside expected range.\n        months_to_add_col = (F.lit(2) + F.rand(self.seed)) * F.lit(events_span_in_months)\n        modified_date_col = F.add_months(F.col(ColNames.timestamp), months_to_add_col)\n        time_col = F.date_format(F.col(ColNames.timestamp), \" HH:mm:ss\")\n        error_rows_df = error_rows_df.withColumn(ColNames.timestamp, F.concat(modified_date_col, time_col))\n        error_rows_df = error_rows_df.withColumn(\"is_modified\", F.lit(True))\n\n        # Combine changed and unchanged rows dataframes.\n        return unchanged_rows_df.union(error_rows_df)\n\n    def generate_erroneous_type_values(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Generates errors for sampled rows. Errors are custom defined, for instance a random string, or corrupt timestamp.\n        Does not cast the columns to a different type.\n\n        Args:\n            df (pyspark.sql.DataFrame): dataframe that may have out of bound and null records.\n\n        Returns:\n            pyspark.sql.DataFrame: dataframe with erroneous rows, and possibly, with nulls and out of bound records.\n        \"\"\"\n\n        if self.error_prob == 0:\n            # TODO logging\n            return df\n\n        # Split dataframe by whether the row has been modified during the error-adding process already.\n        # Already errored rows do not get further changes.\n        df = df.cache()\n        previously_modified_rows_df = df.where(F.col(\"is_modified\"))\n        unmodified_rows_df = df.where(~(F.col(\"is_modified\")))\n        df.unpersist()\n\n        # From unmodified rows, select a subset for adding errors to, depending on config parameter.\n        unmodified_rows_df = unmodified_rows_df.cache()\n        error_row_prob = self.error_prob\n        unchanged_row_prob = 1.0 - error_row_prob\n        unchanged_rows_df, error_rows_df = unmodified_rows_df.randomSplit(\n            [unchanged_row_prob, error_row_prob], seed=self.seed\n        )\n        # Gather rows that are not modified in this step (combine previously-modified rows and not-selected unmodified rows).\n        unchanged_rows_df = unchanged_rows_df.union(previously_modified_rows_df)\n\n        # Iterate over mandatory columns to mutate the value, depending on column data type.\n        for struct_schema in BronzeEventDataObject.SCHEMA:\n            if struct_schema.name not in self.error_generation_allowed_columns:\n                continue\n\n            column = struct_schema.name\n            col_dtype = struct_schema.dataType\n\n            if col_dtype in [BinaryType()]:\n                # md5 is a smaller hash,\n                to_value = F.unhex(F.md5(F.base64(F.col(column)).cast(StringType())))\n\n            if col_dtype in [FloatType(), IntegerType()]:\n                # changes mcc, lat, lon\n                to_value = (F.col(column) + ((F.rand() + F.lit(180)) * 10000)).cast(\"int\")\n\n            if column == ColNames.timestamp and col_dtype == StringType():\n                # Timezone difference manipulation may be performed here, if cleaning module were to support it.\n                # statically one timezone difference\n                # timezone_to = random.randint(0, 12)\n                to_value = F.concat(\n                    F.substring(F.col(column), 1, 10),\n                    F.lit(\"T\"),\n                    F.substring(F.col(column), 12, 9),\n                    # TODO: Temporary remove of timezone addition as cleaning\n                    # module does not support it\n                    # F.lit(f\"+0{timezone_to}:00\")\n                )\n\n            if column == ColNames.cell_id and col_dtype == StringType():\n                random_string = \"\".join(random.choice(string.ascii_uppercase + string.digits) for _ in range(6)) + \"_\"\n                to_value = F.concat(F.lit(random_string), (F.rand() * 100).cast(\"int\"))\n\n            error_rows_df = error_rows_df.withColumn(column, to_value)\n        error_rows_df = error_rows_df.withColumn(\"is_modified\", F.lit(True))\n        return unchanged_rows_df.union(error_rows_df)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events_errors/#components.ingestion.synthetic.synthetic_events_errors.SyntheticEventsErrors.calc_hashed_user_id","title":"<code>calc_hashed_user_id(df)</code>","text":"<p>Calculates SHA2 hash of user id, takes the first 31 bits and converts them to a non-negative 32-bit integer.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Data of clean synthetic events with a user id column.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: Dataframe, where user_id column is transformered to a hashed value.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events_errors.py</code> <pre><code>def calc_hashed_user_id(self, df) -&gt; DataFrame:\n    \"\"\"\n    Calculates SHA2 hash of user id, takes the first 31 bits and converts them to a non-negative 32-bit integer.\n\n    Args:\n        df (pyspark.sql.DataFrame): Data of clean synthetic events with a user id column.\n\n    Returns:\n        pyspark.sql.DataFrame: Dataframe, where user_id column is transformered to a hashed value.\n\n    \"\"\"\n    # TODO: is this method used?\n    df = df.withColumn(\"hashed_user_id\", F.sha2(df[ColNames.user_id].cast(\"string\"), 256))\n    df = df.withColumn(ColNames.user_id, F.unhex(F.col(\"hashed_user_id\")))\n    df = df.drop(\"hashed_user_id\")\n\n    return df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events_errors/#components.ingestion.synthetic.synthetic_events_errors.SyntheticEventsErrors.generate_erroneous_type_values","title":"<code>generate_erroneous_type_values(df)</code>","text":"<p>Generates errors for sampled rows. Errors are custom defined, for instance a random string, or corrupt timestamp. Does not cast the columns to a different type.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe that may have out of bound and null records.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: dataframe with erroneous rows, and possibly, with nulls and out of bound records.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events_errors.py</code> <pre><code>def generate_erroneous_type_values(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Generates errors for sampled rows. Errors are custom defined, for instance a random string, or corrupt timestamp.\n    Does not cast the columns to a different type.\n\n    Args:\n        df (pyspark.sql.DataFrame): dataframe that may have out of bound and null records.\n\n    Returns:\n        pyspark.sql.DataFrame: dataframe with erroneous rows, and possibly, with nulls and out of bound records.\n    \"\"\"\n\n    if self.error_prob == 0:\n        # TODO logging\n        return df\n\n    # Split dataframe by whether the row has been modified during the error-adding process already.\n    # Already errored rows do not get further changes.\n    df = df.cache()\n    previously_modified_rows_df = df.where(F.col(\"is_modified\"))\n    unmodified_rows_df = df.where(~(F.col(\"is_modified\")))\n    df.unpersist()\n\n    # From unmodified rows, select a subset for adding errors to, depending on config parameter.\n    unmodified_rows_df = unmodified_rows_df.cache()\n    error_row_prob = self.error_prob\n    unchanged_row_prob = 1.0 - error_row_prob\n    unchanged_rows_df, error_rows_df = unmodified_rows_df.randomSplit(\n        [unchanged_row_prob, error_row_prob], seed=self.seed\n    )\n    # Gather rows that are not modified in this step (combine previously-modified rows and not-selected unmodified rows).\n    unchanged_rows_df = unchanged_rows_df.union(previously_modified_rows_df)\n\n    # Iterate over mandatory columns to mutate the value, depending on column data type.\n    for struct_schema in BronzeEventDataObject.SCHEMA:\n        if struct_schema.name not in self.error_generation_allowed_columns:\n            continue\n\n        column = struct_schema.name\n        col_dtype = struct_schema.dataType\n\n        if col_dtype in [BinaryType()]:\n            # md5 is a smaller hash,\n            to_value = F.unhex(F.md5(F.base64(F.col(column)).cast(StringType())))\n\n        if col_dtype in [FloatType(), IntegerType()]:\n            # changes mcc, lat, lon\n            to_value = (F.col(column) + ((F.rand() + F.lit(180)) * 10000)).cast(\"int\")\n\n        if column == ColNames.timestamp and col_dtype == StringType():\n            # Timezone difference manipulation may be performed here, if cleaning module were to support it.\n            # statically one timezone difference\n            # timezone_to = random.randint(0, 12)\n            to_value = F.concat(\n                F.substring(F.col(column), 1, 10),\n                F.lit(\"T\"),\n                F.substring(F.col(column), 12, 9),\n                # TODO: Temporary remove of timezone addition as cleaning\n                # module does not support it\n                # F.lit(f\"+0{timezone_to}:00\")\n            )\n\n        if column == ColNames.cell_id and col_dtype == StringType():\n            random_string = \"\".join(random.choice(string.ascii_uppercase + string.digits) for _ in range(6)) + \"_\"\n            to_value = F.concat(F.lit(random_string), (F.rand() * 100).cast(\"int\"))\n\n        error_rows_df = error_rows_df.withColumn(column, to_value)\n    error_rows_df = error_rows_df.withColumn(\"is_modified\", F.lit(True))\n    return unchanged_rows_df.union(error_rows_df)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events_errors/#components.ingestion.synthetic.synthetic_events_errors.SyntheticEventsErrors.generate_errors","title":"<code>generate_errors(synth_df_raw)</code>","text":"<p>Transforms a dataframe with clean synthetic records, by calculating year, month and day columns, creating an event_id column, and generating all types of erronous records. Calls all error generation functions.</p> <p>Parameters:</p> Name Type Description Default <code>synth_df_raw</code> <code>DataFrame</code> <p>Data of raw and clean synthetic events.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: Dataframe, with erroneous records, according to probabilities defined in the configuration.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events_errors.py</code> <pre><code>def generate_errors(self, synth_df_raw: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms a dataframe with clean synthetic records, by calculating year, month and day columns,\n    creating an event_id column, and generating all types of erronous records.\n    Calls all error generation functions.\n\n    Args:\n        synth_df_raw (pyspark.sql.DataFrame): Data of raw and clean synthetic events.\n\n    Returns:\n        pyspark.sql.DataFrame: Dataframe, with erroneous records, according to probabilities defined in the configuration.\n    \"\"\"\n\n    synth_df_raw = synth_df_raw.withColumn(ColNames.year, F.year(F.col(ColNames.timestamp)))\n    synth_df_raw = synth_df_raw.withColumn(ColNames.month, F.month(F.col(ColNames.timestamp)))\n    synth_df_raw = synth_df_raw.withColumn(ColNames.day, F.dayofmonth(F.col(ColNames.timestamp)))\n    synth_df_raw = synth_df_raw.withColumn(\"is_modified\", F.lit(False))\n\n    synth_df = synth_df_raw.cache()\n\n    synth_df = self.generate_nulls_in_mandatory_fields(synth_df)\n    synth_df = self.generate_out_of_bounds_dates(synth_df)\n    synth_df = self.generate_erroneous_type_values(synth_df)\n\n    synth_df = synth_df.drop(\"is_modified\")\n    return synth_df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events_errors/#components.ingestion.synthetic.synthetic_events_errors.SyntheticEventsErrors.generate_nulls_in_mandatory_fields","title":"<code>generate_nulls_in_mandatory_fields(df)</code>","text":"<p>Generates null values in some fields of some rows based on configuration parameters.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>clean synthetic data</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: synthetic records dataframe with nulls in some columns of some rows</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events_errors.py</code> <pre><code>def generate_nulls_in_mandatory_fields(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Generates null values in some fields of some rows based on configuration parameters.\n\n    Args:\n        df (pyspark.sql.DataFrame): clean synthetic data\n\n    Returns:\n        pyspark.sql.DataFrame: synthetic records dataframe with nulls in some columns of some rows\n    \"\"\"\n\n    # Two probability parameters from config apply:\n    # First one sets how many rows (as fraction of all rows) are selected for possible value nulling.\n    # Second one sets the likelyhood for each column to be set to null.\n    # If 1.0, then all mandatory columns of each selected row will be nulled.\n\n    if self.null_row_prob == 0.0:\n        # TODO logging\n        return df\n\n    # Split input dataframe to unchanged and changed portions\n    df = df.cache()\n    error_row_prob = self.null_row_prob\n    unchanged_row_prob = 1.0 - error_row_prob\n    unchanged_rows_df, error_rows_df = df.randomSplit([unchanged_row_prob, error_row_prob], seed=self.seed)\n\n    # Randomly select one column to be set as null to ensure at least one column is nulled.\n    error_rows_df = error_rows_df.withColumn(\n        \"rand_selection\", F.floor(F.rand() * (1 + len(self.error_generation_allowed_columns)))\n    )\n    # Randomly set some column values of rows to null, likelyhood based on ratio confing param.\n    for i, column in enumerate(self.error_generation_allowed_columns):\n        error_rows_df = error_rows_df.withColumn(\n            column,\n            F.when(\n                (F.rand(seed=self.seed) &lt; self.max_ratio_of_mandatory_columns) | (F.col(\"rand_selection\") == i),\n                F.lit(None),\n            ).otherwise(F.col(column)),\n        )\n    error_rows_df = error_rows_df.drop(\"rand_selection\")\n    error_rows_df = error_rows_df.withColumn(\"is_modified\", lit(True))\n\n    # Re-combine unchanged and changed rows of the dataframe.\n    return unchanged_rows_df.union(error_rows_df)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events_errors/#components.ingestion.synthetic.synthetic_events_errors.SyntheticEventsErrors.generate_out_of_bounds_dates","title":"<code>generate_out_of_bounds_dates(df)</code>","text":"<p>Transforms the timestamp column values to be out of bound of the selected period, based on probabilities from configuration. Only rows with non-null timestamp values can become altered here.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe of clean synthetic events.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: Data where some timestamp column values are out of bounds as per config.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events_errors.py</code> <pre><code>def generate_out_of_bounds_dates(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the timestamp column values to be out of bound of the selected period,\n    based on probabilities from configuration.\n    Only rows with non-null timestamp values can become altered here.\n\n    Args:\n        df (pyspark.sql.DataFrame): Dataframe of clean synthetic events.\n\n    Returns:\n        pyspark.sql.DataFrame: Data where some timestamp column values are out of bounds as per config.\n    \"\"\"\n\n    if self.out_of_bounds_prob == 0.0:\n        # TODO logging\n        return df\n\n    # Calculate approximate span in months from config parameters.\n    # The parameters should approximately cover the range of \"clean\" dates, so that the erroneous values can be generated outside the range.\n    events_span_in_months = max(\n        1, (pd.Timestamp(self.ending_timestamp) - pd.Timestamp(self.starting_timestamp)).days / 30\n    )\n\n    # Split rows by null/non-null timestamp.\n    df = df.cache()\n    null_timestamp_df = df.where(F.col(ColNames.timestamp).isNull())\n    nonnull_timestamp_df = df.where(F.col(ColNames.timestamp).isNotNull())\n    df.unpersist()\n\n    # From non-null timestamp rows, select a subset for adding errors to, depending on config parameter.\n    nonnull_timestamp_df = nonnull_timestamp_df.cache()\n    error_row_prob = self.out_of_bounds_prob\n    unchanged_row_prob = 1.0 - error_row_prob\n    unchanged_rows_df, error_rows_df = nonnull_timestamp_df.randomSplit(\n        [unchanged_row_prob, error_row_prob], seed=self.seed\n    )\n    # Combine null timestamp rows and not-modified non-null timestamp rows.\n    unchanged_rows_df = unchanged_rows_df.union(null_timestamp_df)\n\n    # Add months offset to error rows to make their timestamp values become outside expected range.\n    months_to_add_col = (F.lit(2) + F.rand(self.seed)) * F.lit(events_span_in_months)\n    modified_date_col = F.add_months(F.col(ColNames.timestamp), months_to_add_col)\n    time_col = F.date_format(F.col(ColNames.timestamp), \" HH:mm:ss\")\n    error_rows_df = error_rows_df.withColumn(ColNames.timestamp, F.concat(modified_date_col, time_col))\n    error_rows_df = error_rows_df.withColumn(\"is_modified\", F.lit(True))\n\n    # Combine changed and unchanged rows dataframes.\n    return unchanged_rows_df.union(error_rows_df)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/","title":"synthetic_network","text":"<p>Module that generates a MNO synthetic network.</p>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.CellIDGenerator","title":"<code>CellIDGenerator</code>","text":"<p>Abstract class for cell ID generation.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>class CellIDGenerator(metaclass=ABCMeta):\n    \"\"\"\n    Abstract class for cell ID generation.\n    \"\"\"\n\n    def __init__(self, rng: int | Random) -&gt; None:\n        \"\"\"Cell ID Generator constructor\n\n        Args:\n            rng (int | Random): either an integer to act as a seed for RNG, or an instantiated Random object.\n        \"\"\"\n        if isinstance(rng, int):\n            self.rng = Random(rng)\n        elif isinstance(rng, Random):\n            self.rng = rng\n\n    @abstractmethod\n    def generate_cell_ids(self, n_cells: int) -&gt; list[str]:\n        \"\"\"Method that generates random cell IDs.\n\n        Args:\n            n_cells (int): number of cell IDs to generate.\n\n        Returns:\n            list[str]: list of cell IDs.\n        \"\"\"\n        return None\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.CellIDGenerator.__init__","title":"<code>__init__(rng)</code>","text":"<p>Cell ID Generator constructor</p> <p>Parameters:</p> Name Type Description Default <code>rng</code> <code>int | Random</code> <p>either an integer to act as a seed for RNG, or an instantiated Random object.</p> required Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>def __init__(self, rng: int | Random) -&gt; None:\n    \"\"\"Cell ID Generator constructor\n\n    Args:\n        rng (int | Random): either an integer to act as a seed for RNG, or an instantiated Random object.\n    \"\"\"\n    if isinstance(rng, int):\n        self.rng = Random(rng)\n    elif isinstance(rng, Random):\n        self.rng = rng\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.CellIDGenerator.generate_cell_ids","title":"<code>generate_cell_ids(n_cells)</code>  <code>abstractmethod</code>","text":"<p>Method that generates random cell IDs.</p> <p>Parameters:</p> Name Type Description Default <code>n_cells</code> <code>int</code> <p>number of cell IDs to generate.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: list of cell IDs.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>@abstractmethod\ndef generate_cell_ids(self, n_cells: int) -&gt; list[str]:\n    \"\"\"Method that generates random cell IDs.\n\n    Args:\n        n_cells (int): number of cell IDs to generate.\n\n    Returns:\n        list[str]: list of cell IDs.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.CellIDGeneratorBuilder","title":"<code>CellIDGeneratorBuilder</code>","text":"<p>Type/method of cell ID generation enumeration class.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>class CellIDGeneratorBuilder:\n    \"\"\"\n    Type/method of cell ID generation enumeration class.\n    \"\"\"\n\n    RANDOM_CELL_ID = \"random_cell_id\"\n\n    CONSTRUCTORS = {RANDOM_CELL_ID: RandomCellIDGenerator}\n\n    @staticmethod\n    def build(constructor_key: str, rng: int | Random) -&gt; CellIDGenerator:\n        \"\"\"\n        Method that builds a CellIDGenerator.\n\n        Args:\n            constructor_key (str): Key of the constructor\n            rng (int | Random): either an integer to act as a seed for RNG, or an instantiated Random object.\n\n        Raises:\n            ValueError: If the given constructor_key is not supported\n\n        Returns:\n            CellIDGenerator: Class that generates random cell_id's\n        \"\"\"\n        try:\n            constructor = CellIDGeneratorBuilder.CONSTRUCTORS[constructor_key]\n        except KeyError as e:\n            raise ValueError(f\"Random cell ID generator: {constructor_key} is not supported\") from e\n\n        return constructor(rng)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.CellIDGeneratorBuilder.build","title":"<code>build(constructor_key, rng)</code>  <code>staticmethod</code>","text":"<p>Method that builds a CellIDGenerator.</p> <p>Parameters:</p> Name Type Description Default <code>constructor_key</code> <code>str</code> <p>Key of the constructor</p> required <code>rng</code> <code>int | Random</code> <p>either an integer to act as a seed for RNG, or an instantiated Random object.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the given constructor_key is not supported</p> <p>Returns:</p> Name Type Description <code>CellIDGenerator</code> <code>CellIDGenerator</code> <p>Class that generates random cell_id's</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>@staticmethod\ndef build(constructor_key: str, rng: int | Random) -&gt; CellIDGenerator:\n    \"\"\"\n    Method that builds a CellIDGenerator.\n\n    Args:\n        constructor_key (str): Key of the constructor\n        rng (int | Random): either an integer to act as a seed for RNG, or an instantiated Random object.\n\n    Raises:\n        ValueError: If the given constructor_key is not supported\n\n    Returns:\n        CellIDGenerator: Class that generates random cell_id's\n    \"\"\"\n    try:\n        constructor = CellIDGeneratorBuilder.CONSTRUCTORS[constructor_key]\n    except KeyError as e:\n        raise ValueError(f\"Random cell ID generator: {constructor_key} is not supported\") from e\n\n    return constructor(rng)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.RandomCellIDGenerator","title":"<code>RandomCellIDGenerator</code>","text":"<p>             Bases: <code>CellIDGenerator</code></p> <p>Class that generates completely random cell IDs. Inherits from the AbstractCellIDGenerator class.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>class RandomCellIDGenerator(CellIDGenerator):\n    \"\"\"\n    Class that generates completely random cell IDs. Inherits from the AbstractCellIDGenerator class.\n    \"\"\"\n\n    def generate_cell_ids(self, n_cells: int) -&gt; list[str]:\n        \"\"\"Generate UNIQUE random cell IDs with no logic behind it, i.e. not following CGI/eCGI standards.\n        The resuling cell IDs are 14- or 15-digit strings.\n\n        Args:\n            n_cells (int): number of cell IDs to generate.\n\n        Returns:\n            list[str]: list of cell IDs.\n        \"\"\"\n        return list(map(str, self.rng.sample(range(10_000_000_000_000, 999_999_999_999_999), n_cells)))\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.RandomCellIDGenerator.generate_cell_ids","title":"<code>generate_cell_ids(n_cells)</code>","text":"<p>Generate UNIQUE random cell IDs with no logic behind it, i.e. not following CGI/eCGI standards. The resuling cell IDs are 14- or 15-digit strings.</p> <p>Parameters:</p> Name Type Description Default <code>n_cells</code> <code>int</code> <p>number of cell IDs to generate.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: list of cell IDs.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>def generate_cell_ids(self, n_cells: int) -&gt; list[str]:\n    \"\"\"Generate UNIQUE random cell IDs with no logic behind it, i.e. not following CGI/eCGI standards.\n    The resuling cell IDs are 14- or 15-digit strings.\n\n    Args:\n        n_cells (int): number of cell IDs to generate.\n\n    Returns:\n        list[str]: list of cell IDs.\n    \"\"\"\n    return list(map(str, self.rng.sample(range(10_000_000_000_000, 999_999_999_999_999), n_cells)))\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.SyntheticNetwork","title":"<code>SyntheticNetwork</code>","text":"<p>             Bases: <code>Component</code></p> <p>Class that generates the synthetic network topology data. It inherits from the Component abstract class.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>class SyntheticNetwork(Component):\n    \"\"\"\n    Class that generates the synthetic network topology data. It inherits from the Component abstract class.\n    \"\"\"\n\n    COMPONENT_ID = \"SyntheticNetwork\"\n\n    def __init__(self, general_config_path: str, component_config_path: str):\n        super().__init__(general_config_path=general_config_path, component_config_path=component_config_path)\n        self.seed = self.config.getint(self.COMPONENT_ID, \"seed\")\n        self.rng = Random(self.seed)\n        self.n_cells = self.config.getint(self.COMPONENT_ID, \"n_cells\")\n        self.cell_type_options = (\n            self.config.get(self.COMPONENT_ID, \"cell_type_options\").strip().replace(\" \", \"\").split(\",\")\n        )\n        self.tech = [\"5G\", \"LTE\", \"UMTS\", \"GSM\"]\n        self.latitude_min = self.config.getfloat(self.COMPONENT_ID, \"latitude_min\")\n        self.latitude_max = self.config.getfloat(self.COMPONENT_ID, \"latitude_max\")\n        self.longitude_min = self.config.getfloat(self.COMPONENT_ID, \"longitude_min\")\n        self.longitude_max = self.config.getfloat(self.COMPONENT_ID, \"longitude_max\")\n        self.altitude_min = self.config.getfloat(self.COMPONENT_ID, \"altitude_min\")\n        self.altitude_max = self.config.getfloat(self.COMPONENT_ID, \"altitude_max\")\n        self.antenna_height_max = self.config.getfloat(self.COMPONENT_ID, \"antenna_height_max\")\n        self.power_min = self.config.getfloat(self.COMPONENT_ID, \"power_min\")\n        self.power_max = self.config.getfloat(self.COMPONENT_ID, \"power_max\")\n        self.frequency_min = self.config.getfloat(self.COMPONENT_ID, \"frequency_min\")\n        self.frequency_max = self.config.getfloat(self.COMPONENT_ID, \"frequency_max\")\n\n        self.timestamp_format = self.config.get(self.COMPONENT_ID, \"timestamp_format\")\n        self.earliest_valid_date_start = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"earliest_valid_date_start\"), self.timestamp_format\n        )\n        self.latest_valid_date_end = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"latest_valid_date_end\"), self.timestamp_format\n        )\n\n        self.date_format = self.config.get(self.COMPONENT_ID, \"date_format\")\n\n        self.starting_date = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"starting_date\"), self.date_format\n        ).date()\n        self.ending_date = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"ending_date\"), self.date_format\n        ).date()\n\n        self.date_range = [\n            (self.starting_date + datetime.timedelta(days=dd))\n            for dd in range((self.ending_date - self.starting_date).days + 1)\n        ]\n\n        # Cell generation object\n        cell_id_generation_type = self.config.get(self.COMPONENT_ID, \"cell_id_generation_type\")\n\n        self.cell_id_generator = CellIDGeneratorBuilder.build(cell_id_generation_type, self.seed)\n\n        self.no_optional_fields_probability = self.config.getfloat(self.COMPONENT_ID, \"no_optional_fields_probability\")\n        self.mandatory_null_probability = self.config.getfloat(self.COMPONENT_ID, \"mandatory_null_probability\")\n        self.out_of_bounds_values_probability = self.config.getfloat(\n            self.COMPONENT_ID, \"out_of_bounds_values_probability\"\n        )\n        self.erroneous_values_probability = self.config.getfloat(self.COMPONENT_ID, \"erroneous_values_probability\")\n\n    def initalize_data_objects(self):\n        output_network_data_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"network_data_bronze\")\n        bronze_network = BronzeNetworkDataObject(\n            self.spark, output_network_data_path, partition_columns=[ColNames.year, ColNames.month, ColNames.day]\n        )\n        self.output_data_objects = {\"SyntheticNetwork\": bronze_network}\n\n    def read(self):\n        pass  # No input datasets are used in this component\n\n    def transform(self):\n        spark = self.spark\n\n        # Create Spark DataFrame with all valid cells and all optional fields\n\n        cells_df = spark.createDataFrame(self.clean_cells_generator(), schema=BronzeNetworkDataObject.SCHEMA)\n\n        # With certain probability, set ALL optional fields of a row to null.\n        # Fixing F.rand(seed=self.seed) will generate the same random column for every column, so\n        # it could be optimized to be generated only once\n        # If random optional fields should be set to zero (not all at the same time), use seed = self.seed + i(col_name)\n\n        for col_name in BronzeNetworkDataObject.OPTIONAL_COLUMNS:\n            cells_df = cells_df.withColumn(\n                col_name,\n                F.when(F.rand(seed=self.seed) &lt; self.no_optional_fields_probability, None).otherwise(F.col(col_name)),\n            )\n\n        cells_df = self.generate_errors(cells_df)\n\n        self.output_data_objects[\"SyntheticNetwork\"].df = cells_df\n\n    def clean_cells_generator(self):\n        \"\"\"Method that generates valid and fully complete (mandatory and optional) cell physical attibutes.\n\n        An underlying set of cells are created, covering the config-specified date interval.\n        Then, for each cell and date,\n        the valid_date_start and valid_date_end fields, marking the time interval in which the cell is operational, are\n        comparted with the date:\n            a) If  date &lt; valid_date_start, the cell-date row will not appear.\n            b) If valid_date_start &lt;= date &lt; valid_date_end, the cell-date row will appear and\n                the valid_date_end will be null, as the cell was currently operational\n            c) If valid_date_end &lt;= date, the cell-date row will appear and\n                the valid_date_end will NOT be null, marking the past, now known, time interval of operation.\n\n        Yields:\n            (\n                cell_id (str),\n                latitude (float),\n                longitude (float),\n                altitudes (float),\n                antenna_height (float),\n                directionality (int),\n                azimuth_angle (float | None),\n                elevation_angle (float),\n                hor_beam_width (float),\n                ver_beam_width (float),\n                power (float),\n                frequency (int),\n                technology (str),\n                valid_date_start (str),\n                valid_date_end (str | None)\n                cell_type (str),\n                year (int),\n                month (int),\n                day (int)\n            )\n        \"\"\"\n        # MANDATORY FIELDS\n        cell_ids = self.cell_id_generator.generate_cell_ids(self.n_cells)\n\n        latitudes = [self.rng.uniform(self.latitude_min, self.latitude_max) for _ in range(self.n_cells)]\n        longitudes = [self.rng.uniform(self.longitude_min, self.longitude_max) for _ in range(self.n_cells)]\n\n        # OPTIONAL FIELDS\n        altitudes = [self.rng.uniform(self.altitude_min, self.altitude_max) for _ in range(self.n_cells)]\n\n        # antenna height always positive\n        antenna_heights = [self.rng.uniform(0, self.antenna_height_max) for _ in range(self.n_cells)]\n\n        # Directionality: 0 or 1\n        directionalities = [self.rng.choice([0, 1]) for _ in range(self.n_cells)]\n\n        def random_azimuth_angle(directionality):\n            if directionality == 0:\n                return None\n            else:\n                return self.rng.uniform(0, 360)\n\n        # Azimuth angle: None if directionality is 0, float in [0, 360] if directionality is 1\n        azimuth_angles = [random_azimuth_angle(direc) for direc in directionalities]\n\n        # Eleveation angle: in [-90, 90]\n        elevation_angles = [self.rng.uniform(-90, 90) for _ in range(self.n_cells)]\n\n        # Horizontal/Vertical beam width: float in [0, 360]\n        hor_beam_widths = [self.rng.uniform(0, 360) for _ in range(self.n_cells)]\n        ver_beam_widths = [self.rng.uniform(0, 360) for _ in range(self.n_cells)]\n\n        # Power, float in specified range (unit: watts, W)\n        powers = [self.rng.uniform(self.power_min, self.power_max) for _ in range(self.n_cells)]\n\n        # Frequency: int in specifed range (unit: MHz)\n        frequencies = [self.rng.randint(self.frequency_min, self.frequency_max) for _ in range(self.n_cells)]\n\n        # Technology: str\n        technologies = [self.rng.choice(self.tech) for _ in range(self.n_cells)]\n\n        # # Valid start date, should be in the timestamp interval provided via config file\n        # span_seconds = (self.latest_valid_date_end - self.earliest_valid_date_start).total_seconds()\n\n        # # Start date will be some random nb of seconds after the earliest valid date start\n        # valid_date_start_dts = [\n        #     self.earliest_valid_date_start + datetime.timedelta(seconds=self.rng.uniform(0, span_seconds))\n        #     for _ in range(self.n_cells)\n        # ]\n\n        # # Remaining seconds from the valid date starts to the ending date\n        # remaining_seconds = [\n        #     (self.latest_valid_date_end - start_dt).total_seconds() for start_dt in valid_date_start_dts\n        # ]\n        # # Choose valid date ends ALWAYS after the valid date start\n        # # Minimum of two seconds, as valid date end is excluded from the time window,\n        # # so cell will be valid for at least 1 second\n        # valid_date_end_dts = [\n        #     (start_dt + datetime.timedelta(seconds=self.rng.uniform(2, rem_secs)))\n        #     for (start_dt, rem_secs) in zip(valid_date_start_dts, remaining_seconds)\n        # ]\n\n        valid_date_start_dts = [self.earliest_valid_date_start for _ in range(self.n_cells)]\n        valid_date_end_dts = [self.latest_valid_date_end for _ in range(self.n_cells)]\n\n        def check_operational(_curr_date: datetime.date, _end_datetime: datetime.datetime):\n            if _curr_date &lt; _end_datetime.date():  # still operational, return None\n                return None\n            else:\n                return _end_datetime.strftime(self.timestamp_format)\n\n        # Cell type: str in option list\n        cell_types = [self.rng.choice(self.cell_type_options) for _ in range(self.n_cells)]\n\n        for date in self.date_range:\n            for i in range(self.n_cells):\n                # Assume we do not have info about future cells\n                if valid_date_start_dts[i].date() &lt;= date:\n                    yield (\n                        cell_ids[i],\n                        latitudes[i],\n                        longitudes[i],\n                        altitudes[i],\n                        antenna_heights[i],\n                        directionalities[i],\n                        azimuth_angles[i],\n                        elevation_angles[i],\n                        hor_beam_widths[i],\n                        ver_beam_widths[i],\n                        powers[i],\n                        frequencies[i],\n                        technologies[i],\n                        valid_date_start_dts[i].strftime(self.timestamp_format),\n                        check_operational(date, valid_date_end_dts[i]),  # null if still operational today\n                        cell_types[i],\n                        date.year,\n                        date.month,\n                        date.day,\n                    )\n\n    def generate_errors(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Function handling the generation of out-of-bounds, null and erroneous values in different columns\n        according to the config-specified probabilities\n\n        Args:\n            df (DataFrame): clean DataFrame\n\n        Returns:\n            DataFrame: DataFrame after the generation of different invalid or null values\n        \"\"\"\n\n        if self.out_of_bounds_values_probability &gt; 0:\n            df = self.generate_out_of_bounds_values(df)\n\n        if self.mandatory_null_probability &gt; 0:\n            df = self.generate_nulls_in_mandatory_columns(df)\n\n        if self.erroneous_values_probability &gt; 0:\n            df = self.generate_erroneous_values(df)\n\n        return df\n\n    def generate_nulls_in_mandatory_columns(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Generates null values in the mandatory fields based on probabilities form config\n\n        Args:\n            df (DataFrame): synthetic dataframe\n\n        Returns:\n            DataFrame: synthetic dataframe with nulls in some mandatory fields\n        \"\"\"\n        # Use different seed for each column\n        for i, col_name in enumerate(BronzeNetworkDataObject.MANDATORY_COLUMNS):\n            df = df.withColumn(\n                col_name,\n                F.when(F.rand(seed=self.seed + i + 1) &lt; self.mandatory_null_probability, None).otherwise(\n                    F.col(col_name)\n                ),\n            )\n\n        return df\n\n    def generate_out_of_bounds_values(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Function that generates out-of-bounds values for the appropriate columns of the data object\n\n        Args:\n            df (DataFrame): cell dataframe with in-bound values\n\n        Returns:\n            DataFrame: cell dataframe with some out-of-bounds values\n        \"\"\"\n        # (2 * F.round(F.rand(seed=self.seed-1)) - 1) -&gt; random column of 1s and -1s\n        df = df.withColumn(\n            ColNames.latitude,\n            F.when(\n                F.rand(seed=self.seed - 1) &lt; self.out_of_bounds_values_probability,\n                F.col(ColNames.latitude) + (2 * F.round(F.rand(seed=self.seed - 1)) - 1) * 90,\n            )\n            .otherwise(F.col(ColNames.latitude))\n            .cast(FloatType()),  # the operation turns it into DoubleType, so cast it to Float\n        )\n\n        df = df.withColumn(\n            ColNames.longitude,\n            F.when(\n                F.rand(seed=self.seed - 2) &lt; self.out_of_bounds_values_probability,\n                F.col(ColNames.longitude) + (2 * F.round(F.rand(seed=self.seed - 2)) - 1) * 180,\n            )\n            .otherwise(F.col(ColNames.longitude))\n            .cast(FloatType()),\n        )\n\n        # antenna height, non positive\n        df = df.withColumn(\n            ColNames.antenna_height,\n            F.when(F.rand(seed=self.seed - 3) &lt; self.out_of_bounds_values_probability, -F.col(ColNames.antenna_height))\n            .otherwise(F.col(ColNames.antenna_height))\n            .cast(FloatType()),\n        )\n\n        # directionality: int different from 0 or 1. Just add a static 5 to the value\n        df = df.withColumn(\n            ColNames.directionality,\n            F.when(\n                F.rand(seed=self.seed - 4) &lt; self.out_of_bounds_values_probability, F.col(ColNames.directionality) + 5\n            )\n            .otherwise(F.col(ColNames.directionality))\n            .cast(IntegerType()),\n        )\n\n        # azimuth angle: outside of [0, 360]\n        df = df.withColumn(\n            ColNames.azimuth_angle,\n            F.when(\n                F.rand(seed=self.seed - 5) &lt; self.out_of_bounds_values_probability,\n                F.col(ColNames.azimuth_angle) + (2 * F.round(F.rand(seed=self.seed - 5)) - 1) * 360,\n            )\n            .otherwise(F.col(ColNames.azimuth_angle))\n            .cast(FloatType()),\n        )\n\n        # elevation_angle: outside of [-90, 90]\n        df = df.withColumn(\n            ColNames.elevation_angle,\n            F.when(\n                F.rand(seed=self.seed - 6) &lt; self.out_of_bounds_values_probability,\n                F.col(ColNames.elevation_angle) + (2 * F.round(F.rand(seed=self.seed - 6)) - 1) * 180,\n            )\n            .otherwise(F.col(ColNames.elevation_angle))\n            .cast(FloatType()),\n        )\n\n        # horizontal_beam_width: outside of [0, 360]\n        df = df.withColumn(\n            ColNames.horizontal_beam_width,\n            F.when(\n                F.rand(seed=self.seed - 7) &lt; self.out_of_bounds_values_probability,\n                F.col(ColNames.horizontal_beam_width) + (2 * F.round(F.rand(seed=self.seed - 7)) - 1) * 360,\n            )\n            .otherwise(F.col(ColNames.horizontal_beam_width))\n            .cast(FloatType()),\n        )\n\n        # vertical_beam_width: outside of [0, 360]\n        df = df.withColumn(\n            ColNames.vertical_beam_width,\n            F.when(\n                F.rand(seed=self.seed - 8) &lt; self.out_of_bounds_values_probability,\n                F.col(ColNames.vertical_beam_width) + (2 * F.round(F.rand(seed=self.seed - 8)) - 1) * 360,\n            )\n            .otherwise(F.col(ColNames.vertical_beam_width))\n            .cast(FloatType()),\n        )\n\n        # power: non positive value\n        df = df.withColumn(\n            ColNames.power,\n            F.when(F.rand(seed=self.seed - 9) &lt; self.out_of_bounds_values_probability, -F.col(ColNames.power))\n            .otherwise(F.col(ColNames.power))\n            .cast(FloatType()),\n        )\n\n        # frequency: non positive vallue\n        df = df.withColumn(\n            ColNames.frequency,\n            F.when(F.rand(seed=self.seed - 9) &lt; self.out_of_bounds_values_probability, -F.col(ColNames.frequency))\n            .otherwise(F.col(ColNames.frequency))\n            .cast(IntegerType()),\n        )\n        return df\n\n    def generate_erroneous_values(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Function that generates erroneous values in the cell_id, valid_date_start and valid_date_end columns\n\n        Args:\n            df (DataFrame): DataFrame before the generation of erroneous values\n\n        Returns:\n            DataFrame: DataFrame with erroneous values\n        \"\"\"\n        # Erroneous cells: for now, a string not of 14 or 15 digits\n        df = df.withColumn(\n            ColNames.cell_id,\n            F.when(\n                F.rand(seed=self.seed * 1000) &lt; self.erroneous_values_probability,\n                (\n                    F.when(\n                        F.rand(seed=self.seed * 2000) &gt; 0.5,\n                        F.concat(\n                            F.col(ColNames.cell_id), F.substring(F.col(ColNames.cell_id), 3, 3)\n                        ),  # 17 or 18 digits\n                    ).otherwise(\n                        F.substring(F.col(ColNames.cell_id), 3, 10)  # 10 digits\n                    )\n                ),\n            )\n            .otherwise(F.col(ColNames.cell_id))\n            .cast(LongType())\n            .cast(StringType()),\n        )\n\n        # Dates\n        df_as_is, df_swap, df_wrong = df.randomSplit(\n            weights=[\n                1 - self.erroneous_values_probability,\n                self.erroneous_values_probability / 2,\n                self.erroneous_values_probability / 2,\n            ],\n            seed=self.seed,\n        )\n        # For some columns, swap valid_date_start and valid_date_end\n        df_swap = df_swap.withColumns(\n            {ColNames.valid_date_start: ColNames.valid_date_end, ColNames.valid_date_end: ColNames.valid_date_start}\n        )\n\n        chars_to_change = [\"0\", \"T\", \":\", \"/\", \"2\"]\n        changed_chars = [\"0\", \"T\", \":\", \"/\", \"2\"]\n        # Now, for dates as well, make the timestamp format incorrect\n\n        self.rng.shuffle(changed_chars)\n        df_wrong = df_wrong.withColumn(\n            ColNames.valid_date_start,\n            F.translate(F.col(ColNames.valid_date_start), \"\".join(chars_to_change), \"\".join(changed_chars)),\n        )\n\n        self.rng.shuffle(changed_chars)\n        df_wrong = df_wrong.withColumn(\n            ColNames.valid_date_end,\n            F.translate(F.col(ColNames.valid_date_end), \"\".join(chars_to_change), \"\".join(changed_chars)),\n        )\n\n        df = df_as_is.union(df_swap).union(df_wrong)\n\n        return df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.SyntheticNetwork.clean_cells_generator","title":"<code>clean_cells_generator()</code>","text":"<p>Method that generates valid and fully complete (mandatory and optional) cell physical attibutes.</p> <p>An underlying set of cells are created, covering the config-specified date interval. Then, for each cell and date, the valid_date_start and valid_date_end fields, marking the time interval in which the cell is operational, are comparted with the date:     a) If  date &lt; valid_date_start, the cell-date row will not appear.     b) If valid_date_start &lt;= date &lt; valid_date_end, the cell-date row will appear and         the valid_date_end will be null, as the cell was currently operational     c) If valid_date_end &lt;= date, the cell-date row will appear and         the valid_date_end will NOT be null, marking the past, now known, time interval of operation.</p> <p>Yields:</p> Type Description <p>( cell_id (str), latitude (float), longitude (float), altitudes (float), antenna_height (float), directionality (int), azimuth_angle (float | None), elevation_angle (float), hor_beam_width (float), ver_beam_width (float), power (float), frequency (int), technology (str), valid_date_start (str), valid_date_end (str | None) cell_type (str), year (int), month (int), day (int)</p> <p>)</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>def clean_cells_generator(self):\n    \"\"\"Method that generates valid and fully complete (mandatory and optional) cell physical attibutes.\n\n    An underlying set of cells are created, covering the config-specified date interval.\n    Then, for each cell and date,\n    the valid_date_start and valid_date_end fields, marking the time interval in which the cell is operational, are\n    comparted with the date:\n        a) If  date &lt; valid_date_start, the cell-date row will not appear.\n        b) If valid_date_start &lt;= date &lt; valid_date_end, the cell-date row will appear and\n            the valid_date_end will be null, as the cell was currently operational\n        c) If valid_date_end &lt;= date, the cell-date row will appear and\n            the valid_date_end will NOT be null, marking the past, now known, time interval of operation.\n\n    Yields:\n        (\n            cell_id (str),\n            latitude (float),\n            longitude (float),\n            altitudes (float),\n            antenna_height (float),\n            directionality (int),\n            azimuth_angle (float | None),\n            elevation_angle (float),\n            hor_beam_width (float),\n            ver_beam_width (float),\n            power (float),\n            frequency (int),\n            technology (str),\n            valid_date_start (str),\n            valid_date_end (str | None)\n            cell_type (str),\n            year (int),\n            month (int),\n            day (int)\n        )\n    \"\"\"\n    # MANDATORY FIELDS\n    cell_ids = self.cell_id_generator.generate_cell_ids(self.n_cells)\n\n    latitudes = [self.rng.uniform(self.latitude_min, self.latitude_max) for _ in range(self.n_cells)]\n    longitudes = [self.rng.uniform(self.longitude_min, self.longitude_max) for _ in range(self.n_cells)]\n\n    # OPTIONAL FIELDS\n    altitudes = [self.rng.uniform(self.altitude_min, self.altitude_max) for _ in range(self.n_cells)]\n\n    # antenna height always positive\n    antenna_heights = [self.rng.uniform(0, self.antenna_height_max) for _ in range(self.n_cells)]\n\n    # Directionality: 0 or 1\n    directionalities = [self.rng.choice([0, 1]) for _ in range(self.n_cells)]\n\n    def random_azimuth_angle(directionality):\n        if directionality == 0:\n            return None\n        else:\n            return self.rng.uniform(0, 360)\n\n    # Azimuth angle: None if directionality is 0, float in [0, 360] if directionality is 1\n    azimuth_angles = [random_azimuth_angle(direc) for direc in directionalities]\n\n    # Eleveation angle: in [-90, 90]\n    elevation_angles = [self.rng.uniform(-90, 90) for _ in range(self.n_cells)]\n\n    # Horizontal/Vertical beam width: float in [0, 360]\n    hor_beam_widths = [self.rng.uniform(0, 360) for _ in range(self.n_cells)]\n    ver_beam_widths = [self.rng.uniform(0, 360) for _ in range(self.n_cells)]\n\n    # Power, float in specified range (unit: watts, W)\n    powers = [self.rng.uniform(self.power_min, self.power_max) for _ in range(self.n_cells)]\n\n    # Frequency: int in specifed range (unit: MHz)\n    frequencies = [self.rng.randint(self.frequency_min, self.frequency_max) for _ in range(self.n_cells)]\n\n    # Technology: str\n    technologies = [self.rng.choice(self.tech) for _ in range(self.n_cells)]\n\n    # # Valid start date, should be in the timestamp interval provided via config file\n    # span_seconds = (self.latest_valid_date_end - self.earliest_valid_date_start).total_seconds()\n\n    # # Start date will be some random nb of seconds after the earliest valid date start\n    # valid_date_start_dts = [\n    #     self.earliest_valid_date_start + datetime.timedelta(seconds=self.rng.uniform(0, span_seconds))\n    #     for _ in range(self.n_cells)\n    # ]\n\n    # # Remaining seconds from the valid date starts to the ending date\n    # remaining_seconds = [\n    #     (self.latest_valid_date_end - start_dt).total_seconds() for start_dt in valid_date_start_dts\n    # ]\n    # # Choose valid date ends ALWAYS after the valid date start\n    # # Minimum of two seconds, as valid date end is excluded from the time window,\n    # # so cell will be valid for at least 1 second\n    # valid_date_end_dts = [\n    #     (start_dt + datetime.timedelta(seconds=self.rng.uniform(2, rem_secs)))\n    #     for (start_dt, rem_secs) in zip(valid_date_start_dts, remaining_seconds)\n    # ]\n\n    valid_date_start_dts = [self.earliest_valid_date_start for _ in range(self.n_cells)]\n    valid_date_end_dts = [self.latest_valid_date_end for _ in range(self.n_cells)]\n\n    def check_operational(_curr_date: datetime.date, _end_datetime: datetime.datetime):\n        if _curr_date &lt; _end_datetime.date():  # still operational, return None\n            return None\n        else:\n            return _end_datetime.strftime(self.timestamp_format)\n\n    # Cell type: str in option list\n    cell_types = [self.rng.choice(self.cell_type_options) for _ in range(self.n_cells)]\n\n    for date in self.date_range:\n        for i in range(self.n_cells):\n            # Assume we do not have info about future cells\n            if valid_date_start_dts[i].date() &lt;= date:\n                yield (\n                    cell_ids[i],\n                    latitudes[i],\n                    longitudes[i],\n                    altitudes[i],\n                    antenna_heights[i],\n                    directionalities[i],\n                    azimuth_angles[i],\n                    elevation_angles[i],\n                    hor_beam_widths[i],\n                    ver_beam_widths[i],\n                    powers[i],\n                    frequencies[i],\n                    technologies[i],\n                    valid_date_start_dts[i].strftime(self.timestamp_format),\n                    check_operational(date, valid_date_end_dts[i]),  # null if still operational today\n                    cell_types[i],\n                    date.year,\n                    date.month,\n                    date.day,\n                )\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.SyntheticNetwork.generate_erroneous_values","title":"<code>generate_erroneous_values(df)</code>","text":"<p>Function that generates erroneous values in the cell_id, valid_date_start and valid_date_end columns</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame before the generation of erroneous values</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with erroneous values</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>def generate_erroneous_values(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Function that generates erroneous values in the cell_id, valid_date_start and valid_date_end columns\n\n    Args:\n        df (DataFrame): DataFrame before the generation of erroneous values\n\n    Returns:\n        DataFrame: DataFrame with erroneous values\n    \"\"\"\n    # Erroneous cells: for now, a string not of 14 or 15 digits\n    df = df.withColumn(\n        ColNames.cell_id,\n        F.when(\n            F.rand(seed=self.seed * 1000) &lt; self.erroneous_values_probability,\n            (\n                F.when(\n                    F.rand(seed=self.seed * 2000) &gt; 0.5,\n                    F.concat(\n                        F.col(ColNames.cell_id), F.substring(F.col(ColNames.cell_id), 3, 3)\n                    ),  # 17 or 18 digits\n                ).otherwise(\n                    F.substring(F.col(ColNames.cell_id), 3, 10)  # 10 digits\n                )\n            ),\n        )\n        .otherwise(F.col(ColNames.cell_id))\n        .cast(LongType())\n        .cast(StringType()),\n    )\n\n    # Dates\n    df_as_is, df_swap, df_wrong = df.randomSplit(\n        weights=[\n            1 - self.erroneous_values_probability,\n            self.erroneous_values_probability / 2,\n            self.erroneous_values_probability / 2,\n        ],\n        seed=self.seed,\n    )\n    # For some columns, swap valid_date_start and valid_date_end\n    df_swap = df_swap.withColumns(\n        {ColNames.valid_date_start: ColNames.valid_date_end, ColNames.valid_date_end: ColNames.valid_date_start}\n    )\n\n    chars_to_change = [\"0\", \"T\", \":\", \"/\", \"2\"]\n    changed_chars = [\"0\", \"T\", \":\", \"/\", \"2\"]\n    # Now, for dates as well, make the timestamp format incorrect\n\n    self.rng.shuffle(changed_chars)\n    df_wrong = df_wrong.withColumn(\n        ColNames.valid_date_start,\n        F.translate(F.col(ColNames.valid_date_start), \"\".join(chars_to_change), \"\".join(changed_chars)),\n    )\n\n    self.rng.shuffle(changed_chars)\n    df_wrong = df_wrong.withColumn(\n        ColNames.valid_date_end,\n        F.translate(F.col(ColNames.valid_date_end), \"\".join(chars_to_change), \"\".join(changed_chars)),\n    )\n\n    df = df_as_is.union(df_swap).union(df_wrong)\n\n    return df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.SyntheticNetwork.generate_errors","title":"<code>generate_errors(df)</code>","text":"<p>Function handling the generation of out-of-bounds, null and erroneous values in different columns according to the config-specified probabilities</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>clean DataFrame</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame after the generation of different invalid or null values</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>def generate_errors(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Function handling the generation of out-of-bounds, null and erroneous values in different columns\n    according to the config-specified probabilities\n\n    Args:\n        df (DataFrame): clean DataFrame\n\n    Returns:\n        DataFrame: DataFrame after the generation of different invalid or null values\n    \"\"\"\n\n    if self.out_of_bounds_values_probability &gt; 0:\n        df = self.generate_out_of_bounds_values(df)\n\n    if self.mandatory_null_probability &gt; 0:\n        df = self.generate_nulls_in_mandatory_columns(df)\n\n    if self.erroneous_values_probability &gt; 0:\n        df = self.generate_erroneous_values(df)\n\n    return df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.SyntheticNetwork.generate_nulls_in_mandatory_columns","title":"<code>generate_nulls_in_mandatory_columns(df)</code>","text":"<p>Generates null values in the mandatory fields based on probabilities form config</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>synthetic dataframe</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>synthetic dataframe with nulls in some mandatory fields</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>def generate_nulls_in_mandatory_columns(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Generates null values in the mandatory fields based on probabilities form config\n\n    Args:\n        df (DataFrame): synthetic dataframe\n\n    Returns:\n        DataFrame: synthetic dataframe with nulls in some mandatory fields\n    \"\"\"\n    # Use different seed for each column\n    for i, col_name in enumerate(BronzeNetworkDataObject.MANDATORY_COLUMNS):\n        df = df.withColumn(\n            col_name,\n            F.when(F.rand(seed=self.seed + i + 1) &lt; self.mandatory_null_probability, None).otherwise(\n                F.col(col_name)\n            ),\n        )\n\n    return df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.SyntheticNetwork.generate_out_of_bounds_values","title":"<code>generate_out_of_bounds_values(df)</code>","text":"<p>Function that generates out-of-bounds values for the appropriate columns of the data object</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>cell dataframe with in-bound values</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>cell dataframe with some out-of-bounds values</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>def generate_out_of_bounds_values(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Function that generates out-of-bounds values for the appropriate columns of the data object\n\n    Args:\n        df (DataFrame): cell dataframe with in-bound values\n\n    Returns:\n        DataFrame: cell dataframe with some out-of-bounds values\n    \"\"\"\n    # (2 * F.round(F.rand(seed=self.seed-1)) - 1) -&gt; random column of 1s and -1s\n    df = df.withColumn(\n        ColNames.latitude,\n        F.when(\n            F.rand(seed=self.seed - 1) &lt; self.out_of_bounds_values_probability,\n            F.col(ColNames.latitude) + (2 * F.round(F.rand(seed=self.seed - 1)) - 1) * 90,\n        )\n        .otherwise(F.col(ColNames.latitude))\n        .cast(FloatType()),  # the operation turns it into DoubleType, so cast it to Float\n    )\n\n    df = df.withColumn(\n        ColNames.longitude,\n        F.when(\n            F.rand(seed=self.seed - 2) &lt; self.out_of_bounds_values_probability,\n            F.col(ColNames.longitude) + (2 * F.round(F.rand(seed=self.seed - 2)) - 1) * 180,\n        )\n        .otherwise(F.col(ColNames.longitude))\n        .cast(FloatType()),\n    )\n\n    # antenna height, non positive\n    df = df.withColumn(\n        ColNames.antenna_height,\n        F.when(F.rand(seed=self.seed - 3) &lt; self.out_of_bounds_values_probability, -F.col(ColNames.antenna_height))\n        .otherwise(F.col(ColNames.antenna_height))\n        .cast(FloatType()),\n    )\n\n    # directionality: int different from 0 or 1. Just add a static 5 to the value\n    df = df.withColumn(\n        ColNames.directionality,\n        F.when(\n            F.rand(seed=self.seed - 4) &lt; self.out_of_bounds_values_probability, F.col(ColNames.directionality) + 5\n        )\n        .otherwise(F.col(ColNames.directionality))\n        .cast(IntegerType()),\n    )\n\n    # azimuth angle: outside of [0, 360]\n    df = df.withColumn(\n        ColNames.azimuth_angle,\n        F.when(\n            F.rand(seed=self.seed - 5) &lt; self.out_of_bounds_values_probability,\n            F.col(ColNames.azimuth_angle) + (2 * F.round(F.rand(seed=self.seed - 5)) - 1) * 360,\n        )\n        .otherwise(F.col(ColNames.azimuth_angle))\n        .cast(FloatType()),\n    )\n\n    # elevation_angle: outside of [-90, 90]\n    df = df.withColumn(\n        ColNames.elevation_angle,\n        F.when(\n            F.rand(seed=self.seed - 6) &lt; self.out_of_bounds_values_probability,\n            F.col(ColNames.elevation_angle) + (2 * F.round(F.rand(seed=self.seed - 6)) - 1) * 180,\n        )\n        .otherwise(F.col(ColNames.elevation_angle))\n        .cast(FloatType()),\n    )\n\n    # horizontal_beam_width: outside of [0, 360]\n    df = df.withColumn(\n        ColNames.horizontal_beam_width,\n        F.when(\n            F.rand(seed=self.seed - 7) &lt; self.out_of_bounds_values_probability,\n            F.col(ColNames.horizontal_beam_width) + (2 * F.round(F.rand(seed=self.seed - 7)) - 1) * 360,\n        )\n        .otherwise(F.col(ColNames.horizontal_beam_width))\n        .cast(FloatType()),\n    )\n\n    # vertical_beam_width: outside of [0, 360]\n    df = df.withColumn(\n        ColNames.vertical_beam_width,\n        F.when(\n            F.rand(seed=self.seed - 8) &lt; self.out_of_bounds_values_probability,\n            F.col(ColNames.vertical_beam_width) + (2 * F.round(F.rand(seed=self.seed - 8)) - 1) * 360,\n        )\n        .otherwise(F.col(ColNames.vertical_beam_width))\n        .cast(FloatType()),\n    )\n\n    # power: non positive value\n    df = df.withColumn(\n        ColNames.power,\n        F.when(F.rand(seed=self.seed - 9) &lt; self.out_of_bounds_values_probability, -F.col(ColNames.power))\n        .otherwise(F.col(ColNames.power))\n        .cast(FloatType()),\n    )\n\n    # frequency: non positive vallue\n    df = df.withColumn(\n        ColNames.frequency,\n        F.when(F.rand(seed=self.seed - 9) &lt; self.out_of_bounds_values_probability, -F.col(ColNames.frequency))\n        .otherwise(F.col(ColNames.frequency))\n        .cast(IntegerType()),\n    )\n    return df\n</code></pre>"},{"location":"reference/components/quality/","title":"quality","text":""},{"location":"reference/components/quality/event_quality_warnings/","title":"event_quality_warnings","text":""},{"location":"reference/components/quality/event_quality_warnings/event_quality_warnings/","title":"event_quality_warnings","text":""},{"location":"reference/components/quality/event_quality_warnings/event_quality_warnings/#components.quality.event_quality_warnings.event_quality_warnings.EventQualityWarnings","title":"<code>EventQualityWarnings</code>","text":"<p>             Bases: <code>Component</code></p> <p>Component that calculates Quality Warnings from the Quality Metrics generated by the Event Cleaning component</p> Source code in <code>multimno/components/quality/event_quality_warnings/event_quality_warnings.py</code> <pre><code>class EventQualityWarnings(Component):\n    \"\"\"\n    Component that calculates Quality Warnings from the Quality Metrics generated by the Event Cleaning component\n    \"\"\"\n\n    COMPONENT_ID = \"EventQualityWarnings\"\n\n    dict_convert_to_num_days = {\"week\": 7, \"month\": 30}\n    # dict to store info regarding error type\n    # first element - corresponding encoding of ErrorTypes class\n    # second element - naming constants for coresponding measure definitions, conditions, and warning texts\n    dict_error_type_info = {\n        \"missing_value\": [ErrorTypes.missing_value, \"Missing value rate\"],\n        \"not_right_syntactic_format\": [ErrorTypes.not_right_syntactic_format, \"Wrong type/format rate\"],\n        \"out_of_admissible_values\": [ErrorTypes.out_of_admissible_values, \"Out of range rate\"],\n        \"no_location\": [ErrorTypes.no_location, \"No location error rate\"],\n        \"out_of_bounding_box\": [ErrorTypes.out_of_bounding_box, \"Out of bounding box error rate\"],\n        \"deduplication_diff_location\": [\n            ErrorTypes.different_location_duplicate,\n            \"Deduplication different locations rate\",\n        ],\n        \"deduplication_same_location\": [ErrorTypes.same_location_duplicate, \"Deduplication same locations rate\"],\n    }\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.lookback_period = self.config.get(self.SUB_COMPONENT_ID, \"lookback_period\")\n        self.lookback_period_in_days = self.dict_convert_to_num_days[self.lookback_period]\n\n        self.data_period_start = self.config.get(self.SUB_COMPONENT_ID, \"data_period_start\")\n        self.data_period_end = self.config.get(self.SUB_COMPONENT_ID, \"data_period_end\")\n\n        self.qw_dfs_log = []\n        self.qw_dfs_plots = []\n\n        # FOR SYNTACTIC QUALITY WARNINGS\n        self.do_size_raw_data_qw = self.config.getboolean(self.SUB_COMPONENT_ID, \"do_size_raw_data_qw\", fallback=False)\n        self.do_size_clean_data_qw = self.config.getboolean(\n            self.SUB_COMPONENT_ID, \"do_size_clean_data_qw\", fallback=False\n        )\n\n        self.data_size_tresholds = self.config.geteval(self.SUB_COMPONENT_ID, \"data_size_tresholds\", fallback=None)\n\n        self.do_error_rate_by_date_qw = self.config.getboolean(\n            self.SUB_COMPONENT_ID, \"do_error_rate_by_date_qw\", fallback=False\n        )\n\n        self.do_error_rate_by_date_and_cell_qw = self.config.getboolean(\n            self.SUB_COMPONENT_ID, \"do_error_rate_by_date_and_cell_qw\", fallback=False\n        )\n\n        self.do_error_rate_by_date_and_user_qw = self.config.getboolean(\n            self.SUB_COMPONENT_ID, \"do_error_rate_by_date_and_user_qw\", fallback=False\n        )\n\n        self.do_error_rate_by_date_and_cell_user_qw = self.config.getboolean(\n            self.SUB_COMPONENT_ID, \"do_error_rate_by_date_and_cell_user_qw\", fallback=False\n        )\n\n        self.error_rate_tresholds = self.config.geteval(self.SUB_COMPONENT_ID, \"error_rate_tresholds\", fallback=None)\n\n        self.error_type_qw_checks = self.config.geteval(self.SUB_COMPONENT_ID, \"error_type_qw_checks\", fallback=None)\n\n        self.missing_value_thresholds = self.config.geteval(\n            self.SUB_COMPONENT_ID, \"missing_value_thresholds\", fallback=None\n        )\n\n        self.out_of_admissible_values_thresholds = self.config.geteval(\n            self.SUB_COMPONENT_ID, \"out_of_admissible_values_thresholds\", fallback=None\n        )\n\n        self.not_right_syntactic_format_thresholds = self.config.geteval(\n            self.SUB_COMPONENT_ID, \"not_right_syntactic_format_thresholds\", fallback=None\n        )\n\n        self.no_location_thresholds = self.config.geteval(\n            self.SUB_COMPONENT_ID, \"no_location_thresholds\", fallback=None\n        )\n\n        self.out_of_bounding_box_thresholds = self.config.geteval(\n            self.SUB_COMPONENT_ID, \"out_of_bounding_box_thresholds\", fallback=None\n        )\n        # FOR DEDUPLICATION QUALITY WARNINGS\n        self.deduplication_diff_location_thresholds = self.config.geteval(\n            self.SUB_COMPONENT_ID, \"deduplication_diff_location_thresholds\", fallback=None\n        )\n        self.deduplication_same_location_thresholds = self.config.geteval(\n            self.SUB_COMPONENT_ID, \"deduplication_same_location_thresholds\", fallback=None\n        )\n\n    def initalize_data_objects(self):\n        self.input_qm_data_objects = {}\n        self.output_qw_data_objects = {}\n        self.clear_destination_directory = self.config.getboolean(self.SUB_COMPONENT_ID, \"clear_destination_directory\")\n\n        self.input_qm_by_column_path_key = self.config.get(self.SUB_COMPONENT_ID, \"input_qm_by_column_path_key\")\n        self.input_qm_freq_distr_path_key = self.config.get(self.SUB_COMPONENT_ID, \"input_qm_freq_distr_path_key\")\n        self.output_qw_log_table_path_key = self.config.get(self.SUB_COMPONENT_ID, \"output_qw_log_table_path_key\")\n        self.output_qw_for_plots_path_key = self.config.get(\n            self.SUB_COMPONENT_ID, \"output_qw_for_plots_path_key\", fallback=None\n        )\n\n        self.input_qm_by_column_path = self.config.get(CONFIG_SILVER_PATHS_KEY, self.input_qm_by_column_path_key)\n        if check_if_data_path_exists(self.spark, self.input_qm_by_column_path):\n            self.input_qm_data_objects[SilverEventDataSyntacticQualityMetricsByColumn.ID] = (\n                SilverEventDataSyntacticQualityMetricsByColumn(self.spark, self.input_qm_by_column_path)\n            )\n        else:\n            self.logger.warning(\"Wrong path for Quality Metrics By Column, terminating component execution\")\n            raise ValueError(\"Invalid path for Quality Metrics By Column\")\n\n        self.input_qm_freq_distr_path = self.config.get(CONFIG_SILVER_PATHS_KEY, self.input_qm_freq_distr_path_key)\n        if check_if_data_path_exists(self.spark, self.input_qm_freq_distr_path):\n            self.input_qm_data_objects[SilverEventDataSyntacticQualityMetricsFrequencyDistribution.ID] = (\n                SilverEventDataSyntacticQualityMetricsFrequencyDistribution(self.spark, self.input_qm_freq_distr_path)\n            )\n        else:\n            self.logger.warning(\n                \"Wrong path for Quality Metrics Frequency Distribution, terminating component execution\"\n            )\n            raise ValueError(\"Invalid path for Quality Metrics Frequency Distribution\")\n\n        self.output_qw_log_table_path = self.config.get(CONFIG_SILVER_PATHS_KEY, self.output_qw_log_table_path_key)\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, self.output_qw_log_table_path)\n        check_or_create_data_path(self.spark, self.output_qw_log_table_path)\n        self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsLogTable.ID] = (\n            SilverEventDataSyntacticQualityWarningsLogTable(self.spark, self.output_qw_log_table_path)\n        )\n        # no plots information is intended for EventDeduplicationQualityWarnings\n        if self.output_qw_for_plots_path_key is not None:\n            self.output_qw_for_plots_path = self.config.get(CONFIG_SILVER_PATHS_KEY, self.output_qw_for_plots_path_key)\n            if self.clear_destination_directory:\n                delete_file_or_folder(self.spark, self.output_qw_for_plots_path)\n            check_or_create_data_path(self.spark, self.output_qw_for_plots_path)\n            self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsForPlots.ID] = (\n                SilverEventDataSyntacticQualityWarningsForPlots(self.spark, self.output_qw_for_plots_path)\n            )\n\n    def read(self):\n        self.input_qm_data_objects[SilverEventDataSyntacticQualityMetricsByColumn.ID].read()\n        self.input_qm_data_objects[SilverEventDataSyntacticQualityMetricsFrequencyDistribution.ID].read()\n\n    def write(self):\n        self.save_quality_warnings_log_table(self.qw_dfs_log)\n        if self.output_qw_for_plots_path_key is not None:\n            self.save_quality_warnings_for_plots(self.qw_dfs_plots)\n\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n        self.transform()  # Transforms the input_df\n        self.write()\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n        # Read QA Metrics of EventCleaning Component, the period of intrest is\n        #  [data_period_start-lookback_period_in_days, data_period_end]\n        # Since QualityWarnings are calculated based on prior data\n        # TODO: deal with cases when df_qa_by_column, df_qa_freq_distribution do not have data for\n        # whole defined period\n        # TODO: dynamically define/check the possible research period of QW  based on data period\n        #  of df_qa_by_column and df_qa_freq_distribution\n        # TODO: implement min_period conf param which is minimal amount of days with previous data to\n        #  have in order to calculate QW (the case for first days in reaserch period)\n        sdate = pd.to_datetime(self.data_period_start) - pd.Timedelta(days=self.lookback_period_in_days)\n        edate = pd.to_datetime(self.data_period_end)\n\n        df_qa_by_column = self.input_qm_data_objects[SilverEventDataSyntacticQualityMetricsByColumn.ID].df.where(\n            psf.col(ColNames.date).between(sdate, edate)\n        )\n\n        df_qa_freq_distribution = self.input_qm_data_objects[\n            SilverEventDataSyntacticQualityMetricsFrequencyDistribution.ID\n        ].df.where(psf.col(ColNames.date).between(sdate, edate))\n\n        df_qa_by_column = df_qa_by_column.cache()\n        # TODO: maybe makes sense to first sum init and final freq, cache and parse this aggregation\n        # to further QW functions\n        df_qa_freq_distribution = df_qa_freq_distribution.cache()\n\n        if self.do_size_raw_data_qw:\n            # for raw data size QW compute warnings and also retrive data to plot distribution of initial frequency\n            df_raw_data_qw, df_raw_plots = self.data_size_qw(\n                df_qa_freq_distribution,\n                self.lookback_period_in_days,\n                self.data_size_tresholds[\"SIZE_RAW_DATA_BYDATE_VARIABILITY\"],\n                self.data_size_tresholds[\"SIZE_RAW_DATA_BYDATE_ABS_VALUE_LOWER_LIMIT\"],\n                self.data_size_tresholds[\"SIZE_RAW_DATA_BYDATE_ABS_VALUE_UPPER_LIMIT\"],\n                type_of_data=\"raw\",\n            )\n            self.qw_dfs_log.append(df_raw_data_qw)\n            self.qw_dfs_plots.append(df_raw_plots)\n\n        if self.do_size_clean_data_qw:\n            # for clean data size QW compute warnings and also retrive data to plot distribution of total frequency\n            df_clean_data_qw, df_clean_plots = self.data_size_qw(\n                df_qa_freq_distribution,\n                self.lookback_period_in_days,\n                self.data_size_tresholds[\"SIZE_CLEAN_DATA_BYDATE_VARIABILITY\"],\n                self.data_size_tresholds[\"SIZE_CLEAN_DATA_BYDATE_ABS_VALUE_LOWER_LIMIT\"],\n                self.data_size_tresholds[\"SIZE_CLEAN_DATA_BYDATE_ABS_VALUE_UPPER_LIMIT\"],\n                type_of_data=\"clean\",\n            )\n            self.qw_dfs_log.append(df_clean_data_qw)\n            self.qw_dfs_plots.append(df_clean_plots)\n\n        if self.do_error_rate_by_date_qw:\n            # for error rate by date QW compute warnings and also retrive data to\n            # plot distribution of error rate by date\n            df_error_rate_by_date_qw, df_error_rate_plots = self.error_rate_qw(\n                df_qa_freq_distribution,\n                self.lookback_period_in_days,\n                [ColNames.date],\n                self.error_rate_tresholds[\"TOTAL_ERROR_RATE_BYDATE_OVER_AVERAGE\"],\n                self.error_rate_tresholds[\"TOTAL_ERROR_RATE_BYDATE_VARIABILITY\"],\n                self.error_rate_tresholds[\"TOTAL_ERROR_RATE_BYDATE_ABS_VALUE_UPPER_LIMIT\"],\n                save_data_for_plots=True,\n            )\n\n            self.qw_dfs_log.append(df_error_rate_by_date_qw)\n            self.qw_dfs_plots.append(df_error_rate_plots)\n        # The current aggrement is that for next error rates (more granular ones) do not store any data for plots\n        # although it could be done with save_data_for_plots=True\n        # TODO: should we consider error rate of null user_id or/and null cell_id in QW computation\n        if self.do_error_rate_by_date_and_cell_qw:\n            df_error_rate_by_date_and_cell_qw, _ = self.error_rate_qw(\n                df_qa_freq_distribution,\n                self.lookback_period_in_days,\n                [ColNames.date, ColNames.cell_id],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYCELL_OVER_AVERAGE\"],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYCELL_VARIABILITY\"],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYCELL_ABS_VALUE_UPPER_LIMIT\"],\n            )\n\n            self.qw_dfs_log.append(df_error_rate_by_date_and_cell_qw)\n\n        if self.do_error_rate_by_date_and_user_qw:\n            df_error_rate_by_date_and_user_qw, _ = self.error_rate_qw(\n                df_qa_freq_distribution,\n                self.lookback_period_in_days,\n                [ColNames.date, ColNames.user_id],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYUSER_OVER_AVERAGE\"],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYUSER_VARIABILITY\"],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYUSER_ABS_VALUE_UPPER_LIMIT\"],\n            )\n\n            self.qw_dfs_log.append(df_error_rate_by_date_and_user_qw)\n\n        if self.do_error_rate_by_date_and_cell_user_qw:\n            df_error_rate_by_date_and_cell_user_qw, _ = self.error_rate_qw(\n                df_qa_freq_distribution,\n                self.lookback_period_in_days,\n                [ColNames.date, ColNames.cell_id, ColNames.user_id],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYCELL_USER_OVER_AVERAGE\"],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYCELL_USER_VARIABILITY\"],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYCELL_USER_ABS_VALUE_UPPER_LIMIT\"],\n            )\n\n            self.qw_dfs_log.append(df_error_rate_by_date_and_cell_user_qw)\n\n        # Two previous types of QW were using df_qa_freq_distribution only\n        # Now calculate error rate for different error types like missing_value, wrong type\n        # based on two QA metrics - df_qa_by_column and df_qa_freq_distribution\n        # error_type_qw_checks - dict('error_type':[relevant columns])\n        for error_type, field_names in self.error_type_qw_checks.items():\n            if field_names == []:\n                self.logger.info(f\"No field name(s) were specified for error type: {error_type}\")\n            else:\n                # if you have a new error_type and thus new error_type_thresholds entry in config\n                # make sure to add it to class atributes and to this block with elif statement\n                if error_type == \"missing_value\":\n                    error_type_thresholds = self.missing_value_thresholds\n                elif error_type == \"out_of_admissible_values\":\n                    error_type_thresholds = self.out_of_admissible_values_thresholds\n                elif error_type == \"not_right_syntactic_format\":\n                    error_type_thresholds = self.not_right_syntactic_format_thresholds\n                elif error_type == \"no_location\":\n                    error_type_thresholds = self.no_location_thresholds\n                elif error_type == \"out_of_bounding_box\":\n                    error_type_thresholds = self.out_of_bounding_box_thresholds\n                elif error_type == \"deduplication_diff_location\":\n                    error_type_thresholds = self.deduplication_diff_location_thresholds\n                elif error_type == \"deduplication_same_location\":\n                    error_type_thresholds = self.deduplication_same_location_thresholds\n                else:\n                    self.logger.warning(\n                        f\"Unexpected error type in error_type_qw_checks config param\"\n                        f\": {error_type}, skipping calculation for this qw\"\n                    )\n                    continue\n\n            for field_name in field_names:\n                if field_name in error_type_thresholds.keys():\n                    error_type_qw, _ = self.error_type_rate_qw(\n                        df_qa_by_column,\n                        df_qa_freq_distribution,\n                        field_name,\n                        error_type,\n                        self.lookback_period_in_days,\n                        *list(error_type_thresholds[field_name].values()),\n                    )\n                    self.qw_dfs_log.append(error_type_qw)\n                else:\n                    self.logger.warning(\n                        f\"No thresholds were specified for field {field_name} of {error_type} error_type\"\n                    )\n\n        self.spark.catalog.clearCache()\n\n    def data_size_qw(\n        self,\n        df_freq_distribution: DataFrame,\n        lookback_period_in_days: int,\n        variablility: int | float,\n        lower_limit: int | float,\n        upper_limit: int | float,\n        type_of_data: str,\n        measure_definition_canva: str = f\"{MeasureDefinitions.size_data}\",\n        cond_warn_variability_canva: str = f\"{Conditions.size_data_variability}-{Warnings.size_data_variability}\",\n        cond_warn_upper_lower_canva: str = f\"{Conditions.size_data_upper_lower}-{Warnings.size_data_upper_lower}\",\n    ) -&gt; tuple[DataFrame]:\n        \"\"\"\n        A unified function to check both raw and clean data sizes, calculates four types of QWs:\n        LOWER_VARIABILITY - for each row using calculated mean and std compute lower variability limit\n            which is mean - SD*variability, check if  daily_value is lower tan limit\n        UPPER_VARIABILITY - for each row using calculated mean and std compute upper variability limit\n             which is mean + SD*variability, check if  daily_value exceeds limit\n        ABS_LOWER_LIMIT - check if daily_value is lower than absolute number lower_limit\n        ABS_UPPER_LIMIT - check if daily_value exceeds absolute number upper_limit\n        All four QWs depend on thresholds, in case if statement condition is met -&gt; store cond-warn-condition_value\n            information in array column,\n        some rows may have several QWs. In the end array column is exploded and cond-warn-consition_value information\n            is split into three corresponding columns.\n        The function returns almost ready dfs for SilverEventDataSyntacticQualityWarningsLogTable\n             and SilverEventDataSyntacticQualityWarningsForPlots DOs\n\n        Args:\n            df_freq_distribution (DataFrame): df with frequency data\n            lookback_period_in_days (int): lenght of lookback period in days\n            variablility (int | float): config param, the number of SD to define the upper and lower varibaility\n                limits: mean_size \u00b1 SD*variability, which daily_value should not exceed/be lower\n            lower_limit (int | float): absolute number which daily_value should not be lower\n            upper_limit (int | float): absolute number which daily_value can not exceed\n            type_of_data (str): which type of data raw or clean to check for QWs\n            measure_definition_canva (str): canva text to use for measure_definition column (see measure_definition.py)\n            cond_warn_variability_canva (str): canva text to use for lower_upper_variability cases\n                of data_size QWs (see conditions.py and warnings.py)\n            cond_warn_upper_lower_canva (str): canva text to use for lower_upper_limit cases\n                of data_size QWs (see conditions.py and warnings.py)\n\n        Returns:\n            tuple(DataFrame, DataFrame): a tuple, where first df\n                is used for warning log table, and the second df - for plots\n        \"\"\"\n        # based on type_of_data calculate either total daily initial freqeuncy or total daily final frequency\n        if type_of_data == \"raw\":\n            sum_column = ColNames.initial_frequency\n        else:\n            sum_column = ColNames.final_frequency\n        # fill in string canvases\n        measure_definition = measure_definition_canva.format(type_of_data=type_of_data)\n        cond_warn_variability = cond_warn_variability_canva.format(SD=variablility, type_of_data=type_of_data)\n        cond_warn_upper_lower = cond_warn_upper_lower_canva.format(\n            X=lower_limit, Y=upper_limit, type_of_data=type_of_data\n        )\n        # define lookback period\n        window = Window.orderBy(ColNames.date).rowsBetween(-lookback_period_in_days, -1)\n        # prepare data\n        # calculate mean and std over window, and based on them UCL - for UPPER_VARIABILITY check and LCL\n        #   - for LOWER_VARIABILITY check\n        # create empty array cond_warn_condition_value column to store information about qws\n        df_prep = (\n            df_freq_distribution.groupBy(ColNames.date)\n            .agg(psf.sum(sum_column).alias(ColNames.daily_value))\n            .withColumns(\n                {\n                    ColNames.average: psf.avg(ColNames.daily_value).over(window),\n                    \"std_freq\": psf.stddev(ColNames.daily_value).over(window),\n                    ColNames.UCL: psf.col(ColNames.average) + variablility * psf.col(\"std_freq\"),\n                    ColNames.LCL: psf.col(ColNames.average) - variablility * psf.col(\"std_freq\"),\n                    \"cond_warn_condition_value\": psf.array(),\n                }\n            )\n        )\n\n        df_prep = df_prep.cache()\n        # continue with QWs checks\n        # first filter data by period from [data_period_start-lookback_period_in_days, data_period_end]\n        # to [data_period_start, data_period_end]\n        # - a specified research period of QW\n        df_qw = df_prep.filter(psf.col(ColNames.date) &gt;= self.data_period_start)\n        # perform LOWER_VARIABILITY, UPPER_VARIABILITY, ABS_LOWER_LIMIT and ABS_UPPER_LIMIT checks\n        # if condition is met append information about condition-warning_text-condition_value as a string\n        # into array column 'cond_warn_condition_value'\n        df_qw = (\n            df_qw.withColumn(\n                \"cond_warn_condition_value\",\n                psf.when(\n                    (psf.col(ColNames.daily_value) &lt; psf.col(ColNames.LCL)),\n                    psf.array_append(\n                        psf.col(\"cond_warn_condition_value\"),\n                        psf.concat(psf.lit(f\"{cond_warn_variability}-\"), psf.col(ColNames.LCL).cast(\"string\")),\n                    ),\n                ).otherwise(psf.col(\"cond_warn_condition_value\")),\n            )\n            .withColumn(\n                \"cond_warn_condition_value\",\n                psf.when(\n                    (psf.col(ColNames.daily_value) &gt; psf.col(ColNames.UCL)),\n                    psf.array_append(\n                        psf.col(\"cond_warn_condition_value\"),\n                        psf.concat(psf.lit(f\"{cond_warn_variability}-\"), psf.col(ColNames.UCL).cast(\"string\")),\n                    ),\n                ).otherwise(psf.col(\"cond_warn_condition_value\")),\n            )\n            .withColumn(\n                \"cond_warn_condition_value\",\n                psf.when(\n                    (psf.col(ColNames.daily_value) &lt; psf.lit(lower_limit)),\n                    psf.array_append(\n                        psf.col(\"cond_warn_condition_value\"), psf.lit(f\"{cond_warn_upper_lower}-{str(lower_limit)}\")\n                    ),\n                ).otherwise(psf.col(\"cond_warn_condition_value\")),\n            )\n            .withColumn(\n                \"cond_warn_condition_value\",\n                psf.when(\n                    (psf.col(ColNames.daily_value) &gt; psf.lit(upper_limit)),\n                    psf.array_append(\n                        psf.col(\"cond_warn_condition_value\"), psf.lit(f\"{cond_warn_upper_lower}-{str(upper_limit)}\")\n                    ),\n                ).otherwise(psf.col(\"cond_warn_condition_value\")),\n            )\n        )\n        # explode array column 'cond_warn_condition_value'\n        df_qw = df_qw.withColumn(\"cond_warn_condition_value\", psf.explode(psf.col(\"cond_warn_condition_value\")))\n        # add some column constants\n        # split \"cond_warn_condition_value\" column into three: condition, wanring_text, conditon_value to\n        # match SilverEventDataSyntacticQualityWarningsLogTable.SCHEMA\n        # select only needed columns\n        df_qw = df_qw.withColumns(\n            {\n                ColNames.lookback_period: psf.lit(self.lookback_period),\n                ColNames.measure_definition: psf.lit(measure_definition),\n                ColNames.condition: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(0),\n                ColNames.warning_text: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(1),\n                ColNames.condition_value: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(2).cast(\"float\"),\n            }\n        ).select(self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsLogTable.ID].SCHEMA.fieldNames())\n\n        # save data for plots\n        # no filter by date because we need previous data of first days for plots\n        df_plots = df_prep.withColumns(\n            {\n                ColNames.lookback_period: psf.lit(self.lookback_period),\n                ColNames.type_of_qw: psf.lit(f\"{type_of_data}_data_size\"),\n            }\n        ).select(self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsForPlots.ID].SCHEMA.fieldNames())\n\n        return df_qw, df_plots\n\n    def error_rate_qw(\n        self,\n        df_freq_distribution: DataFrame,\n        lookback_period_in_days: int,\n        variables: list[str],\n        error_rate_over_average: int | float,\n        error_rate_upper_variability: int | float,\n        error_rate_upper_limit: int | float,\n        error_rate_measure_definition_canva: str = f\"{MeasureDefinitions.error_rate}\",\n        error_rate_cond_warn_over_average_canva: str = f\"{Conditions.error_rate_over_average}-{Warnings.error_rate_over_average}\",\n        error_rate_cond_warn_upper_variability_canva: str = f\"{Conditions.error_rate_upper_variability}-{Warnings.error_rate_upper_variability}\",\n        error_rate_cond_warn_upper_limit_canva: str = f\"{Conditions.error_rate_upper_limit}-{Warnings.error_rate_upper_limit}\",\n        save_data_for_plots: bool = False,\n    ) -&gt; tuple[DataFrame | None]:\n        \"\"\"\n        Prepare data for error rate calculation. First fill in different string canvas,\n            then define window of aggregation, and calculate error_rate over the window on follwoing formula:\n            (Total initial frequency - Total final frequency) / Total initial frequency*100.\n            Parse preprocessed input to self.rate_common_qw function which calculates three types\n                of QWs: OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT\n\n        Args:\n            df_freq_distribution (DataFrame): df with frequency data.\n            lookback_period_in_days (int): number of days prior to date of interest.\n            variables (list[str]): list of column names by which error rate is calculated, kind of granularity level\n            error_rate_over_average (int | float): config param, specifies the upper limit which a daily value\n                can not exceed its corresponding mean error rate\n            error_rate_upper_variability (int | float): config param, the number of SD to define the upper\n                varibaility limit: mean_rate + SD*error_rate_upper_variability, which error rate can't exceed\n            error_rate_upper_limit (int | float): absolute number which error rate can not exceed\n            error_rate_measure_definition_canva (str): canva text to use for measure_definition\n                column (see measure_definition.py)\n            error_rate_cond_warn_over_average_canva (str): canva text to use for over_average cases of\n                error_rate QWs (see conditions.py and warnings.py)\n            error_rate_cond_warn_upper_variability_canva (str): canva text to use for upper_variability cases of\n                error_rate QWs (see conditions.py and warnings.py)\n            error_rate_cond_warn_upper_limit_canva (str): canva text to use for upper_limit cases of error_rate\n                QWs (see conditions.py and warnings.py)\n            save_data_for_plots (bool): boolean, decide whether to store error rate and its corresponding average\n                and upper variability limit for plots, default False\n        Returns:\n            tuple(DataFrame | None): a tuple, where first df is used for warning log table,\n                and the second df - for plots (could be also None)\n        \"\"\"\n        # fill in all string comnstants with relevant information\n        # based on variables, error_rate_over_average, error_rate_upper_variability, error_rate_upper_limit args\n        error_rate_measure_definition = error_rate_measure_definition_canva.format(variables=\"&amp;\".join(variables))\n\n        error_rate_cond_warn_over_average = error_rate_cond_warn_over_average_canva.format(\n            variables=\"&amp;\".join(variables), X=error_rate_over_average\n        )\n        error_rate_cond_warn_upper_variability = error_rate_cond_warn_upper_variability_canva.format(\n            variables=\"&amp;\".join(variables), SD=error_rate_upper_variability\n        )\n        error_rate_cond_warn_upper_limit = error_rate_cond_warn_upper_limit_canva.format(\n            variables=\"&amp;\".join(variables), X=error_rate_upper_limit\n        )\n        # qws of error rate by date is calculated based on previous days\n        if variables == [ColNames.date]:\n            window = Window.orderBy(ColNames.date).rowsBetween(-lookback_period_in_days, -1)\n        else:\n            # qws of error rate by date and other colum(s) is calculated over all \"data points\" of the same date\n            window = Window.partitionBy(ColNames.date)\n        # calculate error rate, a.k.a daily_value\n        df_qw = (\n            df_freq_distribution.groupBy(*variables)\n            .agg(\n                psf.sum(ColNames.initial_frequency).alias(\"sum_init_freq\"),\n                psf.sum(ColNames.final_frequency).alias(\"sum_final_freq\"),\n            )\n            .withColumn(\n                ColNames.daily_value,\n                (psf.col(\"sum_init_freq\") - psf.col(\"sum_final_freq\")) / psf.col(\"sum_init_freq\") * 100,\n            )\n        )\n        # using self.rate_common_qw funciton calculate three types of QWs\n        #   -&gt; OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT\n        # return a tuple of (df_log, df_plots | None)\n        qw_result = self.rate_common_qw(\n            df_qw,\n            window,\n            error_rate_upper_variability,\n            error_rate_over_average,\n            error_rate_upper_limit,\n            error_rate_measure_definition,\n            error_rate_cond_warn_upper_variability,\n            error_rate_cond_warn_over_average,\n            error_rate_cond_warn_upper_limit,\n            save_data_for_plots,\n        )\n\n        return qw_result\n\n    def error_type_rate_qw(\n        self,\n        df_qa_by_column: DataFrame,\n        df_freq_distribution: DataFrame,\n        field_name: str | None,\n        error_type: str,\n        lookback_period_in_days: int,\n        error_type_rate_over_average: int | float,\n        error_type_rate_upper_variability: int | float,\n        error_type_rate_upper_limit: int | float,\n        error_type_rate_measure_definition_canva: str = f\"{MeasureDefinitions.error_type_rate}\",\n        error_type_rate_cond_warn_over_average_canva: str = f\"{Conditions.error_type_rate_over_average}-{Warnings.error_type_rate_over_average}\",\n        error_type_rate_cond_warn_upper_variability_canva: str = f\"{Conditions.error_type_rate_upper_variability}-{Warnings.error_type_rate_upper_variability}\",\n        error_type_rate_cond_warn_upper_limit_canva: str = f\"{Conditions.error_type_rate_upper_limit}-{Warnings.error_type_rate_upper_limit}\",\n    ) -&gt; tuple[DataFrame | None]:\n        \"\"\"\n        Prepare data for error type rate calculation. First fill in different string canvas, then based\n            on field name and error type calculate their corresponding error rate using formula:\n            number of errors of this error_type&amp;field_name combo / Total initial frequency *100 (BY DATE).\n            Parse preprocessed input along with window (which is a lookback period)\n            to self.rate_common_qw function which calculates three types of QWs:\n            OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT\n\n\n        Args:\n            df_qa_by_column (DataFrame): df with qa by column data.\n            df_freq_distribution (DataFrame): df with frequency data.\n            field_name (str | None): config param, the name of column of which to check error_type.\n            error_type (str): config param, the name of error type.\n            lookback_period_in_days (int): number of days prior to date of intrest.\n            error_type_rate_over_average (int | float): config param, specifies the upper limit over which daily\n                value can not exceed its corresponding mean error rate.\n            error_type_rate_upper_variability (int | float): config param, the number of SD to define the upper\n                varibaility limit: mean_rate + SD*error_type_rate_upper_variability, which daily value can not exceed\n            error_type_rate_upper_limit (int | float): absolute number which daily value can not exceed\n            error_type_rate_measure_definition_canva (str): canva text to use for measure_definition\n                column (see measure_definition.py)\n            error_type_rate_cond_warn_over_average_canva (str): canva text to use for over_average cases of\n                error_type_rate QWs (see conditions.py and warnings.py)\n            error_type_rate_cond_warn_upper_variability_canva (str): canva text to use for upper_variability\n                cases of error_type_rate QWs (see conditions.py and warnings.py)\n            error_type_rate_cond_warn_upper_limit_canva (str): canva text to use for upper_limit cases of\n                error_type_rate QWs (see conditions.py and warnings.py)\n        Returns:\n            tuple(DataFrame | None): a tuple, where first df is used for warning log table,\n                and the second df - for plots, but since save_data_for_plots always False, output=None\n        \"\"\"\n        # based on error_type, error_type_rate_over_average, error_type_rate_upper_variability,\n        #  error_type_rate_upper_limit\n        # fill in string canvases\n        colname_error_type, error_type_qw_name = self.dict_error_type_info[error_type]\n\n        error_type_rate_measure_definition = error_type_rate_measure_definition_canva.format(\n            error_type_name=error_type_qw_name, field_name=str(field_name)\n        )\n\n        error_type_rate_cond_warn_over_average = error_type_rate_cond_warn_over_average_canva.format(\n            error_type_name=error_type_qw_name, field_name=str(field_name), X=error_type_rate_over_average\n        )\n        error_type_rate_cond_warn_upper_variability = error_type_rate_cond_warn_upper_variability_canva.format(\n            error_type_name=error_type_qw_name, field_name=str(field_name), SD=error_type_rate_upper_variability\n        )\n        error_type_rate_cond_warn_upper_limit = error_type_rate_cond_warn_upper_limit_canva.format(\n            error_type_name=error_type_qw_name, field_name=str(field_name), X=error_type_rate_upper_limit\n        )\n        # for error_type that have more then one or applicable columns\n        # filter df_qa_by_column by field_name and error_type\n        if field_name is not None:\n            df_qa_by_column = df_qa_by_column.filter(\n                (psf.col(ColNames.variable) == field_name) &amp; (psf.col(ColNames.type_of_error) == colname_error_type)\n            ).select(ColNames.date, ColNames.value)\n        else:\n            # for error_types which technically do not belong specifically to one of event\n            # columns filter only by error_type (e.g. no_location error_type)\n            df_qa_by_column = df_qa_by_column.filter(psf.col(ColNames.type_of_error) == colname_error_type).select(\n                ColNames.date, ColNames.value\n            )\n        # calculate total daily initial frequency\n        df_freq_distribution = (\n            df_freq_distribution.groupby(ColNames.date)\n            .agg(psf.sum(ColNames.initial_frequency).alias(\"sum_init_freq\"))\n            .select(ColNames.date, \"sum_init_freq\")\n        )\n        # for each date combine two type of information number of errors and total daily initial frequency\n        df_combined = df_qa_by_column.join(df_freq_distribution, on=ColNames.date, how=\"inner\")\n        # for each date calculate error_type_rate, a.k.a daily_value\n        df_temp = df_combined.withColumn(\n            ColNames.daily_value, (psf.col(ColNames.value) / psf.col(\"sum_init_freq\")) * 100\n        )\n\n        # qws will be caluclated based on previous days\n        window = Window.orderBy(ColNames.date).rowsBetween(-lookback_period_in_days, -1)\n        # using self.rate_common_qw funciton calculate three types of QWs -&gt;\n        # OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT\n        # return a tuple of (df_log, None)\n        qw_result = self.rate_common_qw(\n            df_temp,\n            window,\n            error_type_rate_upper_variability,\n            error_type_rate_over_average,\n            error_type_rate_upper_limit,\n            error_type_rate_measure_definition,\n            error_type_rate_cond_warn_upper_variability,\n            error_type_rate_cond_warn_over_average,\n            error_type_rate_cond_warn_upper_limit,\n        )\n        return qw_result\n\n    def rate_common_qw(\n        self,\n        df_temp: DataFrame,\n        window: Window,\n        rate_upper_variability: int | float,\n        rate_over_average: int | float,\n        rate_upper_limit: int | float,\n        measure_definition: str,\n        cond_warn_upper_variability: str,\n        cond_warn_over_average: str,\n        cond_warn_upper_limit: str,\n        save_data_for_plots: bool = False,\n    ) -&gt; tuple[DataFrame | None]:\n        \"\"\"\n        Take input df with \"daily_value\" column, and calculates three types of QWs:\n        OVER_AVERAGE - for each row first based on specified window take mean of values, and then check if\n            daily_value exceeds mean by more than rate_over_average\n        UPPER_VARIABILITY - for each row using already calculated mean compute upper variability limit which is\n            mean + SD*rate_upper_variability, check if  daily_value exceeds it\n        ABS_UPPER_LIMIT - check if daily_value exceeds absolute number rate_upper_limit\n        All three QWs depend on specified thresholds, if daily_value exceeds one of calculated values it will\n            store cond-warn-condition_value information in array column,\n        some rows may have several QWs. In the end array column is exploded and cond-warn-condition_value\n            information is split into three corresponding columns.\n        The function returns almost ready df for SilverEventDataSyntacticQualityWarningsLogTable DO, and based\n            on save_data_for_plots arg returns either almost ready data for plots or None\n\n        Args:\n            df_temp (DataFrame): temprory data that must have daily_value column to\n                be used in further QW calculations\n            window (Window): a window within which perform aggregation\n            rate_upper_variability (int|float): config param, specifies the upper limit over which daily value\n                 can not exceed its corresponding mean error rate\n            rate_over_average (int|float): config param, the number of SD to define the upper varibaility\n                 limit: mean_rate + SD*error_rate_upper_variability, which daily value can not exceed\n            rate_upper_limit (int|float): absolute number which daily value can not exceed\n            measure_definition (str): canva text to use for measure_definition column (see measure_definition.py)\n            cond_warn_over_average (str): canva text to use for over_average cases (see conditions.py and warnings.py)\n            cond_warn_upper_variability (str): canva text to use for\n                upper_variability cases (see conditions.py and warnings.py)\n            cond_warn_upper_limit (str): canva text to use for upper_limit cases (see conditions.py and warnings.py)\n            save_data_for_plots (bool): boolean, decide whether to store daily_value and its corresponding average\n                and upper variability limit for plots. Defaults to False.\n        Returns:\n             tuple(DataFrame | None): a tuple, where first df is used for\n                warning log table, and the second df - for plots\n        \"\"\"\n        # prepare data\n        # calculate mean and std over window, and based on them UCL - for UPPER_VARIABILITY check\n        # ratio_perc - for OVER_AVERAGE check\n        # create empty array cond_warn_condition_value column to store inromation about qws\n        df_prep = df_temp.withColumns(\n            {\n                ColNames.average: psf.avg(ColNames.daily_value).over(window),\n                \"std_rate\": psf.stddev(ColNames.daily_value).over(window),\n                \"ratio_perc\": (psf.col(ColNames.daily_value) / psf.col(ColNames.average)) * 100,\n                ColNames.UCL: psf.col(ColNames.average) + rate_upper_variability * psf.col(\"std_rate\"),\n                \"cond_warn_condition_value\": psf.array(),\n            }\n        )\n        # if save_data_for_plots=True, add some new columns with constant values\n        # and select only applicable to SilverEventDataSyntacticQualityWarningsForPlots.SCHEMA\n        # else - return None\n        if save_data_for_plots:\n            df_prep = df_prep.cache()\n            df_plots = df_prep.withColumns(\n                {\n                    ColNames.lookback_period: psf.lit(self.lookback_period),\n                    ColNames.type_of_qw: psf.lit(\"error_rate\"),\n                    ColNames.LCL: psf.lit(None).cast(\"float\"),\n                }\n            ).select(\n                self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsForPlots.ID].SCHEMA.fieldNames()\n            )\n        else:\n            df_plots = None\n\n        # continue with QWs checks\n        # first filter temp data by period from [data_period_start-lookback_period_in_days, data_period_end]\n        #  to [data_period_start, data_period_end]\n        # filter is aaplied after plot block because the first days of research period needs previous data to plot\n        df_qw = df_prep.filter(psf.col(ColNames.date) &gt;= self.data_period_start)\n        # perform UPPER_VARIABILITY, OVER_AVERAGE, and ABS_UPPER_LIMIT checks\n        # if condition is met store information about condition-warning_text-condition_value as a string into\n        # array column 'cond_warn_condition_value'\n        df_qw = (\n            df_qw.withColumn(\n                \"cond_warn_condition_value\",\n                psf.when(\n                    psf.col(ColNames.daily_value) &gt; psf.col(ColNames.UCL),\n                    psf.array_append(\n                        psf.col(\"cond_warn_condition_value\"),\n                        psf.concat(psf.lit(f\"{cond_warn_upper_variability}-\"), psf.col(ColNames.UCL).cast(\"string\")),\n                    ),\n                ).otherwise(psf.col(\"cond_warn_condition_value\")),\n            )\n            .withColumn(\n                \"cond_warn_condition_value\",\n                psf.when(\n                    psf.col(\"ratio_perc\") &gt; psf.lit(100 + rate_over_average),\n                    psf.array_append(\n                        psf.col(\"cond_warn_condition_value\"),\n                        psf.lit(f\"{cond_warn_over_average}-{str(rate_over_average)}\"),\n                    ),\n                ).otherwise(psf.col(\"cond_warn_condition_value\")),\n            )\n            .withColumn(\n                \"cond_warn_condition_value\",\n                psf.when(\n                    psf.col(ColNames.daily_value) &gt; psf.lit(rate_upper_limit),\n                    psf.array_append(\n                        psf.col(\"cond_warn_condition_value\"),\n                        psf.lit(f\"{cond_warn_upper_limit}-{str(rate_upper_limit)}\"),\n                    ),\n                ).otherwise(psf.col(\"cond_warn_condition_value\")),\n            )\n        )\n        # explode array column 'cond_warn_condition_value'\n        df_qw = df_qw.withColumn(\"cond_warn_condition_value\", psf.explode(psf.col(\"cond_warn_condition_value\")))\n        # add some column constants\n        # split \"cond_warn_condition_value\" column into three: condition, wanring_text, conditon_value to match\n        # SilverEventDataSyntacticQualityWarningsLogTable.SCHEMA\n        # select only needed columns\n        df_qw = df_qw.withColumns(\n            {\n                ColNames.lookback_period: psf.lit(self.lookback_period),\n                ColNames.measure_definition: psf.lit(measure_definition),\n                ColNames.condition: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(0),\n                ColNames.warning_text: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(1),\n                ColNames.condition_value: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(2).cast(\"float\"),\n            }\n        ).select(self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsLogTable.ID].SCHEMA.fieldNames())\n\n        return (df_qw, df_plots)\n\n    def save_quality_warnings_output(\n        self,\n        dfs_qw: list[DataFrame | None],\n        output_do: SilverEventDataSyntacticQualityWarningsLogTable | SilverEventDataSyntacticQualityWarningsForPlots,\n    ):\n        \"\"\"\n        Concatenates all elements in dfs_qw list, adjustes to schema of output_do, and using write\n            method of output_do stores the result\n\n        Args:\n            dfs_qw (list): _description_\n            output_do (SilverEventDataSyntacticQualityWarningsLogTable | \\\n                SilverEventDataSyntacticQualityWarningsForPlots): _description_\n        \"\"\"\n\n        output_do.df = reduce(lambda x, y: x.union(y), dfs_qw)\n\n        output_do.df = self.spark.createDataFrame(output_do.df.rdd, output_do.SCHEMA)\n\n        output_do.write()\n\n    def save_quality_warnings_log_table(self, dfs_qw):\n\n        self.save_quality_warnings_output(\n            dfs_qw, self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsLogTable.ID]\n        )\n\n    def save_quality_warnings_for_plots(self, dfs_qw):\n\n        self.save_quality_warnings_output(\n            dfs_qw, self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsForPlots.ID]\n        )\n</code></pre>"},{"location":"reference/components/quality/event_quality_warnings/event_quality_warnings/#components.quality.event_quality_warnings.event_quality_warnings.EventQualityWarnings.data_size_qw","title":"<code>data_size_qw(df_freq_distribution, lookback_period_in_days, variablility, lower_limit, upper_limit, type_of_data, measure_definition_canva=f'{MeasureDefinitions.size_data}', cond_warn_variability_canva=f'{Conditions.size_data_variability}-{Warnings.size_data_variability}', cond_warn_upper_lower_canva=f'{Conditions.size_data_upper_lower}-{Warnings.size_data_upper_lower}')</code>","text":"<p>A unified function to check both raw and clean data sizes, calculates four types of QWs: LOWER_VARIABILITY - for each row using calculated mean and std compute lower variability limit     which is mean - SDvariability, check if  daily_value is lower tan limit UPPER_VARIABILITY - for each row using calculated mean and std compute upper variability limit      which is mean + SDvariability, check if  daily_value exceeds limit ABS_LOWER_LIMIT - check if daily_value is lower than absolute number lower_limit ABS_UPPER_LIMIT - check if daily_value exceeds absolute number upper_limit All four QWs depend on thresholds, in case if statement condition is met -&gt; store cond-warn-condition_value     information in array column, some rows may have several QWs. In the end array column is exploded and cond-warn-consition_value information     is split into three corresponding columns. The function returns almost ready dfs for SilverEventDataSyntacticQualityWarningsLogTable      and SilverEventDataSyntacticQualityWarningsForPlots DOs</p> <p>Parameters:</p> Name Type Description Default <code>df_freq_distribution</code> <code>DataFrame</code> <p>df with frequency data</p> required <code>lookback_period_in_days</code> <code>int</code> <p>lenght of lookback period in days</p> required <code>variablility</code> <code>int | float</code> <p>config param, the number of SD to define the upper and lower varibaility limits: mean_size \u00b1 SD*variability, which daily_value should not exceed/be lower</p> required <code>lower_limit</code> <code>int | float</code> <p>absolute number which daily_value should not be lower</p> required <code>upper_limit</code> <code>int | float</code> <p>absolute number which daily_value can not exceed</p> required <code>type_of_data</code> <code>str</code> <p>which type of data raw or clean to check for QWs</p> required <code>measure_definition_canva</code> <code>str</code> <p>canva text to use for measure_definition column (see measure_definition.py)</p> <code>f'{size_data}'</code> <code>cond_warn_variability_canva</code> <code>str</code> <p>canva text to use for lower_upper_variability cases of data_size QWs (see conditions.py and warnings.py)</p> <code>f'{size_data_variability}-{size_data_variability}'</code> <code>cond_warn_upper_lower_canva</code> <code>str</code> <p>canva text to use for lower_upper_limit cases of data_size QWs (see conditions.py and warnings.py)</p> <code>f'{size_data_upper_lower}-{size_data_upper_lower}'</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>(DataFrame, DataFrame)</code> <p>a tuple, where first df is used for warning log table, and the second df - for plots</p> Source code in <code>multimno/components/quality/event_quality_warnings/event_quality_warnings.py</code> <pre><code>def data_size_qw(\n    self,\n    df_freq_distribution: DataFrame,\n    lookback_period_in_days: int,\n    variablility: int | float,\n    lower_limit: int | float,\n    upper_limit: int | float,\n    type_of_data: str,\n    measure_definition_canva: str = f\"{MeasureDefinitions.size_data}\",\n    cond_warn_variability_canva: str = f\"{Conditions.size_data_variability}-{Warnings.size_data_variability}\",\n    cond_warn_upper_lower_canva: str = f\"{Conditions.size_data_upper_lower}-{Warnings.size_data_upper_lower}\",\n) -&gt; tuple[DataFrame]:\n    \"\"\"\n    A unified function to check both raw and clean data sizes, calculates four types of QWs:\n    LOWER_VARIABILITY - for each row using calculated mean and std compute lower variability limit\n        which is mean - SD*variability, check if  daily_value is lower tan limit\n    UPPER_VARIABILITY - for each row using calculated mean and std compute upper variability limit\n         which is mean + SD*variability, check if  daily_value exceeds limit\n    ABS_LOWER_LIMIT - check if daily_value is lower than absolute number lower_limit\n    ABS_UPPER_LIMIT - check if daily_value exceeds absolute number upper_limit\n    All four QWs depend on thresholds, in case if statement condition is met -&gt; store cond-warn-condition_value\n        information in array column,\n    some rows may have several QWs. In the end array column is exploded and cond-warn-consition_value information\n        is split into three corresponding columns.\n    The function returns almost ready dfs for SilverEventDataSyntacticQualityWarningsLogTable\n         and SilverEventDataSyntacticQualityWarningsForPlots DOs\n\n    Args:\n        df_freq_distribution (DataFrame): df with frequency data\n        lookback_period_in_days (int): lenght of lookback period in days\n        variablility (int | float): config param, the number of SD to define the upper and lower varibaility\n            limits: mean_size \u00b1 SD*variability, which daily_value should not exceed/be lower\n        lower_limit (int | float): absolute number which daily_value should not be lower\n        upper_limit (int | float): absolute number which daily_value can not exceed\n        type_of_data (str): which type of data raw or clean to check for QWs\n        measure_definition_canva (str): canva text to use for measure_definition column (see measure_definition.py)\n        cond_warn_variability_canva (str): canva text to use for lower_upper_variability cases\n            of data_size QWs (see conditions.py and warnings.py)\n        cond_warn_upper_lower_canva (str): canva text to use for lower_upper_limit cases\n            of data_size QWs (see conditions.py and warnings.py)\n\n    Returns:\n        tuple(DataFrame, DataFrame): a tuple, where first df\n            is used for warning log table, and the second df - for plots\n    \"\"\"\n    # based on type_of_data calculate either total daily initial freqeuncy or total daily final frequency\n    if type_of_data == \"raw\":\n        sum_column = ColNames.initial_frequency\n    else:\n        sum_column = ColNames.final_frequency\n    # fill in string canvases\n    measure_definition = measure_definition_canva.format(type_of_data=type_of_data)\n    cond_warn_variability = cond_warn_variability_canva.format(SD=variablility, type_of_data=type_of_data)\n    cond_warn_upper_lower = cond_warn_upper_lower_canva.format(\n        X=lower_limit, Y=upper_limit, type_of_data=type_of_data\n    )\n    # define lookback period\n    window = Window.orderBy(ColNames.date).rowsBetween(-lookback_period_in_days, -1)\n    # prepare data\n    # calculate mean and std over window, and based on them UCL - for UPPER_VARIABILITY check and LCL\n    #   - for LOWER_VARIABILITY check\n    # create empty array cond_warn_condition_value column to store information about qws\n    df_prep = (\n        df_freq_distribution.groupBy(ColNames.date)\n        .agg(psf.sum(sum_column).alias(ColNames.daily_value))\n        .withColumns(\n            {\n                ColNames.average: psf.avg(ColNames.daily_value).over(window),\n                \"std_freq\": psf.stddev(ColNames.daily_value).over(window),\n                ColNames.UCL: psf.col(ColNames.average) + variablility * psf.col(\"std_freq\"),\n                ColNames.LCL: psf.col(ColNames.average) - variablility * psf.col(\"std_freq\"),\n                \"cond_warn_condition_value\": psf.array(),\n            }\n        )\n    )\n\n    df_prep = df_prep.cache()\n    # continue with QWs checks\n    # first filter data by period from [data_period_start-lookback_period_in_days, data_period_end]\n    # to [data_period_start, data_period_end]\n    # - a specified research period of QW\n    df_qw = df_prep.filter(psf.col(ColNames.date) &gt;= self.data_period_start)\n    # perform LOWER_VARIABILITY, UPPER_VARIABILITY, ABS_LOWER_LIMIT and ABS_UPPER_LIMIT checks\n    # if condition is met append information about condition-warning_text-condition_value as a string\n    # into array column 'cond_warn_condition_value'\n    df_qw = (\n        df_qw.withColumn(\n            \"cond_warn_condition_value\",\n            psf.when(\n                (psf.col(ColNames.daily_value) &lt; psf.col(ColNames.LCL)),\n                psf.array_append(\n                    psf.col(\"cond_warn_condition_value\"),\n                    psf.concat(psf.lit(f\"{cond_warn_variability}-\"), psf.col(ColNames.LCL).cast(\"string\")),\n                ),\n            ).otherwise(psf.col(\"cond_warn_condition_value\")),\n        )\n        .withColumn(\n            \"cond_warn_condition_value\",\n            psf.when(\n                (psf.col(ColNames.daily_value) &gt; psf.col(ColNames.UCL)),\n                psf.array_append(\n                    psf.col(\"cond_warn_condition_value\"),\n                    psf.concat(psf.lit(f\"{cond_warn_variability}-\"), psf.col(ColNames.UCL).cast(\"string\")),\n                ),\n            ).otherwise(psf.col(\"cond_warn_condition_value\")),\n        )\n        .withColumn(\n            \"cond_warn_condition_value\",\n            psf.when(\n                (psf.col(ColNames.daily_value) &lt; psf.lit(lower_limit)),\n                psf.array_append(\n                    psf.col(\"cond_warn_condition_value\"), psf.lit(f\"{cond_warn_upper_lower}-{str(lower_limit)}\")\n                ),\n            ).otherwise(psf.col(\"cond_warn_condition_value\")),\n        )\n        .withColumn(\n            \"cond_warn_condition_value\",\n            psf.when(\n                (psf.col(ColNames.daily_value) &gt; psf.lit(upper_limit)),\n                psf.array_append(\n                    psf.col(\"cond_warn_condition_value\"), psf.lit(f\"{cond_warn_upper_lower}-{str(upper_limit)}\")\n                ),\n            ).otherwise(psf.col(\"cond_warn_condition_value\")),\n        )\n    )\n    # explode array column 'cond_warn_condition_value'\n    df_qw = df_qw.withColumn(\"cond_warn_condition_value\", psf.explode(psf.col(\"cond_warn_condition_value\")))\n    # add some column constants\n    # split \"cond_warn_condition_value\" column into three: condition, wanring_text, conditon_value to\n    # match SilverEventDataSyntacticQualityWarningsLogTable.SCHEMA\n    # select only needed columns\n    df_qw = df_qw.withColumns(\n        {\n            ColNames.lookback_period: psf.lit(self.lookback_period),\n            ColNames.measure_definition: psf.lit(measure_definition),\n            ColNames.condition: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(0),\n            ColNames.warning_text: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(1),\n            ColNames.condition_value: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(2).cast(\"float\"),\n        }\n    ).select(self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsLogTable.ID].SCHEMA.fieldNames())\n\n    # save data for plots\n    # no filter by date because we need previous data of first days for plots\n    df_plots = df_prep.withColumns(\n        {\n            ColNames.lookback_period: psf.lit(self.lookback_period),\n            ColNames.type_of_qw: psf.lit(f\"{type_of_data}_data_size\"),\n        }\n    ).select(self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsForPlots.ID].SCHEMA.fieldNames())\n\n    return df_qw, df_plots\n</code></pre>"},{"location":"reference/components/quality/event_quality_warnings/event_quality_warnings/#components.quality.event_quality_warnings.event_quality_warnings.EventQualityWarnings.error_rate_qw","title":"<code>error_rate_qw(df_freq_distribution, lookback_period_in_days, variables, error_rate_over_average, error_rate_upper_variability, error_rate_upper_limit, error_rate_measure_definition_canva=f'{MeasureDefinitions.error_rate}', error_rate_cond_warn_over_average_canva=f'{Conditions.error_rate_over_average}-{Warnings.error_rate_over_average}', error_rate_cond_warn_upper_variability_canva=f'{Conditions.error_rate_upper_variability}-{Warnings.error_rate_upper_variability}', error_rate_cond_warn_upper_limit_canva=f'{Conditions.error_rate_upper_limit}-{Warnings.error_rate_upper_limit}', save_data_for_plots=False)</code>","text":"<p>Prepare data for error rate calculation. First fill in different string canvas,     then define window of aggregation, and calculate error_rate over the window on follwoing formula:     (Total initial frequency - Total final frequency) / Total initial frequency*100.     Parse preprocessed input to self.rate_common_qw function which calculates three types         of QWs: OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT</p> <p>Parameters:</p> Name Type Description Default <code>df_freq_distribution</code> <code>DataFrame</code> <p>df with frequency data.</p> required <code>lookback_period_in_days</code> <code>int</code> <p>number of days prior to date of interest.</p> required <code>variables</code> <code>list[str]</code> <p>list of column names by which error rate is calculated, kind of granularity level</p> required <code>error_rate_over_average</code> <code>int | float</code> <p>config param, specifies the upper limit which a daily value can not exceed its corresponding mean error rate</p> required <code>error_rate_upper_variability</code> <code>int | float</code> <p>config param, the number of SD to define the upper varibaility limit: mean_rate + SD*error_rate_upper_variability, which error rate can't exceed</p> required <code>error_rate_upper_limit</code> <code>int | float</code> <p>absolute number which error rate can not exceed</p> required <code>error_rate_measure_definition_canva</code> <code>str</code> <p>canva text to use for measure_definition column (see measure_definition.py)</p> <code>f'{error_rate}'</code> <code>error_rate_cond_warn_over_average_canva</code> <code>str</code> <p>canva text to use for over_average cases of error_rate QWs (see conditions.py and warnings.py)</p> <code>f'{error_rate_over_average}-{error_rate_over_average}'</code> <code>error_rate_cond_warn_upper_variability_canva</code> <code>str</code> <p>canva text to use for upper_variability cases of error_rate QWs (see conditions.py and warnings.py)</p> <code>f'{error_rate_upper_variability}-{error_rate_upper_variability}'</code> <code>error_rate_cond_warn_upper_limit_canva</code> <code>str</code> <p>canva text to use for upper_limit cases of error_rate QWs (see conditions.py and warnings.py)</p> <code>f'{error_rate_upper_limit}-{error_rate_upper_limit}'</code> <code>save_data_for_plots</code> <code>bool</code> <p>boolean, decide whether to store error rate and its corresponding average and upper variability limit for plots, default False</p> <code>False</code> <p>Returns:     tuple(DataFrame | None): a tuple, where first df is used for warning log table,         and the second df - for plots (could be also None)</p> Source code in <code>multimno/components/quality/event_quality_warnings/event_quality_warnings.py</code> <pre><code>def error_rate_qw(\n    self,\n    df_freq_distribution: DataFrame,\n    lookback_period_in_days: int,\n    variables: list[str],\n    error_rate_over_average: int | float,\n    error_rate_upper_variability: int | float,\n    error_rate_upper_limit: int | float,\n    error_rate_measure_definition_canva: str = f\"{MeasureDefinitions.error_rate}\",\n    error_rate_cond_warn_over_average_canva: str = f\"{Conditions.error_rate_over_average}-{Warnings.error_rate_over_average}\",\n    error_rate_cond_warn_upper_variability_canva: str = f\"{Conditions.error_rate_upper_variability}-{Warnings.error_rate_upper_variability}\",\n    error_rate_cond_warn_upper_limit_canva: str = f\"{Conditions.error_rate_upper_limit}-{Warnings.error_rate_upper_limit}\",\n    save_data_for_plots: bool = False,\n) -&gt; tuple[DataFrame | None]:\n    \"\"\"\n    Prepare data for error rate calculation. First fill in different string canvas,\n        then define window of aggregation, and calculate error_rate over the window on follwoing formula:\n        (Total initial frequency - Total final frequency) / Total initial frequency*100.\n        Parse preprocessed input to self.rate_common_qw function which calculates three types\n            of QWs: OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT\n\n    Args:\n        df_freq_distribution (DataFrame): df with frequency data.\n        lookback_period_in_days (int): number of days prior to date of interest.\n        variables (list[str]): list of column names by which error rate is calculated, kind of granularity level\n        error_rate_over_average (int | float): config param, specifies the upper limit which a daily value\n            can not exceed its corresponding mean error rate\n        error_rate_upper_variability (int | float): config param, the number of SD to define the upper\n            varibaility limit: mean_rate + SD*error_rate_upper_variability, which error rate can't exceed\n        error_rate_upper_limit (int | float): absolute number which error rate can not exceed\n        error_rate_measure_definition_canva (str): canva text to use for measure_definition\n            column (see measure_definition.py)\n        error_rate_cond_warn_over_average_canva (str): canva text to use for over_average cases of\n            error_rate QWs (see conditions.py and warnings.py)\n        error_rate_cond_warn_upper_variability_canva (str): canva text to use for upper_variability cases of\n            error_rate QWs (see conditions.py and warnings.py)\n        error_rate_cond_warn_upper_limit_canva (str): canva text to use for upper_limit cases of error_rate\n            QWs (see conditions.py and warnings.py)\n        save_data_for_plots (bool): boolean, decide whether to store error rate and its corresponding average\n            and upper variability limit for plots, default False\n    Returns:\n        tuple(DataFrame | None): a tuple, where first df is used for warning log table,\n            and the second df - for plots (could be also None)\n    \"\"\"\n    # fill in all string comnstants with relevant information\n    # based on variables, error_rate_over_average, error_rate_upper_variability, error_rate_upper_limit args\n    error_rate_measure_definition = error_rate_measure_definition_canva.format(variables=\"&amp;\".join(variables))\n\n    error_rate_cond_warn_over_average = error_rate_cond_warn_over_average_canva.format(\n        variables=\"&amp;\".join(variables), X=error_rate_over_average\n    )\n    error_rate_cond_warn_upper_variability = error_rate_cond_warn_upper_variability_canva.format(\n        variables=\"&amp;\".join(variables), SD=error_rate_upper_variability\n    )\n    error_rate_cond_warn_upper_limit = error_rate_cond_warn_upper_limit_canva.format(\n        variables=\"&amp;\".join(variables), X=error_rate_upper_limit\n    )\n    # qws of error rate by date is calculated based on previous days\n    if variables == [ColNames.date]:\n        window = Window.orderBy(ColNames.date).rowsBetween(-lookback_period_in_days, -1)\n    else:\n        # qws of error rate by date and other colum(s) is calculated over all \"data points\" of the same date\n        window = Window.partitionBy(ColNames.date)\n    # calculate error rate, a.k.a daily_value\n    df_qw = (\n        df_freq_distribution.groupBy(*variables)\n        .agg(\n            psf.sum(ColNames.initial_frequency).alias(\"sum_init_freq\"),\n            psf.sum(ColNames.final_frequency).alias(\"sum_final_freq\"),\n        )\n        .withColumn(\n            ColNames.daily_value,\n            (psf.col(\"sum_init_freq\") - psf.col(\"sum_final_freq\")) / psf.col(\"sum_init_freq\") * 100,\n        )\n    )\n    # using self.rate_common_qw funciton calculate three types of QWs\n    #   -&gt; OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT\n    # return a tuple of (df_log, df_plots | None)\n    qw_result = self.rate_common_qw(\n        df_qw,\n        window,\n        error_rate_upper_variability,\n        error_rate_over_average,\n        error_rate_upper_limit,\n        error_rate_measure_definition,\n        error_rate_cond_warn_upper_variability,\n        error_rate_cond_warn_over_average,\n        error_rate_cond_warn_upper_limit,\n        save_data_for_plots,\n    )\n\n    return qw_result\n</code></pre>"},{"location":"reference/components/quality/event_quality_warnings/event_quality_warnings/#components.quality.event_quality_warnings.event_quality_warnings.EventQualityWarnings.error_type_rate_qw","title":"<code>error_type_rate_qw(df_qa_by_column, df_freq_distribution, field_name, error_type, lookback_period_in_days, error_type_rate_over_average, error_type_rate_upper_variability, error_type_rate_upper_limit, error_type_rate_measure_definition_canva=f'{MeasureDefinitions.error_type_rate}', error_type_rate_cond_warn_over_average_canva=f'{Conditions.error_type_rate_over_average}-{Warnings.error_type_rate_over_average}', error_type_rate_cond_warn_upper_variability_canva=f'{Conditions.error_type_rate_upper_variability}-{Warnings.error_type_rate_upper_variability}', error_type_rate_cond_warn_upper_limit_canva=f'{Conditions.error_type_rate_upper_limit}-{Warnings.error_type_rate_upper_limit}')</code>","text":"<p>Prepare data for error type rate calculation. First fill in different string canvas, then based     on field name and error type calculate their corresponding error rate using formula:     number of errors of this error_type&amp;field_name combo / Total initial frequency *100 (BY DATE).     Parse preprocessed input along with window (which is a lookback period)     to self.rate_common_qw function which calculates three types of QWs:     OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT</p> <p>Parameters:</p> Name Type Description Default <code>df_qa_by_column</code> <code>DataFrame</code> <p>df with qa by column data.</p> required <code>df_freq_distribution</code> <code>DataFrame</code> <p>df with frequency data.</p> required <code>field_name</code> <code>str | None</code> <p>config param, the name of column of which to check error_type.</p> required <code>error_type</code> <code>str</code> <p>config param, the name of error type.</p> required <code>lookback_period_in_days</code> <code>int</code> <p>number of days prior to date of intrest.</p> required <code>error_type_rate_over_average</code> <code>int | float</code> <p>config param, specifies the upper limit over which daily value can not exceed its corresponding mean error rate.</p> required <code>error_type_rate_upper_variability</code> <code>int | float</code> <p>config param, the number of SD to define the upper varibaility limit: mean_rate + SD*error_type_rate_upper_variability, which daily value can not exceed</p> required <code>error_type_rate_upper_limit</code> <code>int | float</code> <p>absolute number which daily value can not exceed</p> required <code>error_type_rate_measure_definition_canva</code> <code>str</code> <p>canva text to use for measure_definition column (see measure_definition.py)</p> <code>f'{error_type_rate}'</code> <code>error_type_rate_cond_warn_over_average_canva</code> <code>str</code> <p>canva text to use for over_average cases of error_type_rate QWs (see conditions.py and warnings.py)</p> <code>f'{error_type_rate_over_average}-{error_type_rate_over_average}'</code> <code>error_type_rate_cond_warn_upper_variability_canva</code> <code>str</code> <p>canva text to use for upper_variability cases of error_type_rate QWs (see conditions.py and warnings.py)</p> <code>f'{error_type_rate_upper_variability}-{error_type_rate_upper_variability}'</code> <code>error_type_rate_cond_warn_upper_limit_canva</code> <code>str</code> <p>canva text to use for upper_limit cases of error_type_rate QWs (see conditions.py and warnings.py)</p> <code>f'{error_type_rate_upper_limit}-{error_type_rate_upper_limit}'</code> <p>Returns:     tuple(DataFrame | None): a tuple, where first df is used for warning log table,         and the second df - for plots, but since save_data_for_plots always False, output=None</p> Source code in <code>multimno/components/quality/event_quality_warnings/event_quality_warnings.py</code> <pre><code>def error_type_rate_qw(\n    self,\n    df_qa_by_column: DataFrame,\n    df_freq_distribution: DataFrame,\n    field_name: str | None,\n    error_type: str,\n    lookback_period_in_days: int,\n    error_type_rate_over_average: int | float,\n    error_type_rate_upper_variability: int | float,\n    error_type_rate_upper_limit: int | float,\n    error_type_rate_measure_definition_canva: str = f\"{MeasureDefinitions.error_type_rate}\",\n    error_type_rate_cond_warn_over_average_canva: str = f\"{Conditions.error_type_rate_over_average}-{Warnings.error_type_rate_over_average}\",\n    error_type_rate_cond_warn_upper_variability_canva: str = f\"{Conditions.error_type_rate_upper_variability}-{Warnings.error_type_rate_upper_variability}\",\n    error_type_rate_cond_warn_upper_limit_canva: str = f\"{Conditions.error_type_rate_upper_limit}-{Warnings.error_type_rate_upper_limit}\",\n) -&gt; tuple[DataFrame | None]:\n    \"\"\"\n    Prepare data for error type rate calculation. First fill in different string canvas, then based\n        on field name and error type calculate their corresponding error rate using formula:\n        number of errors of this error_type&amp;field_name combo / Total initial frequency *100 (BY DATE).\n        Parse preprocessed input along with window (which is a lookback period)\n        to self.rate_common_qw function which calculates three types of QWs:\n        OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT\n\n\n    Args:\n        df_qa_by_column (DataFrame): df with qa by column data.\n        df_freq_distribution (DataFrame): df with frequency data.\n        field_name (str | None): config param, the name of column of which to check error_type.\n        error_type (str): config param, the name of error type.\n        lookback_period_in_days (int): number of days prior to date of intrest.\n        error_type_rate_over_average (int | float): config param, specifies the upper limit over which daily\n            value can not exceed its corresponding mean error rate.\n        error_type_rate_upper_variability (int | float): config param, the number of SD to define the upper\n            varibaility limit: mean_rate + SD*error_type_rate_upper_variability, which daily value can not exceed\n        error_type_rate_upper_limit (int | float): absolute number which daily value can not exceed\n        error_type_rate_measure_definition_canva (str): canva text to use for measure_definition\n            column (see measure_definition.py)\n        error_type_rate_cond_warn_over_average_canva (str): canva text to use for over_average cases of\n            error_type_rate QWs (see conditions.py and warnings.py)\n        error_type_rate_cond_warn_upper_variability_canva (str): canva text to use for upper_variability\n            cases of error_type_rate QWs (see conditions.py and warnings.py)\n        error_type_rate_cond_warn_upper_limit_canva (str): canva text to use for upper_limit cases of\n            error_type_rate QWs (see conditions.py and warnings.py)\n    Returns:\n        tuple(DataFrame | None): a tuple, where first df is used for warning log table,\n            and the second df - for plots, but since save_data_for_plots always False, output=None\n    \"\"\"\n    # based on error_type, error_type_rate_over_average, error_type_rate_upper_variability,\n    #  error_type_rate_upper_limit\n    # fill in string canvases\n    colname_error_type, error_type_qw_name = self.dict_error_type_info[error_type]\n\n    error_type_rate_measure_definition = error_type_rate_measure_definition_canva.format(\n        error_type_name=error_type_qw_name, field_name=str(field_name)\n    )\n\n    error_type_rate_cond_warn_over_average = error_type_rate_cond_warn_over_average_canva.format(\n        error_type_name=error_type_qw_name, field_name=str(field_name), X=error_type_rate_over_average\n    )\n    error_type_rate_cond_warn_upper_variability = error_type_rate_cond_warn_upper_variability_canva.format(\n        error_type_name=error_type_qw_name, field_name=str(field_name), SD=error_type_rate_upper_variability\n    )\n    error_type_rate_cond_warn_upper_limit = error_type_rate_cond_warn_upper_limit_canva.format(\n        error_type_name=error_type_qw_name, field_name=str(field_name), X=error_type_rate_upper_limit\n    )\n    # for error_type that have more then one or applicable columns\n    # filter df_qa_by_column by field_name and error_type\n    if field_name is not None:\n        df_qa_by_column = df_qa_by_column.filter(\n            (psf.col(ColNames.variable) == field_name) &amp; (psf.col(ColNames.type_of_error) == colname_error_type)\n        ).select(ColNames.date, ColNames.value)\n    else:\n        # for error_types which technically do not belong specifically to one of event\n        # columns filter only by error_type (e.g. no_location error_type)\n        df_qa_by_column = df_qa_by_column.filter(psf.col(ColNames.type_of_error) == colname_error_type).select(\n            ColNames.date, ColNames.value\n        )\n    # calculate total daily initial frequency\n    df_freq_distribution = (\n        df_freq_distribution.groupby(ColNames.date)\n        .agg(psf.sum(ColNames.initial_frequency).alias(\"sum_init_freq\"))\n        .select(ColNames.date, \"sum_init_freq\")\n    )\n    # for each date combine two type of information number of errors and total daily initial frequency\n    df_combined = df_qa_by_column.join(df_freq_distribution, on=ColNames.date, how=\"inner\")\n    # for each date calculate error_type_rate, a.k.a daily_value\n    df_temp = df_combined.withColumn(\n        ColNames.daily_value, (psf.col(ColNames.value) / psf.col(\"sum_init_freq\")) * 100\n    )\n\n    # qws will be caluclated based on previous days\n    window = Window.orderBy(ColNames.date).rowsBetween(-lookback_period_in_days, -1)\n    # using self.rate_common_qw funciton calculate three types of QWs -&gt;\n    # OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT\n    # return a tuple of (df_log, None)\n    qw_result = self.rate_common_qw(\n        df_temp,\n        window,\n        error_type_rate_upper_variability,\n        error_type_rate_over_average,\n        error_type_rate_upper_limit,\n        error_type_rate_measure_definition,\n        error_type_rate_cond_warn_upper_variability,\n        error_type_rate_cond_warn_over_average,\n        error_type_rate_cond_warn_upper_limit,\n    )\n    return qw_result\n</code></pre>"},{"location":"reference/components/quality/event_quality_warnings/event_quality_warnings/#components.quality.event_quality_warnings.event_quality_warnings.EventQualityWarnings.rate_common_qw","title":"<code>rate_common_qw(df_temp, window, rate_upper_variability, rate_over_average, rate_upper_limit, measure_definition, cond_warn_upper_variability, cond_warn_over_average, cond_warn_upper_limit, save_data_for_plots=False)</code>","text":"<p>Take input df with \"daily_value\" column, and calculates three types of QWs: OVER_AVERAGE - for each row first based on specified window take mean of values, and then check if     daily_value exceeds mean by more than rate_over_average UPPER_VARIABILITY - for each row using already calculated mean compute upper variability limit which is     mean + SD*rate_upper_variability, check if  daily_value exceeds it ABS_UPPER_LIMIT - check if daily_value exceeds absolute number rate_upper_limit All three QWs depend on specified thresholds, if daily_value exceeds one of calculated values it will     store cond-warn-condition_value information in array column, some rows may have several QWs. In the end array column is exploded and cond-warn-condition_value     information is split into three corresponding columns. The function returns almost ready df for SilverEventDataSyntacticQualityWarningsLogTable DO, and based     on save_data_for_plots arg returns either almost ready data for plots or None</p> <p>Parameters:</p> Name Type Description Default <code>df_temp</code> <code>DataFrame</code> <p>temprory data that must have daily_value column to be used in further QW calculations</p> required <code>window</code> <code>Window</code> <p>a window within which perform aggregation</p> required <code>rate_upper_variability</code> <code>int | float</code> <p>config param, specifies the upper limit over which daily value  can not exceed its corresponding mean error rate</p> required <code>rate_over_average</code> <code>int | float</code> <p>config param, the number of SD to define the upper varibaility  limit: mean_rate + SD*error_rate_upper_variability, which daily value can not exceed</p> required <code>rate_upper_limit</code> <code>int | float</code> <p>absolute number which daily value can not exceed</p> required <code>measure_definition</code> <code>str</code> <p>canva text to use for measure_definition column (see measure_definition.py)</p> required <code>cond_warn_over_average</code> <code>str</code> <p>canva text to use for over_average cases (see conditions.py and warnings.py)</p> required <code>cond_warn_upper_variability</code> <code>str</code> <p>canva text to use for upper_variability cases (see conditions.py and warnings.py)</p> required <code>cond_warn_upper_limit</code> <code>str</code> <p>canva text to use for upper_limit cases (see conditions.py and warnings.py)</p> required <code>save_data_for_plots</code> <code>bool</code> <p>boolean, decide whether to store daily_value and its corresponding average and upper variability limit for plots. Defaults to False.</p> <code>False</code> <p>Returns:      tuple(DataFrame | None): a tuple, where first df is used for         warning log table, and the second df - for plots</p> Source code in <code>multimno/components/quality/event_quality_warnings/event_quality_warnings.py</code> <pre><code>def rate_common_qw(\n    self,\n    df_temp: DataFrame,\n    window: Window,\n    rate_upper_variability: int | float,\n    rate_over_average: int | float,\n    rate_upper_limit: int | float,\n    measure_definition: str,\n    cond_warn_upper_variability: str,\n    cond_warn_over_average: str,\n    cond_warn_upper_limit: str,\n    save_data_for_plots: bool = False,\n) -&gt; tuple[DataFrame | None]:\n    \"\"\"\n    Take input df with \"daily_value\" column, and calculates three types of QWs:\n    OVER_AVERAGE - for each row first based on specified window take mean of values, and then check if\n        daily_value exceeds mean by more than rate_over_average\n    UPPER_VARIABILITY - for each row using already calculated mean compute upper variability limit which is\n        mean + SD*rate_upper_variability, check if  daily_value exceeds it\n    ABS_UPPER_LIMIT - check if daily_value exceeds absolute number rate_upper_limit\n    All three QWs depend on specified thresholds, if daily_value exceeds one of calculated values it will\n        store cond-warn-condition_value information in array column,\n    some rows may have several QWs. In the end array column is exploded and cond-warn-condition_value\n        information is split into three corresponding columns.\n    The function returns almost ready df for SilverEventDataSyntacticQualityWarningsLogTable DO, and based\n        on save_data_for_plots arg returns either almost ready data for plots or None\n\n    Args:\n        df_temp (DataFrame): temprory data that must have daily_value column to\n            be used in further QW calculations\n        window (Window): a window within which perform aggregation\n        rate_upper_variability (int|float): config param, specifies the upper limit over which daily value\n             can not exceed its corresponding mean error rate\n        rate_over_average (int|float): config param, the number of SD to define the upper varibaility\n             limit: mean_rate + SD*error_rate_upper_variability, which daily value can not exceed\n        rate_upper_limit (int|float): absolute number which daily value can not exceed\n        measure_definition (str): canva text to use for measure_definition column (see measure_definition.py)\n        cond_warn_over_average (str): canva text to use for over_average cases (see conditions.py and warnings.py)\n        cond_warn_upper_variability (str): canva text to use for\n            upper_variability cases (see conditions.py and warnings.py)\n        cond_warn_upper_limit (str): canva text to use for upper_limit cases (see conditions.py and warnings.py)\n        save_data_for_plots (bool): boolean, decide whether to store daily_value and its corresponding average\n            and upper variability limit for plots. Defaults to False.\n    Returns:\n         tuple(DataFrame | None): a tuple, where first df is used for\n            warning log table, and the second df - for plots\n    \"\"\"\n    # prepare data\n    # calculate mean and std over window, and based on them UCL - for UPPER_VARIABILITY check\n    # ratio_perc - for OVER_AVERAGE check\n    # create empty array cond_warn_condition_value column to store inromation about qws\n    df_prep = df_temp.withColumns(\n        {\n            ColNames.average: psf.avg(ColNames.daily_value).over(window),\n            \"std_rate\": psf.stddev(ColNames.daily_value).over(window),\n            \"ratio_perc\": (psf.col(ColNames.daily_value) / psf.col(ColNames.average)) * 100,\n            ColNames.UCL: psf.col(ColNames.average) + rate_upper_variability * psf.col(\"std_rate\"),\n            \"cond_warn_condition_value\": psf.array(),\n        }\n    )\n    # if save_data_for_plots=True, add some new columns with constant values\n    # and select only applicable to SilverEventDataSyntacticQualityWarningsForPlots.SCHEMA\n    # else - return None\n    if save_data_for_plots:\n        df_prep = df_prep.cache()\n        df_plots = df_prep.withColumns(\n            {\n                ColNames.lookback_period: psf.lit(self.lookback_period),\n                ColNames.type_of_qw: psf.lit(\"error_rate\"),\n                ColNames.LCL: psf.lit(None).cast(\"float\"),\n            }\n        ).select(\n            self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsForPlots.ID].SCHEMA.fieldNames()\n        )\n    else:\n        df_plots = None\n\n    # continue with QWs checks\n    # first filter temp data by period from [data_period_start-lookback_period_in_days, data_period_end]\n    #  to [data_period_start, data_period_end]\n    # filter is aaplied after plot block because the first days of research period needs previous data to plot\n    df_qw = df_prep.filter(psf.col(ColNames.date) &gt;= self.data_period_start)\n    # perform UPPER_VARIABILITY, OVER_AVERAGE, and ABS_UPPER_LIMIT checks\n    # if condition is met store information about condition-warning_text-condition_value as a string into\n    # array column 'cond_warn_condition_value'\n    df_qw = (\n        df_qw.withColumn(\n            \"cond_warn_condition_value\",\n            psf.when(\n                psf.col(ColNames.daily_value) &gt; psf.col(ColNames.UCL),\n                psf.array_append(\n                    psf.col(\"cond_warn_condition_value\"),\n                    psf.concat(psf.lit(f\"{cond_warn_upper_variability}-\"), psf.col(ColNames.UCL).cast(\"string\")),\n                ),\n            ).otherwise(psf.col(\"cond_warn_condition_value\")),\n        )\n        .withColumn(\n            \"cond_warn_condition_value\",\n            psf.when(\n                psf.col(\"ratio_perc\") &gt; psf.lit(100 + rate_over_average),\n                psf.array_append(\n                    psf.col(\"cond_warn_condition_value\"),\n                    psf.lit(f\"{cond_warn_over_average}-{str(rate_over_average)}\"),\n                ),\n            ).otherwise(psf.col(\"cond_warn_condition_value\")),\n        )\n        .withColumn(\n            \"cond_warn_condition_value\",\n            psf.when(\n                psf.col(ColNames.daily_value) &gt; psf.lit(rate_upper_limit),\n                psf.array_append(\n                    psf.col(\"cond_warn_condition_value\"),\n                    psf.lit(f\"{cond_warn_upper_limit}-{str(rate_upper_limit)}\"),\n                ),\n            ).otherwise(psf.col(\"cond_warn_condition_value\")),\n        )\n    )\n    # explode array column 'cond_warn_condition_value'\n    df_qw = df_qw.withColumn(\"cond_warn_condition_value\", psf.explode(psf.col(\"cond_warn_condition_value\")))\n    # add some column constants\n    # split \"cond_warn_condition_value\" column into three: condition, wanring_text, conditon_value to match\n    # SilverEventDataSyntacticQualityWarningsLogTable.SCHEMA\n    # select only needed columns\n    df_qw = df_qw.withColumns(\n        {\n            ColNames.lookback_period: psf.lit(self.lookback_period),\n            ColNames.measure_definition: psf.lit(measure_definition),\n            ColNames.condition: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(0),\n            ColNames.warning_text: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(1),\n            ColNames.condition_value: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(2).cast(\"float\"),\n        }\n    ).select(self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsLogTable.ID].SCHEMA.fieldNames())\n\n    return (df_qw, df_plots)\n</code></pre>"},{"location":"reference/components/quality/event_quality_warnings/event_quality_warnings/#components.quality.event_quality_warnings.event_quality_warnings.EventQualityWarnings.save_quality_warnings_output","title":"<code>save_quality_warnings_output(dfs_qw, output_do)</code>","text":"<p>Concatenates all elements in dfs_qw list, adjustes to schema of output_do, and using write     method of output_do stores the result</p> <p>Parameters:</p> Name Type Description Default <code>dfs_qw</code> <code>list</code> <p>description</p> required <code>output_do</code> <code>SilverEventDataSyntacticQualityWarningsLogTable | SilverEventDataSyntacticQualityWarningsForPlots</code> <p>description</p> required Source code in <code>multimno/components/quality/event_quality_warnings/event_quality_warnings.py</code> <pre><code>def save_quality_warnings_output(\n    self,\n    dfs_qw: list[DataFrame | None],\n    output_do: SilverEventDataSyntacticQualityWarningsLogTable | SilverEventDataSyntacticQualityWarningsForPlots,\n):\n    \"\"\"\n    Concatenates all elements in dfs_qw list, adjustes to schema of output_do, and using write\n        method of output_do stores the result\n\n    Args:\n        dfs_qw (list): _description_\n        output_do (SilverEventDataSyntacticQualityWarningsLogTable | \\\n            SilverEventDataSyntacticQualityWarningsForPlots): _description_\n    \"\"\"\n\n    output_do.df = reduce(lambda x, y: x.union(y), dfs_qw)\n\n    output_do.df = self.spark.createDataFrame(output_do.df.rdd, output_do.SCHEMA)\n\n    output_do.write()\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/","title":"network_quality_warnings","text":""},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/","title":"network_quality_warnings","text":"<p>Module that generates the quality warnings associated to the syntactic checks/cleaning of the raw MNO Network Topology Data.</p>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings","title":"<code>NetworkQualityWarnings</code>","text":"<p>             Bases: <code>Component</code></p> <p>Class that produces the log tables and data required for plotting associated to Network Topology Data cleaning/syntactic checks.</p> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>class NetworkQualityWarnings(Component):\n    \"\"\"\n    Class that produces the log tables and data required for plotting associated to Network Topology Data\n    cleaning/syntactic checks.\n    \"\"\"\n\n    COMPONENT_ID = \"NetworkQualityWarnings\"\n\n    PERIOD_DURATION = {\"week\": 7, \"month\": 30, \"quarter\": 90}\n\n    TITLE = \"MNO Network Topology Data Quality Warnings\"\n\n    MEASURE_DEFINITION = {\n        \"SIZE_RAW_DATA\": \"Value of the size of the raw data object\",\n        \"SIZE_CLEAN_DATA\": \"Value of the size of the clean data object\",\n        \"TOTAL_ERROR_RATE\": \"Error rate\",\n        \"Missing_value_RATE\": \"Missing rate value of {field_name}\".format,\n        \"Out_of_range_RATE\": \"Out of range rate of {field_name}\".format,\n        \"Parsing_error_RATE\": \"Parsing error rate of {field_name}\".format,\n    }\n\n    ERROR_TYPE = {\n        \"Missing_value_RATE\": NetworkErrorType.NULL_VALUE,\n        \"Out_of_range_RATE\": NetworkErrorType.OUT_OF_RANGE,\n        \"Parsing_error_RATE\": NetworkErrorType.CANNOT_PARSE,\n    }\n\n    CONDITION = {\n        \"Missing_value_RATE\": {\n            \"AVERAGE\": \"Missing value rate of {field_name} is over the previous period average by more than {value} %\".format,\n            \"VARIABILITY\": (\n                \"Missing value rate of {field_name} is over the upper control limit calculated on the basis of average and standard deviation \"\n                + \"of the distribution of the missing value rate of {field_name} in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\"\n            ).format,\n            \"ABS_VALUE_UPPER_LIMIT\": \"Missing value rate of {field_name} is over the value {value}\".format,\n        },\n        \"Out_of_range_RATE\": {\n            \"AVERAGE\": \"Out of range rate of {field_name} is over the previous period average by more than {value} %\".format,\n            \"VARIABILITY\": (\n                \"Out of range rate of {field_name} is over the upper control limit calculated on the basis of average and standard deviation \"\n                + \"of the distribution of the out of range rate of {field_name} in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\"\n            ).format,\n            \"ABS_VALUE_UPPER_LIMIT\": \"Out of range rate of {field_name} is over the value {value} %\".format,\n        },\n        \"Parsing_error_RATE\": {\n            \"AVERAGE\": \"Parsing error rate of {field_name} is over the previous period average by more than {value} %\".format,\n            \"VARIABILITY\": (\n                \"Parsing error rate of {field_name} is over the upper control limit calculated on the basis of average and standard deviation \"\n                + \"of the distribution of the parsing error rate of {field_name} in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\"\n            ).format,\n            \"ABS_VALUE_UPPER_LIMIT\": \"Parsing error rate of {field_name} is over the value {value}\".format,\n        },\n    }\n\n    WARNING_MESSAGE = {\n        \"Missing_value_RATE\": {\n            \"AVERAGE\": \"The missing value rate of {field_name} after the syntactic check procedure is unexpectedly high with respect to the previous period.\".format,\n            \"VARIABILITY\": (\n                \"The missing value rate of {field_name} after the syntactic check procedure is unexpectedly high \"\n                + \"with respect to previous period taking into account its usual variability.\"\n            ).format,\n            \"ABS_VALUE_UPPER_LIMIT\": \"The missing value rate of {field_name} is over the threshold\".format,\n        },\n        \"Out_of_range_RATE\": {\n            \"AVERAGE\": \"The out of range rate of {field_name} after the syntactic check procedure is unexpectedly high with respect to the previous period.\".format,\n            \"VARIABILITY\": (\n                \"The out of range of {field_name} after the syntactic check procedure is unexpectedly high \"\n                + \"with respect to previous period taking into account its usual variability.\"\n            ).format,\n            \"ABS_VALUE_UPPER_LIMIT\": \"The out of range rate of {field_name} is over the threshold\".format,\n        },\n        \"Parsing_error_RATE\": {\n            \"AVERAGE\": \"The parsing error rate of {field_name} after the syntactic check procedure is unexpectedly high with respect to the previous period.\".format,\n            \"VARIABILITY\": (\n                \"The parsing error of {field_name} after the syntactic check procedure is unexpectedly high \"\n                + \"with respect to previous period taking into account its usual variability.\"\n            ).format,\n            \"ABS_VALUE_UPPER_LIMIT\": \"The parsing error rate of {field_name} is over the threshold\".format,\n        },\n    }\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.timestamp = datetime.datetime.now()\n\n        # Read date of study\n        self.date_format = self.config.get(self.COMPONENT_ID, \"date_format\")\n        date_of_study = self.config.get(self.COMPONENT_ID, \"date_of_study\")\n\n        try:\n            self.date_of_study = datetime.datetime.strptime(date_of_study, self.date_format).date()\n        except ValueError as e:\n            self.logger.error(str(e), exc_info=True)\n            raise e\n\n        # Read lookback period\n        self.lookback_period = self.config.get(self.COMPONENT_ID, \"lookback_period\")\n\n        if self.lookback_period not in [\"week\", \"month\", \"quarter\"]:\n            error_msg = (\n                \"Configuration parameter `lookback_period` must be one of `week`, `month`, or `quarter`, \"\n                f\"but {self.lookback_period} was passed\"\n            )\n\n            self.logger.error(error_msg)\n            raise ValueError(error_msg)\n\n        self.lookback_dates = [\n            self.date_of_study - datetime.timedelta(days=d)\n            for d in range(1, self.PERIOD_DURATION[self.lookback_period] + 1)\n        ]\n\n        self.lookback_period_start = min(self.lookback_dates)\n        self.lookback_period_end = max(self.lookback_dates)\n\n        # Read thresholds and use read values instead of default ones when appropriate\n        self.thresholds = self.get_thresholds()\n\n        self.warnings = []\n\n        self.plots_data = dict()\n\n    def get_thresholds(self) -&gt; dict:\n        \"\"\"\n        Method that reads the threshold-related parameters, contained in the config file,\n        and saves them in memory to use them instead of the ones specified by default.\n        Raises:\n            ValueError: non-numerical value that cannot be parsed to float has been used in\n                the config file\n            ValueError: Negative value for a given parameter has been given, when only\n                non-negative values make sense and are thus allowed.\n\n        Returns:\n            thresholds: dict containing the different thresholds used for computing the\n                quality warnings, with the same structure as\n                multimno.core.constants.network_default_thresholds.NETWORK_DEFAULT_THRESHOLDS\n        \"\"\"\n        config_thresholds = self.config.geteval(self.COMPONENT_ID, \"thresholds\")\n        thresholds = NETWORK_DEFAULT_THRESHOLDS\n\n        for error_key in config_thresholds.keys():\n            if error_key not in NETWORK_DEFAULT_THRESHOLDS:\n                self.logger.info(f\"Parameter key {error_key} unknown -- ignored\")\n                continue\n\n            if error_key in [\"SIZE_RAW_DATA\", \"SIZE_CLEAN_DATA\", \"TOTAL_ERROR_RATE\"]:\n                for param_key, val in config_thresholds[error_key].items():\n                    if param_key not in NETWORK_DEFAULT_THRESHOLDS[error_key]:\n                        self.logger.info(f\"Unknown parameter {param_key} under {error_key} -- ignored\")\n                        continue\n\n                    try:\n                        val = float(val)\n                    except ValueError as e:\n                        error_msg = (\n                            f\"Expected numeric value for parameter {param_key} under {error_key} - got {val} instead.\"\n                        )\n                        self.logger.error(error_msg)\n                        raise e\n\n                    if val &lt; 0:\n                        error_msg = f\"Parameter {param_key} under {error_key} must be non-negative - got {val}\"\n                        self.logger.error(error_msg)\n                        raise ValueError(error_msg)\n\n                    thresholds[error_key][param_key] = val\n\n            else:  # then is one of 'Missing_value_RATE', 'Out_of_range_RATE', 'Parsing_error_RATE'\n                for field_name in config_thresholds[error_key]:\n                    if field_name not in NETWORK_DEFAULT_THRESHOLDS[error_key]:\n                        self.logger.info(f\"Unknown field name {field_name} under {error_key} -- ignored\")\n                        continue\n\n                    for param_key, val in config_thresholds[error_key][field_name].items():\n                        if param_key not in NETWORK_DEFAULT_THRESHOLDS[error_key][field_name]:\n                            self.logger.info(\n                                f'Unknown parameter {param_key} under {error_key}[\"{field_name}\"] -- ignored'\n                            )\n                            continue\n\n                        try:\n                            val = float(val)\n                        except ValueError as e:\n                            error_msg = f'Expected numeric value for parameter {param_key} under {error_key}[\"{field_name}\"] - got {val} instead.'\n                            self.logger.error(error_msg)\n                            raise e\n\n                        if val &lt; 0:\n                            error_msg = f'Parameter {param_key} under {error_key}[\"{field_name}\"] must be non-negative - got {val}'\n                            self.logger.error(error_msg)\n                            raise ValueError(error_msg)\n\n                        thresholds[error_key][field_name][param_key] = val\n\n        return thresholds\n\n    def initalize_data_objects(self):\n        input_silver_quality_metrics_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"network_syntactic_quality_metrics_by_column\"\n        )\n        output_silver_log_table_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"network_syntactic_quality_warnings_log_table\"\n        )\n\n        output_silver_line_plot_data_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"network_syntactic_quality_warnings_line_plot_data\"\n        )\n\n        output_silver_pie_plot_data_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"network_syntactic_quality_warnings_pie_plot_data\"\n        )\n\n        silver_quality_metrics = SilverNetworkDataQualityMetricsByColumn(\n            self.spark,\n            input_silver_quality_metrics_path,\n            partition_columns=[ColNames.year, ColNames.month, ColNames.day],\n        )\n\n        silver_log_table = SilverNetworkDataSyntacticQualityWarningsLogTable(\n            self.spark, output_silver_log_table_path, partition_columns=[ColNames.year, ColNames.month, ColNames.day]\n        )\n\n        silver_line_plot_data = SilverNetworkSyntacticQualityWarningsLinePlotData(\n            self.spark, output_silver_line_plot_data_path\n        )\n\n        silver_pie_plot_data = SilverNetworkSyntacticQualityWarningsPiePlotData(\n            self.spark, output_silver_pie_plot_data_path\n        )\n\n        self.input_data_objects = {silver_quality_metrics.ID: silver_quality_metrics}\n        self.output_data_objects = {\n            silver_log_table.ID: silver_log_table,\n            silver_line_plot_data.ID: silver_line_plot_data,\n            silver_pie_plot_data.ID: silver_pie_plot_data,\n        }\n\n    def transform(self):\n\n        # Check if both the date of study and the specified lookback period dates are in file\n        self.check_needed_dates()\n\n        lookback_stats, lookback_initial_rows, lookback_final_rows = self.get_lookback_period_statistics()\n\n        today_values = self.get_study_date_values()\n\n        raw_average, raw_UCL, raw_LCL = self.raw_size_warnings(lookback_stats, today_values)\n\n        clean_average, clean_UCL, clean_LCL = self.clean_size_warnings(lookback_stats, today_values)\n\n        error_rate, error_rate_avg, error_rate_UCL = self.error_rate_warnings(\n            lookback_initial_rows, lookback_final_rows, today_values\n        )\n\n        self.all_specific_error_warnings(lookback_stats, today_values)\n\n        self.output_data_objects[SilverNetworkDataSyntacticQualityWarningsLogTable.ID].df = self.spark.createDataFrame(\n            self.warnings, SilverNetworkDataSyntacticQualityWarningsLogTable.SCHEMA\n        )\n\n        lookback_initial_rows[self.date_of_study] = today_values[None][NetworkErrorType.INITIAL_ROWS]\n        lookback_final_rows[self.date_of_study] = today_values[None][NetworkErrorType.FINAL_ROWS]\n\n        self.create_plots_data(\n            lookback_initial_rows=lookback_initial_rows,\n            lookback_final_rows=lookback_final_rows,\n            today_values=today_values,\n            error_rate=error_rate,\n            raw_average=raw_average,\n            clean_average=clean_average,\n            error_rate_avg=error_rate_avg,\n            raw_UCL=raw_UCL,\n            clean_UCL=clean_UCL,\n            error_rate_UCL=error_rate_UCL,\n            raw_LCL=raw_LCL,\n            clean_LCL=clean_LCL,\n        )\n\n    def check_needed_dates(self) -&gt; None:\n        \"\"\"\n        Method that checks if both the date of study and the dates necessary to generate\n        the quality warnings, specified through the lookback_period parameter, are present\n        in the input data.\n        \"\"\"\n\n        # Collect all distinct dates in the input quality metrics within the needed range\n        metrics = self.input_data_objects[SilverNetworkDataQualityMetricsByColumn.ID].df\n        dates = (\n            metrics.filter(\n                F.col(\"date\")\n                # left- and right- inclusive\n                .between(\n                    self.date_of_study - datetime.timedelta(days=self.PERIOD_DURATION[self.lookback_period]),\n                    self.date_of_study,\n                )\n            )\n            .select(F.col(ColNames.date))\n            .distinct()\n            .collect()\n        )\n\n        dates = [row[ColNames.date] for row in dates]\n\n        if self.date_of_study not in dates:\n            raise ValueError(\n                f\"The date of study, {self.date_of_study}, is not present in the input Quality Metrics data\"\n            )\n\n        if set(self.lookback_dates).intersection(set(dates)) != set(self.lookback_dates):\n            error_msg = f\"\"\"\n                The following dates from the lookback period are not present in the\n                input Quality Metrics data:\n                {\n                    sorted(\n                        map(\n                            lambda x: x.strftime(self.date_format),\n                            set(self.lookback_dates).difference(set(dates))\n                        )\n                    )\n                }\"\"\"\n\n            self.logger.error(error_msg)\n            raise ValueError(error_msg)\n\n    def get_lookback_period_statistics(self) -&gt; dict:\n        \"\"\"Method that computes the necessary statistics (average and standard deviation) from the\n        Quality Metrics of the lookback period.\n\n        Returns:\n            statistics (dict): dictionary containing said necessary statistics, with the following\n                structure:\n                {\n                    field_name1: {\n                        type_error1: {\n                            'average': 12.2,\n                            'stddev': 17.5\n                        },\n                        type_error2 : {\n                            'average': 4.5,\n                            'stddev': 10.1\n                        }\n                    },\n                    ...\n                }\n            initial_rows (dict): dictionary indicating the number of rows before syntactic checks for the previous period, {date -&gt; size_raw_data}\n            final_rows (dict): dictionary indicating the number of rows after syntactic checks for the previous period, {date -&gt; size_raw_data}\n        \"\"\"\n        intermediate_df = self.input_data_objects[SilverNetworkDataQualityMetricsByColumn.ID].df.filter(\n            F.col(ColNames.date).between(self.lookback_period_start, self.lookback_period_end)\n        )\n\n        intermediate_df.cache()\n\n        lookback_stats = (\n            intermediate_df.groupBy([ColNames.field_name, ColNames.type_code])\n            .agg(F.mean(F.col(ColNames.value)).alias(\"average\"), F.stddev_samp(F.col(ColNames.value)).alias(\"stddev\"))\n            .collect()\n        )\n\n        error_rate_data = (\n            intermediate_df.select([ColNames.type_code, ColNames.value, ColNames.date])\n            .filter(F.col(ColNames.type_code).isin([NetworkErrorType.INITIAL_ROWS, NetworkErrorType.FINAL_ROWS]))\n            .collect()\n        )\n\n        intermediate_df.unpersist()\n\n        initial_rows = {}\n        final_rows = {}\n\n        for row in error_rate_data:\n            if row[ColNames.type_code] == NetworkErrorType.INITIAL_ROWS:\n                initial_rows[row[ColNames.date]] = row[\"value\"]\n            else:\n                final_rows[row[ColNames.date]] = row[\"value\"]\n\n        statistics = dict()\n        for row in lookback_stats:\n            field_name, type_code, average, stddev = (\n                row[ColNames.field_name],\n                row[ColNames.type_code],\n                row[\"average\"],\n                row[\"stddev\"],\n            )\n            if field_name not in statistics:\n                statistics[field_name] = dict()\n            statistics[field_name][type_code] = {\"average\": average, \"stddev\": stddev}\n\n        return statistics, initial_rows, final_rows\n\n    def get_study_date_values(self) -&gt; dict:\n        \"\"\"Method that reads and returns the quality metrics of the date of study.\n\n        Returns:\n            today_values (dict): dictionary containing said values, with the following\n                structure:\n                {\n                    field_name1: {\n                        type_error1: 123,\n                        type_error2: 23,\n                        type_error3: 0\n                    },\n                    field_name2: {\n                        type_error1: 0,\n                        type_error2: 0,\n                        type_error3: 300\n                    },\n                }\n        \"\"\"\n        today_metrics = (\n            self.input_data_objects[SilverNetworkDataQualityMetricsByColumn.ID]\n            .df.filter(F.col(ColNames.date) == F.lit(self.date_of_study))\n            .select([ColNames.field_name, ColNames.type_code, ColNames.value])\n        ).collect()\n\n        today_values = {}\n\n        for row in today_metrics:\n            field_name, type_code, value = row[ColNames.field_name], row[ColNames.type_code], row[ColNames.value]\n\n            if field_name not in today_values:\n                today_values[field_name] = {}\n\n            today_values[field_name][type_code] = value\n\n        return today_values\n\n    def register_warning(\n        self, measure_definition: str, daily_value: float, condition: str, condition_value: float, warning_text: str\n    ) -&gt; None:\n        \"\"\"Method that abstracts away the creation in the correct format and data type, each of the quality warnings\n        that will be recorded in the log table.\n\n        Args:\n            measure_definition (str): measure that raised the warning (e.g. Error rate)\n            daily_value (float): measure's value in the date of study that raised the warning\n            condition (str): test that was checked in order to raise the warning\n            condition_value (float): value against which the date of study's daily_value was compared\n            warning_text (str): verbose explanation of the condition being satisfied and the warning\n                being raised\n        \"\"\"\n        warning = {\n            ColNames.title: self.TITLE,\n            ColNames.date: self.date_of_study,\n            ColNames.timestamp: self.timestamp,\n            ColNames.measure_definition: measure_definition,\n            ColNames.daily_value: float(daily_value),\n            ColNames.condition: condition,\n            ColNames.lookback_period: self.lookback_period,\n            ColNames.condition_value: float(condition_value),\n            ColNames.warning_text: warning_text,\n            ColNames.year: self.date_of_study.year,\n            ColNames.month: self.date_of_study.month,\n            ColNames.day: self.date_of_study.day,\n        }\n\n        self.warnings.append(Row(**warning))\n\n    def raw_size_warnings(self, lookback_stats, today_values):\n        \"\"\"Method that generates the quality warnings, that will be recorded in the output log table,\n        for the metric regarding the initial number of rows of the raw input network topology data prior\n        to the syntactic check procedure.\n\n        A total of six warnings might be generated:\n            - The study date's number of rows is greater than the average number of rows over the previous period\n                by more than a threshold percentage.\n            - The study date's number of rows is smaller than the average number of rows over the previous period\n                by more than a threshold percentage.\n            - The study date's number of rows is greater than the average number of rows over the previous period\n                by a config-specified number of standard deviations.\n            - The study date's number of rows is smaller than the average number of rows over the previous period\n                by a config-specified number of standard deviations.\n            - The study date's number of rows is greater than a config-specified threshold.\n            - The study date's number of rows is smaller than a config-specified threshold.\n        \"\"\"\n        current_val = today_values[None][NetworkErrorType.INITIAL_ROWS]\n        previous_avg = lookback_stats[None][NetworkErrorType.INITIAL_ROWS][\"average\"]\n        previous_std = lookback_stats[None][NetworkErrorType.INITIAL_ROWS][\"stddev\"]\n\n        measure_definition = self.MEASURE_DEFINITION[\"SIZE_RAW_DATA\"]\n\n        over_average = self.thresholds[\"SIZE_RAW_DATA\"][\"OVER_AVERAGE\"]\n        under_average = self.thresholds[\"SIZE_RAW_DATA\"][\"UNDER_AVERAGE\"]\n        variability = self.thresholds[\"SIZE_RAW_DATA\"][\"VARIABILITY\"]\n        absolute_upper_control_limit = self.thresholds[\"SIZE_RAW_DATA\"][\"ABS_VALUE_UPPER_LIMIT\"]\n        absolute_lower_control_limit = self.thresholds[\"SIZE_RAW_DATA\"][\"ABS_VALUE_LOWER_LIMIT\"]\n\n        if previous_avg == 0:\n            self.logger.warning(\n                f\"\"\"\n                The average of the number of rows in the raw input network topology data is 0\n                for the lookback period especified ({self.lookback_period_start} to {self.lookback_period_end},\n                 both included)\n                \"\"\"\n            )\n        else:\n            pct_difference = 100 * current_val / previous_avg - 100\n            if pct_difference &gt; over_average:\n                self.register_warning(\n                    measure_definition,\n                    pct_difference,\n                    f\"Size is greater than the previous period's average by more than {over_average} %\",\n                    over_average,\n                    \"The number of cells is unexpectedly high with respect to the previous period, please check if there have been issues in the network\",\n                )\n\n            if pct_difference &lt; -under_average:\n                self.register_warning(\n                    measure_definition,\n                    pct_difference,\n                    f\"Size is smaller than the previous period's average by more than {under_average} %\",\n                    under_average,\n                    \"The number of cells is unexpectedly low with respect to the previous period, please check if there have been issues in the network.\",\n                )\n\n        upper_control_limit = previous_avg + variability * previous_std\n        lower_control_limit = previous_avg - variability * previous_std\n\n        if current_val &gt; upper_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                \"Size is out of the upper control limit calculated on the basis of average and standard deviation of the distribution \"\n                f\"of the size in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\",\n                upper_control_limit,\n                \"The number of cells is unexpectedly high with respect to the previous period, taking into account the usual \"\n                \"variability of the cell numbers, please check if there have been issues in the network.\",\n            )\n        if current_val &lt; lower_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                \"Size is out of the lower control limit calculated on the basis of average and standard deviation of the distribution \"\n                f\"of the size in the previous period. Lower control limit = (average - {variability}\u00b7stddev)\",\n                lower_control_limit,\n                \"The number of cells is unexpectedly low with respect to the previous period, taking into account the usual \"\n                \"variability of the cell numbers, please check if there have been issues in the network.\",\n            )\n\n        # Use of UCL and LCL as default values for the absolute limits\n        if absolute_upper_control_limit is None:\n            absolute_upper_control_limit = upper_control_limit\n        if absolute_lower_control_limit is None:\n            absolute_lower_control_limit = lower_control_limit\n\n        if current_val &gt; absolute_upper_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                f\"The size is over the threshold {absolute_upper_control_limit}\",\n                absolute_upper_control_limit,\n                \"The number of cells is over the threshold, please check if there have been changes in the network.\",\n            )\n        if current_val &lt; absolute_lower_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                f\"The size is under the threshold {absolute_lower_control_limit}\",\n                absolute_upper_control_limit,\n                \"The number of cells is under the threshold, please check if there have been changes in the network.\",\n            )\n\n        return previous_avg, upper_control_limit, lower_control_limit\n\n    def clean_size_warnings(self, lookback_stats, today_values):\n        \"\"\"Method that generates quality warnings, that will be recorded in the output log table,\n        for the metric regarding the final number of rows of the raw input network topology data prior\n        to the syntactic check procedure.\n\n        A total of six warnings might be generated:\n            - The study date's number of rows is greater than the average number of rows over the previous period\n                by more than a threshold percentage.\n            - The study date's number of rows is smaller than the average number of rows over the previous period\n                by more than a threshold percentage.\n            - The study date's number of rows is greater than the average number of rows over the previous period\n                by a config-specified number of standard deviations.\n            - The study date's number of rows is smaller than the average number of rows over the previous period\n                by a config-specified number of standard deviations.\n            - The study date's number of rows is greater than a config-specified threshold.\n            - The study date's number of rows is smaller than a config-specified threshold.\n        \"\"\"\n        current_val = today_values[None][NetworkErrorType.FINAL_ROWS]\n        previous_avg = lookback_stats[None][NetworkErrorType.FINAL_ROWS][\"average\"]\n        previous_std = lookback_stats[None][NetworkErrorType.FINAL_ROWS][\"stddev\"]\n\n        measure_definition = self.MEASURE_DEFINITION[\"SIZE_CLEAN_DATA\"]\n\n        over_average = self.thresholds[\"SIZE_CLEAN_DATA\"][\"OVER_AVERAGE\"]\n        under_average = self.thresholds[\"SIZE_CLEAN_DATA\"][\"UNDER_AVERAGE\"]\n        variability = self.thresholds[\"SIZE_CLEAN_DATA\"][\"VARIABILITY\"]\n        absolute_upper_control_limit = self.thresholds[\"SIZE_CLEAN_DATA\"][\"ABS_VALUE_UPPER_LIMIT\"]\n        absolute_lower_control_limit = self.thresholds[\"SIZE_CLEAN_DATA\"][\"ABS_VALUE_LOWER_LIMIT\"]\n\n        if previous_avg == 0:\n            self.logger.warning(\n                f\"\"\"\n                The average of the number of rows in the clean input network topology data after syntactic checks is 0\n                for the lookback period especified ({self.lookback_period_start} to {self.lookback_period_end}, \n                both included)\n                \"\"\"\n            )\n        else:\n            pct_difference = 100 * current_val / previous_avg - 100\n            if pct_difference &gt; over_average:\n                self.register_warning(\n                    measure_definition,\n                    pct_difference,\n                    f\"Size is greater than the previous period's average by {over_average} %\",\n                    over_average,\n                    \"The number of cells after the syntactic checks procedure is unexpectedly high with respect to the previous period.\",\n                )\n\n            if pct_difference &lt; -under_average:\n                self.register_warning(\n                    measure_definition,\n                    pct_difference,\n                    f\"Size is smaller than the previous period's average by {under_average} %\",\n                    under_average,\n                    \"The number of cells after the syntactic checks procedure is unexpectedly low with respect to the previous period.\",\n                )\n\n        upper_control_limit = previous_avg + variability * previous_std\n        lower_control_limit = previous_avg - variability * previous_std\n\n        if current_val &gt; upper_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                \"Size is out of the upper control limit calculated on the basis of average and standard deviation of the distribution \"\n                f\"of the size in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\",\n                upper_control_limit,\n                \"The number of cells is unexpectedly high with respect to the previous period, taking into account the usual \"\n                \"variability of the cell numbers, please check if there have been issues in the network.\",\n            )\n        if current_val &lt; lower_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                \"Size is out of the lower control limit calculated on the basis of average and standard deviation of the distribution \"\n                f\"of the size in the previous period. Lower control limit = (average - {variability}\u00b7stddev)\",\n                lower_control_limit,\n                \"The number of cells is unexpectedly low with respect to the previous period, taking into account the usual \"\n                \"variability of the cell numbers, please check if there have been issues in the network.\",\n            )\n\n        # Use of UCL and LCL as default values for the absolute limits\n        if absolute_upper_control_limit is None:\n            absolute_upper_control_limit = upper_control_limit\n        if absolute_lower_control_limit is None:\n            absolute_lower_control_limit = lower_control_limit\n\n        if current_val &gt; absolute_upper_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                f\"The size is over the threshold {absolute_upper_control_limit}\",\n                absolute_upper_control_limit,\n                \"The number of cells after the syntactic checks procedure is over the threshold.\",\n            )\n        if current_val &lt; absolute_lower_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                f\"The size is under the threshold {absolute_lower_control_limit}\",\n                absolute_upper_control_limit,\n                \"The number of cells after the syntactic checks procedure is under the threshold.\",\n            )\n\n        return previous_avg, upper_control_limit, lower_control_limit\n\n    def error_rate_warnings(self, initial_rows, final_rows, today_values):\n        \"\"\"Method that generates quality warnings, that will be recorded in the output log table,\n        for the metric regarding the error rate observed in the syntactic check procedure.\n\n        A total of three warnings might be generated:\n            - The study date's error rate is greater than the average number of rows over the previous period\n                by more than a threshold percentage.\n            - The study date's error rate is greater than the average number of rows over the previous period\n                by a config-specified number of standard deviations.\n            - The study date's error rate is greater than a config-specified threshold.\n        \"\"\"\n        if len(initial_rows) != len(final_rows):\n            raise ValueError(\n                \"Input Quality Metrics do not have information on the number of rows \"\n                \"before and after syntactic checks on all dates considered!\"\n            )\n\n        error_rate = {\n            date: 100 * (initial_rows[date] - final_rows[date]) / initial_rows[date] for date in initial_rows.keys()\n        }\n\n        current_val = (\n            100\n            * (today_values[None][NetworkErrorType.INITIAL_ROWS] - today_values[None][NetworkErrorType.FINAL_ROWS])\n            / today_values[None][NetworkErrorType.INITIAL_ROWS]\n        )\n        previous_avg = sum(error_rate[date] for date in self.lookback_dates) / len(self.lookback_dates)\n        previous_std = math.sqrt(\n            sum((error_rate[date] - previous_avg) ** 2 for date in self.lookback_dates) / (len(self.lookback_dates) - 1)\n        )\n\n        measure_definition = self.MEASURE_DEFINITION[\"TOTAL_ERROR_RATE\"]\n\n        over_average = self.thresholds[\"TOTAL_ERROR_RATE\"][\"OVER_AVERAGE\"]\n        variability = self.thresholds[\"TOTAL_ERROR_RATE\"][\"VARIABILITY\"]\n        absolute_upper_control_limit = self.thresholds[\"TOTAL_ERROR_RATE\"][\"ABS_VALUE_UPPER_LIMIT\"]\n\n        if previous_avg == 0:\n            self.logger.warning(\n                f\"\"\"\n                The average error rate in the input network topology data 0\n                for the lookback period especified ({self.lookback_period_start} to {self.lookback_period_end}, \n                both included)\n                \"\"\"\n            )\n        else:\n            pct_difference = 100 * current_val / previous_avg - 100\n            if pct_difference &gt; over_average:\n                self.register_warning(\n                    measure_definition,\n                    pct_difference,\n                    f\"Error rate is greater than the previous period's average by more than {over_average} %\",\n                    over_average,\n                    \"The error rate after the syntactic checks procedure is unexpectedly high with respect to the previous period.\",\n                )\n        upper_control_limit = previous_avg + variability * previous_std\n\n        if current_val &gt; upper_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                \"Error rate is over the upper control limit calculated on the basis of average and standard deviation of the distribution \"\n                f\"of the size in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\",\n                upper_control_limit,\n                \"The error rate after the syntactic checks procedure is unexpectedly high with respect to the previous period, taking into account the usual \"\n                \"variability.\",\n            )\n\n        if current_val &gt; absolute_upper_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                f\"The error rate is over the value {absolute_upper_control_limit}\",\n                absolute_upper_control_limit,\n                \"The error rate after the syntactic checks procedure is over the threshold.\",\n            )\n        error_rate[self.date_of_study] = current_val\n        return error_rate, previous_avg, upper_control_limit\n\n    def all_specific_error_warnings(self, lookback_stats, today_values):\n        \"\"\"Parent method for the creation of warnings for each type of error rate\n\n        lookback_stats (dict): contains error information of each date of the lookback period\n        today_values (dict): contains error information of the date of study\n        \"\"\"\n        error_rate_types = [\"Missing_value_RATE\", \"Out_of_range_RATE\", \"Parsing_error_RATE\"]\n\n        for error_rate_type in error_rate_types:\n            for field_name in self.thresholds[error_rate_type]:\n                self.specific_error_warnings(error_rate_type, field_name, lookback_stats, today_values)\n\n    def specific_error_warnings(self, error_rate_type, field_name, lookback_stats, today_values):\n        \"\"\"Method that generates the quality warnings, that will be recorded in the output log table,\n        for the metric regarding a specific error type considered in the network syntactic checks.\n\n        A total of three warnings might be generated:\n            - The study date's error rate for this error and this field is greater than the average number of rows over the previous period\n                by more than a threshold percentage.\n            - The study date's error rate for this error and this field is greater than the average number of rows over the previous period\n                by a config-specified number of standard deviations.\n            - The study date's error rate for this error and this field is greater than a config-specified threshold.\n        \"\"\"\n        if error_rate_type not in self.ERROR_TYPE:\n            raise ValueError(f\"Unknown error type for quality warning `{error_rate_type}`\")\n\n        network_error_type = self.ERROR_TYPE[error_rate_type]\n\n        over_average = self.thresholds[error_rate_type][field_name][\"AVERAGE\"]\n        variability = self.thresholds[error_rate_type][field_name][\"VARIABILITY\"]\n        absolute_upper_control_limit = self.thresholds[error_rate_type][field_name][\"ABS_VALUE_UPPER_LIMIT\"]\n\n        if network_error_type == NetworkErrorType.OUT_OF_RANGE and field_name is None:\n            field_name = \"dates\"\n        current_val = today_values[field_name][network_error_type]\n        previous_avg = lookback_stats[field_name][network_error_type][\"average\"]\n        previous_std = lookback_stats[field_name][network_error_type][\"stddev\"]\n\n        if previous_avg == 0:\n            self.logger.warning(\n                f\"\"\"\n                The average {error_rate_type} for field {field_name} in the input network topology data is 0 for the lookback period\n                especified ({self.lookback_period_start} to {self.lookback_period_end}, both included)\n                \"\"\"\n            )\n        else:\n            pct_difference = 100 * current_val / previous_avg - 100\n            if pct_difference &gt; over_average:\n                self.register_warning(\n                    self.MEASURE_DEFINITION[error_rate_type](field_name=field_name),\n                    pct_difference,\n                    self.CONDITION[error_rate_type][\"AVERAGE\"](field_name=field_name, value=over_average),\n                    over_average,\n                    self.WARNING_MESSAGE[error_rate_type][\"AVERAGE\"](field_name=field_name),\n                )\n        upper_control_limit = previous_avg + variability * previous_std\n\n        if current_val &gt; upper_control_limit:\n            self.register_warning(\n                self.MEASURE_DEFINITION[error_rate_type](field_name=field_name),\n                current_val,\n                self.CONDITION[error_rate_type][\"VARIABILITY\"](field_name=field_name, variability=variability),\n                variability,\n                self.WARNING_MESSAGE[error_rate_type][\"VARIABILITY\"](field_name=field_name),\n            )\n\n        if current_val &gt; absolute_upper_control_limit:\n            self.register_warning(\n                self.MEASURE_DEFINITION[error_rate_type](field_name=field_name),\n                current_val,\n                self.CONDITION[error_rate_type][\"ABS_VALUE_UPPER_LIMIT\"](\n                    field_name=field_name, value=absolute_upper_control_limit\n                ),\n                absolute_upper_control_limit,\n                self.WARNING_MESSAGE[error_rate_type][\"ABS_VALUE_UPPER_LIMIT\"](field_name=field_name),\n            )\n\n    def create_plots_data(\n        self,\n        lookback_initial_rows,\n        lookback_final_rows,\n        today_values,\n        error_rate,\n        raw_average,\n        clean_average,\n        error_rate_avg,\n        raw_UCL,\n        clean_UCL,\n        error_rate_UCL,\n        raw_LCL,\n        clean_LCL,\n    ):\n        \"\"\"Method that takes the data needed to generate the required plots, and formats it for\n        easier plotting and saving later on.\n\n        Args:\n            lookback_initial_rows (dict): contains data on the lookback period dates' number of rows before syntactic checks\n            lookback_final_rows (dict): contains data on the lookback period dates' number of rows after syntactic checks\n            today_values (dict): contains data on the date of study error counts and rows before and after the syntactic checks\n            error_rate (dict): cotains data on the error rates for all lookback dates and date of study.\n            raw_average (float): average of rows in the raw data before syntactic checks\n            clean_average (float): average of rows in the clean data after syntactic checks\n            error_rate_avg (float): average of the error rate observed in the syntactic checks\n            raw_UCL (float): upper control limit for the rows in the raw data before syntactic checks\n            clean_UCL (float): upper control limit for the rows in the clean data after syntactic checks\n            error_rate_UCL (float): upper control limit for the error rate\n            raw_LCL (float): lower control limit for the rows in the raw data before syntactic checks\n            clean_LCL (float): lower control limit for the rows in the raw data after syntactic checks\n        \"\"\"\n        # self.plots_data = {\"rows_before_syntactic_check\": [], \"rows_after_syntactic_check\": [], \"error_rate\": []}\n\n        plots_data = {\"line_plot\": [], \"pie_plot\": []}\n\n        for date in sorted(self.lookback_dates) + [self.date_of_study]:\n            plots_data[\"line_plot\"].extend(\n                [\n                    Row(\n                        **{\n                            ColNames.date: date,\n                            ColNames.daily_value: float(lookback_initial_rows[date]),\n                            ColNames.average: float(raw_average),\n                            ColNames.UCL: float(raw_UCL),\n                            ColNames.LCL: float(raw_LCL),\n                            ColNames.variable: \"rows_before_syntactic_check\",\n                            ColNames.year: self.date_of_study.year,\n                            ColNames.month: self.date_of_study.month,\n                            ColNames.day: self.date_of_study.day,\n                            ColNames.timestamp: self.timestamp,\n                        }\n                    )\n                ]\n            )\n\n            plots_data[\"line_plot\"].extend(\n                [\n                    Row(\n                        **{\n                            ColNames.date: date,\n                            ColNames.daily_value: float(lookback_final_rows[date]),\n                            ColNames.average: float(clean_average),\n                            ColNames.UCL: float(clean_UCL),\n                            ColNames.LCL: float(clean_LCL),\n                            ColNames.variable: \"rows_after_syntactic_check\",\n                            ColNames.year: self.date_of_study.year,\n                            ColNames.month: self.date_of_study.month,\n                            ColNames.day: self.date_of_study.day,\n                            ColNames.timestamp: self.timestamp,\n                        }\n                    )\n                ]\n            )\n\n            plots_data[\"line_plot\"].extend(\n                [\n                    Row(\n                        **{\n                            ColNames.date: date,\n                            ColNames.daily_value: float(error_rate[date]),\n                            ColNames.average: float(error_rate_avg),\n                            ColNames.UCL: float(error_rate_UCL),\n                            ColNames.LCL: None,\n                            ColNames.variable: \"error_rate\",\n                            ColNames.year: self.date_of_study.year,\n                            ColNames.month: self.date_of_study.month,\n                            ColNames.day: self.date_of_study.day,\n                            ColNames.timestamp: self.timestamp,\n                        }\n                    )\n                ]\n            )\n\n        # Now, the data for the pie charts\n        # Ugly way to get the relation error_code -&gt; error attribute name\n        error_types = {getattr(NetworkErrorType, att): att for att in dir(NetworkErrorType) if not att.startswith(\"__\")}\n\n        for field_name, error_counts in today_values.items():\n            if field_name in [None, \"dates\"]:\n                continue\n\n            # boolean check if this field had any errors or not\n            field_has_errors = False\n\n            for key in error_counts.keys():\n                if key != NetworkErrorType.NO_ERROR:\n                    if error_counts[key] &gt; 0:\n                        # self.plots_data[field_name].append(\n                        #     field_name, error_types[key], error_counts[key]\n                        # )\n                        field_has_errors = True\n                        plots_data[\"pie_plot\"].append(\n                            Row(\n                                **{\n                                    ColNames.type_code: error_types[key],\n                                    ColNames.value: error_counts[key],\n                                    ColNames.variable: field_name,\n                                    ColNames.year: self.date_of_study.year,\n                                    ColNames.month: self.date_of_study.month,\n                                    ColNames.day: self.date_of_study.day,\n                                    ColNames.timestamp: self.timestamp,\n                                }\n                            )\n                        )\n            if not field_has_errors:\n                self.logger.info(f\"Field `{field_name}` had no errors\")\n        self.output_data_objects[SilverNetworkSyntacticQualityWarningsLinePlotData.ID].df = self.spark.createDataFrame(\n            plots_data[\"line_plot\"], schema=SilverNetworkSyntacticQualityWarningsLinePlotData.SCHEMA\n        )\n\n        self.output_data_objects[SilverNetworkSyntacticQualityWarningsPiePlotData.ID].df = self.spark.createDataFrame(\n            plots_data[\"pie_plot\"], schema=SilverNetworkSyntacticQualityWarningsPiePlotData.SCHEMA\n        )\n\n    def write(self):\n        # Write regular data objects\n        self.logger.info(\"Writing data objects...\")\n        super().write()\n        self.logger.info(\"Data objects written.\")\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.all_specific_error_warnings","title":"<code>all_specific_error_warnings(lookback_stats, today_values)</code>","text":"<p>Parent method for the creation of warnings for each type of error rate</p> <p>lookback_stats (dict): contains error information of each date of the lookback period today_values (dict): contains error information of the date of study</p> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def all_specific_error_warnings(self, lookback_stats, today_values):\n    \"\"\"Parent method for the creation of warnings for each type of error rate\n\n    lookback_stats (dict): contains error information of each date of the lookback period\n    today_values (dict): contains error information of the date of study\n    \"\"\"\n    error_rate_types = [\"Missing_value_RATE\", \"Out_of_range_RATE\", \"Parsing_error_RATE\"]\n\n    for error_rate_type in error_rate_types:\n        for field_name in self.thresholds[error_rate_type]:\n            self.specific_error_warnings(error_rate_type, field_name, lookback_stats, today_values)\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.check_needed_dates","title":"<code>check_needed_dates()</code>","text":"<p>Method that checks if both the date of study and the dates necessary to generate the quality warnings, specified through the lookback_period parameter, are present in the input data.</p> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def check_needed_dates(self) -&gt; None:\n    \"\"\"\n    Method that checks if both the date of study and the dates necessary to generate\n    the quality warnings, specified through the lookback_period parameter, are present\n    in the input data.\n    \"\"\"\n\n    # Collect all distinct dates in the input quality metrics within the needed range\n    metrics = self.input_data_objects[SilverNetworkDataQualityMetricsByColumn.ID].df\n    dates = (\n        metrics.filter(\n            F.col(\"date\")\n            # left- and right- inclusive\n            .between(\n                self.date_of_study - datetime.timedelta(days=self.PERIOD_DURATION[self.lookback_period]),\n                self.date_of_study,\n            )\n        )\n        .select(F.col(ColNames.date))\n        .distinct()\n        .collect()\n    )\n\n    dates = [row[ColNames.date] for row in dates]\n\n    if self.date_of_study not in dates:\n        raise ValueError(\n            f\"The date of study, {self.date_of_study}, is not present in the input Quality Metrics data\"\n        )\n\n    if set(self.lookback_dates).intersection(set(dates)) != set(self.lookback_dates):\n        error_msg = f\"\"\"\n            The following dates from the lookback period are not present in the\n            input Quality Metrics data:\n            {\n                sorted(\n                    map(\n                        lambda x: x.strftime(self.date_format),\n                        set(self.lookback_dates).difference(set(dates))\n                    )\n                )\n            }\"\"\"\n\n        self.logger.error(error_msg)\n        raise ValueError(error_msg)\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.clean_size_warnings","title":"<code>clean_size_warnings(lookback_stats, today_values)</code>","text":"<p>Method that generates quality warnings, that will be recorded in the output log table, for the metric regarding the final number of rows of the raw input network topology data prior to the syntactic check procedure.</p> A total of six warnings might be generated <ul> <li>The study date's number of rows is greater than the average number of rows over the previous period     by more than a threshold percentage.</li> <li>The study date's number of rows is smaller than the average number of rows over the previous period     by more than a threshold percentage.</li> <li>The study date's number of rows is greater than the average number of rows over the previous period     by a config-specified number of standard deviations.</li> <li>The study date's number of rows is smaller than the average number of rows over the previous period     by a config-specified number of standard deviations.</li> <li>The study date's number of rows is greater than a config-specified threshold.</li> <li>The study date's number of rows is smaller than a config-specified threshold.</li> </ul> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def clean_size_warnings(self, lookback_stats, today_values):\n    \"\"\"Method that generates quality warnings, that will be recorded in the output log table,\n    for the metric regarding the final number of rows of the raw input network topology data prior\n    to the syntactic check procedure.\n\n    A total of six warnings might be generated:\n        - The study date's number of rows is greater than the average number of rows over the previous period\n            by more than a threshold percentage.\n        - The study date's number of rows is smaller than the average number of rows over the previous period\n            by more than a threshold percentage.\n        - The study date's number of rows is greater than the average number of rows over the previous period\n            by a config-specified number of standard deviations.\n        - The study date's number of rows is smaller than the average number of rows over the previous period\n            by a config-specified number of standard deviations.\n        - The study date's number of rows is greater than a config-specified threshold.\n        - The study date's number of rows is smaller than a config-specified threshold.\n    \"\"\"\n    current_val = today_values[None][NetworkErrorType.FINAL_ROWS]\n    previous_avg = lookback_stats[None][NetworkErrorType.FINAL_ROWS][\"average\"]\n    previous_std = lookback_stats[None][NetworkErrorType.FINAL_ROWS][\"stddev\"]\n\n    measure_definition = self.MEASURE_DEFINITION[\"SIZE_CLEAN_DATA\"]\n\n    over_average = self.thresholds[\"SIZE_CLEAN_DATA\"][\"OVER_AVERAGE\"]\n    under_average = self.thresholds[\"SIZE_CLEAN_DATA\"][\"UNDER_AVERAGE\"]\n    variability = self.thresholds[\"SIZE_CLEAN_DATA\"][\"VARIABILITY\"]\n    absolute_upper_control_limit = self.thresholds[\"SIZE_CLEAN_DATA\"][\"ABS_VALUE_UPPER_LIMIT\"]\n    absolute_lower_control_limit = self.thresholds[\"SIZE_CLEAN_DATA\"][\"ABS_VALUE_LOWER_LIMIT\"]\n\n    if previous_avg == 0:\n        self.logger.warning(\n            f\"\"\"\n            The average of the number of rows in the clean input network topology data after syntactic checks is 0\n            for the lookback period especified ({self.lookback_period_start} to {self.lookback_period_end}, \n            both included)\n            \"\"\"\n        )\n    else:\n        pct_difference = 100 * current_val / previous_avg - 100\n        if pct_difference &gt; over_average:\n            self.register_warning(\n                measure_definition,\n                pct_difference,\n                f\"Size is greater than the previous period's average by {over_average} %\",\n                over_average,\n                \"The number of cells after the syntactic checks procedure is unexpectedly high with respect to the previous period.\",\n            )\n\n        if pct_difference &lt; -under_average:\n            self.register_warning(\n                measure_definition,\n                pct_difference,\n                f\"Size is smaller than the previous period's average by {under_average} %\",\n                under_average,\n                \"The number of cells after the syntactic checks procedure is unexpectedly low with respect to the previous period.\",\n            )\n\n    upper_control_limit = previous_avg + variability * previous_std\n    lower_control_limit = previous_avg - variability * previous_std\n\n    if current_val &gt; upper_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            \"Size is out of the upper control limit calculated on the basis of average and standard deviation of the distribution \"\n            f\"of the size in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\",\n            upper_control_limit,\n            \"The number of cells is unexpectedly high with respect to the previous period, taking into account the usual \"\n            \"variability of the cell numbers, please check if there have been issues in the network.\",\n        )\n    if current_val &lt; lower_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            \"Size is out of the lower control limit calculated on the basis of average and standard deviation of the distribution \"\n            f\"of the size in the previous period. Lower control limit = (average - {variability}\u00b7stddev)\",\n            lower_control_limit,\n            \"The number of cells is unexpectedly low with respect to the previous period, taking into account the usual \"\n            \"variability of the cell numbers, please check if there have been issues in the network.\",\n        )\n\n    # Use of UCL and LCL as default values for the absolute limits\n    if absolute_upper_control_limit is None:\n        absolute_upper_control_limit = upper_control_limit\n    if absolute_lower_control_limit is None:\n        absolute_lower_control_limit = lower_control_limit\n\n    if current_val &gt; absolute_upper_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            f\"The size is over the threshold {absolute_upper_control_limit}\",\n            absolute_upper_control_limit,\n            \"The number of cells after the syntactic checks procedure is over the threshold.\",\n        )\n    if current_val &lt; absolute_lower_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            f\"The size is under the threshold {absolute_lower_control_limit}\",\n            absolute_upper_control_limit,\n            \"The number of cells after the syntactic checks procedure is under the threshold.\",\n        )\n\n    return previous_avg, upper_control_limit, lower_control_limit\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.create_plots_data","title":"<code>create_plots_data(lookback_initial_rows, lookback_final_rows, today_values, error_rate, raw_average, clean_average, error_rate_avg, raw_UCL, clean_UCL, error_rate_UCL, raw_LCL, clean_LCL)</code>","text":"<p>Method that takes the data needed to generate the required plots, and formats it for easier plotting and saving later on.</p> <p>Parameters:</p> Name Type Description Default <code>lookback_initial_rows</code> <code>dict</code> <p>contains data on the lookback period dates' number of rows before syntactic checks</p> required <code>lookback_final_rows</code> <code>dict</code> <p>contains data on the lookback period dates' number of rows after syntactic checks</p> required <code>today_values</code> <code>dict</code> <p>contains data on the date of study error counts and rows before and after the syntactic checks</p> required <code>error_rate</code> <code>dict</code> <p>cotains data on the error rates for all lookback dates and date of study.</p> required <code>raw_average</code> <code>float</code> <p>average of rows in the raw data before syntactic checks</p> required <code>clean_average</code> <code>float</code> <p>average of rows in the clean data after syntactic checks</p> required <code>error_rate_avg</code> <code>float</code> <p>average of the error rate observed in the syntactic checks</p> required <code>raw_UCL</code> <code>float</code> <p>upper control limit for the rows in the raw data before syntactic checks</p> required <code>clean_UCL</code> <code>float</code> <p>upper control limit for the rows in the clean data after syntactic checks</p> required <code>error_rate_UCL</code> <code>float</code> <p>upper control limit for the error rate</p> required <code>raw_LCL</code> <code>float</code> <p>lower control limit for the rows in the raw data before syntactic checks</p> required <code>clean_LCL</code> <code>float</code> <p>lower control limit for the rows in the raw data after syntactic checks</p> required Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def create_plots_data(\n    self,\n    lookback_initial_rows,\n    lookback_final_rows,\n    today_values,\n    error_rate,\n    raw_average,\n    clean_average,\n    error_rate_avg,\n    raw_UCL,\n    clean_UCL,\n    error_rate_UCL,\n    raw_LCL,\n    clean_LCL,\n):\n    \"\"\"Method that takes the data needed to generate the required plots, and formats it for\n    easier plotting and saving later on.\n\n    Args:\n        lookback_initial_rows (dict): contains data on the lookback period dates' number of rows before syntactic checks\n        lookback_final_rows (dict): contains data on the lookback period dates' number of rows after syntactic checks\n        today_values (dict): contains data on the date of study error counts and rows before and after the syntactic checks\n        error_rate (dict): cotains data on the error rates for all lookback dates and date of study.\n        raw_average (float): average of rows in the raw data before syntactic checks\n        clean_average (float): average of rows in the clean data after syntactic checks\n        error_rate_avg (float): average of the error rate observed in the syntactic checks\n        raw_UCL (float): upper control limit for the rows in the raw data before syntactic checks\n        clean_UCL (float): upper control limit for the rows in the clean data after syntactic checks\n        error_rate_UCL (float): upper control limit for the error rate\n        raw_LCL (float): lower control limit for the rows in the raw data before syntactic checks\n        clean_LCL (float): lower control limit for the rows in the raw data after syntactic checks\n    \"\"\"\n    # self.plots_data = {\"rows_before_syntactic_check\": [], \"rows_after_syntactic_check\": [], \"error_rate\": []}\n\n    plots_data = {\"line_plot\": [], \"pie_plot\": []}\n\n    for date in sorted(self.lookback_dates) + [self.date_of_study]:\n        plots_data[\"line_plot\"].extend(\n            [\n                Row(\n                    **{\n                        ColNames.date: date,\n                        ColNames.daily_value: float(lookback_initial_rows[date]),\n                        ColNames.average: float(raw_average),\n                        ColNames.UCL: float(raw_UCL),\n                        ColNames.LCL: float(raw_LCL),\n                        ColNames.variable: \"rows_before_syntactic_check\",\n                        ColNames.year: self.date_of_study.year,\n                        ColNames.month: self.date_of_study.month,\n                        ColNames.day: self.date_of_study.day,\n                        ColNames.timestamp: self.timestamp,\n                    }\n                )\n            ]\n        )\n\n        plots_data[\"line_plot\"].extend(\n            [\n                Row(\n                    **{\n                        ColNames.date: date,\n                        ColNames.daily_value: float(lookback_final_rows[date]),\n                        ColNames.average: float(clean_average),\n                        ColNames.UCL: float(clean_UCL),\n                        ColNames.LCL: float(clean_LCL),\n                        ColNames.variable: \"rows_after_syntactic_check\",\n                        ColNames.year: self.date_of_study.year,\n                        ColNames.month: self.date_of_study.month,\n                        ColNames.day: self.date_of_study.day,\n                        ColNames.timestamp: self.timestamp,\n                    }\n                )\n            ]\n        )\n\n        plots_data[\"line_plot\"].extend(\n            [\n                Row(\n                    **{\n                        ColNames.date: date,\n                        ColNames.daily_value: float(error_rate[date]),\n                        ColNames.average: float(error_rate_avg),\n                        ColNames.UCL: float(error_rate_UCL),\n                        ColNames.LCL: None,\n                        ColNames.variable: \"error_rate\",\n                        ColNames.year: self.date_of_study.year,\n                        ColNames.month: self.date_of_study.month,\n                        ColNames.day: self.date_of_study.day,\n                        ColNames.timestamp: self.timestamp,\n                    }\n                )\n            ]\n        )\n\n    # Now, the data for the pie charts\n    # Ugly way to get the relation error_code -&gt; error attribute name\n    error_types = {getattr(NetworkErrorType, att): att for att in dir(NetworkErrorType) if not att.startswith(\"__\")}\n\n    for field_name, error_counts in today_values.items():\n        if field_name in [None, \"dates\"]:\n            continue\n\n        # boolean check if this field had any errors or not\n        field_has_errors = False\n\n        for key in error_counts.keys():\n            if key != NetworkErrorType.NO_ERROR:\n                if error_counts[key] &gt; 0:\n                    # self.plots_data[field_name].append(\n                    #     field_name, error_types[key], error_counts[key]\n                    # )\n                    field_has_errors = True\n                    plots_data[\"pie_plot\"].append(\n                        Row(\n                            **{\n                                ColNames.type_code: error_types[key],\n                                ColNames.value: error_counts[key],\n                                ColNames.variable: field_name,\n                                ColNames.year: self.date_of_study.year,\n                                ColNames.month: self.date_of_study.month,\n                                ColNames.day: self.date_of_study.day,\n                                ColNames.timestamp: self.timestamp,\n                            }\n                        )\n                    )\n        if not field_has_errors:\n            self.logger.info(f\"Field `{field_name}` had no errors\")\n    self.output_data_objects[SilverNetworkSyntacticQualityWarningsLinePlotData.ID].df = self.spark.createDataFrame(\n        plots_data[\"line_plot\"], schema=SilverNetworkSyntacticQualityWarningsLinePlotData.SCHEMA\n    )\n\n    self.output_data_objects[SilverNetworkSyntacticQualityWarningsPiePlotData.ID].df = self.spark.createDataFrame(\n        plots_data[\"pie_plot\"], schema=SilverNetworkSyntacticQualityWarningsPiePlotData.SCHEMA\n    )\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.error_rate_warnings","title":"<code>error_rate_warnings(initial_rows, final_rows, today_values)</code>","text":"<p>Method that generates quality warnings, that will be recorded in the output log table, for the metric regarding the error rate observed in the syntactic check procedure.</p> A total of three warnings might be generated <ul> <li>The study date's error rate is greater than the average number of rows over the previous period     by more than a threshold percentage.</li> <li>The study date's error rate is greater than the average number of rows over the previous period     by a config-specified number of standard deviations.</li> <li>The study date's error rate is greater than a config-specified threshold.</li> </ul> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def error_rate_warnings(self, initial_rows, final_rows, today_values):\n    \"\"\"Method that generates quality warnings, that will be recorded in the output log table,\n    for the metric regarding the error rate observed in the syntactic check procedure.\n\n    A total of three warnings might be generated:\n        - The study date's error rate is greater than the average number of rows over the previous period\n            by more than a threshold percentage.\n        - The study date's error rate is greater than the average number of rows over the previous period\n            by a config-specified number of standard deviations.\n        - The study date's error rate is greater than a config-specified threshold.\n    \"\"\"\n    if len(initial_rows) != len(final_rows):\n        raise ValueError(\n            \"Input Quality Metrics do not have information on the number of rows \"\n            \"before and after syntactic checks on all dates considered!\"\n        )\n\n    error_rate = {\n        date: 100 * (initial_rows[date] - final_rows[date]) / initial_rows[date] for date in initial_rows.keys()\n    }\n\n    current_val = (\n        100\n        * (today_values[None][NetworkErrorType.INITIAL_ROWS] - today_values[None][NetworkErrorType.FINAL_ROWS])\n        / today_values[None][NetworkErrorType.INITIAL_ROWS]\n    )\n    previous_avg = sum(error_rate[date] for date in self.lookback_dates) / len(self.lookback_dates)\n    previous_std = math.sqrt(\n        sum((error_rate[date] - previous_avg) ** 2 for date in self.lookback_dates) / (len(self.lookback_dates) - 1)\n    )\n\n    measure_definition = self.MEASURE_DEFINITION[\"TOTAL_ERROR_RATE\"]\n\n    over_average = self.thresholds[\"TOTAL_ERROR_RATE\"][\"OVER_AVERAGE\"]\n    variability = self.thresholds[\"TOTAL_ERROR_RATE\"][\"VARIABILITY\"]\n    absolute_upper_control_limit = self.thresholds[\"TOTAL_ERROR_RATE\"][\"ABS_VALUE_UPPER_LIMIT\"]\n\n    if previous_avg == 0:\n        self.logger.warning(\n            f\"\"\"\n            The average error rate in the input network topology data 0\n            for the lookback period especified ({self.lookback_period_start} to {self.lookback_period_end}, \n            both included)\n            \"\"\"\n        )\n    else:\n        pct_difference = 100 * current_val / previous_avg - 100\n        if pct_difference &gt; over_average:\n            self.register_warning(\n                measure_definition,\n                pct_difference,\n                f\"Error rate is greater than the previous period's average by more than {over_average} %\",\n                over_average,\n                \"The error rate after the syntactic checks procedure is unexpectedly high with respect to the previous period.\",\n            )\n    upper_control_limit = previous_avg + variability * previous_std\n\n    if current_val &gt; upper_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            \"Error rate is over the upper control limit calculated on the basis of average and standard deviation of the distribution \"\n            f\"of the size in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\",\n            upper_control_limit,\n            \"The error rate after the syntactic checks procedure is unexpectedly high with respect to the previous period, taking into account the usual \"\n            \"variability.\",\n        )\n\n    if current_val &gt; absolute_upper_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            f\"The error rate is over the value {absolute_upper_control_limit}\",\n            absolute_upper_control_limit,\n            \"The error rate after the syntactic checks procedure is over the threshold.\",\n        )\n    error_rate[self.date_of_study] = current_val\n    return error_rate, previous_avg, upper_control_limit\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.get_lookback_period_statistics","title":"<code>get_lookback_period_statistics()</code>","text":"<p>Method that computes the necessary statistics (average and standard deviation) from the Quality Metrics of the lookback period.</p> <p>Returns:</p> Name Type Description <code>statistics</code> <code>dict</code> <p>dictionary containing said necessary statistics, with the following structure: {     field_name1: {         type_error1: {             'average': 12.2,             'stddev': 17.5         },         type_error2 : {             'average': 4.5,             'stddev': 10.1         }     },     ... }</p> <code>initial_rows</code> <code>dict</code> <p>dictionary indicating the number of rows before syntactic checks for the previous period, {date -&gt; size_raw_data}</p> <code>final_rows</code> <code>dict</code> <p>dictionary indicating the number of rows after syntactic checks for the previous period, {date -&gt; size_raw_data}</p> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def get_lookback_period_statistics(self) -&gt; dict:\n    \"\"\"Method that computes the necessary statistics (average and standard deviation) from the\n    Quality Metrics of the lookback period.\n\n    Returns:\n        statistics (dict): dictionary containing said necessary statistics, with the following\n            structure:\n            {\n                field_name1: {\n                    type_error1: {\n                        'average': 12.2,\n                        'stddev': 17.5\n                    },\n                    type_error2 : {\n                        'average': 4.5,\n                        'stddev': 10.1\n                    }\n                },\n                ...\n            }\n        initial_rows (dict): dictionary indicating the number of rows before syntactic checks for the previous period, {date -&gt; size_raw_data}\n        final_rows (dict): dictionary indicating the number of rows after syntactic checks for the previous period, {date -&gt; size_raw_data}\n    \"\"\"\n    intermediate_df = self.input_data_objects[SilverNetworkDataQualityMetricsByColumn.ID].df.filter(\n        F.col(ColNames.date).between(self.lookback_period_start, self.lookback_period_end)\n    )\n\n    intermediate_df.cache()\n\n    lookback_stats = (\n        intermediate_df.groupBy([ColNames.field_name, ColNames.type_code])\n        .agg(F.mean(F.col(ColNames.value)).alias(\"average\"), F.stddev_samp(F.col(ColNames.value)).alias(\"stddev\"))\n        .collect()\n    )\n\n    error_rate_data = (\n        intermediate_df.select([ColNames.type_code, ColNames.value, ColNames.date])\n        .filter(F.col(ColNames.type_code).isin([NetworkErrorType.INITIAL_ROWS, NetworkErrorType.FINAL_ROWS]))\n        .collect()\n    )\n\n    intermediate_df.unpersist()\n\n    initial_rows = {}\n    final_rows = {}\n\n    for row in error_rate_data:\n        if row[ColNames.type_code] == NetworkErrorType.INITIAL_ROWS:\n            initial_rows[row[ColNames.date]] = row[\"value\"]\n        else:\n            final_rows[row[ColNames.date]] = row[\"value\"]\n\n    statistics = dict()\n    for row in lookback_stats:\n        field_name, type_code, average, stddev = (\n            row[ColNames.field_name],\n            row[ColNames.type_code],\n            row[\"average\"],\n            row[\"stddev\"],\n        )\n        if field_name not in statistics:\n            statistics[field_name] = dict()\n        statistics[field_name][type_code] = {\"average\": average, \"stddev\": stddev}\n\n    return statistics, initial_rows, final_rows\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.get_study_date_values","title":"<code>get_study_date_values()</code>","text":"<p>Method that reads and returns the quality metrics of the date of study.</p> <p>Returns:</p> Name Type Description <code>today_values</code> <code>dict</code> <p>dictionary containing said values, with the following structure: {     field_name1: {         type_error1: 123,         type_error2: 23,         type_error3: 0     },     field_name2: {         type_error1: 0,         type_error2: 0,         type_error3: 300     }, }</p> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def get_study_date_values(self) -&gt; dict:\n    \"\"\"Method that reads and returns the quality metrics of the date of study.\n\n    Returns:\n        today_values (dict): dictionary containing said values, with the following\n            structure:\n            {\n                field_name1: {\n                    type_error1: 123,\n                    type_error2: 23,\n                    type_error3: 0\n                },\n                field_name2: {\n                    type_error1: 0,\n                    type_error2: 0,\n                    type_error3: 300\n                },\n            }\n    \"\"\"\n    today_metrics = (\n        self.input_data_objects[SilverNetworkDataQualityMetricsByColumn.ID]\n        .df.filter(F.col(ColNames.date) == F.lit(self.date_of_study))\n        .select([ColNames.field_name, ColNames.type_code, ColNames.value])\n    ).collect()\n\n    today_values = {}\n\n    for row in today_metrics:\n        field_name, type_code, value = row[ColNames.field_name], row[ColNames.type_code], row[ColNames.value]\n\n        if field_name not in today_values:\n            today_values[field_name] = {}\n\n        today_values[field_name][type_code] = value\n\n    return today_values\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.get_thresholds","title":"<code>get_thresholds()</code>","text":"<p>Method that reads the threshold-related parameters, contained in the config file, and saves them in memory to use them instead of the ones specified by default. Raises:     ValueError: non-numerical value that cannot be parsed to float has been used in         the config file     ValueError: Negative value for a given parameter has been given, when only         non-negative values make sense and are thus allowed.</p> <p>Returns:</p> Name Type Description <code>thresholds</code> <code>dict</code> <p>dict containing the different thresholds used for computing the quality warnings, with the same structure as multimno.core.constants.network_default_thresholds.NETWORK_DEFAULT_THRESHOLDS</p> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def get_thresholds(self) -&gt; dict:\n    \"\"\"\n    Method that reads the threshold-related parameters, contained in the config file,\n    and saves them in memory to use them instead of the ones specified by default.\n    Raises:\n        ValueError: non-numerical value that cannot be parsed to float has been used in\n            the config file\n        ValueError: Negative value for a given parameter has been given, when only\n            non-negative values make sense and are thus allowed.\n\n    Returns:\n        thresholds: dict containing the different thresholds used for computing the\n            quality warnings, with the same structure as\n            multimno.core.constants.network_default_thresholds.NETWORK_DEFAULT_THRESHOLDS\n    \"\"\"\n    config_thresholds = self.config.geteval(self.COMPONENT_ID, \"thresholds\")\n    thresholds = NETWORK_DEFAULT_THRESHOLDS\n\n    for error_key in config_thresholds.keys():\n        if error_key not in NETWORK_DEFAULT_THRESHOLDS:\n            self.logger.info(f\"Parameter key {error_key} unknown -- ignored\")\n            continue\n\n        if error_key in [\"SIZE_RAW_DATA\", \"SIZE_CLEAN_DATA\", \"TOTAL_ERROR_RATE\"]:\n            for param_key, val in config_thresholds[error_key].items():\n                if param_key not in NETWORK_DEFAULT_THRESHOLDS[error_key]:\n                    self.logger.info(f\"Unknown parameter {param_key} under {error_key} -- ignored\")\n                    continue\n\n                try:\n                    val = float(val)\n                except ValueError as e:\n                    error_msg = (\n                        f\"Expected numeric value for parameter {param_key} under {error_key} - got {val} instead.\"\n                    )\n                    self.logger.error(error_msg)\n                    raise e\n\n                if val &lt; 0:\n                    error_msg = f\"Parameter {param_key} under {error_key} must be non-negative - got {val}\"\n                    self.logger.error(error_msg)\n                    raise ValueError(error_msg)\n\n                thresholds[error_key][param_key] = val\n\n        else:  # then is one of 'Missing_value_RATE', 'Out_of_range_RATE', 'Parsing_error_RATE'\n            for field_name in config_thresholds[error_key]:\n                if field_name not in NETWORK_DEFAULT_THRESHOLDS[error_key]:\n                    self.logger.info(f\"Unknown field name {field_name} under {error_key} -- ignored\")\n                    continue\n\n                for param_key, val in config_thresholds[error_key][field_name].items():\n                    if param_key not in NETWORK_DEFAULT_THRESHOLDS[error_key][field_name]:\n                        self.logger.info(\n                            f'Unknown parameter {param_key} under {error_key}[\"{field_name}\"] -- ignored'\n                        )\n                        continue\n\n                    try:\n                        val = float(val)\n                    except ValueError as e:\n                        error_msg = f'Expected numeric value for parameter {param_key} under {error_key}[\"{field_name}\"] - got {val} instead.'\n                        self.logger.error(error_msg)\n                        raise e\n\n                    if val &lt; 0:\n                        error_msg = f'Parameter {param_key} under {error_key}[\"{field_name}\"] must be non-negative - got {val}'\n                        self.logger.error(error_msg)\n                        raise ValueError(error_msg)\n\n                    thresholds[error_key][field_name][param_key] = val\n\n    return thresholds\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.raw_size_warnings","title":"<code>raw_size_warnings(lookback_stats, today_values)</code>","text":"<p>Method that generates the quality warnings, that will be recorded in the output log table, for the metric regarding the initial number of rows of the raw input network topology data prior to the syntactic check procedure.</p> A total of six warnings might be generated <ul> <li>The study date's number of rows is greater than the average number of rows over the previous period     by more than a threshold percentage.</li> <li>The study date's number of rows is smaller than the average number of rows over the previous period     by more than a threshold percentage.</li> <li>The study date's number of rows is greater than the average number of rows over the previous period     by a config-specified number of standard deviations.</li> <li>The study date's number of rows is smaller than the average number of rows over the previous period     by a config-specified number of standard deviations.</li> <li>The study date's number of rows is greater than a config-specified threshold.</li> <li>The study date's number of rows is smaller than a config-specified threshold.</li> </ul> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def raw_size_warnings(self, lookback_stats, today_values):\n    \"\"\"Method that generates the quality warnings, that will be recorded in the output log table,\n    for the metric regarding the initial number of rows of the raw input network topology data prior\n    to the syntactic check procedure.\n\n    A total of six warnings might be generated:\n        - The study date's number of rows is greater than the average number of rows over the previous period\n            by more than a threshold percentage.\n        - The study date's number of rows is smaller than the average number of rows over the previous period\n            by more than a threshold percentage.\n        - The study date's number of rows is greater than the average number of rows over the previous period\n            by a config-specified number of standard deviations.\n        - The study date's number of rows is smaller than the average number of rows over the previous period\n            by a config-specified number of standard deviations.\n        - The study date's number of rows is greater than a config-specified threshold.\n        - The study date's number of rows is smaller than a config-specified threshold.\n    \"\"\"\n    current_val = today_values[None][NetworkErrorType.INITIAL_ROWS]\n    previous_avg = lookback_stats[None][NetworkErrorType.INITIAL_ROWS][\"average\"]\n    previous_std = lookback_stats[None][NetworkErrorType.INITIAL_ROWS][\"stddev\"]\n\n    measure_definition = self.MEASURE_DEFINITION[\"SIZE_RAW_DATA\"]\n\n    over_average = self.thresholds[\"SIZE_RAW_DATA\"][\"OVER_AVERAGE\"]\n    under_average = self.thresholds[\"SIZE_RAW_DATA\"][\"UNDER_AVERAGE\"]\n    variability = self.thresholds[\"SIZE_RAW_DATA\"][\"VARIABILITY\"]\n    absolute_upper_control_limit = self.thresholds[\"SIZE_RAW_DATA\"][\"ABS_VALUE_UPPER_LIMIT\"]\n    absolute_lower_control_limit = self.thresholds[\"SIZE_RAW_DATA\"][\"ABS_VALUE_LOWER_LIMIT\"]\n\n    if previous_avg == 0:\n        self.logger.warning(\n            f\"\"\"\n            The average of the number of rows in the raw input network topology data is 0\n            for the lookback period especified ({self.lookback_period_start} to {self.lookback_period_end},\n             both included)\n            \"\"\"\n        )\n    else:\n        pct_difference = 100 * current_val / previous_avg - 100\n        if pct_difference &gt; over_average:\n            self.register_warning(\n                measure_definition,\n                pct_difference,\n                f\"Size is greater than the previous period's average by more than {over_average} %\",\n                over_average,\n                \"The number of cells is unexpectedly high with respect to the previous period, please check if there have been issues in the network\",\n            )\n\n        if pct_difference &lt; -under_average:\n            self.register_warning(\n                measure_definition,\n                pct_difference,\n                f\"Size is smaller than the previous period's average by more than {under_average} %\",\n                under_average,\n                \"The number of cells is unexpectedly low with respect to the previous period, please check if there have been issues in the network.\",\n            )\n\n    upper_control_limit = previous_avg + variability * previous_std\n    lower_control_limit = previous_avg - variability * previous_std\n\n    if current_val &gt; upper_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            \"Size is out of the upper control limit calculated on the basis of average and standard deviation of the distribution \"\n            f\"of the size in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\",\n            upper_control_limit,\n            \"The number of cells is unexpectedly high with respect to the previous period, taking into account the usual \"\n            \"variability of the cell numbers, please check if there have been issues in the network.\",\n        )\n    if current_val &lt; lower_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            \"Size is out of the lower control limit calculated on the basis of average and standard deviation of the distribution \"\n            f\"of the size in the previous period. Lower control limit = (average - {variability}\u00b7stddev)\",\n            lower_control_limit,\n            \"The number of cells is unexpectedly low with respect to the previous period, taking into account the usual \"\n            \"variability of the cell numbers, please check if there have been issues in the network.\",\n        )\n\n    # Use of UCL and LCL as default values for the absolute limits\n    if absolute_upper_control_limit is None:\n        absolute_upper_control_limit = upper_control_limit\n    if absolute_lower_control_limit is None:\n        absolute_lower_control_limit = lower_control_limit\n\n    if current_val &gt; absolute_upper_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            f\"The size is over the threshold {absolute_upper_control_limit}\",\n            absolute_upper_control_limit,\n            \"The number of cells is over the threshold, please check if there have been changes in the network.\",\n        )\n    if current_val &lt; absolute_lower_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            f\"The size is under the threshold {absolute_lower_control_limit}\",\n            absolute_upper_control_limit,\n            \"The number of cells is under the threshold, please check if there have been changes in the network.\",\n        )\n\n    return previous_avg, upper_control_limit, lower_control_limit\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.register_warning","title":"<code>register_warning(measure_definition, daily_value, condition, condition_value, warning_text)</code>","text":"<p>Method that abstracts away the creation in the correct format and data type, each of the quality warnings that will be recorded in the log table.</p> <p>Parameters:</p> Name Type Description Default <code>measure_definition</code> <code>str</code> <p>measure that raised the warning (e.g. Error rate)</p> required <code>daily_value</code> <code>float</code> <p>measure's value in the date of study that raised the warning</p> required <code>condition</code> <code>str</code> <p>test that was checked in order to raise the warning</p> required <code>condition_value</code> <code>float</code> <p>value against which the date of study's daily_value was compared</p> required <code>warning_text</code> <code>str</code> <p>verbose explanation of the condition being satisfied and the warning being raised</p> required Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def register_warning(\n    self, measure_definition: str, daily_value: float, condition: str, condition_value: float, warning_text: str\n) -&gt; None:\n    \"\"\"Method that abstracts away the creation in the correct format and data type, each of the quality warnings\n    that will be recorded in the log table.\n\n    Args:\n        measure_definition (str): measure that raised the warning (e.g. Error rate)\n        daily_value (float): measure's value in the date of study that raised the warning\n        condition (str): test that was checked in order to raise the warning\n        condition_value (float): value against which the date of study's daily_value was compared\n        warning_text (str): verbose explanation of the condition being satisfied and the warning\n            being raised\n    \"\"\"\n    warning = {\n        ColNames.title: self.TITLE,\n        ColNames.date: self.date_of_study,\n        ColNames.timestamp: self.timestamp,\n        ColNames.measure_definition: measure_definition,\n        ColNames.daily_value: float(daily_value),\n        ColNames.condition: condition,\n        ColNames.lookback_period: self.lookback_period,\n        ColNames.condition_value: float(condition_value),\n        ColNames.warning_text: warning_text,\n        ColNames.year: self.date_of_study.year,\n        ColNames.month: self.date_of_study.month,\n        ColNames.day: self.date_of_study.day,\n    }\n\n    self.warnings.append(Row(**warning))\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.specific_error_warnings","title":"<code>specific_error_warnings(error_rate_type, field_name, lookback_stats, today_values)</code>","text":"<p>Method that generates the quality warnings, that will be recorded in the output log table, for the metric regarding a specific error type considered in the network syntactic checks.</p> A total of three warnings might be generated <ul> <li>The study date's error rate for this error and this field is greater than the average number of rows over the previous period     by more than a threshold percentage.</li> <li>The study date's error rate for this error and this field is greater than the average number of rows over the previous period     by a config-specified number of standard deviations.</li> <li>The study date's error rate for this error and this field is greater than a config-specified threshold.</li> </ul> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def specific_error_warnings(self, error_rate_type, field_name, lookback_stats, today_values):\n    \"\"\"Method that generates the quality warnings, that will be recorded in the output log table,\n    for the metric regarding a specific error type considered in the network syntactic checks.\n\n    A total of three warnings might be generated:\n        - The study date's error rate for this error and this field is greater than the average number of rows over the previous period\n            by more than a threshold percentage.\n        - The study date's error rate for this error and this field is greater than the average number of rows over the previous period\n            by a config-specified number of standard deviations.\n        - The study date's error rate for this error and this field is greater than a config-specified threshold.\n    \"\"\"\n    if error_rate_type not in self.ERROR_TYPE:\n        raise ValueError(f\"Unknown error type for quality warning `{error_rate_type}`\")\n\n    network_error_type = self.ERROR_TYPE[error_rate_type]\n\n    over_average = self.thresholds[error_rate_type][field_name][\"AVERAGE\"]\n    variability = self.thresholds[error_rate_type][field_name][\"VARIABILITY\"]\n    absolute_upper_control_limit = self.thresholds[error_rate_type][field_name][\"ABS_VALUE_UPPER_LIMIT\"]\n\n    if network_error_type == NetworkErrorType.OUT_OF_RANGE and field_name is None:\n        field_name = \"dates\"\n    current_val = today_values[field_name][network_error_type]\n    previous_avg = lookback_stats[field_name][network_error_type][\"average\"]\n    previous_std = lookback_stats[field_name][network_error_type][\"stddev\"]\n\n    if previous_avg == 0:\n        self.logger.warning(\n            f\"\"\"\n            The average {error_rate_type} for field {field_name} in the input network topology data is 0 for the lookback period\n            especified ({self.lookback_period_start} to {self.lookback_period_end}, both included)\n            \"\"\"\n        )\n    else:\n        pct_difference = 100 * current_val / previous_avg - 100\n        if pct_difference &gt; over_average:\n            self.register_warning(\n                self.MEASURE_DEFINITION[error_rate_type](field_name=field_name),\n                pct_difference,\n                self.CONDITION[error_rate_type][\"AVERAGE\"](field_name=field_name, value=over_average),\n                over_average,\n                self.WARNING_MESSAGE[error_rate_type][\"AVERAGE\"](field_name=field_name),\n            )\n    upper_control_limit = previous_avg + variability * previous_std\n\n    if current_val &gt; upper_control_limit:\n        self.register_warning(\n            self.MEASURE_DEFINITION[error_rate_type](field_name=field_name),\n            current_val,\n            self.CONDITION[error_rate_type][\"VARIABILITY\"](field_name=field_name, variability=variability),\n            variability,\n            self.WARNING_MESSAGE[error_rate_type][\"VARIABILITY\"](field_name=field_name),\n        )\n\n    if current_val &gt; absolute_upper_control_limit:\n        self.register_warning(\n            self.MEASURE_DEFINITION[error_rate_type](field_name=field_name),\n            current_val,\n            self.CONDITION[error_rate_type][\"ABS_VALUE_UPPER_LIMIT\"](\n                field_name=field_name, value=absolute_upper_control_limit\n            ),\n            absolute_upper_control_limit,\n            self.WARNING_MESSAGE[error_rate_type][\"ABS_VALUE_UPPER_LIMIT\"](field_name=field_name),\n        )\n</code></pre>"},{"location":"reference/components/quality/semantic_quality_warnings/","title":"semantic_quality_warnings","text":""},{"location":"reference/components/quality/semantic_quality_warnings/semantic_quality_warnings/","title":"semantic_quality_warnings","text":""},{"location":"reference/components/quality/semantic_quality_warnings/semantic_quality_warnings/#components.quality.semantic_quality_warnings.semantic_quality_warnings.SemanticQualityWarnings","title":"<code>SemanticQualityWarnings</code>","text":"<p>             Bases: <code>Component</code></p> Source code in <code>multimno/components/quality/semantic_quality_warnings/semantic_quality_warnings.py</code> <pre><code>class SemanticQualityWarnings(Component):\n    \"\"\" \"\"\"\n\n    COMPONENT_ID = \"SemanticQualityWarnings\"\n\n    MINIMUM_STD_LOOKBACK_DAYS = 3\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.timestamp = datetime.datetime.now()\n\n        # Read date of study\n        self.date_format = self.config.get(self.COMPONENT_ID, \"date_format\")\n        date_of_study = self.config.get(self.COMPONENT_ID, \"date_of_study\")\n\n        try:\n            self.date_of_study = datetime.datetime.strptime(date_of_study, self.date_format).date()\n        except ValueError as e:\n            self.logger.error(str(e), exc_info=True)\n            raise e\n\n        self.thresholds = self.get_thresholds()\n\n        self.warning_long_format = []\n\n    def get_thresholds(self) -&gt; dict:\n        \"\"\"\n        Method that reads the threshold-related parameters, contained in the config file,\n        and saves them in memory to use them instead of the ones specified by default.\n\n        Raises:\n            ValueError: non-numerical value that cannot be parsed to float (or int) has been used in\n                the config file\n            ValueError: Negative value for a given parameter has been given, when only\n                non-negative values make sense and are thus allowed.\n\n        Returns:\n            thresholds: dict containing the different thresholds used for computing the\n                quality warnings, with the same structure as\n                multimno.core.constants.network_default_thresholds.ASEMANTIC_DEFAULT_THRESHOLDS\n        \"\"\"\n        config_thresholds = self.config.geteval(self.COMPONENT_ID, \"thresholds\")\n        thresholds = SEMANTIC_DEFAULT_THRESHOLDS\n\n        for error_key in config_thresholds.keys():\n            if error_key not in SEMANTIC_DEFAULT_THRESHOLDS:\n                self.logger.info(f\"Parameter key {error_key} unknown -- ignored\")\n                continue\n\n            for param_key, val in config_thresholds[error_key].items():\n                if param_key not in SEMANTIC_DEFAULT_THRESHOLDS[error_key]:\n                    self.logger.info(f\"Unknown parameter {param_key} under {error_key} -- ignored\")\n                    continue\n\n                if param_key == \"sd_lookback_days\":\n                    try:\n                        val = int(val)\n                    except ValueError as e:\n                        error_msg = (\n                            f\"Expected integer value for parameter {param_key} under {error_key} - got {val} instead.\"\n                        )\n                        self.logger.error(error_msg)\n                        raise e\n                else:\n                    try:\n                        val = float(val)\n                    except ValueError as e:\n                        error_msg = (\n                            f\"Expected numeric value for parameter {param_key} under {error_key} - got {val} instead.\"\n                        )\n                        self.logger.error(error_msg)\n                        raise e\n\n                if val &lt; 0:\n                    error_msg = f\"Parameter {param_key} under {error_key} must be non-negative - got {val}\"\n                    self.logger.error(error_msg)\n                    raise ValueError(error_msg)\n\n                thresholds[error_key][param_key] = val\n\n        return thresholds\n\n    def initalize_data_objects(self):\n        input_silver_quality_metrics_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"event_device_semantic_quality_metrics\"\n        )\n        output_silver_log_table_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"event_device_semantic_quality_warnings_log_table\"\n        )\n        output_silver_bar_plot_data_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"event_device_semantic_quality_warnings_bar_plot_data\"\n        )\n\n        silver_quality_metrics = SilverEventSemanticQualityMetrics(\n            self.spark,\n            input_silver_quality_metrics_path,\n            partition_columns=[ColNames.year, ColNames.month, ColNames.day],\n        )\n\n        silver_log_table = SilverEventSemanticQualityWarningsLogTable(\n            self.spark, output_silver_log_table_path, partition_columns=[ColNames.year, ColNames.month, ColNames.day]\n        )\n\n        silver_bar_plot_data = SilverEventSemanticQualityWarningsBarPlotData(\n            self.spark, output_silver_bar_plot_data_path\n        )\n\n        self.input_data_objects = {silver_quality_metrics.ID: silver_quality_metrics}\n        self.output_data_objects = {\n            silver_log_table.ID: silver_log_table,\n            silver_bar_plot_data.ID: silver_bar_plot_data,\n        }\n\n    def transform(self):\n        # Pushup filter, select only dates needed\n        # Since currently each QW has a different lookback period, we filter up to the\n        # furthest day in the past needed\n\n        metrics_df = self.input_data_objects[SilverEventSemanticQualityMetrics.ID].df\n\n        furthest_lookback = max(self.thresholds[key][\"sd_lookback_days\"] for key in self.thresholds.keys())\n\n        metrics_df = metrics_df.withColumn(\n            \"date\", F.make_date(year=F.col(ColNames.year), month=F.col(ColNames.month), day=F.col(ColNames.day))\n        ).filter(\n            F.col(\"date\").between(self.date_of_study - datetime.timedelta(days=furthest_lookback), self.date_of_study)\n        )\n\n        # Get all necessary metrics\n        error_counts = metrics_df.select([\"date\", ColNames.type_of_error, ColNames.value]).collect()\n\n        error_counts = [row.asDict() for row in error_counts]\n\n        error_stats = dict()\n        for count in error_counts:\n            date = count[\"date\"]\n            if date not in error_stats:\n                error_stats[date] = dict()\n\n            error_stats[date][count[ColNames.type_of_error]] = count[ColNames.value]\n\n        # If study date not present in the data, throw an exception\n        if self.date_of_study not in error_stats.keys():\n            raise ValueError(\n                f\"The date of study, {self.date_of_study.strftime(self.date_format)}, has no semantic checks metrics!\"\n            )\n\n        for key in error_stats.keys():\n            error_stats[key] = {\"count\": error_stats[key]}\n            error_stats[key][\"total\"] = sum(error_stats[key][\"count\"].values())\n            error_stats[key][\"percentage\"] = {\n                type_of_error: 100 * val / error_stats[key][\"total\"]\n                for type_of_error, val in error_stats[key][\"count\"].items()\n            }\n\n        for error_name in self.thresholds.keys():\n            self.quality_warnings_by_error(error_name, error_stats)\n\n        self.set_output_log_table()\n\n        self.create_plots_data(error_stats)\n\n    def quality_warnings_by_error(self, error_name: str, error_stats: dict):\n        \"\"\"Method that generates the quality warnings that will be recorded in the output log table,\n        for each type of error.\n\n        In the case that the data needed for a specific error's lookback period is not present, only the current date's\n        error percentage is computed and no warning is raised.\n\n        Args:\n            error_name (str): name of the error, an attribute of multimno.core.constants.error_types.SemanticErrorType\n            error_stats (dict): contains different values concerning each type of error, its counts, percentages, etc.\n        \"\"\"\n        # Get the code of the error given its name\n        error_code = getattr(SemanticErrorType, error_name)\n\n        # lookback days for this error\n        lookback_span = self.thresholds[error_name][\"sd_lookback_days\"]\n        lookback_dates = [self.date_of_study - datetime.timedelta(days=dd) for dd in range(1, lookback_span + 1)]\n\n        if not all(lookback_date in error_stats.keys() for lookback_date in lookback_dates):\n            # cannot compute lookback mean and average, so only showing this date's percentages\n            self.register_warning(\n                date=self.date_of_study,\n                error_code=error_code,\n                value=error_stats[self.date_of_study][\"percentage\"][error_code],\n                upper_control_limit=None,\n                display_warning=False,\n            )\n        else:\n            if lookback_span &lt; self.MINIMUM_STD_LOOKBACK_DAYS:\n                upper_control_limit = self.thresholds[error_name][\"min_percentage\"]\n                self.logger.info(\n                    f\"Lookback days for {error_name} lower than {self.MINIMUM_STD_LOOKBACK_DAYS} - using fixed \"\n                    f\"threshold {upper_control_limit}% instead of using average and standard deviation\"\n                )\n            else:\n                previous_avg = sum(error_stats[dd][\"percentage\"][error_code] for dd in lookback_dates) / lookback_span\n                previous_std = math.sqrt(\n                    sum((error_stats[dd][\"percentage\"][error_code] - previous_avg) ** 2 for dd in lookback_dates)\n                    / (lookback_span - 1)\n                )\n\n                upper_control_limit = previous_avg + self.thresholds[error_name][\"min_sd\"] * previous_std\n\n            # Now compare with todays value\n            if error_stats[self.date_of_study][\"percentage\"][error_code] &gt; upper_control_limit:\n                display_warning = True\n            else:\n                display_warning = False\n\n            self.register_warning(\n                date=self.date_of_study,\n                error_code=error_code,\n                value=error_stats[self.date_of_study][\"percentage\"][error_code],\n                upper_control_limit=upper_control_limit,\n                display_warning=display_warning,\n            )\n\n    def register_warning(\n        self, date: datetime.date, error_code: int, value: float, upper_control_limit: float, display_warning: bool\n    ):\n        \"\"\"Method that abstracts away the creation in the correct format and data type, each of the quality\n        warnings that will be recorded in the log table.\n\n        Args:\n            date (datetime.date): study date, for which the warnings are being calculated\n            error_code (int): code of the error\n            value (float): observed percentage of this specific error for the study date\n            upper_control_limit (float): upper control limit, used as threshold for the warning\n            display_warning (bool): whether the warning should be raised or not. It is currently independent of\n                the arguments values, but in theory it should be equal to (value &gt; control_limit)\n        \"\"\"\n        self.warning_long_format.append((date, error_code, value, upper_control_limit, display_warning))\n\n    def set_output_log_table(self):\n        \"\"\"\n        Method that formats the warnings into the expected table format\n        \"\"\"\n        warning_logs = pd.DataFrame(\n            self.warning_long_format, columns=[ColNames.date, ColNames.type_of_error, \"value\", \"UCL\", \"display\"]\n        ).pivot(index=ColNames.date, columns=[ColNames.type_of_error])\n        column_names = []\n        for name, code in warning_logs.columns:\n            if name == \"value\":\n                column_names.append(f\"Error {code}\")\n            elif name == \"UCL\":\n                column_names.append(f\"Error {code} upper control limit\")\n            elif name == \"display\":\n                column_names.append(f\"Error {code} display warning\")\n        warning_logs.columns = column_names\n        warning_logs = warning_logs.assign(execution_id=self.timestamp)\n        warning_logs = warning_logs.reset_index().assign(\n            **{\n                ColNames.year: lambda df_: df_[\"date\"].apply(lambda x: x.year),\n                ColNames.month: lambda df_: df_[\"date\"].apply(lambda x: x.month),\n                ColNames.day: lambda df_: df_[\"date\"].apply(lambda x: x.day),\n            }\n        )\n\n        # Force expected order of columns\n        warning_logs = warning_logs[SilverEventSemanticQualityWarningsLogTable.SCHEMA.names]\n\n        log_table_df = self.spark.createDataFrame(\n            warning_logs, schema=SilverEventSemanticQualityWarningsLogTable.SCHEMA\n        )\n\n        self.output_data_objects[SilverEventSemanticQualityWarningsLogTable.ID].df = log_table_df\n\n    def create_plots_data(self, error_stats):\n        \"\"\"\n        Method that takes the data needed to generate the required plots, and formats it for\n        easier plotting and saving later on.\n        \"\"\"\n        plot_data = []\n\n        def format_error_code(code):\n            if code == SemanticErrorType.NO_ERROR:\n                return \"No Error\"\n\n            return f\"Error {code}\"\n\n        for date in error_stats:\n            for error_code in error_stats[date][\"count\"]:\n                plot_data.append(\n                    Row(\n                        **{\n                            ColNames.date: date,\n                            ColNames.type_of_error: format_error_code(error_code),\n                            ColNames.value: float(error_stats[date][\"count\"][error_code]),\n                            ColNames.variable: \"Number of occurrences\",\n                            ColNames.year: self.date_of_study.year,\n                            ColNames.month: self.date_of_study.month,\n                            ColNames.day: self.date_of_study.day,\n                            ColNames.timestamp: self.timestamp,\n                        }\n                    )\n                )\n            for error_code in error_stats[date][\"percentage\"]:\n                plot_data.append(\n                    Row(\n                        **{\n                            ColNames.date: date,\n                            ColNames.type_of_error: format_error_code(error_code),\n                            ColNames.value: float(error_stats[date][\"percentage\"][error_code]),\n                            ColNames.variable: \"Percentage\",\n                            ColNames.year: self.date_of_study.year,\n                            ColNames.month: self.date_of_study.month,\n                            ColNames.day: self.date_of_study.day,\n                            ColNames.timestamp: self.timestamp,\n                        }\n                    )\n                )\n        self.output_data_objects[SilverEventSemanticQualityWarningsBarPlotData.ID].df = self.spark.createDataFrame(\n            plot_data, schema=SilverEventSemanticQualityWarningsBarPlotData.SCHEMA\n        )\n\n    def write(self):\n        # Write regular data objects\n        self.logger.info(\"Writing data objects...\")\n        super().write()\n        self.logger.info(\"Data objects written.\")\n</code></pre>"},{"location":"reference/components/quality/semantic_quality_warnings/semantic_quality_warnings/#components.quality.semantic_quality_warnings.semantic_quality_warnings.SemanticQualityWarnings.create_plots_data","title":"<code>create_plots_data(error_stats)</code>","text":"<p>Method that takes the data needed to generate the required plots, and formats it for easier plotting and saving later on.</p> Source code in <code>multimno/components/quality/semantic_quality_warnings/semantic_quality_warnings.py</code> <pre><code>def create_plots_data(self, error_stats):\n    \"\"\"\n    Method that takes the data needed to generate the required plots, and formats it for\n    easier plotting and saving later on.\n    \"\"\"\n    plot_data = []\n\n    def format_error_code(code):\n        if code == SemanticErrorType.NO_ERROR:\n            return \"No Error\"\n\n        return f\"Error {code}\"\n\n    for date in error_stats:\n        for error_code in error_stats[date][\"count\"]:\n            plot_data.append(\n                Row(\n                    **{\n                        ColNames.date: date,\n                        ColNames.type_of_error: format_error_code(error_code),\n                        ColNames.value: float(error_stats[date][\"count\"][error_code]),\n                        ColNames.variable: \"Number of occurrences\",\n                        ColNames.year: self.date_of_study.year,\n                        ColNames.month: self.date_of_study.month,\n                        ColNames.day: self.date_of_study.day,\n                        ColNames.timestamp: self.timestamp,\n                    }\n                )\n            )\n        for error_code in error_stats[date][\"percentage\"]:\n            plot_data.append(\n                Row(\n                    **{\n                        ColNames.date: date,\n                        ColNames.type_of_error: format_error_code(error_code),\n                        ColNames.value: float(error_stats[date][\"percentage\"][error_code]),\n                        ColNames.variable: \"Percentage\",\n                        ColNames.year: self.date_of_study.year,\n                        ColNames.month: self.date_of_study.month,\n                        ColNames.day: self.date_of_study.day,\n                        ColNames.timestamp: self.timestamp,\n                    }\n                )\n            )\n    self.output_data_objects[SilverEventSemanticQualityWarningsBarPlotData.ID].df = self.spark.createDataFrame(\n        plot_data, schema=SilverEventSemanticQualityWarningsBarPlotData.SCHEMA\n    )\n</code></pre>"},{"location":"reference/components/quality/semantic_quality_warnings/semantic_quality_warnings/#components.quality.semantic_quality_warnings.semantic_quality_warnings.SemanticQualityWarnings.get_thresholds","title":"<code>get_thresholds()</code>","text":"<p>Method that reads the threshold-related parameters, contained in the config file, and saves them in memory to use them instead of the ones specified by default.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>non-numerical value that cannot be parsed to float (or int) has been used in the config file</p> <code>ValueError</code> <p>Negative value for a given parameter has been given, when only non-negative values make sense and are thus allowed.</p> <p>Returns:</p> Name Type Description <code>thresholds</code> <code>dict</code> <p>dict containing the different thresholds used for computing the quality warnings, with the same structure as multimno.core.constants.network_default_thresholds.ASEMANTIC_DEFAULT_THRESHOLDS</p> Source code in <code>multimno/components/quality/semantic_quality_warnings/semantic_quality_warnings.py</code> <pre><code>def get_thresholds(self) -&gt; dict:\n    \"\"\"\n    Method that reads the threshold-related parameters, contained in the config file,\n    and saves them in memory to use them instead of the ones specified by default.\n\n    Raises:\n        ValueError: non-numerical value that cannot be parsed to float (or int) has been used in\n            the config file\n        ValueError: Negative value for a given parameter has been given, when only\n            non-negative values make sense and are thus allowed.\n\n    Returns:\n        thresholds: dict containing the different thresholds used for computing the\n            quality warnings, with the same structure as\n            multimno.core.constants.network_default_thresholds.ASEMANTIC_DEFAULT_THRESHOLDS\n    \"\"\"\n    config_thresholds = self.config.geteval(self.COMPONENT_ID, \"thresholds\")\n    thresholds = SEMANTIC_DEFAULT_THRESHOLDS\n\n    for error_key in config_thresholds.keys():\n        if error_key not in SEMANTIC_DEFAULT_THRESHOLDS:\n            self.logger.info(f\"Parameter key {error_key} unknown -- ignored\")\n            continue\n\n        for param_key, val in config_thresholds[error_key].items():\n            if param_key not in SEMANTIC_DEFAULT_THRESHOLDS[error_key]:\n                self.logger.info(f\"Unknown parameter {param_key} under {error_key} -- ignored\")\n                continue\n\n            if param_key == \"sd_lookback_days\":\n                try:\n                    val = int(val)\n                except ValueError as e:\n                    error_msg = (\n                        f\"Expected integer value for parameter {param_key} under {error_key} - got {val} instead.\"\n                    )\n                    self.logger.error(error_msg)\n                    raise e\n            else:\n                try:\n                    val = float(val)\n                except ValueError as e:\n                    error_msg = (\n                        f\"Expected numeric value for parameter {param_key} under {error_key} - got {val} instead.\"\n                    )\n                    self.logger.error(error_msg)\n                    raise e\n\n            if val &lt; 0:\n                error_msg = f\"Parameter {param_key} under {error_key} must be non-negative - got {val}\"\n                self.logger.error(error_msg)\n                raise ValueError(error_msg)\n\n            thresholds[error_key][param_key] = val\n\n    return thresholds\n</code></pre>"},{"location":"reference/components/quality/semantic_quality_warnings/semantic_quality_warnings/#components.quality.semantic_quality_warnings.semantic_quality_warnings.SemanticQualityWarnings.quality_warnings_by_error","title":"<code>quality_warnings_by_error(error_name, error_stats)</code>","text":"<p>Method that generates the quality warnings that will be recorded in the output log table, for each type of error.</p> <p>In the case that the data needed for a specific error's lookback period is not present, only the current date's error percentage is computed and no warning is raised.</p> <p>Parameters:</p> Name Type Description Default <code>error_name</code> <code>str</code> <p>name of the error, an attribute of multimno.core.constants.error_types.SemanticErrorType</p> required <code>error_stats</code> <code>dict</code> <p>contains different values concerning each type of error, its counts, percentages, etc.</p> required Source code in <code>multimno/components/quality/semantic_quality_warnings/semantic_quality_warnings.py</code> <pre><code>def quality_warnings_by_error(self, error_name: str, error_stats: dict):\n    \"\"\"Method that generates the quality warnings that will be recorded in the output log table,\n    for each type of error.\n\n    In the case that the data needed for a specific error's lookback period is not present, only the current date's\n    error percentage is computed and no warning is raised.\n\n    Args:\n        error_name (str): name of the error, an attribute of multimno.core.constants.error_types.SemanticErrorType\n        error_stats (dict): contains different values concerning each type of error, its counts, percentages, etc.\n    \"\"\"\n    # Get the code of the error given its name\n    error_code = getattr(SemanticErrorType, error_name)\n\n    # lookback days for this error\n    lookback_span = self.thresholds[error_name][\"sd_lookback_days\"]\n    lookback_dates = [self.date_of_study - datetime.timedelta(days=dd) for dd in range(1, lookback_span + 1)]\n\n    if not all(lookback_date in error_stats.keys() for lookback_date in lookback_dates):\n        # cannot compute lookback mean and average, so only showing this date's percentages\n        self.register_warning(\n            date=self.date_of_study,\n            error_code=error_code,\n            value=error_stats[self.date_of_study][\"percentage\"][error_code],\n            upper_control_limit=None,\n            display_warning=False,\n        )\n    else:\n        if lookback_span &lt; self.MINIMUM_STD_LOOKBACK_DAYS:\n            upper_control_limit = self.thresholds[error_name][\"min_percentage\"]\n            self.logger.info(\n                f\"Lookback days for {error_name} lower than {self.MINIMUM_STD_LOOKBACK_DAYS} - using fixed \"\n                f\"threshold {upper_control_limit}% instead of using average and standard deviation\"\n            )\n        else:\n            previous_avg = sum(error_stats[dd][\"percentage\"][error_code] for dd in lookback_dates) / lookback_span\n            previous_std = math.sqrt(\n                sum((error_stats[dd][\"percentage\"][error_code] - previous_avg) ** 2 for dd in lookback_dates)\n                / (lookback_span - 1)\n            )\n\n            upper_control_limit = previous_avg + self.thresholds[error_name][\"min_sd\"] * previous_std\n\n        # Now compare with todays value\n        if error_stats[self.date_of_study][\"percentage\"][error_code] &gt; upper_control_limit:\n            display_warning = True\n        else:\n            display_warning = False\n\n        self.register_warning(\n            date=self.date_of_study,\n            error_code=error_code,\n            value=error_stats[self.date_of_study][\"percentage\"][error_code],\n            upper_control_limit=upper_control_limit,\n            display_warning=display_warning,\n        )\n</code></pre>"},{"location":"reference/components/quality/semantic_quality_warnings/semantic_quality_warnings/#components.quality.semantic_quality_warnings.semantic_quality_warnings.SemanticQualityWarnings.register_warning","title":"<code>register_warning(date, error_code, value, upper_control_limit, display_warning)</code>","text":"<p>Method that abstracts away the creation in the correct format and data type, each of the quality warnings that will be recorded in the log table.</p> <p>Parameters:</p> Name Type Description Default <code>date</code> <code>date</code> <p>study date, for which the warnings are being calculated</p> required <code>error_code</code> <code>int</code> <p>code of the error</p> required <code>value</code> <code>float</code> <p>observed percentage of this specific error for the study date</p> required <code>upper_control_limit</code> <code>float</code> <p>upper control limit, used as threshold for the warning</p> required <code>display_warning</code> <code>bool</code> <p>whether the warning should be raised or not. It is currently independent of the arguments values, but in theory it should be equal to (value &gt; control_limit)</p> required Source code in <code>multimno/components/quality/semantic_quality_warnings/semantic_quality_warnings.py</code> <pre><code>def register_warning(\n    self, date: datetime.date, error_code: int, value: float, upper_control_limit: float, display_warning: bool\n):\n    \"\"\"Method that abstracts away the creation in the correct format and data type, each of the quality\n    warnings that will be recorded in the log table.\n\n    Args:\n        date (datetime.date): study date, for which the warnings are being calculated\n        error_code (int): code of the error\n        value (float): observed percentage of this specific error for the study date\n        upper_control_limit (float): upper control limit, used as threshold for the warning\n        display_warning (bool): whether the warning should be raised or not. It is currently independent of\n            the arguments values, but in theory it should be equal to (value &gt; control_limit)\n    \"\"\"\n    self.warning_long_format.append((date, error_code, value, upper_control_limit, display_warning))\n</code></pre>"},{"location":"reference/components/quality/semantic_quality_warnings/semantic_quality_warnings/#components.quality.semantic_quality_warnings.semantic_quality_warnings.SemanticQualityWarnings.set_output_log_table","title":"<code>set_output_log_table()</code>","text":"<p>Method that formats the warnings into the expected table format</p> Source code in <code>multimno/components/quality/semantic_quality_warnings/semantic_quality_warnings.py</code> <pre><code>def set_output_log_table(self):\n    \"\"\"\n    Method that formats the warnings into the expected table format\n    \"\"\"\n    warning_logs = pd.DataFrame(\n        self.warning_long_format, columns=[ColNames.date, ColNames.type_of_error, \"value\", \"UCL\", \"display\"]\n    ).pivot(index=ColNames.date, columns=[ColNames.type_of_error])\n    column_names = []\n    for name, code in warning_logs.columns:\n        if name == \"value\":\n            column_names.append(f\"Error {code}\")\n        elif name == \"UCL\":\n            column_names.append(f\"Error {code} upper control limit\")\n        elif name == \"display\":\n            column_names.append(f\"Error {code} display warning\")\n    warning_logs.columns = column_names\n    warning_logs = warning_logs.assign(execution_id=self.timestamp)\n    warning_logs = warning_logs.reset_index().assign(\n        **{\n            ColNames.year: lambda df_: df_[\"date\"].apply(lambda x: x.year),\n            ColNames.month: lambda df_: df_[\"date\"].apply(lambda x: x.month),\n            ColNames.day: lambda df_: df_[\"date\"].apply(lambda x: x.day),\n        }\n    )\n\n    # Force expected order of columns\n    warning_logs = warning_logs[SilverEventSemanticQualityWarningsLogTable.SCHEMA.names]\n\n    log_table_df = self.spark.createDataFrame(\n        warning_logs, schema=SilverEventSemanticQualityWarningsLogTable.SCHEMA\n    )\n\n    self.output_data_objects[SilverEventSemanticQualityWarningsLogTable.ID].df = log_table_df\n</code></pre>"},{"location":"reference/core/","title":"core","text":""},{"location":"reference/core/component/","title":"component","text":"<p>Module that defines the abstract pipeline component class</p>"},{"location":"reference/core/component/#core.component.Component","title":"<code>Component</code>","text":"<p>Class that models a pipeline component.</p> Source code in <code>multimno/core/component.py</code> <pre><code>class Component(metaclass=ABCMeta):\n    \"\"\"\n    Class that models a pipeline component.\n    \"\"\"\n\n    COMPONENT_ID: str = None\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        self.input_data_objects: Dict[str, DataObject] = None\n        self.output_data_objects: Dict[str, DataObject] = None\n        self.config: ConfigParser = parse_configuration(general_config_path, component_config_path)\n        self.SUB_COMPONENT_ID: str | None = self.config.get(self.COMPONENT_ID, \"SUB_COMPONENT_ID\", fallback=None)\n        self.logger: Logger = generate_logger(self.config)\n        self.spark: SparkSession = generate_spark_session(self.config)\n        self.initalize_data_objects()\n\n    @abstractmethod\n    def initalize_data_objects(self):\n        \"\"\"\n        Method that initializes the data objects associated with the component.\n        \"\"\"\n\n    def read(self):\n        \"\"\"\n        Method that performs the read operation of the input data objects of the component.\n        \"\"\"\n        for data_object in self.input_data_objects.values():\n            data_object.read()\n\n    @abstractmethod\n    def transform(self):\n        \"\"\"\n        Method that performs the data transformations needed to set the dataframes of the output\n         data objects from the input data objects.\n        \"\"\"\n\n    def write(self):\n        \"\"\"\n        Method that performs the write operation of the output data objects.\n        \"\"\"\n        for data_object in self.output_data_objects.values():\n            data_object.write()\n\n    def execute(self):\n        \"\"\"\n        Method that performs the read, transform and write methods of the component.\n        \"\"\"\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n        self.transform()\n        self.write()\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n</code></pre>"},{"location":"reference/core/component/#core.component.Component.execute","title":"<code>execute()</code>","text":"<p>Method that performs the read, transform and write methods of the component.</p> Source code in <code>multimno/core/component.py</code> <pre><code>def execute(self):\n    \"\"\"\n    Method that performs the read, transform and write methods of the component.\n    \"\"\"\n    self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n    self.read()\n    self.transform()\n    self.write()\n    self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n</code></pre>"},{"location":"reference/core/component/#core.component.Component.initalize_data_objects","title":"<code>initalize_data_objects()</code>  <code>abstractmethod</code>","text":"<p>Method that initializes the data objects associated with the component.</p> Source code in <code>multimno/core/component.py</code> <pre><code>@abstractmethod\ndef initalize_data_objects(self):\n    \"\"\"\n    Method that initializes the data objects associated with the component.\n    \"\"\"\n</code></pre>"},{"location":"reference/core/component/#core.component.Component.read","title":"<code>read()</code>","text":"<p>Method that performs the read operation of the input data objects of the component.</p> Source code in <code>multimno/core/component.py</code> <pre><code>def read(self):\n    \"\"\"\n    Method that performs the read operation of the input data objects of the component.\n    \"\"\"\n    for data_object in self.input_data_objects.values():\n        data_object.read()\n</code></pre>"},{"location":"reference/core/component/#core.component.Component.transform","title":"<code>transform()</code>  <code>abstractmethod</code>","text":"<p>Method that performs the data transformations needed to set the dataframes of the output  data objects from the input data objects.</p> Source code in <code>multimno/core/component.py</code> <pre><code>@abstractmethod\ndef transform(self):\n    \"\"\"\n    Method that performs the data transformations needed to set the dataframes of the output\n     data objects from the input data objects.\n    \"\"\"\n</code></pre>"},{"location":"reference/core/component/#core.component.Component.write","title":"<code>write()</code>","text":"<p>Method that performs the write operation of the output data objects.</p> Source code in <code>multimno/core/component.py</code> <pre><code>def write(self):\n    \"\"\"\n    Method that performs the write operation of the output data objects.\n    \"\"\"\n    for data_object in self.output_data_objects.values():\n        data_object.write()\n</code></pre>"},{"location":"reference/core/configuration/","title":"configuration","text":"<p>Module that manages the application configuration.</p>"},{"location":"reference/core/configuration/#core.configuration.parse_configuration","title":"<code>parse_configuration(general_config_path, component_config_path='')</code>","text":"<p>Function that parses a list of configurations in a single ConfigParser object. It expects the first element of the list to be the path to general configuration path. It will override values of the general configuration file with component configuration data.</p> <p>Parameters:</p> Name Type Description Default <code>general_config_path</code> <code>list</code> <p>Path to the general configuration file.</p> required <code>component_config_path</code> <code>str</code> <p>Path to the component configuration file.</p> <code>''</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the general configuration path is doesn't exist</p> <p>Returns:</p> Name Type Description <code>config</code> <code>ConfigParser</code> <p>ConfigParser object with the configuration data.</p> Source code in <code>multimno/core/configuration.py</code> <pre><code>def parse_configuration(general_config_path: str, component_config_path: str = \"\") -&gt; ConfigParser:\n    \"\"\"Function that parses a list of configurations in a single ConfigParser object. It expects\n    the first element of the list to be the path to general configuration path. It will override\n    values of the general configuration file with component configuration data.\n\n    Args:\n        general_config_path (list): Path to the general configuration file.\n        component_config_path (str): Path to the component configuration file.\n\n    Raises:\n        FileNotFoundError: If the general configuration path is doesn't exist\n\n    Returns:\n        config: ConfigParser object with the configuration data.\n    \"\"\"\n\n    # Check general configuration file\n    if not os.path.exists(general_config_path):\n        raise FileNotFoundError(f\"General Config file Not found: {general_config_path}\")\n\n    config_paths = [general_config_path, component_config_path]\n\n    converters = {\n        \"list\": lambda val: [i.strip() for i in val.strip().split(\"\\n\")],\n        \"eval\": eval,\n    }\n\n    parser: ConfigParser = ConfigParser(\n        converters=converters, interpolation=ExtendedInterpolation(), inline_comment_prefixes=\"#\"\n    )\n    parser.optionxform = str\n    parser.read(config_paths)\n\n    return parser\n</code></pre>"},{"location":"reference/core/grid/","title":"grid","text":"<p>This module provides functionality for generating a grid based on the INSPIRE grid system specification.</p>"},{"location":"reference/core/grid/#core.grid.GridGenerator","title":"<code>GridGenerator</code>","text":"<p>Abstract class that provides functionality for generating a grid.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>class GridGenerator(metaclass=ABCMeta):\n    \"\"\"\n    Abstract class that provides functionality for generating a grid.\n    \"\"\"\n\n    def __init__(self, spark: SparkSession) -&gt; None:\n        self.spark: SparkSession = spark\n\n    @abstractmethod\n    def cover_extent_with_grid_ids(self, extent: tuple) -&gt; DataFrame:\n        \"\"\"Cover given extent with grid_ids on given resolution.\"\"\"\n\n    @abstractmethod\n    def cover_polygon_with_grid_ids(self, polygon_sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"Cover given polygon with grid_ids on given resolution.\"\"\"\n\n    @abstractmethod\n    def grid_ids_to_centroids(self, sdf: DataFrame, to_crs: int) -&gt; DataFrame:\n        \"\"\"Get geometry centroids from grid_ids with given coordinate system.\"\"\"\n\n    @abstractmethod\n    def grid_ids_to_tiles(self, sdf: DataFrame, to_crs: int) -&gt; DataFrame:\n        \"\"\"Get grid polygons from grid_ids with given coordinate system.\"\"\"\n\n    @abstractmethod\n    def get_parent_grid_id(self, sdf: DataFrame, resolution: int) -&gt; DataFrame:\n        \"\"\"Get parent grid_id on given resolution.\"\"\"\n\n    @abstractmethod\n    def get_children_grid_ids(self, sdf: DataFrame, resolution: int) -&gt; DataFrame:\n        \"\"\"Get children grid_ids on given resolution.\"\"\"\n</code></pre>"},{"location":"reference/core/grid/#core.grid.GridGenerator.cover_extent_with_grid_ids","title":"<code>cover_extent_with_grid_ids(extent)</code>  <code>abstractmethod</code>","text":"<p>Cover given extent with grid_ids on given resolution.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>@abstractmethod\ndef cover_extent_with_grid_ids(self, extent: tuple) -&gt; DataFrame:\n    \"\"\"Cover given extent with grid_ids on given resolution.\"\"\"\n</code></pre>"},{"location":"reference/core/grid/#core.grid.GridGenerator.cover_polygon_with_grid_ids","title":"<code>cover_polygon_with_grid_ids(polygon_sdf)</code>  <code>abstractmethod</code>","text":"<p>Cover given polygon with grid_ids on given resolution.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>@abstractmethod\ndef cover_polygon_with_grid_ids(self, polygon_sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"Cover given polygon with grid_ids on given resolution.\"\"\"\n</code></pre>"},{"location":"reference/core/grid/#core.grid.GridGenerator.get_children_grid_ids","title":"<code>get_children_grid_ids(sdf, resolution)</code>  <code>abstractmethod</code>","text":"<p>Get children grid_ids on given resolution.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>@abstractmethod\ndef get_children_grid_ids(self, sdf: DataFrame, resolution: int) -&gt; DataFrame:\n    \"\"\"Get children grid_ids on given resolution.\"\"\"\n</code></pre>"},{"location":"reference/core/grid/#core.grid.GridGenerator.get_parent_grid_id","title":"<code>get_parent_grid_id(sdf, resolution)</code>  <code>abstractmethod</code>","text":"<p>Get parent grid_id on given resolution.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>@abstractmethod\ndef get_parent_grid_id(self, sdf: DataFrame, resolution: int) -&gt; DataFrame:\n    \"\"\"Get parent grid_id on given resolution.\"\"\"\n</code></pre>"},{"location":"reference/core/grid/#core.grid.GridGenerator.grid_ids_to_centroids","title":"<code>grid_ids_to_centroids(sdf, to_crs)</code>  <code>abstractmethod</code>","text":"<p>Get geometry centroids from grid_ids with given coordinate system.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>@abstractmethod\ndef grid_ids_to_centroids(self, sdf: DataFrame, to_crs: int) -&gt; DataFrame:\n    \"\"\"Get geometry centroids from grid_ids with given coordinate system.\"\"\"\n</code></pre>"},{"location":"reference/core/grid/#core.grid.GridGenerator.grid_ids_to_tiles","title":"<code>grid_ids_to_tiles(sdf, to_crs)</code>  <code>abstractmethod</code>","text":"<p>Get grid polygons from grid_ids with given coordinate system.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>@abstractmethod\ndef grid_ids_to_tiles(self, sdf: DataFrame, to_crs: int) -&gt; DataFrame:\n    \"\"\"Get grid polygons from grid_ids with given coordinate system.\"\"\"\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator","title":"<code>InspireGridGenerator</code>","text":"<p>             Bases: <code>GridGenerator</code></p> <p>A class used to generate a grid based on the INSPIRE grid system specification.</p> <p>Attributes:</p> Name Type Description <code>GRID_CRS_EPSG_CODE</code> <code>int</code> <p>The EPSG code for the grid's CRS.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>class InspireGridGenerator(GridGenerator):\n    \"\"\"A class used to generate a grid based on the INSPIRE grid system specification.\n\n    Attributes:\n        GRID_CRS_EPSG_CODE (int): The EPSG code for the grid's CRS.\n    \"\"\"\n\n    GRID_CRS_EPSG_CODE = 3035\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        resolution=100,\n        geometry_col_name: str = \"geometry\",\n        grid_id_col_name: str = \"grid_id\",\n        grid_partition_size: int = 2000,\n    ) -&gt; None:\n        \"\"\"Initializes the InspireGridGenerator with the given parameters.\n\n        Args:\n            spark (SparkSession): The SparkSession to use.\n            resolution (int, optional): The resolution of the grid. Defaults to 100. Has to be divisible by 100.\n            geometry_col_name (str, optional): The name of the geometry column. Defaults to 'geometry'.\n            grid_id_col_name (str, optional): The name of the grid ID column. Defaults to 'grid_id'.\n            grid_partition_size (int, optional): The size of the grid partitions, defined as number of tiles\n                in x and y dimensions of subdivisions of the intital grid. Defaults to 2000.\n\n        Raises:\n            ValueError: If the resolution is not divisible by 100.\n        \"\"\"\n        if resolution % 100 != 0:\n            raise ValueError(\"Resolution must be divisible by 100\")\n\n        super().__init__(spark)\n        self.geometry_col_name = geometry_col_name\n        self.grid_id_col_name = grid_id_col_name\n        self.resolution = resolution\n        self.grid_partition_size = grid_partition_size\n        self.resolution_str = self._format_distance(resolution)\n\n    @staticmethod\n    def _format_distance(value: int) -&gt; str:\n        \"\"\"Formats the given distance value to string.\n\n        Args:\n            value (int): The distance value to format.\n\n        Returns:\n            str: The formatted distance value.\n        \"\"\"\n        if value &lt;= 1000:\n            return f\"{value}m\"\n        else:\n            return f\"{value/1000}km\"\n\n    def _project_latlon_extent(self, extent: List[float]) -&gt; Union[List[float], List[float]]:\n        \"\"\"Projects the given extent from lat/lon to the grid's CRS.\n\n        Args:\n            extent (List[float]): The extent to project. Order: [lon_min, lat_min, lon_max, lat_max]\n\n        Returns:\n            List[float]: The projected extent.\n        \"\"\"\n        transformer = Transformer.from_crs(\"EPSG:4326\", f\"EPSG:{self.GRID_CRS_EPSG_CODE}\")\n        # EPSG4326: xx -&gt; lat, yy -&gt; lon\n        # EPSG3035: xx -&gt; northing, yy -&gt; easting\n        xx_bottomleft, yy_bottomleft = transformer.transform(extent[1], extent[0])  # bottom-left corner\n        xx_topright, yy_topright = transformer.transform(extent[3], extent[2])  # top-right corner\n        xx_bottomright, yy_bottomright = transformer.transform(extent[1], extent[2])  # bottom-right corner\n        xx_topleft, yy_topleft = transformer.transform(extent[3], extent[0])\n\n        return (\n            [xx_bottomleft, yy_bottomleft, xx_topright, yy_topright],\n            [xx_bottomright, yy_bottomright, xx_topleft, yy_topleft],\n        )\n\n    @staticmethod\n    def _project_bounding_box(extent: List[float], auxiliar_coords: List[float]) -&gt; (List[float], List[float]):\n        \"\"\"Returns the bottom-left and top-right coordinates of the rectangular bounding box in the projected CRS\n        that covers the bounding box defined from the bottom-left and top-right corners in lat/lon.\n\n        Args:\n            extent (List[float]): Coordinates in the projected CRS that are the transformation of the minimum and\n                maximum latitude and longitude, in [x_bottomleft, y_bottomleft, x_topright, y_topright] order.\n            auxiliar_coords (List[float]): Auxiliar coordinates in the prohected CRS that are the transformation\n                of the other two cornes of the lat/lon rectangular bounding box, in\n                [x_bottomright, y_bottomright, x_topleft, y_topleft] order\n\n        Returns:\n            List[float]: The projected extent, in [x_bottomleft, y_bottomleft, x_topright, y_topright] order.\n            List[float]: Raster cover bounds, in [x_topleft, y_topleft, x_bottomright, y_bottomright] order.\n        \"\"\"\n        cover_x_bottomleft = min(extent[0], auxiliar_coords[0])  # min lat\n        cover_y_bottomleft = min(extent[1], auxiliar_coords[3])  # min lon\n\n        cover_x_topright = max(extent[2], auxiliar_coords[2])  # max lat\n        cover_y_topright = max(extent[3], auxiliar_coords[1])  # max lon\n\n        cover_x_topleft = max(extent[2], auxiliar_coords[2])  # max lat\n        cover_y_topleft = min(extent[1], auxiliar_coords[3])  # min lon\n\n        cover_x_bottomright = min(extent[0], auxiliar_coords[0])  # min lat\n        cover_y_bottomright = max(extent[3], auxiliar_coords[1])  # max lon\n\n        return (\n            [cover_y_bottomleft, cover_x_bottomleft, cover_y_topright, cover_x_topright],\n            [cover_x_topleft, cover_y_topleft, cover_x_bottomright, cover_y_bottomright],\n        )\n\n    def _snap_extent_to_grid(self, extent: List[float]) -&gt; List[float]:\n        \"\"\"Snaps the given extent to the grid.\n\n        Args:\n            extent (List[float]): The extent to snap.\n\n        Returns:\n            List[float]: The snapped extent.\n        \"\"\"\n        return [round(coord / self.resolution) * self.resolution for coord in extent]\n\n    def _extend_grid_extent(self, extent: List[float], extension_factor: int = 5) -&gt; List[float]:\n        \"\"\"Extends the given extent by the specified factor in all directions.\n\n        Args:\n            extent (List[float]): The extent to extend.\n            extension_factor (int, optional): The factor by which to extend the extent. Defaults to 5.\n\n        Returns:\n            List[float]: The extended extent.\n        \"\"\"\n        extension_size = self.resolution * extension_factor\n        return [\n            extent[0] - extension_size,\n            extent[1] - extension_size,\n            extent[2] + extension_size,\n            extent[3] + extension_size,\n        ]\n\n    def _extend_grid_raster_bounds(self, raster_bounds: List[float], extension_factor: int = 5) -&gt; List[float]:\n        \"\"\"Extends the given extent by the specified factor in all directions.\n\n        Args:.\n            extent (List[float]): Raster cover bounds, in [x_topleft, y_topleft, x_bottomright, y_bottomright] order.\n\n            extension_factor (int, optional): The factor by which to extend the extent. Defaults to 5.\n\n        Returns:\n            List[float]: The extended extent.\n        \"\"\"\n        extension_size = self.resolution * extension_factor\n        return [\n            raster_bounds[0] + extension_size,  # x topleft\n            raster_bounds[1] - extension_size,  # y topleft\n            raster_bounds[2] - extension_size,  # x bottomright\n            raster_bounds[3] + extension_size,  # y bottomright\n        ]\n\n    def _get_grid_height(self, raster_bounds: List[float]) -&gt; int:\n        \"\"\"Calculates the height of the grid for the given extent.\n\n        Args:\n            raster_bounds (List[float]): The raster_bounds for which to calculate the grid height.\n\n        Returns:\n            int: The grid height.\n        \"\"\"\n        return int((raster_bounds[0] - raster_bounds[2]) / self.resolution)\n\n    def _get_grid_width(self, raster_bounds: List[float]) -&gt; int:\n        \"\"\"Calculates the width of the grid for the given extent.\n\n        Args:\n            raster_bounds (List[float]): The raster_bounds for which to calculate the grid width.\n\n        Returns:\n            int: The grid width.\n        \"\"\"\n        return int((raster_bounds[3] - raster_bounds[1]) / self.resolution)\n\n    def _get_grid_blueprint(self, extent: List[float]) -&gt; DataFrame:\n        \"\"\"Generates a blueprint for the grid for the given extent as a raster of grid resolution.\n        Splits initial raster into smaller rasters of size grid_partition_size x grid_partition_size.\n\n        Args:\n            extent (List[float]): The extent for which to generate the grid blueprint.\n\n        Returns:\n            DataFrame: The grid blueprint.\n        \"\"\"\n        extent, auxiliar_coords = self._project_latlon_extent(extent)\n        extent, raster_bounds = self._project_bounding_box(extent, auxiliar_coords)\n\n        extent = self._snap_extent_to_grid(extent)\n        raster_bounds = self._snap_extent_to_grid(raster_bounds)\n\n        extent = self._extend_grid_extent(extent)\n        raster_bounds = self._extend_grid_raster_bounds(raster_bounds)\n\n        grid_height = self._get_grid_height(raster_bounds)\n        grid_width = self._get_grid_width(raster_bounds)\n\n        # ONLY FOR EPSG:3035!!!! which has (northing, easting) order, BUT (Y, X) axis names in its EPSG (not in code)\n        # raster_bounds[1]: easting of the top left corner. \"X\" axis\n        # raster_bounds[0]: northing of the top left corner. \"Y\" axis\n\n        sdf = self.spark.sql(\n            f\"\"\"SELECT RS_MakeEmptyRaster(1, \"B\", {grid_width}, \n                                {grid_height}, \n                                {raster_bounds[1]},\n                                {raster_bounds[0]}, \n                                {self.resolution}, \n                               -{self.resolution}, 0.0, 0.0, {self.GRID_CRS_EPSG_CODE}) as raster\"\"\"\n        )\n\n        sdf = sdf.selectExpr(f\"RS_TileExplode(raster,{self.grid_partition_size}, {self.grid_partition_size})\")\n        return sdf.repartition(sdf.count())\n\n    @staticmethod\n    def _get_polygon_sdf_extent(polygon_sdf: DataFrame) -&gt; List[float]:\n        \"\"\"Gets the extent of the given polygon DataFrame.\n\n        Args:\n            polygon_sdf (DataFrame): The polygon DataFrame.\n\n        Returns:\n            List[float]: The extent of the polygon DataFrame.\n        \"\"\"\n        polygon_sdf = polygon_sdf.withColumn(\"bbox\", STF.ST_Envelope(polygon_sdf[\"geometry\"]))\n        polygon_sdf = (\n            polygon_sdf.withColumn(\"x_min\", STF.ST_XMin(polygon_sdf[\"bbox\"]))\n            .withColumn(\"y_min\", STF.ST_YMin(polygon_sdf[\"bbox\"]))\n            .withColumn(\"x_max\", STF.ST_XMax(polygon_sdf[\"bbox\"]))\n            .withColumn(\"y_max\", STF.ST_YMax(polygon_sdf[\"bbox\"]))\n        )\n\n        return polygon_sdf.select(\"x_min\", \"y_min\", \"x_max\", \"y_max\").collect()[0][0:]\n\n    def _get_grid_intersection_with_mask(self, sdf: DataFrame, polygon_sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"Gets the intersection of the grid with the given mask.\n\n        Args:\n            sdf (DataFrame): The DataFrame representing the grid.\n            polygon_sdf (DataFrame): The DataFrame representing the mask.\n\n        Returns:\n            DataFrame: The DataFrame representing the intersection of the grid with the mask.\n        \"\"\"\n        polygon_sdf = polygon_sdf.withColumn(\n            \"geometry\",\n            STF.ST_Transform(polygon_sdf[\"geometry\"], F.lit(\"EPSG:4326\"), F.lit(f\"EPSG:{self.GRID_CRS_EPSG_CODE}\")),\n        )\n\n        sdf = sdf.join(\n            polygon_sdf, STP.ST_Intersects(sdf[self.geometry_col_name], polygon_sdf[\"geometry\"]), \"inner\"\n        ).drop(polygon_sdf[\"geometry\"])\n\n        return sdf\n\n    def _get_grid_intersection_with_mask(self, sdf: DataFrame, polygon_sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"Covers the given extent with grid centroids.\n\n        Args:\n            extent (List[float]): The extent to cover.\n\n        Returns:\n            DataFrame: The DataFrame representing the grid centroids covering the extent.\n        \"\"\"\n        polygon_sdf = polygon_sdf.withColumn(\n            \"geometry\",\n            STF.ST_Transform(polygon_sdf[\"geometry\"], F.lit(\"EPSG:4326\"), F.lit(f\"EPSG:{self.GRID_CRS_EPSG_CODE}\")),\n        )\n\n        sdf = sdf.join(\n            polygon_sdf, STP.ST_Intersects(sdf[self.geometry_col_name], polygon_sdf[\"geometry\"]), \"inner\"\n        ).drop(polygon_sdf[\"geometry\"])\n\n        return sdf\n\n    def cover_extent_with_grid_centroids(self, extent: List[float]) -&gt; DataFrame:\n        \"\"\"Covers the given polygon with grid centroids.\n\n        Args:\n            extent (List[float]): The extent to cover.\n\n        Returns:\n            DataFrame: The DataFrame representing the grid centroids covering the polygon.\n        \"\"\"\n        sdf = self._get_grid_blueprint(extent)\n\n        sdf = sdf.selectExpr(\"explode(RS_PixelAsCentroids(tile, 1)) as exploded\").selectExpr(\n            f\"exploded.geom as {self.geometry_col_name}\"\n        )\n\n        sdf = sdf.withColumn(\n            self.grid_id_col_name,\n            F.concat(\n                F.lit(self.resolution_str),\n                F.lit(\"N\"),\n                (STF.ST_Y(sdf[\"geometry\"]) - self.resolution / 2).cast(IntegerType()),\n                F.lit(\"E\"),\n                (STF.ST_X(sdf[\"geometry\"]) - self.resolution / 2).cast(IntegerType()),\n            ),\n        )\n\n        return sdf\n\n    def cover_polygon_with_grid_centroids(self, polygon_sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"Covers the given extent with grid IDs.\n\n        Args:\n            polygon_sdf (DataFrame): The DataFrame representing the polygon.\n\n        Returns:\n            DataFrame: The DataFrame representing the grid IDs covering the extent.\n        \"\"\"\n        extent = self._get_polygon_sdf_extent(polygon_sdf)\n\n        sdf = self.cover_extent_with_grid_centroids(extent)\n\n        sdf = self._get_grid_intersection_with_mask(sdf, polygon_sdf)\n\n        return sdf\n\n    def cover_extent_with_grid_ids(self, extent: List[float]) -&gt; DataFrame:\n        \"\"\"Covers the given polygon with grid IDs.\n\n        Args:\n            extent (List[float]): The extent to cover.\n\n        Returns:\n            DataFrame: The DataFrame representing the grid IDs covering the polygon.\n        \"\"\"\n        sdf = self.cover_extent_with_grid_centroids(extent)\n\n        sdf = sdf.drop(self.geometry_col_name)\n\n        return sdf\n\n    def cover_polygon_with_grid_ids(self, polygon_sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"Covers the given polygon with grid IDs.\n\n        Args:\n            polygon_sdf (DataFrame): The DataFrame representing the polygon.\n\n        Returns:\n            DataFrame: The DataFrame representing the grid IDs covering the polygon.\n        \"\"\"\n        extent = self._get_polygon_sdf_extent(polygon_sdf)\n\n        sdf = self.cover_extent_with_grid_centroids(extent)\n\n        sdf = self._get_grid_intersection_with_mask(sdf, polygon_sdf)\n\n        sdf = sdf.drop(\"geometry\")\n\n        return sdf\n\n    def cover_extent_with_grid_tiles(self, extent: List[float]) -&gt; DataFrame:\n        \"\"\"Covers the given extent with grid tiles.\n\n        Args:\n            extent (List[float]): The extent to cover.\n\n        Returns:\n            DataFrame: The DataFrame representing the grid tiles covering the extent.\n        \"\"\"\n        sdf = self._get_grid_blueprint(extent)\n\n        sdf = sdf.selectExpr(\"explode(RS_PixelAsPolygons(tile, 1)) as exploded\").selectExpr(\n            f\"exploded.geom as {self.geometry_col_name}\"\n        )\n\n        sdf = sdf.withColumn(\n            self.grid_id_col_name,\n            F.concat(\n                F.lit(self.resolution_str),\n                F.lit(\"N\"),\n                STF.ST_XMin(sdf[\"geometry\"]).cast(IntegerType()),\n                F.lit(\"E\"),\n                STF.ST_YMin(sdf[\"geometry\"]).cast(IntegerType()),\n            ),\n        )\n\n        return sdf\n\n    def cover_polygon_with_grid_tiles(self, polygon_sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"Covers the given polygon with grid tiles.\n\n        Args:\n            polygon_sdf (DataFrame): The DataFrame representing the polygon.\n\n        Returns:\n            DataFrame: The DataFrame representing the grid tiles covering the polygon.\n        \"\"\"\n        extent = self._get_polygon_sdf_extent(polygon_sdf)\n\n        sdf = self.cover_extent_with_grid_tiles(extent)\n\n        sdf = self._get_grid_intersection_with_mask(sdf, polygon_sdf)\n\n        return sdf\n\n    def grid_ids_to_centroids(self, sdf: DataFrame, to_crs: int = None) -&gt; DataFrame:\n        \"\"\"Converts grid IDs to centroids.\n\n        Args:\n            sdf (DataFrame): The DataFrame containing the grid IDs.\n            to_crs (int, optional): The CRS to which to convert the centroids. If not provided,\n                the centroids will be in default grid crs.\n\n        Returns:\n            DataFrame: The DataFrame containing the centroids.\n        \"\"\"\n        sdf = sdf.withColumn(\n            self.geometry_col_name,\n            STC.ST_Point(\n                F.regexp_extract(sdf[self.grid_id_col_name], r\"E(\\d+)\", 1) + self.resolution / 2,\n                F.regexp_extract(sdf[self.grid_id_col_name], r\"N(\\d+)\", 1) + self.resolution / 2,\n            ),\n        )\n\n        sdf = sdf.withColumn(\n            self.geometry_col_name, STF.ST_SetSRID(sdf[self.geometry_col_name], self.GRID_CRS_EPSG_CODE)\n        )\n\n        if to_crs:\n            sdf = sdf.withColumn(\n                self.geometry_col_name, STF.ST_Transform(sdf[self.geometry_col_name], F.lit(f\"EPSG:{to_crs}\"))\n            )\n\n        return sdf\n\n    def grid_ids_to_tiles(self, sdf: DataFrame, to_crs: int = None) -&gt; DataFrame:\n        \"\"\"Converts grid IDs to tiles.\n\n        Args:\n            sdf (DataFrame): The DataFrame containing the grid IDs.\n            to_crs (int, optional): The CRS to which to convert the tiles. If not provided, the centroids\n                will be in default grid crs.\n        Returns:\n            DataFrame: The DataFrame containing the tiles.\n        \"\"\"\n        sdf = sdf.withColumn(\n            self.geometry_col_name,\n            STC.ST_PolygonFromEnvelope(\n                F.regexp_extract(sdf[self.grid_id_col_name], r\"E(\\d+)\", 1),\n                F.regexp_extract(sdf[self.grid_id_col_name], r\"N(\\d+)\", 1),\n                F.regexp_extract(sdf[self.grid_id_col_name], r\"E(\\d+)\", 1) + self.resolution,\n                F.regexp_extract(sdf[self.grid_id_col_name], r\"N(\\d+)\", 1) + self.resolution,\n            ),\n        )\n\n        sdf = sdf.withColumn(\n            self.geometry_col_name, STF.ST_SetSRID(sdf[self.geometry_col_name], self.GRID_CRS_EPSG_CODE)\n        )\n\n        if to_crs:\n            sdf = sdf.withColumn(\n                self.geometry_col_name, STF.ST_Transform(sdf[self.geometry_col_name], F.lit(f\"EPSG:{to_crs}\"))\n            )\n\n        return sdf\n\n    def get_children_grid_ids(self, grid_id, resolution):\n        # TODO: Implement this method\n        pass\n\n    def get_parent_grid_id(self, grid_id, resolution):\n        # TODO: Implement this method\n        pass\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.__init__","title":"<code>__init__(spark, resolution=100, geometry_col_name='geometry', grid_id_col_name='grid_id', grid_partition_size=2000)</code>","text":"<p>Initializes the InspireGridGenerator with the given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>The SparkSession to use.</p> required <code>resolution</code> <code>int</code> <p>The resolution of the grid. Defaults to 100. Has to be divisible by 100.</p> <code>100</code> <code>geometry_col_name</code> <code>str</code> <p>The name of the geometry column. Defaults to 'geometry'.</p> <code>'geometry'</code> <code>grid_id_col_name</code> <code>str</code> <p>The name of the grid ID column. Defaults to 'grid_id'.</p> <code>'grid_id'</code> <code>grid_partition_size</code> <code>int</code> <p>The size of the grid partitions, defined as number of tiles in x and y dimensions of subdivisions of the intital grid. Defaults to 2000.</p> <code>2000</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the resolution is not divisible by 100.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def __init__(\n    self,\n    spark: SparkSession,\n    resolution=100,\n    geometry_col_name: str = \"geometry\",\n    grid_id_col_name: str = \"grid_id\",\n    grid_partition_size: int = 2000,\n) -&gt; None:\n    \"\"\"Initializes the InspireGridGenerator with the given parameters.\n\n    Args:\n        spark (SparkSession): The SparkSession to use.\n        resolution (int, optional): The resolution of the grid. Defaults to 100. Has to be divisible by 100.\n        geometry_col_name (str, optional): The name of the geometry column. Defaults to 'geometry'.\n        grid_id_col_name (str, optional): The name of the grid ID column. Defaults to 'grid_id'.\n        grid_partition_size (int, optional): The size of the grid partitions, defined as number of tiles\n            in x and y dimensions of subdivisions of the intital grid. Defaults to 2000.\n\n    Raises:\n        ValueError: If the resolution is not divisible by 100.\n    \"\"\"\n    if resolution % 100 != 0:\n        raise ValueError(\"Resolution must be divisible by 100\")\n\n    super().__init__(spark)\n    self.geometry_col_name = geometry_col_name\n    self.grid_id_col_name = grid_id_col_name\n    self.resolution = resolution\n    self.grid_partition_size = grid_partition_size\n    self.resolution_str = self._format_distance(resolution)\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.cover_extent_with_grid_centroids","title":"<code>cover_extent_with_grid_centroids(extent)</code>","text":"<p>Covers the given polygon with grid centroids.</p> <p>Parameters:</p> Name Type Description Default <code>extent</code> <code>List[float]</code> <p>The extent to cover.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The DataFrame representing the grid centroids covering the polygon.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def cover_extent_with_grid_centroids(self, extent: List[float]) -&gt; DataFrame:\n    \"\"\"Covers the given polygon with grid centroids.\n\n    Args:\n        extent (List[float]): The extent to cover.\n\n    Returns:\n        DataFrame: The DataFrame representing the grid centroids covering the polygon.\n    \"\"\"\n    sdf = self._get_grid_blueprint(extent)\n\n    sdf = sdf.selectExpr(\"explode(RS_PixelAsCentroids(tile, 1)) as exploded\").selectExpr(\n        f\"exploded.geom as {self.geometry_col_name}\"\n    )\n\n    sdf = sdf.withColumn(\n        self.grid_id_col_name,\n        F.concat(\n            F.lit(self.resolution_str),\n            F.lit(\"N\"),\n            (STF.ST_Y(sdf[\"geometry\"]) - self.resolution / 2).cast(IntegerType()),\n            F.lit(\"E\"),\n            (STF.ST_X(sdf[\"geometry\"]) - self.resolution / 2).cast(IntegerType()),\n        ),\n    )\n\n    return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.cover_extent_with_grid_ids","title":"<code>cover_extent_with_grid_ids(extent)</code>","text":"<p>Covers the given polygon with grid IDs.</p> <p>Parameters:</p> Name Type Description Default <code>extent</code> <code>List[float]</code> <p>The extent to cover.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The DataFrame representing the grid IDs covering the polygon.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def cover_extent_with_grid_ids(self, extent: List[float]) -&gt; DataFrame:\n    \"\"\"Covers the given polygon with grid IDs.\n\n    Args:\n        extent (List[float]): The extent to cover.\n\n    Returns:\n        DataFrame: The DataFrame representing the grid IDs covering the polygon.\n    \"\"\"\n    sdf = self.cover_extent_with_grid_centroids(extent)\n\n    sdf = sdf.drop(self.geometry_col_name)\n\n    return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.cover_extent_with_grid_tiles","title":"<code>cover_extent_with_grid_tiles(extent)</code>","text":"<p>Covers the given extent with grid tiles.</p> <p>Parameters:</p> Name Type Description Default <code>extent</code> <code>List[float]</code> <p>The extent to cover.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The DataFrame representing the grid tiles covering the extent.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def cover_extent_with_grid_tiles(self, extent: List[float]) -&gt; DataFrame:\n    \"\"\"Covers the given extent with grid tiles.\n\n    Args:\n        extent (List[float]): The extent to cover.\n\n    Returns:\n        DataFrame: The DataFrame representing the grid tiles covering the extent.\n    \"\"\"\n    sdf = self._get_grid_blueprint(extent)\n\n    sdf = sdf.selectExpr(\"explode(RS_PixelAsPolygons(tile, 1)) as exploded\").selectExpr(\n        f\"exploded.geom as {self.geometry_col_name}\"\n    )\n\n    sdf = sdf.withColumn(\n        self.grid_id_col_name,\n        F.concat(\n            F.lit(self.resolution_str),\n            F.lit(\"N\"),\n            STF.ST_XMin(sdf[\"geometry\"]).cast(IntegerType()),\n            F.lit(\"E\"),\n            STF.ST_YMin(sdf[\"geometry\"]).cast(IntegerType()),\n        ),\n    )\n\n    return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.cover_polygon_with_grid_centroids","title":"<code>cover_polygon_with_grid_centroids(polygon_sdf)</code>","text":"<p>Covers the given extent with grid IDs.</p> <p>Parameters:</p> Name Type Description Default <code>polygon_sdf</code> <code>DataFrame</code> <p>The DataFrame representing the polygon.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The DataFrame representing the grid IDs covering the extent.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def cover_polygon_with_grid_centroids(self, polygon_sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"Covers the given extent with grid IDs.\n\n    Args:\n        polygon_sdf (DataFrame): The DataFrame representing the polygon.\n\n    Returns:\n        DataFrame: The DataFrame representing the grid IDs covering the extent.\n    \"\"\"\n    extent = self._get_polygon_sdf_extent(polygon_sdf)\n\n    sdf = self.cover_extent_with_grid_centroids(extent)\n\n    sdf = self._get_grid_intersection_with_mask(sdf, polygon_sdf)\n\n    return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.cover_polygon_with_grid_ids","title":"<code>cover_polygon_with_grid_ids(polygon_sdf)</code>","text":"<p>Covers the given polygon with grid IDs.</p> <p>Parameters:</p> Name Type Description Default <code>polygon_sdf</code> <code>DataFrame</code> <p>The DataFrame representing the polygon.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The DataFrame representing the grid IDs covering the polygon.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def cover_polygon_with_grid_ids(self, polygon_sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"Covers the given polygon with grid IDs.\n\n    Args:\n        polygon_sdf (DataFrame): The DataFrame representing the polygon.\n\n    Returns:\n        DataFrame: The DataFrame representing the grid IDs covering the polygon.\n    \"\"\"\n    extent = self._get_polygon_sdf_extent(polygon_sdf)\n\n    sdf = self.cover_extent_with_grid_centroids(extent)\n\n    sdf = self._get_grid_intersection_with_mask(sdf, polygon_sdf)\n\n    sdf = sdf.drop(\"geometry\")\n\n    return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.cover_polygon_with_grid_tiles","title":"<code>cover_polygon_with_grid_tiles(polygon_sdf)</code>","text":"<p>Covers the given polygon with grid tiles.</p> <p>Parameters:</p> Name Type Description Default <code>polygon_sdf</code> <code>DataFrame</code> <p>The DataFrame representing the polygon.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The DataFrame representing the grid tiles covering the polygon.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def cover_polygon_with_grid_tiles(self, polygon_sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"Covers the given polygon with grid tiles.\n\n    Args:\n        polygon_sdf (DataFrame): The DataFrame representing the polygon.\n\n    Returns:\n        DataFrame: The DataFrame representing the grid tiles covering the polygon.\n    \"\"\"\n    extent = self._get_polygon_sdf_extent(polygon_sdf)\n\n    sdf = self.cover_extent_with_grid_tiles(extent)\n\n    sdf = self._get_grid_intersection_with_mask(sdf, polygon_sdf)\n\n    return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.grid_ids_to_centroids","title":"<code>grid_ids_to_centroids(sdf, to_crs=None)</code>","text":"<p>Converts grid IDs to centroids.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The DataFrame containing the grid IDs.</p> required <code>to_crs</code> <code>int</code> <p>The CRS to which to convert the centroids. If not provided, the centroids will be in default grid crs.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The DataFrame containing the centroids.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def grid_ids_to_centroids(self, sdf: DataFrame, to_crs: int = None) -&gt; DataFrame:\n    \"\"\"Converts grid IDs to centroids.\n\n    Args:\n        sdf (DataFrame): The DataFrame containing the grid IDs.\n        to_crs (int, optional): The CRS to which to convert the centroids. If not provided,\n            the centroids will be in default grid crs.\n\n    Returns:\n        DataFrame: The DataFrame containing the centroids.\n    \"\"\"\n    sdf = sdf.withColumn(\n        self.geometry_col_name,\n        STC.ST_Point(\n            F.regexp_extract(sdf[self.grid_id_col_name], r\"E(\\d+)\", 1) + self.resolution / 2,\n            F.regexp_extract(sdf[self.grid_id_col_name], r\"N(\\d+)\", 1) + self.resolution / 2,\n        ),\n    )\n\n    sdf = sdf.withColumn(\n        self.geometry_col_name, STF.ST_SetSRID(sdf[self.geometry_col_name], self.GRID_CRS_EPSG_CODE)\n    )\n\n    if to_crs:\n        sdf = sdf.withColumn(\n            self.geometry_col_name, STF.ST_Transform(sdf[self.geometry_col_name], F.lit(f\"EPSG:{to_crs}\"))\n        )\n\n    return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.grid_ids_to_tiles","title":"<code>grid_ids_to_tiles(sdf, to_crs=None)</code>","text":"<p>Converts grid IDs to tiles.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The DataFrame containing the grid IDs.</p> required <code>to_crs</code> <code>int</code> <p>The CRS to which to convert the tiles. If not provided, the centroids will be in default grid crs.</p> <code>None</code> <p>Returns:     DataFrame: The DataFrame containing the tiles.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def grid_ids_to_tiles(self, sdf: DataFrame, to_crs: int = None) -&gt; DataFrame:\n    \"\"\"Converts grid IDs to tiles.\n\n    Args:\n        sdf (DataFrame): The DataFrame containing the grid IDs.\n        to_crs (int, optional): The CRS to which to convert the tiles. If not provided, the centroids\n            will be in default grid crs.\n    Returns:\n        DataFrame: The DataFrame containing the tiles.\n    \"\"\"\n    sdf = sdf.withColumn(\n        self.geometry_col_name,\n        STC.ST_PolygonFromEnvelope(\n            F.regexp_extract(sdf[self.grid_id_col_name], r\"E(\\d+)\", 1),\n            F.regexp_extract(sdf[self.grid_id_col_name], r\"N(\\d+)\", 1),\n            F.regexp_extract(sdf[self.grid_id_col_name], r\"E(\\d+)\", 1) + self.resolution,\n            F.regexp_extract(sdf[self.grid_id_col_name], r\"N(\\d+)\", 1) + self.resolution,\n        ),\n    )\n\n    sdf = sdf.withColumn(\n        self.geometry_col_name, STF.ST_SetSRID(sdf[self.geometry_col_name], self.GRID_CRS_EPSG_CODE)\n    )\n\n    if to_crs:\n        sdf = sdf.withColumn(\n            self.geometry_col_name, STF.ST_Transform(sdf[self.geometry_col_name], F.lit(f\"EPSG:{to_crs}\"))\n        )\n\n    return sdf\n</code></pre>"},{"location":"reference/core/io_interface/","title":"io_interface","text":"<p>Module that implements classes for reading data from different data sources into a Spark DataFrames.</p>"},{"location":"reference/core/io_interface/#core.io_interface.CsvInterface","title":"<code>CsvInterface</code>","text":"<p>             Bases: <code>PathInterface</code></p> <p>Class that implements the PathInterface abstract class for reading/writing data from a csv data source.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class CsvInterface(PathInterface):\n    \"\"\"Class that implements the PathInterface abstract class for reading/writing data from a csv data source.\"\"\"\n\n    FILE_FORMAT = \"csv\"\n\n    def read_from_interface(\n        self, spark: SparkSession, path: str, schema: StructType, header: bool = True, sep: str = \",\"\n    ) -&gt; DataFrame:\n        \"\"\"Method that reads data from a csv type data source as a Spark DataFrame.\n\n        Args:\n            spark (SparkSession): Spark session.\n            path (str): Path to the data.\n            schema (StructType, optional): Schema of the data. Defaults to None.\n\n        Returns:\n            df: Spark dataframe.\n        \"\"\"\n        return spark.read.csv(path, schema=schema, header=header, sep=sep)\n\n    def write_from_interface(\n        self, df: DataFrame, path: str, partition_columns: list[str] = None, header: bool = True, sep: str = \",\"\n    ):\n        \"\"\"Method that writes data from a Spark DataFrame to a csv data source.\n\n        Args:\n            df (DataFrame): Spark DataFrame.\n            path (str): Path to the data.\n            partition_columns (list[str], optional): columns used for a partition write.\n        Raises:\n            NotImplementedError: csv files should not be written in this architecture.\n        \"\"\"\n        if partition_columns is None:\n            partition_columns = []\n        df.write.option(\"header\", header).option(\"sep\", sep).mode(\"overwrite\").format(\"csv\").save(path)\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.CsvInterface.read_from_interface","title":"<code>read_from_interface(spark, path, schema, header=True, sep=',')</code>","text":"<p>Method that reads data from a csv type data source as a Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session.</p> required <code>path</code> <code>str</code> <p>Path to the data.</p> required <code>schema</code> <code>StructType</code> <p>Schema of the data. Defaults to None.</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>Spark dataframe.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def read_from_interface(\n    self, spark: SparkSession, path: str, schema: StructType, header: bool = True, sep: str = \",\"\n) -&gt; DataFrame:\n    \"\"\"Method that reads data from a csv type data source as a Spark DataFrame.\n\n    Args:\n        spark (SparkSession): Spark session.\n        path (str): Path to the data.\n        schema (StructType, optional): Schema of the data. Defaults to None.\n\n    Returns:\n        df: Spark dataframe.\n    \"\"\"\n    return spark.read.csv(path, schema=schema, header=header, sep=sep)\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.CsvInterface.write_from_interface","title":"<code>write_from_interface(df, path, partition_columns=None, header=True, sep=',')</code>","text":"<p>Method that writes data from a Spark DataFrame to a csv data source.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Spark DataFrame.</p> required <code>path</code> <code>str</code> <p>Path to the data.</p> required <code>partition_columns</code> <code>list[str]</code> <p>columns used for a partition write.</p> <code>None</code> <p>Raises:     NotImplementedError: csv files should not be written in this architecture.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def write_from_interface(\n    self, df: DataFrame, path: str, partition_columns: list[str] = None, header: bool = True, sep: str = \",\"\n):\n    \"\"\"Method that writes data from a Spark DataFrame to a csv data source.\n\n    Args:\n        df (DataFrame): Spark DataFrame.\n        path (str): Path to the data.\n        partition_columns (list[str], optional): columns used for a partition write.\n    Raises:\n        NotImplementedError: csv files should not be written in this architecture.\n    \"\"\"\n    if partition_columns is None:\n        partition_columns = []\n    df.write.option(\"header\", header).option(\"sep\", sep).mode(\"overwrite\").format(\"csv\").save(path)\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.GeoParquetInterface","title":"<code>GeoParquetInterface</code>","text":"<p>             Bases: <code>PathInterface</code></p> <p>Class that implements the PathInterface abstract class for reading/writing data from a geoparquet data source.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class GeoParquetInterface(PathInterface):\n    \"\"\"Class that implements the PathInterface abstract class for reading/writing data from a geoparquet data source.\"\"\"\n\n    FILE_FORMAT = \"geoparquet\"\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.IOInterface","title":"<code>IOInterface</code>","text":"<p>Abstract interface that provides functionality for reading and writing data</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class IOInterface(metaclass=ABCMeta):\n    \"\"\"Abstract interface that provides functionality for reading and writing data\"\"\"\n\n    @classmethod\n    def __subclasshook__(cls, subclass: type) -&gt; bool:\n        if cls is IOInterface:\n            attrs: list[str] = []\n            callables: list[str] = [\"read_from_interface\", \"write_from_interface\"]\n            ret: bool = True\n            for attr in attrs:\n                ret = ret and (hasattr(subclass, attr) and isinstance(getattr(subclass, attr), property))\n            for call in callables:\n                ret = ret and (hasattr(subclass, call) and callable(getattr(subclass, call)))\n            return ret\n        else:\n            return NotImplemented\n\n    @abstractmethod\n    def read_from_interface(self, *args, **kwargs) -&gt; DataFrame:\n        pass\n\n    @abstractmethod\n    def write_from_interface(self, df: DataFrame, *args, **kwargs):\n        pass\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.JsonInterface","title":"<code>JsonInterface</code>","text":"<p>             Bases: <code>PathInterface</code></p> <p>Class that implements the PathInterface abstract class for reading/writing data from a json data source.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class JsonInterface(PathInterface):\n    \"\"\"Class that implements the PathInterface abstract class for reading/writing data from a json data source.\"\"\"\n\n    FILE_FORMAT = \"json\"\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.ParquetInterface","title":"<code>ParquetInterface</code>","text":"<p>             Bases: <code>PathInterface</code></p> <p>Class that implements the PathInterface abstract class for reading/writing data from a parquet data source.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class ParquetInterface(PathInterface):\n    \"\"\"Class that implements the PathInterface abstract class for reading/writing data from a parquet data source.\"\"\"\n\n    FILE_FORMAT = \"parquet\"\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.PathInterface","title":"<code>PathInterface</code>","text":"<p>             Bases: <code>IOInterface</code></p> <p>Abstract interface for reading/writing data from a file type data source.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class PathInterface(IOInterface, metaclass=ABCMeta):\n    \"\"\"Abstract interface for reading/writing data from a file type data source.\"\"\"\n\n    FILE_FORMAT = \"\"\n\n    def read_from_interface(self, spark: SparkSession, path: str, schema: StructType = None) -&gt; DataFrame:\n        \"\"\"Method that reads data from a file type data source as a Spark DataFrame.\n\n        Args:\n            spark (SparkSession): Spark session.\n            path (str): Path to the data.\n            schema (StructType, optional): Schema of the data. Defaults to None.\n\n        Returns:\n            df: Spark dataframe.\n        \"\"\"\n        return spark.read.schema(schema).format(self.FILE_FORMAT).load(path)  # Read schema  # File format  # Load path\n\n    def write_from_interface(self, df: DataFrame, path: str, partition_columns: list[str] = None):\n        \"\"\"Method that writes data from a Spark DataFrame to a file type data source.\n\n        Args:\n            df (DataFrame): Spark DataFrame.\n            path (str): Path to the data.\n            partition_columns (list[str], optional): columns used for a partition write.\n        \"\"\"\n        # Args check\n        if partition_columns is None:\n            partition_columns = []\n\n        df.write.format(\n            self.FILE_FORMAT,  # File format\n        ).partitionBy(partition_columns).mode(\n            \"overwrite\"\n        ).save(path)\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.PathInterface.read_from_interface","title":"<code>read_from_interface(spark, path, schema=None)</code>","text":"<p>Method that reads data from a file type data source as a Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session.</p> required <code>path</code> <code>str</code> <p>Path to the data.</p> required <code>schema</code> <code>StructType</code> <p>Schema of the data. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>Spark dataframe.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def read_from_interface(self, spark: SparkSession, path: str, schema: StructType = None) -&gt; DataFrame:\n    \"\"\"Method that reads data from a file type data source as a Spark DataFrame.\n\n    Args:\n        spark (SparkSession): Spark session.\n        path (str): Path to the data.\n        schema (StructType, optional): Schema of the data. Defaults to None.\n\n    Returns:\n        df: Spark dataframe.\n    \"\"\"\n    return spark.read.schema(schema).format(self.FILE_FORMAT).load(path)  # Read schema  # File format  # Load path\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.PathInterface.write_from_interface","title":"<code>write_from_interface(df, path, partition_columns=None)</code>","text":"<p>Method that writes data from a Spark DataFrame to a file type data source.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Spark DataFrame.</p> required <code>path</code> <code>str</code> <p>Path to the data.</p> required <code>partition_columns</code> <code>list[str]</code> <p>columns used for a partition write.</p> <code>None</code> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def write_from_interface(self, df: DataFrame, path: str, partition_columns: list[str] = None):\n    \"\"\"Method that writes data from a Spark DataFrame to a file type data source.\n\n    Args:\n        df (DataFrame): Spark DataFrame.\n        path (str): Path to the data.\n        partition_columns (list[str], optional): columns used for a partition write.\n    \"\"\"\n    # Args check\n    if partition_columns is None:\n        partition_columns = []\n\n    df.write.format(\n        self.FILE_FORMAT,  # File format\n    ).partitionBy(partition_columns).mode(\n        \"overwrite\"\n    ).save(path)\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.ShapefileInterface","title":"<code>ShapefileInterface</code>","text":"<p>             Bases: <code>PathInterface</code></p> <p>Class that implements the PathInterface abstract class for reading/writing data from a ShapeFile data source.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class ShapefileInterface(PathInterface):\n    \"\"\"Class that implements the PathInterface abstract class for reading/writing data from a ShapeFile data source.\"\"\"\n\n    def read_from_interface(self, spark: SparkSession, path: str, schema: StructType = None) -&gt; DataFrame:\n        \"\"\"Method that reads data from a ShapeFile type data source as a Spark DataFrame.\n\n        Args:\n            spark (SparkSession): Spark session.\n            path (str): Path to the data.\n            schema (StructType, optional): Schema of the data. Defaults to None.\n\n        Returns:\n            df: Spark dataframe.\n        \"\"\"\n        df = ShapefileReader.readToGeometryRDD(spark.sparkContext, path)\n        return Adapter.toDf(df, spark)\n\n    def write_from_interface(self, df: DataFrame, path: str, partition_columns: list = None):\n        \"\"\"Method that writes data from a Spark DataFrame to a ShapeFile data source.\n\n        Args:\n            df (DataFrame): Spark DataFrame.\n            path (str): Path to the data.\n            partition_columns (list[str], optional): columns used for a partition write.\n        Raises:\n            NotImplementedError: ShapeFile files should not be written in this architecture.\n        \"\"\"\n        raise NotImplementedError(\"Not implemented as Shapefiles shouldn't be written\")\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.ShapefileInterface.read_from_interface","title":"<code>read_from_interface(spark, path, schema=None)</code>","text":"<p>Method that reads data from a ShapeFile type data source as a Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session.</p> required <code>path</code> <code>str</code> <p>Path to the data.</p> required <code>schema</code> <code>StructType</code> <p>Schema of the data. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>Spark dataframe.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def read_from_interface(self, spark: SparkSession, path: str, schema: StructType = None) -&gt; DataFrame:\n    \"\"\"Method that reads data from a ShapeFile type data source as a Spark DataFrame.\n\n    Args:\n        spark (SparkSession): Spark session.\n        path (str): Path to the data.\n        schema (StructType, optional): Schema of the data. Defaults to None.\n\n    Returns:\n        df: Spark dataframe.\n    \"\"\"\n    df = ShapefileReader.readToGeometryRDD(spark.sparkContext, path)\n    return Adapter.toDf(df, spark)\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.ShapefileInterface.write_from_interface","title":"<code>write_from_interface(df, path, partition_columns=None)</code>","text":"<p>Method that writes data from a Spark DataFrame to a ShapeFile data source.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Spark DataFrame.</p> required <code>path</code> <code>str</code> <p>Path to the data.</p> required <code>partition_columns</code> <code>list[str]</code> <p>columns used for a partition write.</p> <code>None</code> <p>Raises:     NotImplementedError: ShapeFile files should not be written in this architecture.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def write_from_interface(self, df: DataFrame, path: str, partition_columns: list = None):\n    \"\"\"Method that writes data from a Spark DataFrame to a ShapeFile data source.\n\n    Args:\n        df (DataFrame): Spark DataFrame.\n        path (str): Path to the data.\n        partition_columns (list[str], optional): columns used for a partition write.\n    Raises:\n        NotImplementedError: ShapeFile files should not be written in this architecture.\n    \"\"\"\n    raise NotImplementedError(\"Not implemented as Shapefiles shouldn't be written\")\n</code></pre>"},{"location":"reference/core/log/","title":"log","text":"<p>Module that manages the logging functionality.</p>"},{"location":"reference/core/log/#core.log.generate_logger","title":"<code>generate_logger(config)</code>","text":"<p>Function that initializes a logger.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ConfigParser</code> <p>Object with the final configuration.</p> required <p>Returns:</p> Type Description <code>Logger</code> <p>Python logging object.</p> Source code in <code>multimno/core/log.py</code> <pre><code>def generate_logger(config: ConfigParser):\n    \"\"\"Function that initializes a logger.\n\n    Args:\n        config (ConfigParser): Object with the final configuration.\n\n    Returns:\n        (logging.Logger): Python logging object.\n    \"\"\"\n    # Parse config\n    log_level = config.get(LOG_CONFIG_KEY, \"level\")\n    log_format = config.get(LOG_CONFIG_KEY, \"format\")\n    datefmt = config.get(LOG_CONFIG_KEY, \"datefmt\")\n\n    # Establish python log config templates\n    loggin_config = {\n        \"version\": 1,\n        \"disable_existing_loggers\": False,\n        \"formatters\": {\"verbose\": {\"format\": (log_format), \"datefmt\": datefmt}},\n        \"handlers\": None,\n        \"loggers\": {},\n    }\n\n    console_handler_config = {\n        \"level\": \"\",\n        \"class\": \"logging.StreamHandler\",\n        \"formatter\": \"verbose\",\n    }\n\n    logger_template_config = {\"level\": \"\", \"handlers\": [], \"propagate\": False}\n\n    # Create console handler\n    log_config = copy.deepcopy(loggin_config)\n    console_handler = copy.deepcopy(console_handler_config)\n    console_handler[\"level\"] = log_level\n    console_handler[\"stream\"] = stdout\n\n    # Create a console handler in Warning level\n    warning_level_console_handler = copy.deepcopy(console_handler_config)\n    warning_level_console_handler[\"level\"] = \"WARNING\"\n\n    # Add handlers to log config\n    log_config[\"handlers\"] = {\"console\": console_handler, \"root_console\": warning_level_console_handler}\n\n    # Create application logger and add it to the python log config\n    logger_template = copy.deepcopy(logger_template_config)\n    logger_template[\"level\"] = log_level\n    logger_template[\"handlers\"] = [\"console\"]\n    log_config[\"loggers\"][APP_NAME] = logger_template\n\n    # Create root logger that all logging using dependency will use\n    root_logger = copy.deepcopy(logger_template_config)\n    root_logger[\"level\"] = \"DEBUG\" if log_level == \"DEBUG\" else \"WARNING\"\n    root_logger[\"handlers\"] = [\"root_console\"]\n    log_config[\"loggers\"][\"\"] = root_logger  # Make all loggers inherit config\n\n    # Establish base logging\n    logging.config.dictConfig(log_config)\n\n    # Return application logger\n    return logging.getLogger(APP_NAME)\n</code></pre>"},{"location":"reference/core/settings/","title":"settings","text":"<p>Settings module</p>"},{"location":"reference/core/spark_session/","title":"spark_session","text":"<p>Module that manages the spark session.</p>"},{"location":"reference/core/spark_session/#core.spark_session.check_if_data_path_exists","title":"<code>check_if_data_path_exists(spark, data_path)</code>","text":"<p>Checks whether data path exists, returns True if it does, False if not</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>active SparkSession</p> required <code>data_path</code> <code>str</code> <p>path to check</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether the passed path exists</p> Source code in <code>multimno/core/spark_session.py</code> <pre><code>def check_if_data_path_exists(spark: SparkSession, data_path: str) -&gt; bool:\n    \"\"\"\n    Checks whether data path exists, returns True if it does, False if not\n\n    Args:\n        spark (SparkSession): active SparkSession\n        data_path (str): path to check\n\n    Returns:\n        bool: Whether the passed path exists\n    \"\"\"\n    conf = spark._jsc.hadoopConfiguration()\n    uri = spark._jvm.java.net.URI.create(data_path)\n    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(uri, conf)\n    return fs.exists(spark._jvm.org.apache.hadoop.fs.Path(data_path))\n</code></pre>"},{"location":"reference/core/spark_session/#core.spark_session.check_or_create_data_path","title":"<code>check_or_create_data_path(spark, data_path)</code>","text":"<p>Create the provided path on a file system. If path already exists, do nothing.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>active SparkSession</p> required <code>data_path</code> <code>str</code> <p>path to check</p> required Source code in <code>multimno/core/spark_session.py</code> <pre><code>def check_or_create_data_path(spark: SparkSession, data_path: str):\n    \"\"\"\n    Create the provided path on a file system. If path already exists, do nothing.\n\n    Args:\n        spark (SparkSession): active SparkSession\n        data_path (str): path to check\n    \"\"\"\n    conf = spark._jsc.hadoopConfiguration()\n    uri = spark._jvm.java.net.URI.create(data_path)\n    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(uri, conf)\n    path = spark._jvm.org.apache.hadoop.fs.Path(data_path)\n    if not fs.exists(path):\n        fs.mkdirs(path)\n</code></pre>"},{"location":"reference/core/spark_session/#core.spark_session.delete_file_or_folder","title":"<code>delete_file_or_folder(spark, data_path)</code>","text":"<p>Deletes file or folder with given path</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Currently active spark session</p> required <code>data_path</code> <code>str</code> <p>Path to remove</p> required Source code in <code>multimno/core/spark_session.py</code> <pre><code>def delete_file_or_folder(spark: SparkSession, data_path: str):\n    \"\"\"\n    Deletes file or folder with given path\n\n    Args:\n        spark (SparkSession): Currently active spark session\n        data_path (str): Path to remove\n    \"\"\"\n    conf = spark._jsc.hadoopConfiguration()\n    uri = spark._jvm.java.net.URI.create(data_path)\n    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(uri, conf)\n    path = spark._jvm.org.apache.hadoop.fs.Path(data_path)\n    fs.delete(path, True)\n</code></pre>"},{"location":"reference/core/spark_session/#core.spark_session.generate_spark_session","title":"<code>generate_spark_session(config)</code>","text":"<p>Function that generates a Spark Sedona session.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ConfigParser</code> <p>Object with the final configuration.</p> required <p>Returns:</p> Name Type Description <code>SparkSession</code> <code>SparkSession</code> <p>Session of spark.</p> Source code in <code>multimno/core/spark_session.py</code> <pre><code>def generate_spark_session(config: ConfigParser) -&gt; SparkSession:\n    \"\"\"Function that generates a Spark Sedona session.\n\n    Args:\n        config (ConfigParser): Object with the final configuration.\n\n    Returns:\n        SparkSession: Session of spark.\n    \"\"\"\n    conf_dict = dict(config[SPARK_CONFIG_KEY])\n    master = conf_dict.pop(\"spark.master\")\n    session_name = conf_dict.pop(\"session_name\")\n\n    builder = SedonaContext.builder().appName(f\"{session_name}\").master(master)\n\n    # Configuration file spark configs\n    for k, v in conf_dict.items():\n        builder = builder.config(k, v)\n\n    ##################\n    # SEDONA\n    ##################\n\n    # Set sedona session\n    spark = SedonaContext.create(builder.getOrCreate())\n    sc = spark.sparkContext\n    sc.setSystemProperty(\"sedona.global.charset\", \"utf8\")\n\n    # Set log\n    sc.setLogLevel(\"ERROR\")\n    log4j = sc._jvm.org.apache.log4j\n    log4j.LogManager.getRootLogger().setLevel(log4j.Level.ERROR)\n\n    return spark\n</code></pre>"},{"location":"reference/core/spark_session/#core.spark_session.list_all_files_helper","title":"<code>list_all_files_helper(path, fs, conf)</code>","text":"<p>This function is used by list_all_files_recursively. This should not be called elsewhere Recursively traverses the file tree from given spot saving all files to a list and returns it.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>py4j.java_gateway.JavaObject: Object from parent function</p> required <code>fs</code> <code>JavaClass</code> <p>Object from parent function</p> required <code>conf</code> <code>JavaObject</code> <p>Object from parent function</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list[str]</code> <p>List of all files this folder and subdirectories of this folder.</p> Source code in <code>multimno/core/spark_session.py</code> <pre><code>def list_all_files_helper(\n    path: py4j.java_gateway.JavaObject, fs: py4j.java_gateway.JavaClass, conf: py4j.java_gateway.JavaObject\n) -&gt; list[str]:\n    \"\"\"\n    This function is used by list_all_files_recursively. This should not be called elsewhere\n    Recursively traverses the file tree from given spot saving all files to a list and returns it.\n\n    Args:\n        path (str): py4j.java_gateway.JavaObject: Object from parent function\n        fs (py4j.java_gateway.JavaClass): Object from parent function\n        conf (py4j.java_gateway.JavaObject): Object from parent function\n\n    Returns:\n        list: List of all files this folder and subdirectories of this folder.\n    \"\"\"\n    files_list = []\n\n    for f in fs.listStatus(path):\n        if f.isDirectory():\n            files_list.extend(list_all_files_helper(f.getPath(), fs, conf))\n        else:\n            files_list.append(str(f.getPath()))\n\n    return files_list\n</code></pre>"},{"location":"reference/core/spark_session/#core.spark_session.list_all_files_recursively","title":"<code>list_all_files_recursively(spark, data_path)</code>","text":"<p>If path is a file, returns a singleton list with this path. If path is a folder, return a list of all files in this folder and any of its subfolders</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Currently active spark session</p> required <code>data_path</code> <code>str</code> <p>Path to list the files of</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of all files in that folder and its subfolders</p> Source code in <code>multimno/core/spark_session.py</code> <pre><code>def list_all_files_recursively(spark: SparkSession, data_path: str) -&gt; list[str]:\n    \"\"\"\n    If path is a file, returns a singleton list with this path.\n    If path is a folder, return a list of all files in this folder and any of its subfolders\n\n    Args:\n        spark (SparkSession): Currently active spark session\n        data_path (str): Path to list the files of\n\n    Returns:\n        list[str]: A list of all files in that folder and its subfolders\n    \"\"\"\n    conf = spark._jsc.hadoopConfiguration()\n    uri = spark._jvm.java.net.URI.create(data_path)\n    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(uri, conf)\n    path = spark._jvm.org.apache.hadoop.fs.Path(data_path)\n    return list_all_files_helper(path, fs, conf)\n</code></pre>"},{"location":"reference/core/spark_session/#core.spark_session.list_parquet_partition_col_values","title":"<code>list_parquet_partition_col_values(spark, data_path)</code>","text":"<p>Lists all partition column values given a partition parquet folder</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Currently active spark session</p> required <code>data_path</code> <code>str</code> <p>Path of parquet</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>str, list[str]: Name of partition column, List of partition col values</p> Source code in <code>multimno/core/spark_session.py</code> <pre><code>def list_parquet_partition_col_values(spark: SparkSession, data_path: str) -&gt; list[str]:\n    \"\"\"\n    Lists all partition column values given a partition parquet folder\n\n    Args:\n        spark (SparkSession): Currently active spark session\n        data_path (str): Path of parquet\n\n    Returns:\n        str, list[str]: Name of partition column, List of partition col values\n    \"\"\"\n\n    hadoop = spark._jvm.org.apache.hadoop\n    fs = hadoop.fs.FileSystem\n    conf = hadoop.conf.Configuration()\n    path = hadoop.fs.Path(data_path)\n\n    partitions = []\n    for f in fs.get(conf).listStatus(path):\n        if f.isDirectory():\n            partitions.append(str(f.getPath().getName()))\n\n    if len(partitions) == 0:\n        return None, None\n\n    partition_col = partitions[0].split(\"=\")[0]\n\n    partitions = [p.split(\"=\")[1] for p in partitions]\n    return partition_col, sorted(partitions)\n</code></pre>"},{"location":"reference/core/utils/","title":"utils","text":"<p>This module contains utility functions for the multimno package.</p>"},{"location":"reference/core/utils/#core.utils.calc_hashed_user_id","title":"<code>calc_hashed_user_id(df, user_column=ColNames.user_id)</code>","text":"<p>Calculates SHA2 hash of user id, takes the first 31 bits and converts them to a non-negative 32-bit integer.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Data of clean synthetic events with a user id column.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: Dataframe, where user_id column is transformered to a hashed value.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def calc_hashed_user_id(df: DataFrame, user_column: str = ColNames.user_id) -&gt; DataFrame:\n    \"\"\"\n    Calculates SHA2 hash of user id, takes the first 31 bits and converts them to a non-negative 32-bit integer.\n\n    Args:\n        df (pyspark.sql.DataFrame): Data of clean synthetic events with a user id column.\n\n    Returns:\n        pyspark.sql.DataFrame: Dataframe, where user_id column is transformered to a hashed value.\n\n    \"\"\"\n\n    df = df.withColumn(user_column, F.unhex(F.sha2(F.col(user_column).cast(\"string\"), 256)))\n    return df\n</code></pre>"},{"location":"reference/core/utils/#core.utils.get_epsg_from_geometry_column","title":"<code>get_epsg_from_geometry_column(df)</code>","text":"<p>Get the EPSG code from the geometry column of a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with a geometry column.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the DataFrame contains multiple EPSG codes.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>EPSG code of the geometry column.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def get_epsg_from_geometry_column(df: DataFrame) -&gt; int:\n    \"\"\"\n    Get the EPSG code from the geometry column of a DataFrame.\n\n    Args:\n        df (DataFrame): DataFrame with a geometry column.\n\n    Raises:\n        ValueError: If the DataFrame contains multiple EPSG codes.\n\n    Returns:\n        int: EPSG code of the geometry column.\n    \"\"\"\n    # Get the EPSG code from the geometry column\n    temp = df.select(STF.ST_SRID(\"geometry\")).distinct().persist()\n    if temp.count() &gt; 1:\n        raise ValueError(\"Dataframe contains multiple EPSG codes\")\n\n    epsg = temp.collect()[0][0]\n    return epsg\n</code></pre>"},{"location":"reference/core/utils/#core.utils.spark_to_geopandas","title":"<code>spark_to_geopandas(df, epsg=None)</code>","text":"<p>Convert a Spark DataFrame to a geopandas GeoDataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Spark DataFrame to convert.</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: GeoDataFrame with the same data as the input DataFrame.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def spark_to_geopandas(df: DataFrame, epsg: int = None) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Convert a Spark DataFrame to a geopandas GeoDataFrame.\n\n    Args:\n        df (DataFrame): Spark DataFrame to convert.\n\n    Returns:\n        gpd.GeoDataFrame: GeoDataFrame with the same data as the input DataFrame.\n    \"\"\"\n    # Convert the DataFrame to a GeoDataFrame\n    if epsg is None:\n        epsg = get_epsg_from_geometry_column(df)\n    gdf = gpd.GeoDataFrame(df.toPandas(), crs=f\"EPSG:{epsg}\")\n\n    return gdf\n</code></pre>"},{"location":"reference/core/constants/","title":"constants","text":""},{"location":"reference/core/constants/columns/","title":"columns","text":"<p>Reusable internal column names. Useful for referring to the the same column across multiple components.</p>"},{"location":"reference/core/constants/columns/#core.constants.columns.ColNames","title":"<code>ColNames</code>","text":"<p>Class that enumerates all the column names.</p> Source code in <code>multimno/core/constants/columns.py</code> <pre><code>class ColNames:\n    \"\"\"\n    Class that enumerates all the column names.\n    \"\"\"\n\n    user_id = \"user_id\"\n    partition_id = \"partition_id\"\n    timestamp = \"timestamp\"\n    mcc = \"mcc\"\n    cell_id = \"cell_id\"\n    latitude = \"latitude\"\n    longitude = \"longitude\"\n    error_flag = \"error_flag\"\n\n    altitude = \"altitude\"\n    antenna_height = \"antenna_height\"\n    directionality = \"directionality\"\n    azimuth_angle = \"azimuth_angle\"\n    elevation_angle = \"elevation_angle\"\n    horizontal_beam_width = \"horizontal_beam_width\"\n    vertical_beam_width = \"vertical_beam_width\"\n    power = \"power\"\n    frequency = \"frequency\"\n    technology = \"technology\"\n    valid_date_start = \"valid_date_start\"\n    valid_date_end = \"valid_date_end\"\n    cell_type = \"cell_type\"\n\n    loc_error = \"loc_error\"\n    event_id = \"event_id\"\n\n    year = \"year\"\n    month = \"month\"\n    day = \"day\"\n    user_id_modulo = \"user_id_modulo\"\n\n    # for QA by column\n    variable = \"variable\"\n    type_of_error = \"type_of_error\"\n    type_of_transformation = \"type_of_transformation\"\n    value = \"value\"\n    result_timestamp = \"result_timestamp\"\n    data_period_start = \"data_period_start\"\n    data_period_end = \"data_period_end\"\n\n    initial_frequency = \"initial_frequency\"\n    final_frequency = \"final_frequency\"\n    date = \"date\"\n\n    # warnings\n    # log table\n    measure_definition = \"measure_definition\"\n    lookback_period = \"lookback_period\"\n    daily_value = \"daily_value\"\n    condition_value = \"condition_value\"\n    condition = \"condition\"\n    warning_text = \"warning_text\"\n    # for plots\n    type_of_qw = \"type_of_qw\"\n    average = \"average\"\n    UCL = \"UCL\"\n    LCL = \"LCL\"\n    title = \"title\"\n\n    # for grid generation\n    geometry = \"geometry\"\n    grid_id = \"grid_id\"\n    elevation = \"elevation\"\n    land_use = \"land_use\"\n    field_name = \"field_name\"\n    type_code = \"type_code\"\n    prior_probability = \"prior_probability\"\n\n    # device activity statistics\n    event_cnt = \"event_cnt\"\n    unique_cell_cnt = \"unique_cell_cnt\"\n    unique_location_cnt = \"unique_location_cnt\"\n    sum_distance_m = \"sum_distance_m\"\n    unique_hour_cnt = \"unique_hour_cnt\"\n    mean_time_gap = \"mean_time_gap\"\n    stdev_time_gap = \"stdev_time_gap\"\n\n    # signal\n    signal_strength = \"signal_strength\"\n    distance_to_cell = \"distance_to_cell\"\n    distance_to_cell_3D = \"distance_to_cell_3D\"\n    joined_geometry = \"joined_geometry\"\n    range = \"range\"\n    path_loss_exponent = \"path_loss_exponent\"\n    azimuth_signal_strength_back_loss = \"azimuth_signal_strength_back_loss\"\n    elevation_signal_strength_back_loss = \"elevation_signal_strength_back_loss\"\n\n    # for cell footprint\n    signal_dominance = \"signal_dominance\"\n    group_id = \"group_id\"\n    cells = \"cells\"\n    group_size = \"group_size\"\n\n    # time segments\n    time_segment_id = \"time_segment_id\"\n    start_timestamp = \"start_timestamp\"\n    end_timestamp = \"end_timestamp\"\n    state = \"state\"\n    is_last = \"is_last\"\n\n    # for cell connection probability\n    cell_connection_probability = \"cell_connection_probability\"\n    posterior_probability = \"posterior_probability\"\n\n    # dps (daily permanence score)\n    dps = \"dps\"\n    time_slot_initial_time = \"time_slot_initial_time\"\n    time_slot_duration = \"time_slot_duration\"\n\n    # diaries\n    stay_type = \"stay_type\"\n    activity_type = \"activity_type\"\n    initial_timestamp = \"initial_timestamp\"\n    final_timestamp = \"final_timestamp\"\n</code></pre>"},{"location":"reference/core/constants/conditions/","title":"conditions","text":""},{"location":"reference/core/constants/error_types/","title":"error_types","text":"<p>Transformations Error types module.</p>"},{"location":"reference/core/constants/error_types/#core.constants.error_types.ErrorTypes","title":"<code>ErrorTypes</code>","text":"<p>Class that enumerates the multiple error types of data transformations.</p> Source code in <code>multimno/core/constants/error_types.py</code> <pre><code>class ErrorTypes:\n    \"\"\"\n    Class that enumerates the multiple error types of data transformations.\n    \"\"\"\n\n    missing_value = 1\n    not_right_syntactic_format = 2\n    out_of_admissible_values = 3\n    inconsistency_between_variables = 4\n    no_location = 5\n    out_of_bounding_box = 6\n    no_error = 9\n    different_location_duplicate = 10\n    same_location_duplicate = 11\n\n    # This shows the possible error types that can happen in syntactic event cleaning\n    # This is used for creating the quality metrics data object\n    event_syntactic_cleaning_possible_errors = [1, 2, 3, 4, 5, 6, 9]\n\n    # This shows the possible error types that can happen in deduplication\n    event_deduplication_possible_types = [10, 11]\n</code></pre>"},{"location":"reference/core/constants/error_types/#core.constants.error_types.NetworkErrorType","title":"<code>NetworkErrorType</code>","text":"<p>Class that enumerates the multiple error types present in network topology data.</p> Source code in <code>multimno/core/constants/error_types.py</code> <pre><code>class NetworkErrorType:\n    \"\"\"\n    Class that enumerates the multiple error types present in network topology data.\n    \"\"\"\n\n    NO_ERROR = 0\n    NULL_VALUE = 1\n    OUT_OF_RANGE = 2\n    UNSUPPORTED_TYPE = 3\n    CANNOT_PARSE = 4\n    INITIAL_ROWS = 100\n    FINAL_ROWS = 101\n</code></pre>"},{"location":"reference/core/constants/error_types/#core.constants.error_types.SemanticErrorType","title":"<code>SemanticErrorType</code>","text":"<p>Class that enumerates the multiple error types associated to event semantic checks.</p> Source code in <code>multimno/core/constants/error_types.py</code> <pre><code>class SemanticErrorType:\n    \"\"\"\n    Class that enumerates the multiple error types associated to event semantic checks.\n    \"\"\"\n\n    NO_ERROR = 0\n    CELL_ID_NON_EXISTENT = 1\n    CELL_ID_NOT_VALID = 2\n    INCORRECT_EVENT_LOCATION = 3\n    SUSPICIOUS_EVENT_LOCATION = 4\n</code></pre>"},{"location":"reference/core/constants/measure_definitions/","title":"measure_definitions","text":""},{"location":"reference/core/constants/network_default_thresholds/","title":"network_default_thresholds","text":"<p>Contains the default threshold values used by the Network Syntactic Quality Warnings</p>"},{"location":"reference/core/constants/semantic_qw_default_thresholds/","title":"semantic_qw_default_thresholds","text":"<p>Contains the default threshold values used by the Event Device Semantic Quality Warnings</p>"},{"location":"reference/core/constants/transformations/","title":"transformations","text":"<p>Data transformations types modukle</p>"},{"location":"reference/core/constants/transformations/#core.constants.transformations.Transformations","title":"<code>Transformations</code>","text":"<p>Class that enumerates the multiple data transformations types.</p> Source code in <code>multimno/core/constants/transformations.py</code> <pre><code>class Transformations:\n    \"\"\"\n    Class that enumerates the multiple data transformations types.\n    \"\"\"\n\n    converted_timestamp = 1\n    other_conversion = 2\n    no_transformation = 9\n\n    event_syntactic_cleaning_possible_transformations = [1, 2, 9]\n</code></pre>"},{"location":"reference/core/constants/warnings/","title":"warnings","text":""},{"location":"reference/core/data_objects/","title":"data_objects","text":""},{"location":"reference/core/data_objects/data_object/","title":"data_object","text":"<p>Module that defines the data object abstract classes</p>"},{"location":"reference/core/data_objects/data_object/#core.data_objects.data_object.DataObject","title":"<code>DataObject</code>","text":"<p>Abstract class that models a DataObject. It defines its data schema including the attributes that compose it.</p> Source code in <code>multimno/core/data_objects/data_object.py</code> <pre><code>class DataObject(metaclass=ABCMeta):\n    \"\"\"\n    Abstract class that models a DataObject. It defines its data schema including the attributes that compose it.\n    \"\"\"\n\n    ID: str = None\n    SCHEMA: StructType = None\n\n    def __init__(self, spark: SparkSession) -&gt; None:\n        self.df: DataFrame = None\n        self.spark: SparkSession = spark\n        self.interface: IOInterface = None\n\n    def read(self, *args, **kwargs):\n        \"\"\"\n        Method that performs the read operation of the data object dataframe through an IOInterface.\n        \"\"\"\n        self.df = self.interface.read_from_interface(*args, **kwargs)\n\n    def write(self, *args, **kwargs):\n        \"\"\"\n        Method that performs the write operation of the data object dataframe through an IOInterface.\n        \"\"\"\n        self.interface.write_from_interface(self.df, *args, **kwargs)\n</code></pre>"},{"location":"reference/core/data_objects/data_object/#core.data_objects.data_object.DataObject.read","title":"<code>read(*args, **kwargs)</code>","text":"<p>Method that performs the read operation of the data object dataframe through an IOInterface.</p> Source code in <code>multimno/core/data_objects/data_object.py</code> <pre><code>def read(self, *args, **kwargs):\n    \"\"\"\n    Method that performs the read operation of the data object dataframe through an IOInterface.\n    \"\"\"\n    self.df = self.interface.read_from_interface(*args, **kwargs)\n</code></pre>"},{"location":"reference/core/data_objects/data_object/#core.data_objects.data_object.DataObject.write","title":"<code>write(*args, **kwargs)</code>","text":"<p>Method that performs the write operation of the data object dataframe through an IOInterface.</p> Source code in <code>multimno/core/data_objects/data_object.py</code> <pre><code>def write(self, *args, **kwargs):\n    \"\"\"\n    Method that performs the write operation of the data object dataframe through an IOInterface.\n    \"\"\"\n    self.interface.write_from_interface(self.df, *args, **kwargs)\n</code></pre>"},{"location":"reference/core/data_objects/data_object/#core.data_objects.data_object.PathDataObject","title":"<code>PathDataObject</code>","text":"<p>             Bases: <code>DataObject</code></p> <p>Abstract Class that models DataObjects that will use a PathInterface for IO operations. It inherits the DataObject abstract class.</p> Source code in <code>multimno/core/data_objects/data_object.py</code> <pre><code>class PathDataObject(DataObject, metaclass=ABCMeta):\n    \"\"\"Abstract Class that models DataObjects that will use a PathInterface for IO operations.\n    It inherits the DataObject abstract class.\n    \"\"\"\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark)\n        self.interface: PathInterface = None\n        self.default_path: str = default_path\n\n    def read(self, *args, path: str = None, **kwargs):\n        if path is None:\n            path = self.default_path\n        self.df = self.interface.read_from_interface(self.spark, path, self.SCHEMA)\n\n    def write(self, *args, path: str = None, partition_columns: list[str] = None, **kwargs):\n        if path is None:\n            path = self.default_path\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/bronze/","title":"bronze","text":""},{"location":"reference/core/data_objects/bronze/bronze_event_data_object/","title":"bronze_event_data_object","text":"<p>Bronze MNO Event data module</p>"},{"location":"reference/core/data_objects/bronze/bronze_event_data_object/#core.data_objects.bronze.bronze_event_data_object.BronzeEventDataObject","title":"<code>BronzeEventDataObject</code>","text":"<p>             Bases: <code>PathDataObject</code></p> <p>Class that models the RAW MNO Event data.</p> Source code in <code>multimno/core/data_objects/bronze/bronze_event_data_object.py</code> <pre><code>class BronzeEventDataObject(PathDataObject):\n    \"\"\"\n    Class that models the RAW MNO Event data.\n    \"\"\"\n\n    ID = \"BronzeEventDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=True),\n            StructField(ColNames.timestamp, StringType(), nullable=True),\n            StructField(ColNames.mcc, IntegerType(), nullable=True),\n            StructField(ColNames.cell_id, StringType(), nullable=True),\n            StructField(ColNames.latitude, FloatType(), nullable=True),\n            StructField(ColNames.longitude, FloatType(), nullable=True),\n            StructField(ColNames.loc_error, FloatType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=True),\n            StructField(ColNames.month, ByteType(), nullable=True),\n            StructField(ColNames.day, ByteType(), nullable=True),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/bronze/bronze_network_physical_data_object/","title":"bronze_network_physical_data_object","text":"<p>Bronze MNO Network Topology Data module</p> <p>Currently, only considers the \"Cell Locations with Physical Properties\" type</p>"},{"location":"reference/core/data_objects/bronze/bronze_network_physical_data_object/#core.data_objects.bronze.bronze_network_physical_data_object.BronzeNetworkDataObject","title":"<code>BronzeNetworkDataObject</code>","text":"<p>             Bases: <code>PathDataObject</code></p> <p>Class that models the RAW MNO Network Topology Data, based on the physical properties of the cells.</p> Source code in <code>multimno/core/data_objects/bronze/bronze_network_physical_data_object.py</code> <pre><code>class BronzeNetworkDataObject(PathDataObject):\n    \"\"\"\n    Class that models the RAW MNO Network Topology Data, based on the physical\n    properties of the cells.\n    \"\"\"\n\n    ID = \"BronzeNetworkDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.cell_id, StringType(), nullable=True),\n            StructField(ColNames.latitude, FloatType(), nullable=True),\n            StructField(ColNames.longitude, FloatType(), nullable=True),\n            StructField(ColNames.altitude, FloatType(), nullable=True),\n            StructField(ColNames.antenna_height, FloatType(), nullable=True),\n            StructField(ColNames.directionality, IntegerType(), nullable=True),\n            StructField(ColNames.azimuth_angle, FloatType(), nullable=True),\n            StructField(ColNames.elevation_angle, FloatType(), nullable=True),\n            StructField(ColNames.horizontal_beam_width, FloatType(), nullable=True),\n            StructField(ColNames.vertical_beam_width, FloatType(), nullable=True),\n            StructField(ColNames.power, FloatType(), nullable=True),\n            StructField(ColNames.frequency, IntegerType(), nullable=True),\n            StructField(ColNames.technology, StringType(), nullable=True),\n            StructField(ColNames.valid_date_start, StringType(), nullable=True),\n            StructField(ColNames.valid_date_end, StringType(), nullable=True),\n            StructField(ColNames.cell_type, StringType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=True),\n            StructField(ColNames.month, ByteType(), nullable=True),\n            StructField(ColNames.day, ByteType(), nullable=True),\n        ]\n    )\n    MANDATORY_COLUMNS = [ColNames.cell_id, ColNames.latitude, ColNames.longitude]\n\n    OPTIONAL_COLUMNS = [\n        ColNames.altitude,\n        ColNames.antenna_height,\n        ColNames.directionality,\n        ColNames.azimuth_angle,\n        ColNames.elevation_angle,\n        ColNames.horizontal_beam_width,\n        ColNames.vertical_beam_width,\n        ColNames.power,\n        ColNames.frequency,\n        ColNames.technology,\n        ColNames.valid_date_start,\n        ColNames.valid_date_end,\n        ColNames.cell_type,\n    ]\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/bronze/bronze_synthetic_diaries_data_object/","title":"bronze_synthetic_diaries_data_object","text":"<p>Bronze Synthetic Diaries Data module</p>"},{"location":"reference/core/data_objects/bronze/bronze_synthetic_diaries_data_object/#core.data_objects.bronze.bronze_synthetic_diaries_data_object.BronzeSyntheticDiariesDataObject","title":"<code>BronzeSyntheticDiariesDataObject</code>","text":"<p>             Bases: <code>PathDataObject</code></p> <p>Class that models synthetically-generated agents activity-trip diaries.</p> <pre><code>    ''''''\n</code></pre> Source code in <code>multimno/core/data_objects/bronze/bronze_synthetic_diaries_data_object.py</code> <pre><code>class BronzeSyntheticDiariesDataObject(PathDataObject):\n    \"\"\"\n    Class that models synthetically-generated agents activity-trip diaries.\n\n            ''''''\n\n    \"\"\"\n\n    ID = \"BronzeSyntheticDiariesDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.activity_type, StringType(), nullable=True),\n            StructField(ColNames.stay_type, StringType(), nullable=True),\n            StructField(ColNames.longitude, FloatType(), nullable=True),\n            StructField(ColNames.latitude, FloatType(), nullable=True),\n            StructField(ColNames.initial_timestamp, TimestampType(), nullable=True),\n            StructField(ColNames.final_timestamp, TimestampType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/landing/","title":"landing","text":""},{"location":"reference/core/data_objects/silver/","title":"silver","text":""},{"location":"reference/core/data_objects/silver/silver_cell_connection_probabilities_data_object/","title":"silver_cell_connection_probabilities_data_object","text":"<p>Cell connection probabilities.</p>"},{"location":"reference/core/data_objects/silver/silver_cell_connection_probabilities_data_object/#core.data_objects.silver.silver_cell_connection_probabilities_data_object.SilverCellConnectionProbabilitiesDataObject","title":"<code>SilverCellConnectionProbabilitiesDataObject</code>","text":"<p>             Bases: <code>PathDataObject</code></p> Source code in <code>multimno/core/data_objects/silver/silver_cell_connection_probabilities_data_object.py</code> <pre><code>class SilverCellConnectionProbabilitiesDataObject(PathDataObject):\n    \"\"\" \"\"\"\n\n    ID = \"SilverCellConnectionProbabilitiesDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.cell_id, StringType(), nullable=True),\n            StructField(ColNames.grid_id, StringType(), nullable=True),\n            StructField(ColNames.valid_date_start, DateType(), nullable=True),\n            StructField(ColNames.valid_date_end, DateType(), nullable=True),\n            StructField(ColNames.cell_connection_probability, FloatType(), nullable=True),\n            StructField(ColNames.posterior_probability, FloatType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=True),\n            StructField(ColNames.month, ByteType(), nullable=True),\n            StructField(ColNames.day, ByteType(), nullable=True),\n        ]\n    )\n\n    MANDATORY_COLUMNS = [\n        ColNames.cell_id,\n        ColNames.grid_id,\n        ColNames.valid_date_start,\n        ColNames.valid_date_end,\n        ColNames.cell_connection_probability,\n        ColNames.posterior_probability,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    ]\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_cell_footprint_data_object/","title":"silver_cell_footprint_data_object","text":""},{"location":"reference/core/data_objects/silver/silver_cell_footprint_data_object/#core.data_objects.silver.silver_cell_footprint_data_object.SilverCellFootprintDataObject","title":"<code>SilverCellFootprintDataObject</code>","text":"<p>             Bases: <code>PathDataObject</code></p> Source code in <code>multimno/core/data_objects/silver/silver_cell_footprint_data_object.py</code> <pre><code>class SilverCellFootprintDataObject(PathDataObject):\n    \"\"\" \"\"\"\n\n    ID = \"SilverCellFootprintDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.cell_id, StringType(), nullable=True),\n            StructField(ColNames.grid_id, StringType(), nullable=True),\n            StructField(ColNames.valid_date_start, DateType(), nullable=True),\n            StructField(ColNames.valid_date_end, DateType(), nullable=True),\n            StructField(ColNames.signal_dominance, FloatType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=True),\n            StructField(ColNames.month, ByteType(), nullable=True),\n            StructField(ColNames.day, ByteType(), nullable=True),\n        ]\n    )\n\n    MANDATORY_COLUMNS = [\n        ColNames.cell_id,\n        ColNames.grid_id,\n        ColNames.valid_date_start,\n        ColNames.valid_date_end,\n        ColNames.signal_dominance,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    ]\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_cell_intersection_groups_data_object/","title":"silver_cell_intersection_groups_data_object","text":""},{"location":"reference/core/data_objects/silver/silver_cell_intersection_groups_data_object/#core.data_objects.silver.silver_cell_intersection_groups_data_object.SilverCellIntersectionGroupsDataObject","title":"<code>SilverCellIntersectionGroupsDataObject</code>","text":"<p>             Bases: <code>PathDataObject</code></p> Source code in <code>multimno/core/data_objects/silver/silver_cell_intersection_groups_data_object.py</code> <pre><code>class SilverCellIntersectionGroupsDataObject(PathDataObject):\n    \"\"\" \"\"\"\n\n    ID = \"SilverCellIntersectionGroupsDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.group_id, StringType(), nullable=True),\n            StructField(ColNames.cells, ArrayType(StringType()), nullable=True),\n            StructField(ColNames.group_size, IntegerType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=True),\n            StructField(ColNames.month, ByteType(), nullable=True),\n            StructField(ColNames.day, ByteType(), nullable=True),\n        ]\n    )\n\n    MANDATORY_COLUMNS = [\n        ColNames.group_id,\n        ColNames.cells,\n        ColNames.group_size,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    ]\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_daily_permanence_score_data_object/","title":"silver_daily_permanence_score_data_object","text":"<p>Silver Daily Permanence Score data module</p>"},{"location":"reference/core/data_objects/silver/silver_daily_permanence_score_data_object/#core.data_objects.silver.silver_daily_permanence_score_data_object.SilverDailyPermanenceScoreDataObject","title":"<code>SilverDailyPermanenceScoreDataObject</code>","text":"<p>             Bases: <code>PathDataObject</code></p> <p>Class that models the Daily Permanence Score data.</p> Source code in <code>multimno/core/data_objects/silver/silver_daily_permanence_score_data_object.py</code> <pre><code>class SilverDailyPermanenceScoreDataObject(PathDataObject):\n    \"\"\"\n    Class that models the Daily Permanence Score data.\n    \"\"\"\n\n    ID = \"SilverDailyPermanenceScoreDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.grid_id, StringType(), nullable=False),\n            StructField(ColNames.time_slot_initial_time, TimestampType(), nullable=False),\n            StructField(ColNames.time_slot_duration, IntegerType(), nullable=False),\n            StructField(ColNames.dps, IntegerType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.user_id_modulo, IntegerType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface: ParquetInterface = ParquetInterface()\n        self.partition_columns = [ColNames.year, ColNames.month, ColNames.day, ColNames.user_id_modulo]\n\n        # Clear path\n        self.first_write = True\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        # If it is the first writing of this data object, clear the input directory, otherwise add\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n\n        self.df.write.format(\n            self.interface.FILE_FORMAT,  # File format\n        ).partitionBy(self.partition_columns).mode(\n            \"append\"\n        ).save(path)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_device_activity_statistics/","title":"silver_device_activity_statistics","text":"<p>Silver Device Activity Statistics module</p>"},{"location":"reference/core/data_objects/silver/silver_device_activity_statistics/#core.data_objects.silver.silver_device_activity_statistics.SilverDeviceActivityStatistics","title":"<code>SilverDeviceActivityStatistics</code>","text":"<p>             Bases: <code>PathDataObject</code></p> <p>Class that models the cleaned MNO Event data.</p> Source code in <code>multimno/core/data_objects/silver/silver_device_activity_statistics.py</code> <pre><code>class SilverDeviceActivityStatistics(PathDataObject):\n    \"\"\"\n    Class that models the cleaned MNO Event data.\n    \"\"\"\n\n    ID = \"SilverDeviceActivityStatisticsDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.event_cnt, IntegerType(), nullable=False),\n            StructField(ColNames.unique_cell_cnt, ShortType(), nullable=False),\n            StructField(ColNames.unique_location_cnt, ShortType(), nullable=False),\n            StructField(ColNames.sum_distance_m, IntegerType(), nullable=True),\n            StructField(ColNames.unique_hour_cnt, ByteType(), nullable=False),\n            StructField(ColNames.mean_time_gap, IntegerType(), nullable=True),\n            StructField(ColNames.stdev_time_gap, FloatType(), nullable=True),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface: ParquetInterface = ParquetInterface()\n        self.partition_columns = [ColNames.year, ColNames.month, ColNames.day]\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        # If it is the first writing of this data object, clear the input directory, otherwise add\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n\n        self.df.write.format(\n            self.interface.FILE_FORMAT,  # File format\n        ).partitionBy(self.partition_columns).mode(\n            \"append\"\n        ).save(path)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_event_data_object/","title":"silver_event_data_object","text":"<p>Silver MNO Event data module</p>"},{"location":"reference/core/data_objects/silver/silver_event_data_object/#core.data_objects.silver.silver_event_data_object.SilverEventDataObject","title":"<code>SilverEventDataObject</code>","text":"<p>             Bases: <code>PathDataObject</code></p> <p>Class that models the cleaned MNO Event data.</p> Source code in <code>multimno/core/data_objects/silver/silver_event_data_object.py</code> <pre><code>class SilverEventDataObject(PathDataObject):\n    \"\"\"\n    Class that models the cleaned MNO Event data.\n    \"\"\"\n\n    ID = \"SilverEventDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.mcc, IntegerType(), nullable=False),\n            StructField(ColNames.cell_id, StringType(), nullable=True),\n            StructField(ColNames.latitude, FloatType(), nullable=True),\n            StructField(ColNames.longitude, FloatType(), nullable=True),\n            StructField(ColNames.loc_error, FloatType(), nullable=True),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.user_id_modulo, IntegerType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface: ParquetInterface = ParquetInterface()\n        self.partition_columns = [ColNames.year, ColNames.month, ColNames.day, ColNames.user_id_modulo]\n\n        # Clear path\n        self.first_write = True\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        # If it is the first writing of this data object, clear the input directory, otherwise add\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n\n        self.df.write.format(\n            self.interface.FILE_FORMAT,  # File format\n        ).partitionBy(self.partition_columns).mode(\n            \"append\"\n        ).save(path)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_metrics_by_column/","title":"silver_event_data_syntactic_quality_metrics_by_column","text":"<p>Silver Event Data quality metrics.</p>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_metrics_by_column/#core.data_objects.silver.silver_event_data_syntactic_quality_metrics_by_column.SilverEventDataSyntacticQualityMetricsByColumn","title":"<code>SilverEventDataSyntacticQualityMetricsByColumn</code>","text":"<p>             Bases: <code>PathDataObject</code></p> <p>Class that models the Silver Event Data quality metrics DataObject.</p> Source code in <code>multimno/core/data_objects/silver/silver_event_data_syntactic_quality_metrics_by_column.py</code> <pre><code>class SilverEventDataSyntacticQualityMetricsByColumn(PathDataObject):\n    \"\"\"\n    Class that models the Silver Event Data quality metrics DataObject.\n    \"\"\"\n\n    ID = \"SilverEventDataSyntacticQualityMetricsByColumn\"\n    SCHEMA = StructType(\n        [\n            StructField(\"result_timestamp\", TimestampType(), nullable=False),\n            StructField(\"date\", DateType(), nullable=False),\n            StructField(\"variable\", StringType(), nullable=True),\n            StructField(\"type_of_error\", ShortType(), nullable=True),\n            StructField(\"type_of_transformation\", ShortType(), nullable=True),\n            StructField(\"value\", IntegerType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface: ParquetInterface = ParquetInterface()\n        self.partition_columns = [\"date\"]\n\n        # (variable, type_of_error, type_of_transformation) : value\n        self.error_and_transformation_counts = defaultdict(int)\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.df.write.format(\n            self.interface.FILE_FORMAT,  # File format\n        ).partitionBy(partition_columns).mode(\n            \"append\"\n        ).save(path)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_metrics_frequency_distribution/","title":"silver_event_data_syntactic_quality_metrics_frequency_distribution","text":"<p>Silver Event Data deduplication frequency quality metrics.</p>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_metrics_frequency_distribution/#core.data_objects.silver.silver_event_data_syntactic_quality_metrics_frequency_distribution.SilverEventDataSyntacticQualityMetricsFrequencyDistribution","title":"<code>SilverEventDataSyntacticQualityMetricsFrequencyDistribution</code>","text":"<p>             Bases: <code>PathDataObject</code></p> <p>Class that models the Silver Event Data syntactic frequency quality metrics DataObject.</p> Source code in <code>multimno/core/data_objects/silver/silver_event_data_syntactic_quality_metrics_frequency_distribution.py</code> <pre><code>class SilverEventDataSyntacticQualityMetricsFrequencyDistribution(PathDataObject):\n    \"\"\"\n    Class that models the Silver Event Data syntactic\n    frequency quality metrics DataObject.\n    \"\"\"\n\n    ID = \"SilverEventDataSyntacticQualityMetricsFrequencyDistribution\"\n    SCHEMA = StructType(\n        [\n            StructField(\"cell_id\", StringType(), nullable=True),\n            StructField(\"user_id\", BinaryType(), nullable=True),\n            StructField(\"initial_frequency\", IntegerType(), nullable=False),\n            StructField(\"final_frequency\", IntegerType(), nullable=False),\n            StructField(\"date\", DateType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface: ParquetInterface = ParquetInterface()\n        self.partition_columns = [\"date\"]\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.df.write.format(\n            self.interface.FILE_FORMAT,  # File format\n        ).partitionBy(partition_columns).mode(\n            \"append\"\n        ).save(path)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_warnings_for_plots/","title":"silver_event_data_syntactic_quality_warnings_for_plots","text":"<p>Silver Event Data quality warning for plots table.</p>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_warnings_for_plots/#core.data_objects.silver.silver_event_data_syntactic_quality_warnings_for_plots.SilverEventDataSyntacticQualityWarningsForPlots","title":"<code>SilverEventDataSyntacticQualityWarningsForPlots</code>","text":"<p>             Bases: <code>PathDataObject</code></p> <p>Class that store data to plot raw, clean data sizez and error rate.</p> Source code in <code>multimno/core/data_objects/silver/silver_event_data_syntactic_quality_warnings_for_plots.py</code> <pre><code>class SilverEventDataSyntacticQualityWarningsForPlots(PathDataObject):\n    \"\"\"\n    Class that store data to plot raw, clean data sizez and error rate.\n    \"\"\"\n\n    ID = \"SilverEventDataSyntacticQualityWarningsForPlots\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.date, DateType(), nullable=False),\n            StructField(ColNames.type_of_qw, StringType(), nullable=False),\n            StructField(ColNames.lookback_period, StringType(), nullable=False),\n            StructField(ColNames.daily_value, FloatType(), nullable=False),\n            StructField(ColNames.average, FloatType(), nullable=True),\n            StructField(ColNames.LCL, FloatType(), nullable=True),\n            StructField(ColNames.UCL, FloatType(), nullable=True),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface: ParquetInterface = ParquetInterface()\n        self.partition_columns = [ColNames.date]\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.df.write.format(\n            self.interface.FILE_FORMAT,\n        ).partitionBy(partition_columns).mode(\n            \"append\"\n        ).save(path)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_warnings_log_table/","title":"silver_event_data_syntactic_quality_warnings_log_table","text":"<p>Silver Event Data Quality Warning log table.</p>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_warnings_log_table/#core.data_objects.silver.silver_event_data_syntactic_quality_warnings_log_table.SilverEventDataSyntacticQualityWarningsLogTable","title":"<code>SilverEventDataSyntacticQualityWarningsLogTable</code>","text":"<p>             Bases: <code>PathDataObject</code></p> <p>Class that stores information about Event Quallity Warnings</p> Source code in <code>multimno/core/data_objects/silver/silver_event_data_syntactic_quality_warnings_log_table.py</code> <pre><code>class SilverEventDataSyntacticQualityWarningsLogTable(PathDataObject):\n    \"\"\"\n    Class that stores information about Event Quallity Warnings\n    \"\"\"\n\n    ID = \"SilverEventDataSyntacticQualityWarningsLogTable\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.date, DateType(), nullable=False),\n            StructField(ColNames.measure_definition, StringType(), nullable=False),\n            StructField(ColNames.lookback_period, StringType(), nullable=False),\n            StructField(ColNames.daily_value, FloatType(), nullable=False),\n            StructField(ColNames.condition_value, FloatType(), nullable=True),\n            StructField(ColNames.condition, StringType(), nullable=False),\n            StructField(ColNames.warning_text, StringType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface: ParquetInterface = ParquetInterface()\n        self.partition_columns = [ColNames.date]\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.df.write.format(\n            self.interface.FILE_FORMAT,\n        ).partitionBy(partition_columns).mode(\n            \"append\"\n        ).save(path)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_event_flagged_data_object/","title":"silver_event_flagged_data_object","text":"<p>Silver MNO Event data module with flags computed in Semantic Checks</p>"},{"location":"reference/core/data_objects/silver/silver_event_flagged_data_object/#core.data_objects.silver.silver_event_flagged_data_object.SilverEventFlaggedDataObject","title":"<code>SilverEventFlaggedDataObject</code>","text":"<p>             Bases: <code>PathDataObject</code></p> <p>Class that models the cleaned MNO Event data, with flags computed in the semantic checks module.</p> Source code in <code>multimno/core/data_objects/silver/silver_event_flagged_data_object.py</code> <pre><code>class SilverEventFlaggedDataObject(PathDataObject):\n    \"\"\"\n    Class that models the cleaned MNO Event data, with flags computed\n    in the semantic checks module.\n    \"\"\"\n\n    ID = \"SilverEventFlaggedDO\"\n\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.mcc, IntegerType(), nullable=False),\n            StructField(ColNames.cell_id, StringType(), nullable=True),\n            StructField(ColNames.latitude, FloatType(), nullable=True),\n            StructField(ColNames.longitude, FloatType(), nullable=True),\n            StructField(ColNames.loc_error, FloatType(), nullable=True),\n            StructField(ColNames.error_flag, IntegerType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.user_id_modulo, IntegerType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = [ColNames.year, ColNames.month, ColNames.day, ColNames.user_id_modulo]\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_grid_data_object/","title":"silver_grid_data_object","text":""},{"location":"reference/core/data_objects/silver/silver_grid_data_object/#core.data_objects.silver.silver_grid_data_object.SilverGridDataObject","title":"<code>SilverGridDataObject</code>","text":"<p>             Bases: <code>PathDataObject</code></p> <p>Class that models operational grid.</p> Source code in <code>multimno/core/data_objects/silver/silver_grid_data_object.py</code> <pre><code>class SilverGridDataObject(PathDataObject):\n    \"\"\"\n    Class that models operational grid.\n    \"\"\"\n\n    ID = \"SilverGridDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.geometry, GeometryType(), nullable=False),\n            StructField(ColNames.grid_id, StringType(), nullable=False),\n            StructField(ColNames.elevation, FloatType(), nullable=True),\n            StructField(ColNames.land_use, StringType(), nullable=True),\n            StructField(ColNames.prior_probability, FloatType(), nullable=True),\n        ]\n    )\n\n    MANDATORY_COLUMNS = [ColNames.grid_id, ColNames.geometry]\n    OPTIONAL_COLUMNS = [ColNames.elevation, ColNames.land_use, ColNames.prior_probability]\n\n    def __init__(\n        self, spark: SparkSession, default_path: str, partition_columns: list[str] = None, default_crs: int = 3035\n    ) -&gt; None:\n\n        super().__init__(spark, default_path)\n        self.interface: GeoParquetInterface = GeoParquetInterface()\n        self.partition_columns = partition_columns\n\n        # Clear path\n        self.first_write = True\n        self.default_crs = default_crs\n\n    def read(self):\n\n        self.df = self.interface.read_from_interface(self.spark, self.default_path, self.SCHEMA)\n        self.df = self.df.withColumn(ColNames.geometry, STF.ST_SetSRID((ColNames.geometry), F.lit(self.default_crs)))\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_network_data_object/","title":"silver_network_data_object","text":"<p>Silver MNO Network Topology Data module</p>"},{"location":"reference/core/data_objects/silver/silver_network_data_object/#core.data_objects.silver.silver_network_data_object.SilverNetworkDataObject","title":"<code>SilverNetworkDataObject</code>","text":"<p>             Bases: <code>PathDataObject</code></p> <p>Class that models the clean MNO Network Topology Data, based on the physical properties of the cells.</p> Source code in <code>multimno/core/data_objects/silver/silver_network_data_object.py</code> <pre><code>class SilverNetworkDataObject(PathDataObject):\n    \"\"\"\n    Class that models the clean MNO Network Topology Data, based on the physical\n    properties of the cells.\n    \"\"\"\n\n    ID = \"SilverNetworkDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.cell_id, StringType(), nullable=False),\n            StructField(ColNames.latitude, FloatType(), nullable=False),\n            StructField(ColNames.longitude, FloatType(), nullable=False),\n            StructField(ColNames.altitude, FloatType(), nullable=True),\n            StructField(ColNames.antenna_height, FloatType(), nullable=True),\n            StructField(ColNames.directionality, IntegerType(), nullable=True),\n            StructField(ColNames.azimuth_angle, FloatType(), nullable=True),\n            StructField(ColNames.elevation_angle, FloatType(), nullable=True),\n            StructField(ColNames.horizontal_beam_width, FloatType(), nullable=True),\n            StructField(ColNames.vertical_beam_width, FloatType(), nullable=True),\n            StructField(ColNames.power, FloatType(), nullable=True),\n            StructField(ColNames.frequency, IntegerType(), nullable=True),\n            StructField(ColNames.technology, StringType(), nullable=True),\n            StructField(ColNames.valid_date_start, TimestampType(), nullable=True),\n            StructField(ColNames.valid_date_end, TimestampType(), nullable=True),\n            StructField(ColNames.cell_type, StringType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_network_data_syntactic_quality_metrics_by_column/","title":"silver_network_data_syntactic_quality_metrics_by_column","text":"<p>Silver Network topology quality metrics.</p>"},{"location":"reference/core/data_objects/silver/silver_network_data_syntactic_quality_metrics_by_column/#core.data_objects.silver.silver_network_data_syntactic_quality_metrics_by_column.SilverNetworkDataQualityMetricsByColumn","title":"<code>SilverNetworkDataQualityMetricsByColumn</code>","text":"<p>             Bases: <code>PathDataObject</code></p> <p>Class that models the Silver Network Topology data quality metrics data object.</p> Source code in <code>multimno/core/data_objects/silver/silver_network_data_syntactic_quality_metrics_by_column.py</code> <pre><code>class SilverNetworkDataQualityMetricsByColumn(PathDataObject):\n    \"\"\"\n    Class that models the Silver Network Topology data quality metrics data object.\n    \"\"\"\n\n    ID = \"SilverNetworkDataQualityMetricsByColumn\"\n\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.result_timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.date, DateType(), nullable=False),\n            StructField(ColNames.field_name, StringType(), nullable=True),\n            StructField(ColNames.type_code, IntegerType(), nullable=False),\n            StructField(ColNames.value, IntegerType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_network_syntactic_quality_warnings_log_table/","title":"silver_network_syntactic_quality_warnings_log_table","text":"<p>Silver MNO Network Topology Quality Warnings Log Table Data Object</p>"},{"location":"reference/core/data_objects/silver/silver_network_syntactic_quality_warnings_log_table/#core.data_objects.silver.silver_network_syntactic_quality_warnings_log_table.SilverNetworkDataSyntacticQualityWarningsLogTable","title":"<code>SilverNetworkDataSyntacticQualityWarningsLogTable</code>","text":"<p>             Bases: <code>PathDataObject</code></p> <p>Class that models the log table keeping track of the quality warnings that may arise from the syntactic checks and cleaning of the MNO Network Topology Data.</p> Source code in <code>multimno/core/data_objects/silver/silver_network_syntactic_quality_warnings_log_table.py</code> <pre><code>class SilverNetworkDataSyntacticQualityWarningsLogTable(PathDataObject):\n    \"\"\"\n    Class that models the log table keeping track of the quality warnings that may arise from\n    the syntactic checks and cleaning of the MNO Network Topology Data.\n    \"\"\"\n\n    ID = \"SilverNetworkDataSyntacticQualityWarningsLogTable\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.title, StringType(), nullable=False),\n            StructField(ColNames.date, DateType(), nullable=False),  # date of study analysed\n            StructField(ColNames.timestamp, TimestampType(), nullable=False),  # moment when QW where generated\n            StructField(ColNames.measure_definition, StringType(), nullable=False),\n            StructField(ColNames.daily_value, FloatType(), nullable=False),\n            StructField(ColNames.condition, StringType(), nullable=False),\n            StructField(ColNames.lookback_period, StringType(), nullable=False),  # using same name as for events\n            StructField(ColNames.condition_value, FloatType(), nullable=False),\n            StructField(ColNames.warning_text, StringType(), nullable=False),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        # Always append\n        self.df.write.format(\n            self.interface.FILE_FORMAT,\n        ).partitionBy(partition_columns).mode(\n            \"append\"\n        ).save(path)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_network_syntactic_quality_warnings_plot_data/","title":"silver_network_syntactic_quality_warnings_plot_data","text":"<p>Silver MNO Network Topology Quality Warnings Data Object for the generation of plots</p>"},{"location":"reference/core/data_objects/silver/silver_network_syntactic_quality_warnings_plot_data/#core.data_objects.silver.silver_network_syntactic_quality_warnings_plot_data.SilverNetworkSyntacticQualityWarningsLinePlotData","title":"<code>SilverNetworkSyntacticQualityWarningsLinePlotData</code>","text":"<p>             Bases: <code>PathDataObject</code></p> <p>Class that models the data required to produce line plots reflecting the daily evolution of the number of rows before and after the syntactic checks, as well as the overall error rate.</p> Source code in <code>multimno/core/data_objects/silver/silver_network_syntactic_quality_warnings_plot_data.py</code> <pre><code>class SilverNetworkSyntacticQualityWarningsLinePlotData(PathDataObject):\n    \"\"\"\n    Class that models the data required to produce line plots reflecting the daily evolution of the number\n    of rows before and after the syntactic checks, as well as the overall error rate.\n    \"\"\"\n\n    ID = \"SilverNetworkSyntacticQualityWarningsLinePlotData\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.date, DateType(), nullable=False),\n            StructField(ColNames.daily_value, FloatType(), nullable=False),\n            StructField(ColNames.average, FloatType(), nullable=False),\n            StructField(ColNames.LCL, FloatType(), nullable=False),\n            StructField(ColNames.UCL, FloatType(), nullable=True),\n            # partition columns\n            StructField(ColNames.variable, StringType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.timestamp, TimestampType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = [ColNames.variable, ColNames.year, ColNames.month, ColNames.day, ColNames.timestamp]\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_network_syntactic_quality_warnings_plot_data/#core.data_objects.silver.silver_network_syntactic_quality_warnings_plot_data.SilverNetworkSyntacticQualityWarningsPiePlotData","title":"<code>SilverNetworkSyntacticQualityWarningsPiePlotData</code>","text":"<p>             Bases: <code>PathDataObject</code></p> <p>Class that models the data required to produce pie plots reflecting the percentage of each type of error for each field of the network topology data object.</p> Source code in <code>multimno/core/data_objects/silver/silver_network_syntactic_quality_warnings_plot_data.py</code> <pre><code>class SilverNetworkSyntacticQualityWarningsPiePlotData(PathDataObject):\n    \"\"\"\n    Class that models the data required to produce pie plots reflecting the percentage of each type of error\n    for each field of the network topology data object.\n    \"\"\"\n\n    ID = \"SilverNetworkSyntacticQualityWarningsPiePlotData\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.type_of_error, StringType(), nullable=False),\n            StructField(ColNames.value, IntegerType(), nullable=False),\n            # partition columns\n            StructField(ColNames.variable, StringType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.timestamp, TimestampType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = [ColNames.variable, ColNames.year, ColNames.month, ColNames.day, ColNames.timestamp]\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_semantic_quality_metrics/","title":"silver_semantic_quality_metrics","text":""},{"location":"reference/core/data_objects/silver/silver_semantic_quality_metrics/#core.data_objects.silver.silver_semantic_quality_metrics.SilverEventSemanticQualityMetrics","title":"<code>SilverEventSemanticQualityMetrics</code>","text":"<p>             Bases: <code>PathDataObject</code></p> Source code in <code>multimno/core/data_objects/silver/silver_semantic_quality_metrics.py</code> <pre><code>class SilverEventSemanticQualityMetrics(PathDataObject):\n    \"\"\" \"\"\"\n\n    ID = \"SilverEventSemanticQualityMetrics\"\n\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.result_timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.variable, StringType(), nullable=False),\n            StructField(ColNames.type_of_error, IntegerType(), nullable=False),\n            StructField(ColNames.value, LongType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        # Always append\n        self.df.write.format(\n            self.interface.FILE_FORMAT,\n        ).partitionBy(partition_columns).mode(\n            \"overwrite\"\n        ).save(path)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_semantic_quality_warnings_log_table/","title":"silver_semantic_quality_warnings_log_table","text":"<p>Silver MNO Network Topology Quality Warnings Log Table Data Object</p>"},{"location":"reference/core/data_objects/silver/silver_semantic_quality_warnings_log_table/#core.data_objects.silver.silver_semantic_quality_warnings_log_table.SilverEventSemanticQualityWarningsLogTable","title":"<code>SilverEventSemanticQualityWarningsLogTable</code>","text":"<p>             Bases: <code>PathDataObject</code></p> <p>Class that models the log table keeping track of the quality warnings that may arise from the semantic checks of the MNO Event data.</p> Source code in <code>multimno/core/data_objects/silver/silver_semantic_quality_warnings_log_table.py</code> <pre><code>class SilverEventSemanticQualityWarningsLogTable(PathDataObject):\n    \"\"\"\n    Class that models the log table keeping track of the quality warnings that may arise from\n    the semantic checks of the MNO Event data.\n    \"\"\"\n\n    ID = \"SilverEventSemanticQualityWarningsLogTable\"\n\n    SCHEMA = StructType(\n        [\n            StructField(\"date\", DateType(), nullable=False),\n            StructField(\"Error 1\", FloatType(), nullable=False),\n            StructField(\"Error 2\", FloatType(), nullable=False),\n            StructField(\"Error 3\", FloatType(), nullable=False),\n            StructField(\"Error 4\", FloatType(), nullable=False),\n            StructField(\"Error 1 upper control limit\", FloatType(), nullable=True),\n            StructField(\"Error 2 upper control limit\", FloatType(), nullable=True),\n            StructField(\"Error 3 upper control limit\", FloatType(), nullable=True),\n            StructField(\"Error 4 upper control limit\", FloatType(), nullable=True),\n            StructField(\"Error 1 display warning\", BooleanType(), nullable=False),\n            StructField(\"Error 2 display warning\", BooleanType(), nullable=False),\n            StructField(\"Error 3 display warning\", BooleanType(), nullable=False),\n            StructField(\"Error 4 display warning\", BooleanType(), nullable=False),\n            StructField(\"execution_id\", TimestampType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        # Always append\n        self.df.write.format(\n            self.interface.FILE_FORMAT,\n        ).partitionBy(partition_columns).mode(\n            \"append\"\n        ).save(path)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_semantic_quality_warnings_plot_data/","title":"silver_semantic_quality_warnings_plot_data","text":"<p>Silver MNO Network Topology Quality Warnings Log Table Data Object</p>"},{"location":"reference/core/data_objects/silver/silver_semantic_quality_warnings_plot_data/#core.data_objects.silver.silver_semantic_quality_warnings_plot_data.SilverEventSemanticQualityWarningsBarPlotData","title":"<code>SilverEventSemanticQualityWarningsBarPlotData</code>","text":"<p>             Bases: <code>PathDataObject</code></p> <p>Class that models the data required to produce line plots reflecting the daily evolution of the number of each type of error, as well as records without errors, as seen during the semantic checks of MNO Event data.</p> Source code in <code>multimno/core/data_objects/silver/silver_semantic_quality_warnings_plot_data.py</code> <pre><code>class SilverEventSemanticQualityWarningsBarPlotData(PathDataObject):\n    \"\"\"\n    Class that models the data required to produce line plots reflecting the daily evolution of the number\n    of each type of error, as well as records without errors, as seen during the semantic checks of MNO Event data.\n    \"\"\"\n\n    ID = \"SilverEventSemanticQualityWarningBarPlotData\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.date, DateType(), nullable=False),\n            StructField(ColNames.type_of_error, StringType(), nullable=False),\n            StructField(ColNames.value, FloatType(), nullable=False),\n            # partition columns\n            StructField(ColNames.variable, StringType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.timestamp, TimestampType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = [ColNames.variable, ColNames.year, ColNames.month, ColNames.day, ColNames.timestamp]\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_signal_strength_data_object/","title":"silver_signal_strength_data_object","text":""},{"location":"reference/core/data_objects/silver/silver_signal_strength_data_object/#core.data_objects.silver.silver_signal_strength_data_object.SilverSignalStrengthDataObject","title":"<code>SilverSignalStrengthDataObject</code>","text":"<p>             Bases: <code>PathDataObject</code></p> Source code in <code>multimno/core/data_objects/silver/silver_signal_strength_data_object.py</code> <pre><code>class SilverSignalStrengthDataObject(PathDataObject):\n    \"\"\" \"\"\"\n\n    ID = \"SilverSignalStrengthDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.cell_id, StringType(), nullable=True),\n            StructField(ColNames.grid_id, StringType(), nullable=True),\n            StructField(ColNames.valid_date_start, DateType(), nullable=True),\n            StructField(ColNames.valid_date_end, DateType(), nullable=True),\n            StructField(ColNames.signal_strength, FloatType(), nullable=True),\n            StructField(ColNames.distance_to_cell, FloatType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=True),\n            StructField(ColNames.month, ByteType(), nullable=True),\n            StructField(ColNames.day, ByteType(), nullable=True),\n        ]\n    )\n\n    MANDATORY_COLUMNS = [\n        ColNames.cell_id,\n        ColNames.grid_id,\n        ColNames.valid_date_start,\n        ColNames.valid_date_end,\n        ColNames.signal_strength,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    ]\n\n    OPTIONAL_COLUMNS = [ColNames.distance_to_cell]\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_time_segments_data_object/","title":"silver_time_segments_data_object","text":""},{"location":"reference/core/data_objects/silver/silver_time_segments_data_object/#core.data_objects.silver.silver_time_segments_data_object.SilverTimeSegmentsDataObject","title":"<code>SilverTimeSegmentsDataObject</code>","text":"<p>             Bases: <code>PathDataObject</code></p> <p>Class that models the cleaned MNO Event data.</p> Source code in <code>multimno/core/data_objects/silver/silver_time_segments_data_object.py</code> <pre><code>class SilverTimeSegmentsDataObject(PathDataObject):\n    \"\"\"\n    Class that models the cleaned MNO Event data.\n    \"\"\"\n\n    ID = \"SilverTimeSegmentsDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.time_segment_id, IntegerType(), nullable=False),\n            StructField(ColNames.start_timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.end_timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.mcc, ShortType(), nullable=False),\n            StructField(ColNames.cells, ArrayType(StringType()), nullable=False),\n            StructField(ColNames.state, StringType(), nullable=False),\n            StructField(ColNames.is_last, BooleanType(), nullable=True),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.user_id_modulo, IntegerType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface: ParquetInterface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n        # Clear path\n        self.first_write = True\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        # If it is the first writing of this data object, clear the input directory, otherwise add\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"}]}