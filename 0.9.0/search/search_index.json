{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MultiMNO","text":"<p>This repository contains code that processes MNO Data to generate population and mobility insights.</p>"},{"location":"#sections","title":"Sections","text":"<ul> <li>User Manual: Guide on how to setup, configure and execute the software.</li> <li>Dev Guide: Guidelines and best practices for contributing to the repository and setting up a development environment.</li> <li>Pipeline: Data Flow diagram of the data processing pipeline.</li> <li>Reference: Code documentation.</li> <li>System Requirements: Mandatory requirements to execute the software.</li> <li>License: Software license - EUROPEAN UNION PUBLIC LICENCE v. 1.2.</li> <li>Test Report: Report of the testing performed with the multimno release.</li> </ul>"},{"location":"LICENSE/","title":"License","text":"<p>EUROPEAN UNION PUBLIC LICENCE v. 1.2  EUPL \u00a9 the European Union 2007, 2016 </p> <p>This European Union Public Licence (the \u2018EUPL\u2019) applies to the Work (as defined below) which is provided under the  terms of this Licence. Any use of the Work, other than as authorised under this Licence is prohibited (to the extent such  use is covered by a right of the copyright holder of the Work).  The Work is provided under the terms of this Licence when the Licensor (as defined below) has placed the following  notice immediately following the copyright notice for the Work:                            Licensed under the EUPL  or has expressed by any other means his willingness to license under the EUPL. </p> <p>1.Definitions  In this Licence, the following terms have the following meaning:  \u2014 \u2018The Licence\u2019:this Licence.  \u2014 \u2018The Original Work\u2019:the work or software distributed or communicated by the Licensor under this Licence, available  as Source Code and also as Executable Code as the case may be.  \u2014 \u2018Derivative Works\u2019:the works or software that could be created by the Licensee, based upon the Original Work or  modifications thereof. This Licence does not define the extent of modification or dependence on the Original Work  required in order to classify a work as a Derivative Work; this extent is determined by copyright law applicable in  the country mentioned in Article 15.  \u2014 \u2018The Work\u2019:the Original Work or its Derivative Works.  \u2014 \u2018The Source Code\u2019:the human-readable form of the Work which is the most convenient for people to study and  modify.  \u2014 \u2018The Executable Code\u2019:any code which has generally been compiled and which is meant to be interpreted by  a computer as a program.  \u2014 \u2018The Licensor\u2019:the natural or legal person that distributes or communicates the Work under the Licence.  \u2014 \u2018Contributor(s)\u2019:any natural or legal person who modifies the Work under the Licence, or otherwise contributes to  the creation of a Derivative Work.  \u2014 \u2018The Licensee\u2019 or \u2018You\u2019:any natural or legal person who makes any usage of the Work under the terms of the  Licence.  \u2014 \u2018Distribution\u2019 or \u2018Communication\u2019:any act of selling, giving, lending, renting, distributing, communicating,  transmitting, or otherwise making available, online or offline, copies of the Work or providing access to its essential  functionalities at the disposal of any other natural or legal person. </p> <p>2.Scope of the rights granted by the Licence  The Licensor hereby grants You a worldwide, royalty-free, non-exclusive, sublicensable licence to do the following, for  the duration of copyright vested in the Original Work:  \u2014 use the Work in any circumstance and for all usage,  \u2014 reproduce the Work,  \u2014 modify the Work, and make Derivative Works based upon the Work,  \u2014 communicate to the public, including the right to make available or display the Work or copies thereof to the public  and perform publicly, as the case may be, the Work,  \u2014 distribute the Work or copies thereof,  \u2014 lend and rent the Work or copies thereof,  \u2014 sublicense rights in the Work or copies thereof.  Those rights can be exercised on any media, supports and formats, whether now known or later invented, as far as the  applicable law permits so.  In the countries where moral rights apply, the Licensor waives his right to exercise his moral right to the extent allowed  by law in order to make effective the licence of the economic rights here above listed.  The Licensor grants to the Licensee royalty-free, non-exclusive usage rights to any patents held by the Licensor, to the  extent necessary to make use of the rights granted on the Work under this Licence. </p> <p>3.Communication of the Source Code  The Licensor may provide the Work either in its Source Code form, or as Executable Code. If the Work is provided as  Executable Code, the Licensor provides in addition a machine-readable copy of the Source Code of the Work along with  each copy of the Work that the Licensor distributes or indicates, in a notice following the copyright notice attached to  the Work, a repository where the Source Code is easily and freely accessible for as long as the Licensor continues to  distribute or communicate the Work. </p> <p>4.Limitations on copyright  Nothing in this Licence is intended to deprive the Licensee of the benefits from any exception or limitation to the  exclusive rights of the rights owners in the Work, of the exhaustion of those rights or of other applicable limitations  thereto. </p> <p>5.Obligations of the Licensee  The grant of the rights mentioned above is subject to some restrictions and obligations imposed on the Licensee. Those  obligations are the following: </p> <p>Attribution right: The Licensee shall keep intact all copyright, patent or trademarks notices and all notices that refer to  the Licence and to the disclaimer of warranties. The Licensee must include a copy of such notices and a copy of the  Licence with every copy of the Work he/she distributes or communicates. The Licensee must cause any Derivative Work  to carry prominent notices stating that the Work has been modified and the date of modification. </p> <p>Copyleft clause: If the Licensee distributes or communicates copies of the Original Works or Derivative Works, this  Distribution or Communication will be done under the terms of this Licence or of a later version of this Licence unless  the Original Work is expressly distributed only under this version of the Licence \u2014 for example by communicating  \u2018EUPL v. 1.2 only\u2019. The Licensee (becoming Licensor) cannot offer or impose any additional terms or conditions on the  Work or Derivative Work that alter or restrict the terms of the Licence. </p> <p>Compatibility clause: If the Licensee Distributes or Communicates Derivative Works or copies thereof based upon both  the Work and another work licensed under a Compatible Licence, this Distribution or Communication can be done  under the terms of this Compatible Licence. For the sake of this clause, \u2018Compatible Licence\u2019 refers to the licences listed  in the appendix attached to this Licence. Should the Licensee's obligations under the Compatible Licence conflict with  his/her obligations under this Licence, the obligations of the Compatible Licence shall prevail. </p> <p>Provision of Source Code: When distributing or communicating copies of the Work, the Licensee will provide  a machine-readable copy of the Source Code or indicate a repository where this Source will be easily and freely available  for as long as the Licensee continues to distribute or communicate the Work.  Legal Protection: This Licence does not grant permission to use the trade names, trademarks, service marks, or names  of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and  reproducing the content of the copyright notice. </p> <p>6.Chain of Authorship  The original Licensor warrants that the copyright in the Original Work granted hereunder is owned by him/her or  licensed to him/her and that he/she has the power and authority to grant the Licence.  Each Contributor warrants that the copyright in the modifications he/she brings to the Work are owned by him/her or  licensed to him/her and that he/she has the power and authority to grant the Licence.  Each time You accept the Licence, the original Licensor and subsequent Contributors grant You a licence to their contributions  to the Work, under the terms of this Licence. </p> <p>7.Disclaimer of Warranty  The Work is a work in progress, which is continuously improved by numerous Contributors. It is not a finished work  and may therefore contain defects or \u2018bugs\u2019 inherent to this type of development.  For the above reason, the Work is provided under the Licence on an \u2018as is\u2019 basis and without warranties of any kind  concerning the Work, including without limitation merchantability, fitness for a particular purpose, absence of defects or  errors, accuracy, non-infringement of intellectual property rights other than copyright as stated in Article 6 of this  Licence.  This disclaimer of warranty is an essential part of the Licence and a condition for the grant of any rights to the Work. </p> <p>8.Disclaimer of Liability  Except in the cases of wilful misconduct or damages directly caused to natural persons, the Licensor will in no event be  liable for any direct or indirect, material or moral, damages of any kind, arising out of the Licence or of the use of the  Work, including without limitation, damages for loss of goodwill, work stoppage, computer failure or malfunction, loss  of data or any commercial damage, even if the Licensor has been advised of the possibility of such damage. However,  the Licensor will be liable under statutory product liability laws as far such laws apply to the Work. </p> <p>9.Additional agreements  While distributing the Work, You may choose to conclude an additional agreement, defining obligations or services  consistent with this Licence. However, if accepting obligations, You may act only on your own behalf and on your sole  responsibility, not on behalf of the original Licensor or any other Contributor, and only if You agree to indemnify,  defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against such Contributor by  the fact You have accepted any warranty or additional liability. </p> <p>10.Acceptance of the Licence  The provisions of this Licence can be accepted by clicking on an icon \u2018I agree\u2019 placed under the bottom of a window  displaying the text of this Licence or by affirming consent in any other similar way, in accordance with the rules of  applicable law. Clicking on that icon indicates your clear and irrevocable acceptance of this Licence and all of its terms  and conditions.  Similarly, you irrevocably accept this Licence and all of its terms and conditions by exercising any rights granted to You  by Article 2 of this Licence, such as the use of the Work, the creation by You of a Derivative Work or the Distribution  or Communication by You of the Work or copies thereof. </p> <p>11.Information to the public  In case of any Distribution or Communication of the Work by means of electronic communication by You (for example,  by offering to download the Work from a remote location) the distribution channel or media (for example, a website)  must at least provide to the public the information requested by the applicable law regarding the Licensor, the Licence  and the way it may be accessible, concluded, stored and reproduced by the Licensee. </p> <p>12.Termination of the Licence  The Licence and the rights granted hereunder will terminate automatically upon any breach by the Licensee of the terms  of the Licence.  Such a termination will not terminate the licences of any person who has received the Work from the Licensee under  the Licence, provided such persons remain in full compliance with the Licence. </p> <p>13.Miscellaneous  Without prejudice of Article 9 above, the Licence represents the complete agreement between the Parties as to the  Work.  If any provision of the Licence is invalid or unenforceable under applicable law, this will not affect the validity or  enforceability of the Licence as a whole. Such provision will be construed or reformed so as necessary to make it valid  and enforceable.  The European Commission may publish other linguistic versions or new versions of this Licence or updated versions of  the Appendix, so far this is required and reasonable, without reducing the scope of the rights granted by the Licence.  New versions of the Licence will be published with a unique version number.  All linguistic versions of this Licence, approved by the European Commission, have identical value. Parties can take  advantage of the linguistic version of their choice. </p> <p>14.Jurisdiction  Without prejudice to specific agreement between parties,  \u2014 any litigation resulting from the interpretation of this License, arising between the European Union institutions,  bodies, offices or agencies, as a Licensor, and any Licensee, will be subject to the jurisdiction of the Court of Justice  of the European Union, as laid down in article 272 of the Treaty on the Functioning of the European Union,  \u2014 any litigation arising between other parties and resulting from the interpretation of this License, will be subject to  the exclusive jurisdiction of the competent court where the Licensor resides or conducts its primary business. </p> <p>15.Applicable Law  Without prejudice to specific agreement between parties,  \u2014 this Licence shall be governed by the law of the European Union Member State where the Licensor has his seat,  resides or has his registered office,  \u2014 this licence shall be governed by Belgian law if the Licensor has no seat, residence or registered office inside  a European Union Member State. </p> <pre><code>                                                     Appendix\n</code></pre> <p>\u2018Compatible Licences\u2019 according to Article 5 EUPL are:  \u2014 GNU General Public License (GPL) v. 2, v. 3  \u2014 GNU Affero General Public License (AGPL) v. 3  \u2014 Open Software License (OSL) v. 2.1, v. 3.0  \u2014 Eclipse Public License (EPL) v. 1.0  \u2014 CeCILL v. 2.0, v. 2.1  \u2014 Mozilla Public Licence (MPL) v. 2  \u2014 GNU Lesser General Public Licence (LGPL) v. 2.1, v. 3  \u2014 Creative Commons Attribution-ShareAlike v. 3.0 Unported (CC BY-SA 3.0) for works other than software  \u2014 European Union Public Licence (EUPL) v. 1.1, v. 1.2  \u2014 Qu\u00e9bec Free and Open-Source Licence \u2014 Reciprocity (LiLiQ-R) or Strong Reciprocity (LiLiQ-R+).</p> <p>The European Commission may update this Appendix to later versions of the above licences without producing  a new version of the EUPL, as long as they provide the rights granted in Article 2 of this Licence and protect the  covered Source Code from exclusive appropriation.  All other changes or additions to this Appendix require the production of a new EUPL version. </p>"},{"location":"pipeline/","title":"Pipeline","text":"<pre><code>%%{\n  init: {\n    'theme': 'base',\n    'themeVariables': {\n      'primaryColor': '#BB2528',\n      'primaryTextColor': '#000000',\n      'primaryBorderColor': '#7C0000',\n      'lineColor': '#F8B229',\n      'secondaryColor': '#006100',\n      'tertiaryColor': '#fff'\n    }\n  }\n}%%\nflowchart TD;\n    %% --- Reference Data ---\n    subgraph Setup-Ingestion\n    %% [SETUP ONCE]\n    %% &lt;Grid Setup&gt;\n\n    %% Inspire grid generation\n    CountriesData[(Countries Data)]--&gt;InspireGridGeneration--&gt;InspireGridData[(Inspire Grid)];\n    %% GridEnrichment\n    InspireGridData--&gt;GridEnrichment--&gt;EnrichedGrid[(EnrichedGrid   )];\n    %% GeozonesGridMapping\n    InspireGridData--&gt;GeozonesGridMapping--&gt;GeozonesGridMap[(GeozonesGridMap)]\n    CountryZoneData[(CountryZone Data)]--&gt;GeozonesGridMapping\n    end\n\n    subgraph Daily\n    subgraph MNO-Data Cleaning\n    %% [DAILY]\n    %% &lt;MNO Data Cleaning&gt;\n    %% RAW Network cleaning\n    PhysicalNetworkRAWData[(MNO-Network   Physical   RAW)]--&gt;NetworkCleaning--&gt;PhysicalNetworkData[(MNO-Network   Physical)];\n    NetworkCleaning--&gt;NetworkQAData[(MNO-Network   Quality Checks)];\n    %% RAW Network QA\n    NetworkQAData--&gt;NetworkQualityWarnings--&gt;NetworkWarnings[(Network   Quality Warnings)];\n    NetworkQualityWarnings--&gt;NetworkReports{{Network   QA    graph data   csv}};\n    %% -- EVENTS --\n    %% RAW Events cleaning\n    EventsRAWData[(MNO-Event   RAW)]--&gt;EventCleaning--&gt;EventsData[(MNO-Event)];\n    EventCleaning--&gt;EventsQA[(MNO-Event   Quality Checks)]--&gt;EventQualityWarnings;\n    EventCleaning--&gt;EventsQAfreq[(MNO-Event   Quality Checks   frequency)];\n    %% RAW Events Warnings\n    EventsQAfreq--&gt;EventQualityWarnings;\n    EventQualityWarnings--&gt;EventsWarnings[(Events   Quality Warnings)];\n    EventQualityWarnings--&gt;EventsReports{{Event QA    graph data   csv}};\n    %% Event Semantic Checks\n    EventsData--&gt;SemanticCleaning--&gt;EventsSemanticCleaned[(Events   Semantic   Cleaned)];\n    PhysicalNetworkData--&gt;SemanticCleaning;\n    SemanticCleaning--&gt;DeviceSemanticQualityMetrics[(Device   Semantic   Quality   Metrics)];\n    %% Event Semantic Warnings\n    EventsSemanticCleaned--&gt;SemanticQualityWarnings--&gt;EventSemanticWarnings[(Event   Semantic   Quality   Warnings)];\n    DeviceSemanticQualityMetrics--&gt;SemanticQualityWarnings--&gt;EventSemanticReports{{Event Semantic QA    graph data   csv}};\n    %% Device activity Statistics\n    EventsData--&gt;DeviceActivityStatistics--&gt;DeviceActivityStatisticsData[(Device   Activity   Statistics)];\n    PhysicalNetworkData--&gt;DeviceActivityStatistics;\n    end\n\n\n    subgraph Network Processing\n    %% [DAILY]\n    %% &lt;Network Coverage&gt;\n    %% Cell Footprint\n    PhysicalNetworkData--&gt;CellFootprintEstimation--&gt;CellFootprintData[(Cell Footprint Values)];\n    %% Cell Connection Probability\n    CellFootprintData--&gt;CellConnectionProbabilityEstimation;\n    InspireGridData--&gt;CellConnectionProbabilityEstimation--&gt;CellConnectionProbabilityData[(Cell Connection Probability)];\n    %% Cell Proximity Estimation\n    CellFootprintData--&gt;CellProximityEstimation--&gt;CellIntersectionsGroupsData[(Cell Intersection Groups)]\n    CellProximityEstimation--&gt;CellDistanceData[(Cell Distance)]\n    end\n\n    subgraph Daily Products\n    %% [DAILY]\n    %% &lt;Present Population Estimation&gt;\n    EventsSemanticCleaned--&gt;PresentPopulation--&gt;PresentPopulationData[(Present   Population)];\n    CellConnectionProbabilityData--&gt;PresentPopulation;\n    InspireGridData--&gt;PresentPopulation;\n\n    %% &lt;Tourism&gt;\n    %% Continuous Time segmentation\n    EventsSemanticCleaned--&gt;ContinuousTimeSegmentation--&gt;DailyCTSdata[(Daily Continuous Time Segmentation)];\n    CellFootprintData--&gt;ContinuousTimeSegmentation;\n    CellIntersectionGroupsData--&gt;ContinuousTimeSegmentation;\n    %% Tourism Stays estimation\n    DailyCTSdata--&gt;TourismStaysEstimation--&gt;TourismStaysData[(TourismStays)];\n    CellConnectionProbabilityData--&gt;TourismStaysEstimation;\n    GeozonesGridMap--&gt;TourismStaysEstimation;\n    end\n    end\n\n    subgraph Daily-1\n    %% [DAILY-1]\n    %% &lt;Usual Environment&gt;\n\n    %% Daily Permanence Score\n    EventsSemanticCleaned--&gt;DailyPermanenceScore--&gt;DPSdata[(Daily   Permanence   Score   Data)];\n    CellFootprintData--&gt;DailyPermanenceScore;\n    end\n\n    %% --- Longitudinal module ---\n    subgraph MidTerm Products\n    %% [MIDTERM]\n    %% &lt;Usual Environment&gt;\n\n    %% Midterm Permanence Score\n    HolidayData[(Holiday   Data)]\n    DPSdata--&gt;MidTermPermanenceScore--&gt;MPSdata[(MidTerm   Permanence   Score   Data)];\n    HolidayData--&gt;MidTermPermanenceScore;\n    end\n\n    subgraph LongTerm Products\n    %% [LONGTERM]\n    %% &lt;Usual Environment&gt;\n\n    %% Longterm Permanence Score\n    MPSdata--&gt;LongTermPermanenceScore--&gt;LPSdata[(LongTerm   Permanence   Score   Data)];\n    %% UE-L\n    LPSdata--&gt;UsualEnvironmentLabelling--&gt;UELdata[(UsualEnvironment   Labelling   Data)];\n    %% UE-A\n    UELdata--&gt;UsualEnvironmentAggregation--&gt;UEAdata[(UsualEnvironment   Aggregation   Data)];\n    InspireGridData--&gt;UsualEnvironmentAggregation;\n    UsualEnvironmentAggregation--&gt;HomeLocationData[(Home Location   Data)]\n\n    %% &lt;Tourism&gt;\n\n    %% Tourism Inbound\n    %% INPUT: &lt;MccIsoTz&gt;\n    MccIsoTzMap[(Mcc-Iso-Tz Mapping)]\n    %% \n    MccIsoTzMap--&gt;TourismStatisticsCalculation--&gt;TourismTripData[(TourismTrip   Data)];\n    TourismStaysData--&gt;TourismStatisticsCalculation;\n    TourismStatisticsCalculation--&gt;TourismZoneDeparturesNightsSpentData[(Tourism   ZoneDepartures   NightsSpent)];\n    TourismStatisticsCalculation--&gt;TourismTripAvgDestinationsNightsSpentData[(Tourism   TripAvgDestinations   NightsSpent)];\n    %% Tourism Outbound\n    DailyCTSdata--&gt;TourismOutboundStatisticsCalculation;\n    MccIsoTzMap--&gt;TourismOutboundStatisticsCalculation--&gt;TourismOutboundTripData[(TourismOutbound   TripData)];\n    TourismOutboundStatisticsCalculation--&gt;TourismOutboundNightsSpentData[(TourismOutbound   NightsSpent)]\n    end\n\n\n    subgraph Multiple Longterm\n    %% [MULTIPLE LONGTERM]\n    %% &lt; Internal Migration &gt;\n    EnrichedGrid--&gt;InternalMigration--&gt;InternalMigrationData[(InternalMigration Data)];\n    GeozonesGridMap--&gt;InternalMigration;\n    UELdata--&gt;InternalMigration;\n    InternalMigration--&gt;InternalMigrationQM[(InternalMigration   QualityMetrics)]\n\n    end\n\n\n    subgraph Final Product Pipeline\n    %% [OUTPUT INDICATORS]\n    %% - Present Population\n    PresentPopulationData--&gt;OutputIndicators--&gt;PresentPopulationGold[(Present   Population   Gold)];\n    %% - Usual Environment\n    UEAdata--&gt;OutputIndicators--&gt;UEADataGold[(UsualEnvironment\\Gold)]\n    %% - Home location\n    HomeLocationData--&gt;OutputIndicators--&gt;HomeLocationDataGold[(Home Location   Gold)]\n    %% Internal Migration\n    InternalMigrationData--&gt;OutputIndicators--&gt;InternalMigrationDataGold[(InternalMigration   Gold)]\n    %% Tourism Inbound\n    TourismZoneDeparturesNightsSpentData--&gt;OutputIndicators--&gt;TourismZoneDeparturesNightsSpentDataGold[(Tourism   ZoneDeparturesn   NightsSpent   Gold)]\n    TourismTripAvgDestinationsNightsSpentData--&gt;OutputIndicators--&gt;TourismTripAvgDestinationsNightsSpentDataGold[(Tourism   TripAvgDestinations   NightsSpent   Gold)]\n    %% Tourism Outbound\n    TourismOutboundNightsSpentData--&gt;OutputIndicators--&gt;TourismOutboundNightsSpentDataGold[(TourismOutbound   NightsSpent   Gold)]\n    end\n\n    classDef green fill:#229954,stroke:#333,stroke-width:2px;\n    classDef light_green fill:#AFE1AF,stroke:#333,stroke-width:1px;\n    classDef bronze fill:#CD7F32,stroke:#333,stroke-width:2px;\n    classDef silver fill:#adadad,stroke:#333,stroke-width:2px;\n    classDef light_silver fill:#dcdcdc,stroke:#333,stroke-width:2px;\n    classDef gold fill:#FFD700,stroke:#333,stroke-width:2px;\n\n    %% ++++++++++++++++ BRONZE ++++++++++++++++\n    class PhysicalNetworkRAWData,EventsRAWData bronze\n    class CountriesData,CountryZoneData bronze\n    class HolidayData,MccIsoTzMap bronze\n\n    %% ++++++++++++++++ SILVER ++++++++++++++++\n    %% --- Reference Data ---\n    class InspireGridData,EnrichedGrid,GeozonesGridMap light_silver\n    %% --- DAILY - CLEANING ---\n    %% -- NETWORK --\n    class PhysicalNetworkData light_silver\n    class NetworkQAData,NetworkWarnings silver\n    class CellFootprintData,CellConnectionProbabilityData,CellIntersectionGroupsData light_silver\n    class CellIntersectionsGroupsData,CellDistanceData light_silver\n    %% -- EVENTS --\n    %% event cleaning\n    class EventsData light_silver\n    class EventsQA,EventsQAfreq,EventsWarnings silver\n\n    %% event deduplicated\n    class EventsDeduplicated light_silver\n    class EventsDeduplicatedQA,EventsDeduplicatedQAfreq,EventsDeduplicatedWarnings silver\n\n    %% device activity statistics\n    class DeviceActivityStatisticsData light_silver\n    %% events semantic clean\n    class EventsSemanticCleaned light_silver\n    class DeviceSemanticQualityMetrics,EventSemanticWarnings silver\n\n    %% --- DAILY - PP ---\n    %% Present population\n    class PresentPopulationData light_silver\n    %% --- DAILY - UE ---\n    %% Daily Permanence Score\n    class DPSdata light_silver\n    %% --- DAILY - TOURISM ---\n    %% Continuous Time segmentation\n    class DailyCTSdata,TourismStaysData light_silver\n\n    %% --- MIDTERM - UE ---\n    %% Midterm Permanence Score\n    class MPSdata light_silver\n\n    %% --- LONGTERM - UE ---\n    %% Longterm Permanence Score\n    class LPSdata light_silver\n    %% UE data\n    class UELdata,UEAdata,HomeLocationData light_silver\n    %% --- LONGTERM - TOURISM ---\n    %% Inbound\n    class TourismTripData,TourismZoneDeparturesNightsSpentData,TourismTripAvgDestinationsNightsSpentData light_silver\n    %% Outbound\n    class TourismOutboundTripData,TourismOutboundNightsSpentData light_silver\n\n    %% --- MULTIPLE LONGTERM - Internal Migration ---\n    class InternalMigrationData light_silver\n\n    %% ++++++++++++++++ GOLD ++++++++++++++++\n\n    %% Reports\n    class NetworkReports gold\n    class EventsDeduplicatedReports gold\n    class EventsReports gold\n    class EventSemanticReports gold\n    class InternalMigrationQM gold\n\n    %% Present Population\n    class PresentPopulationGold gold\n    %% Usual Environment\n    class UEADataGold,HomeLocationDataGold gold\n    %% Tourism - Inbound\n    class TourismZoneDeparturesNightsSpentDataGold,TourismTripAvgDestinationsNightsSpentDataGold gold\n    %% Tourism - Outbound\n    class TourismOutboundNightsSpentDataGold gold\n    %% Internal migration\n    class InternalMigrationDataGold gold\n\n    %% ++++++++++++++++ Components ++++++++++++++++\n    %% [SETUP ONCE]\n    class InspireGridGeneration,GridEnrichment,GeozonesGridMapping light_green\n\n    %% [DAILY]\n    %% &lt;MNO Data Cleaning&gt;\n    class NetworkCleaning light_green\n    class NetworkQualityWarnings green\n    %% Events\n    class EventCleaning,EventDeduplication,SemanticCleaning light_green\n    class EventQualityWarnings,EventQualityWarnings2,SemanticQualityWarnings green\n    %% -&gt; Device Activity Statistics should start from semantic cleaned events\n    class DeviceActivityStatistics light_green\n    %% &lt;Network Coverage&gt;\n    class CellFootprintEstimation,CellConnectionProbabilityEstimation,CellProximityEstimation light_green\n    %% &lt;Present Population Estimation&gt;\n    class PresentPopulation light_green\n    %% &lt;Tourism&gt;\n    class ContinuousTimeSegmentation,TourismStaysEstimation light_green\n\n    %% [DAILY-1]\n    %% &lt;Usual Environment&gt;\n    class DailyPermanenceScore light_green\n\n    %% [MIDTERM]\n    %% &lt;Usual Environment&gt;\n    class MidTermPermanenceScore light_green\n\n    %% [LONGTERM]\n    %% &lt;Usual Environment&gt;\n    class LongTermPermanenceScore,UsualEnvironmentLabelling,UsualEnvironmentAggregation light_green\n    %% &lt;Tourism&gt;\n    %% Tourism Inbound\n    class TourismStatisticsCalculation light_green\n    %% Tourism Outbound\n    class TourismOutboundStatisticsCalculation light_green\n\n    %% [MULTIPLE LONGTERM]\n    %% &lt; Internal Migration &gt;\n    class InternalMigration light_green\n\n    %% [OUTPUT INDICATORS]\n    class OutputIndicators light_green\n</code></pre>"},{"location":"system_requirements/","title":"System Requirements","text":"<p>Multimno is a python library which requires the installation of additional system &amp; python libraries. In this section the requirements for executing this software are defined. </p> <p>In the case of using the docker image provided for local testing, the system only needs to comply with the Hardware Requirements and Docker requirements as the docker image will have all the software requirements already installed.</p>"},{"location":"system_requirements/#hardware-requirements","title":"Hardware requirements","text":""},{"location":"system_requirements/#minimum-requirements","title":"Minimum requirements","text":"<p>The hardware specification needed will vary depending on the input data volumetry. However, we recommend this settings as minimum requirements for a single node cluster:</p> <ul> <li>Cores: 4</li> <li>RAM: 16 Gb</li> <li>Disk: 32 Gb of free space</li> <li>OS: <ul> <li>Linux:<ul> <li>Debian 12+<ul> <li>Ubuntu 22.04 (Recommended)</li> </ul> </li> <li>Fedora 18+<ul> <li>RHEL 7</li> <li>Amazon Linux 2023</li> </ul> </li> </ul> </li> <li>Mac: 12.6+</li> <li>Windows: 11 &amp; WSL2 with Ubuntu 22.04 </li> </ul> </li> </ul>"},{"location":"system_requirements/#software-requirements","title":"Software Requirements","text":""},{"location":"system_requirements/#os-libraries","title":"OS Libraries","text":"Library Version Python &gt;= 3.9 Java JDK &gt;= 11 Apache Spark 3.5.1 GDAL &gt;= 3.6.2"},{"location":"system_requirements/#spark-libraries-jars","title":"Spark Libraries (jars)","text":"Library Version Apache Sedona &gt;= 1.6.0 Geotools wrapper 28.2"},{"location":"system_requirements/#python-libraries","title":"Python Libraries","text":"Library Version numpy &gt;=1.26,&lt;1.27 pandas &gt;=2.0,&lt;2.3 pyarrow 17.0 requests 2.31.0 toml 0.10 apache-sedona &gt;= 1.6.0 geopandas 1.0 shapely 2.0 pyspark 3.5.1 py4j 0.10.9.7"},{"location":"system_requirements/#docker-requirements","title":"Docker requirements","text":"<p>In the case of using the docker image provided for single node execution the following requirements must be fulfilled:   - Docker-engine: &gt;=25.X   - Docker-compose: &gt;=2.24.X   - Internet connection to Ubuntu/Spark/Docker official repositories for building the docker image  </p>"},{"location":"DevGuide/","title":"Developer Guide","text":"<p>The developer guide contains two sections: - Contribute: Guidelines on how to contribute to the software. - Developer Guidelines: Guidelines and tips on how to develop and test the software.  </p>"},{"location":"DevGuide/1_contribute/","title":"Contribute","text":"<p>In this document the general rules and guidelines for contributing to the multimno repository are detailed.</p>"},{"location":"DevGuide/1_contribute/#source-control-strategy","title":"Source control strategy","text":"<p>This repository uses three principal branches for source control:</p> <ul> <li>main: Branch where the official releases are tagged. The HEAD of the branch corresponds to the    latest release of the software.  </li> <li>integration: Branch used for preproduction testing and validation from which a release to the main branch will be generated.</li> <li>development: Branch that centralizes the latest features developed in the repository. After enough features/bugs have been delivered to this branch, a snapshot will be created in the integration branch for testing before  generating a release.</li> </ul> <p>These three branches shall only accept changes by the repository administrator. Commits shall not be performed directly in these branches except for small hotfixes in the integration branch.</p> <p>All features and bug fixes will be developed in branches that origin from the development branch.</p>"},{"location":"DevGuide/1_contribute/#forking-the-repository","title":"Forking the repository","text":"<p>Developers that want to contribute to the multimno repository shall fork the repository with all its branches. This can  be done through the github website. After creating a fork of the repository, developers can clone the forked repo in  their computers.</p>"},{"location":"DevGuide/1_contribute/#create-an-issue-with-the-development-that-will-be-performed","title":"Create an Issue with the development that will be performed","text":"<p>First of all, Check if the issue you will develop already exists.  Then, create an issue in the multimno repository stating the objective of the development that will be performed. Templates for creating issues for features or fixes are provided in the repository.</p>"},{"location":"DevGuide/1_contribute/#creating-a-featurefix-branch","title":"Creating a feature/fix branch","text":"<p>Within the forked repository developers shall create a branch that originates from the development branch.  This branch shall have the following naming convention:</p> <ul> <li>feat_\\&lt;name&gt;: If it is a new feature.</li> <li>fix_\\&lt;name&gt;: If it is a bug solution.</li> </ul> <p>Please remember to keep the forked development branch up-to-date with the latest changes.</p> <p>Don't forget to look up the developer guidelines to check the code style, testing and development practices that shall be followed to develop new code for the multimno repository.</p>"},{"location":"DevGuide/1_contribute/#opening-a-pull-request","title":"Opening a pull request","text":"<p>Before opening a pull request in the multimno repository verify: * Latest development changes are merged into the branch that performs the pull request.  * All tests pass successfully. * All the new code is documented following the Google style docstrings. * New tests for the code developed are included and pass successfully.</p> <p>Use the github web to create a pull request. The pull request must deliver your developed branch to the development branch of the multimno repository. Associate the PR(pull request) to the previously created issue.</p>"},{"location":"DevGuide/1_contribute/#the-review-process-pull-request-closure","title":"The review process &amp; pull request closure","text":"<p>The repository administrators will review the pull request performed to the development branch. </p> <ul> <li>If the changes are accepted, they will be incorporated in the development branch and the pull request will be closed. </li> <li>If the changes are not accepted, the repository administrators may indicate as a comment in the pull request feedback  and modifications needed to accept the pull request. However, the pull request may be desestimated to which it  will be closed and changes will not be incorporated.</li> </ul>"},{"location":"DevGuide/2_dev_guidelines/","title":"Developer Guidelines","text":"<p>The repository contains a devcontainer configuration compatible with VsCode. This configuration will create a docker container with all the necessary libraries and configurations to develop and execute the source code. </p>"},{"location":"DevGuide/2_dev_guidelines/#development-environment-setup","title":"\ud83d\udee0\ufe0f Development Environment Setup","text":"<p>This repository provides a docker dev-container with a system completely configured to execute the source code as well as useful libraries and tools for the development of the multimno software.  Thanks to the dev-container, it is guaranteed that all developers are developing/executing code in the same environment.</p> <p>The dev-container specification is all stored inside the <code>.devcontainer</code> directory.</p>"},{"location":"DevGuide/2_dev_guidelines/#configuring-the-docker-container","title":"Configuring the docker container","text":"<p>Configuration parameters for building the docker image and for creating the docker container are specified in the configuration file <code>.devcontainer/.env</code> file. This file contains user specific container configuration (like the path in the host machine to the data). As this file will change  for each developer it is ignored for the git version control and must be created after cloning the repository.</p> <p>A <code>template file</code> is stored in this repostiory. You can use this file as a baseline copying it to the <code>.devcontainer</code> directory.</p> <pre><code>cp resources/templates/dev_container_template.env .devcontainer/.env\n</code></pre> <p>Please edit the <code>.devcontainer/.env</code> file <code>Docker run parameters</code> section  with your preferences:</p> <pre><code># ------------------- Docker Build parameters -------------------\nSPARK_VERSION=3.5.1 # Spark/Pyspark version.\nSCALA_VERSION=2.12 # Spark dependency.\nSEDONA_VERSION=1.6.1 # Sedona\nGEOTOOLS_WRAPPER_VERSION=28.2 # Sedona dependency\n\n# ------------------- Docker run parameters -------------------\nCONTAINER_NAME=multimno_dev_container # Container name.\nDATA_DIR=../sample_data # Path of the host machine to the data to be used within the container.\nSPARK_LOGS_DIR=../sample_data/logs # Path of the host machine to where the spark logs will be stored.\nJL_PORT=8888 # Port of the host machine to deploy a jupyterlab.\nSUI_PORT=4040 # Port for the Spark UI.\nSHS_PORT=18080 # Port for the Spark History Server.\nCONTAINER_CPU=4 # CPU cores of the container.\nCONTAINER_MEM=24g # RAM of the container.\n</code></pre>"},{"location":"DevGuide/2_dev_guidelines/#starting-the-dev-environment","title":"Starting the dev environment","text":""},{"location":"DevGuide/2_dev_guidelines/#vscode","title":"VsCode","text":"<p>Prerequisite: Dev-Container/Docker extension</p> <p>In VsCode: F1 -&gt; Dev Containers: Rebuild and Reopen in container</p>"},{"location":"DevGuide/2_dev_guidelines/#manual","title":"Manual","text":""},{"location":"DevGuide/2_dev_guidelines/#docker-image-creation","title":"Docker image creation","text":"<p>Execute the following command: <pre><code>docker compose -f .devcontainer/docker-compose.yml --env-file=.devcontainer/.env build\n</code></pre></p>"},{"location":"DevGuide/2_dev_guidelines/#docker-container-creation","title":"Docker container creation","text":"<p>Create a container and start a shell session in it with the commands: <pre><code>docker compose -f .devcontainer/docker-compose.yml --env-file=.devcontainer/.env up -d\ndocker exec -it multimno_dev_container bash\n</code></pre></p>"},{"location":"DevGuide/2_dev_guidelines/#stopping-the-dev-environment","title":"Stopping the dev environment","text":""},{"location":"DevGuide/2_dev_guidelines/#vscode_1","title":"VsCode","text":"<p>Closing VsCode will automatically stop the devcontainer.</p>"},{"location":"DevGuide/2_dev_guidelines/#manual_1","title":"Manual","text":"<p>Exit the terminal with:</p> <p>Ctrl+D or writing <code>exit</code></p>"},{"location":"DevGuide/2_dev_guidelines/#deleting-the-dev-environment","title":"Deleting the dev environment","text":"<p>Delete the container created with: <pre><code>docker compose -f .devcontainer/docker-compose.yml --env-file=.devcontainer/.env down\n</code></pre></p>"},{"location":"DevGuide/2_dev_guidelines/#execution","title":"\ud83d\udc0e Execution","text":""},{"location":"DevGuide/2_dev_guidelines/#launching-a-single-component","title":"Launching a single component","text":"<p>In a terminal execute the command:  </p> <pre><code>spark-submit multimno/main_multimno.py &lt;component_id&gt; &lt;path_to_general_config&gt; &lt;path_to_component_config&gt; \n</code></pre> <p>Example:  </p> <pre><code>spark-submit multimno/main_multimno.py SyntheticEvents pipe_configs/configurations/general_config.ini pipe_configs/configurations/synthetic_events/synth_config.ini \n</code></pre>"},{"location":"DevGuide/2_dev_guidelines/#launching-a-pipeline","title":"Launching a pipeline","text":"<p>In a terminal execute the command:  </p> <pre><code>python multimno/orchestrator_multimno.py &lt;pipeline_json_path&gt;\n</code></pre> <p>Example:  </p> <pre><code>python multimno/orchestrator_multimno.py pipe_configs/pipelines/pipeline.json \n</code></pre>"},{"location":"DevGuide/2_dev_guidelines/#monitoringdebug","title":"\ud83d\udd0d Monitoring/Debug","text":""},{"location":"DevGuide/2_dev_guidelines/#launching-a-spark-history-server","title":"Launching a spark history server","text":"<p>The history server will access SparkUI logs stored at the path ${SPARK_LOGS_DIR} defined in the <code>.devcontainer/.env</code> file.</p> <p>Starting the history server <pre><code>start-history-server.sh \n</code></pre> Accesing the history server * Go to the address http://localhost:18080</p>"},{"location":"DevGuide/2_dev_guidelines/#style","title":"\ud83e\udeb6 Style","text":""},{"location":"DevGuide/2_dev_guidelines/#coding-style","title":"Coding style","text":"<p>The code shall follow the standard PEP 8 which is the coding style proposed for writing clean, readable, and maintainable Python code.  It was created to promote consistency in Python code and make it easier for developers to collaborate on projects. </p> <p>PEP 8 official guide</p>"},{"location":"DevGuide/2_dev_guidelines/#docstring-style","title":"Docstring style","text":"<p>The docstrings written in the code shall follow the Google Docstrings style. Adhering  to a unique docstring style  guarantees consistency within software development in a project. Google Docstrings are the most popular convention for  docstrings which facilitates readability and collaboration in open-source projects. </p> <p>Google Docstrings official guide</p>"},{"location":"DevGuide/2_dev_guidelines/#code-cleaning","title":"\ud83e\uddfc Code cleaning","text":"<p>Developed code shall be formatted and jupyter notebooks shall be cleaned of outputs to guarantee consistency and reduce  unnecessary differences between commits.</p>"},{"location":"DevGuide/2_dev_guidelines/#pre-commit-git-hook","title":"Pre-Commit Git Hook","text":"<p>The repository includes a pre-commit Git hook located at <code>resources/hooks/pre-commit</code>. This hook is designed to be executed automatically before any commit to the repository. It performs code linting and Jupyter notebook cleaning as defined in the subsequent sections.</p>"},{"location":"DevGuide/2_dev_guidelines/#automatic-setup","title":"Automatic Setup","text":"<p>The pre-commit hook is automatically configured when launching the devcontainer through Visual Studio Code.</p>"},{"location":"DevGuide/2_dev_guidelines/#manual-setup","title":"Manual Setup","text":"<p>To manually set up the pre-commit hook, execute the following command:</p> <pre><code>./resources/hooks/set_git_hooks.sh\n</code></pre> <p>This ensures that the pre-commit hook is properly installed and executable, thereby maintaining code quality and consistency across the repository.</p>"},{"location":"DevGuide/2_dev_guidelines/#code-linting","title":"Code Linting","text":"<p>The python code generated shall be formatted with black. For formatting all source code execute the following command:</p> <pre><code>black -l 120 multimno tests/test_code/\n</code></pre>"},{"location":"DevGuide/2_dev_guidelines/#clean-jupyter-notebooks","title":"Clean jupyter notebooks","text":"<pre><code>find ./notebooks/ -type f -name \\*.ipynb | xargs jupyter nbconvert --clear-output --inplace\n</code></pre>"},{"location":"DevGuide/2_dev_guidelines/#code-security-scan","title":"Code Security Scan","text":"<pre><code>bandit -r multimno\n</code></pre>"},{"location":"DevGuide/2_dev_guidelines/#testing","title":"\ud83e\uddea Testing","text":""},{"location":"DevGuide/2_dev_guidelines/#launch-tests","title":"Launch Tests","text":""},{"location":"DevGuide/2_dev_guidelines/#manual_2","title":"Manual","text":"<pre><code>pytest tests/test_code/multimno\n</code></pre>"},{"location":"DevGuide/2_dev_guidelines/#vscode_2","title":"VsCode","text":"<p>1) Open the test view at the left panel.  2) Launch tests.</p>"},{"location":"DevGuide/2_dev_guidelines/#generate-test-coverage-reports","title":"Generate test &amp; coverage reports","text":"<p>Launch the command</p> <pre><code>pytest --cov-config=tests/.coveragerc \\\n    --cov-report=\"html:docs/autodoc/coverage\" \\\n    --cov=multimno --html=docs/autodoc/test_report.md \\\n    --self-contained-html tests/test_code/multimno\n</code></pre> <p>Test reports will be stored in the dir: <code>docs/autodoc</code></p>"},{"location":"DevGuide/2_dev_guidelines/#see-coverage-in-ide-vscode-extension","title":"See coverage in IDE (VsCode extension)","text":"<p>1) Launch tests with coverage to generate the coverage report (xml) <pre><code>pytest --cov-report=\"xml\" --cov=multimno tests/test_code/multimno\n</code></pre> 1) Install the extension: Coverage Gutters 2) Right click and select Coverage Gutters: Watch</p> <p>Note: You can see the coverage percentage at the bottom bar</p>"},{"location":"DevGuide/2_dev_guidelines/#code-documentation","title":"\ud83d\udcc4 Code Documentation","text":""},{"location":"DevGuide/2_dev_guidelines/#documentation-server-debugmkdocs","title":"Documentation server Debug(Mkdocs)","text":"<p>A code documentation can be deployed using mkdocs backend. </p> <p>1) Create documentation (This will launch all tests) <pre><code>./resources/scripts/generate_docs.sh\n</code></pre> 2) Launch doc server</p> <p><pre><code>mkdocs serve\n</code></pre> and navigate to the address: http://127.0.0.1:8000</p>"},{"location":"DevGuide/2_dev_guidelines/#documentation-deploy-mike","title":"Documentation deploy (mike)","text":"<p>Set <code>latest</code> as default version <pre><code>mike set-default --push latest\n</code></pre></p> <p>Deploy a version of the documentation with:</p> <pre><code>mike deploy --push --update-aliases &lt;version&gt; latest\n</code></pre> <p>Example:</p> <pre><code>mike deploy --push --update-aliases 0.2 latest\n</code></pre>"},{"location":"DevGuide/2_dev_guidelines/#licensing","title":"\ud83d\udcdc Licensing","text":""},{"location":"DevGuide/2_dev_guidelines/#generating-license-data","title":"Generating License Data","text":"<p>Licensing data can be regenerated using the development container (devcontainer) and the scripts located in the <code>resources/scripts/license/</code> directory. These scripts include:</p> <ul> <li><code>generate_license.sh</code> \u2013 A Bash script that invokes CycloneDX to generate the SBOM.</li> <li><code>generate_concise_license_data.py</code> \u2013 A Python script that parses the SBOM and produces the concise licensing data. This script is automatically executed by <code>generate_license.sh</code>.</li> </ul>"},{"location":"DevGuide/2_dev_guidelines/#usage","title":"Usage","text":"<p>To generate the licensing data, execute the following commands within the devcontainer terminal:</p> <pre><code>./resources/scripts/license/generate_license.sh\n</code></pre> <p>This process will update the licensing files accordingly.</p>"},{"location":"UserManual/","title":"User Manual","text":"<p>This document presents the user manual of the multimno software. Three sections are included: - Configuration: Section explaining the configuration values for all the components of the pipeline. - Setup: Section explaining how to install and deploy the software. - Execution: Section explaining how to execute the software.  </p>"},{"location":"UserManual/cloud_usage/","title":"Cloud Deployment &amp; Execution","text":""},{"location":"UserManual/cloud_usage/#introduction","title":"Introduction","text":"<p>In production environments, Multimno software will typically be deployed within a cloud infrastructure hosted on the premises of a Mobile Network Operator (MNO). This document outlines the standard workflow for provisioning the necessary cloud infrastructure, leveraging cloud-managed services for Big Data processing.</p> <p>The document begins with a high-level overview of cloud-based deployments, followed by detailed, practical examples of implementation on AWS and GCP.</p>"},{"location":"UserManual/cloud_usage/#multimno-cloud-usage-requirements","title":"Multimno Cloud Usage Requirements","text":""},{"location":"UserManual/cloud_usage/#overview","title":"Overview","text":"<p>Multimno requires two primary components to operate effectively in a cloud environment:</p> <ul> <li> <p>Centralized Storage </p> <ul> <li>Description: Centralized storage is a cloud-based storage solution that allows for the storage and retrieval of large amounts of data. It provides a scalable, durable, and highly available storage infrastructure. Centralized storage solutions typically support various data formats and access protocols, making them suitable for diverse data storage needs.</li> <li>Requirements:<ul> <li>Network connectivity: Ensure that the compute engine cluster has network access to the centralized storage.</li> <li>Centralized storage connectivity: Properly configure access permissions and credentials to allow seamless interaction between the compute engine cluster and the centralized storage.</li> </ul> </li> <li>Recommended: Cloud blob storage solutions such as Amazon S3 or Google Cloud Storage (GCS).</li> </ul> </li> <li> <p>Compute Engine Cluster</p> <ul> <li>Description: A compute engine cluster is a collection of virtual machines or instances that work together to process large datasets using distributed computing frameworks like Hadoop, Spark, and others. These clusters could be managed by cloud providers, offering ease of setup, scaling, and maintenance.</li> <li>Requirements:<ul> <li>Network connectivity: Ensure that the compute engine cluster can communicate with other necessary services, including centralized storage and other relevant cloud services if needed(Google Big Query, Amazon Redshift).</li> <li>Centralized storage connectivity: The compute engine cluster must be able to read from and write to the centralized storage.</li> <li>Hadoop ecosystem installed: The cluster should have the necessary Hadoop ecosystem components installed and configured, such as HDFS, YARN, Spark, and other relevant tools.</li> </ul> </li> <li>Recommended: Cloud-managed Hadoop ecosystem services such as Amazon EMR or Google Cloud Dataproc.  </li> </ul> </li> </ul>"},{"location":"UserManual/cloud_usage/#additional-environments","title":"Additional Environments","text":"<p>Multimno is versatile and can be deployed in various Hadoop ecosystem environments, including but not limited to: - Kubernetes clusters - Serverless services - On-premise setups  </p> <p>However, this documentation will focus exclusively on the recommended cloud-managed Hadoop services, such as Amazon EMR and Google Cloud Dataproc. Deployments in Kubernetes clusters, serverless services, and on-premise setups will not be covered in this document.</p>"},{"location":"UserManual/cloud_usage/#cloud-deployment","title":"Cloud Deployment","text":"<p>Multimno cloud deployment is based on the following steps:</p> <p>1) Compile code and dependencies 2) Configuration setup 3) Input data setup 4) Bootstrapping 5) Execution  </p>"},{"location":"UserManual/cloud_usage/#1-compile-code-and-dependencies","title":"1. Compile Code and Dependencies","text":"<p>To execute Multimno, the code and its dependencies need to be deployed to the cluster.</p> <ul> <li> <p>Compile Code to a <code>.whl</code> File:  </p> <ul> <li>Use the <code>./deployment/generate_deployment_package.sh</code> script to compile the code.</li> <li>This script will generate a <code>.whl</code> file containing all the code and dependencies.</li> <li>To execute the compilation, run:     <pre><code>./deployment/generate_deployment_package.sh\n</code></pre></li> <li>The compiled code in <code>.whl</code> format and the application entry points will be located in the newly created <code>upload</code> directory.</li> </ul> </li> <li> <p>Upload <code>.whl</code> File to Cloud Storage:</p> <ul> <li>Upload the generated <code>.whl</code> file to the cloud storage (e.g., Amazon S3 or Google Cloud Storage).</li> </ul> </li> <li> <p>Handle Dependencies Without Internet Connection:</p> <ul> <li>If the cluster does not have internet access, manually download dependencies and upload them to the cloud storage.</li> <li>This includes Apache Sedona jars and Python dependencies.</li> <li>Use the <code>./deployment/generate_deployment_package.sh</code> script with the <code>-j</code> (sedona jars) and <code>-d</code> (python dependencies) flags to download these files along with the compilation:     <pre><code>./deployment/generate_deployment_package.sh -j -d\n</code></pre></li> <li>The compiled code and jars will be located in the newly created <code>upload</code> directory. Upload both of them to the cloud storage.</li> </ul> </li> </ul>"},{"location":"UserManual/cloud_usage/#2-configuration-setup","title":"2. Configuration Setup","text":"<p>Prepare the configuration files for the desired execution environment. Usually, you will need to change the values at the <code>[General]</code>  section (country of study properties) and the value <code>home_dir</code> at the <code>[Paths]</code> section. </p> <p>Then for a concrete execution, edit the study dates of each of the components that will be launched.</p> <p>Remember to read the configuration guide before changing any settings to ensure proper configuration and avoid potential issues.</p>"},{"location":"UserManual/cloud_usage/#3-input-data-setup","title":"3. Input data setup","text":"<p>Follow the guidelines on the execution guide for setting the input data and upload it to the cloud storage.</p>"},{"location":"UserManual/cloud_usage/#4-bootstrapping","title":"4. Bootstrapping","text":"<p>The bootstrapping process involves setting up the necessary environment on the compute engine cluster to ensure that Multimno can execute correctly. This includes installing the code, configuring dependencies, and setting up the runtime environment. The following steps outline the detailed bootstrapping procedure:</p>"},{"location":"UserManual/cloud_usage/#prerequisites","title":"Prerequisites","text":"<ul> <li>Ensure that all necessary files, including the compiled <code>.whl</code> file and any additional dependencies, are uploaded to the cloud storage.</li> </ul>"},{"location":"UserManual/cloud_usage/#steps","title":"Steps","text":"<ol> <li> <p>Generate a Virtual Environment (venv):</p> <ul> <li>Create a virtual environment to isolate the Multimno dependencies from the system-wide packages.</li> <li>Execute the following command to create a virtual environment:     <pre><code>python3 -m venv /path/to/venv --copies\n</code></pre></li> <li>Activate the virtual environment:     <pre><code>source /path/to/venv/bin/activate\n</code></pre></li> </ul> </li> <li> <p>Install the Code and Dependencies in the Virtual Environment:</p> <ul> <li>Install the compiled <code>.whl</code> file and its dependencies within the virtual environment.</li> <li>If the cluster has internet access, use the following command to install the <code>.whl</code> file and dependencies:     <pre><code>pip install /path/to/compiled_package.whl\n</code></pre></li> <li>If the cluster does not have internet access, manually install the dependencies:<ul> <li>Download the required dependencies and upload them to the cloud storage.</li> <li>Use the following command to install the dependencies from the local files:     <pre><code>pip install /path/to/additional_dependencies_dir/*\n</code></pre></li> </ul> </li> </ul> </li> <li> <p>Copy Apache Sedona JAR Files to <code>$SPARK_HOME/jars</code>:</p> <ul> <li>If the cluster does not have internet access, manually copy the Apache Sedona JAR files to the <code>$SPARK_HOME/jars</code> directory.</li> <li>Ensure that the JAR files are uploaded to the cloud storage.</li> <li>Use the following command to copy the JAR files to the appropriate directory:     <pre><code>cp /path/to/sedona_jars/*.jar $SPARK_HOME/jars/\n</code></pre></li> </ul> </li> </ol>"},{"location":"UserManual/cloud_usage/#5-execution","title":"5. Execution","text":""},{"location":"UserManual/cloud_usage/#software-execution-in-the-cloud","title":"Software Execution in the Cloud","text":"<p>The execution phase involves running the Multimno pipeline, which is composed of multiple isolated components. These components are orchestrated by a Python script named <code>orchestrator_multimno.py</code>. This script is responsible for executing <code>spark-submit</code> commands for each component based on the configuration specified in a <code>pipeline.json</code> file. In a cloud environment, this script is initiated from the master node and performs <code>spark-submit</code> commands with the <code>master=yarn</code> setting, which is typically the default configuration in cloud-managed Hadoop ecosystems.</p>"},{"location":"UserManual/cloud_usage/#steps-for-execution","title":"Steps for Execution","text":"<ol> <li> <p>Establish the Virtual Environment for Spark-Submit:</p> <ul> <li>Utilize the virtual environment created during the bootstrapping process.</li> <li>Configure the <code>spark-submit</code> commands to use this virtual environment by setting the appropriate flags in the <code>pipeline.json</code> file.</li> <li>The following <code>spark-submit</code> arguments should be included in the <code>pipeline.json</code> to ensure the correct Python environment is used:     <pre><code>{\n    \"spark_submit_args\": [\n        \"--conf=spark.pyspark.python=/opt/venv/multimno/bin/python3\",\n        \"--conf=spark.yarn.appMasterEnv.PYSPARK_PYTHON=/opt/venv/multimno/bin/python3\",\n        \"--conf=spark.yarn.appMasterEnv.PYSPARK_DRIVER_PYTHON=/opt/venv/multimno/bin/python3\",\n        \"--conf=spark.executorEnv.PYSPARK_PYTHON=/opt/venv/multimno/bin/python3\"\n    ]\n}\n</code></pre></li> </ul> </li> <li> <p>Execute the Orchestrator Script:</p> <ul> <li>Launch the <code>orchestrator_multimno.py</code> script using the virtual environment to ensure all dependencies are correctly resolved.</li> <li>Execute the script with the following command:     <pre><code>/opt/venv/multimno/bin/python3 /opt/workdir/multimno/orchestrator_multimno.py configurations/pipeline.json\n</code></pre></li> </ul> </li> </ol>"},{"location":"UserManual/cloud_usage/#cloud-examples","title":"Cloud Examples","text":"<p>In this section, we provide detailed examples of bootstrapping and execution for the recommended cloud-managed Hadoop services. These examples are intended to guide users through the process of setting up and running the Multimno pipeline on Amazon EMR and Google Dataproc.</p>"},{"location":"UserManual/cloud_usage/#amazon-emr","title":"Amazon EMR","text":"<p>The directory <code>deployment/aws_emr_example</code> contains comprehensive examples of bootstrap and execution scripts specifically tailored for AWS EMR version 7.2.0.</p>"},{"location":"UserManual/cloud_usage/#bootstrap","title":"Bootstrap","text":"<p>The file <code>deployment/aws_emr_example/bootstrap_emr_720.sh</code> serves as an exemplary bootstrap script for environments with internet connectivity. This script performs the following actions:</p> <ol> <li> <p>Download the Code:</p> <ul> <li>The script retrieves the necessary code from the specified cloud storage or repository.</li> </ul> </li> <li> <p>Setup Sedona JARs:</p> <ul> <li>It configures the Apache Sedona JAR files required for spatial data processing by copying them to the appropriate directory.</li> </ul> </li> <li> <p>Python Environment Setup:</p> <ul> <li>The script creates a Python virtual environment to isolate the dependencies.</li> <li>It installs the Multimno code along with all required dependencies within this virtual environment.</li> </ul> </li> </ol> <p>This bootstrap script ensures that the AWS EMR cluster is properly configured with all necessary components to run the Multimno pipeline.</p>"},{"location":"UserManual/cloud_usage/#execution","title":"Execution","text":"<p>The script <code>deployment/aws_emr_example/launch.sh</code> provides an example of how to execute the Multimno pipeline using the <code>orchestrator_multimno.py</code> script. In the context of AWS EMR, this script can be executed as an EMR Step. The following points outline the execution process:</p> <ol> <li> <p>EMR Step Configuration:</p> <ul> <li>Since the execution involves running an orchestrator script rather than a direct <code>spark-submit</code> command, the step must be configured to launch a bash command.</li> <li>This is achieved using the <code>command-runner.jar</code>, which allows the execution of arbitrary shell commands on the EMR cluster.</li> </ul> </li> <li> <p>Launching the Script:</p> <ul> <li>The <code>command-runner.jar</code> is used to execute the <code>launch.sh</code> script, which in turn runs the <code>orchestrator_multimno.py</code> script with the specified configuration file (<code>pipeline.json</code>).</li> <li>Example:     <pre><code>aws emr add-steps --cluster-id $cluster_id --steps Type=CUSTOM_JAR,Name=\"RunMultimnoOrchestrator\",ActionOnFailure=CONTINUE,Jar=\"command-runner.jar\",Args=[\"bash\",\"/opt/workdir/code/launch.sh\", \"$pipeline_path\"]\n</code></pre></li> </ul> </li> </ol>"},{"location":"UserManual/cloud_usage/#google-dataproc","title":"Google Dataproc","text":"<p>The directory <code>deployment/gcp_dataproc_example</code> contains comprehensive examples of bootstrap and execution scripts specifically tailored for Google Dataproc.</p>"},{"location":"UserManual/cloud_usage/#bootstrap_1","title":"Bootstrap","text":"<p>The file <code>deployment/gcp_dataproc_example/bootstrap_dataproc_22.sh</code> serves as an exemplary bootstrap script for environments with no internet connectivity. This script performs the following actions:</p> <ol> <li> <p>Download the Code:</p> <ul> <li>The script retrieves the necessary code from the specified cloud storage or repository.</li> </ul> </li> <li> <p>Setup Sedona JARs:</p> <ul> <li>It configures the Apache Sedona JAR files required for spatial data processing by copying them to the appropriate directory.</li> </ul> </li> <li> <p>Python Environment Setup:</p> <ul> <li>The script creates a Python virtual environment to isolate the dependencies.</li> <li>It installs the Multimno code along with all required dependencies within this virtual environment. It assumes all dependencies have been setup at cloud storage.</li> </ul> </li> </ol> <p>This bootstrap script ensures that the Google Dataproc cluster is properly configured with all necessary components to run the Multimno pipeline.</p>"},{"location":"UserManual/cloud_usage/#execution_1","title":"Execution","text":"<p>The script <code>deployment/gcp_dataproc_example/launch.sh</code> provides an example of how to execute the Multimno pipeline using the <code>orchestrator_multimno.py</code> script. In the context of Google Dataproc, this script can be executed as a Dataproc Job. The following points outline the execution process:</p> <ol> <li> <p>Dataproc Job Configuration:</p> <ul> <li>Since the execution involves running an orchestrator script rather than a direct <code>spark-submit</code> command, the job must be configured to launch a bash command.</li> <li>This is achieved using the <code>Pig</code> job which can launch a custom command.</li> </ul> </li> <li> <p>Launching the Script:</p> <ul> <li>The <code>gcloud</code> command is used to submit the <code>launch.sh</code> script as a Dataproc Job, which in turn runs the <code>orchestrator_multimno.py</code> script with the specified configuration file (<code>pipeline.json</code>).</li> <li>Example:     <pre><code>gcloud dataproc jobs submit pig  \\\n    --region=$region \\\n    --cluster=$cluster \\\n    --async \\\n    --jars=\"gs://$bucket/eurostat/code/launch.sh\" \\\n    -e=\"sh /opt/workdir/code/launch.sh $pipeline_path\"\n</code></pre></li> </ul> </li> </ol>"},{"location":"UserManual/execution/","title":"Execution","text":"<p>The multimno software is a python application that launches a single component with a given configuration.  This atomic design allows the application to be integrated with multiple orchestration software. </p> <p>At the moment a python script called <code>orchestrator_multimno.py</code> is provided which will execute a  pipeline of components sequentially using <code>spark-submit</code> commands.</p> <p>The execution process can be divided into four steps:</p>"},{"location":"UserManual/execution/#1-setting-input-data","title":"1. Setting Input Data","text":"<p>The following input data is required to execute the multimno software addtionally from the MNO Data: - [National Holidays data] - [Mcc ISO Timezone mapping data] - [Countries data] - [Geographic zones data] - [Inbound Estimation factors data]</p> <p>The details of this data are defined at the Tecnical documentation: Report D4.2 at Annex I. An example of each of this datasets are included in the repository at: <code>sample_data/lakehouse/bronze</code>. </p>"},{"location":"UserManual/execution/#2-pipeline-definition","title":"2. Pipeline definition","text":"<p>The pipeline is defined as a json file that glues all the configuration files  and defines the sequential execution order of the components. The structure is as follows:</p> <ul> <li>general_config_path: Path to the general configuration file</li> <li>spark_submit_args: List containing arguments that will be passed to the spark-submit command. It can be empty.</li> <li>pipeline: List containing the order in which the components will be executed. Each item is composed of the values:<ul> <li>component_id: Id of the component to be executed.</li> <li>component_config_path: Path to the component configuration file.</li> </ul> </li> </ul> <p>Example: <pre><code>{\n    \"general_config_path\": \"pipe_configs/configurations/general_config.ini\",\n    \"spark_submit_args\": [\n        \"--master=spark://spark-master:7077\",\n        \"--packages=org.apache.sedona:sedona-spark-3.5_2.12:1.6.0,org.datasyslab:geotools-wrapper:1.6.0-28.2\"\n    ],\n    \"pipeline\": [\n        {\n            \"component_id\": \"InspireGridGeneration\",\n            \"component_config_path\": \"pipe_configs/configurations/grid/grid_generation.ini\"\n        },\n        {\n            \"component_id\": \"EventCleaning\",\n            \"component_config_path\": \"pipe_configs/configurations/event/event_cleaning.ini\"\n        }\n    ]\n}\n</code></pre></p> <p>Configuration for executing a demo pipeline is given in the file: <code>pipe_configs/pipelines/pipeline.json</code> This file contains the order of the execution of the pipeline components and references to its demo configuration files  that are given as well in the repository.</p>"},{"location":"UserManual/execution/#3-execution-configuration","title":"3. Execution Configuration","text":"<p>Each component of the pipeline to be executed must be configured to the user desired settings. It is recommended to take  the configurations defined in <code>pipe_configs/configurations</code> as a base and refine them using the configuration guide.</p>"},{"location":"UserManual/execution/#spark-configuration","title":"Spark configuration","text":"<p>spark-submit args</p> <p>The entrypoint for the pipeline execution: <code>orchestrator_multimno.py</code>, performs <code>spark-submit</code> commands to execute each component of the pipeline as a Spark Job. To define <code>spark-submit</code> arguments edit the <code>spark_submit_args</code> variable in the pipeline.json. </p> <p>This variable follows the same syntax as <code>spark-submit</code> arguments.</p> <ul> <li>Spark submit documentation: https://spark.apache.org/docs/latest/submitting-applications.html</li> </ul> <p>Spark Configuration</p> <p>To define Spark session specific configurations, edit the <code>[Spark]</code> section in the general_configuration file. If you want to change the Spark configuration for only one component in the pipeline, you can edit the the <code>[Spark]</code> section in the component configuration file which will override values defined in the general configuration file.</p> <ul> <li>Spark configuration documentation: https://spark.apache.org/docs/latest/configuration.html</li> </ul>"},{"location":"UserManual/execution/#4-execution","title":"4. Execution","text":""},{"location":"UserManual/execution/#pipeline-execution","title":"Pipeline execution","text":"<p>For executing a pipeline the <code>orchestrator_multimno.py</code> entrypoint shall be used. This takes as input a path to a json file with the pipeline definition as defined in the previous Pipeline definition section. </p> <p>Example: <pre><code>./multimno/orchestrator_multimno.py pipe_configs/pipelines/pipeline.json\n</code></pre></p> <p>Warning</p> <p>The <code>orchestrator_multimno.py</code> must be located in the same directory as the <code>main_multimno.py</code> file.</p>"},{"location":"UserManual/execution/#single-component-execution","title":"Single component execution","text":"<p>If you want to only launch a component you can perform manually the spark-submit command from a terminal using the <code>main_multimno.py</code> entrypoint:</p> <p>The entrypoint of the application is a main.py which receives the following positional parameters: - component_id: Id of the component that will be launched. - general_config_path: Path to the general configuration file of the application. - component_config_path: Path to the component configuration file.</p> <pre><code>spark-submit multimno/main_multimno.py &lt;component_id&gt; &lt;general_config_path&gt; &lt;component_config_path&gt;\n</code></pre> <p>Example:</p> <pre><code>spark-submit multimno/main_multimno.py InspireGridGeneration pipe_configs/configurations/general_config.ini pipe_configs/configurations/grid/grid_generation.ini\n</code></pre>"},{"location":"UserManual/setup_guide/","title":"Setup Guide","text":""},{"location":"UserManual/setup_guide/#introduction","title":"Introduction","text":"<p>The multimno software is a python application using the PySpark library to harness the power of Apache Spark, a fast and general-purpose cluster-computing system. PySpark provides an interface for Apache Spark in Python, enabling developers to utilize Spark's high-level APIs and distributed computing capabilities while working in the Python programming language. The Spark framework is critical to this application as it handles the distribution of data and computation across the cluster, ensures fault tolerance, and optimizes execution for performance gains. Deployment of a PySpark application can be done in two ways:</p> <p>1) Cluster mode: On a Cluster, where Spark distributes tasks across the nodes, allowing for parallel processing and efficient handling of large-scale data workloads. Recommended for production environments. 2) Local mode: On a single node, where Spark runs on a single machine. Recommended for development and testing environments. </p>"},{"location":"UserManual/setup_guide/#cluster-mode","title":"Cluster Mode","text":"<p>There are multiple ways of deploying a Spark cluster: Standalone, YARN managed, Kubernetes, Cloud managed services...  </p> <p>This guide will not enter in the specific steps for deploying a Spark cluster and only explain the requirements  and software installation steps in an existing cluster. All nodes in the cluster created shall satisfy the OS libraries requirements.</p> <p>Spark cluster mode official documentation: https://spark.apache.org/docs/latest/cluster-overview.html</p>"},{"location":"UserManual/setup_guide/#package-deploy-the-code","title":"Package &amp; deploy the code","text":"<p>There are two ways of deploying the code:</p>"},{"location":"UserManual/setup_guide/#whl-method","title":"WHL method","text":"<p>Package the code into a whl file that contains all the code and python dependencies required. Use the following  command to package it: <pre><code>python3 -m pip build\n</code></pre></p> <p>The python version and OS used to package the code must be the same of the nodes of the spark cluster.  The python library build must be installed beforehand. To install it use: <code>pip install build --upgrade</code></p> <p>Then, just install this package with all its dependencies into every node of the cluster with: <pre><code>pip install multimno*.whl\n</code></pre></p> <p>Internet connection is required to download all needed dependencies from the pypi repository</p>"},{"location":"UserManual/setup_guide/#zip-method","title":"ZIP method","text":"<p>Zip all code under <code>multimno</code> directory into a single file. Then send it to the Spark server through the  spark-submit configuration parameter: <code>--py-files=multimno.zip</code>. When using this method, the cluster must have the  python dependencies installed beforehand.</p>"},{"location":"UserManual/setup_guide/#install-dependencies","title":"Install Dependencies","text":"<p>Multimno software is a pyspark application that needs both java and python depencies intalled to run.</p>"},{"location":"UserManual/setup_guide/#java-dependencies","title":"Java Dependencies","text":"<p>The application uses the Apache Sedona engine to perform spatial calculations. In order to install this engine,  the jar files must be downloaded to the cluster. These files can be downloaded at execution time through the maven repository, specifying  them in the spark configuration, or they can be downloaded manually into the <code>$SPARK_HOME/jars</code> dir of every node in the cluster.</p> <p>Reference - Sedona installation: https://sedona.apache.org/1.6.1/setup/cluster/</p>"},{"location":"UserManual/setup_guide/#python-dependencies","title":"Python Dependencies","text":"<p>A requirement of a pyspark application is that the python version must be alligned for all the cluster.</p> <p>The software needs a set of python dependencies to be installed in every node of the cluster. These dependencies will be  installed automatically when using the WHL method. If you are using the ZIP method  you will need to install them manually into every node of the cluster through the requirements file. Install the dependencies of the  <code>pyproject.toml</code> file into every node of the cluster.</p>"},{"location":"UserManual/setup_guide/#local-mode","title":"Local Mode","text":"<p>There are two ways of setting up a system for executing the source code in local mode:   1) Building the docker image provided. (Recommended)   2) Installing and setting up all required system libraries.  </p>"},{"location":"UserManual/setup_guide/#docker-setup","title":"Docker setup","text":"<p>A Dockerfile is provided to build a docker image with all necessary dependencies for the code execution.</p>"},{"location":"UserManual/setup_guide/#installing-docker-software","title":"Installing docker software","text":"<p>To use the docker image it is necessary to have the docker engine installed. Please follow the official docker  guide to set it up in your system: -  Official guide: Click here</p>"},{"location":"UserManual/setup_guide/#docker-image-creation","title":"Docker image creation","text":"<p>Execute the following command: <pre><code>docker build -t multimno:1.0-prod --target=multimno-prod .\n</code></pre></p>"},{"location":"UserManual/setup_guide/#docker-container-creation","title":"Docker container creation","text":"<p>Run an example pipeline within a container <pre><code>docker run --rm --name=multimno-container -v \"${PWD}/sample_data:/opt/data\" -v \"${PWD}/pipe_configs:/opt/app/pipe_configs\" multimno:1.0-prod pipe_configs/pipelines/pipeline.json\n</code></pre></p> <p>Run a container in interactive mode <pre><code>docker run -it --name=multimno-container -v \"${PWD}/sample_data:/opt/data\" -v \"${PWD}/pipe_configs:/opt/app/pipe_configs\" --entrypoint=bash multimno:1.0-prod \n</code></pre></p> <p>After performing this command your shell(command-line) will be inside the container and you can perform  the execution steps to try out the code.</p>"},{"location":"UserManual/setup_guide/#clean-up","title":"Clean up","text":"<p>Exit the terminal with:</p> <p>Ctrl+D or writing <code>exit</code></p> <p>Delete the container created with: <pre><code>docker rm multimno-container\n</code></pre></p> <p>Delete the docker image with: <pre><code>docker rmi multimno:1.0-prod\n</code></pre></p>"},{"location":"UserManual/setup_guide/#software-setup","title":"Software setup","text":"<p>The software is designed to be executed on a Linux operating system. It is recommended to use Ubuntu 22.04 LTS. However, the following environments are also supported:</p> <ul> <li>macOS: Version 12.6 or later.</li> <li>Windows 11: Using Windows Subsystem for Linux 2 (WSL2) with Ubuntu 22.04 as the configured distribution.</li> </ul> <p>Please ensure that your system meets these requirements to guarantee optimal performance and compatibility.</p>"},{"location":"UserManual/setup_guide/#install-system-libs","title":"Install system libs","text":"<pre><code>sudo apt-get update &amp;&amp; \\\n    sudo apt-get install -y --no-install-recommends \\\n      sudo \\\n      openjdk-17-jdk \\\n      build-essential \\\n      software-properties-common \\\n      openssh-client openssh-server \\\n      gdal-bin \\\n      libgdal-dev \\\n      ssh\n</code></pre>"},{"location":"UserManual/setup_guide/#download-spark-source-code","title":"Download Spark source code","text":"<pre><code>SPARK_VERSION=3.5.1\nexport SPARK_HOME=${SPARK_HOME:-\"/opt/spark\"}\nmkdir -p ${SPARK_HOME}\ncd ${SPARK_HOME}\nwget https://dlcdn.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.tgz \\\n  &amp;&amp; tar -xvzf spark-${SPARK_VERSION}-bin-hadoop3.tgz --directory /opt/spark --strip-components 1 \\\n  &amp;&amp; rm -rf spark-${SPARK_VERSION}-bin-hadoop3.tgz\nexport PATH=\"${PATH}:$SPARK_HOME/bin:$SPARK_HOME/sbin\"\n</code></pre>"},{"location":"UserManual/setup_guide/#install-python-requirements","title":"Install python requirements","text":"<pre><code>pip install --upgrade pip uv\nuv pip install -r pyproject.toml\n</code></pre> <p>You can use a virtualenv for avoiding conflicts with other python libraries.</p>"},{"location":"UserManual/setup_guide/#install-spark-dependencies","title":"Install Spark dependencies","text":"<pre><code>SCALA_VERSION=2.12\nSEDONA_VERSION=1.5.1\nGEOTOOLS_WRAPPER_VERSION=28.2\nchmod +x ./resources/scripts/install_sedona_jars.sh\n./resources/scripts/install_sedona_jars.sh ${SPARK_VERSION} ${SCALA_VERSION} ${SEDONA_VERSION} ${GEOTOOLS_WRAPPER_VERSION} \n</code></pre>"},{"location":"UserManual/setup_guide/#setup-spark-configuration","title":"Setup spark configuration","text":"<pre><code>cp conf/spark-defaults.conf \"$SPARK_HOME/conf/spark-defaults.conf\"\ncp conf/log4j2.properties \"$SPARK_HOME/conf/log4j2.properties\"\n</code></pre>"},{"location":"UserManual/configuration/","title":"Configuration","text":""},{"location":"UserManual/configuration/#configuration-section-structure","title":"Configuration Section Structure","text":"<p>The configuration section is divided in four sections:  </p> <ul> <li>Pipeline: Main pipeline components to generate indicators from Mno Data.  </li> <li>Optional: Components that are optional to the pipeline execution. They enrich some data objects which may lead to quality improvements of the final results but are not essential to the pipeline.  </li> <li>QualityWarnings: Components that analyze quality metrics of data objects.  </li> <li>SyntheticMnoData: Components that are used to create synthetic Mno Data. Mainly used for testing purposes.  </li> </ul>"},{"location":"UserManual/configuration/#configuration-files-used","title":"Configuration files used","text":"<p>The multimno application requires from multiple configuration files.  </p> <ul> <li>One general configuration file describing general parameters like file paths, logging, spark and  common values for all components in the pipeline.  </li> <li>A configuration file per each component of the pipeline with configuration parameters exclusive to the component. Values defined in these files can override values defined in the general configuration file.</li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/0_general_configuration/","title":"General Configuration","text":"<p>The general configuration file contains transversal settings for all the pipeline. It is an INI file composed of three main sections:</p> <p>General: Section containing configuration values which are pipeline wide. </p> <p>Logging: Section which contains the logger settings.</p> <p>Paths: Section containing the definition of all paths to be used.</p> <p>Spark: Apache Spark configuration values. </p>"},{"location":"UserManual/configuration/1_Pipeline/0_general_configuration/#general-section","title":"General section","text":"<ul> <li>local_timezone: str, Specifies the time zone in IANA format (e.g., Europe/Madrid) to represent regional time settings.</li> <li>local_mcc: int, MCC code of the country of study.</li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/0_general_configuration/#logging-section","title":"Logging Section","text":"<ul> <li>level: The minimum logging level for console output. Messages with a level equal to or higher than this will be output to the console.</li> <li>format: The format of the log messages for console output.</li> <li>datefmt: The format of the date/time in the log messages for console output.</li> <li>file_log_level: The minimum logging level for file output. Messages with a level equal to or higher than this will be written to the log file.</li> <li>file_format: The format of the log messages for file output.</li> <li>report_path: The path where the log file will be written.</li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/0_general_configuration/#paths-section","title":"Paths Section","text":"<ul> <li>home_dir: The main directory for the application. </li> <li>lakehouse_dir: The directory for the \"lakehouse\" data.</li> <li>exports_dir: The directory where exported data is stored.</li> <li>landing_dir: The directory for the \"landing\" data within the lakehouse.</li> <li>bronze_dir: The directory for the \"bronze\" data within the lakehouse.</li> <li>silver_dir: The directory for the \"silver\" data within the lakehouse.</li> <li>gold_dir: The directory for the \"gold\" data within the lakehouse.</li> <li>app_log_dir: The directory where application logs are stored.</li> <li>spark_log_dir: The directory where Spark logs are stored (Output for the Spark History Server).</li> <li>silver_quality_metrics_dir: The directory for quality metrics of the \"silver\" data.</li> <li>silver_quality_warnings_dir: The directory for quality warnings of the \"silver\" data.</li> <li>gold_quality_warnings_dir: The directory for quality warnings of the \"gold\" data.</li> <li>spark_checkpoint_dir: The directory for Spark checkpoints.</li> </ul> <p>All the [Paths] subsections (Paths.Landing, Paths.Bronze, Paths.Silver, Paths.Gold), contain the different paths for the output of the pipeline components. It is recommended to use the default values as all this paths are relative to the ones specified in the [Paths] section.</p>"},{"location":"UserManual/configuration/1_Pipeline/0_general_configuration/#spark-section","title":"Spark Section","text":"<p>Parameters defined in this section will be used to create the spark session. Values supported are based in: Spark configuration. As an exception, the parameter <code>session_name</code> has been included which identifies the name of the spark session.</p> <p>Example:</p> <pre><code>[General]\nlocal_timezone = Europe/Madrid\nlocal_mcc = 214\n\n[Logging]\nlevel = INFO\nformat= %(asctime)-20s %(message)s\ndatefmt = %y-%m-%d %H:%M:%S\nfile_log_level = INFO\nfile_format = %(asctime)-20s |%(name)s| %(levelname)s: %(message)s\nreport_path = /opt/data/app_log\n\n[Paths]\n# Main paths\nhome_dir = /opt/data\nlakehouse_dir = ${Paths:home_dir}/lakehouse\nexports_dir = ${Paths:home_dir}/exports\n# Lakehouse\nlanding_dir = ${Paths:lakehouse_dir}/landing\nbronze_dir = ${Paths:lakehouse_dir}/bronze\nsilver_dir = ${Paths:lakehouse_dir}/silver\ngold_dir = ${Paths:lakehouse_dir}/gold\n# Logs dir\napp_log_dir = ${Paths:home_dir}/log\nspark_log_dir = ${Paths:home_dir}/spark_log\n# Quality metrics dir\nsilver_quality_metrics_dir = ${Paths:silver_dir}/quality_metrics\n# Quality warnings dir\nsilver_quality_warnings_dir = ${Paths:silver_dir}/quality_warnings\n# Quality graphs\ngold_quality_warnings_dir = ${Paths:gold_dir}/quality_warnings\n# spark\nspark_checkpoint_dir = ${Paths:home_dir}/spark/spark_checkpoint\n\n[Spark]\n# parameters in this section are passed to Spark except session_name \nsession_name = MultiMNO\nspark.master = local[*]\nspark.driver.host = localhost\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/10_daily_permanence_score/","title":"DailyPermanenceScore Configuration","text":"<p>To initialise and run the component two configs are used -  <code>general_config.ini</code> and component\u2019s config <code>daily_permanence_score.ini</code>. In  <code>general_config.ini</code> all paths to the data objects used by DailyPermanenceScore component shall be specified. An example with the specified paths is shown below:</p> <pre><code>[Paths.Silver]\n...\nevent_data_silver_flagged = ${Paths:silver_dir}/mno_events_flagged\ncell_footprint_data_silver = ${Paths:silver_dir}/cell_footprint\ncell_to_group_data_silver = ${Paths:silver_dir}/cell_to_group\ngroup_to_tile_data_silver = ${Paths:silver_dir}/group_to_tile\ndaily_permanence_score_data_silver = ${Paths:silver_dir}/daily_permanence_score\n...\n</code></pre> <p>Below there is a description of the sub component\u2019s config  - <code>daily_permanence_score.ini</code>. </p> <p>Parameters are as follows:</p> <p>Under  <code>[DailyPermanenceScore]</code> config section: </p> <ul> <li> <p>data_period_start - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-08), the date from which start DailyPermanenceScore processing. Make sure events and cell footprint data are available for all the dates between data_period_start and data_period_end, as well as for the previous day to data_period_start and the posterior day to data_period_end. </p> </li> <li> <p>data_period_end - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-15), the date till which we will perform DailyPermanenceScore processing. Make sure events and cell footprint data are available for all the dates between data_period_start and data_period_end, as well as for the previous day to data_period_start and the posterior day to data_period_end. </p> </li> <li> <p>time_slot_number - integer, can take the values 24, 48, or 96. Number of equal-length time slots in which to divide each date for the daily permanence score calculation. Recommended value: 24, so that the day is divided in 24 1-hour time slots. The values 48 and 96 result in 30-min and 15-min time slots, respectively.</p> </li> <li> <p>max_time_thresh - integer, in seconds. In case of 2 consecutive events taking place in different cells, if the time difference between the events is lower than this threshold, then the both the first event's assigned end time and the second event's assigned start time will be equal to the average between the first and second event's timestamps. If the time difference between the 2 consecutive events is higher than this threshold, then the assigned end time for the first event will be equal to the first event's timestamp plus half the value of this threshold; and the assigned start time for the second event will be equal to the second event's timestamp minus half the value of this threshold. For the case of 2 consecutive events taking place in different cells, if the time difference between the events is higher than the corresponding threshold (either <code>max_time_thresh_day</code> or <code>max_time_thresh_night</code>), then the event timestamps are also extended half this value of <code>max_time_thresh</code>. Recommended value: 900 seconds (15 minutes).</p> </li> <li> <p>max_time_thresh_day - integer, in seconds. In case of 2 consecutive events in the same cell, if the time of at least one of the events is included in the \"day time\", i.e. from 9:00 to 22:59, then <code>max_time_thresh_day</code> is the threshold to be applied. If the time difference between the events is lower than this threshold, then the both the first event's assigned end time and the second event's assigned start time will be equal to the average between the first and second event's timestamps. Otherwise, the assigned end time for the first event will be equal to the first event's timestamp plus half the value of <code>max_time_thresh</code>; and the assigned start time for the second event will be equal to the second event's timestamp minus half the value of <code>max_time_thresh</code>. Recommended value: 7200 seconds (2 hours).</p> </li> <li> <p>max_time_thresh_night - integer, in seconds. In case of 2 consecutive events in the same cell, if the time of both events is included in the \"night time\", i.e. from 23:00 to 8:59, then <code>max_time_thresh_night</code> is the threshold to be applied. If the time difference between the events is lower than this threshold, then the both the first event's assigned end time and the second event's assigned start time will be equal to the average between the first and second event's timestamps. Otherwise, the assigned end time for the first event will be equal to the first event's timestamp plus half the value of <code>max_time_thresh</code>; and the assigned start time for the second event will be equal to the second event's timestamp minus half the value of <code>max_time_thresh</code>. Recommended value: 28800 seconds (8 hours).</p> </li> <li> <p>max_vel_thresh - float, in metres per second (m/s). In order to evaluate if an event corresponds to a \"move\", the speed between the previous event and the next event is calculated. If the speed exceeds <code>max_vel_thresh</code>, then the event is tagged as a move and will be discarded for daily permanence score calculation. Recommended value: 13.889 m/s (50 km/h).</p> </li> <li> <p>event_error_flags_to_include - set of integers, e.g. \"{0}\". It indicates the values of the \"error_flag\" column of the input event data that will be kept. Rows with \"error_flag\" values not included in this set will be discarded and will not be considered for any step of the daily permanence score component. Recommended value: {0}.</p> </li> </ul> <p>--- Optional configuration values ---</p> <ul> <li> <p>broadcast_footprints - bool, default: False. If True, broadcast cell_footprints to each executor. Only recommendeded for small countries like Luxembourg.</p> </li> <li> <p>clear_destination_directory - bool, default: False. if True, the component will clear all the data in output paths.</p> </li> <li> <p>partition_chunk_size - int, default None. Number of <code>user_id_modulo</code> partitions. Should be the same value as <code>number_of_partitions</code> in <code>EventCleaning</code> component. If not given will process all <code>user_id_modulo</code> partitions at the same time.</p> </li> <li> <p>number_of_partitions - int, default None. Number of <code>user_id_modulo</code> partitions that will be processed at the same time. This value shall be lower than <code>partition_chunk_size</code>. If not given will process all <code>user_id_modulo</code> partitions at the same time.</p> </li> <li> <p>use_200m_grid - bool, default: False. If True, the component will aggregate cell footprint data from 100m to 200m grid resolution for DPS calculations. If False, the component will use initial 100m resolution.</p> </li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/10_daily_permanence_score/#configuration-example","title":"Configuration example","text":"<pre><code>[Logging]\nlevel = DEBUG\n\n[Spark]\nsession_name = DailyPermanenceScore\n\n[DailyPermanenceScore]\ndata_period_start = 2023-01-02\ndata_period_end = 2023-01-02\n\ntime_slot_number = 24\n\nmax_time_thresh = 900  # 15 min\nmax_time_thresh_day = 7_200  # 2 h\nmax_time_thresh_night = 28_800  # 8 h\n\nmax_speed_thresh = 13.88888889  # 50 km/h\n\nevent_error_flags_to_include = {0}\n\n# Optional\nbroadcast_footprints = False\nclear_destination_directory = False\npartition_chunk_size = 256\nnumber_of_partitions = 64\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/11_midterm_permanence_score/","title":"MidtermPermanenceScore Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>midterm_permanence_score.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>...\n[Paths.Bronze]\nholiday_calendar_data_bronze = ${Paths:bronze_dir}/holiday_calendar\n\n...\n\n[Paths.Silver]\ncell_footprint_data_silver = ${Paths:silver_dir}/cell_footprint\ndaily_permanence_score_data_silver = ${Paths:silver_dir}/daily_permanence_score\nmidterm_permanence_score_data_siilver = ${Paths:silver_dir}/midterm_permanence_score\n...\n</code></pre> <p>The expected parameters in <code>midterm_permanence_score.ini</code> are as follows:  - start_month: string, with <code>YYYY-MM</code> format, indicating the first month for which mid-term permanence score and metrics are to be computed. All months between start_month and end_month, both inclusive, will be processed individually. Example: <code>2023-01</code>.  - end_month: string, with <code>YYYY-MM</code> format, indicating the last month for which mid-term permanence score and metrics are to be computed. All months between start_month and end_month, both inclusive, will be processed individually. Must be a month equal or later than start_month. Example: <code>2023-08</code>.  - before_regularity_days: positive integer, it represents the number of days previous to a month that will be loaded and used in the calculation of the mid-term regularity metrics by searching for the latest date in which a given user performed a stay in a given grid tile. Example: <code>7</code>.  - after_regularity_days: positive integer, it represents the number of days following a month that will be loaded and used in the calculation of the mid-term regularity metrics by searching for the earliest date in which a given user performed a stay in a given grid tile. Example: <code>7</code>.  - day_start_hour: integer between 0 and 23 (both inclusive) that marks the starting time of a given day in the mid-term analysis to be performed. Example: <code>4</code>. A value of <code>4</code> means that the time slots of the Daily Permanence Score contained between the 04:00 of day $D$ (inclusive) and the 04:00 of day $D+1$ (not inclusive) are considered to belong to day $D$.  - country_of_study: two-letter, upper-case string, marking the ISO Alpha 2 code of the country being studied, used to load the holiday dates of that country that are used to define the workdays and holidays day types. Example: <code>ES</code> (Spain).  - weekend_start: integer between 1 and 7, marks the first day of the week (inclusive) that belongs to the weekends day type. Monday is 1, Tuesday is 2, ..., and Sunday is 7. Example: <code>6</code>.  - weekend_end: integer between 1 and 7, marks the last day of the week (inclusive) that belongs to the weekends day type. Monday is 1, Tuesday is 2, ..., and Sunday is 7. Example: <code>7</code>.  - night_time_start: string, <code>HH:MM</code> format, marking the starting hour and minutes (inclusive) of the night_time time interval. Only allowed values for the minutes <code>MM</code> are <code>00</code>, <code>15</code>, <code>30</code>, or <code>45</code>. The time interval limits must be compatible with the time slot limits and durations of the Daily Permanence Score data used. See the Time interval additional information for additional restrictions of this parameter. Example: <code>18:00</code>.  - night_time_end: string, <code>HH:MM</code> format, marking the ending hour and minutes (exclusive) of the night_time time interval. Only allowed values for the minutes <code>MM</code> are <code>00</code>, <code>15</code>, <code>30</code>, or <code>45</code>. The time interval limits must be compatible with the time slot limits and durations of the Daily Permanence Score data used. See the Time interval additional information for additional restrictions of this parameter. Example: <code>08:00</code>.  - working_hours_start: string, <code>HH:MM</code> format, marking the starting hour and minutes (inclusive) of the working_hours time interval. Only allowed values for the minutes <code>MM</code> are <code>00</code>, <code>15</code>, <code>30</code>, or <code>45</code>. The time interval limits must be compatible with the time slot limits and durations of the Daily Permanence Score data used. See the Time interval additional information for additional restrictions of this parameter. Example: <code>07:30</code>.  - working_hours_end: string, <code>HH:MM</code> format, marking the ending hour and minutes (exclusive) of the working_hours time interval. Only allowed values for the minutes <code>MM</code> are <code>00</code>, <code>15</code>, <code>30</code>, or <code>45</code>. The time interval limits must be compatible with the time slot limits and durations of the Daily Permanence Score data used. See the Time interval additional information for additional restrictions of this parameter. Example: <code>17:30</code>.  - evening_time_start: string, <code>HH:MM</code> format, marking the starting hour and minutes (inclusive) of the evening_time time interval. Only allowed values for the minutes <code>MM</code> are <code>00</code>, <code>15</code>, <code>30</code>, or <code>45</code>. The time interval limits must be compatible with the time slot limits and durations of the Daily Permanence Score data used. See the Time interval additional information for additional restrictions of this parameter. Example: <code>18:00</code>.  - evening_time_end: string, <code>HH:MM</code> format, marking the ending hour and minutes (exclusive) of the evening_time time interval. Only allowed values for the minutes <code>MM</code> are <code>00</code>, <code>15</code>, <code>30</code>, or <code>45</code>. The time interval limits must be compatible with the time slot limits and durations of the Daily Permanence Score data used. See the Time interval additional information for additional restrictions of this parameter. Example: <code>21:00</code>.  - period_combinations: dictionary indicating the combinations of sub-monthly and sub-daily periods (i.e., day types and time intervals) that are to be considered for the mid-term permanence score and metrics computation, for each month betwen start_month and end_month. The structure is as follows (a full example can be in Configuration example):    - The keys of the dictionary must be one of the possible day type values surrounded by quotes:      - <code>\"all\"</code>: every day of the month.      - <code>\"workdays\"</code>: every day of the month that does not belong to the weekend and is not a holiday in the country of study.      - <code>\"holidays\"</code>: every day of the month that is a holiday in the country of study.      - <code>\"weekends\"</code>: every day of the month that is part of the weekend.      - <code>\"mondays\"</code>: every Monday of the month.      - <code>\"tuesdays\"</code>: every Tuesday of the month.      - <code>\"wednesdays\"</code>: every Wednesday of the month.      - <code>\"thursdays\"</code>: every Thursday of the month.      - <code>\"fridays\"</code>: every Friday of the month.      - <code>\"saturdays\"</code>: every Saturday of the month.      - <code>\"sundays\"</code>: every Sunday of the month.    - The value assigned to a key must be a non-empty list surrounded by square brackets containing some of the possible time interval values:      - <code>\"all\"</code>: all time slots of the day.      - <code>\"night_time\"</code>: time slots of the day contained in the hour interval defined by night_time_start and night_time_end.      - <code>\"working_hours\"</code>: time slots of the day contained in the hour interval defined by working_hours_start and working_hours_end.      - <code>\"evening_time\"</code>: time slots of the day contained in the hour interval defined by evening_time_start and evening_time_end.</p> <p>--- Optional configuration values ---</p> <ul> <li> <p>clear_destination_directory - bool, default: False. if True, the component will clear all the data in output paths.</p> </li> <li> <p>partition_chunk_size - int, default None. Number of <code>user_id_modulo</code> partitions. Should be the same value as <code>number_of_partitions</code> in <code>EventCleaning</code> component. If not given will process all <code>user_id_modulo</code> partitions at the same time.</p> </li> <li> <p>number_of_partitions - int, default None. Number of <code>user_id_modulo</code> partitions that will be processed at the same time. This value shall be lower than <code>partition_chunk_size</code>. If not given will process all <code>user_id_modulo</code> partitions at the same time.</p> </li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/11_midterm_permanence_score/#time-interval-additional-information","title":"Time interval additional information","text":"<p>There are some nuances and restrictions related to the definition of the different time intervals and the day_start_hour parameter:  - By definition, a time interval will belong to the date that contains its start hour. See the following example:    - Suppose that day_start_hour has been set to <code>4</code>, so that the day \"starts\" at 04:00.    - Suppose that night_time_start has been set to <code>20:15</code> and night_time_end has been set to <code>06:30</code>. Then, the night time interval starts at 20:15 in the evening of some day $D$, crosses midnight, and ends at 06:30 in the morning of the following day $D+1$.    - In this case, the start of the time interval, 20:15, is between the 04:00 of day $D$ and the 04:00 of day $D+1$. Then, this time interval will be assigned to the date $D$.</p> <ul> <li>The following configuration is not allowed for the time intervals night_time, working_hours, and evening_time:</li> <li>day_start_hour is different from <code>0</code>, and</li> <li>the start of the interval is between 00:00 (exclusive) and day_start_hour (exclusive), and</li> <li> <p>the end of the interval is between 00:00 (exclusive) and the start of the interval (exclusive).</p> <p>Example of a not-allowed configuration under this restriction:    - day_start_hour is <code>4</code>, that is, 04:00.    - night_time_start is <code>03:00</code>.    - night_time_end is <code>01:15</code>.  - The following configuration is not allowed for the time intervals working_hours and evening_time:    - the start of the interval is between 00:00 (inclusive) and day_start_hour (exclusive)    - the end of the interval is later than day_start_hour (exclusive).</p> <p>Example of a not-allowed configuration under this restriction: - day_start_hour is <code>4</code>, that is, 04:00. - working_hours_start is <code>03:00</code>. - working_hours_end is <code>18:00</code>.</p> </li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/11_midterm_permanence_score/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = MidtermPermanenceScore\n\n[MidtermPermanenceScore]\n\nstart_month = 2023-02\nend_month = 2023-03\n\nbefore_regularity_days = 7\nafter_regularity_days = 7\nday_start_hour = 4  # at what time the day starts, e.g. day gos from 4AM Mon to 4AM Tue\n\ncountry_of_study = ES\n\nnight_time_start = 18:00\nnight_time_end = 08:00\n\nworking_hours_start = 08:00\nworking_hours_end = 17:00\n\nevening_time_start = 16:00\nevening_time_end = 22:00\n\nweekend_start = 6\nweekend_end = 7\n\nperiod_combinations = {\n    \"all\": [\"all\", \"night_time\", \"evening_time\", \"working_hours\"],\n    \"workdays\": [\"night_time\", \"working_hours\"],\n    \"holidays\": [\"all\", \"night_time\"],\n    \"weekends\": [\"all\", \"night_time\"],\n    \"mondays\": [\"all\"]\n    }\n\n# Optional\nclear_destination_directory = True\nnumber_of_partitions = 8\npartition_chunk_size = 4\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/12_longterm_permanence_score/","title":"LongtermPermanenceScore Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>longterm_permanence_score.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Silver]\n...\nmidterm_permanence_score_data_siilver = ${Paths:silver_dir}/midterm_permanence_score\nlongterm_permanence_score_data_siilver = ${Paths:silver_dir}/longterm_permanence_score\n...\n</code></pre> <p>The expected parameters in <code>longterm_permanence_score.ini</code> are as follows:  - start_month: string, in <code>YYYY-MM</code> format, it indicates the first month to be included in the long-term permanence score computations and analyses. All months between start_month and end_month, both inclusive, will be considered in the long-term analyses. Example: <code>2023-01</code>.  - end_month: string, in <code>YYYY-MM</code> format, it indicates the last month to be included in the long-term permanence score computations and analyses. All months between start_month and end_month, both inclusive, will be considered in the long-term analyses. Example: <code>2023-06</code>.  - winter_months: comma-separated sequence of integers between 1 and 12, they indicate which months of the year belong to the winter season. 1 represents January, 2 represents February, ..., and 12 represents December. Example: <code>12, 1, 2</code>.  - spring_months: comma-separated sequence of integers between 1 and 12, they indicate which months of the year belong to the spring season. 1 represents January, 2 represents February, ..., and 12 represents December. Example: <code>3, 4, 5</code>.  - summer_months: comma-separated sequence of integers between 1 and 12, they indicate which months of the year belong to the summer season. 1 represents January, 2 represents February, ..., and 12 represents December. Example: <code>6, 7, 8</code>.  - autumn_months: comma-separated sequence of integers between 1 and 12, they indicate which months of the year belong to the autumn season. 1 represents January, 2 represents February, ..., and 12 represents December. Example: <code>9, 10, 11</code>.  - period_combinations: dictionary indicating the combinations of sub-yearly, sub-monthly and sub-daily periods (i.e., seasons, day types and time intervals) to consider in the long-term analyses. Each combination will result in the computation of indicators resulting from aggregating the data of months belonging to the particular season, day type and time interval defined by the combination. The structure is as follows (a full example can be found in Configuration example):     - The keys of the dictionary must be one of the possible season values:         - <code>\"all\"</code>: every month between start_month and end_month, both inclusive, are considered in the aggregation.         - <code>\"winter\"</code>: every month included in the winter_months list between start_month and end_month, both inclusive, are considered in the aggregation. If this key appears then the winter_months list must contain at least one month.         - <code>\"spring\"</code>: every month included in the spring_months list between start_month and end_month, both inclusive, are considered in the aggregation. If this key appears then the spring_months list must contain at least one month.         - <code>\"summer\"</code>: every month included in the summer_months list between start_month and end_month, both inclusive, are considered in the aggregation. If this key appears then the summer_months list must contain at least one month.         - <code>\"autumn\"</code>: every month included in the autumn_months list between start_month and end_month, both inclusive, are considered in the aggregation. If this key appears then the autumn_months list must contain at least one month.     - The values assigned to a key is, in turn, another dictionary with the following structure:         - The keys of this dictionary must be one of the possible day type values surrounded by quotes:             - <code>\"all\"</code>             - <code>\"workdays\"</code>             - <code>\"holidays\"</code>             - <code>\"weekends\"</code>             - <code>\"mondays\"</code>             - <code>\"tuesdays\"</code>             - <code>\"wednesdays\"</code>             - <code>\"thursdays\"</code>             - <code>\"fridays\"</code>             - <code>\"saturdays\"</code>             - <code>\"sundays\"</code>         - The value assigned to a key must be a non-empty list surrounded by square brackets containing some of the possible time interval values:             - <code>\"all\"</code>             - <code>\"night_time\"</code>             - <code>\"working_hours\"</code>             - <code>\"evening_time\"</code></p> <pre><code>The **period_combinations** example that appears in the [Configuration example](#configuration-example) would result in the computation of the long-term permanence metrics for the following combinations:\n\n| season | day type | time interval   |\n|--------|----------|-----------------|\n| all    | all      | all             |\n| all    | all      | night_time      |\n| winter | all      | all             |\n| spring | all      | all             |\n| spring | workdays | working_hours   |\n</code></pre> <p>--- Optional configuration values ---</p> <ul> <li> <p>partition_chunk_size - int, default None. Number of <code>user_id_modulo</code> partitions. Should be the same value as <code>number_of_partitions</code> in <code>EventCleaning</code> component. If not given will process all <code>user_id_modulo</code> partitions at the same time.</p> </li> <li> <p>number_of_partitions - int, default None. Number of <code>user_id_modulo</code> partitions that will be processed at the same time. This value shall be lower than <code>partition_chunk_size</code>. If not given will process all <code>user_id_modulo</code> partitions at the same time.</p> </li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/12_longterm_permanence_score/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = LongtermPermanenceScore\n\n[LongtermPermanenceScore]\nstart_month = 2023-01\nend_month = 2023-06\n\nwinter_months = 12, 1, 2\nspring_months = 3, 4, 5\nsummer_months = 6, 7, 8\nautumn_months = 9, 10, 11\n\nperiod_combinations = {\n    \"all\": {\n        \"all\": [\"all\", \"night_time\"]\n    },\n    \"winter\": {\n        \"all\": [\"all\"],\n    },\n    \"spring\": {\n        \"all\": [\"all\"],\n        \"workdays\": [\"working_hours\"]\n    }\n    }\n\n# Optional\nnumber_of_partitions = 8\npartition_chunk_size = 4\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/13_usual_environment_labeling/","title":"UsualEnvironmentLabeling Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>usual_environment_labeling.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Silver]\n...\nlongterm_permanence_score_data_siilver = ${Paths:silver_dir}/longterm_permanence_score\nusual_environment_labels_data_silver = ${Paths:silver_dir}/usual_environment_labels\n...\n</code></pre> <p>The expected parameters in <code>usual_environment_labeling.ini</code> are as follows:</p> <ul> <li> <p>start_month: string, in <code>YYYY-MM</code> format, it indicates the first month to be included in the Usual Environment Labeling process. Long-term permanence metrics data shall be available for the corresponding start_month, end_month and season. Example: <code>2023-01</code>.</p> </li> <li> <p>end_month: string, in <code>YYYY-MM</code> format, it indicates the last month to be included in the Usual Environment Labeling process. Long-term permanence metrics data shall be available for the corresponding start_month, end_month and season. Example: <code>2023-06</code>.</p> </li> <li> <p>season: string, it indicates the season to be included in the Usual Environment Labeling process. Long-term permanence metrics data shall be available for the corresponding start_month, end_month and season. The months that correspond to each season are defined in the configuration of the Long-term permanence score method. Example: <code>all</code>. Options:</p> <ul> <li><code>\"all\"</code>: every month between start_month and end_month, both inclusive, are considered in the aggregation.</li> <li><code>\"winter\"</code>: every month included in the winter_months list between start_month and end_month, both inclusive, are considered in the aggregation.</li> <li><code>\"spring\"</code>: every month included in the spring_months list between start_month and end_month, both inclusive, are considered in the aggregation.</li> <li><code>\"summer\"</code>: every month included in the summer_months list between start_month and end_month, both inclusive, are considered in the aggregation.</li> <li><code>\"autumn\"</code>: every month included in the autumn_months list between start_month and end_month, both inclusive, are considered in the aggregation.</li> </ul> </li> <li> <p>total_ps_threshold: int, permanence score. It represents a long-term permanence score (ps) threshold to apply to discard rarely observed devices. Rarely observed devices are defined as those devices for which the total observed device's ps is lower than this threshold. Example: <code>1500</code>.</p> </li> <li> <p>total_ndays_threshold: int, number of days. It represents a frequency (number of days) threshold to apply to discard discontinuously observed devices. Discontinuously observed devices are defined as those devices for which the total observed device's frequency is lower than this threshold. Example <code>50</code>: </p> </li> <li> <p>ue_gap_ps_threshold: float, percentage. It represents a long-term permanence score (ps) threshold to apply in the preselection of tiles that may be tagged as usual environment (ue). This threshold is relative (a percentage) to the total ps device observation in the corresponding day type and time interval. The tiles associated to each device are sorted by ps in descending order, and then those that are below a ps gap that is bigger than <code>device_observation_ps * ue_gap_ps_threshold / 100</code> are filtered out. Example: <code>20</code> (%).</p> </li> <li> <p>gap_ps_threshold: int, permanence score. It represents a long-term permanence score (ps) threshold to apply in the preselection of tiles that may be tagged as home or work locations. This threshold is an absolute ps value, and its recommended value is 1. The tiles associated to each device are sorted by ps in descending order, and then those that are below a ps gap that is bigger than <code>gap_ps_threshold</code> are filtered out. Example: <code>1</code>.</p> </li> <li> <p>ue_ps_threshold: float, percentage. It represents a long-term permanence score (ps) threshold for the selection of tiles that may be tagged as usual environment (ue). This threshold is relative (a percentage) to the total ps device observation in the corresponding day type and time interval. The tiles associated to one device that have passed the ue gap cut and which have a ps higher than <code>device_observation_ps * ue_ps_threshold / 100</code> are tagged as ue. Example: <code>70</code> (%).</p> </li> <li> <p>home_ps_threshold: float, percentage. It represents a long-term permanence score (ps) threshold for the selection of tiles that may be tagged as home location. This threshold is relative (a percentage) to the total ps device observation in the corresponding day type and time interval. The tiles associated to one device that have passed the home gap cut and which have a ps higher than <code>device_observation_ps * home_ps_threshold / 100</code> are tagged as home. Example: <code>80</code> (%).</p> </li> <li> <p>work_ps_threshold: float, percentage. It represents a long-term permanence score (ps) threshold for the selection of tiles that may be tagged as work location. This threshold is relative (a percentage) to the total ps device observation in the corresponding day type and time interval. The tiles associated to one device that have passed the work gap cut and which have a ps higher than <code>device_observation_ps * work_ps_threshold / 100</code> are tagged as work. Example: <code>80</code> (%).</p> </li> <li> <p>ue_ndays_threshold: float, percentage. It represents a frequency (number of days) threshold for the selection of tiles that may be tagged as usual environment (ue). This threshold is relative (a percentage) to the total observed frequency for the device in the corresponding day type and time interval. The tiles associated to one device that have passed the ue gap cut and which have a frequency higher than device_observation_frequency * ue_ndays_threshold / 100 are tagged as ue. Example: <code>70</code> (%).</p> </li> <li> <p>home_ndays_threshold: float, percentage. It represents a frequency (number of days) threshold for the selection of tiles that may be tagged as home location. This threshold is relative (a percentage) to the total observed frequency for the device in the corresponding day type and time interval. The tiles associated to one device that have passed the home gap cut and which have a frequency higher than device_observation_frequency * home_ndays_threshold / 100 are tagged as home. Example: <code>70</code> (%).</p> </li> <li> <p>ue_ndays_threshold: float, percentage. It represents a frequency (number of days) threshold for the selection of tiles that may be tagged as work location. This threshold is relative (a percentage) to the total observed frequency for the device in the corresponding day type and time interval. The tiles associated to one device that have passed the work gap cut and which have a frequency higher than device_observation_frequency * work_ndays_threshold / 100 are tagged as work. Example: <code>70</code> (%).</p> </li> <li> <p>disaggregate_to_100m_grid: bool, default: False. If True, disaggregate the usual environment labels from 200m to 100m grid resolution. If False, the component will use the initial resolution.</p> </li> </ul> <p>--- Optional configuration values ---</p> <ul> <li> <p>partition_chunk_size - int, default None. Number of <code>user_id_modulo</code> partitions. Should be the same value as <code>number_of_partitions</code> in <code>EventCleaning</code> component. If not given will process all <code>user_id_modulo</code> partitions at the same time.</p> </li> <li> <p>number_of_partitions - int, default None. Number of <code>user_id_modulo</code> partitions that will be processed at the same time. This value shall be lower than <code>partition_chunk_size</code>. If not given will process all <code>user_id_modulo</code> partitions at the same time.</p> </li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/13_usual_environment_labeling/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = UsualEnvironmentLabeling\n\n[UsualEnvironmentLabeling]\nstart_month = 2023-01\nend_month = 2023-04\nseason = all\n\n# filtering rarely/discontinuously observed devices\ntotal_ps_threshold = 1500\ntotal_ndays_threshold = 50\n\nue_gap_ps_threshold = 20\ngap_ps_threshold = 1\n\nue_ps_threshold = 70\nhome_ps_threshold = 80\nwork_ps_threshold = 80\n\nue_ndays_threshold = 70\nhome_ndays_threshold = 80\nwork_ndays_threshold = 80\n\n# Optional\nnumber_of_partitions = 8\npartition_chunk_size = 4\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/14_usual_environment_aggregation/","title":"UsualEnvironmentAggregation Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>ue_aggregation.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Silver]\n...\nusual_environment_labels_data_silver = ${Paths:silver_dir}/usual_environment_labels\ngrid_data_silver = ${Paths:silver_dir}/grid\naggregated_usual_environments_silver = ${Paths:silver_dir}/aggregated_usual_environment\n# only if used\nenriched_grid_data_silver = ${Paths:silver_dir}/grid_enriched\n...\n</code></pre> <p>The expected parameters in <code>ue_aggregation.ini</code> are as follows:</p> <ul> <li> <p>start_month: string, in <code>YYYY-MM</code> format, it indicates the first month to be included in the Usual Environment Aggregation process. Long-term permanence metrics data shall be available for the corresponding start_month, end_month and season. Example: <code>2023-01</code>.</p> </li> <li> <p>end_month: string, in <code>YYYY-MM</code> format, it indicates the last month to be included in the Usual Environment Aggregation process. Long-term permanence metrics data shall be available for the corresponding start_month, end_month and season. Example: <code>2023-06</code>.</p> </li> <li> <p>season: string, it indicates the season to be included in the Usual Environment Aggregation process. Long-term permanence metrics data shall be available for the corresponding start_month, end_month and season. The months that correspond to each season are defined in the configuration of the Long-term permanence score method. Example: <code>all</code>. Options:</p> <ul> <li><code>\"all\"</code>: every month between start_month and end_month, both inclusive, are considered in the aggregation.</li> <li><code>\"winter\"</code>: every month included in the winter_months list between start_month and end_month, both inclusive, are considered in the aggregation.</li> <li><code>\"spring\"</code>: every month included in the spring_months list between start_month and end_month, both inclusive, are considered in the aggregation.</li> <li><code>\"summer\"</code>: every month included in the summer_months list between start_month and end_month, both inclusive, are considered in the aggregation.</li> <li><code>\"autumn\"</code>: every month included in the autumn_months list between start_month and end_month, both inclusive, are considered in the aggregation.</li> </ul> </li> <li> <p>clear_destination_directory: bool, if to delete all previous outputs before running the component.</p> </li> <li> <p>uniform_tile_weights: int, if to use uniform tile weights in the aggregation process. If <code>True</code>, the weights of the tiles are equal. If <code>False</code>, the weights of the tiles are calculated based on landuse information.</p> </li> <li> <p>landuse_prior_weights - dictionary, keys are land use types, values are weights for these landuse types. The land use types are: </p> <ul> <li>residential_builtup</li> <li>other_builtup</li> <li>roads</li> <li>other_human_activity</li> <li>open_area</li> <li>forest</li> <li>water</li> </ul> </li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/14_usual_environment_aggregation/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = UsualEnvironmentLabeling\n\n[UsualEnvironmentLabeling]\nclear_destination_directory = True\n\nstart_month = 2023-01\nend_month = 2023-03\n\nseason = all\nuniform_tile_weights = True\nlanduse_weights = {\n    \"residential_builtup\": 1.0,\n    \"other_builtup\": 1.0,\n    \"roads\": 0.5,\n    \"other_human_activity\": 0.1,\n    \"open_area\": 0.0,\n    \"forest\": 0.0,\n    \"water\": 0.0\n    }\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/15_internal_migration/","title":"InternalMigration Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>internal_migration.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Silver]\n...\ninternal_migration_previous_ue_labels_silver = ${Paths:ue_dir}/internal_migration_previous_ue_labels\ninternal_migration_new_ue_labels_silver = ${Paths:ue_dir}/internal_migration_new_ue_labels\ninternal_migration_silver = ${Paths:ue_dir}/internal_migration\n# only if used\nenriched_grid_data_silver = ${Paths:silver_dir}/grid_enriched\n...\n</code></pre> <p>The expected parameters in <code>internal_migration.ini</code> are as follows:  - clear_destination_directory: bool, whether to clear the output directory before running the component. Example: <code>True</code>.  - clear_quality_metrics_directory: bool, whether to clear the quality metrics directory before running the component. Example: <code>False</code>.  - migration_threshold: float between 0.0 and 1.0, it sets the threshold for considering that a device is to be considered for internal migration based on the metric comparing the set of home tiles in the two long-term time periods being compared. Example: <code>0.5</code>.  - uniform_tile_weights: bool, whether to use uniform tile weights for the home-labelled tiles. If <code>True</code>, the weights of all home tiles of a device are equal. If <code>False</code>, the weights of the home tiles will be proportional to the provided weights, and it is mandatory to provide a path under <code>internal_migration_enriched_silver_grid</code> on the <code>general_config.in</code> file.  - zoning_dataset_id: string, ID of the zones dataset that will be used to map grid tiles to zones. Example: <code>nuts</code>.  - hierarchical_levels: comma-separated list of positive integers, they are the hierarchical levels of the zones dataset used that should be processed by this component. If the zones dataset was not hierarchical, this should just take the value <code>1</code>. Example: <code>1,2,3</code>.  - start_month_previous: string, in <code>YYYY-MM</code> format, it indicates the first month of the first long-term period used to compute the internal migration. Example: <code>2023-01</code>.  - end_month_previous: string, in <code>YYYY-MM</code> format, it indicates the last month of the first long-term period used to compute the internal migration. Example: <code>2023-06</code>.  - season_previous: string, value of the season of the first long-term period used to compute the internal migration. Allowed values are: <code>all</code>, <code>spring</code>, <code>summer</code>, <code>autumn</code>, <code>winter</code>. Example: <code>all</code>.  - start_month_new: string, in <code>YYYY-MM</code> format, it indicates the first month of the second long-term period used to compute the internal migration. Example: <code>2023-07</code>.  - end_month_new: string, in <code>YYYY-MM</code> format, it indicates the last month of the second long-term period used to compute the internal migration. Example: <code>2023-12</code>.  - season_new: string, value of the season of the second long-term period used to compute the internal migration. Allowed values are: <code>all</code>, <code>spring</code>, <code>summer</code>, <code>autumn</code>, <code>winter</code>. Example: <code>all</code>. - landuse_prior_weights - dictionary, keys are land use types, values are weights for these landuse types. The land use types are:      - residential_builtup     - other_builtup     - roads     - other_human_activity     - open_area     - forest     - water</p>"},{"location":"UserManual/configuration/1_Pipeline/15_internal_migration/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = InternalMigration\n\n[InternalMigration]\nclear_destination_directory = True\nclear_quality_metrics_directory = False\nmigration_threshold = 0.5\nuniform_tile_weights = True\n\nzoning_dataset_id = nuts  # ID of the zoning dataset\nhierarchical_levels = 1  # Hierarchical level(s) of the zoning dataset. Comma-separated list\n\nstart_month_previous = 2023-01  # Start month (inclusive) of the first long-term period\nend_month_previous = 2023-06  # End month (inclusive) of the first long-term period\nseason_previous = all  # Allowed values: `all`, `spring`, `summer`, `autumn`, `winter`. First long term period\n\nstart_month_new = 2023-07  # Start month (inclusive) of the second long-term period\nend_month_new = 2023-12  # End month (inclusive) of the second long-term period\nseason_new = all  # Allowed values: `all`, `spring`, `summer`, `autumn`, `winter`. Second long term period\n\nlanduse_weights = {\n    \"residential_builtup\": 1.0,\n    \"other_builtup\": 1.0,\n    \"roads\": 0.5,\n    \"other_human_activity\": 0.1,\n    \"open_area\": 0.0,\n    \"forest\": 0.0,\n    \"water\": 0.0\n    }\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/16_tourism_stays_estimation/","title":"TourismStaysEstimation Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>tourism_stays_estimation.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Silver]\n...\ntime_segments_silver = ${Paths:silver_dir}/time_segments\ncell_connection_probabilities_data_silver = ${Paths:silver_dir}/cell_conn_probs\ngeozones_grid_map_data_silver = ${Paths:silver_dir}/geozones_grid_map\ntourism_stays_estimation_silver = ${Paths:silver_dir}/tourism_stays_estimation\n...\n</code></pre> <p>The expected parameters in <code>tourism_stays_estimation.ini</code> are as follows:  - clear_destination_directory: bool, whether to clear the output directory before running the component. Example: <code>True</code>.  - zoning_dataset_ids_list: list of str, the dataset ids of the zoning datasets to map time segments cell footprints to. Example: <code>'nuts'</code>.    - Special dataset names <code>INSPIRE_1km</code> and <code>INSPIRE_100m</code> map the stays to the corresponding level of grid ids using grid generation. These datasets do not need the <code>geozones_grid_map_data_silver</code> input data to be provided.  - min_duration_segment_m: int, the minimum duration of a segment in minutes to be considered as tourism stay. Example: <code>180</code>.  - functional_midnight_h: int, the hour of the day to consider as the functional midnight to mark overnight tourism stays. Example: <code>4</code>.  - min_duration_segment_night_m: int, the minimum duration of a segment in minutes to be considered as overnight tourism stay. Example: <code>300</code>.  - filter_ue_segments: bool, whether to filter out all segments of users who have inbound usual environment. Example: <code>False</code>.</p>"},{"location":"UserManual/configuration/1_Pipeline/16_tourism_stays_estimation/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = TourismStaysEstimation\n\n[InternalMigration]\ndata_period_start = 2023-01-07\ndata_period_end = 2023-01-11\n\nclear_destination_directory = true\nzoning_dataset_ids_list = ['nuts', 'INSPIRE_1km']\nmin_duration_segment_m = 180\nfunctional_midnight_h = 4\nmin_duration_segment_night_m = 300\nfilter_ue_segments = false\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/17_output_indicators/","title":"OutputIndicators Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>output_indicators.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. For example:</p> <pre><code>[Paths.Bronze]\n# Optional dataset for Inbound Tourism use case, contains deduplication and mno-to-target-population factors\n# for nationalities that visit the local country\ninbound_estimation_factors_bronze = ${Paths:bronze_dir}/inbound_estimation_factors\n\n[Paths.Silver]\n# Only needed for Present Population and Usual Environment use cases, when spatial aggregation is to be performed\n# for a custom zoning (not INSPIRE grid)\ngeozones_grid_map_data_silver = ${Paths:silver_dir}/geozones_grid_map\n\n# Present Population\npresent_population_silver = ${Paths:silver_dir}/present_population\n\n# Usual Environment\naggregated_usual_environments_silver = ${Paths:ue_dir}/aggregated_usual_environment\n\n# Internal Migration\ninternal_migration_silver = ${Paths:ue_dir}/internal_migration\n\n# Inbound Tourism\ntourism_geozone_aggregations_silver = ${Paths:silver_dir}/tourism_geozone_aggregations\ntourism_trip_aggregations_silver = ${Paths:silver_dir}/tourism_trip_aggregations\n\n# Outbound Tourism\ntourism_outbound_aggregations_silver = ${Paths:silver_dir}/tourism_outbound_aggregations\n\n\n[Paths.Gold]\n# Present Population\npresent_population_zone_gold = ${Paths:gold_dir}/present_population_zone_gold\n\n# Usual Environment\naggregated_usual_environments_zone_gold = ${Paths:gold_dir}/aggregated_usual_environments_zone_gold\n\n# Internal Migration\ninternal_migration_gold = ${Paths:gold_dir}/internal_migration\n\n# Inbound Tourism\ntourism_geozone_aggregations_gold = ${Paths:gold_dir}/tourism_geozone_aggregations\ntourism_trip_aggregations_gold = ${Paths:gold_dir}/tourism_trip_aggregations\n\n# Outbound Tourism\ntourism_outbound_aggregations_gold = ${Paths:gold_dir}/tourism_outbound_aggregations\n</code></pre> <p>The OutputIndicators component can be executed for one of five use cases, processing the output data objects of that use case and preparing them to be exported out of the MNO premises. The execution is configured through the <code>output_indicators.ini</code> file, that contains the following sections.</p>"},{"location":"UserManual/configuration/1_Pipeline/17_output_indicators/#general-section","title":"General section","text":"<p>Under the general section, the user may specify for which use case outputs the process will be executed and other parameters that are common to all use cases. The following parameters are expected:  - use_case: string, it specifies the use case whose outputs will be processed by the component. Currently there are five accepted values:    - <code>PresentPopulationEstimation</code> for the Present Population use case    - <code>UsualEnvironmentAggregation</code> for the Usual Environment use case    - <code>InternalMigration</code> for the Internal Migration use case    - <code>TourismStatisticsCalculation</code> for the Inbound Tourism use case    - <code>TourismOutboundStatisticsCalculation</code> for the Outbound Tourism use case  - clear_destination_directory: boolean, whether to delete all previous results in the output directory before running the component.</p> <ul> <li>deduplication_factor_local: float, this factor will be used to multiply the appropriate values computed at device level to take into account the possibility that a real person might own more than one device. In particular, this factor will be applied to indicators that are based on devices that are part of the local MNO network. Example: <code>0.98</code>.</li> <li>deduplication_factor_default_inbound: float, this factor will be used to multiply the appropriate values computed at device level to take into account the possibility that a real person might own more than one device. In particular, this factor will be applied to indicators that are based on devices that belong to an MNO of a country other than the local one, and will only be used as a default value for countries whose factor is not specified in the InboundEstimationFactors data object. Example: <code>0.95</code>.</li> <li>mno_to_target_population_factor_local: float, this factor will be used to multiply the (now deduplicated) population value computed at device level to represent the total target population. In particular, this factor will be applied to indicators that are based on devices that are part of the local MNO network .Example: <code>5.4</code>.</li> <li>mno_to_target_population_factor_default_inbound: float, this factor will be used to multiply the (now deduplicated) population value computed at device level to represent the total target population. In particular, this factor will be applied to indicators that are based on devices that belong to an MNO of a country other than the local one, and will only be used as a default value for countries whose factor is not specified in the InboundEstimationFactors data object. Example: <code>4.2</code>.</li> <li>k: integer, it indicates the threshold value to select what records should have k-anonymity applied to them, that is, all records that have a population value strictly lower than k. Example: <code>5</code>.</li> <li>anonymity_type: string, it indicates what k-anonymity metholodogy to employ. If it is set to <code>delete</code>, it will delete all records that have a population value strictly lower than k. If it is set to <code>obfuscate</code>, it will replace all population values strictly lower than k by the negative value <code>-1</code>. Example: <code>delete</code>.</li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/17_output_indicators/#present-population-section","title":"Present Population section","text":"<p>In this section, the user may configure exactly what part of the output of the present population use case will be processed by the component.  - zoning_dataset_id: string, ID of the zones dataset to use to aggregate the data from grid level to zone level. The user may also specify one of the reserved dataset names <code>INSPIRE_100m</code> and <code>INSPIRE_1km</code>, in which case no <code>geozones_grid_map_data_silver</code> needs to be read.  - hierarchical_levels: comma-separated list of positive integers, they are the hierarchical levels of the zones dataset used that should be processed by this component. If the zones dataset was not hierarchical, this should just take the value <code>1</code>. The list of levels is ignored and set to be <code>1</code> if a different value is encountered when zoning_dataset_id is one of the reserved dataset names. Example: <code>1,2,3</code>.  - start_date: string, in <code>YYYY-MM-DD</code> format, indicates the starting date (inclusive) for which the present population data should be processed by this component. Example: <code>2023-01-01</code>.  - end_date: string, in <code>YYYY-MM-DD</code> format, indicates the ending date (inclusive) for which the present population data should be processed by this component. Example: <code>2023-01-01</code>.</p>"},{"location":"UserManual/configuration/1_Pipeline/17_output_indicators/#usual-environment-section","title":"Usual Environment section","text":"<p>In this section, the user may configure exactly what part of the output of the usual environment use case (also encompassing the home location use case) will be processed by the component.  - zoning_dataset_id: string, ID of the zones dataset to use to aggregate the data from grid level to zone level. The user may also specify one of the reserved dataset names <code>INSPIRE_100m</code> and <code>INSPIRE_1km</code>, in which case no <code>geozones_grid_map_data_silver</code> needs to be read.  - hierarchical_levels: comma-separated list of positive integers, they are the hierarchical levels of the zones dataset used that should be processed by this component. If the zones dataset was not hierarchical, this should just take the value <code>1</code>. The list of levels is ignored and set to be <code>1</code> if a different value is encountered when zoning_dataset_id is one of the reserved dataset names. Example: <code>1,2,3</code>.  - labels: comma-separated list of strings, they indicate what usual environment labels should be processed by this component. Allowed values are <code>home</code>, <code>work</code>, <code>ue</code>. Example: <code>ue, home, work</code>.  - start_month: string, in <code>YYYY-MM</code> format, it indicates the first month of the period used to compute the specific usual environment dataset that the user desires to process through this component. Example: <code>2023-01</code>.  - end_month: string, in <code>YYYY-MM</code> format, it indicates the last month of the period used to compute the specific usual environment dataset that the user desires to process through this component. Example: <code>2023-03</code>.  - season: string, value of the season of the usual environment dataset that the user desires to process through this component. Allowed values are: <code>all</code>, <code>spring</code>, <code>summer</code>, <code>autumn</code>, <code>winter</code>. Example: <code>all</code>.</p> <p>## Internal Migration section In this section, the user may configure exactly what part of the output of the home location use case will be processed by the component.  - zoning_dataset_id: string, ID of the zones dataset that the internal migration data refers to.  - hierarchical_levels: comma-separated list of positive integers, they are the hierarchical levels of the zones dataset used that should be processed by this component. If the zones dataset was not hierarchical, this should just take the value <code>1</code>. Example: <code>1,2,3</code>.  - start_month_previous: string, in <code>YYYY-MM</code> format, it indicates the first month of the first long-term period used to compute the internal migration. Example: <code>2023-01</code>.  - end_month_previous: string, in <code>YYYY-MM</code> format, it indicates the last month of the first long-term period used to compute the internal migration. Example: <code>2023-06</code>.  - season_previous: string, value of the season of the first long-term period used to compute the internal migration. Allowed values are: <code>all</code>, <code>spring</code>, <code>summer</code>, <code>autumn</code>, <code>winter</code>. Example: <code>all</code>.  - start_month_new: string, in <code>YYYY-MM</code> format, it indicates the first month of the second long-term period used to compute the internal migration. Example: <code>2023-07</code>.  - end_month_new: string, in <code>YYYY-MM</code> format, it indicates the last month of the second long-term period used to compute the internal migration. Example: <code>2023-12</code>.  - season_new: string, value of the season of the second long-term period used to compute the internal migration. Allowed values are: <code>all</code>, <code>spring</code>, <code>summer</code>, <code>autumn</code>, <code>winter</code>. Example: <code>all</code>.</p> <p>## Inbound Tourism section In this section, the user may configure exactly what part of the output of the inbound tourism use case will be processed by the component.  - zoning_dataset_id: string, ID of the zones dataset that the inbound tourism data refers to.  - hierarchical_levels: comma-separated list of positive integers, they are the hierarchical levels of the zones dataset used that should be processed by this component. If the zones dataset was not hierarchical, this should just take the value <code>1</code>. The list of levels is ignored and set to be <code>1</code> if a different value is encountered when zoning_dataset_id is one of the reserved dataset names. Example: <code>1,2,3</code>.  - start_month: string, in <code>YYYY-MM</code> format, it indicates the first month of the month interval of inbound tourism data that the user desires to process through this component. Example: <code>2023-01</code>.  - end_month: string, in <code>YYYY-MM</code> format, it indicates the last month of the month interval of inbound tourism data that the user desires to process through this component. Example: <code>2023-03</code>.</p> <p>## Outbound Tourism section In this section, the user may configure exactly what part of the output of the outbound tourism use case will be processed by the component.  - start_month: string, in <code>YYYY-MM</code> format, it indicates the first month of the month interval of outbound tourism data that the user desires to process through this component. Example: <code>2023-01</code>.  - end_month: string, in <code>YYYY-MM</code> format, it indicates the last month of the month interval of outbound tourism data that the user desires to process through this component. Example: <code>2023-03</code>.</p> <p>## Configuration example</p> <pre><code>[Spark]\nsession_name = OutputIndicators\n\n[OutputIndicators]\n\nuse_case = InternalMigration\n\nclear_destination_directory = False\ndeduplication_factor_local = 0.95\ndeduplication_factor_default_inbound = 0.95\nmno_to_target_population_factor_local = 5.4\nmno_to_target_population_factor_default_inbound = 4.2\nanonymity_type = obfuscate  # `obfuscate`, `delete`\nk = 5\n\n\n[OutputIndicators.InternalMigration]\n# Target InternalMigration dataset\nzoning_dataset_id = nuts  # ID of the zoning dataset\nhierarchical_levels = 1,2,3,4  # Hierarchical level(s) of the zoning dataset. Comma-separated list\n\nstart_month_previous = 2023-01  # Start month (inclusive) of the first long-term period\nend_month_previous = 2023-06  # End month (inclusive) of the first long-term period\nseason_previous = all  # Allowed values: `all`, `spring`, `summer`, `autumn`, `winter`. First long term period\n\nstart_month_new = 2023-07  # Start month (inclusive) of the second long-term period\nend_month_new = 2023-12  # End month (inclusive) of the second long-term period\nseason_new = all  # Allowed values: `all`, `spring`, `summer`, `autumn`, `winter`. Second long term period\n\n\n[OutputIndicators.PresentPopulationEstimation]\n# Target InternalMigration dataset\nzoning_dataset_id = nuts  # ID of the zoning dataset\nhierarchical_levels = 1,2,3  # Hierarchical level(s) of the zoning dataset. Comma-separated list\n\nstart_date = 2023-01-03  # start date (inclusive)\nend_date = 2023-01-03  # end date (inclusive)\n\n\n[OutputIndicators.UsualEnvironmentAggregation]\nzoning_dataset_id = nuts  # ID of the zoning dataset\nhierarchical_levels = 1,2,3  # Hierarchical level(s) of the zoning dataset. Comma-separated list\nlabels = ue, home, work  # Allowed values: `ue`, `home`, `work`. Comma-separated list\nstart_month = 2023-01  # Start month (inclusive)\nend_month = 2023-06  # End month (inclusive)\nseason = all\n\n\n[OutputIndicators.TourismStatisticsCalculation]\nzoning_dataset_id = nuts\nhierarchical_levels = 1,2,3\n\nstart_month = 2023-01\nend_month = 2023-02\n\n\n[OutputIndicators.TourismOutboundStatisticsCalculation]\nstart_month = 2023-01\nend_month = 2023-02\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/1_inspire_grid_generation/","title":"InspireGridGeneration Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>grid_generation.ini</code>. In <code>general_config.ini</code> to execute the component specify all paths to its four corresponding data objects (input + output). Example: </p> <pre><code>[Paths.Silver]\ngrid_data_silver = ${Paths:silver_dir}/grid\n</code></pre> <p>In grid_generation.ini parameters are as follows: </p> <ul> <li> <p>clear_destination_directory - boolean, if True, the component will clear all the data in output paths.</p> </li> <li> <p>grid_mask - string, the mask to be used for grid generation. It can be either 'extent' or 'polygon'. If 'extent' is chosen, the extent parameter should be provided. If 'polygon' is chosen, reference country iso2 code should be provided.</p> </li> <li> <p>extent - list, the extent of the grid to be generated if 'extent' is chosen spatial mask type. The format is [min_lon, min_lat, max_lon, max_lat].</p> </li> <li> <p>reference_country - string, iso2 country code to use as a spatial mask for grid generation.</p> </li> <li> <p>country_buffer - integer, buffer distance to extend country polygon for grid generation.</p> </li> <li> <p>grid_generation_partition_size - integer, the size of the partition to be used for grid generation as a size of a side of grid subsquare. Default value is 500 grid tiles, so generation will be done with 500 by 500 grid subsquares.</p> </li> <li> <p>grid_processing_partition_quadkey_level - integer, the level of quadkey for resulted grid partitioning. Default value is level 8.  </p> </li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/1_inspire_grid_generation/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = InspireGridGeneration\n\n[InspireGridGeneration]\nclear_destination_directory = True\ngrid_mask = polygon # extent or polygon\nextent = [-4.5699,39.9101,-2.8544,40.9416] # [min_lon, min_lat, max_lon, max_lat]\nreference_country = ES # ISO A2 code\ncountry_buffer = 10000 # meters\ngrid_generation_partition_size = 500\ngrid_processing_partition_quadkey_level = 8\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/20_multimno_aggregation/","title":"MultiMNOAggregation Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>multimno_aggregation.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. In this aggregation component that brings together the data computed individually in several MNOs, the number of input data paths depends on the number of different MNOs that the user worked with. This number will be specified below in the configuration section related to the relevant use case. Example (data from two MNOs):</p> <pre><code>...\n[Paths.Gold]\n# Present Population UC data objects\nsingle_mno_1_present_population_zone_gold = ${Paths:gold_dir}/present_population_MNO_1\nsingle_mno_2_present_population_zone_gold = ${Paths:gold_dir}/present_population_MNO_2\nmultimno_aggregated_present_population_zone_gold = ${Paths:gold_dir}/multimno_present_population\n\n# Usual Environment / Home location UC data objects\nsingle_mno_1_usual_environment_zone_gold = ${Paths:gold_dir}/usual_environment_MNO_1\nsingle_mno_2_usual_environment_zone_gold = ${Paths:gold_dir}/usual_environment_MNO_2\nmultimno_aggregated_usual_environment_zone_gold = ${Paths:gold_dir}/multimno_usual_environment\n\n# Internal migration UC data objects\nsingle_mno_1_internal_migration_gold = ${Paths:gold_dir}/internal_migration_MNO_1\nsingle_mno_2_internal_migration_gold = ${Paths:gold_dir}/internal_migration_MNO_2\nmultimno_internal_migration_gold = ${Paths:gold_dir}/multimno_internal_migration\n\n# Inbound tourism UC data objects\nsingle_mno_1_inbound_tourism_zone_aggregations_gold = ${Paths:gold_dir}/inbound_tourism_zone_aggregations_MNO_1\nsingle_mno_2_inbound_tourism_zone_aggregations_gold = ${Paths:gold_dir}/inbound_tourism_zone_aggregations_MNO_2\nmultimno_inbound_tourism_zone_aggregations_gold = ${Paths:gold_dir}/multimno_inbound_tourism_zone_aggregations\n\n# Outbound tourism UC data objects\nsingle_mno_1_outbound_tourism_aggregations_gold = ${Paths:gold_dir}/outbound_tourism_aggregations_MNO_1\nsingle_mno_2_outbound_tourism_aggregations_gold = ${Paths:gold_dir}/outbound_tourism_aggregations_MNO_2\nmultimno_outbound_tourism_aggregations_gold = ${Paths:gold_dir}/multimno_outbound_tourism_aggregations\n...\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/20_multimno_aggregation/#general-section","title":"General section","text":"<p>Under the general section, the user may specify for which use case's outputs the process will be executed and other parameters that are common to all use cases. The following parameters are expected:  - use_case: string, it specifies the use case whose outputs will be processed by the component. Currently there are five accepted values:    - <code>PresentPopulationEstimation</code> for the Present Population use case    - <code>UsualEnvironmentAggregation</code> for the Usual Environment use case    - <code>InternalMigration</code> for the Internal Migration use case    - <code>TourismStatisticsCalculation</code> for the Inbound Tourism use case    - <code>TourismOutboundStatisticsCalculation</code> for the Outbound Tourism use case  - clear_destination_directory: boolean, whether to delete all previous results in the output directory before running the component.  - number_of_single_mnos: positive integer equal or greater than <code>2</code>, it indicates the number of MNOs whose data will be aggregated by this component. Example: <code>2</code>. This number also dictates:    - The number of input data paths that need to be specified in the general configuration. The name format of these configuration parameters is <code>single_mno_i_present_population_zone_gold</code> (for the Present Population use case; name changes depending on the use case, see example above), where <code>i</code> must be an integer between <code>1</code> and number_of_single_mnos. All input data path parameters are mandatory. See the configuration example above for the case where there are two MNOs.    - The number of MNO factors. The name format of these configuration parameters is <code>single_mno_i_factor</code>, where <code>i</code> must be an integer between <code>1</code> and number_of_single_mnos. All factor parameters are mandatory and are described below.  - single_mno_<code>i</code>_factor: float between 0 and 1, this parameter (for all values <code>i</code> between <code>1</code> and number_of_single_mnos) represents the weight that the indicator produced by this specific MNO will have in the final aggregation. Example: <code>0.6</code>. See the configuration example below for the case where number_of_single_mnos is equal to <code>2</code>.</p> <p>## Present Population section In this section, the user may configure exactly what part of the output of the present population use case will be processed by the component.  - zoning_dataset_id: string, ID of the zones dataset that the present population data refers to.  - hierarchical_levels: comma-separated list of positive integers, they are the hierarchical levels of the zones dataset used that should be processed by this component. If the zones dataset was not hierarchical, this should just take the value <code>1</code>. The list of levels is ignored and set to be <code>1</code> if a different value is encountered when zoning_dataset_id is one of the reserved dataset names. Example: <code>1,2,3</code>.  - start_date: string, in <code>YYYY-MM-DD</code> format, indicates the starting date (inclusive) for which the present population data should be processed by this component. Example: <code>2023-01-01</code>.  - end_date: string, in <code>YYYY-MM-DD</code> format, indicates the ending date (inclusive) for which the present population data should be processed by this component. Example: <code>2023-01-01</code>.</p> <p>## Usual Environment section In this section, the user may configure exactly what part of the output of the usual environment use case (also encompassing the home location use case) will be processed by the component.  - zoning_dataset_id: string, ID of the zones dataset that the usual environment data refers to.  - hierarchical_levels: comma-separated list of positive integers, they are the hierarchical levels of the zones dataset used that should be processed by this component. If the zones dataset was not hierarchical, this should just take the value <code>1</code>. The list of levels is ignored and set to be <code>1</code> if a different value is encountered when zoning_dataset_id is one of the reserved dataset names. Example: <code>1,2,3</code>.  - labels: comma-separated list of strings, they indicate what usual environment labels should be processed by this component. Allowed values are <code>home</code>, <code>work</code>, <code>ue</code>. Example: <code>ue, home, work</code>.  - start_month: string, in <code>YYYY-MM</code> format, it indicates the first month of the period used to compute the specific usual environment dataset that the user desires to process through this component. Example: <code>2023-01</code>.  - end_month: string, in <code>YYYY-MM</code> format, it indicates the last month of the period used to compute the specific usual environment dataset that the user desires to process through this component. Example: <code>2023-03</code>.  - season: string, value of the season of the usual environment dataset that the user desires to process through this component. Allowed values are: <code>all</code>, <code>spring</code>, <code>summer</code>, <code>autumn</code>, <code>winter</code>. Example: <code>all</code>.</p> <p>## Internal Migration section In this section, the user may configure exactly what part of the output of the home location use case will be processed by the component.  - zoning_dataset_id: string, ID of the zones dataset that the internal migration data refers to.  - hierarchical_levels: comma-separated list of positive integers, they are the hierarchical levels of the zones dataset used that should be processed by this component. If the zones dataset was not hierarchical, this should just take the value <code>1</code>. Example: <code>1,2,3</code>.  - start_month_previous: string, in <code>YYYY-MM</code> format, it indicates the first month of the first long-term period used to compute the internal migration. Example: <code>2023-01</code>.  - end_month_previous: string, in <code>YYYY-MM</code> format, it indicates the last month of the first long-term period used to compute the internal migration. Example: <code>2023-06</code>.  - season_previous: string, value of the season of the first long-term period used to compute the internal migration. Allowed values are: <code>all</code>, <code>spring</code>, <code>summer</code>, <code>autumn</code>, <code>winter</code>. Example: <code>all</code>.  - start_month_new: string, in <code>YYYY-MM</code> format, it indicates the first month of the second long-term period used to compute the internal migration. Example: <code>2023-07</code>.  - end_month_new: string, in <code>YYYY-MM</code> format, it indicates the last month of the second long-term period used to compute the internal migration. Example: <code>2023-12</code>.  - season_new: string, value of the season of the second long-term period used to compute the internal migration. Allowed values are: <code>all</code>, <code>spring</code>, <code>summer</code>, <code>autumn</code>, <code>winter</code>. Example: <code>all</code>.</p> <p>## Inbound Tourism section In this section, the user may configure exactly what part of the output of the inbound tourism use case will be processed by the component.  - zoning_dataset_id: string, ID of the zones dataset that the inbound tourism data refers to.  - hierarchical_levels: comma-separated list of positive integers, they are the hierarchical levels of the zones dataset used that should be processed by this component. If the zones dataset was not hierarchical, this should just take the value <code>1</code>. The list of levels is ignored and set to be <code>1</code> if a different value is encountered when zoning_dataset_id is one of the reserved dataset names. Example: <code>1,2,3</code>.  - start_month: string, in <code>YYYY-MM</code> format, it indicates the first month of the month interval of inbound tourism data that the user desires to process through this component. Example: <code>2023-01</code>.  - end_month: string, in <code>YYYY-MM</code> format, it indicates the last month of the month interval of inbound tourism data that the user desires to process through this component. Example: <code>2023-03</code>.</p> <p>## Outbound Tourism section In this section, the user may configure exactly what part of the output of the outbound tourism use case will be processed by the component.  - start_month: string, in <code>YYYY-MM</code> format, it indicates the first month of the month interval of outbound tourism data that the user desires to process through this component. Example: <code>2023-01</code>.  - end_month: string, in <code>YYYY-MM</code> format, it indicates the last month of the month interval of outbound tourism data that the user desires to process through this component. Example: <code>2023-03</code>.</p>"},{"location":"UserManual/configuration/1_Pipeline/20_multimno_aggregation/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = MultiMNOAggregation\n\n\n[MultiMNOAggregation]\nuse_case = PresentPopulationEstimation\nclear_destination_directory = True\nnumber_of_single_mnos = 2\nsingle_mno_1_factor = 0.6\nsingle_mno_2_factor = 0.4\n\n\n[MultiMNOAggregation.PresentPopulationEstimation]\n# Target PresentPopulationEstimation dataset\nzoning_dataset_id = nuts  # ID of the zoning dataset\nhierarchical_levels = 1,2,3,4  # Hierarchical level(s) of the zoning dataset. Comma-separated list\nstart_date = 2023-01-08  # start date (inclusive)\nend_date = 2023-01-08  # end date (inclusive)\n\n\n[MultiMNOAggregation.UsualEnvironmentAggregation]\n# Target UsualEnvironmentAggregation dataset\nzoning_dataset_id = nuts  # ID of the zoning dataset\nhierarchical_levels = 1,2,3,4  # Hierarchical level(s) of the zoning dataset. Comma-separated list\nlabels = ue, home, work  # Allowed values: `ue`, `home`, `work`. Comma-separated list\nstart_month = 2023-01  # Start month (inclusive)\nend_month = 2023-01  # End month (inclusive)\nseason = all  # Allowed values: `all`, `spring`, `summer`, `autumn`, `winter`.\n\n\n[MultiMNOAggregation.InternalMigration]\nzoning_dataset_id = nuts  # ID of the zoning dataset\nhierarchical_levels = 1  # Hierarchical level(s) of the zoning dataset. Comma-separated list\n\nstart_month_previous = 2023-01  # Start month (inclusive) of the first long-term period\nend_month_previous = 2023-06  # End month (inclusive) of the first long-term period\nseason_previous = all  # Allowed values: `all`, `spring`, `summer`, `autumn`, `winter`. First long term period\n\nstart_month_new = 2023-07  # Start month (inclusive) of the second long-term period\nend_month_new = 2023-12  # End month (inclusive) of the second long-term period\nseason_new = all  # Allowed values: `all`, `spring`, `summer`, `autumn`, `winter`. Second long term period\n\n\n[MultiMNOAggregation.TourismStatisticsCalculation]\nzoning_dataset_id = nuts\nhierarchical_levels = 1,2,3\n\nstart_month = 2023-01\nend_month = 2023-01\n\n\n[MultiMNOAggregation.TourismOutboundStatisticsCalculation]\nstart_month = 2023-01\nend_month = 2023-02\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/21_tourism_statistics_calculation/","title":"TourismStatisticsCalculation Configuration","text":"<p>To initialise and run the component two configs are used: <code>general_config.ini</code> and <code>tourism_statistics_calculation.ini</code>. </p>"},{"location":"UserManual/configuration/1_Pipeline/21_tourism_statistics_calculation/#general-configuration","title":"General configuration","text":"<p>In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <p><pre><code>...\n[Paths.Silver]\n...\ntourism_stays_silver = ${Paths:silver_dir}/tourism_stays\nmcc_iso_timezones_data_bronze = ${Paths:bronze_dir}/mcc_iso_timezones\ntourism_trips_silver = ${Paths:silver_dir}/tourism_trips\ntourism_geozone_aggregations_silver = ${Paths:silver_dir}/tourism_geozone_aggregations\ntourism_trip_aggregations_silver = ${Paths:silver_dir}/tourism_trip_aggregations\n...\n</code></pre> <code>tourism_stays_silver</code>, <code>mcc_iso_timezones_data_bronze</code> and <code>tourism_trips_silver</code> are input data paths.</p> <p><code>tourism_geozone_aggregations_silver</code>, <code>tourism_trip_aggregations_silver</code> and <code>tourism_trips_silver</code> are output data paths.</p> <p><code>tourism_trips_silver</code> is both the input and output path and it does not need to contain data for the component to be executed, but its data can be used as input if it does.</p>"},{"location":"UserManual/configuration/1_Pipeline/21_tourism_statistics_calculation/#configuration-parameters","title":"Configuration parameters","text":"<p>The configuration file <code>tourism_statistics_calculation.ini</code> has three sections: </p> <p><code>Spark</code> and <code>Logging</code> contain generic session name and logging parameters. </p> <p>The section <code>TourismStatisticsCalculation</code> controls component logic and contains the following parameters:</p> <ul> <li>data_period_start: YYYY-MM format string. Indicates the first month for which the component will generate results for. Example: <code>2023-01</code>.  </li> <li>data_period_end: YYYY-MM format string. Indicates the last month for which the component will generate results for. Example: <code>2023-02</code>.</li> <li>clear_destination_directory: Boolean. Indicates if existing results should be deleted before execution. If True, existing data in paths <code>tourism_geozone_aggregations_silver</code> and <code>tourism_trip_aggregations_silver</code> will be deleted before calculations start. Example: <code>True</code>.</li> <li>delete_existing_trips: Boolean. Indicates if existing trips (from previous executions) should be deleted before execution. If True, existing data in path <code>tourism_trips_silver</code> will be deleted before calculations start. If they are not deleted, they may be used as input data during the execution. Example: <code>False</code>.</li> <li>zoning_dataset_ids_and_levels_list: List of zoning dataset name and hierarchical level pairs. Each entry pair in the list should specify the name of a zoning dataset and the list of the hierarchical levels to calculate results for. For single-level datasets (such as <code>INSPIRE_1KM</code>), the hierarchical level should be <code>[1]</code>. Example:<code>[('test_dataset',[1,2,3])]</code>.</li> <li>max_trip_gap_h: Integer. Maximum number of hours allowed between two stays for them to be possibly marked as part of the same trip. Additionally is used to determine the size of the look-forward window to retrieve next month entries when processing monthly data. Example: <code>24</code>.</li> <li>max_visit_gap_h: Integer. Maxmimum number of hours allowed between two stays for them to be possibly marked as part of the same visit. Example: <code>24</code>.</li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/21_tourism_statistics_calculation/#configuration-example","title":"Configuration example","text":"<pre><code>[Logging]\nlevel = DEBUG\n\n[Spark]\nsession_name = TourismStatisticsCalculation\n\n[TourismStatisticsCalculation]\ndata_period_start = 2023-01\ndata_period_end = 2023-02\nclear_destination_directory = true\ndelete_existing_trips = False\nzoning_dataset_ids_and_levels_list = [('test_dataset',[1,2,3])]\nmax_trip_gap_h = 24\nmax_visit_gap_h = 24\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/22_tourism_outbound_statistics_calculation/","title":"TourismOutboundStatisticsCalculation Configuration","text":"<p>To initialise and run the component two configs are used: <code>general_config.ini</code> and <code>tourism_outbound_statistics_calculation.ini</code>. </p>"},{"location":"UserManual/configuration/1_Pipeline/22_tourism_outbound_statistics_calculation/#general-configuration","title":"General configuration","text":"<p>In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <p><pre><code>...\n[Paths.Silver]\n...\ntime_segments_silver = ${Paths:silver_dir}/time_segments\nmcc_iso_timezones_data_bronze = ${Paths:bronze_dir}/mcc_iso_timezones\ntourism_outbound_trips_silver = ${Paths:silver_dir}/tourism_outbound_trips\ntourism_outbound_aggregations_silver = ${Paths:silver_dir}/tourism_outbound_aggregations\n...\n</code></pre> <code>time_segments_silver</code>, <code>mcc_iso_timezones_data_bronze</code> and <code>tourism_outbound_trips_silver</code> are input data paths.</p> <p><code>tourism_outbound_trips_silver</code> and <code>tourism_outbound_aggregations_silver</code> are output data paths.</p> <p><code>tourism_outbound_trips_silver</code> is both the input and output path and it does not need to contain data for the component to be executed, but its data can be used as input if it does.</p>"},{"location":"UserManual/configuration/1_Pipeline/22_tourism_outbound_statistics_calculation/#configuration-parameters","title":"Configuration parameters","text":"<p>The component configuration file <code>tourism_outbound_statistics_calculation.ini</code> has three sections:</p> <p><code>Spark</code> and <code>Logging</code> contain generic session name and logging parameters. </p> <p>The section <code>TourismOutboundStatisticsCalculation</code> controls component logic and contains the following parameters:</p> <ul> <li>data_period_start: YYYY-MM format string. Indicates the first month for which the component will generate results for. Example: <code>2023-01</code>.  </li> <li>data_period_end: YYYY-MM format string. Indicates the last month for which the component will generate results for. Example: <code>2023-02</code>.</li> <li>clear_destination_directory: Boolean. Indicates if existing results should be deleted before execution. If True, existing data in path <code>tourism_outbound_aggregations_silver</code> will be deleted before calculations start. Example: <code>True</code>.</li> <li>delete_existing_trips: Boolean. Indicates if existing trips (from previous executions) should be deleted before execution. If True, existing data in path <code>tourism_outbound_trips_silver</code> will be deleted before calculations start. If they are not deleted, they may be used as input data during the execution. Example: <code>False</code>.</li> <li>max_outbound_trip_gap_h: Integer. Maximum number of hours allowed between two time segments for them to be possibly marked as part of the same trip. Additionally is used to determine the size of the look-forward window to retrieve next month entries when processing monthly data. Example: <code>24</code>.</li> <li>min_duration_segment_m: Integer. Minimum duration in minutes for a time segment to be used as input data. Example: <code>72</code>.</li> <li>functional_midnight_h: Integer. Hour of day acting as the functional midnight. Example: <code>4</code>.</li> <li>min_duration_segment_night_m: Integer. Minimum duration in minutes for a time segment to be possibly marked as an overnight segment. Example: <code>200</code>.</li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/22_tourism_outbound_statistics_calculation/#configuration-example","title":"Configuration example","text":"<pre><code>[Logging]\nlevel = DEBUG\n\n[Spark]\nsession_name = TourismOutboundStatisticsCalculation\n\n[TourismOutboundStatisticsCalculation]\ndata_period_start = 2023-01\ndata_period_end = 2023-02\nclear_destination_directory = true\ndelete_existing_trips = False\nmax_outbound_trip_gap_h = 72\nmin_duration_segment_m = 180\nfunctional_midnight_h = 4\nmin_duration_segment_night_m = 200\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/2_event_data_cleaning/","title":"EventCleaning Configuration","text":"<p>To initialise and run the component two configs are used - general_config.ini and event_cleaning.ini.  In general_config.ini to execute Event Cleaning component specify all paths to its four corresponding data objects (input + output). Example: </p> <pre><code>[Paths.Bronze]\nevent_data_bronze = ${Paths:bronze_dir}/mno_events\n\n[Paths.Silver]\nevent_data_silver = ${Paths:silver_dir}/mno_events\nevent_syntactic_quality_metrics_by_column = ${Paths:silver_dir}/event_syntactic_quality_metrics_by_column\nevent_syntactic_quality_metrics_frequency_distribution = ${Paths:silver_dir}/event_syntactic_quality_metrics_frequency_distribution\n</code></pre> <p>In event_cleaning.ini parameters are as follows: </p> <ul> <li> <p>data_period_start - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-01), the date from which start Event Cleaning</p> </li> <li> <p>data_period_end - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-05), the date till which perform Event Cleaning</p> </li> <li> <p>data_folder_date_format - string, to what string format convert dates so they match the naming of input data folders (it is expected that input data is divided into separate folders for each date of research period). Example: if you know that data for 2023-01-01 is stored in f\"{bronze_event_path}/20230101\", then the format to convert 2023-01-01 date to 20230101 string using strftimewill be %Y%m%d</p> </li> <li> <p>spark_data_folder_date_format - string, as for data_folder_date_format it depends on folder\u2019s naming pattern of input data but since datetime patterns in pyspark and strftime differ, it is a separate config param. Used to convert string to datetype when creating date column in frequency distribution table </p> </li> <li> <p>timestamp_format - str, expected string format of timestamp column when converting it to timestamp type</p> </li> <li> <p>do_bounding_box_filtering- boolean, True/False, decides whether to apply bounding box filtering</p> </li> <li> <p>bounding_box - dictionary, with following keys 'min_lon', 'max_lon', 'min_lat', and 'max_lat' and integer/float values, to specify coordinates of bounding box, within which records should fall, make sure that records and bounding box are in the same src </p> </li> <li> <p>number_of_partitions - an integer, that determines the value of the modulo operator. This value will determine the number expected partitions as to the last partitioning column user_id_modulo. This value does not affect the number of folders in terms of other partitioning columns (day, month, year).</p> </li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/2_event_data_cleaning/#configuration-example","title":"Configuration example","text":"<pre><code>[EventCleaning]\ndata_period_start = 2023-01-01\ndata_period_end = 2023-01-05\ndata_folder_date_format = %Y%m%d\nspark_data_folder_date_format = yyyyMMdd\ntimestamp_format = yyyy-MM-dd'T'HH:mm:ss\ndo_bounding_box_filtering = True\ndo_same_location_deduplication = True\nbounding_box = {\n    'min_lon': -180,\n    'max_lon': 180,\n    'min_lat': -90,\n    'max_lat': 90\n    }\nnumber_of_partitions = 256\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/3_network_cleaning/","title":"NetworkCleaning Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>network_cleaning.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Bronze]\n...\nnetwork_data_bronze = ${Paths:bronze_dir}/mno_network\n...\n\n[Paths.Silver]\n...\nnetwork_data_silver = ${Paths:silver_dir}/mno_network\nnetwork_syntactic_quality_metrics_by_column = ${Paths:silver_quality_metrics_dir}/network_syntactic_quality_metrics_by_column\n...\n</code></pre> <p>The expected parameters in <code>network_cleaning.ini</code> are as follows:</p> <ul> <li>latitude_min: float, minimum accepted latitude (WGS84) for the latitude of cells in the input data. Values lower than this will be treated as out of bounds/range.</li> <li>latitude_max: float, maximum accepted latitude (WGS84) for the latitude of cells in the input data. Values higher than this will be treated as out of bounds/range.</li> <li>longitude_min: float, minimum accepted longitude (WGS84) for the longitude of cells in the input data. Values lower than this will be treated as out of bounds/range.</li> <li>longitude_max: float, minimum accepted longitude (WGS84) for the longitude of cells in the input data. Values higher than this will be treated as out of bounds/range.</li> <li>cell_type_options: comma-separated list of strings, this parameter indicates the accepted values in the <code>cell_type</code> field. Other values will be treated as out of bounds/range. Example: <code>macrocell, microcell, picocell</code>.</li> <li>technology_options: comma-separated list of strings, this parameter indicates the accepted values in the <code>technology</code> field. Other values will be treated as out of bounds/range. Example: <code>5G, LTE, UMTS, GSM</code>.</li> <li>data_period_start: string, format should be the \"yyyy-MM-dd\" (e.g., <code>2023-01-01</code>), the first date for which data will be processed by the component. All dates between this one and the specified in <code>data_period_end</code> will be processed (both inclusive).</li> <li>data_period_end: string, format should be \"yyyy-MM-dd\" (e.g., <code>2023-01-09</code>), the last date for which data will be processed by the component. All dates between the specified in <code>data_period_start</code> and this one will be processed (both inclusive).</li> <li>valid_date_timestamp_format: string, the timestamp format that is expected to be in the input network data and that will be parsed with PySpark using thiis format. Example: <code>yyyy-MM-dd'T'HH:mm:ss</code></li> <li>frequent_error_criterion: string, criterion to use when computing the most frequent errors encountered. It can take two values: <code>absolute</code> if one wants to find the top k most frequent errors (e.g., <code>k=10</code>); or <code>percentage</code> if one wants to find the most frequent errors that represent <code>k</code> percentage of all errors found. Example: <code>percentage</code>.</li> <li>top_k_errors: integer if <code>frequent_error_criterion=absolute</code> or float if <code>top_k_errors</code> if <code>frequent_error_criterion=percentage</code>, represents what portion of the most frequent errors to save. Example: <code>10</code>.</li> <li>do_cell_cgi_check: boolean, default: False. If set to True, cell_id's that do not follow the CGI format will be removed.</li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/3_network_cleaning/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = NetworkCleaning\n\n[NetworkCleaning]\n# Bounding box\nlatitude_min = 40.352\nlatitude_max = 40.486\nlongitude_min = -3.751\nlongitude_max = -3.579\n\ncell_type_options = macrocell, microcell, picocell\ntechnology_options = 5G, LTE, UMTS, GSM\n# Left- and right-inclusive date range for the data to be read\ndata_period_start = 2023-01-01\ndata_period_end = 2023-01-09\n\nvalid_date_timestamp_format = yyyy-MM-dd'T'HH:mm:ss\n\nfrequent_error_criterion = percentage  # allowed values: `absolute`, `percentage`\ntop_k_errors = 40.5\ndo_cell_cgi_check = False\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/4_cell_footprint_estimation/","title":"CellFootprintEstimation Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>cell_footprint_estimation.ini</code>. In <code>general_config.ini</code> to execute the component specify all paths to its corresponding data objects (input + output). Example: </p> <pre><code>[Paths.Silver]\nnetwork_data_silver = ${Paths:silver_dir}/mno_network\ngrid_data_silver = ${Paths:silver_dir}/grid\ncell_footprint_data_silver = ${Paths:silver_dir}/cell_footprint\n</code></pre> <p>In cell_footprint_estimation.ini parameters are as follows: </p> <ul> <li> <p>clear_destination_directory - boolean, if True, the component will clear all the data in output paths.</p> </li> <li> <p>data_period_start - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-01), the first date of the date interval for which the cell footprint estimation will be executed.</p> </li> <li> <p>data_period_end - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-05), the last date of the date interval for which the cell footprint estimation will be executed.</p> </li> <li> <p>logistic_function_steepness - float, the steepness of the logistic function used to estimate the signal dominance (cell footprint) value from signal strength. Example: 0.2.</p> </li> <li> <p>logistic_function_midpoint - float, the midpoint of the logistic function used to estimate the signal dominance (cell footprint) value from signal strength. Example: -92.5.</p> </li> <li> <p>use_elevation - boolean, if True, the elevation data will be used for signal strength modeling. If False, the elevation will be set to 0.</p> </li> <li> <p>do_azimuth_angle_adjustments - boolean, if True, signal strengths values will be adjusted based on azimuth and antenna horizontal beam widths for directional cells. If False, no adjustments will be made.</p> </li> <li> <p>do_elevation_angle_adjustments - boolean, if True, signal strengths values will be adjusted based on tilt and antenna vertical beam widths for directional cells. If False, no adjustments will be made.</p> </li> <li> <p>cartesian_crs - integer, the coordinate reference system (CRS) to use for the cartesian coordinates. Example: 3035.</p> </li> <li> <p>do_percentage_of_best_sd_pruning - boolean, if True, the cells per grid tile with signal dominance values that are strictly lower than the threshold percentage of best signal dominance will be pruned. If False, no pruning of this method will be performed.</p> </li> <li> <p>percentage_of_best_sd_threshold - float between 0 and 100, the threshold percentage from the best signal dominance value. For example, if this threshold is set to 90 (percent) and the highest signal dominance in a tile is 1.0, then all signal dominances with values strictly lower than 0.9 will be discarded (since $0.9 = 90\\% \\text{ of } 1.0$). Example: 90.</p> </li> <li> <p>do_max_cells_per_tile_pruning - boolean, if True, there will be a maximum number of cells per grid tile that will be kept, other cells will be pruned. If False, no pruning of this method will be performed.</p> </li> <li> <p>max_cells_per_grid_tile - integer, the maximum number of cells per grid tile. Example: 10.</p> </li> <li> <p>do_dynamic_coverage_range_calculation - boolean, if True, the component will calculate the dynamic coverage range for each cell based on signal dominance threshold value. If False, default range will be used. Might be useful to reduce computational load, but might take longer to calculate.</p> </li> <li> <p>do_sd_threshold_pruning - boolean, if True, the cells with signal dominance values that equal or lower than the threshold will be pruned. If False, no pruning of this method will be performed.</p> </li> <li> <p>signal_dominance_threshold - float, the threshold value for signal dominance. Example: 0.01.</p> </li> <li> <p>coverage_range_line_buffer - integer, the buffer distance of cell range line in meters for dynammic range calculation. The value should be set based on reference grid resolution. Example: 50.</p> </li> <li> <p>repartition_number - integer, the number of partitions to use for the repartitioning of the data during dynamic range calculations. Example: 10.</p> </li> <li> <p>default_cell_physical_properties - dictionary, the default physical properties of the cell types. These properties will be assigned to cells of corresponding type if the properties are not found in the network topology data. If cell types are not peresent in network topology data, the default type properties will be assigned to all cells.</p> </li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/4_cell_footprint_estimation/#configuration-example","title":"Configuration example","text":"<pre><code>[CellFootprintEstimation]\nclear_destination_directory = True\ndata_period_start = 2023-01-01\ndata_period_end = 2023-01-01\nlogistic_function_steepness = 0.2\nlogistic_function_midpoint = -92.5\n\nuse_elevation = False\ndo_azimuth_angle_adjustments = True\ndo_elevation_angle_adjustments = True\ncartesian_crs = 3035\n\ndo_percentage_of_best_sd_pruning = False\npercentage_of_best_sd_threshold = 90 # percentage\n\ndo_max_cells_per_tile_pruning = False\nmax_cells_per_grid_tile = 100\n\ndo_dynamic_coverage_range_calculation = False\nsignal_dominance_threshold = 0.01\ncoverage_range_line_buffer = 50\nrepartition_number = 10\n\ndo_sd_threshold_pruning = True\n\ndefault_cell_physical_properties = {\n    'macrocell': {\n        'power': 10,\n        'range': 10000,\n        'path_loss_exponent': 3.75,\n        'antenna_height': 30,\n        'elevation_angle': 5,\n        'vertical_beam_width': 9,\n        'horizontal_beam_width': 65,\n        'azimuth_signal_strength_back_loss': -30,\n        'elevation_signal_strength_back_loss': -30,\n    },\n    'microcell': {\n        'power': 5,\n        'range': 1000,\n        'path_loss_exponent': 6.0,\n        'antenna_height': 8,\n        'elevation_angle': 5,\n        'vertical_beam_width': 9,\n        'horizontal_beam_width': 65,\n        'azimuth_signal_strength_back_loss': -30,\n        'elevation_signal_strength_back_loss': -30,\n    },\n    'default': {\n        'power': 5,\n        'range': 5000,\n        'path_loss_exponent': 3.75,\n        'antenna_height': 8,\n        'elevation_angle': 5,\n        'vertical_beam_width': 9,\n        'horizontal_beam_width': 65,\n        'azimuth_signal_strength_back_loss': -30,\n        'elevation_signal_strength_back_loss': -30,\n        }\n    }\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/5_cell_connection_probability/","title":"CellConnectionProbabilityEstimation Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>cell_connection_probability_estimation</code>.ini. In <code>general_config.ini</code> to execute the component specify all paths to its four corresponding data objects (input + output). Example: </p> <pre><code>[Paths.Bronze]\nevent_data_bronze = ${Paths:bronze_dir}/mno_events\n\n[Paths.Silver]\ncell_footprint_data_silver = ${Paths:silver_dir}/cell_footprint\ngrid_data_silver = ${Paths:silver_dir}/grid\ncell_connection_probabilities_data_silver = ${Paths:silver_dir}/cell_conn_probs\n# only if used\nenriched_grid_data_silver = ${Paths:silver_dir}/grid_enriched\n</code></pre> <p>In cell_connection_probability_estimation.ini parameters are as follows:  - clear_destination_directory - boolean, if True, the component will clear all the data in output paths.</p> <ul> <li> <p>partition_number - integer, the number of partitions to use for the Spark DataFrame. The higher the number, the more memory is required.</p> </li> <li> <p>data_period_start - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-01), the date from which start Event Cleaning</p> </li> <li> <p>data_period_end - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-05), the date till which perform Event Cleaning</p> </li> <li> <p>use_land_use_prior - boolean, if True, the land use prior will be used for cell connection posterior probability estimation. If False, the land use prior will not be used, only connection probability based on cell footprint will be estimated.</p> </li> <li> <p>landuse_prior_weights - dictionary, keys are land use types, values are weights for these landuse types. The land use types are: </p> <ul> <li>residential_builtup</li> <li>other_builtup</li> <li>roads</li> <li>other_human_activity</li> <li>open_area</li> <li>forest</li> <li>water</li> </ul> </li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/5_cell_connection_probability/#configuration-example","title":"Configuration example","text":"<pre><code>[CellConnectionProbabilityEstimation]\ndata_period_start = 2023-01-01\ndata_period_end = 2023-01-15\nuse_land_use_prior = False\n\nlanduse_prior_weights = {\n    \"residential_builtup\": 1.0,\n    \"other_builtup\": 1.0,\n    \"roads\": 0.5,\n    \"other_human_activity\": 0.1,\n    \"open_area\": 0.0,\n    \"forest\": 0.1,\n    \"water\": 0.0\n    }\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/5_cell_footprint_intersections/","title":"CellFootprintIntersections Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>cell_footprint_intersections.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <p><pre><code>[Paths.Silver]\n...\n# input data object\ncell_footprint_data_silver = ${Paths:silver_dir}/cell_footprint\n\n# output data objects\ncell_to_group_data_silver = ${Paths:silver_dir}/cell_to_group\ngroup_to_tile_data_silver = ${Paths:silver_dir}/group_to_tile\n...\n</code></pre> The expected parameters in <code>cell_footprint_intersections.ini</code> are as follows:</p> <ul> <li>clear_destination_directory: bool, whether to clear the output directory before running the component. Example: <code>True</code>.</li> <li>data_period_start: string, in <code>YYYY-MM-DD</code> format, it indicates the first date of the date interval for which to compute the cell footprint intersection components. Example: <code>2023-01-01</code>.</li> <li>data_period_end: string, in <code>YYYY-MM-DD</code> format, it indicates the last date of the date interval for which to compute the cell footprint intersection components. Example: <code>2023-01-03</code>.</li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/5_cell_footprint_intersections/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = CellFootprintIntersections\n\n[CellFootprintIntersections]\nclear_destination_directory = True\ndata_period_start = 2023-01-01\ndata_period_end = 2023-01-03\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/6_event_semantic_cleaning/","title":"SemanticCleaning Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>event_semantic_cleaning.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Silver]\n...\nnetwork_data_silver = ${Paths:silver_dir}/mno_network\nevent_data_silver_flagged = ${Paths:silver_dir}/mno_events_flagged\nevent_device_semantic_quality_metrics = ${Paths:silver_quality_metrics_dir}/semantic_quality_metrics\n...\n</code></pre> <p>The expected parameters in <code>event_semantic_cleaning.ini</code> are as follows: - data_period_start: string, format should be the one specified <code>data_period_format</code> (e.g., <code>2023-01-01</code> for <code>%Y-%m-%d</code>), the first date for which data will be processed by the component. All dates between this one and the specified in <code>data_period_end</code> will be processed (both inclusive). - data_period_end: string, format should be \"yyyy-MM-dd\" (e.g., <code>2023-01-09</code> for <code>%Y-%m-%d</code>), the last date for which data will be processed by the component. All dates between the specified in <code>data_period_start</code> and this one will be processed (both inclusive). - data_period_format: string, it indicates the format expected in <code>data_period_start</code> and <code>data_period_end</code>. For example, use <code>%Y-%m-%d</code> for the usual \"2023-01-09\" format separated by <code>-</code>. - semantic_min_distance_m: float, minimum distance (in metres) between two consecutive events above which they will be considered for flagging as suspicious or incorrect location. Example: <code>10000</code>. - semantic_min_speed_m_s: float, minimum mean speed (in metres per second) between two consecutive events above whihc they will be considered for flagging as suspicious or incorrect location. Example: <code>55</code>. - do_different_location_deduplication: boolean, True/False. Determines whether to flag duplicates with different location information (cases where a single user has one or more rows with identical timestamp values, but non-identical values in any other columns).</p>"},{"location":"UserManual/configuration/1_Pipeline/6_event_semantic_cleaning/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = SemanticCleaning\n\n[SemanticCleaning]\ndata_period_start = 2023-01-01\ndata_period_end = 2023-01-09\ndate_format = %Y-%m-%d\n\nsemantic_min_distance_m = 10000\nsemantic_min_speed_m_s = 55\n\ndo_different_location_deduplication = True\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/7_device_activity_statistics/","title":"DeviceActivityStatistics Configuration","text":"<p>To initialise and run the component two configs are used - general_config.ini and device_activity_statistics.ini.  In general_config.ini to execute Device Activity Statistics component specify all paths to its three corresponding data objects (input + output). The local timezone must also be specified in the general config. Example: </p> <pre><code>[Timezone]\nlocal_timezone = UTC\n\n[Paths.Silver]\n# Data\nnetwork_data_silver = ${Paths:silver_dir}/mno_network\n\ndevice_activity_statistics = ${Paths:silver_dir}/device_activity_statistics\n</code></pre> <p>In device_activity_statistics.ini parameters are as follows: </p> <ul> <li> <p>data_period_start - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-01), the date from which start Event Cleaning</p> </li> <li> <p>data_period_end - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-05), the date till which perform Event Cleaning</p> </li> <li> <p>clear_destination_directory - boolean, whether to empty the destination directory before running or not</p> </li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/7_device_activity_statistics/#configuration-example","title":"Configuration example","text":"<pre><code>[DeviceActivityStatistics]\nclear_destination_directory = True\ndata_period_start = 2023-01-01\ndata_period_end = 2023-01-04\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/8_continuous_time_segmentation/","title":"ContinuousTimeSegmentation Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>time_segments.ini</code>. In <code>general_config.ini</code> to execute the component specify all paths to its four corresponding data objects (input + output). Example: </p> <pre><code>[Paths.Silver]\nevent_data_silver_flagged = ${Paths:silver_dir}/mno_events_flagged\nevent_cache = ${Paths:silver_dir}/mno_events_cache\ncell_intersection_groups_data_silver = ${Paths:silver_dir}/cell_intersection_groups\ntime_segments_silver = ${Paths:silver_dir}/time_segments\n</code></pre> <p>In time_segments.ini parameters are as follows: </p> <ul> <li> <p>data_period_start - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-01), the date from which start Event Cleaning</p> </li> <li> <p>data_period_end - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-05), the date till which perform Event Cleaning</p> </li> <li> <p>clear_time_segments_directory - boolean, if True, the component will delete any existing time segments before calculation. If False, existing time segments will be used as input data where relevant. </p> </li> <li> <p>event_error_flags_to_include - list of integers, the list of error flags that should be included in the time segments processing. Default value is [0], so only events with no errors are included.</p> </li> <li> <p>min_time_stay_s - integer, the minimum dwell time in seconds for a time segments to be considered as a \"stay\". Default value is 15 minutes.</p> </li> <li> <p>max_time_missing_stay_s - integer, maximum time difference between events to be considered a \u201cstay\u201d. If larger, the time segment will be marked \u201cunknown\u201d. Default value is 12 hours to support devices being offline at home or work addresses.</p> </li> <li> <p>max_time_missing_move_s - integer, maximum time difference between events to be considered a \u201cmove\u201d. If larger, the time segment will be marked \u201cunknown\u201d. Default value is 2 hours.</p> </li> <li> <p>max_time_missing_abroad_s - integer, maximum time difference between events to be considered a \u201cabroad\u201d. If larger, the time segment will be marked \u201cunknown\u201d. Default value is 72 hours.</p> </li> <li> <p>pad_time_s - integer, half the size of an isolated time segment: between two \u201cunknowns\u201d time segments. It expands the isolated event in time, by \u201cpadding\u201d from the \u201cunknown\u201d time segments on both sides. Default value is 5 minutes.</p> </li> <li> <p>domains_to_include - list[string], List of event domains that will be processed. Allowed values \"inbound\", \"domestic\", \"outbound\".</p> </li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/8_continuous_time_segmentation/#configuration-example","title":"Configuration example","text":"<pre><code>[ContinuousTimeSegmentation]\ndata_period_start = 2023-01-01\ndata_period_end = 2023-01-05\n\nis_first_run = true\nevent_error_flags_to_include = [0]\n\nmin_time_stay_s = 900\nmax_time_missing_stay_s = 43200\nmax_time_missing_move_s = 7200\nmax_time_missing_abroad_s = 259200\npad_time_s = 300\n\ndomains_to_include = [\"inbound\", \"domestic\", \"outbound\"]\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/9_present_population_estimation/","title":"PresentPopulationEstimation Configuration","text":"<p>To initialise and run the component two configs are used -  <code>general_config.ini</code> and component\u2019s config <code>present_population_estimation.ini</code>. In  <code>general_config.ini</code> all paths to the data objects used by PresentPopulationEstimation component shall be specified. An example with the specified paths is shown below:</p> <pre><code>[Paths.Silver]\n...\ngrid_data_silver = ${Paths:silver_dir}/grid\ncell_connection_probabilities_data_silver = ${Paths:silver_dir}/cell_connection_probabilities_data_silver\nevent_data_silver_flagged = ${Paths:silver_dir}/mno_events_flagged\npresent_population_silver = ${Paths:silver_dir}/present_population\n...\n</code></pre> <p>Below there is a description of the sub component\u2019s config  - <code>present_population_estimation.ini</code>. </p> <p>Parameters are as follows:</p> <p>Under  <code>[PresentPopulationEstimation]</code> config section: </p> <ul> <li>data_period_start - string, format should be \u201cyyyy-MM-dd HH:mm:ss\u201c (e.g. 2023-01-08 12:30:00). Determines when the first time point is generated.</li> <li>data_period_end - string, format should be \u201cyyyy-MM-dd HH:mm:ss\u201c (e.g. 2023-01-08 12:30:00). No time points can be generated after this time. A time point can be generated at this exact time.</li> <li>time_point_gap_s - integer, in seconds. Determines the interval between two time points. Starting from <code>data_period_start</code>, one time point is generated after each <code>time_point_gap_s</code> seconds until <code>data_period_end</code> is reached.</li> <li>tolerance_period_s - integer, in seconds. Determines the size of the temporal window of each time point. Only events within this distance from the time point are included in the results calculation of that point. </li> <li>max_iterations - integer. Maximum number of iteration allowed for the Bayesian process for each time point.</li> <li>min_difference_threshold - float. Minumum difference between Bayesian process prior and posterior population estimates needed to continue iterating the process.</li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/9_present_population_estimation/#configuration-example-grid-level-aggregation","title":"Configuration example: grid-level aggregation","text":"<pre><code>[Logging]\nlevel = DEBUG\n\n[Spark]\nsession_name = PresentPopulationEstimationGrid\n\n[PresentPopulationEstimation]\ndata_period_start = 2023-01-01 00:00:00 # Starting bound when to create time points. The first time point is created at this timestamp. \ndata_period_end = 2023-01-02 00:00:00 # Ending bound when to create time points. No time points are generated later than this timestamp. A time point can happen to be generated on this timestamp, but this is not always the case.\ntime_point_gap_s = 43200 # space between consecutive time points\ntolerance_period_s = 3600 # Maximum allowed time difference for an event to be included in a time point\nnr_of_user_id_partitions = 128 # Total number of user_id_modulo partitions. TODO should be a global conf value\nnr_of_user_id_partitions_per_slice = 32 # Number of user_id_modulo partitions to process at one time\nmax_iterations = 20 # Number of iterations allowed for the Bayesian process\nmin_difference_threshold = 10000 # Minimum total difference between Bayesian process prior and posterior needed to continue processing \n</code></pre>"},{"location":"UserManual/configuration/2_Optional/1_grid_enrichment/","title":"GridEnrichment Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>grid_enrichment.ini</code>. In <code>general_config.ini</code> to execute the component specify all paths to its corresponding data objects (input + output). </p> <pre><code>[Paths.Bronze]\ntransportation_data_bronze = ${Paths:bronze_dir}/spatial/transportation\nlanduse_data_bronze = ${Paths:bronze_dir}/spatial/landuse\n[Paths.Silver]\ngrid_data_silver = ${Paths:silver_dir}/grid\nenriched_grid_data_silver = ${Paths:silver_dir}/grid_enriched\n</code></pre> <p>In grid_enrichment.ini parameters are as follows: </p> <ul> <li> <p>clear_destination_directory - boolean, if True, the component will clear all the data in output paths.</p> </li> <li> <p>quadkey_batch_size - integer, the number of quadkeys to process in a single batch. The higher the number, the more memory is required.</p> </li> <li> <p>do_landuse_enrichment - boolean, if True, the component will enrich the grid with landuse prior probabilities and Path Loss Exponent environment coefficient.</p> </li> <li> <p>transportation_category_buffer_m - dictionary, buffer distance for each transportation category in meters. Used to convert transportation lines to polygons.</p> </li> <li> <p>do_elevation_enrichment - boolean, if True, the component will enrich the grid with elevation data. Not implemented yet.</p> </li> </ul>"},{"location":"UserManual/configuration/2_Optional/1_grid_enrichment/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = InspireGridGeneration\n\n[InspireGridGeneration]\n[Spark]\nsession_name = GridEnrichment\n\n[GridEnrichment]\nclear_destination_directory = True\nquadkey_batch_size = 2\n\ndo_landuse_enrichment = True\ntransportation_category_buffer_m = {\n    \"primary\": 30,\n    \"secondary\": 15,\n    \"tertiary\": 5,\n    \"pedestrian\": 5,\n    \"railroad\": 15,\n    \"unknown\": 2\n    }\n\ndo_elevation_enrichment = False\n</code></pre>"},{"location":"UserManual/configuration/2_Optional/2_geozones_to_grid_mapping/","title":"GeozonesGridMapping Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>geozones_grid_mapping.ini</code>. In <code>general_config.ini</code> to execute the component specify all paths to its corresponding data objects (input + output). </p> <pre><code>[Paths.Bronze]\ngeographic_zones_data_bronze = ${Paths:bronze_dir}/spatial/geographic_zones \nadmin_units_data_bronze = ${Paths:bronze_dir}/spatial/admin_units \n[Paths.Silver]\ngrid_data_silver = ${Paths:silver_dir}/grid\ngeozones_grid_map_data_silver = ${Paths:silver_dir}/geozones_grid_map\n</code></pre> <p>In geozones_grid_mapping.ini parameters are as follows: </p> <ul> <li> <p>clear_destination_directory - boolean, if True, the component will clear all the data in output paths.</p> </li> <li> <p>zoning_type - string, type of zoning data object to be used for mapping. Possible values are \"admin\" and \"other\".</p> </li> <li> <p>dataset_ids - list, ids of zonning datasets to use for mapping. Grid mapping will be done for each dataset separately.</p> </li> </ul>"},{"location":"UserManual/configuration/2_Optional/2_geozones_to_grid_mapping/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = GeozonesGridMapping\n\n[GeozonesGridMapping]\nclear_destination_directory = True\nzoning_type = other # admin or other\ndataset_ids = ['nuts'] # list of dataset ids to map grid to\n</code></pre>"},{"location":"UserManual/configuration/3_QualityWarnings/1_network_quality_warnings/","title":"NetworkQualityWarnings Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>network_quality_warnings.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Silver]\n...\nnetwork_syntactic_quality_metrics_by_column = ${Paths:silver_quality_metrics_dir}/network_syntactic_quality_metrics_by_column\nnetwork_syntactic_quality_warnings_log_table = ${Paths:silver_quality_warnings_dir}/network_syntactic_quality_warnings_log_table\nnetwork_syntactic_quality_warnings_line_plot_data = ${Paths:silver_quality_warnings_dir}/network_syntactic_quality_warnings_line_plot_data\nnetwork_syntactic_quality_warnings_pie_plot_data = ${Paths:silver_quality_warnings_dir}/network_syntactic_quality_warnings_pie_plot_data\n...\n</code></pre> <p>The expected parameters in <code>network_quality_warnings.ini</code> are as follows: - date_of_study: string, format should be the one specified <code>date_format</code> (e.g., <code>2023-01-01</code> for <code>%Y-%m-%d</code>), the first date for which data will be processed by the component. All dates between this one and the specified in <code>data_period_end</code> will be processed (both inclusive). - date_format: string, it indicates the format expected in <code>date_of_study</code>. For example, use <code>%Y-%m-%d</code> for the usual \"2023-01-01\" format separated by <code>-</code>. - lookback_period: string, it indicates the length of the lookback period used to compare the metrics of the date of study with past data volume and error rates. Three possible values are accepted: <code>week</code>, <code>month</code>, and <code>quarter</code>. - thresholds: dictionary, indicating the different thresholds to be used for raising warnings.</p>"},{"location":"UserManual/configuration/3_QualityWarnings/1_network_quality_warnings/#thresholds","title":"Thresholds","text":"<p>The thresholds parameter is a dictionary of dictionaries, used to indicate the different thresholds to be used for each type of warning. In the case that one of the thresholds is missing, a default value will be used instead. The default values are contained in <code>multimno.core.constants.network_default_thresholds.NETWORK_DEFAULT_THRESHOLDS</code>.</p> <p>Ihe dictionary structure is as follows: - <code>\"SIZE_RAW_DATA\"</code>: refers to the size of the input data.   - <code>\"OVER_AVERAGE\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the input raw file is greater than this threshold, in percentage different, when compared to the average over the lookback period. By default, the value is <code>30</code>.   - <code>\"UNDER_AVERAGE\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the input raw file is lower than this threshold, in percentage different, when compared to the average over the lookback period. By default, the value is <code>30</code>.   - <code>\"VARIABILITY\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the input raw file differs from its average over the lookback period by more than its standard deviation multiplied by this threshold. This is, this value is a threshold of the number of standard deviations than a value can differ from its average over the lookback period, either over or under the average. By default, the value is <code>2</code>.   - <code>\"ABS_VALUE_UPPER_LIMIT\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the input raw file is greater than this value. By default, it is equal to the threshold calculated from the <code>VARIABILITY</code> parameter over the average.   - <code>\"ABS_VALUE_LOWER_LIMIT\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the input raw file is greater than this value. By default, it is equal to the threshold calculated from the <code>VARIABILITY</code> parameter under the average.</p> <ul> <li><code>\"SIZE_CLEAN_DATA\"</code>: refers to the size of the output data.</li> <li><code>\"OVER_AVERAGE\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the output processed file is greater than this threshold, in percentage different, when compared to the average over the lookback period. By default, the value is <code>30</code>.</li> <li><code>\"UNDER_AVERAGE\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the output processed file is lower than this threshold, in percentage different, when compared to the average over the lookback period. By default, the value is <code>30</code>.</li> <li><code>\"VARIABILITY\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the output processed file differs from its average over the lookback period by more than its standard deviation multiplied by this threshold. This is, this value is a threshold of the number of standard deviations than a value can differ from its average over the lookback period, either over or under the average. By default, the value is <code>2</code>.</li> <li><code>\"ABS_VALUE_UPPER_LIMIT\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the output processed file is greater than this value. by default, it is equal to the threshold calculated from the <code>VARIABILITY</code> parameter over the average.</li> <li> <p><code>\"ABS_VALUE_LOWER_LIMIT\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the output processed file is greater than this value. By default, it is equal to the threshold calculated from the <code>VARIABILITY</code> parameter under the average.</p> </li> <li> <p><code>\"TOTAL_ERROR_RATE\"</code>: refers to the percentage of rows preserved from the input file, i.e., the rows that passed the cleaning/check procedure.</p> </li> <li><code>\"OVER_AVERAGE\"</code>: integer or float, indicates the threshold for a warning to be raised when the error rate is greater than this threshold, in percentage different, when compared to the average over the lookback period. By default, the value is <code>30</code>.</li> <li><code>\"VARIABILITY\"</code>: integer or float, indicates the threshold for a warning to be raised when the error rate differs from its average over the lookback period by more than its standard deviation multiplied by this threshold. This is, this value is a threshold of the number of standard deviations than a value can differ from its average over the lookback period, obly over the average. By default, the value is <code>2</code>.</li> <li> <p><code>\"ABS_VALUE_UPPER_LIMIT\"</code>: integer or float, indicates the threshold for a warning to be raised when the error rate is greater than this value. By default, the value is <code>20</code>.</p> </li> <li> <p><code>\"Missing_value_RATE\"</code>: refers to the percentage of missing/null values of a given field in the input data. The value of this dictionary key is a dictionary where the keys are field names, and the values are a dictionary containing the thresholds form this error type and field.</p> </li> <li>Admitted keys (i.e., fields) are: <code>cell_id</code>, <code>valid_date_start</code>, <code>latitude</code>, <code>longitude</code>, <code>altitude</code>, <code>antenna_height</code>, <code>directionality</code>, <code>azimuth_angle</code>, <code>elevation_angle</code>, <code>horizontal_beam_width</code>, <code>vertical_beam_width</code>, <code>power</code>, <code>frequency</code>, <code>technology</code>, and <code>cell_type</code>.</li> <li> <p>Each key (i.e., field) has the following thresholds:</p> <ul> <li><code>\"AVERAGE\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate is greater than this threshold, in percentage different, when compared to the average over the lookback period. By default, the value is <code>30</code> for <code>cell_id</code>, <code>latitude</code>, and <code>longitude</code>, and <code>60</code> otherwise.</li> <li><code>\"VARIABILITY\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate differs from its average over the lookback period by more than its standard deviation multiplied by this threshold. This is, this value is a threshold of the number of standard deviations than a value can differ from its average over the lookback period, obly over the average. By default, the value is <code>2</code> for <code>cell_id</code>, <code>latitude</code>, and <code>longitude</code>, and <code>3</code> otherwise.</li> <li><code>\"ABS_VALUE_UPPER_LIMIT\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate is greater than this value. By default, the value is <code>20</code> for <code>cell_id</code>, <code>latitude</code>, and <code>longitude</code>, and <code>50</code> otherwise.</li> </ul> </li> <li> <p><code>\"Out_of_range_RATE\"</code>: refers to the percentage of out of bounds, out of range or invalid values of a given field in the input data. The value of this dictionary key is a dictionary where the keys are field names, and the values are a dictionary containing the thresholds form this error type and field.</p> </li> <li>Admitted keys (i.e., fields) are: <code>\"cell_id\"</code>, <code>\"latitude\"</code>, <code>\"longitude\"</code>, <code>\"antenna_height\"</code>, <code>\"directionality\"</code>, <code>\"azimuth_angle\"</code>, <code>\"elevation_angle\"</code>, <code>\"horizontal_beam_width\"</code>, <code>\"vertical_beam_width\"</code>, <code>\"power\"</code>, <code>\"frequency\"</code>, <code>\"technology\"</code>, and <code>\"cell_type\"</code>. Exceptionally, the <code>None</code> value is also accepted, referring to the specific error where <code>valid_date_end</code> is a point int time earlier than <code>valid_date_start</code>.</li> <li> <p>Each key has the following thresholds:</p> <ul> <li><code>\"AVERAGE\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate is greater than this threshold, in percentage different, when compared to the average over the lookback period. By default, the value is <code>20</code> for <code>cell_id</code>, <code>latitude</code>, and <code>longitude</code>, and <code>60</code> otherwise.</li> <li><code>\"VARIABILITY\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate differs from its average over the lookback period by more than its standard deviation multiplied by this threshold. This is, this value is a threshold of the number of standard deviations than a value can differ from its average over the lookback period, obly over the average. By default, the value is <code>2</code> for <code>cell_id</code>, <code>latitude</code>, and <code>longitude</code>, and <code>3</code> otherwise.</li> <li><code>\"ABS_VALUE_UPPER_LIMIT\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate is greater than this value. By default, the value is <code>20</code> for <code>cell_id</code>, <code>latitude</code>, and <code>longitude</code>, and <code>50</code> otherwise.</li> </ul> </li> <li> <p><code>\"Parsing_error_RATE\"</code>: refers to values that could not be parsed.</p> </li> <li><code>\"valid_date_start\"</code>:<ul> <li><code>\"AVERAGE\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate is greater than this threshold, in percentage different, when compared to the average over the lookback period. By default, the value is <code>60</code>.</li> <li><code>\"VARIABILITY\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate differs from its average over the lookback period by more than its standard deviation multiplied by this threshold. This is, this value is a threshold of the number of standard deviations than a value can differ from its average over the lookback period, obly over the average. By default, the value is <code>3</code>.</li> <li><code>\"ABS_VALUE_UPPER_LIMIT\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate is greater than this value. By default, the value is <code>50</code>.</li> </ul> </li> </ul>"},{"location":"UserManual/configuration/3_QualityWarnings/1_network_quality_warnings/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = NetworkQualityWarnings\n\n[NetworkQualityWarnings]\ndate_format = %Y-%m-%d\ndate_of_study = 2023-01-08\nlookback_period = week\n\n# All values must be numeric\n# Missing parameter will take the default value\n# Incorrect value will throw an error\nthresholds = {\n    \"SIZE_RAW_DATA\": {\n        \"OVER_AVERAGE\": 30,\n        \"UNDER_AVERAGE\": 30,\n        \"VARIABILITY\": 2,\n        \"ABS_VALUE_UPPER_LIMIT\": None,\n        \"ABS_VALUE_LOWER_LIMIT\": None,\n    },\n    \"SIZE_CLEAN_DATA\": {\n        \"OVER_AVERAGE\": 30,\n        \"UNDER_AVERAGE\": 30,\n        \"VARIABILITY\": 2,\n        \"ABS_VALUE_UPPER_LIMIT\": None,\n        \"ABS_VALUE_LOWER_LIMIT\": None,\n    },\n    \"TOTAL_ERROR_RATE\": {\n        \"OVER_AVERAGE\": 30,\n        \"VARIABILITY\": 2,\n        \"ABS_VALUE_UPPER_LIMIT\": 20,\n    },\n    \"Missing_value_RATE\": {\n        \"cell_id\": {\n            \"AVERAGE\": 30,\n            \"VARIABILITY\": 2,\n            \"ABS_VALUE_UPPER_LIMIT\": 20,\n        },\n        \"valid_date_start\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"latitude\": {\n            \"AVERAGE\": 30,\n            \"VARIABILITY\": 2,\n            \"ABS_VALUE_UPPER_LIMIT\": 20,\n        },\n        \"longitude\": {\n            \"AVERAGE\": 30,\n            \"VARIABILITY\": 2,\n            \"ABS_VALUE_UPPER_LIMIT\": 20,\n        },\n        \"altitude\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"antenna_height\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"directionality\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"azimuth_angle\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"elevation_angle\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"horizontal_beam_width\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"vertical_beam_width\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"power\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"frequency\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"technology\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"cell_type\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n    },\n    \"Out_of_range_RATE\": {\n        \"cell_id\": {\n            \"AVERAGE\": 30,\n            \"VARIABILITY\": 2,\n            \"ABS_VALUE_UPPER_LIMIT\": 20,\n        },\n        \"latitude\": {\n            \"AVERAGE\": 30,\n            \"VARIABILITY\": 2,\n            \"ABS_VALUE_UPPER_LIMIT\": 20,\n        },\n        \"longitude\": {\n            \"AVERAGE\": 30,\n            \"VARIABILITY\": 2,\n            \"ABS_VALUE_UPPER_LIMIT\": 20,\n        },\n        \"antenna_height\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"directionality\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"azimuth_angle\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"elevation_angle\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"horizontal_beam_width\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"vertical_beam_width\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"power\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"frequency\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"technology\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"cell_type\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        None: {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n    },\n    \"Parsing_error_RATE\": {\n        \"valid_date_start\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"valid_date_end\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        }\n    }\n    }\n</code></pre>"},{"location":"UserManual/configuration/3_QualityWarnings/2_event_quality_warnings/","title":"EventQualityWarnings Configuration","text":"<p>To initialise and run the component two configs are used -  <code>general_config.ini</code> and component\u2019s config (either  <code>event_cleaning_quality_warnings.ini</code> or  <code>event_deduplication_quality_warnings.ini</code>). In  <code>general_config.ini</code> to execute Event Quality Warnings component specify all paths to its corresponding data objects. Example with specified paths for both cases:</p> <pre><code>[Paths.Silver]\n...\n# for Event Cleaning Quality Warnings\nevent_syntactic_quality_metrics_by_column = ${Paths:silver_dir}/event_syntactic_quality_metrics_by_column\nevent_syntactic_quality_metrics_frequency_distribution = ${Paths:silver_dir}/event_syntactic_quality_metrics_frequency_distribution\nevent_syntactic_quality_warnings_log_table = ${Paths:silver_dir}/event_syntactic_quality_warnings_log_table\nevent_syntactic_quality_warnings_for_plots = ${Paths:silver_dir}/event_syntactic_quality_warnings_for_plots\n</code></pre> <p>Below there is a description of one of sub component\u2019s config  - <code>event_cleaning_quality_warnings.ini</code>. </p> <p>Parameters are as follows:</p> <p>Under <code>[EventQualityWarnings]</code> config section: </p> <ul> <li> <p>input_qm_by_column_path_key - string, key in Paths.Silver section in general config for a path to corresponding Quality Metrics By Column data</p> </li> <li> <p>input_qm_freq_distr_path_key - string, key in Paths.Silver section in general config for a path to corresponding Quality Metrics Frequency Distribution data</p> </li> <li> <p>output_qw_log_table_path_key - string, key in Paths.Silver section in general config for a path to write output Quality Warnings Log Table</p> </li> <li> <p>output_qw_for_plots_path_key - string, key in Paths.Silver section in general config for a path to write output Quality Warnings ForPLots</p> </li> <li> <p>data_period_start - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-08), the date from which start Event Quality Warnings, by now make sure the first day(s) of research period has enough previous data in in Quality Metrics Frequency Fistribution and Quality Metrics By Column </p> </li> <li> <p>data_period_end - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-15), the date till which perform Event Quality Warnings</p> </li> <li> <p>lookback_period - the length of lookback period, represented as string (could be either \u2018week' or 'month') which than will get its numeric representation in number of days</p> </li> <li> <p>do_size_raw_data_qw - boolean, whether perform QW checks on <code>initial_frequency</code> column in Quality Metrics Frequency Fistribution</p> </li> <li> <p>do_size_clean_data_qw - boolean, whether perform QW checks on <code>final_frequency</code> column in Quality Metrics Frequency Distribution</p> </li> <li> <p>data_size_tresholds - dictionary, with keys as thresholds names and values as corresponding thresholds values for  <code>do_size_raw_data_qw</code> and <code>do_size_clean_data_qw</code></p> </li> <li> <p>do_error_rate_by_date_qw - boolean, whether to perform QW checks on total error rate by <code>date</code></p> </li> <li> <p>do_error_rate_by_date_and_cell_qw -  boolean, whether to perform QW checks on total error rate by <code>date</code> and <code>cell_id</code></p> </li> <li> <p>do_error_rate_by_date_and_user_qw -  boolean, whether to perform QW checks on total error rate by <code>date</code> and <code>user_id</code></p> </li> <li> <p>do_error_rate_by_date_and_cell_user_qw - boolean, whether to perform QW checks on total error rate by <code>date</code>, <code>cell_id</code> and <code>user_id</code></p> </li> <li> <p>error_rate_tresholds - dictionary, with keys as thresholds names and values as corresponding thresholds values for  <code>do_error_rate_by_date_qw</code>, <code>do_error_rate_by_date_and_cell_qw</code>, <code>do_error_rate_by_date_and_user_qw</code>, and <code>do_error_rate_by_date_and_cell_user_qw</code></p> </li> <li> <p>error_type_qw_checks - dictionary, where the keys are names of error types (please see <code>multimno/core/constants/error_types.py</code> file) and values list of column names on which you want to perform QWs of the this error type. Example: during Event Cleaning three columns are checked for null values, if you want to check error rate of <code>missing_value</code> type for all mentioned columns specify them in the list. Some error types might have None for column names, which means that technically this kind or error do not belong to just one column but several (e.g. for <code>no_location</code> error three columns are used - cell_id, lat, lon): </p> </li> </ul> <p><pre><code>error_type_qw_checks = {\n    'missing_value': ['user_id', 'mcc', 'timestamp'],\n    'out_of_bounding_box':[None]\n    }\n</code></pre> If you do not intend to run QWs on some error type leave its corresponding list of columns empty.</p> <ul> <li>thresholds for each error_type &amp; column combination you want to compute QWs  - thresholds are combined in groups: each set of thresholds relevant to some error type is a separate config param of type dictionary, where keys are column names, values is another dictionary of structure: <code>threshold_name:threshold_value</code>. Example: </li> </ul> <p><pre><code>missing_value_thresholds = {\n    'user_id': {\"Missing_value_RATE_BYDATE_USER_AVERAGE\": 30,\n                \"Missing_value_RATE_BYDATE_USER_VARIABILITY\": 2,\n                \"Missing_value_RATE_BYDATE_USER_ABS_VALUE_UPPER_LIMIT\": 20\n               }, \n    'mcc': {\"Missing_value_RATE_BYDATE_MCC_AVERAGE\": 30,\n            \"Missing_value_RATE_BYDATE_MCC_VARIABILITY\": 2,\n            \"Missing_value_RATE_BYDATE_MCC_ABS_VALUE_UPPER_LIMIT\": 20\n           }, \n    'timestamp': {\"Missing_value_RATE_BYDATE_TIMESTAMP_AVERAGE\": 30,\n                  \"Missing_value_RATE_BYDATE_TIMESTAMP_VARIABILITY\": 2,\n                  \"Missing_value_RATE_BYDATE_TIMESTAMP_ABS_VALUE_UPPER_LIMIT\": 20\n                 }\n    }\nout_of_bounding_box_thresholds = {\n    None: {\"Out_of_bbox_RATE_BYDATE_AVERAGE\": 30,\n           \"Out_of_bbox_RATE_BYDATE_VARIABILITY\": 2,\n           \"Out_of_bbox_RATE_BYDATE_ABS_VALUE_UPPER_LIMIT\": 20\n          }\n    }\n</code></pre> Make sure, that for each column of interest specified in <code>error_type_qw_checks</code> there are corresponding thresholds. The order of thresholds is important and should be: <code>AVERAGE</code>, <code>VARIABILITY</code>, and <code>ABS_VALUE_UPPER_LIMIT</code> (at least by now all error type QWs follow the same logic and thus their computation is done within one function with ordered threshold arguments). Currently the code supports running QWs on following thresholds: </p> <p><pre><code># possible thresholds in event_cleaning_quality_warnings.ini\nmissing_value_thresholds\n\nout_of_admissible_values_thresholds\n\nnot_right_syntactic_format_thresholds\n\nno_location_thresholds\n\nno_domain_thresholds\n\nout_of_bounding_box_thresholds\n\ndeduplication_same_location_thresholds\n</code></pre> - clear_destination_directory - boolean, if True deletes all output of the Component in init stage</p>"},{"location":"UserManual/configuration/3_QualityWarnings/2_event_quality_warnings/#configuration-example","title":"Configuration example","text":"<pre><code>[EventQualityWarnings]\n# keys in Paths.Silver section in general config\ninput_qm_by_column_path_key = event_syntactic_quality_metrics_by_column\ninput_qm_freq_distr_path_key = event_syntactic_quality_metrics_frequency_distribution\noutput_qw_log_table_path_key = event_syntactic_quality_warnings_log_table\noutput_qw_for_plots_path_key = event_syntactic_quality_warnings_for_plots\n# BY NOW make sure that the first day(s) of research period has enough previous data\n# of df_qa_by_column and df_qa_freq_distribution \n# (e.g. staring from 2023-01-01, if period is a week and start period is 2023-01-08)\ndata_period_start = 2023-01-01\n# you can exceed max(df_qa_by_column.date) \n# although you will still get QWs for dates till max(df_qa_by_column.date), including\ndata_period_end = 2023-01-09\n# should be either week or month\nlookback_period = week\n# SIZE QA\ndo_size_raw_data_qw = True\ndo_size_clean_data_qw = True\ndata_size_tresholds = {\n    \"SIZE_RAW_DATA_BYDATE_VARIABILITY\": 3,\n    \"SIZE_RAW_DATA_BYDATE_ABS_VALUE_UPPER_LIMIT\": 10000000,\n    \"SIZE_RAW_DATA_BYDATE_ABS_VALUE_LOWER_LIMIT\": 0,\n    \"SIZE_CLEAN_DATA_BYDATE_VARIABILITY\": 3,\n    \"SIZE_CLEAN_DATA_BYDATE_ABS_VALUE_UPPER_LIMIT\": 10000000,\n    \"SIZE_CLEAN_DATA_BYDATE_ABS_VALUE_LOWER_LIMIT\": 0,\n    }\n# ERROR RATE QW\ndo_error_rate_by_date_qw = True\ndo_error_rate_by_date_and_cell_qw = False\ndo_error_rate_by_date_and_user_qw = True\ndo_error_rate_by_date_and_cell_user_qw = True\nerror_rate_tresholds = {\n    \"TOTAL_ERROR_RATE_BYDATE_OVER_AVERAGE\": 30,\n    \"TOTAL_ERROR_RATE_BYDATE_VARIABILITY\": 2,\n    \"TOTAL_ERROR_RATE_BYDATE_ABS_VALUE_UPPER_LIMIT\": 20,\n    \"ERROR_RATE_BYDATE_BYCELL_OVER_AVERAGE\": 30,\n    \"ERROR_RATE_BYDATE_BYCELL_VARIABILITY\": 2,\n    \"ERROR_RATE_BYDATE_BYCELL_ABS_VALUE_UPPER_LIMIT\": 20,\n    \"ERROR_RATE_BYDATE_BYUSER_OVER_AVERAGE\": 30,\n    \"ERROR_RATE_BYDATE_BYUSER_VARIABILITY\": 2,\n    \"ERROR_RATE_BYDATE_BYUSER_ABS_VALUE_UPPER_LIMIT\": 20,\n    \"ERROR_RATE_BYDATE_BYCELL_USER_OVER_AVERAGE\": 30,\n    \"ERROR_RATE_BYDATE_BYCELL_USER_VARIABILITY\": 2,\n    \"ERROR_RATE_BYDATE_BYCELL_USER_ABS_VALUE_UPPER_LIMIT\": 20,\n    }\n# ERROR TYPE QW\n# for each type of error (key), specified the colums you want to check, naming of columns must be oidentical to ColNames\n# if you do not want to run qw on some error_type leave the list empty\n# None - for no_location and out_of_bounding_box because they do not have more than one column used for this error_type\n# for more clarity please check event_cleaning.py\nerror_type_qw_checks = {\n    'missing_value': ['user_id', 'mcc', 'timestamp'],\n    'out_of_admissible_values': ['cell_id', 'mcc', 'mnc', 'plmn', 'timestamp'],\n    'not_right_syntactic_format': ['timestamp'], \n    'no_domain': [None],\n    'no_location':[None], \n    'out_of_bounding_box':[None],\n    'same_location_duplicate':[None]\n    }\n# for each dict_error_type_thresholds make sure you specified all relevant columns\n# the order of thresholds is important, should be: AVERAGE, VARIABILITY, and ABS_VALUE_UPPER_LIMIT\nmissing_value_thresholds = {\n    'user_id': {\"Missing_value_RATE_BYDATE_USER_AVERAGE\": 30,\n                \"Missing_value_RATE_BYDATE_USER_VARIABILITY\": 2,\n                \"Missing_value_RATE_BYDATE_USER_ABS_VALUE_UPPER_LIMIT\": 20\n               }, \n    'mcc': {\"Missing_value_RATE_BYDATE_MCC_AVERAGE\": 30,\n            \"Missing_value_RATE_BYDATE_MCC_VARIABILITY\": 2,\n            \"Missing_value_RATE_BYDATE_MCC_ABS_VALUE_UPPER_LIMIT\": 20\n           },\n    'mnc': {\"Missing_value_RATE_BYDATE_MNC_AVERAGE\": 30,\n            \"Missing_value_RATE_BYDATE_MNC_VARIABILITY\": 2,\n            \"Missing_value_RATE_BYDATE_MNC_ABS_VALUE_UPPER_LIMIT\": 20\n           }, \n    'plmn': {\"Missing_value_RATE_BYDATE_PLMN_AVERAGE\": 30,\n            \"Missing_value_RATE_BYDATE_PLMN_VARIABILITY\": 2,\n            \"Missing_value_RATE_BYDATE_PLMN_ABS_VALUE_UPPER_LIMIT\": 20\n           }, \n    'timestamp': {\"Missing_value_RATE_BYDATE_TIMESTAMP_AVERAGE\": 30,\n                  \"Missing_value_RATE_BYDATE_TIMESTAMP_VARIABILITY\": 2,\n                  \"Missing_value_RATE_BYDATE_TIMESTAMP_ABS_VALUE_UPPER_LIMIT\": 20\n                 }\n    }\nout_of_admissible_values_thresholds = {\n    'cell_id': {\"Out_of_range_RATE_BYDATE_CELL_AVERAGE\": 30,\n                \"Out_of_range_RATE_BYDATE_CELL_VARIABILITY\": 2,\n                \"Out_of_range_RATE_BYDATE_CELL_ABS_VALUE_UPPER_LIMIT\": 20\n               }, \n    'mcc': {\"Out_of_range_RATE_BYDATE_MCC_AVERAGE\": 30,\n            \"Out_of_range_RATE_BYDATE_MCC_VARIABILITY\": 2,\n            \"Out_of_range_RATE_BYDATE_MCC_ABS_VALUE_UPPER_LIMIT\": 20\n           }, \n    'mnc': {\"Out_of_range_RATE_BYDATE_MNC_AVERAGE\": 30,\n            \"Out_of_range_RATE_BYDATE_MNC_VARIABILITY\": 2,\n            \"Out_of_range_RATE_BYDATE_MNC_ABS_VALUE_UPPER_LIMIT\": 20\n           },\n    'plmn': {\"Out_of_range_RATE_BYDATE_PLMN_AVERAGE\": 30,\n            \"Out_of_range_RATE_BYDATE_PLMN_VARIABILITY\": 2,\n            \"Out_of_range_RATE_BYDATE_PLMN_ABS_VALUE_UPPER_LIMIT\": 20\n           },\n    'timestamp': {\"Out_of_range_RATE_BYDATE_TIMESTAMP_AVERAGE\": 30,\n                  \"Out_of_range_RATE_BYDATE_TIMESTAMP_VARIABILITY\": 2,\n                  \"Out_of_range_RATE_BYDATE_TIMESTAMP_ABS_VALUE_UPPER_LIMIT\": 20\n                 }\n    }\nnot_right_syntactic_format_thresholds = {\n    'timestamp': {\"Wrong_type_RATE_BYDATE_TIMESTAMP_AVERAGE\": 30,\n                  \"Wrong_type_RATE_BYDATE_TIMESTAMP_VARIABILITY\": 2,\n                  \"Wrong_type_RATE_BYDATE_TIMESTAMP_ABS_VALUE_UPPER_LIMIT\": 20\n                 }\n    }\nno_location_thresholds = {\n    None: {\"No_location_RATE_BYDATE_AVERAGE\": 30,\n           \"No_location_RATE_BYDATE_VARIABILITY\": 2,\n           \"No_location_RATE_BYDATE_ABS_VALUE_UPPER_LIMIT\": 20\n          }\n    }\nno_domain_thresholds = {\n    None: {\"No_domain_RATE_BYDATE_AVERAGE\": 30,\n           \"No_domain_RATE_BYDATE_VARIABILITY\": 2,\n           \"No_domain_RATE_BYDATE_ABS_VALUE_UPPER_LIMIT\": 20\n          }\n    }\nout_of_bounding_box_thresholds = {\n    None: {\"Out_of_bbox_RATE_BYDATE_AVERAGE\": 30,\n           \"Out_of_bbox_RATE_BYDATE_VARIABILITY\": 2,\n           \"Out_of_bbox_RATE_BYDATE_ABS_VALUE_UPPER_LIMIT\": 20\n          }\n    }\ndeduplication_same_location_thresholds = {\n    None: {\"Deduplication_same_location_RATE_BYDATE_AVERAGE\": 30,\n           \"Deduplication_same_location_RATE_BYDATE_VARIABILITY\": 2,\n           \"Deduplication_same_location_RATE_BYDATE_ABS_VALUE_UPPER_LIMIT\": 20\n          }\n    }\n\nclear_destination_directory = True\n</code></pre>"},{"location":"UserManual/configuration/3_QualityWarnings/3_semantic_quality_warnings/","title":"SemanticQualityWarnings Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>event_semantic_quality_warnings.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Silver]\n...\nevent_device_semantic_quality_metrics = ${Paths:silver_quality_metrics_dir}/semantic_quality_metrics\nevent_device_semantic_quality_warnings_log_table = ${Paths:silver_quality_warnings_dir}/semantic_quality_warnings_log_table\nevent_device_semantic_quality_warnings_bar_plot_data = ${Paths:silver_quality_warnings_dir}/semantic_quality_warnings_bar_plot_data\n...\n</code></pre> <p>The expected parameters in <code>event_semantic_quality_warnings.ini</code> are as follows: - date_of_study: string, format should be the one specified <code>date_format</code> (e.g., <code>2023-01-01</code> for <code>%Y-%m-%d</code>), the first date for which data will be processed by the component. All dates between this one and the specified in <code>data_period_end</code> will be processed (both inclusive). - date_format: string, it indicates the format expected in <code>date_of_study</code>. For example, use <code>%Y-%m-%d</code> for the usual \"2023-01-01\" format separated by <code>-</code>. - thresholds: dictionary, indicating the different thresholds to be used for raising warnings.</p>"},{"location":"UserManual/configuration/3_QualityWarnings/3_semantic_quality_warnings/#thresholds","title":"Thresholds","text":"<p>The thresholds parameter is a dictionary of dictionaries, used to indicate the different thresholds and lookback periods to be used for each type of warning. In the case than one of the thresholds is missing, a default value will be used instead. The default values are contained in <code>multimno.core.constants.semantic_qw_default_thresholds.SEMANTIC_DEFAULT_THRESHOLDS</code>.</p> <p>The dictionary structure is as follows:  - <code>\"CELL_ID_NON_EXISTENT\"</code>: refers to events that make reference to a non-existent cell ID.   - <code>\"sd_lookback_days\"</code>: integer, indicates the days to use as a lookback period to calculate the average and standard deviation over. By default, the value is 7.   - <code>\"min_sd\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate differs from the average over the lookback period by more than its standard deviation multiplied by this threshold. By default, the value is 2.   - <code>\"min_percentage\"</code>: integer or float, indicates the threshold for a warning to be raised for this error rate when the lookback date specified is strictly lower than 3. By default, the value is 0.  - <code>\"CELL_ID_NOT_VALID\"</code>: refers to events that make reference to an existent cell ID, but the cell is not operative.   - <code>\"sd_lookback_days\"</code>: integer, indicates the days to use as a lookback period to calculate the average and standard deviation over. By default, the value is 7.   - <code>\"min_sd\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate differs from the average over the lookback period by more than its standard deviation multiplied by this threshold. By default, the value is 2.   - <code>\"min_percentage\"</code>: integer or float, indicates the threshold for a warning to be raised for this error rate when the lookback date specified is strictly lower than 3. By default, the value is 0.  - <code>\"INCORRECT_EVENT_LOCATION\"</code>: refers to events that have been flagged as having an incorrect location.   - <code>\"sd_lookback_days\"</code>: integer, indicates the days to use as a lookback period to calculate the average and standard deviation over. By default, the value is 7.   - <code>\"min_sd\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate differs from the average over the lookback period by more than its standard deviation multiplied by this threshold. By default, the value is 2.   - <code>\"min_percentage\"</code>: integer or float, indicates the threshold for a warning to be raised for this error rate when the lookback date specified is strictly lower than 3. By default, the value is 0.  - <code>\"SUSPICIOUS_EVENT_LOCATION\"</code>: refers to events that have been flagged as having a suspicious location.   - <code>\"sd_lookback_days\"</code>: integer, indicates the days to use as a lookback period to calculate the average and standard deviation over. By default, the value is 7.   - <code>\"min_sd\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate differs from the average over the lookback period by more than its standard deviation multiplied by this threshold. By default, the value is 2.   - <code>\"min_percentage\"</code>: integer or float, indicates the threshold for a warning to be raised for this error rate when the lookback date specified is strictly lower than 3. By default, the value is 0.</p>"},{"location":"UserManual/configuration/3_QualityWarnings/3_semantic_quality_warnings/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = SemanticQualityWarnings\n\n[SemanticQualityWarnings]\n\ndate_format = %Y-%m-%d\ndate_of_study = 2023-01-08\n\nthresholds = {\n    \"CELL_ID_NON_EXISTENT\": {\n        \"sd_lookback_days\": 8,\n        \"min_sd\": 1.5,\n        \"min_percentage\": 5\n    },\n    \"CELL_ID_NOT_VALID\": {\n        \"sd_lookback_days\": 8,\n        \"min_sd\": 1.5,\n        \"min_percentage\": 5\n    },\n    \"INCORRECT_EVENT_LOCATION\": {\n        \"sd_lookback_days\": 8,\n        \"min_sd\": 1.5,\n        \"min_percentage\": 5\n    },\n    \"SUSPICIOUS_EVENT_LOCATION\": {\n        \"sd_lookback_days\": 8,\n        \"min_sd\": 1.5,\n        \"min_percentage\": 5\n    },\n    }\n</code></pre>"},{"location":"UserManual/configuration/3_QualityWarnings/4_cell_footprint_quality_metrics/","title":"CellFootprintQualityMetrics Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>cell_footprint_quality_metrics.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Silver]\n...\nnetwork_data_silver = ${Paths:silver_dir}/mno_network\ncell_footprint_data_silver = ${Paths:silver_dir}/cell_footprint\nevent_data_silver = ${Paths:silver_dir}/mno_event1\n...\n</code></pre> <p>The expected parameters in <code>cell_footprint_quality_metrics.ini</code> are as follows:  - clear_destination_directory: bool, whether to clear the output directory before running the component. Example: <code>True</code>.  - data_period_start: string, with <code>YYYY-MM-DD</code> format, indicating the first date of the date interval for which the cell footprint quality metrics will be computed. All days between data_period_start and data_period_end, both inclusive, will be processed individually. Example: <code>2023-01-01</code>.  - data_period_end: string, with <code>YYYY-MM-DD</code> format, indicating the last date of the date interval for which the cell footprint quality metrics will be computed. All days between data_period_start and data_period_end, both inclusive, will be processed individually. Example: <code>2023-01-03</code>.  - non_crit_cell_pct_threshold: float between <code>0.0</code> and <code>100.0</code>, it indicates the non-critical warning threshold for the percentage of all total cells with no footprint assigned. A non-critical warning will be raised if the percentage of these cells with respect to the total number of cells in this date's network data is strictly higher than this threshold, and strictly lower than crit_cell_pct_threshold. Example: <code>0</code>.  - crit_cell_pct_threshold: float between <code>0.0</code> and <code>100.0</code>, it indicates the critical warning threshold for the percentage of all total cells with no footprint assigned. A critical warning will be raised if the percentage of thece cells with respect to the total number of cells in this date's network data is equal or higher than this threshold. Example: <code>1</code>.  - non_crit_event_pct_threshold: float between <code>0.0</code> and <code>100.0</code>, it indicates the non-critical warning threshold for the percentage of all total events assigned to a cell with no footprint. A non-critical warning will be raised if the percentage of these events with respect to the total number of events of this date is strictly higher than this threshold, and strictly lower than crit_event_pct_threshold. Example: <code>0</code>.  - crit_event_pct_threshold: float between <code>0.0</code> and <code>100.0</code>, it indicates the non-critical warning threshold for the percentage of all total events assigned to a cell with no footprint. A critical warning will be raised if the percentage of these events with respect to the total number of events of this date is equal or higher than this threshold. Example: <code>1</code>.</p>"},{"location":"UserManual/configuration/3_QualityWarnings/4_cell_footprint_quality_metrics/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = CellFootprintQualityMetrics\n\n[CellFootprintQualityMetrics]\nclear_destination_directory = False\n\ndata_period_start = 2023-01-07\ndata_period_end = 2023-01-11\n\n# Threshold for percentage of total cells with no footprint\nnon_crit_cell_pct_threshold = 0\ncrit_cell_pct_threshold = 1\n\n# Threshold for percentage of total events \"missed\" due to cells with no footprint\nnon_crit_event_pct_threshold = 0\ncrit_event_pct_threshold = 1\n</code></pre>"},{"location":"UserManual/configuration/3_QualityWarnings/5_daily_permanence_score_quality_metrics/","title":"DailyPermanenceScoreQualityMetrics Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>daily_permanence_score_quality_metrics.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Silver]\n...\ndaily_permanence_score_data_silver = ${Paths:ue_dir}/daily_permanence_score\ndaily_permanence_score_quality_metrics = ${Paths:silver_quality_metrics_dir}/daily_permanence_score_quality_metrics\n...\n</code></pre> <p>The expected parameters in <code>daily_permanence_score_quality_metrics.ini</code> are as follows:  - clear_destination_directory: bool, whether to clear the quality metrics directory before running the component. Example: <code>False</code>.  - data_period_start: string, with <code>YYYY-MM-DD</code> format, indicating the first date of the date interval for which the daily permanence score quality metrics will be computed. All days between data_period_start and data_period_end, both inclusive, will be processed individually. Example: <code>2023-01-01</code>.  - data_period_end: string, with <code>YYYY-MM-DD</code> format, indicating the last date of the date interval for which the daily permanence score quality metrics will be computed. All days between data_period_start and data_period_end, both inclusive, will be processed individually. Example: <code>2023-01-03</code>.  - unknown_intervals_pct_threshold: float between <code>0.0</code> and <code>100.0</code>, it indicates the percentage threshold of \"unknown\" time slots that a device must have in order to be flagged for the warning of too many devices with a high number of \"unknown\" time slots. Devices with a percentage of their time slots classified as \"unknown\" equal or higher than this threshold will be flagged. Example: <code>50</code>.  - non_crit_unknown_devices_pct_threshold: float between <code>0.0</code> and <code>100.0</code>, it indicates the non-critical warning threshold for high number of devices with \"unknown\" time slots. A non-critical warning will be raised if the percentage of these devices with respect to the total number of devices in this date's daily permanence score data is strictly higher than this threshold, and strictly lower than crit_unknown_devices_pct_threshold. Example: <code>25</code>.  - crit_unknown_devices_pct_threshold: float between <code>0.0</code> and <code>100.0</code>, it indicates the critical warning threshold for high number of devices with \"unknown\" time slots. A critical warning will be raised if the percentage of these devices with respect to the total number of devices in this date's daily permanence score data is equal or higher than this threshold. Example: <code>75</code>.</p>"},{"location":"UserManual/configuration/3_QualityWarnings/5_daily_permanence_score_quality_metrics/#configuration-example","title":"Configuration example","text":"<pre><code>[Logging]\nlevel = DEBUG\n\n[Spark]\nsession_name = DailyPermanenceScoreQualityMetrics\n\n[DailyPermanenceScoreQualityMetrics]\nclear_destination_directory = False\n\ndata_period_start = 2023-01-08\ndata_period_end = 2023-01-09\n\n# A device with a % of \"unknown\" time intervals equal or greater to this threshold will be considered for the\n# warning computation\nunknown_intervals_pct_threshold = 50\n\n# Thresholds for percentage of users with high number of \"unknown\" intervals in a day\nnon_crit_unknown_devices_pct_threshold = 25\ncrit_unknown_devices_pct_threshold = 75\n</code></pre>"},{"location":"UserManual/configuration/4_SyntheticMnoData/1_synthetic_diaries/","title":"SyntheticDiaries Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>synthetic_diaries.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Bronze]\n...\ndiaries_data_bronze = ${Paths:bronze_dir}/diaries\n...\n</code></pre> <p>The expected parameters in <code>synthetic_diaries.ini</code> are as follows:  - number_of_users: integer, number of devices for which to generate synthetic diaries in each target date. Example: <code>100</code>.  - date_format: string, format for date parsing from initial_date field (1989 C standard). Example: <code>%Y-%m-%d</code>.  - initial_date: string, initial date for which to generate synthetic diaries, matching the date format specified in date_format. Example: <code>2023-01-01</code>.  - number_of_dates: integer, number of dates for which synthetic diaries will be generated. Synthetic diaries for initial_date and for the following <code>number_of_dates - 1</code> dates will be generated. If diaries are to be generated for just one date, set this parameter equal to 1. Example: <code>9</code>.</p> <ul> <li>latitude_min: float, degrees. Lower limit of the bounding box within which all locations will be generated. Example: <code>40.352</code>.</li> <li>latitude_max: float, degrees. Upper limit of the bounding box within which all locations will be generated. Must be higher than latitude_min. Example: <code>40.486</code>.</li> <li>longitude_min: float, degrees. Left limit of the bounding box within which all locations will be generated. Example: <code>-3.751</code>.</li> <li> <p>longitude_max: float, degrees. Right limit of the bounding box within which all locations will be generated. Must be higher than longitude_min. Example: <code>-3.579</code>.</p> </li> <li> <p>home_work_distance_min: float, meters. Work location of a user will be generated at a distance higher than this minimum from the user's home location. Example: <code>2000</code> (m).</p> </li> <li>home_work_distance_max: float, meters. Work location of a user will be generated at a distance lower than this maximum from the user's home location. Must be higher than home_work_distance_min. Example: <code>10000</code> (m).</li> <li>other_distance_min: float, meters. For activities of type 'other', the location of the activity will be generated at a distance higher than this minimum from the user's previous activity. Example: <code>1000</code> (m).</li> <li> <p>other_distance_max: float, meters. For activities of type 'other', the location of the activity will be generated at a distance lower than this maximum from the user's previous activity. Must be higher than other_distance_min. Example: <code>3000</code> (m).</p> </li> <li> <p>home_duration_min: float, hours. This is the minimum duration that will be considered for an activity with stay type 'home'. Example: <code>5</code> (hours).</p> </li> <li>home_duration_max: float, hours. This is the maximum duration that will be considered for an activity with stay type 'home'. Must be higher than home_duration_min. Example: <code>12</code> (hours).</li> <li>work_duration_min: float, hours. This is the minimum duration that will be considered for an activity with stay type 'work'. Example: <code>4</code> (hours).</li> <li>work_duration_max: float, hours. This is the maximum duration that will be considered for an activity with stay type 'work'. Must be higher than work_duration_min. Example: <code>8</code> (hours).</li> <li>other_duration_min: float, hours. This is the minimum duration that will be considered for an activity with stay type 'other'. Example: <code>1</code> (hours).</li> <li> <p>other_duration_max: float, hours. This is the maximum duration that will be considered for an activity with stay type 'other'. Must be higher than other_duration_min. Example: <code>3</code> (hours).</p> </li> <li> <p>displacement_speed: float, m/s. Given the location of 2 consecutive generated activities for a user, this speed helps us define the second activity's start time once we know the first activity's end time. The distance between both activities is calculated and then divided by this speed in order to calculate the trip time, which is then added to the first activity's end time in order to obtain the second activity's initial time. Example: <code>3</code> (m/s).</p> </li> </ul>"},{"location":"UserManual/configuration/4_SyntheticMnoData/1_synthetic_diaries/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\n# parameters in this section are passed to Spark except session_name \nsession_name = SyntheticDiarySession\n\n[SyntheticDiaries]\nnumber_of_users = 100\ndate_format = %Y-%m-%d\ninitial_date = 2023-01-01\nnumber_of_dates = 9\n\nlatitude_min = 40.352\nlatitude_max = 40.486\nlongitude_min = -3.751\nlongitude_max = -3.579\n\nhome_work_distance_min = 2_000  # in meters\nhome_work_distance_max = 10_000  # in meters\nother_distance_min = 1_000  # in meters\nother_distance_max = 3_000  # in meters\nhome_duration_min = 5  # in hours\nhome_duration_max = 12  # in hours\nwork_duration_min = 4  # in hours\nwork_duration_max = 8  # in hours\nother_duration_min = 1  # in hours\nother_duration_max = 3  # in hours\ndisplacement_speed = 3  # 3 m/s = 10 km/h\n\nstay_sequence_superset = home,other,work,other,other,other,home\nstay_sequence_probabilities = 1,0.1,0.6,0.4,0.2,0.1,1\n</code></pre>"},{"location":"UserManual/configuration/4_SyntheticMnoData/2_synthetic_network/","title":"LongtermPermanenceScore Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>synthetic_network.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Bronze]\n...\nnetwork_data_bronze = ${Paths:bronze_dir}/mno_network\n...\n</code></pre> <p>The expected parameters in <code>synthetic_network.ini</code> are as follows:  - seed: integer, seed for random number generation used to generate the synthetic network topology data.  - n_cells: positive integer, number of synthetic cells that will be generated. Example: <code>500</code>.  - cell_id_generation_type: string, identifier of the generator of cell IDs to be used. Currently the only option available is <code>random_cell_id</code>, which generates a random 14- or 15- digit string. Example: <code>random_cell_id</code>.  - cell_type_options: comma-separated list of strings, it contains the values that the <code>cell_type</code> field can take for the generated synthetic data. Each option has the same probability of being assigned. Example: <code>macrocell, microcell, picocell, femtocell</code>.  - latitude_min: float, minimum latitude that defines the bounding box in which the coordinates of the synthetic cells will be generated.  - latitude_max: float, maximum latitude that defines the bounding box in which the coordinates of the synthetic cells will be generated.  - longitude_min: float, minimum longitude that defines the bounding box in which the coordinates of the synthetic cells will be generated.  - longitude_max: float, maximum longitude that defines the bounding box in which the coordinates of the synthetic cells will be generated.  - altitude_min: float, minimum value that the altitude field can take in the generated cells. Example: <code>-40</code>.  - altitude_max: float, maximum value that the altitude field can take in the generated cells. Example: <code>5000</code>.  - antenna_height_max: float, maximum value that the <code>antenna_height</code> field might take in the generated cells. Example: <code>120</code>.  - altitude_min: float, minimum value that the altitude field can take in the generated cells. Example: <code>-40</code>.  - altitude_max: float, maximum value that the altitude field can take in the generated cells. Example: <code>5000</code>.  - power_min: float, minimum value that the power field can take in the generated cells. Units are watts (W). Example: <code>0.1</code>.  - power_max: float, maximum value that the power field can take in the generated cells. Units are watts (W). Example: <code>500</code>.  - frequency_min: float, minimum value that the frequency field can take in the generated cells. Units are megahertz (MHz). Example: <code>1</code>.  - frequency_max: float, maximum value that the frequency field can take in the generated cells. Units are megahertz (MHz). Example: <code>4000</code>.  - data_period_format: string, it indicates the format expected in <code>earliest_valid_date_start</code> and <code>latest_valid_date_end</code>. For example, use <code>%Y-%m-%dT%H:%M:%S</code> for the usual \"2023-01-09T00:00:00\" format.  - earliest_valid_date_start: string, it indicates the timestamp value that the <code>valid_date_start</code> will take in all generated cells. Example: <code>2022-12-15T00:00:00</code>.  - latest_valid_date_end: string, it indicates the timestamp value that the <code>valid_date_end</code> will take in all generated cells. Example: <code>2023-01-15T00:00:00</code>.  - date_format: string, it indicates the format expected in <code>starting_date</code> and <code>ending_date</code>. For example, use <code>%Y-%m-%d</code> for the usual \"2023-01-09\" format separated by <code>-</code>. Example: <code>%Y-%m-%d</code>.  - starting_date: string, format should be the one specified <code>date_format</code> (e.g., <code>2023-01-01</code> for <code>%Y-%m-%d</code>), the first date for which data will be generated. All dates between this one and the specified in <code>ending_date</code> will be have data generated (both inclusive). The cell properties will be equal across all dates.   - ending_date: string, format should be the one specified <code>date_format</code> (e.g., <code>2023-01-09</code> for <code>%Y-%m-%d</code>), the last date for which data will be generated. All dates between the specified in <code>starting_date</code> and this one will be have data generated (both inclusive). The cell properties will be equal across all dates.   - no_optional_fields_probability: float, probability that all of the optional fields of a record take the null value. Example: <code>0.05</code>.  - mandatory_null_probability: float, probability that one of the mandatory fields of a record will take a null value. Example: <code>0.05</code>.  - out_of_bounds_values_probability: float, probability that a field of a record will take a value outside its valid values. This could be, for example, a negative power or a latitude outside the $[-90, 90]$ interval. Example: <code>0.05</code>.  - erroneous_values_probability: float, probability that one of the following erroneous values might occur:    - The <code>cell_id</code> takes a non-valid value (not a 14- or 15-digit string).    - The <code>valid_date_start</code> and <code>valid_date_end</code> fields has an invalid timestamp format.    - The <code>valid_date_end</code> is a point in time earlier than <code>valid_date_start</code>.</p> <pre><code>Example: `0.05`.\n</code></pre>"},{"location":"UserManual/configuration/4_SyntheticMnoData/2_synthetic_network/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = SyntheticNetworkSession\n\n[SyntheticNetwork]\nseed = 33\nn_cells = 500\ncell_id_generation_type = random_cell_id  # options: random_cell_id\ncell_type_options = macrocell, microcell, picocell, femtocell\nlatitude_min = 40.352\nlatitude_max = 40.486\nlongitude_min = -3.751\nlongitude_max = -3.579\n\naltitude_min = -40\naltitude_max = 5000\n# antenna_height_min is always 0\nantenna_height_max = 120\npower_min = 0.1\npower_max = 500\nfrequency_min = 1\nfrequency_max = 4000\ntimestamp_format = %Y-%m-%dT%H:%M:%S\nearliest_valid_date_start = 2022-12-15T00:00:00\nlatest_valid_date_end = 2023-01-15T00:00:00\ndate_format = %Y-%m-%d\nstarting_date = 2023-01-01\nending_date = 2023-01-09\n\nno_optional_fields_probability = 0.0\nmandatory_null_probability = 0.0\nout_of_bounds_values_probability = 0.0\nerroneous_values_probability = 0.0\n</code></pre>"},{"location":"UserManual/configuration/4_SyntheticMnoData/3_synthetic_events/","title":"GeozonesGridMapping Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>synthetic_events.ini</code>. In <code>general_config.ini</code> to execute the component specify all paths to its corresponding data objects (input + output). </p> <pre><code>[Paths.Bronze]\ndiaries_data_bronze = ${Paths:bronze_dir}/diaries\nevent_data_bronze = ${Paths:bronze_dir}/mno_events\nnetwork_data_bronze = ${Paths:bronze_dir}/mno_network\n</code></pre> <p>In synthetic_events.ini parameters are as follows: </p> <ul> <li> <p>seed -integer, the random seed value for all subprocesses that involve randomness, such as the random generation of timestamps, latitude, longitude,  random selection of cell to be linked to a point, random selection of rows and columns for null generation, random selection of rows for duplicates generations, etc.</p> </li> <li> <p>event_freq_stays - integer, the frequency in seconds for events to be generated for stays (higher means that less events will be generated for a given stay in a given time interval in synthetic diaries).</p> </li> <li> <p>event_freq_moves - integer, the frequency in seconds for events to be generated for moves (higher means that less events will be generated for a given move in a given time interval in synthetic diaries).</p> </li> <li> <p>error_location_probability - float (between 0.0 and 1.0), ratio of rows from all generated events, to be selected for generating errors in x and y coordinates.</p> </li> <li> <p>error_location_distance_min - integer, the minimum distance in meters to which generated points may reach from the original generated point, when offseting these points to be erroneous</p> </li> <li> <p>error_location_distance_max - integer, the maximum distance in meters to which generated points may reach from the original generated point, when offseting these points to be erroneous.  </p> </li> <li> <p>error_cell_id_probability - float (between 0.0 and 1.0), the ratio of rows that are to be selected from all generated events, for generating events with nonexistent cell ids. These are events that have a syntactically valid cell_id that is not present in the input network data.</p> </li> <li> <p>mcc - integer, the value for the mcc column to be applied to the data of all generated users.</p> </li> <li> <p>maximum_number_of_cells_for_event - integer, the maximimum number of cells to consider, when linking a generated point to a cell in the input network data. If several cells are within the distance provided by closest_cell_distance_max for a given generated point (erroneous or not), a single cell is selected from as many closest cells, randomly, as given by maximum_number_of_cells_for_event.</p> </li> <li> <p>closest_cell_distance_max - integer, the distance in meters to a cell from a generated point, for that cell to be included as one of the possible cells to be linked to the given point.</p> </li> <li> <p>closest_cell_distance_max_for_errors - integer, the distance in meters to a cell from an erroneously generated point, for the cell to be included as one of the possible cells to be linked to that point.</p> </li> <li> <p>closest_cell_distance_max_for_errors - integer, the distance in meters to a cell from an erroneously generated point, for the cell to be included as one of the possible cells to be linked to that point.</p> </li> <li> <p>do_event_error_generation - boolean, whether to generate syntactic errors for clean rows that have been generated.</p> </li> <li> <p>null_row_probability - float, probability to use for sampling rows for which one or more columns will be set to null. If set to 0, no null rows are generated. Which columns on the sampeld rows are selected as null is affected by the parameter column_is_null_probability.</p> </li> <li> <p>out_of_bounds_probability - float, probability to use for sampling rows for which timestamp will be replaced with a timestamp that is out of the temporal bounds set in the input synthetic diaries. These rows will not include any other errors.</p> </li> <li> <p>data_type_error_probability - float, probability to use for sampling rows for which a random selection of columns will be modified as erroneous. Modifications are column specific, for instance the cell_id column will be replaced by random string. </p> </li> <li> <p>column_is_null_probability - float, probability that a given column will be set as null for the rows that have been sampled for null selection. If column_is_null_probability=1, all columns in the rows selected for null generation are replaced with nulls. If null_row_probability=0, this parameter is not used.</p> </li> <li> <p>same_location_duplicates_probability - float, probability for selecting rows that will be transformed into same location duplicates. The value corresponds to the approximate ratio of rows in the final output that will include pair-wise full duplicates. This process generates at a maximum two duplicate records for a given user, not more.</p> </li> <li> <p>different_location_duplicates_probability - float, probability for selecting rows that will be transformed into different location duplicates. The value corresponds to the approximate ratio of rows in the final output that will include pair-wise duplicates in all columns, except for longitude and latitude. This process generates at a maximum two duplicate records for a given user, not more.</p> </li> <li> <p>order_output_by_timestamp - boolean, whether to order the final output by the timestamp column.</p> </li> <li> <p>cartesian_crs - integer, the coordinate reference system (CRS) to use for the cartesian coordinates. The default value is 3035.</p> </li> </ul> <p>Note on probability parameters: These parameters do not necessarily translate to the exact ratios for each probability type in the output data object, as the exact number of rows selected is affected by the random seed, in addition to the probability value itself.</p>"},{"location":"UserManual/configuration/4_SyntheticMnoData/3_synthetic_events/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\n\nsession_name = SyntheticEventsSession\n\n\n[SyntheticEvents]\nseed = 999\nevent_freq_stays = 2400 # s\nevent_freq_moves = 1200  # s\nerror_location_probability = 0.0\nerror_location_distance_min = 1000 # m\nerror_location_distance_max = 10000 # m\nerror_cell_id_probability  = 0.0\nmcc = 214\nmaximum_number_of_cells_for_event = 3\nclosest_cell_distance_max = 5000 # m\nclosest_cell_distance_max_for_errors = 10000 # m\ncartesian_crs = 3035\n\ndo_event_error_generation = True \nnull_row_probability = 0.3 \nout_of_bounds_probability = 0.0 \ndata_type_error_probability = 0.3 \ncolumn_is_null_probability = 0.5 \ndifferent_location_duplicates_probability = 0.3\nsame_location_duplicates_probability = 0.25\n\norder_output_by_timestamp = True\n</code></pre>"},{"location":"autodoc/test_report/","title":"Test Report","text":"test_report.md"},{"location":"autodoc/test_report/#title","title":"test_report.md","text":"<p>Report generated on 15-Apr-2025 at 12:24:43 by pytest-html         v4.1.1</p> Environment No results found. Check the filters. &lt; &gt; Summary <p>64 tests took 00:11:33.</p> <p>(Un)check the boxes to filter the results.</p> There are still tests running. Reload this page to get the latest results! 0 Failed, 64 Passed, 5 Skipped, 0 Expected failures, 0 Unexpected passes, 0 Errors, 0 Reruns Show all details\u00a0/\u00a0Hide all details Result Test Duration Links"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>components<ul> <li>execution<ul> <li>cell_connection_probability<ul> <li>cell_connection_probability</li> </ul> </li> <li>cell_footprint<ul> <li>cell_footprint_estimation</li> </ul> </li> <li>cell_footprint_intersections<ul> <li>cell_footprint_intersections</li> </ul> </li> <li>cell_proximity_estimation<ul> <li>cell_proximity_estimation</li> </ul> </li> <li>daily_permanence_score<ul> <li>daily_permanence_score</li> </ul> </li> <li>device_activity_statistics<ul> <li>device_activity_statistics</li> </ul> </li> <li>event_cleaning<ul> <li>event_cleaning</li> </ul> </li> <li>event_semantic_cleaning<ul> <li>event_semantic_cleaning</li> </ul> </li> <li>geozones_grid_mapping<ul> <li>geozones_grid_mapping</li> </ul> </li> <li>grid_enrichment<ul> <li>grid_enrichment</li> </ul> </li> <li>internal_migration<ul> <li>internal_migration</li> </ul> </li> <li>longterm_permanence_score<ul> <li>longterm_permanence_score</li> </ul> </li> <li>midterm_permanence_score<ul> <li>midterm_permanence_score</li> </ul> </li> <li>multimno_aggregation<ul> <li>multimno_aggregation</li> </ul> </li> <li>network_cleaning<ul> <li>network_cleaning</li> </ul> </li> <li>output_indicators<ul> <li>output_indicators</li> </ul> </li> <li>present_population<ul> <li>present_population_estimation</li> </ul> </li> <li>time_segments<ul> <li>continuous_time_segmentation</li> </ul> </li> <li>tourism_outbound_statistics<ul> <li>tourism_outbound_statistics_calculation</li> </ul> </li> <li>tourism_statistics<ul> <li>tourism_statistics_calculation</li> </ul> </li> <li>tourism_stays_estimation<ul> <li>tourism_stays_estimation</li> </ul> </li> <li>usual_environment_aggregation<ul> <li>usual_environment_aggregation</li> </ul> </li> <li>usual_environment_labeling<ul> <li>usual_environment_labeling</li> </ul> </li> </ul> </li> <li>ingestion<ul> <li>data_filtering<ul> <li>data_filtering</li> </ul> </li> <li>grid_generation<ul> <li>inspire_grid_generation</li> </ul> </li> <li>spatial_data_ingestion<ul> <li>gisco_data_ingestion</li> <li>overture_data_ingestion</li> <li>overture_data_transformation</li> </ul> </li> <li>synthetic<ul> <li>synthetic_diaries</li> <li>synthetic_events</li> <li>synthetic_network</li> </ul> </li> </ul> </li> <li>quality<ul> <li>cell_footprint_quality_metrics<ul> <li>cell_footprint_quality_metrics</li> </ul> </li> <li>daily_permanence_score_quality_metrics<ul> <li>daily_permanence_score_quality_metrics</li> </ul> </li> <li>event_quality_warnings<ul> <li>event_quality_warnings</li> </ul> </li> <li>network_quality_warnings<ul> <li>network_quality_warnings</li> </ul> </li> <li>semantic_quality_warnings<ul> <li>semantic_quality_warnings</li> </ul> </li> </ul> </li> </ul> </li> <li>core<ul> <li>component</li> <li>configuration</li> <li>constants<ul> <li>columns</li> <li>conditions</li> <li>domain_names</li> <li>error_types</li> <li>measure_definitions</li> <li>network_default_thresholds</li> <li>period_names</li> <li>reserved_dataset_ids</li> <li>semantic_qw_default_thresholds</li> <li>spatial</li> <li>transformations</li> <li>warnings</li> </ul> </li> <li>data_objects<ul> <li>bronze<ul> <li>bronze_admin_units_data_object</li> <li>bronze_buildings_data_object</li> <li>bronze_countries_data_object</li> <li>bronze_event_data_object</li> <li>bronze_geographic_zones_data_object</li> <li>bronze_holiday_calendar_data_object</li> <li>bronze_inbound_estimation_factors_data_object</li> <li>bronze_landuse_data_object</li> <li>bronze_mcc_iso_tz_map</li> <li>bronze_network_physical_data_object</li> <li>bronze_synthetic_diaries_data_object</li> <li>bronze_transportation_data_object</li> </ul> </li> <li>data_object</li> <li>landing<ul> <li>landing_geoparquet_data_object</li> <li>landing_http_geojson_data_object</li> </ul> </li> <li>silver<ul> <li>event_cache_data_object</li> <li>silver_aggregated_usual_environments_data_object</li> <li>silver_aggregated_usual_environments_zones_data_object</li> <li>silver_cell_connection_probabilities_data_object</li> <li>silver_cell_distance_data_object</li> <li>silver_cell_footprint_data_object</li> <li>silver_cell_footprint_quality_metrics_data_object</li> <li>silver_cell_intersection_groups_data_object</li> <li>silver_cell_to_group_data_object</li> <li>silver_daily_permanence_score_data_object</li> <li>silver_daily_permanence_score_quality_metrics_data_object</li> <li>silver_device_activity_statistics</li> <li>silver_enriched_grid_data_object</li> <li>silver_event_data_object</li> <li>silver_event_data_syntactic_quality_metrics_by_column</li> <li>silver_event_data_syntactic_quality_metrics_frequency_distribution</li> <li>silver_event_data_syntactic_quality_warnings_for_plots</li> <li>silver_event_data_syntactic_quality_warnings_log_table</li> <li>silver_event_flagged_data_object</li> <li>silver_geozones_grid_map_data_object</li> <li>silver_grid_data_object</li> <li>silver_group_to_tile_data_object</li> <li>silver_internal_migration_data_object</li> <li>silver_internal_migration_quality_metrics_data_object</li> <li>silver_longterm_permanence_score_data_object</li> <li>silver_midterm_permanence_score_data_object</li> <li>silver_network_data_object</li> <li>silver_network_data_syntactic_quality_metrics_by_column</li> <li>silver_network_data_top_frequent_errors_data_object</li> <li>silver_network_row_error_metrics</li> <li>silver_network_syntactic_quality_warnings_log_table</li> <li>silver_network_syntactic_quality_warnings_plot_data</li> <li>silver_present_population_data_object</li> <li>silver_present_population_zone_data_object</li> <li>silver_semantic_quality_metrics</li> <li>silver_semantic_quality_warnings_log_table</li> <li>silver_semantic_quality_warnings_plot_data</li> <li>silver_time_segments_data_object</li> <li>silver_tourism_outbound_nights_spent_data_object</li> <li>silver_tourism_stays_data_object</li> <li>silver_tourism_trip_avg_destinations_nights_spent_data_object</li> <li>silver_tourism_trip_data_object</li> <li>silver_tourism_zone_departures_nights_spent_data_object</li> <li>silver_usual_environment_labeling_quality_metrics_data_object</li> <li>silver_usual_environment_labels_data_object</li> </ul> </li> </ul> </li> <li>exceptions</li> <li>grid</li> <li>io_interface</li> <li>log</li> <li>quadkey_utils</li> <li>settings</li> <li>spark_session</li> <li>utils</li> </ul> </li> <li>main_multimno</li> <li>orchestrator_multimno</li> </ul>"},{"location":"reference/main_multimno/","title":"main_multimno","text":"<p>Application entrypoint for launching a single component.</p> <p>Usage:</p> <pre><code>python multimno/main.py &lt;component_id&gt; &lt;general_config_path&gt; &lt;component_config_path&gt;\n</code></pre> <ul> <li>component_id: ID of the component to be executed.</li> <li>general_config_path: Path to a INI file with the general configuration of the execution.</li> <li>component_config_path: Path to a INI file with the specific configuration of the component.</li> </ul>"},{"location":"reference/main_multimno/#main_multimno.build","title":"<code>build(component_id, general_config_path, component_config_path)</code>","text":"<p>Build a component based on the component_id.</p> <p>Parameters:</p> Name Type Description Default <code>component_id</code> <code>str</code> <p>id of the component</p> required <code>general_config_path</code> <code>str</code> <p>general config path</p> required <code>component_config_path</code> <code>str</code> <p>component config path</p> required <p>Raises:</p> Type Description <code>ComponentNotSupported</code> <p>If the component_id is not supported.</p> <p>Returns:</p> Type Description <code>Component</code> <p>Component constructor.</p> Source code in <code>multimno/main_multimno.py</code> <pre><code>def build(component_id: str, general_config_path: str, component_config_path: str):\n    \"\"\"\n    Build a component based on the component_id.\n\n    Args:\n        component_id (str): id of the component\n        general_config_path (str): general config path\n        component_config_path (str): component config path\n\n    Raises:\n        ComponentNotSupported: If the component_id is not supported.\n\n    Returns:\n        (multimno.core.component.Component): Component constructor.\n    \"\"\"\n    constructor = CONSTRUCTORS.get(component_id)\n    if constructor is None:\n        raise ComponentNotSupported(component_id)\n\n    return constructor(general_config_path, component_config_path)\n</code></pre>"},{"location":"reference/orchestrator_multimno/","title":"orchestrator_multimno","text":"<p>Module that orchestrates MultiMNO pipeline components. A spark-submit will be performed for each  component in the pipeline.</p> <p>Usage: </p> <pre><code>python multimno/orchestrator.py &lt;pipeline.json&gt;\n</code></pre> <ul> <li>pipeline.json: Path to a json file with the pipeline configuration.</li> </ul>"},{"location":"reference/orchestrator_multimno/#orchestrator_multimno.create_logger","title":"<code>create_logger(general_config_path)</code>","text":"<p>Create a logger with the specified configuration.</p> <p>Parameters:</p> Name Type Description Default <code>general_config_path</code> <code>str</code> <p>The path to the general configuration file.</p> required <p>Returns:</p> Type Description <code>Logger</code> <p>The created logger object.</p> Source code in <code>multimno/orchestrator_multimno.py</code> <pre><code>def create_logger(general_config_path: str):\n    \"\"\"\n    Create a logger with the specified configuration.\n\n    Args:\n        general_config_path (str): The path to the general configuration file.\n\n    Returns:\n        (logging.Logger): The created logger object.\n    \"\"\"\n    # Get log configuration\n    config = parse_configuration(general_config_path)\n\n    report_path = config.get(LoggerKeys.LOG_CONFIG_KEY, LoggerKeys.REPORT_PATH, fallback=None)\n    file_format = config.get(LoggerKeys.LOG_CONFIG_KEY, LoggerKeys.FILE_FORMAT, fallback=None)\n    datefmt = config.get(LoggerKeys.LOG_CONFIG_KEY, LoggerKeys.DATEFMT, fallback=None)\n\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.DEBUG)  # Set the logging level\n\n    # Get log path\n    today = datetime.now().strftime(\"%y%m%d_%H%M%S\")\n    log_path = f\"{report_path}/multimno_{today}.log\"\n    # Make report path + log dir\n    os.makedirs(os.path.dirname(log_path), exist_ok=True)\n\n    # Create a file handler\n    file_handler = logging.FileHandler(log_path)\n    file_handler.setLevel(logging.DEBUG)\n\n    # Create a console handler\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(logging.DEBUG)\n\n    # Create a formatter and add it to the handlers\n    formatter = logging.Formatter(fmt=file_format, datefmt=datefmt)\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n\n    # Add the handlers to the logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n\n    return logger\n</code></pre>"},{"location":"reference/components/","title":"components","text":""},{"location":"reference/components/execution/","title":"execution","text":""},{"location":"reference/components/execution/cell_connection_probability/","title":"cell_connection_probability","text":""},{"location":"reference/components/execution/cell_connection_probability/cell_connection_probability/","title":"cell_connection_probability","text":"<p>Module that calculates cell connection probabilities and posterior probabilities.</p>"},{"location":"reference/components/execution/cell_connection_probability/cell_connection_probability/#components.execution.cell_connection_probability.cell_connection_probability.CellConnectionProbabilityEstimation","title":"<code>CellConnectionProbabilityEstimation</code>","text":"<p>               Bases: <code>Component</code></p> <p>Estimates the cell connection probabilities and posterior probabilities for each grid tile. Cell connection probabilities are calculated based on footprint per grid. Posterior probabilities are calculated based on the cell connection probabilities and grid prior probabilities.</p> <p>This class reads in cell footprint estimation and the grid model wit prior probabilities. The output is a DataFrame that represents cell connection probabilities and  posterior probabilities for each cell and grid id combination for a given date.</p> Source code in <code>multimno/components/execution/cell_connection_probability/cell_connection_probability.py</code> <pre><code>class CellConnectionProbabilityEstimation(Component):\n    \"\"\"\n    Estimates the cell connection probabilities and posterior probabilities for each grid tile.\n    Cell connection probabilities are calculated based on footprint per grid.\n    Posterior probabilities are calculated based on the cell connection probabilities\n    and grid prior probabilities.\n\n    This class reads in cell footprint estimation and the grid model wit prior probabilities.\n    The output is a DataFrame that represents cell connection probabilities and\n     posterior probabilities for each cell and grid id combination for a given date.\n    \"\"\"\n\n    COMPONENT_ID = \"CellConnectionProbabilityEstimation\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.data_period_start = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n\n        self.data_period_end = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n\n        self.data_period_dates = [\n            self.data_period_start + datetime.timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n        self.landuse_prior_weights = self.config.geteval(self.COMPONENT_ID, \"landuse_prior_weights\")\n        self.current_date = None\n        self.current_cell_footprint = None\n        self.partition_number = self.config.getint(self.COMPONENT_ID, \"partition_number\")\n\n    def initalize_data_objects(self):\n\n        self.clear_destination_directory = self.config.getboolean(self.COMPONENT_ID, \"clear_destination_directory\")\n\n        # Input\n        self.input_data_objects = {}\n        self.use_land_use_prior = self.config.getboolean(self.COMPONENT_ID, \"use_land_use_prior\")\n\n        inputs = {\n            \"cell_footprint_data_silver\": SilverCellFootprintDataObject,\n        }\n\n        if self.use_land_use_prior:\n            inputs[\"enriched_grid_data_silver\"] = SilverEnrichedGridDataObject\n\n        for key, value in inputs.items():\n            path = self.config.get(CONFIG_SILVER_PATHS_KEY, key)\n            if check_if_data_path_exists(self.spark, path):\n                self.input_data_objects[value.ID] = value(self.spark, path)\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")\n                raise ValueError(f\"Invalid path for {value.ID} in component {self.COMPONENT_ID} initialization\")\n\n        # Output\n        self.output_data_objects = {}\n        silver_cell_probabilities_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"cell_connection_probabilities_data_silver\"\n        )\n\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, silver_cell_probabilities_path)\n\n        self.output_data_objects[SilverCellConnectionProbabilitiesDataObject.ID] = (\n            SilverCellConnectionProbabilitiesDataObject(\n                self.spark,\n                silver_cell_probabilities_path,\n            )\n        )\n\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n        for current_date in self.data_period_dates:\n\n            self.logger.info(f\"Processing cell footprint for {current_date.strftime('%Y-%m-%d')}\")\n\n            self.current_date = current_date\n\n            self.current_cell_footprint = self.input_data_objects[SilverCellFootprintDataObject.ID].df.filter(\n                (F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(current_date))\n            )\n\n            self.transform()\n            self.write()\n            self.spark.catalog.clearCache()\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n\n        cell_footprint_df = self.current_cell_footprint\n\n        # Calculate the cell connection probabilities\n\n        window_spec = Window.partitionBy(ColNames.year, ColNames.month, ColNames.day, ColNames.grid_id)\n\n        cell_conn_probs_df = cell_footprint_df.withColumn(\n            ColNames.cell_connection_probability,\n            F.col(ColNames.signal_dominance) / F.sum(ColNames.signal_dominance).over(window_spec),\n        )\n        # Calculate the posterior probabilities\n\n        if self.use_land_use_prior:\n\n            enriched_grid_sdf = self.input_data_objects[SilverEnrichedGridDataObject.ID].df.select(\n                ColNames.grid_id, ColNames.landuse_area_ratios\n            )\n            # calculate landuse prior probabilities\n            landuse_prior_sdf = self.calculate_landuse_prior_probabilities(\n                enriched_grid_sdf, self.landuse_prior_weights\n            )\n\n            cell_conn_probs_df = cell_conn_probs_df.join(landuse_prior_sdf, on=ColNames.grid_id)\n            cell_conn_probs_df = cell_conn_probs_df.withColumn(\n                ColNames.posterior_probability,\n                F.col(ColNames.cell_connection_probability) * F.col(ColNames.prior_probability),\n            )\n\n        elif not self.use_land_use_prior:\n            # Uniform prior\n            cell_conn_probs_df = cell_conn_probs_df.withColumn(\n                ColNames.posterior_probability,\n                F.col(ColNames.cell_connection_probability),\n            )\n\n        # Normalize the posterior probabilities per cell\n        window_spec = Window.partitionBy(ColNames.year, ColNames.month, ColNames.day, ColNames.cell_id)\n\n        cell_conn_probs_df = cell_conn_probs_df.withColumn(\n            ColNames.posterior_probability,\n            F.col(ColNames.posterior_probability) / F.sum(ColNames.posterior_probability).over(window_spec),\n        )\n\n        # Coalesce potential 0 division results to 0\n        cell_conn_probs_df = cell_conn_probs_df.fillna(0.0, subset=[ColNames.posterior_probability])\n\n        cell_conn_probs_df = utils.apply_schema_casting(\n            cell_conn_probs_df, SilverCellConnectionProbabilitiesDataObject.SCHEMA\n        )\n        cell_conn_probs_df = cell_conn_probs_df.coalesce(self.partition_number)\n\n        self.output_data_objects[SilverCellConnectionProbabilitiesDataObject.ID].df = cell_conn_probs_df\n\n    def calculate_landuse_prior_probabilities(self, enriched_grid_sdf, landuse_prior_weights):\n        \"\"\"\n        Calculates the landuse prior probabilities for each grid tile using area shares of various types\n        of landuse of the tile and preconfigured weights for different landuse types.\n\n        Returns:\n            DataFrame: DataFrame with grid id and prior probabilities\n        \"\"\"\n        # Create a DataFrame from the weights dictionary\n        weights_list = [(k, float(v)) for k, v in landuse_prior_weights.items()]\n        weights_df = self.spark.createDataFrame(weights_list, [\"category\", \"weight\"])\n\n        # Explode the map to rows (one row per category per grid)\n        exploded_df = enriched_grid_sdf.select(\n            ColNames.grid_id, F.explode(ColNames.landuse_area_ratios).alias(\"category\", \"area_ratio\")\n        )\n\n        # Join with weights and calculate weighted values\n        weighted_df = (\n            exploded_df.join(weights_df, on=\"category\", how=\"left\")\n            .withColumn(\"weight\", F.coalesce(F.col(\"weight\"), F.lit(0.0)))  # Default 0.0 for missing weights\n            .withColumn(\"weighted_value\", F.col(\"area_ratio\") * F.col(\"weight\"))\n        )\n\n        # Sum up weighted values by grid_id\n        grid_prior_sdf = weighted_df.groupBy(ColNames.grid_id).agg(F.sum(\"weighted_value\").alias(\"weighted_value\"))\n\n        # Normalize the prior probabilities over all grids\n        total_prior = grid_prior_sdf.agg(F.sum(\"weighted_value\")).collect()[0][0]\n        grid_prior_sdf = grid_prior_sdf.withColumn(ColNames.prior_probability, F.col(\"weighted_value\") / total_prior)\n\n        return grid_prior_sdf\n</code></pre>"},{"location":"reference/components/execution/cell_connection_probability/cell_connection_probability/#components.execution.cell_connection_probability.cell_connection_probability.CellConnectionProbabilityEstimation.calculate_landuse_prior_probabilities","title":"<code>calculate_landuse_prior_probabilities(enriched_grid_sdf, landuse_prior_weights)</code>","text":"<p>Calculates the landuse prior probabilities for each grid tile using area shares of various types of landuse of the tile and preconfigured weights for different landuse types.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <p>DataFrame with grid id and prior probabilities</p> Source code in <code>multimno/components/execution/cell_connection_probability/cell_connection_probability.py</code> <pre><code>def calculate_landuse_prior_probabilities(self, enriched_grid_sdf, landuse_prior_weights):\n    \"\"\"\n    Calculates the landuse prior probabilities for each grid tile using area shares of various types\n    of landuse of the tile and preconfigured weights for different landuse types.\n\n    Returns:\n        DataFrame: DataFrame with grid id and prior probabilities\n    \"\"\"\n    # Create a DataFrame from the weights dictionary\n    weights_list = [(k, float(v)) for k, v in landuse_prior_weights.items()]\n    weights_df = self.spark.createDataFrame(weights_list, [\"category\", \"weight\"])\n\n    # Explode the map to rows (one row per category per grid)\n    exploded_df = enriched_grid_sdf.select(\n        ColNames.grid_id, F.explode(ColNames.landuse_area_ratios).alias(\"category\", \"area_ratio\")\n    )\n\n    # Join with weights and calculate weighted values\n    weighted_df = (\n        exploded_df.join(weights_df, on=\"category\", how=\"left\")\n        .withColumn(\"weight\", F.coalesce(F.col(\"weight\"), F.lit(0.0)))  # Default 0.0 for missing weights\n        .withColumn(\"weighted_value\", F.col(\"area_ratio\") * F.col(\"weight\"))\n    )\n\n    # Sum up weighted values by grid_id\n    grid_prior_sdf = weighted_df.groupBy(ColNames.grid_id).agg(F.sum(\"weighted_value\").alias(\"weighted_value\"))\n\n    # Normalize the prior probabilities over all grids\n    total_prior = grid_prior_sdf.agg(F.sum(\"weighted_value\")).collect()[0][0]\n    grid_prior_sdf = grid_prior_sdf.withColumn(ColNames.prior_probability, F.col(\"weighted_value\") / total_prior)\n\n    return grid_prior_sdf\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/","title":"cell_footprint","text":""},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/","title":"cell_footprint_estimation","text":"<p>Module that computes the grid footprint of cells</p>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation","title":"<code>CellFootprintEstimation</code>","text":"<p>               Bases: <code>Component</code></p> <p>This class is responsible for modeling the signal strength of a cellular network.</p> <p>It takes as input a configuration file and a set of data representing the network's cells and their properties. The class then calculates the signal strength at various points of a grid, taking into account factors such as the distance to the cell, the azimuth and elevation angles, and the directionality of the cell.</p> <p>The class provides methods for adjusting the signal strength based on the horizontal and vertical angles, imputing default cell properties.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>class CellFootprintEstimation(Component):\n    \"\"\"\n    This class is responsible for modeling the signal strength of a cellular network.\n\n    It takes as input a configuration file and a set of data representing the network's cells and their properties.\n    The class then calculates the signal strength at various points of a grid, taking into account factors such\n    as the distance to the cell, the azimuth and elevation angles, and the directionality of the cell.\n\n    The class provides methods for adjusting the signal strength based on the horizontal and vertical angles,\n    imputing default cell properties.\n    \"\"\"\n\n    COMPONENT_ID = \"CellFootprintEstimation\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.data_period_start = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n        self.data_period_end = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n\n        self.do_azimuth_angle_adjustments = self.config.getboolean(self.COMPONENT_ID, \"do_azimuth_angle_adjustments\")\n        self.do_elevation_angle_adjustments = self.config.getboolean(\n            self.COMPONENT_ID, \"do_elevation_angle_adjustments\"\n        )\n        self.default_cell_properties = self.config.geteval(self.COMPONENT_ID, \"default_cell_physical_properties\")\n\n        self.logistic_function_steepness = self.config.getfloat(self.COMPONENT_ID, \"logistic_function_steepness\")\n        self.logistic_function_midpoint = self.config.getfloat(self.COMPONENT_ID, \"logistic_function_midpoint\")\n        self.signal_dominance_threshold = self.config.getfloat(self.COMPONENT_ID, \"signal_dominance_threshold\")\n        self.max_cells_per_grid_tile = self.config.getfloat(self.COMPONENT_ID, \"max_cells_per_grid_tile\")\n\n        self.do_dynamic_coverage_range_calculation = self.config.getboolean(\n            self.COMPONENT_ID, \"do_dynamic_coverage_range_calculation\"\n        )\n        self.repartition_number = self.config.getint(self.COMPONENT_ID, \"repartition_number\")\n        self.coverage_range_line_buffer = self.config.getint(self.COMPONENT_ID, \"coverage_range_line_buffer\")\n\n        self.percentage_of_best_sd_threshold = self.config.getfloat(\n            self.COMPONENT_ID, \"percentage_of_best_sd_threshold\"\n        )\n        self.do_sd_threshold_pruning = self.config.getboolean(self.COMPONENT_ID, \"do_sd_threshold_pruning\")\n        self.do_max_cells_per_tile_pruning = self.config.getboolean(self.COMPONENT_ID, \"do_max_cells_per_tile_pruning\")\n        self.do_percentage_of_best_sd_pruning = self.config.getboolean(\n            self.COMPONENT_ID, \"do_percentage_of_best_sd_pruning\"\n        )\n\n        self.data_period_dates = [\n            self.data_period_start + datetime.timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n        self.sd_azimuth_mapping_sdf = None\n        self.sd_elevation_mapping_sdf = None\n        self.current_date = None\n        self.current_cells_sdf = None\n\n        self.partition_number = self.config.getint(self.COMPONENT_ID, \"partition_number\")\n\n    def initalize_data_objects(self):\n\n        self.clear_destination_directory = self.config.getboolean(self.COMPONENT_ID, \"clear_destination_directory\")\n        self.cartesian_crs = self.config.get(CellFootprintEstimation.COMPONENT_ID, \"cartesian_crs\")\n        self.use_elevation = self.config.getboolean(CellFootprintEstimation.COMPONENT_ID, \"use_elevation\")\n\n        self.input_data_objects = {}\n        # Input\n        inputs = {\n            \"network_data_silver\": SilverNetworkDataObject,\n        }\n        if self.use_elevation:\n            inputs[\"enriched_grid_data_silver\"] = SilverEnrichedGridDataObject\n        else:\n            inputs[\"grid_data_silver\"] = SilverGridDataObject\n\n        for key, value in inputs.items():\n            path = self.config.get(CONFIG_SILVER_PATHS_KEY, key)\n            if check_if_data_path_exists(self.spark, path):\n                self.input_data_objects[value.ID] = value(self.spark, path)\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")\n                raise ValueError(f\"Invalid path for {value.ID}: {path}\")\n\n        # Output\n        self.output_data_objects = {}\n        cell_footprint_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"cell_footprint_data_silver\")\n\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, cell_footprint_path)\n\n        self.output_data_objects[SilverCellFootprintDataObject.ID] = SilverCellFootprintDataObject(\n            self.spark, cell_footprint_path\n        )\n\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n\n        # for every date in the data period, get the events\n        # for that date and calculate the time segments\n        for current_date in self.data_period_dates:\n\n            self.logger.info(f\"Processing cell plan for {current_date.strftime('%Y-%m-%d')}\")\n\n            self.current_date = current_date\n\n            self.current_cells_sdf = (\n                self.input_data_objects[SilverNetworkDataObject.ID]\n                .df.filter(\n                    (\n                        F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n                        == F.lit(current_date)\n                    )\n                )\n                .select(\n                    ColNames.cell_id,\n                    ColNames.cell_type,\n                    ColNames.antenna_height,\n                    ColNames.power,\n                    ColNames.range,\n                    ColNames.horizontal_beam_width,\n                    ColNames.vertical_beam_width,\n                    ColNames.altitude,\n                    ColNames.latitude,\n                    ColNames.longitude,\n                    ColNames.directionality,\n                    ColNames.azimuth_angle,\n                    ColNames.elevation_angle,\n                )\n            )\n\n            self.transform()\n            self.write()\n            self.spark.catalog.clearCache()\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n\n        if self.use_elevation:\n            grid_sdf = self.input_data_objects[SilverEnrichedGridDataObject.ID].df.select(\n                ColNames.grid_id, ColNames.geometry, ColNames.elevation\n            )\n        else:\n            grid_sdf = self.input_data_objects[SilverGridDataObject.ID].df.select(ColNames.grid_id, ColNames.geometry)\n\n        grid_sdf = self.add_z_to_point_geometry(grid_sdf, ColNames.geometry, self.use_elevation)\n        grid_sdf = grid_sdf.withColumnRenamed(ColNames.geometry, ColNames.joined_geometry)\n\n        current_cells_sdf = self.current_cells_sdf\n\n        current_cells_sdf = self.impute_default_cell_properties(current_cells_sdf)\n\n        current_cells_sdf = self.watt_to_dbm(current_cells_sdf)\n\n        # TODO: Add Path Loss Exponent calculation based on grid landuse data\n\n        # Create geometries\n        current_cells_sdf = self.create_cell_point_geometry(current_cells_sdf, self.use_elevation)\n        current_cells_sdf = utils.project_to_crs(current_cells_sdf, 4326, self.cartesian_crs)\n\n        if self.do_azimuth_angle_adjustments:\n            # get standard deviation mapping table for azimuth beam width azimuth back loss pairs\n            self.sd_azimuth_mapping_sdf = self.get_angular_adjustments_sd_mapping(\n                current_cells_sdf,\n                ColNames.horizontal_beam_width,\n                ColNames.azimuth_signal_strength_back_loss,\n                \"azimuth\",\n            )\n\n        self.sd_azimuth_mapping_sdf = F.broadcast(self.sd_azimuth_mapping_sdf)\n\n        if self.do_elevation_angle_adjustments:\n            # get standard deviation mapping table for elevation beam width elevation back loss pairs\n            self.sd_elevation_mapping_sdf = self.get_angular_adjustments_sd_mapping(\n                current_cells_sdf,\n                ColNames.vertical_beam_width,\n                ColNames.elevation_signal_strength_back_loss,\n                \"elevation\",\n            )\n\n        self.sd_elevation_mapping_sdf = F.broadcast(self.sd_elevation_mapping_sdf)\n\n        if self.do_dynamic_coverage_range_calculation:\n            current_cells_sdf = self.calculate_effective_coverage(current_cells_sdf, grid_sdf)\n            current_cells_sdf = current_cells_sdf.dropna(subset=[\"coverage_center\"])\n            current_cells_sdf = current_cells_sdf.filter(F.col(\"coverage_effective_range\") &gt; 100)\n            current_cells_sdf = current_cells_sdf.repartition(self.repartition_number)\n            current_cells_sdf.cache()\n            current_cells_sdf.count()\n            current_cell_grid_sdf = self.spatial_join_within_distance(\n                current_cells_sdf, grid_sdf, \"coverage_center\", \"coverage_effective_range\"\n            )\n        else:\n            current_cells_sdf = current_cells_sdf.repartition(self.repartition_number)\n            current_cell_grid_sdf = self.spatial_join_within_distance(current_cells_sdf, grid_sdf, \"geometry\", \"range\")\n        # Calculate planar and 3D distances\n        current_cell_grid_sdf = self.calculate_cartesian_distances(current_cell_grid_sdf)\n\n        current_cell_grid_sdf = self.calculate_signal_dominance(\n            current_cell_grid_sdf, self.do_azimuth_angle_adjustments, self.do_elevation_angle_adjustments\n        )\n\n        current_cell_grid_sdf = current_cell_grid_sdf.drop(\n            ColNames.distance_to_cell_3D,\n            ColNames.distance_to_cell,\n            ColNames.azimuth_angle,\n            ColNames.elevation_angle,\n            ColNames.horizontal_beam_width,\n            ColNames.vertical_beam_width,\n            ColNames.azimuth_signal_strength_back_loss,\n            ColNames.elevation_signal_strength_back_loss,\n        )\n\n        # Prune small signal dominance values\n        if self.do_sd_threshold_pruning:\n            current_cell_grid_sdf = self.prune_small_signal_dominance(\n                current_cell_grid_sdf, self.signal_dominance_threshold\n            )\n\n        # Prune signal dominance based on percentage of best value\n        if self.do_percentage_of_best_sd_pruning:\n            current_cell_grid_sdf = self.prune_signal_percentage_of_best_sd(\n                current_cell_grid_sdf, self.percentage_of_best_sd_threshold\n            )\n\n        # Prune max cells per grid tile\n        if self.do_max_cells_per_tile_pruning:\n            current_cell_grid_sdf = self.prune_max_cells_per_grid_tile(\n                current_cell_grid_sdf, self.max_cells_per_grid_tile\n            )\n\n        # get year, month, day from current date\n\n        current_cell_grid_sdf = current_cell_grid_sdf.withColumns(\n            {\n                ColNames.year: F.lit(self.current_date.year),\n                ColNames.month: F.lit(self.current_date.month),\n                ColNames.day: F.lit(self.current_date.day),\n            }\n        )\n\n        current_cell_grid_sdf = utils.apply_schema_casting(current_cell_grid_sdf, SilverCellFootprintDataObject.SCHEMA)\n\n        current_cell_grid_sdf = current_cell_grid_sdf.coalesce(self.partition_number)\n\n        self.output_data_objects[SilverCellFootprintDataObject.ID].df = current_cell_grid_sdf\n\n    def impute_default_cell_properties(self, sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Imputes default cell properties for null values in the input DataFrame using\n        default properties for cell types from config.\n\n        Args:\n            sdf (DataFrame): Input DataFrame.\n\n        Returns:\n            DataFrame: DataFrame with imputed default cell properties.\n        \"\"\"\n        default_properties_df = self.create_default_properties_df()\n\n        # add default prefix to the columns of default_properties_df\n        default_properties_df = default_properties_df.select(\n            [F.col(col).alias(f\"default_{col}\") for col in default_properties_df.columns]\n        )\n\n        # assign default cell type to cell types not present in config\n        sdf = sdf.withColumn(\n            ColNames.cell_type,\n            F.when(\n                F.col(ColNames.cell_type).isin(list(self.default_cell_properties.keys())),\n                F.col(ColNames.cell_type),\n            ).otherwise(\"default\"),\n        )\n\n        # all cell types which are absent from the default_properties_df will be assigned default values\n        sdf = sdf.join(\n            default_properties_df,\n            sdf[ColNames.cell_type] == default_properties_df[f\"default_{ColNames.cell_type}\"],\n            how=\"inner\",\n        )\n        # if orignal column is null, assign the default value\n        for col in default_properties_df.columns:\n            col = col.replace(\"default_\", \"\")\n            if col not in sdf.columns:\n                sdf = sdf.withColumn(col, F.lit(None))\n            sdf = sdf.withColumn(col, F.coalesce(F.col(col), F.col(f\"default_{col}\")))\n\n        return sdf.drop(*default_properties_df.columns)\n\n    def create_default_properties_df(self) -&gt; DataFrame:\n        \"\"\"\n        Creates a DataFrame with default cell properties from config dict.\n\n        Returns:\n            DataFrame: A DataFrame with default cell properties.\n        \"\"\"\n\n        rows = [Row(cell_type=k, **v) for k, v in self.default_cell_properties.items()]\n        return self.spark.createDataFrame(rows).withColumnRenamed(\"cell_type\", ColNames.cell_type)\n\n    # create geometry for cells. Set Z values if elevation is taken into account from z column, otherwise to 0\n    @staticmethod\n    def create_cell_point_geometry(sdf: DataFrame, use_elevation: bool) -&gt; DataFrame:\n        \"\"\"\n        Creates cell point geometry.\n        If elevation is taken into account, set Z values from z column, otherwise to 0.\n\n        Args:\n            sdf (DataFrame): Input DataFrame.\n            use_elevation (bool): Whether to use elevation.\n\n        Returns:\n            DataFrame: DataFrame with cell point geometry.\n        \"\"\"\n        if use_elevation:\n            sdf = sdf.withColumn(\n                ColNames.geometry,\n                STC.ST_MakePoint(\n                    F.col(ColNames.longitude),\n                    F.col(ColNames.latitude),\n                    F.col(ColNames.altitude) + F.col(ColNames.antenna_height),\n                ),\n            )\n        else:\n            sdf = sdf.withColumn(\n                ColNames.geometry,\n                STC.ST_MakePoint(\n                    F.col(ColNames.longitude),\n                    F.col(ColNames.latitude),\n                    F.col(ColNames.antenna_height),\n                ),\n            )\n        # assign crs\n        sdf = sdf.withColumn(ColNames.geometry, STF.ST_SetSRID(F.col(ColNames.geometry), F.lit(4326)))\n\n        return sdf.drop(ColNames.latitude, ColNames.longitude, ColNames.altitude, ColNames.antenna_height)\n\n    # add z value to the grid geometry if elevation is taken into account from z column in the grid otherwise set to 0\n    @staticmethod\n    def add_z_to_point_geometry(sdf: DataFrame, geometry_col: str, use_elevation: bool) -&gt; DataFrame:\n        \"\"\"\n        Adds z value to the point geometry (grid centroids).\n        If elevation is taken into account, set Z values from z column, otherwise to 0.\n\n        Args:\n            sdf (DataFrame): Input DataFrame.\n            use_elevation (bool): Whether to use elevation.\n\n        Returns:\n            DataFrame: DataFrame with z value added to point geometry.\n        \"\"\"\n        if use_elevation:\n            sdf = sdf.withColumn(\n                geometry_col,\n                STC.ST_MakePoint(\n                    STF.ST_X(F.col(geometry_col)),\n                    STF.ST_Y(F.col(geometry_col)),\n                    F.col(ColNames.elevation),\n                ),\n            )\n        else:\n            sdf = sdf.withColumn(\n                geometry_col,\n                STC.ST_MakePoint(\n                    STF.ST_X(F.col(geometry_col)),\n                    STF.ST_Y(F.col(geometry_col)),\n                    F.lit(0.0),\n                ),\n            )\n\n        return sdf.drop(ColNames.elevation)\n\n    @staticmethod\n    def spatial_join_within_distance(\n        sdf_from: DataFrame, sdf_to: DataFrame, geometry_col: str, within_distance_col: str\n    ) -&gt; DataFrame:\n        \"\"\"\n        Performs a spatial join within a specified distance.\n\n        Args:\n            sdf_from (DataFrame): Input DataFrame.\n            sdf_to (DataFrame): DataFrame to join with.\n            within_distance_col (str): Column name for the within distance.\n\n        Returns:\n            DataFrame: DataFrame after performing the spatial join.\n        \"\"\"\n\n        sdf_merged = (\n            sdf_from.alias(\"a\")\n            .join(\n                sdf_to.alias(\"b\"),\n                STP.ST_Intersects(\n                    STF.ST_Buffer(f\"a.{geometry_col}\", f\"a.{within_distance_col}\"),\n                    f\"b.{ColNames.joined_geometry}\",\n                ),\n            )\n            .drop(f\"a.{within_distance_col}\")\n        )\n\n        return sdf_merged\n\n    @staticmethod\n    def calculate_cartesian_distances(sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Calculates cartesian distances.\n\n        Args:\n            sdf (DataFrame): Input DataFrame.\n\n        Returns:\n            DataFrame: DataFrame with calculated cartesian distances.\n        \"\"\"\n\n        sdf = sdf.withColumns(\n            {\n                ColNames.distance_to_cell_3D: STF.ST_3DDistance(\n                    F.col(ColNames.geometry), F.col(ColNames.joined_geometry)\n                ),\n                ColNames.distance_to_cell: STF.ST_Distance(F.col(ColNames.geometry), F.col(ColNames.joined_geometry)),\n            }\n        )\n\n        return sdf\n\n    @staticmethod\n    def watt_to_dbm(sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Converts power from watt to dBm.\n\n        Args:\n            sdf (DataFrame): Input DataFrame.\n\n        Returns:\n            DataFrame: DataFrame with power converted to dBm.\n        \"\"\"\n        return sdf.withColumn(ColNames.power, 10.0 * F.log10(F.col(ColNames.power)) + 30.0)\n\n    @staticmethod\n    def join_sd_mapping(\n        sdf: DataFrame,\n        sd_mapping_sdf: DataFrame,\n        beam_width_col: str,\n        signal_front_back_difference_col: str,\n        sd_col: str,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Joins DataFrame with standard deviation mapping.\n\n        Args:\n            sdf (DataFrame): Input DataFrame.\n            sd_mapping_sdf (DataFrame): DataFrame with standard deviation mapping.\n            beam_width_col (str): Column name for the beam width.\n            signal_front_back_difference_col (str): Column name for the signal front-back difference.\n\n        Returns:\n            DataFrame: DataFrame after joining with standard deviation mapping.\n        \"\"\"\n\n        join_condition = (F.col(f\"a.{beam_width_col}\") == F.col(f\"b.{beam_width_col}\")) &amp; (\n            F.col(f\"a.{signal_front_back_difference_col}\") == F.col(f\"b.{signal_front_back_difference_col}\")\n        )\n\n        sdf = sdf.alias(\"a\").join(sd_mapping_sdf.alias(\"b\"), join_condition).select(f\"a.*\", f\"b.{sd_col}\")\n\n        return sdf\n\n    def get_angular_adjustments_sd_mapping(\n        self,\n        cells_sdf: DataFrame,\n        beam_width_col: str,\n        signal_front_back_difference_col: str,\n        angular_adjustment_type: str,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Calculates standard deviations in signal strength based on beam width and front-back cell signal difference.\n\n        Args:\n            cells_sdf (DataFrame): Input DataFrame.\n            beam_width_col (str): Column name for the beam width.\n            signal_front_back_difference_col (str): Column name for the signal front-back difference.\n            angular_adjustment_type (str): Type of angular adjustment.\n\n        Returns:\n            DataFrame: DataFrame with angular adjustments standard deviation mapping.\n        \"\"\"\n\n        sd_mappings = CellFootprintEstimation.get_sd_to_signal_back_loss_mappings(\n            cells_sdf, signal_front_back_difference_col\n        )\n        beam_widths_diff = (\n            cells_sdf.select(F.col(beam_width_col), F.col(signal_front_back_difference_col))\n            .where(F.col(ColNames.directionality) == 1)\n            .distinct()\n        )\n        beam_widths_diff = [row.asDict() for row in beam_widths_diff.collect()]\n\n        beam_sds = []\n        for item in beam_widths_diff:\n            item[f\"sd_{angular_adjustment_type}\"] = self.find_sd(\n                item[beam_width_col],\n                sd_mappings[sd_mappings[signal_front_back_difference_col] == item[signal_front_back_difference_col]],\n            )\n            beam_sds.append(item)\n\n        beam_sd_sdf = self.spark.createDataFrame(beam_sds)\n\n        return beam_sd_sdf\n\n    @staticmethod\n    def find_sd(beam_width: float, mapping: DataFrame) -&gt; float:\n        \"\"\"\n        Finds the standard deviation corresponding to the given beam width using the provided mapping.\n\n        Args:\n            beam_width (float): The width of the beam in degrees.\n            mapping (DataFrame): A DataFrame where each row corresponds to a standard deviation\n                and contains the corresponding angle.\n\n        Returns:\n            float: The standard deviation corresponding to the given beam width.\n        \"\"\"\n        min_diff_index = (abs(mapping[\"deg\"] - beam_width / 2)).idxmin()\n        return float(mapping.loc[min_diff_index, \"sd\"])\n\n    @staticmethod\n    def get_sd_to_signal_back_loss_mappings(cells_sdf: DataFrame, signal_front_back_difference_col: str) -&gt; DataFrame:\n        \"\"\"\n        Generates a DataFrame with mapping of signal strength standard deviation for each\n            elevation/azimuth angle degree.\n\n        Parameters:\n        cells_sdf (DataFrame): A Spark DataFrame containing information about the cells.\n        signal_front_back_difference_col (str): The name of the column that contains the difference\n            in signal strength between\n        the front and back of the cell.\n\n        Returns:\n        DataFrame: A pandas DataFrame with standard deviation mappings.\n\n        \"\"\"\n        db_back_diffs = (\n            cells_sdf.select(F.col(signal_front_back_difference_col))\n            .where(F.col(ColNames.directionality) == 1)\n            .distinct()\n        )\n        db_back_diffs = [row.asDict()[signal_front_back_difference_col] for row in db_back_diffs.collect()]\n        mappings = [\n            CellFootprintEstimation.create_mapping(item, signal_front_back_difference_col) for item in db_back_diffs\n        ]\n\n        return pd.concat(mappings)\n\n    @staticmethod\n    def create_mapping(db_back: float, signal_front_back_difference_col) -&gt; DataFrame:\n        \"\"\"\n        Creates a mapping between standard deviation and the angle\n        at which the signal strength falls to 3 dB below its maximum value.\n\n        Args:\n            db_back (float): The difference in signal strength in dB between the front and back of the signal.\n\n        Returns:\n            DataFrame: A DataFrame where each row corresponds to a\n            standard deviation and contains the corresponding angle.\n        \"\"\"\n        idf = pd.DataFrame({\"sd\": np.arange(180 / 1000, 180, 180 / 1000)})\n        idf[\"deg\"] = idf[\"sd\"].apply(CellFootprintEstimation.get_min3db, db_back=db_back)\n        df = pd.DataFrame({\"deg\": np.arange(1, 181)})\n        df[\"sd\"] = df[\"deg\"].apply(lambda dg: idf.loc[np.abs(idf[\"deg\"] - dg).idxmin(), \"sd\"])\n        df[signal_front_back_difference_col] = db_back\n        return df\n\n    @staticmethod\n    def get_min3db(sd: float, db_back: float) -&gt; float:\n        \"\"\"\n        Finds the angle at which the signal strength falls to 3 dB below its maximum value.\n\n        Args:\n            sd (float): The standard deviation of the normal distribution modeling the signal strength.\n            db_back (float): The difference in signal strength in dB between the front and back of the signal.\n\n        Returns:\n            float: The angle at which the signal strength falls to 3 dB below its maximum value.\n        \"\"\"\n        df = pd.DataFrame({\"a\": np.linspace(0, 180, 720)})\n        df[\"dbLoss\"] = CellFootprintEstimation.norm_dBloss(df[\"a\"], db_back=db_back, sd=sd)\n        return df.loc[np.abs(-3 - df[\"dbLoss\"]).idxmin(), \"a\"]\n\n    @staticmethod\n    def norm_dBloss(a: float, sd: float, db_back: float) -&gt; float:\n        \"\"\"\n        Computes the loss in signal strength in dB as a function of\n        angle from the direction of maximum signal strength.\n\n        Args:\n            a (float): The angle from the direction of maximum signal strength.\n            sd (float): The standard deviation of the normal distribution modeling the signal strength.\n            db_back (float): The difference in signal strength in dB between the front and back of the signal.\n\n        Returns:\n            float: The loss in signal strength in dB at the given angle.\n        \"\"\"\n        a = ((a + 180) % 360) - 180\n        inflate = -db_back / (\n            CellFootprintEstimation.normal_distribution(0, 0, sd)\n            - CellFootprintEstimation.normal_distribution(180, 0, sd)\n        )\n        return (\n            CellFootprintEstimation.normal_distribution(a, 0, sd)\n            - CellFootprintEstimation.normal_distribution(0, 0, sd)\n        ) * inflate\n\n    @staticmethod\n    def normal_distribution(x: float, mean: float, sd: float) -&gt; Union[np.array, list]:\n        \"\"\"\n        Computes the value of the normal distribution with the given mean\n        and standard deviation at the given point.\n\n        Args:\n            x (float): The point at which to evaluate the normal distribution.\n            mean (float): The mean of the normal distribution.\n            sd (float): The standard deviation of the normal distribution.\n\n        Returns:\n            (np.array | list): The value of the normal distribution at the given point,\n            returned as either a numpy array or a list.\n        \"\"\"\n        n_dist = (1.0 / (sqrt(2.0 * pi) * sd)) * np.exp(-0.5 * ((x - mean) / sd) ** 2)\n\n        return n_dist\n\n    def calculate_effective_coverage(self, cells_sdf: DataFrame, grid_sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Calculates effective cell coverage center and range based on desired signal domincance threshold.\n\n        The function first separates the cells into omnidirectional and directional types,\n        then calculates the signal dominance threshold points for each type.\n        The function then calculates the effective coverage center and range for each cell\n        based on the signal dominance threshold points.\n\n        Parameters:\n        cells_sdf (pyspark.sql.DataFrame): A Spark DataFrame containing cell information, including directionality.\n        grid_sdf (pyspark.sql.DataFrame): A Spark DataFrame containing grid used for calculating signal dominance.\n\n        Returns:\n        pyspark.sql.DataFrame: A Spark DataFrame with the effective coverage information for each cell, including the\n                            coverage center and effective range. The DataFrame excludes intermediate columns used\n                            during the calculation.\n        \"\"\"\n        # omnidirectional cells\n        cells_omni_sdf = cells_sdf.filter(F.col(ColNames.directionality) == 0)\n\n        if cells_omni_sdf.rdd.isEmpty():\n            cells_omni_sdf = self.spark.createDataFrame([], cells_sdf.schema)\n        else:\n            cells_omni_sdf = self.get_signal_dominance_threshold_point(cells_omni_sdf, grid_sdf, \"omni\")\n\n            cells_omni_sdf = cells_omni_sdf.withColumn(\"coverage_center\", F.col(\"geometry\")).withColumn(\n                \"coverage_effective_range\", STF.ST_Distance(F.col(\"coverage_center\"), F.col(\"omni\"))\n            )\n        cells_omni_sdf.cache()\n        cells_omni_sdf.count()\n\n        # directional cells\n        cells_directional_sdf = cells_sdf.filter(F.col(ColNames.directionality) == 1)\n\n        if cells_directional_sdf.rdd.isEmpty():\n            cells_directional_sdf = self.spark.createDataFrame([], cells_sdf.schema)\n        else:\n            cells_directional_sdf = self.get_signal_dominance_threshold_point(\n                cells_directional_sdf, grid_sdf, \"directional_front\"\n            )\n\n            cells_directional_sdf.cache()\n            cells_directional_sdf.count()\n\n            cells_directional_sdf = self.get_signal_dominance_threshold_point(\n                cells_directional_sdf, grid_sdf, \"directional_back\"\n            )\n\n            cells_directional_sdf = cells_directional_sdf.withColumn(\n                \"coverage_center\",\n                STF.ST_LineInterpolatePoint(\n                    STF.ST_MakeLine(\n                        cells_directional_sdf[\"directional_front\"], cells_directional_sdf[\"directional_back\"]\n                    ),\n                    0.5,\n                ),\n            )\n            cells_directional_sdf = cells_directional_sdf.withColumn(\n                \"coverage_effective_range\",\n                STF.ST_Distance(cells_directional_sdf[\"coverage_center\"], cells_directional_sdf[\"directional_front\"]),\n            )\n\n        cells_sdf = cells_omni_sdf.unionByName(cells_directional_sdf, allowMissingColumns=True)\n\n        return cells_sdf.drop(\"omni\", \"directional_front\", \"directional_back\")\n\n    def get_signal_dominance_threshold_point(self, cells_sdf, grid_sdf, point_type):\n        \"\"\"\n        Calculates the signal dominance threshold points in cell maximum range.\n\n        For omnidirectional cell types, the signal dominance threshold point is calculated as\n        the furthest point along 90 degrees azimuth direction where signal dominance is less than the threshold.\n        For directional cells types, two signal dominance threshold points are calculated:\n            1. the furthest point along the directionality angle direction where signal dominance is less than the threshold\n            2. the furthest point opposite to the directionality angle direction where signal dominance is less than the threshold\n\n        Args:\n            cells_sdf (DataFrame): The DataFrame containing cell information.\n            grid_sdf (DataFrame): The DataFrame containing grid information.\n            point_type (str): The type of point to calculate the signal dominance threshold for.\n\n        Returns:\n            (DataFrame): The updated cells DataFrame with the signal dominance threshold point added.\n        \"\"\"\n\n        do_azimuth_angle_adjustments = True\n        do_elevation_angle_adjustments = True\n\n        if point_type == \"directional_front\":\n            # Calculate range line for directional front point type\n            cells_sdf = cells_sdf.withColumn(\n                \"range_line\",\n                STF.ST_MakeLine(\n                    \"geometry\",\n                    STC.ST_Point(\n                        STF.ST_X(\"geometry\") + cells_sdf[\"range\"] * F.sin(F.radians(cells_sdf[\"azimuth_angle\"])),\n                        STF.ST_Y(\"geometry\") + cells_sdf[\"range\"] * F.cos(F.radians(cells_sdf[\"azimuth_angle\"])),\n                    ),\n                ),\n            )\n\n        elif point_type == \"directional_back\":\n            # Calculate range line for directional back point type\n            cells_sdf = cells_sdf.withColumn(\n                \"range_line\",\n                STF.ST_MakeLine(\n                    \"geometry\",\n                    STC.ST_Point(\n                        STF.ST_X(\"geometry\") + cells_sdf[\"range\"] * F.sin(F.radians(cells_sdf[\"azimuth_angle\"] + 180)),\n                        STF.ST_Y(\"geometry\") + cells_sdf[\"range\"] * F.cos(F.radians(cells_sdf[\"azimuth_angle\"] + 180)),\n                    ),\n                ),\n            )\n\n        elif point_type == \"omni\":\n            # Calculate range line for omni point type\n            cells_sdf = cells_sdf.withColumn(\n                \"range_line\",\n                STF.ST_MakeLine(\n                    \"geometry\",\n                    STC.ST_Point(\n                        STF.ST_X(\"geometry\") + cells_sdf[\"range\"] * F.sin(F.radians(F.lit(90))),\n                        STF.ST_Y(\"geometry\") + cells_sdf[\"range\"] * F.cos(F.radians(F.lit(90))),\n                    ),\n                ),\n            )\n\n            do_azimuth_angle_adjustments = False\n            do_elevation_angle_adjustments = False\n\n        cells_sdf = cells_sdf.withColumn(\"coverage_range_line_buffer\", F.lit(self.coverage_range_line_buffer))\n\n        cell_grid = self.spatial_join_within_distance(cells_sdf, grid_sdf, \"range_line\", \"coverage_range_line_buffer\")\n\n        # calculate 3d distance and planar distance from cell id to grid ids\n        cell_grid = self.calculate_cartesian_distances(cell_grid)\n\n        cell_grid = self.calculate_signal_dominance(\n            cell_grid, do_elevation_angle_adjustments, do_azimuth_angle_adjustments\n        )\n\n        cell_grid_filtered = cell_grid.filter(F.col(ColNames.signal_dominance) &gt; self.signal_dominance_threshold)\n        cell_counts = cell_grid_filtered.groupBy(\"cell_id\").count().withColumnRenamed(\"count\", \"filtered_count\")\n        cell_grid_joined = cell_grid.join(cell_counts, on=\"cell_id\", how=\"left\").fillna(0, subset=[\"filtered_count\"])\n\n        # In case if desired threshold filter removes all grid tiles in given direction, keep closest grid tile\n        # Otherwise keep furthest point where threshold condition meet\n        window_min = Window.partitionBy(\"cell_id\").orderBy(F.col(\"distance_to_cell\"))\n        window_max = Window.partitionBy(\"cell_id\").orderBy(F.desc(F.col(\"distance_to_cell\")))\n\n        cell_grid_max = (\n            cell_grid_joined.withColumn(\n                \"rank\",\n                F.when(F.col(\"filtered_count\") == 0, F.row_number().over(window_min)).otherwise(\n                    F.row_number().over(window_max)\n                ),\n            )\n            .filter(F.col(\"rank\") == 1)\n            .drop(\"rank\", \"filtered_count\")\n        )\n\n        cells_sdf = cells_sdf.join(\n            cell_grid_max.select(\"cell_id\", \"joined_geometry\").withColumnRenamed(\"joined_geometry\", point_type),\n            \"cell_id\",\n            \"left\",\n        )\n\n        return cells_sdf.drop(\n            \"range_line\", \"distance_to_cell_3d\", \"distance_to_cell\", \"signal_strength\", \"signal_dominance\", \"grid_id\"\n        )\n\n    @staticmethod\n    def calculate_distance_power_loss(sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Calculates distance power loss caluclated as\n        power - path_loss_exponent * 10 * log10(distance_to_cell_3D).\n\n        Args:\n            sdf (DataFrame): Input DataFrame.\n\n        Returns:\n            DataFrame: DataFrame with calculated distance power loss.\n        \"\"\"\n        sdf = sdf.withColumn(\n            ColNames.signal_strength,\n            F.col(ColNames.power)\n            - F.col(ColNames.path_loss_exponent) * 10 * F.log10(F.col(ColNames.distance_to_cell_3D)),\n        )\n\n        return sdf.drop(ColNames.power, ColNames.path_loss_exponent)\n\n    def calculate_signal_dominance(self, cell_grid_gdf, do_elevation_angle_adjustments, do_azimuth_angle_adjustments):\n        \"\"\"\n        Calculates the signal dominance for each cell in a grid DataFrame, optionally adjusting for elevation and azimuth angles.\n\n        This function performs a series of adjustments on the signal strength of each cell in the provided grid DataFrame to calculate the signal dominance. The adjustments include distance power loss, horizontal angle power adjustment (azimuth), and vertical angle power adjustment (elevation), based on the provided flags. Finally, it converts the adjusted signal strength into signal dominance using a logistic function.\n\n        Parameters:\n            cell_grid_gdf (GeoDataFrame): A GeoDataFrame containing the cell grid data.\n            do_elevation_angle_adjustments (bool): Flag to indicate whether to perform elevation angle adjustments.\n            do_azimuth_angle_adjustments (bool): Flag to indicate whether to perform azimuth angle adjustments.\n\n        Returns:\n            (GeoDataFrame): The input GeoDataFrame with an additional column for signal dominance, after applying the specified adjustments.\n        \"\"\"\n\n        # calculate distance power loss\n        cell_grid_gdf = self.calculate_distance_power_loss(cell_grid_gdf)\n\n        # calculate horizontal angle power adjustment\n        if do_azimuth_angle_adjustments:\n\n            cell_grid_gdf_directional = cell_grid_gdf.filter(F.col(ColNames.directionality) == 1)\n\n            cell_grid_gdf_directional = self.join_sd_mapping(\n                cell_grid_gdf_directional,\n                self.sd_azimuth_mapping_sdf,\n                ColNames.horizontal_beam_width,\n                ColNames.azimuth_signal_strength_back_loss,\n                \"sd_azimuth\",\n            )\n            cell_grid_gdf_directional = self.calculate_horizontal_angle_power_adjustment(cell_grid_gdf_directional)\n\n            cell_grid_gdf = cell_grid_gdf_directional.unionByName(\n                cell_grid_gdf.where(F.col(ColNames.directionality) == 0),\n                allowMissingColumns=True,\n            )\n\n        # calculate vertical angle power adjustment\n        if do_elevation_angle_adjustments:\n\n            cell_grid_gdf_directional = cell_grid_gdf.filter(F.col(ColNames.directionality) == 1)\n\n            cell_grid_gdf_directional = self.join_sd_mapping(\n                cell_grid_gdf_directional,\n                self.sd_elevation_mapping_sdf,\n                ColNames.vertical_beam_width,\n                ColNames.elevation_signal_strength_back_loss,\n                \"sd_elevation\",\n            )\n\n            cell_grid_gdf_directional = self.calculate_vertical_angle_power_adjustment(cell_grid_gdf_directional)\n            cell_grid_gdf = cell_grid_gdf_directional.unionByName(\n                cell_grid_gdf.where(F.col(ColNames.directionality) == 0),\n                allowMissingColumns=True,\n            )\n\n        # calculate signal dominance\n        cell_grid_gdf = self.signal_strength_to_signal_dominance(\n            cell_grid_gdf, self.logistic_function_steepness, self.logistic_function_midpoint\n        )\n\n        return cell_grid_gdf\n\n    @staticmethod\n    def calculate_horizontal_angle_power_adjustment(sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Adjusts the signal strength of each cell in the input DataFrame based on the horizontal angle.\n\n        This function calculates the azimuth angle between each cell and a reference point,\n        projects the data to the elevation plane, and adjusts the signal strength based on the\n        relative azimuth angle and the distance to the cell. The adjustment is calculated using\n        a normal distribution model of signal strength.\n\n        Based on https://github.com/MobilePhoneESSnetBigData/mobloc/blob/master/R/signal_strength.R\n\n        Args:\n            sdf (DataFrame): A Spark DataFrame\n\n        Returns:\n            (DataFrame): The input DataFrame with the 'signal_strength' column adjusted and intermediate columns dropped.\n        \"\"\"\n\n        # TODO: simplify math in this function by using Sedona built in spatial methods\n        sdf = sdf.withColumn(\n            \"theta_azim\",\n            (\n                90\n                - F.degrees(\n                    (\n                        F.atan2(\n                            STF.ST_Y(F.col(ColNames.joined_geometry)) - STF.ST_Y(F.col(ColNames.geometry)),\n                            STF.ST_X(F.col(ColNames.joined_geometry)) - STF.ST_X(F.col(ColNames.geometry)),\n                        )\n                    )\n                )\n            ),\n        )\n        sdf = sdf.withColumn(\n            \"theta_azim\",\n            F.when(F.col(\"theta_azim\") &lt; 0, F.col(\"theta_azim\") + 360).otherwise(F.col(\"theta_azim\")),\n        )\n        sdf = sdf.withColumn(\"azim\", (F.col(\"theta_azim\") - F.col(ColNames.azimuth_angle)) % 360)\n        sdf = sdf.withColumn(\n            \"azim\",\n            F.when(F.col(\"azim\") &gt; 180, F.col(\"azim\") - 360).otherwise(\n                F.when(F.col(\"azim\") &lt; -180, F.col(\"azim\") + 360).otherwise(F.col(\"azim\"))\n            ),\n        )\n        sdf = sdf.withColumn(\"a\", F.sin(F.radians(F.col(\"azim\"))) * F.col(ColNames.distance_to_cell))\n\n        # project to elevation plane\n        sdf = sdf.withColumn(\"b\", F.cos(F.radians(F.col(\"azim\"))) * F.col(ColNames.distance_to_cell))\n        sdf = sdf.withColumn(\"c\", STF.ST_Z(ColNames.geometry) - STF.ST_Z(ColNames.joined_geometry))\n\n        sdf = sdf.withColumn(\"d\", F.sqrt(F.col(\"b\") ** 2 + F.col(\"c\") ** 2))\n        sdf = sdf.withColumn(\"lambda\", F.degrees(F.atan2(F.col(\"c\"), F.abs(F.col(\"b\")))))\n        sdf = sdf.withColumn(\n            \"cases\",\n            F.when(\n                F.col(\"b\") &gt; 0,\n                F.when(F.col(ColNames.elevation_angle) &lt; F.col(\"lambda\"), F.lit(1)).otherwise(F.lit(2)),\n            ).otherwise(F.when(F.col(\"lambda\") + F.col(ColNames.elevation_angle) &lt; 90, F.lit(3)).otherwise(F.lit(4))),\n        )\n\n        sdf = sdf.withColumn(\n            \"e\",\n            F.when(\n                F.col(\"cases\") == 1,\n                F.cos(F.radians(F.col(\"lambda\") - F.col(ColNames.elevation_angle))) * F.col(\"d\"),\n            )\n            .when(\n                F.col(\"cases\") == 2,\n                F.cos(F.radians(F.col(ColNames.elevation_angle) - F.col(\"lambda\"))) * F.col(\"d\"),\n            )\n            .when(\n                F.col(\"cases\") == 3,\n                -F.cos(F.radians(F.col(\"lambda\") + F.col(ColNames.elevation_angle))) * F.col(\"d\"),\n            )\n            .otherwise(F.cos(F.radians(180 - F.col(\"lambda\") - F.col(ColNames.elevation_angle))) * F.col(\"d\")),\n        )\n\n        sdf = sdf.withColumn(\"azim2\", F.degrees(F.atan2(F.col(\"a\"), F.col(\"e\"))))\n\n        # finally get power adjustments\n        sdf = CellFootprintEstimation.norm_dBloss_spark(\n            sdf, \"azim2\", \"sd_azimuth\", ColNames.azimuth_signal_strength_back_loss\n        )\n\n        sdf = sdf.withColumn(\"signal_strength\", F.col(\"signal_strength\") + F.col(\"normalized_dBloss\"))\n\n        # cleanup\n        sdf = sdf.drop(\n            \"theta_azim\",\n            \"azim\",\n            \"a\",\n            \"b\",\n            \"c\",\n            \"d\",\n            \"_lambda\",\n            \"cases\",\n            \"e\",\n            \"azim2\",\n            \"sd_azimuth\",\n            \"normalized_dBloss\",\n        )\n\n        return sdf\n\n    @staticmethod\n    def calculate_vertical_angle_power_adjustment(sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Adjusts the signal strength of each cell in the input DataFrame based on the vertical angle.\n\n        This function calculates the elevation angle between each cell and a reference point,\n        and adjusts the signal strength based on the relative elevation angle and the distance to the cell.\n        The adjustment is calculated using a normal distribution model of signal strength.\n\n        Based on https://github.com/MobilePhoneESSnetBigData/mobloc/blob/master/R/signal_strength.R\n\n        Args:\n            sdf (DataFrame): A Spark DataFrame\n\n        Returns:\n            DataFrame: The input DataFrame with the 'signal_strength' column adjusted and intermediate columns dropped.\n        \"\"\"\n\n        sdf = sdf.withColumn(\n            \"gamma_elev\",\n            F.degrees(\n                F.atan2(\n                    STF.ST_Z(ColNames.geometry) - STF.ST_Z(ColNames.joined_geometry),\n                    F.col(ColNames.distance_to_cell),\n                )\n            ),\n        )\n        sdf = sdf.withColumn(\"elev\", (F.col(\"gamma_elev\") - F.col(ColNames.elevation_angle)) % 360)\n\n        sdf = sdf.withColumn(\n            \"elev\",\n            F.when(F.col(\"elev\") &gt; 180, F.col(\"elev\") - 360).otherwise(\n                F.when(F.col(\"elev\") &lt; -180, F.col(\"elev\") + 360).otherwise(F.col(\"elev\"))\n            ),\n        )\n\n        # finally get power adjustments\n        sdf = CellFootprintEstimation.norm_dBloss_spark(\n            sdf, \"elev\", \"sd_elevation\", ColNames.elevation_signal_strength_back_loss\n        )\n\n        sdf = sdf.withColumn(\n            ColNames.signal_strength,\n            F.col(\n                ColNames.signal_strength,\n            )\n            + F.col(\"normalized_dBloss\"),\n        )\n\n        # cleanup\n        sdf = sdf.drop(\"gamma_elev\", \"elev\", \"sd_elevation\", \"normalized_dBloss\")\n\n        return sdf\n\n    @staticmethod\n    def normal_distribution_col(x_col: Column, mean_col: Column, sd_col: Column) -&gt; Column:\n        \"\"\"\n        Computes the value of the normal distribution for each row in a DataFrame based on\n        the provided columns for the point, mean, and standard deviation.\n\n        This function applies the normal distribution formula to each row of the DataFrame using\n        the specified columns for the point (x), mean, and standard deviation (sd).\n        The normal distribution formula used is:\n\n            f(x) = (1 / (sqrt(2 * pi) * sd)) * exp(-0.5 * ((x - mean) / sd)^2)\n\n        where `x` is the value at which the normal distribution is evaluated,\n        `mean` is the mean of the distribution, and `sd` is the standard deviation.\n\n        Parameters:\n            x_col (Column): A Spark DataFrame column representing the point at which to evaluate the normal distribution.\n            mean_col (Column): A Spark DataFrame column representing the mean of the normal distribution.\n            sd_col (Column): A Spark DataFrame column representing the standard deviation of the normal distribution.\n\n        Returns:\n            Column: A Spark DataFrame column with the computed normal distribution values for each row.\n        \"\"\"\n        return (1.0 / (F.sqrt(2.0 * F.lit(pi)) * sd_col)) * F.exp(-0.5 * ((x_col - mean_col) / sd_col) ** 2)\n\n    @staticmethod\n    def norm_dBloss_spark(sdf: DataFrame, angle_col: str, sd_col: str, db_back_col: str) -&gt; DataFrame:\n        \"\"\"\n        Normalizes the dB loss in a Spark DataFrame based on the angle, standard deviation, and dB back loss.\n\n        This method performs several operations to normalize the dB loss for each row in the Spark DataFrame:\n        1. Normalizes the angle to a range of [-180, 180) degrees.\n        2. Calculates the normal distribution of the normalized angles with a mean of 0 and a standard deviation specified by `sd_col`.\n        3. Calculates the normal distribution for angles 0 and 180 degrees using precomputed values, with a mean of 0 and the same standard deviation.\n        4. Computes an inflation factor based on the dB back loss column and the difference between the normal distributions at 0 and 180 degrees.\n        5. Calculates the normalized dB loss by adjusting the normal distribution of the angle with the inflation factor.\n\n        Parameters:\n            sdf (DataFrame): The input Spark DataFrame containing the data.\n            angle_col (str): The name of the column that contains the angles to be normalized.\n            sd_col (str): The name of the column that contains the standard deviation values for the normal distribution calculation.\n            db_back_col (str): The name of the column in that contains the dB back loss values used to calculate the inflation factor.\n\n        Returns:\n            DataFrame: A Spark DataFrame with the normalized dB loss added and intermediate columns removed.\n        \"\"\"\n        # Normalizing angles\n\n        sdf = sdf.withColumn(\"angle_normalized\", ((F.col(angle_col) + 180) % 360) - 180)\n\n        # Calculate the normal distribution for the normalized angles\n        sdf = sdf.withColumn(\n            \"norm_dist_angle\",\n            CellFootprintEstimation.normal_distribution_col(F.col(\"angle_normalized\"), F.lit(0), F.col(sd_col)),\n        )\n\n        # Calculate the normal distribution for 0 and 180 degrees using precomputed values\n        n_dist_0 = CellFootprintEstimation.normal_distribution_col(F.lit(0), F.lit(0), F.col(sd_col))\n        n_dist_180 = CellFootprintEstimation.normal_distribution_col(F.lit(180), F.lit(0), F.col(sd_col))\n\n        # Calculate the inflated factor\n        sdf = sdf.withColumn(\"inflate\", -F.col(db_back_col) / (n_dist_0 - n_dist_180))\n\n        # Calculate the normalized dB loss\n        sdf = sdf.withColumn(\"normalized_dBloss\", (F.col(\"norm_dist_angle\") - n_dist_0) * F.col(\"inflate\"))\n\n        return sdf.drop(\"angle_normalized\", \"norm_dist_angle\", \"inflate\")\n\n    @staticmethod\n    def signal_strength_to_signal_dominance(\n        sdf: DataFrame,\n        logistic_function_steepness: float,\n        logistic_function_midpoint: float,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Converts signal strength to signal dominance using a logistic function.\n        Methodology from A Bayesian approach to location estimation of mobile devices\n        from mobile network operator data. Tennekes and Gootzen (2022).\n\n        The logistic function is defined as 1 / (1 + exp(-scale)),\n        where scale is (signal_strength - logistic_function_midpoint) * logistic_function_steepness.\n\n        Parameters:\n        sdf (DataFrame): SignalStrenghtDataObject Spark DataFrame.\n        logistic_function_steepness (float): The steepness parameter for the logistic function.\n        logistic_function_midpoint (float): The midpoint parameter for the logistic function.\n\n        Returns:\n        DataFrame: A Spark DataFrame with the signal dominance added as a new column.\n        \"\"\"\n        sdf = sdf.withColumn(\n            \"scale\",\n            (F.col(ColNames.signal_strength) - logistic_function_midpoint) * logistic_function_steepness,\n        )\n        sdf = sdf.withColumn(ColNames.signal_dominance, 1 / (1 + F.exp(-F.col(\"scale\"))))\n        sdf = sdf.drop(\"scale\")\n\n        return sdf\n\n    @staticmethod\n    def prune_small_signal_dominance(sdf: DataFrame, signal_dominance_threshold: float) -&gt; DataFrame:\n        \"\"\"\n        Prunes rows from the DataFrame where the signal dominance is less than or equal to the provided threshold.\n\n        Parameters:\n        sdf (DataFrame): A Spark DataFrame containing the signal dominance data.\n        signal_dominance_threshold (float): The threshold for pruning small signal dominance values.\n\n        Returns:\n        DataFrame: A DataFrame with rows having signal dominance less than or equal to the threshold removed.\n        \"\"\"\n        sdf = sdf.filter(F.col(ColNames.signal_dominance) &gt; signal_dominance_threshold)\n        return sdf\n\n    @staticmethod\n    def prune_max_cells_per_grid_tile(sdf: DataFrame, max_cells_per_grid_tile: int) -&gt; DataFrame:\n        \"\"\"\n        Prunes rows from the DataFrame exceeding the maximum number of cells allowed per grid tile.\n\n        The rows are ordered by signal dominance in descending order,\n        and only the top 'max_cells_per_grid_tile' rows are kept for each grid tile.\n\n        Parameters:\n        sdf (DataFrame): A Spark DataFrame containing the signal dominance data.\n        max_cells_per_grid_tile (int): The maximum number of cells allowed per grid tile.\n\n        Returns:\n        DataFrame: A DataFrame with rows exceeding the maximum number of cells per grid tile removed.\n        \"\"\"\n        window = Window.partitionBy(ColNames.grid_id).orderBy(F.desc(ColNames.signal_dominance))\n\n        sdf = sdf.withColumn(\"row_number\", F.row_number().over(window))\n        sdf = sdf.filter(F.col(\"row_number\") &lt;= max_cells_per_grid_tile)\n        sdf = sdf.drop(\"row_number\")\n\n        return sdf\n\n    @staticmethod\n    def prune_signal_percentage_of_best_sd(sdf: DataFrame, percentage_threshold: float) -&gt; DataFrame:\n        \"\"\"\n        Prunes rows from the DataFrame based on a threshold relative to the best signal dominance in a tile.\n\n        The rows are ordered by signal dominance in descending order, and only the rows where the signal dominance\n        is at least a certain percentage of the highest signal dominance in a tile are kept (for each tile).\n\n        Parameters:\n            sdf (DataFrame): A Spark DataFrame containing the signal dominance data.\n            percentage_threshold (float): The threshold for signal dominance in percentage.\n\n        Returns:\n            DataFrame: A DataFrame with rows pruned based on the signal dominance difference threshold.\n        \"\"\"\n        window = Window.partitionBy(ColNames.grid_id).orderBy(F.desc(ColNames.signal_dominance))\n\n        sdf = sdf.withColumn(\"row_number\", F.row_number().over(window))\n\n        sdf = sdf.withColumn(\"max_signal_dominance\", F.first(ColNames.signal_dominance).over(window))\n        sdf = sdf.withColumn(\n            \"signal_dominance_diff_percentage\",\n            100 * sdf[ColNames.signal_dominance] / sdf[\"max_signal_dominance\"],\n        )\n\n        sdf = sdf.filter(\n            (F.col(\"row_number\") == 1) | (F.col(\"signal_dominance_diff_percentage\") &gt;= percentage_threshold)\n        )\n\n        sdf = sdf.drop(\"row_number\", \"max_signal_dominance\", \"signal_dominance_diff_percentage\")\n\n        return sdf\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.add_z_to_point_geometry","title":"<code>add_z_to_point_geometry(sdf, geometry_col, use_elevation)</code>  <code>staticmethod</code>","text":"<p>Adds z value to the point geometry (grid centroids). If elevation is taken into account, set Z values from z column, otherwise to 0.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>use_elevation</code> <code>bool</code> <p>Whether to use elevation.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with z value added to point geometry.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef add_z_to_point_geometry(sdf: DataFrame, geometry_col: str, use_elevation: bool) -&gt; DataFrame:\n    \"\"\"\n    Adds z value to the point geometry (grid centroids).\n    If elevation is taken into account, set Z values from z column, otherwise to 0.\n\n    Args:\n        sdf (DataFrame): Input DataFrame.\n        use_elevation (bool): Whether to use elevation.\n\n    Returns:\n        DataFrame: DataFrame with z value added to point geometry.\n    \"\"\"\n    if use_elevation:\n        sdf = sdf.withColumn(\n            geometry_col,\n            STC.ST_MakePoint(\n                STF.ST_X(F.col(geometry_col)),\n                STF.ST_Y(F.col(geometry_col)),\n                F.col(ColNames.elevation),\n            ),\n        )\n    else:\n        sdf = sdf.withColumn(\n            geometry_col,\n            STC.ST_MakePoint(\n                STF.ST_X(F.col(geometry_col)),\n                STF.ST_Y(F.col(geometry_col)),\n                F.lit(0.0),\n            ),\n        )\n\n    return sdf.drop(ColNames.elevation)\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.calculate_cartesian_distances","title":"<code>calculate_cartesian_distances(sdf)</code>  <code>staticmethod</code>","text":"<p>Calculates cartesian distances.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with calculated cartesian distances.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef calculate_cartesian_distances(sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Calculates cartesian distances.\n\n    Args:\n        sdf (DataFrame): Input DataFrame.\n\n    Returns:\n        DataFrame: DataFrame with calculated cartesian distances.\n    \"\"\"\n\n    sdf = sdf.withColumns(\n        {\n            ColNames.distance_to_cell_3D: STF.ST_3DDistance(\n                F.col(ColNames.geometry), F.col(ColNames.joined_geometry)\n            ),\n            ColNames.distance_to_cell: STF.ST_Distance(F.col(ColNames.geometry), F.col(ColNames.joined_geometry)),\n        }\n    )\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.calculate_distance_power_loss","title":"<code>calculate_distance_power_loss(sdf)</code>  <code>staticmethod</code>","text":"<p>Calculates distance power loss caluclated as power - path_loss_exponent * 10 * log10(distance_to_cell_3D).</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with calculated distance power loss.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef calculate_distance_power_loss(sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Calculates distance power loss caluclated as\n    power - path_loss_exponent * 10 * log10(distance_to_cell_3D).\n\n    Args:\n        sdf (DataFrame): Input DataFrame.\n\n    Returns:\n        DataFrame: DataFrame with calculated distance power loss.\n    \"\"\"\n    sdf = sdf.withColumn(\n        ColNames.signal_strength,\n        F.col(ColNames.power)\n        - F.col(ColNames.path_loss_exponent) * 10 * F.log10(F.col(ColNames.distance_to_cell_3D)),\n    )\n\n    return sdf.drop(ColNames.power, ColNames.path_loss_exponent)\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.calculate_effective_coverage","title":"<code>calculate_effective_coverage(cells_sdf, grid_sdf)</code>","text":"<p>Calculates effective cell coverage center and range based on desired signal domincance threshold.</p> <p>The function first separates the cells into omnidirectional and directional types, then calculates the signal dominance threshold points for each type. The function then calculates the effective coverage center and range for each cell based on the signal dominance threshold points.</p> <p>Parameters: cells_sdf (pyspark.sql.DataFrame): A Spark DataFrame containing cell information, including directionality. grid_sdf (pyspark.sql.DataFrame): A Spark DataFrame containing grid used for calculating signal dominance.</p> <p>pyspark.sql.DataFrame: A Spark DataFrame with the effective coverage information for each cell, including the                     coverage center and effective range. The DataFrame excludes intermediate columns used                     during the calculation.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>def calculate_effective_coverage(self, cells_sdf: DataFrame, grid_sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Calculates effective cell coverage center and range based on desired signal domincance threshold.\n\n    The function first separates the cells into omnidirectional and directional types,\n    then calculates the signal dominance threshold points for each type.\n    The function then calculates the effective coverage center and range for each cell\n    based on the signal dominance threshold points.\n\n    Parameters:\n    cells_sdf (pyspark.sql.DataFrame): A Spark DataFrame containing cell information, including directionality.\n    grid_sdf (pyspark.sql.DataFrame): A Spark DataFrame containing grid used for calculating signal dominance.\n\n    Returns:\n    pyspark.sql.DataFrame: A Spark DataFrame with the effective coverage information for each cell, including the\n                        coverage center and effective range. The DataFrame excludes intermediate columns used\n                        during the calculation.\n    \"\"\"\n    # omnidirectional cells\n    cells_omni_sdf = cells_sdf.filter(F.col(ColNames.directionality) == 0)\n\n    if cells_omni_sdf.rdd.isEmpty():\n        cells_omni_sdf = self.spark.createDataFrame([], cells_sdf.schema)\n    else:\n        cells_omni_sdf = self.get_signal_dominance_threshold_point(cells_omni_sdf, grid_sdf, \"omni\")\n\n        cells_omni_sdf = cells_omni_sdf.withColumn(\"coverage_center\", F.col(\"geometry\")).withColumn(\n            \"coverage_effective_range\", STF.ST_Distance(F.col(\"coverage_center\"), F.col(\"omni\"))\n        )\n    cells_omni_sdf.cache()\n    cells_omni_sdf.count()\n\n    # directional cells\n    cells_directional_sdf = cells_sdf.filter(F.col(ColNames.directionality) == 1)\n\n    if cells_directional_sdf.rdd.isEmpty():\n        cells_directional_sdf = self.spark.createDataFrame([], cells_sdf.schema)\n    else:\n        cells_directional_sdf = self.get_signal_dominance_threshold_point(\n            cells_directional_sdf, grid_sdf, \"directional_front\"\n        )\n\n        cells_directional_sdf.cache()\n        cells_directional_sdf.count()\n\n        cells_directional_sdf = self.get_signal_dominance_threshold_point(\n            cells_directional_sdf, grid_sdf, \"directional_back\"\n        )\n\n        cells_directional_sdf = cells_directional_sdf.withColumn(\n            \"coverage_center\",\n            STF.ST_LineInterpolatePoint(\n                STF.ST_MakeLine(\n                    cells_directional_sdf[\"directional_front\"], cells_directional_sdf[\"directional_back\"]\n                ),\n                0.5,\n            ),\n        )\n        cells_directional_sdf = cells_directional_sdf.withColumn(\n            \"coverage_effective_range\",\n            STF.ST_Distance(cells_directional_sdf[\"coverage_center\"], cells_directional_sdf[\"directional_front\"]),\n        )\n\n    cells_sdf = cells_omni_sdf.unionByName(cells_directional_sdf, allowMissingColumns=True)\n\n    return cells_sdf.drop(\"omni\", \"directional_front\", \"directional_back\")\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.calculate_horizontal_angle_power_adjustment","title":"<code>calculate_horizontal_angle_power_adjustment(sdf)</code>  <code>staticmethod</code>","text":"<p>Adjusts the signal strength of each cell in the input DataFrame based on the horizontal angle.</p> <p>This function calculates the azimuth angle between each cell and a reference point, projects the data to the elevation plane, and adjusts the signal strength based on the relative azimuth angle and the distance to the cell. The adjustment is calculated using a normal distribution model of signal strength.</p> <p>Based on https://github.com/MobilePhoneESSnetBigData/mobloc/blob/master/R/signal_strength.R</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>A Spark DataFrame</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The input DataFrame with the 'signal_strength' column adjusted and intermediate columns dropped.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef calculate_horizontal_angle_power_adjustment(sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Adjusts the signal strength of each cell in the input DataFrame based on the horizontal angle.\n\n    This function calculates the azimuth angle between each cell and a reference point,\n    projects the data to the elevation plane, and adjusts the signal strength based on the\n    relative azimuth angle and the distance to the cell. The adjustment is calculated using\n    a normal distribution model of signal strength.\n\n    Based on https://github.com/MobilePhoneESSnetBigData/mobloc/blob/master/R/signal_strength.R\n\n    Args:\n        sdf (DataFrame): A Spark DataFrame\n\n    Returns:\n        (DataFrame): The input DataFrame with the 'signal_strength' column adjusted and intermediate columns dropped.\n    \"\"\"\n\n    # TODO: simplify math in this function by using Sedona built in spatial methods\n    sdf = sdf.withColumn(\n        \"theta_azim\",\n        (\n            90\n            - F.degrees(\n                (\n                    F.atan2(\n                        STF.ST_Y(F.col(ColNames.joined_geometry)) - STF.ST_Y(F.col(ColNames.geometry)),\n                        STF.ST_X(F.col(ColNames.joined_geometry)) - STF.ST_X(F.col(ColNames.geometry)),\n                    )\n                )\n            )\n        ),\n    )\n    sdf = sdf.withColumn(\n        \"theta_azim\",\n        F.when(F.col(\"theta_azim\") &lt; 0, F.col(\"theta_azim\") + 360).otherwise(F.col(\"theta_azim\")),\n    )\n    sdf = sdf.withColumn(\"azim\", (F.col(\"theta_azim\") - F.col(ColNames.azimuth_angle)) % 360)\n    sdf = sdf.withColumn(\n        \"azim\",\n        F.when(F.col(\"azim\") &gt; 180, F.col(\"azim\") - 360).otherwise(\n            F.when(F.col(\"azim\") &lt; -180, F.col(\"azim\") + 360).otherwise(F.col(\"azim\"))\n        ),\n    )\n    sdf = sdf.withColumn(\"a\", F.sin(F.radians(F.col(\"azim\"))) * F.col(ColNames.distance_to_cell))\n\n    # project to elevation plane\n    sdf = sdf.withColumn(\"b\", F.cos(F.radians(F.col(\"azim\"))) * F.col(ColNames.distance_to_cell))\n    sdf = sdf.withColumn(\"c\", STF.ST_Z(ColNames.geometry) - STF.ST_Z(ColNames.joined_geometry))\n\n    sdf = sdf.withColumn(\"d\", F.sqrt(F.col(\"b\") ** 2 + F.col(\"c\") ** 2))\n    sdf = sdf.withColumn(\"lambda\", F.degrees(F.atan2(F.col(\"c\"), F.abs(F.col(\"b\")))))\n    sdf = sdf.withColumn(\n        \"cases\",\n        F.when(\n            F.col(\"b\") &gt; 0,\n            F.when(F.col(ColNames.elevation_angle) &lt; F.col(\"lambda\"), F.lit(1)).otherwise(F.lit(2)),\n        ).otherwise(F.when(F.col(\"lambda\") + F.col(ColNames.elevation_angle) &lt; 90, F.lit(3)).otherwise(F.lit(4))),\n    )\n\n    sdf = sdf.withColumn(\n        \"e\",\n        F.when(\n            F.col(\"cases\") == 1,\n            F.cos(F.radians(F.col(\"lambda\") - F.col(ColNames.elevation_angle))) * F.col(\"d\"),\n        )\n        .when(\n            F.col(\"cases\") == 2,\n            F.cos(F.radians(F.col(ColNames.elevation_angle) - F.col(\"lambda\"))) * F.col(\"d\"),\n        )\n        .when(\n            F.col(\"cases\") == 3,\n            -F.cos(F.radians(F.col(\"lambda\") + F.col(ColNames.elevation_angle))) * F.col(\"d\"),\n        )\n        .otherwise(F.cos(F.radians(180 - F.col(\"lambda\") - F.col(ColNames.elevation_angle))) * F.col(\"d\")),\n    )\n\n    sdf = sdf.withColumn(\"azim2\", F.degrees(F.atan2(F.col(\"a\"), F.col(\"e\"))))\n\n    # finally get power adjustments\n    sdf = CellFootprintEstimation.norm_dBloss_spark(\n        sdf, \"azim2\", \"sd_azimuth\", ColNames.azimuth_signal_strength_back_loss\n    )\n\n    sdf = sdf.withColumn(\"signal_strength\", F.col(\"signal_strength\") + F.col(\"normalized_dBloss\"))\n\n    # cleanup\n    sdf = sdf.drop(\n        \"theta_azim\",\n        \"azim\",\n        \"a\",\n        \"b\",\n        \"c\",\n        \"d\",\n        \"_lambda\",\n        \"cases\",\n        \"e\",\n        \"azim2\",\n        \"sd_azimuth\",\n        \"normalized_dBloss\",\n    )\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.calculate_signal_dominance","title":"<code>calculate_signal_dominance(cell_grid_gdf, do_elevation_angle_adjustments, do_azimuth_angle_adjustments)</code>","text":"<p>Calculates the signal dominance for each cell in a grid DataFrame, optionally adjusting for elevation and azimuth angles.</p> <p>This function performs a series of adjustments on the signal strength of each cell in the provided grid DataFrame to calculate the signal dominance. The adjustments include distance power loss, horizontal angle power adjustment (azimuth), and vertical angle power adjustment (elevation), based on the provided flags. Finally, it converts the adjusted signal strength into signal dominance using a logistic function.</p> <p>Parameters:</p> Name Type Description Default <code>cell_grid_gdf</code> <code>GeoDataFrame</code> <p>A GeoDataFrame containing the cell grid data.</p> required <code>do_elevation_angle_adjustments</code> <code>bool</code> <p>Flag to indicate whether to perform elevation angle adjustments.</p> required <code>do_azimuth_angle_adjustments</code> <code>bool</code> <p>Flag to indicate whether to perform azimuth angle adjustments.</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>The input GeoDataFrame with an additional column for signal dominance, after applying the specified adjustments.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>def calculate_signal_dominance(self, cell_grid_gdf, do_elevation_angle_adjustments, do_azimuth_angle_adjustments):\n    \"\"\"\n    Calculates the signal dominance for each cell in a grid DataFrame, optionally adjusting for elevation and azimuth angles.\n\n    This function performs a series of adjustments on the signal strength of each cell in the provided grid DataFrame to calculate the signal dominance. The adjustments include distance power loss, horizontal angle power adjustment (azimuth), and vertical angle power adjustment (elevation), based on the provided flags. Finally, it converts the adjusted signal strength into signal dominance using a logistic function.\n\n    Parameters:\n        cell_grid_gdf (GeoDataFrame): A GeoDataFrame containing the cell grid data.\n        do_elevation_angle_adjustments (bool): Flag to indicate whether to perform elevation angle adjustments.\n        do_azimuth_angle_adjustments (bool): Flag to indicate whether to perform azimuth angle adjustments.\n\n    Returns:\n        (GeoDataFrame): The input GeoDataFrame with an additional column for signal dominance, after applying the specified adjustments.\n    \"\"\"\n\n    # calculate distance power loss\n    cell_grid_gdf = self.calculate_distance_power_loss(cell_grid_gdf)\n\n    # calculate horizontal angle power adjustment\n    if do_azimuth_angle_adjustments:\n\n        cell_grid_gdf_directional = cell_grid_gdf.filter(F.col(ColNames.directionality) == 1)\n\n        cell_grid_gdf_directional = self.join_sd_mapping(\n            cell_grid_gdf_directional,\n            self.sd_azimuth_mapping_sdf,\n            ColNames.horizontal_beam_width,\n            ColNames.azimuth_signal_strength_back_loss,\n            \"sd_azimuth\",\n        )\n        cell_grid_gdf_directional = self.calculate_horizontal_angle_power_adjustment(cell_grid_gdf_directional)\n\n        cell_grid_gdf = cell_grid_gdf_directional.unionByName(\n            cell_grid_gdf.where(F.col(ColNames.directionality) == 0),\n            allowMissingColumns=True,\n        )\n\n    # calculate vertical angle power adjustment\n    if do_elevation_angle_adjustments:\n\n        cell_grid_gdf_directional = cell_grid_gdf.filter(F.col(ColNames.directionality) == 1)\n\n        cell_grid_gdf_directional = self.join_sd_mapping(\n            cell_grid_gdf_directional,\n            self.sd_elevation_mapping_sdf,\n            ColNames.vertical_beam_width,\n            ColNames.elevation_signal_strength_back_loss,\n            \"sd_elevation\",\n        )\n\n        cell_grid_gdf_directional = self.calculate_vertical_angle_power_adjustment(cell_grid_gdf_directional)\n        cell_grid_gdf = cell_grid_gdf_directional.unionByName(\n            cell_grid_gdf.where(F.col(ColNames.directionality) == 0),\n            allowMissingColumns=True,\n        )\n\n    # calculate signal dominance\n    cell_grid_gdf = self.signal_strength_to_signal_dominance(\n        cell_grid_gdf, self.logistic_function_steepness, self.logistic_function_midpoint\n    )\n\n    return cell_grid_gdf\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.calculate_vertical_angle_power_adjustment","title":"<code>calculate_vertical_angle_power_adjustment(sdf)</code>  <code>staticmethod</code>","text":"<p>Adjusts the signal strength of each cell in the input DataFrame based on the vertical angle.</p> <p>This function calculates the elevation angle between each cell and a reference point, and adjusts the signal strength based on the relative elevation angle and the distance to the cell. The adjustment is calculated using a normal distribution model of signal strength.</p> <p>Based on https://github.com/MobilePhoneESSnetBigData/mobloc/blob/master/R/signal_strength.R</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>A Spark DataFrame</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The input DataFrame with the 'signal_strength' column adjusted and intermediate columns dropped.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef calculate_vertical_angle_power_adjustment(sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Adjusts the signal strength of each cell in the input DataFrame based on the vertical angle.\n\n    This function calculates the elevation angle between each cell and a reference point,\n    and adjusts the signal strength based on the relative elevation angle and the distance to the cell.\n    The adjustment is calculated using a normal distribution model of signal strength.\n\n    Based on https://github.com/MobilePhoneESSnetBigData/mobloc/blob/master/R/signal_strength.R\n\n    Args:\n        sdf (DataFrame): A Spark DataFrame\n\n    Returns:\n        DataFrame: The input DataFrame with the 'signal_strength' column adjusted and intermediate columns dropped.\n    \"\"\"\n\n    sdf = sdf.withColumn(\n        \"gamma_elev\",\n        F.degrees(\n            F.atan2(\n                STF.ST_Z(ColNames.geometry) - STF.ST_Z(ColNames.joined_geometry),\n                F.col(ColNames.distance_to_cell),\n            )\n        ),\n    )\n    sdf = sdf.withColumn(\"elev\", (F.col(\"gamma_elev\") - F.col(ColNames.elevation_angle)) % 360)\n\n    sdf = sdf.withColumn(\n        \"elev\",\n        F.when(F.col(\"elev\") &gt; 180, F.col(\"elev\") - 360).otherwise(\n            F.when(F.col(\"elev\") &lt; -180, F.col(\"elev\") + 360).otherwise(F.col(\"elev\"))\n        ),\n    )\n\n    # finally get power adjustments\n    sdf = CellFootprintEstimation.norm_dBloss_spark(\n        sdf, \"elev\", \"sd_elevation\", ColNames.elevation_signal_strength_back_loss\n    )\n\n    sdf = sdf.withColumn(\n        ColNames.signal_strength,\n        F.col(\n            ColNames.signal_strength,\n        )\n        + F.col(\"normalized_dBloss\"),\n    )\n\n    # cleanup\n    sdf = sdf.drop(\"gamma_elev\", \"elev\", \"sd_elevation\", \"normalized_dBloss\")\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.create_cell_point_geometry","title":"<code>create_cell_point_geometry(sdf, use_elevation)</code>  <code>staticmethod</code>","text":"<p>Creates cell point geometry. If elevation is taken into account, set Z values from z column, otherwise to 0.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>use_elevation</code> <code>bool</code> <p>Whether to use elevation.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with cell point geometry.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef create_cell_point_geometry(sdf: DataFrame, use_elevation: bool) -&gt; DataFrame:\n    \"\"\"\n    Creates cell point geometry.\n    If elevation is taken into account, set Z values from z column, otherwise to 0.\n\n    Args:\n        sdf (DataFrame): Input DataFrame.\n        use_elevation (bool): Whether to use elevation.\n\n    Returns:\n        DataFrame: DataFrame with cell point geometry.\n    \"\"\"\n    if use_elevation:\n        sdf = sdf.withColumn(\n            ColNames.geometry,\n            STC.ST_MakePoint(\n                F.col(ColNames.longitude),\n                F.col(ColNames.latitude),\n                F.col(ColNames.altitude) + F.col(ColNames.antenna_height),\n            ),\n        )\n    else:\n        sdf = sdf.withColumn(\n            ColNames.geometry,\n            STC.ST_MakePoint(\n                F.col(ColNames.longitude),\n                F.col(ColNames.latitude),\n                F.col(ColNames.antenna_height),\n            ),\n        )\n    # assign crs\n    sdf = sdf.withColumn(ColNames.geometry, STF.ST_SetSRID(F.col(ColNames.geometry), F.lit(4326)))\n\n    return sdf.drop(ColNames.latitude, ColNames.longitude, ColNames.altitude, ColNames.antenna_height)\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.create_default_properties_df","title":"<code>create_default_properties_df()</code>","text":"<p>Creates a DataFrame with default cell properties from config dict.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame with default cell properties.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>def create_default_properties_df(self) -&gt; DataFrame:\n    \"\"\"\n    Creates a DataFrame with default cell properties from config dict.\n\n    Returns:\n        DataFrame: A DataFrame with default cell properties.\n    \"\"\"\n\n    rows = [Row(cell_type=k, **v) for k, v in self.default_cell_properties.items()]\n    return self.spark.createDataFrame(rows).withColumnRenamed(\"cell_type\", ColNames.cell_type)\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.create_mapping","title":"<code>create_mapping(db_back, signal_front_back_difference_col)</code>  <code>staticmethod</code>","text":"<p>Creates a mapping between standard deviation and the angle at which the signal strength falls to 3 dB below its maximum value.</p> <p>Parameters:</p> Name Type Description Default <code>db_back</code> <code>float</code> <p>The difference in signal strength in dB between the front and back of the signal.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame where each row corresponds to a</p> <code>DataFrame</code> <p>standard deviation and contains the corresponding angle.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef create_mapping(db_back: float, signal_front_back_difference_col) -&gt; DataFrame:\n    \"\"\"\n    Creates a mapping between standard deviation and the angle\n    at which the signal strength falls to 3 dB below its maximum value.\n\n    Args:\n        db_back (float): The difference in signal strength in dB between the front and back of the signal.\n\n    Returns:\n        DataFrame: A DataFrame where each row corresponds to a\n        standard deviation and contains the corresponding angle.\n    \"\"\"\n    idf = pd.DataFrame({\"sd\": np.arange(180 / 1000, 180, 180 / 1000)})\n    idf[\"deg\"] = idf[\"sd\"].apply(CellFootprintEstimation.get_min3db, db_back=db_back)\n    df = pd.DataFrame({\"deg\": np.arange(1, 181)})\n    df[\"sd\"] = df[\"deg\"].apply(lambda dg: idf.loc[np.abs(idf[\"deg\"] - dg).idxmin(), \"sd\"])\n    df[signal_front_back_difference_col] = db_back\n    return df\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.find_sd","title":"<code>find_sd(beam_width, mapping)</code>  <code>staticmethod</code>","text":"<p>Finds the standard deviation corresponding to the given beam width using the provided mapping.</p> <p>Parameters:</p> Name Type Description Default <code>beam_width</code> <code>float</code> <p>The width of the beam in degrees.</p> required <code>mapping</code> <code>DataFrame</code> <p>A DataFrame where each row corresponds to a standard deviation and contains the corresponding angle.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The standard deviation corresponding to the given beam width.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef find_sd(beam_width: float, mapping: DataFrame) -&gt; float:\n    \"\"\"\n    Finds the standard deviation corresponding to the given beam width using the provided mapping.\n\n    Args:\n        beam_width (float): The width of the beam in degrees.\n        mapping (DataFrame): A DataFrame where each row corresponds to a standard deviation\n            and contains the corresponding angle.\n\n    Returns:\n        float: The standard deviation corresponding to the given beam width.\n    \"\"\"\n    min_diff_index = (abs(mapping[\"deg\"] - beam_width / 2)).idxmin()\n    return float(mapping.loc[min_diff_index, \"sd\"])\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.get_angular_adjustments_sd_mapping","title":"<code>get_angular_adjustments_sd_mapping(cells_sdf, beam_width_col, signal_front_back_difference_col, angular_adjustment_type)</code>","text":"<p>Calculates standard deviations in signal strength based on beam width and front-back cell signal difference.</p> <p>Parameters:</p> Name Type Description Default <code>cells_sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>beam_width_col</code> <code>str</code> <p>Column name for the beam width.</p> required <code>signal_front_back_difference_col</code> <code>str</code> <p>Column name for the signal front-back difference.</p> required <code>angular_adjustment_type</code> <code>str</code> <p>Type of angular adjustment.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with angular adjustments standard deviation mapping.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>def get_angular_adjustments_sd_mapping(\n    self,\n    cells_sdf: DataFrame,\n    beam_width_col: str,\n    signal_front_back_difference_col: str,\n    angular_adjustment_type: str,\n) -&gt; DataFrame:\n    \"\"\"\n    Calculates standard deviations in signal strength based on beam width and front-back cell signal difference.\n\n    Args:\n        cells_sdf (DataFrame): Input DataFrame.\n        beam_width_col (str): Column name for the beam width.\n        signal_front_back_difference_col (str): Column name for the signal front-back difference.\n        angular_adjustment_type (str): Type of angular adjustment.\n\n    Returns:\n        DataFrame: DataFrame with angular adjustments standard deviation mapping.\n    \"\"\"\n\n    sd_mappings = CellFootprintEstimation.get_sd_to_signal_back_loss_mappings(\n        cells_sdf, signal_front_back_difference_col\n    )\n    beam_widths_diff = (\n        cells_sdf.select(F.col(beam_width_col), F.col(signal_front_back_difference_col))\n        .where(F.col(ColNames.directionality) == 1)\n        .distinct()\n    )\n    beam_widths_diff = [row.asDict() for row in beam_widths_diff.collect()]\n\n    beam_sds = []\n    for item in beam_widths_diff:\n        item[f\"sd_{angular_adjustment_type}\"] = self.find_sd(\n            item[beam_width_col],\n            sd_mappings[sd_mappings[signal_front_back_difference_col] == item[signal_front_back_difference_col]],\n        )\n        beam_sds.append(item)\n\n    beam_sd_sdf = self.spark.createDataFrame(beam_sds)\n\n    return beam_sd_sdf\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.get_min3db","title":"<code>get_min3db(sd, db_back)</code>  <code>staticmethod</code>","text":"<p>Finds the angle at which the signal strength falls to 3 dB below its maximum value.</p> <p>Parameters:</p> Name Type Description Default <code>sd</code> <code>float</code> <p>The standard deviation of the normal distribution modeling the signal strength.</p> required <code>db_back</code> <code>float</code> <p>The difference in signal strength in dB between the front and back of the signal.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The angle at which the signal strength falls to 3 dB below its maximum value.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef get_min3db(sd: float, db_back: float) -&gt; float:\n    \"\"\"\n    Finds the angle at which the signal strength falls to 3 dB below its maximum value.\n\n    Args:\n        sd (float): The standard deviation of the normal distribution modeling the signal strength.\n        db_back (float): The difference in signal strength in dB between the front and back of the signal.\n\n    Returns:\n        float: The angle at which the signal strength falls to 3 dB below its maximum value.\n    \"\"\"\n    df = pd.DataFrame({\"a\": np.linspace(0, 180, 720)})\n    df[\"dbLoss\"] = CellFootprintEstimation.norm_dBloss(df[\"a\"], db_back=db_back, sd=sd)\n    return df.loc[np.abs(-3 - df[\"dbLoss\"]).idxmin(), \"a\"]\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.get_sd_to_signal_back_loss_mappings","title":"<code>get_sd_to_signal_back_loss_mappings(cells_sdf, signal_front_back_difference_col)</code>  <code>staticmethod</code>","text":"<p>Generates a DataFrame with mapping of signal strength standard deviation for each     elevation/azimuth angle degree.</p> <p>Parameters: cells_sdf (DataFrame): A Spark DataFrame containing information about the cells. signal_front_back_difference_col (str): The name of the column that contains the difference     in signal strength between the front and back of the cell.</p> <p>Returns: DataFrame: A pandas DataFrame with standard deviation mappings.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef get_sd_to_signal_back_loss_mappings(cells_sdf: DataFrame, signal_front_back_difference_col: str) -&gt; DataFrame:\n    \"\"\"\n    Generates a DataFrame with mapping of signal strength standard deviation for each\n        elevation/azimuth angle degree.\n\n    Parameters:\n    cells_sdf (DataFrame): A Spark DataFrame containing information about the cells.\n    signal_front_back_difference_col (str): The name of the column that contains the difference\n        in signal strength between\n    the front and back of the cell.\n\n    Returns:\n    DataFrame: A pandas DataFrame with standard deviation mappings.\n\n    \"\"\"\n    db_back_diffs = (\n        cells_sdf.select(F.col(signal_front_back_difference_col))\n        .where(F.col(ColNames.directionality) == 1)\n        .distinct()\n    )\n    db_back_diffs = [row.asDict()[signal_front_back_difference_col] for row in db_back_diffs.collect()]\n    mappings = [\n        CellFootprintEstimation.create_mapping(item, signal_front_back_difference_col) for item in db_back_diffs\n    ]\n\n    return pd.concat(mappings)\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.get_signal_dominance_threshold_point","title":"<code>get_signal_dominance_threshold_point(cells_sdf, grid_sdf, point_type)</code>","text":"<p>Calculates the signal dominance threshold points in cell maximum range.</p> <p>For omnidirectional cell types, the signal dominance threshold point is calculated as the furthest point along 90 degrees azimuth direction where signal dominance is less than the threshold. For directional cells types, two signal dominance threshold points are calculated:     1. the furthest point along the directionality angle direction where signal dominance is less than the threshold     2. the furthest point opposite to the directionality angle direction where signal dominance is less than the threshold</p> <p>Parameters:</p> Name Type Description Default <code>cells_sdf</code> <code>DataFrame</code> <p>The DataFrame containing cell information.</p> required <code>grid_sdf</code> <code>DataFrame</code> <p>The DataFrame containing grid information.</p> required <code>point_type</code> <code>str</code> <p>The type of point to calculate the signal dominance threshold for.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The updated cells DataFrame with the signal dominance threshold point added.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>def get_signal_dominance_threshold_point(self, cells_sdf, grid_sdf, point_type):\n    \"\"\"\n    Calculates the signal dominance threshold points in cell maximum range.\n\n    For omnidirectional cell types, the signal dominance threshold point is calculated as\n    the furthest point along 90 degrees azimuth direction where signal dominance is less than the threshold.\n    For directional cells types, two signal dominance threshold points are calculated:\n        1. the furthest point along the directionality angle direction where signal dominance is less than the threshold\n        2. the furthest point opposite to the directionality angle direction where signal dominance is less than the threshold\n\n    Args:\n        cells_sdf (DataFrame): The DataFrame containing cell information.\n        grid_sdf (DataFrame): The DataFrame containing grid information.\n        point_type (str): The type of point to calculate the signal dominance threshold for.\n\n    Returns:\n        (DataFrame): The updated cells DataFrame with the signal dominance threshold point added.\n    \"\"\"\n\n    do_azimuth_angle_adjustments = True\n    do_elevation_angle_adjustments = True\n\n    if point_type == \"directional_front\":\n        # Calculate range line for directional front point type\n        cells_sdf = cells_sdf.withColumn(\n            \"range_line\",\n            STF.ST_MakeLine(\n                \"geometry\",\n                STC.ST_Point(\n                    STF.ST_X(\"geometry\") + cells_sdf[\"range\"] * F.sin(F.radians(cells_sdf[\"azimuth_angle\"])),\n                    STF.ST_Y(\"geometry\") + cells_sdf[\"range\"] * F.cos(F.radians(cells_sdf[\"azimuth_angle\"])),\n                ),\n            ),\n        )\n\n    elif point_type == \"directional_back\":\n        # Calculate range line for directional back point type\n        cells_sdf = cells_sdf.withColumn(\n            \"range_line\",\n            STF.ST_MakeLine(\n                \"geometry\",\n                STC.ST_Point(\n                    STF.ST_X(\"geometry\") + cells_sdf[\"range\"] * F.sin(F.radians(cells_sdf[\"azimuth_angle\"] + 180)),\n                    STF.ST_Y(\"geometry\") + cells_sdf[\"range\"] * F.cos(F.radians(cells_sdf[\"azimuth_angle\"] + 180)),\n                ),\n            ),\n        )\n\n    elif point_type == \"omni\":\n        # Calculate range line for omni point type\n        cells_sdf = cells_sdf.withColumn(\n            \"range_line\",\n            STF.ST_MakeLine(\n                \"geometry\",\n                STC.ST_Point(\n                    STF.ST_X(\"geometry\") + cells_sdf[\"range\"] * F.sin(F.radians(F.lit(90))),\n                    STF.ST_Y(\"geometry\") + cells_sdf[\"range\"] * F.cos(F.radians(F.lit(90))),\n                ),\n            ),\n        )\n\n        do_azimuth_angle_adjustments = False\n        do_elevation_angle_adjustments = False\n\n    cells_sdf = cells_sdf.withColumn(\"coverage_range_line_buffer\", F.lit(self.coverage_range_line_buffer))\n\n    cell_grid = self.spatial_join_within_distance(cells_sdf, grid_sdf, \"range_line\", \"coverage_range_line_buffer\")\n\n    # calculate 3d distance and planar distance from cell id to grid ids\n    cell_grid = self.calculate_cartesian_distances(cell_grid)\n\n    cell_grid = self.calculate_signal_dominance(\n        cell_grid, do_elevation_angle_adjustments, do_azimuth_angle_adjustments\n    )\n\n    cell_grid_filtered = cell_grid.filter(F.col(ColNames.signal_dominance) &gt; self.signal_dominance_threshold)\n    cell_counts = cell_grid_filtered.groupBy(\"cell_id\").count().withColumnRenamed(\"count\", \"filtered_count\")\n    cell_grid_joined = cell_grid.join(cell_counts, on=\"cell_id\", how=\"left\").fillna(0, subset=[\"filtered_count\"])\n\n    # In case if desired threshold filter removes all grid tiles in given direction, keep closest grid tile\n    # Otherwise keep furthest point where threshold condition meet\n    window_min = Window.partitionBy(\"cell_id\").orderBy(F.col(\"distance_to_cell\"))\n    window_max = Window.partitionBy(\"cell_id\").orderBy(F.desc(F.col(\"distance_to_cell\")))\n\n    cell_grid_max = (\n        cell_grid_joined.withColumn(\n            \"rank\",\n            F.when(F.col(\"filtered_count\") == 0, F.row_number().over(window_min)).otherwise(\n                F.row_number().over(window_max)\n            ),\n        )\n        .filter(F.col(\"rank\") == 1)\n        .drop(\"rank\", \"filtered_count\")\n    )\n\n    cells_sdf = cells_sdf.join(\n        cell_grid_max.select(\"cell_id\", \"joined_geometry\").withColumnRenamed(\"joined_geometry\", point_type),\n        \"cell_id\",\n        \"left\",\n    )\n\n    return cells_sdf.drop(\n        \"range_line\", \"distance_to_cell_3d\", \"distance_to_cell\", \"signal_strength\", \"signal_dominance\", \"grid_id\"\n    )\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.impute_default_cell_properties","title":"<code>impute_default_cell_properties(sdf)</code>","text":"<p>Imputes default cell properties for null values in the input DataFrame using default properties for cell types from config.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with imputed default cell properties.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>def impute_default_cell_properties(self, sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Imputes default cell properties for null values in the input DataFrame using\n    default properties for cell types from config.\n\n    Args:\n        sdf (DataFrame): Input DataFrame.\n\n    Returns:\n        DataFrame: DataFrame with imputed default cell properties.\n    \"\"\"\n    default_properties_df = self.create_default_properties_df()\n\n    # add default prefix to the columns of default_properties_df\n    default_properties_df = default_properties_df.select(\n        [F.col(col).alias(f\"default_{col}\") for col in default_properties_df.columns]\n    )\n\n    # assign default cell type to cell types not present in config\n    sdf = sdf.withColumn(\n        ColNames.cell_type,\n        F.when(\n            F.col(ColNames.cell_type).isin(list(self.default_cell_properties.keys())),\n            F.col(ColNames.cell_type),\n        ).otherwise(\"default\"),\n    )\n\n    # all cell types which are absent from the default_properties_df will be assigned default values\n    sdf = sdf.join(\n        default_properties_df,\n        sdf[ColNames.cell_type] == default_properties_df[f\"default_{ColNames.cell_type}\"],\n        how=\"inner\",\n    )\n    # if orignal column is null, assign the default value\n    for col in default_properties_df.columns:\n        col = col.replace(\"default_\", \"\")\n        if col not in sdf.columns:\n            sdf = sdf.withColumn(col, F.lit(None))\n        sdf = sdf.withColumn(col, F.coalesce(F.col(col), F.col(f\"default_{col}\")))\n\n    return sdf.drop(*default_properties_df.columns)\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.join_sd_mapping","title":"<code>join_sd_mapping(sdf, sd_mapping_sdf, beam_width_col, signal_front_back_difference_col, sd_col)</code>  <code>staticmethod</code>","text":"<p>Joins DataFrame with standard deviation mapping.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>sd_mapping_sdf</code> <code>DataFrame</code> <p>DataFrame with standard deviation mapping.</p> required <code>beam_width_col</code> <code>str</code> <p>Column name for the beam width.</p> required <code>signal_front_back_difference_col</code> <code>str</code> <p>Column name for the signal front-back difference.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame after joining with standard deviation mapping.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef join_sd_mapping(\n    sdf: DataFrame,\n    sd_mapping_sdf: DataFrame,\n    beam_width_col: str,\n    signal_front_back_difference_col: str,\n    sd_col: str,\n) -&gt; DataFrame:\n    \"\"\"\n    Joins DataFrame with standard deviation mapping.\n\n    Args:\n        sdf (DataFrame): Input DataFrame.\n        sd_mapping_sdf (DataFrame): DataFrame with standard deviation mapping.\n        beam_width_col (str): Column name for the beam width.\n        signal_front_back_difference_col (str): Column name for the signal front-back difference.\n\n    Returns:\n        DataFrame: DataFrame after joining with standard deviation mapping.\n    \"\"\"\n\n    join_condition = (F.col(f\"a.{beam_width_col}\") == F.col(f\"b.{beam_width_col}\")) &amp; (\n        F.col(f\"a.{signal_front_back_difference_col}\") == F.col(f\"b.{signal_front_back_difference_col}\")\n    )\n\n    sdf = sdf.alias(\"a\").join(sd_mapping_sdf.alias(\"b\"), join_condition).select(f\"a.*\", f\"b.{sd_col}\")\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.norm_dBloss","title":"<code>norm_dBloss(a, sd, db_back)</code>  <code>staticmethod</code>","text":"<p>Computes the loss in signal strength in dB as a function of angle from the direction of maximum signal strength.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>The angle from the direction of maximum signal strength.</p> required <code>sd</code> <code>float</code> <p>The standard deviation of the normal distribution modeling the signal strength.</p> required <code>db_back</code> <code>float</code> <p>The difference in signal strength in dB between the front and back of the signal.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The loss in signal strength in dB at the given angle.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef norm_dBloss(a: float, sd: float, db_back: float) -&gt; float:\n    \"\"\"\n    Computes the loss in signal strength in dB as a function of\n    angle from the direction of maximum signal strength.\n\n    Args:\n        a (float): The angle from the direction of maximum signal strength.\n        sd (float): The standard deviation of the normal distribution modeling the signal strength.\n        db_back (float): The difference in signal strength in dB between the front and back of the signal.\n\n    Returns:\n        float: The loss in signal strength in dB at the given angle.\n    \"\"\"\n    a = ((a + 180) % 360) - 180\n    inflate = -db_back / (\n        CellFootprintEstimation.normal_distribution(0, 0, sd)\n        - CellFootprintEstimation.normal_distribution(180, 0, sd)\n    )\n    return (\n        CellFootprintEstimation.normal_distribution(a, 0, sd)\n        - CellFootprintEstimation.normal_distribution(0, 0, sd)\n    ) * inflate\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.norm_dBloss_spark","title":"<code>norm_dBloss_spark(sdf, angle_col, sd_col, db_back_col)</code>  <code>staticmethod</code>","text":"<p>Normalizes the dB loss in a Spark DataFrame based on the angle, standard deviation, and dB back loss.</p> <p>This method performs several operations to normalize the dB loss for each row in the Spark DataFrame: 1. Normalizes the angle to a range of [-180, 180) degrees. 2. Calculates the normal distribution of the normalized angles with a mean of 0 and a standard deviation specified by <code>sd_col</code>. 3. Calculates the normal distribution for angles 0 and 180 degrees using precomputed values, with a mean of 0 and the same standard deviation. 4. Computes an inflation factor based on the dB back loss column and the difference between the normal distributions at 0 and 180 degrees. 5. Calculates the normalized dB loss by adjusting the normal distribution of the angle with the inflation factor.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The input Spark DataFrame containing the data.</p> required <code>angle_col</code> <code>str</code> <p>The name of the column that contains the angles to be normalized.</p> required <code>sd_col</code> <code>str</code> <p>The name of the column that contains the standard deviation values for the normal distribution calculation.</p> required <code>db_back_col</code> <code>str</code> <p>The name of the column in that contains the dB back loss values used to calculate the inflation factor.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A Spark DataFrame with the normalized dB loss added and intermediate columns removed.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef norm_dBloss_spark(sdf: DataFrame, angle_col: str, sd_col: str, db_back_col: str) -&gt; DataFrame:\n    \"\"\"\n    Normalizes the dB loss in a Spark DataFrame based on the angle, standard deviation, and dB back loss.\n\n    This method performs several operations to normalize the dB loss for each row in the Spark DataFrame:\n    1. Normalizes the angle to a range of [-180, 180) degrees.\n    2. Calculates the normal distribution of the normalized angles with a mean of 0 and a standard deviation specified by `sd_col`.\n    3. Calculates the normal distribution for angles 0 and 180 degrees using precomputed values, with a mean of 0 and the same standard deviation.\n    4. Computes an inflation factor based on the dB back loss column and the difference between the normal distributions at 0 and 180 degrees.\n    5. Calculates the normalized dB loss by adjusting the normal distribution of the angle with the inflation factor.\n\n    Parameters:\n        sdf (DataFrame): The input Spark DataFrame containing the data.\n        angle_col (str): The name of the column that contains the angles to be normalized.\n        sd_col (str): The name of the column that contains the standard deviation values for the normal distribution calculation.\n        db_back_col (str): The name of the column in that contains the dB back loss values used to calculate the inflation factor.\n\n    Returns:\n        DataFrame: A Spark DataFrame with the normalized dB loss added and intermediate columns removed.\n    \"\"\"\n    # Normalizing angles\n\n    sdf = sdf.withColumn(\"angle_normalized\", ((F.col(angle_col) + 180) % 360) - 180)\n\n    # Calculate the normal distribution for the normalized angles\n    sdf = sdf.withColumn(\n        \"norm_dist_angle\",\n        CellFootprintEstimation.normal_distribution_col(F.col(\"angle_normalized\"), F.lit(0), F.col(sd_col)),\n    )\n\n    # Calculate the normal distribution for 0 and 180 degrees using precomputed values\n    n_dist_0 = CellFootprintEstimation.normal_distribution_col(F.lit(0), F.lit(0), F.col(sd_col))\n    n_dist_180 = CellFootprintEstimation.normal_distribution_col(F.lit(180), F.lit(0), F.col(sd_col))\n\n    # Calculate the inflated factor\n    sdf = sdf.withColumn(\"inflate\", -F.col(db_back_col) / (n_dist_0 - n_dist_180))\n\n    # Calculate the normalized dB loss\n    sdf = sdf.withColumn(\"normalized_dBloss\", (F.col(\"norm_dist_angle\") - n_dist_0) * F.col(\"inflate\"))\n\n    return sdf.drop(\"angle_normalized\", \"norm_dist_angle\", \"inflate\")\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.normal_distribution","title":"<code>normal_distribution(x, mean, sd)</code>  <code>staticmethod</code>","text":"<p>Computes the value of the normal distribution with the given mean and standard deviation at the given point.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float</code> <p>The point at which to evaluate the normal distribution.</p> required <code>mean</code> <code>float</code> <p>The mean of the normal distribution.</p> required <code>sd</code> <code>float</code> <p>The standard deviation of the normal distribution.</p> required <p>Returns:</p> Type Description <code>array | list</code> <p>The value of the normal distribution at the given point,</p> <code>Union[array, list]</code> <p>returned as either a numpy array or a list.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef normal_distribution(x: float, mean: float, sd: float) -&gt; Union[np.array, list]:\n    \"\"\"\n    Computes the value of the normal distribution with the given mean\n    and standard deviation at the given point.\n\n    Args:\n        x (float): The point at which to evaluate the normal distribution.\n        mean (float): The mean of the normal distribution.\n        sd (float): The standard deviation of the normal distribution.\n\n    Returns:\n        (np.array | list): The value of the normal distribution at the given point,\n        returned as either a numpy array or a list.\n    \"\"\"\n    n_dist = (1.0 / (sqrt(2.0 * pi) * sd)) * np.exp(-0.5 * ((x - mean) / sd) ** 2)\n\n    return n_dist\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.normal_distribution_col","title":"<code>normal_distribution_col(x_col, mean_col, sd_col)</code>  <code>staticmethod</code>","text":"<p>Computes the value of the normal distribution for each row in a DataFrame based on the provided columns for the point, mean, and standard deviation.</p> <p>This function applies the normal distribution formula to each row of the DataFrame using the specified columns for the point (x), mean, and standard deviation (sd). The normal distribution formula used is:</p> <pre><code>f(x) = (1 / (sqrt(2 * pi) * sd)) * exp(-0.5 * ((x - mean) / sd)^2)\n</code></pre> <p>where <code>x</code> is the value at which the normal distribution is evaluated, <code>mean</code> is the mean of the distribution, and <code>sd</code> is the standard deviation.</p> <p>Parameters:</p> Name Type Description Default <code>x_col</code> <code>Column</code> <p>A Spark DataFrame column representing the point at which to evaluate the normal distribution.</p> required <code>mean_col</code> <code>Column</code> <p>A Spark DataFrame column representing the mean of the normal distribution.</p> required <code>sd_col</code> <code>Column</code> <p>A Spark DataFrame column representing the standard deviation of the normal distribution.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A Spark DataFrame column with the computed normal distribution values for each row.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef normal_distribution_col(x_col: Column, mean_col: Column, sd_col: Column) -&gt; Column:\n    \"\"\"\n    Computes the value of the normal distribution for each row in a DataFrame based on\n    the provided columns for the point, mean, and standard deviation.\n\n    This function applies the normal distribution formula to each row of the DataFrame using\n    the specified columns for the point (x), mean, and standard deviation (sd).\n    The normal distribution formula used is:\n\n        f(x) = (1 / (sqrt(2 * pi) * sd)) * exp(-0.5 * ((x - mean) / sd)^2)\n\n    where `x` is the value at which the normal distribution is evaluated,\n    `mean` is the mean of the distribution, and `sd` is the standard deviation.\n\n    Parameters:\n        x_col (Column): A Spark DataFrame column representing the point at which to evaluate the normal distribution.\n        mean_col (Column): A Spark DataFrame column representing the mean of the normal distribution.\n        sd_col (Column): A Spark DataFrame column representing the standard deviation of the normal distribution.\n\n    Returns:\n        Column: A Spark DataFrame column with the computed normal distribution values for each row.\n    \"\"\"\n    return (1.0 / (F.sqrt(2.0 * F.lit(pi)) * sd_col)) * F.exp(-0.5 * ((x_col - mean_col) / sd_col) ** 2)\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.prune_max_cells_per_grid_tile","title":"<code>prune_max_cells_per_grid_tile(sdf, max_cells_per_grid_tile)</code>  <code>staticmethod</code>","text":"<p>Prunes rows from the DataFrame exceeding the maximum number of cells allowed per grid tile.</p> <p>The rows are ordered by signal dominance in descending order, and only the top 'max_cells_per_grid_tile' rows are kept for each grid tile.</p> <p>Parameters: sdf (DataFrame): A Spark DataFrame containing the signal dominance data. max_cells_per_grid_tile (int): The maximum number of cells allowed per grid tile.</p> <p>Returns: DataFrame: A DataFrame with rows exceeding the maximum number of cells per grid tile removed.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef prune_max_cells_per_grid_tile(sdf: DataFrame, max_cells_per_grid_tile: int) -&gt; DataFrame:\n    \"\"\"\n    Prunes rows from the DataFrame exceeding the maximum number of cells allowed per grid tile.\n\n    The rows are ordered by signal dominance in descending order,\n    and only the top 'max_cells_per_grid_tile' rows are kept for each grid tile.\n\n    Parameters:\n    sdf (DataFrame): A Spark DataFrame containing the signal dominance data.\n    max_cells_per_grid_tile (int): The maximum number of cells allowed per grid tile.\n\n    Returns:\n    DataFrame: A DataFrame with rows exceeding the maximum number of cells per grid tile removed.\n    \"\"\"\n    window = Window.partitionBy(ColNames.grid_id).orderBy(F.desc(ColNames.signal_dominance))\n\n    sdf = sdf.withColumn(\"row_number\", F.row_number().over(window))\n    sdf = sdf.filter(F.col(\"row_number\") &lt;= max_cells_per_grid_tile)\n    sdf = sdf.drop(\"row_number\")\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.prune_signal_percentage_of_best_sd","title":"<code>prune_signal_percentage_of_best_sd(sdf, percentage_threshold)</code>  <code>staticmethod</code>","text":"<p>Prunes rows from the DataFrame based on a threshold relative to the best signal dominance in a tile.</p> <p>The rows are ordered by signal dominance in descending order, and only the rows where the signal dominance is at least a certain percentage of the highest signal dominance in a tile are kept (for each tile).</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>A Spark DataFrame containing the signal dominance data.</p> required <code>percentage_threshold</code> <code>float</code> <p>The threshold for signal dominance in percentage.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame with rows pruned based on the signal dominance difference threshold.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef prune_signal_percentage_of_best_sd(sdf: DataFrame, percentage_threshold: float) -&gt; DataFrame:\n    \"\"\"\n    Prunes rows from the DataFrame based on a threshold relative to the best signal dominance in a tile.\n\n    The rows are ordered by signal dominance in descending order, and only the rows where the signal dominance\n    is at least a certain percentage of the highest signal dominance in a tile are kept (for each tile).\n\n    Parameters:\n        sdf (DataFrame): A Spark DataFrame containing the signal dominance data.\n        percentage_threshold (float): The threshold for signal dominance in percentage.\n\n    Returns:\n        DataFrame: A DataFrame with rows pruned based on the signal dominance difference threshold.\n    \"\"\"\n    window = Window.partitionBy(ColNames.grid_id).orderBy(F.desc(ColNames.signal_dominance))\n\n    sdf = sdf.withColumn(\"row_number\", F.row_number().over(window))\n\n    sdf = sdf.withColumn(\"max_signal_dominance\", F.first(ColNames.signal_dominance).over(window))\n    sdf = sdf.withColumn(\n        \"signal_dominance_diff_percentage\",\n        100 * sdf[ColNames.signal_dominance] / sdf[\"max_signal_dominance\"],\n    )\n\n    sdf = sdf.filter(\n        (F.col(\"row_number\") == 1) | (F.col(\"signal_dominance_diff_percentage\") &gt;= percentage_threshold)\n    )\n\n    sdf = sdf.drop(\"row_number\", \"max_signal_dominance\", \"signal_dominance_diff_percentage\")\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.prune_small_signal_dominance","title":"<code>prune_small_signal_dominance(sdf, signal_dominance_threshold)</code>  <code>staticmethod</code>","text":"<p>Prunes rows from the DataFrame where the signal dominance is less than or equal to the provided threshold.</p> <p>Parameters: sdf (DataFrame): A Spark DataFrame containing the signal dominance data. signal_dominance_threshold (float): The threshold for pruning small signal dominance values.</p> <p>Returns: DataFrame: A DataFrame with rows having signal dominance less than or equal to the threshold removed.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef prune_small_signal_dominance(sdf: DataFrame, signal_dominance_threshold: float) -&gt; DataFrame:\n    \"\"\"\n    Prunes rows from the DataFrame where the signal dominance is less than or equal to the provided threshold.\n\n    Parameters:\n    sdf (DataFrame): A Spark DataFrame containing the signal dominance data.\n    signal_dominance_threshold (float): The threshold for pruning small signal dominance values.\n\n    Returns:\n    DataFrame: A DataFrame with rows having signal dominance less than or equal to the threshold removed.\n    \"\"\"\n    sdf = sdf.filter(F.col(ColNames.signal_dominance) &gt; signal_dominance_threshold)\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.signal_strength_to_signal_dominance","title":"<code>signal_strength_to_signal_dominance(sdf, logistic_function_steepness, logistic_function_midpoint)</code>  <code>staticmethod</code>","text":"<p>Converts signal strength to signal dominance using a logistic function. Methodology from A Bayesian approach to location estimation of mobile devices from mobile network operator data. Tennekes and Gootzen (2022).</p> <p>The logistic function is defined as 1 / (1 + exp(-scale)), where scale is (signal_strength - logistic_function_midpoint) * logistic_function_steepness.</p> <p>Parameters: sdf (DataFrame): SignalStrenghtDataObject Spark DataFrame. logistic_function_steepness (float): The steepness parameter for the logistic function. logistic_function_midpoint (float): The midpoint parameter for the logistic function.</p> <p>Returns: DataFrame: A Spark DataFrame with the signal dominance added as a new column.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef signal_strength_to_signal_dominance(\n    sdf: DataFrame,\n    logistic_function_steepness: float,\n    logistic_function_midpoint: float,\n) -&gt; DataFrame:\n    \"\"\"\n    Converts signal strength to signal dominance using a logistic function.\n    Methodology from A Bayesian approach to location estimation of mobile devices\n    from mobile network operator data. Tennekes and Gootzen (2022).\n\n    The logistic function is defined as 1 / (1 + exp(-scale)),\n    where scale is (signal_strength - logistic_function_midpoint) * logistic_function_steepness.\n\n    Parameters:\n    sdf (DataFrame): SignalStrenghtDataObject Spark DataFrame.\n    logistic_function_steepness (float): The steepness parameter for the logistic function.\n    logistic_function_midpoint (float): The midpoint parameter for the logistic function.\n\n    Returns:\n    DataFrame: A Spark DataFrame with the signal dominance added as a new column.\n    \"\"\"\n    sdf = sdf.withColumn(\n        \"scale\",\n        (F.col(ColNames.signal_strength) - logistic_function_midpoint) * logistic_function_steepness,\n    )\n    sdf = sdf.withColumn(ColNames.signal_dominance, 1 / (1 + F.exp(-F.col(\"scale\"))))\n    sdf = sdf.drop(\"scale\")\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.spatial_join_within_distance","title":"<code>spatial_join_within_distance(sdf_from, sdf_to, geometry_col, within_distance_col)</code>  <code>staticmethod</code>","text":"<p>Performs a spatial join within a specified distance.</p> <p>Parameters:</p> Name Type Description Default <code>sdf_from</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>sdf_to</code> <code>DataFrame</code> <p>DataFrame to join with.</p> required <code>within_distance_col</code> <code>str</code> <p>Column name for the within distance.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame after performing the spatial join.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef spatial_join_within_distance(\n    sdf_from: DataFrame, sdf_to: DataFrame, geometry_col: str, within_distance_col: str\n) -&gt; DataFrame:\n    \"\"\"\n    Performs a spatial join within a specified distance.\n\n    Args:\n        sdf_from (DataFrame): Input DataFrame.\n        sdf_to (DataFrame): DataFrame to join with.\n        within_distance_col (str): Column name for the within distance.\n\n    Returns:\n        DataFrame: DataFrame after performing the spatial join.\n    \"\"\"\n\n    sdf_merged = (\n        sdf_from.alias(\"a\")\n        .join(\n            sdf_to.alias(\"b\"),\n            STP.ST_Intersects(\n                STF.ST_Buffer(f\"a.{geometry_col}\", f\"a.{within_distance_col}\"),\n                f\"b.{ColNames.joined_geometry}\",\n            ),\n        )\n        .drop(f\"a.{within_distance_col}\")\n    )\n\n    return sdf_merged\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.watt_to_dbm","title":"<code>watt_to_dbm(sdf)</code>  <code>staticmethod</code>","text":"<p>Converts power from watt to dBm.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with power converted to dBm.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef watt_to_dbm(sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Converts power from watt to dBm.\n\n    Args:\n        sdf (DataFrame): Input DataFrame.\n\n    Returns:\n        DataFrame: DataFrame with power converted to dBm.\n    \"\"\"\n    return sdf.withColumn(ColNames.power, 10.0 * F.log10(F.col(ColNames.power)) + 30.0)\n</code></pre>"},{"location":"reference/components/execution/cell_footprint_intersections/","title":"cell_footprint_intersections","text":""},{"location":"reference/components/execution/cell_footprint_intersections/cell_footprint_intersections/","title":"cell_footprint_intersections","text":"<p>Module that implements the Daily Permanence Score functionality</p>"},{"location":"reference/components/execution/cell_footprint_intersections/cell_footprint_intersections/#components.execution.cell_footprint_intersections.cell_footprint_intersections.CellFootprintIntersections","title":"<code>CellFootprintIntersections</code>","text":"<p>               Bases: <code>Component</code></p> <p>A class to calculate the daily permanence score of each user per interval and grid tile.</p> Source code in <code>multimno/components/execution/cell_footprint_intersections/cell_footprint_intersections.py</code> <pre><code>class CellFootprintIntersections(Component):\n    \"\"\"\n    A class to calculate the daily permanence score of each user per interval and grid tile.\n    \"\"\"\n\n    COMPONENT_ID = \"CellFootprintIntersections\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        data_period_start = self.config.get(self.COMPONENT_ID, \"data_period_start\")\n        try:\n            self.data_period_start = datetime.strptime(data_period_start, \"%Y-%m-%d\").date()\n        except ValueError as e:\n            self.logger.error(f\"Could not parse data_period_start = `{data_period_start}`. Expected format: YYYY-MM\")\n            raise e\n\n        data_period_end = self.config.get(self.COMPONENT_ID, \"data_period_end\")\n        try:\n            self.data_period_end = datetime.strptime(data_period_end, \"%Y-%m-%d\").date()\n        except ValueError as e:\n            self.logger.error(f\"Could not parse data_period_end = `{data_period_end}`. Expected format: YYYY-MM\")\n            raise e\n\n        self.data_period_dates = [\n            self.data_period_start + timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n        self.current_date: datetime.date = None\n\n    def initalize_data_objects(self):\n        # Get paths\n        input_cell_footprint_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"cell_footprint_data_silver\")\n\n        output_cell_to_group_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"cell_to_group_data_silver\")\n        output_group_to_tile_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"group_to_tile_data_silver\")\n\n        # Clear destination directory if needed\n        clear_destination_directory = self.config.getboolean(\n            self.COMPONENT_ID, \"clear_destination_directory\", fallback=False\n        )\n        if clear_destination_directory:\n            self.logger.warning(f\"Deleting: {output_cell_to_group_path}\")\n            delete_file_or_folder(self.spark, output_cell_to_group_path)\n\n            self.logger.warning(f\"Deleting: {output_group_to_tile_path}\")\n            delete_file_or_folder(self.spark, output_group_to_tile_path)\n\n        cell_footprint = SilverCellFootprintDataObject(self.spark, input_cell_footprint_path)\n        cell_to_group = SilverCellToGroupDataObject(self.spark, output_cell_to_group_path)\n        group_to_tile = SilverGroupToTileDataObject(self.spark, output_group_to_tile_path)\n\n        self.input_data_objects = {\n            cell_footprint.ID: cell_footprint,\n        }\n\n        self.output_data_objects = {\n            cell_to_group.ID: cell_to_group,\n            group_to_tile.ID: group_to_tile,\n        }\n\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n\n        for current_date in self.data_period_dates:\n            self.current_date = current_date\n            self.logger.info(f\"Processing cell footprint for {current_date.strftime('%Y-%m-%d')}\")\n            self.transform()\n            self.write()\n            self.logger.info(f\"... finished with {current_date.strftime('%Y-%m-%d')}\")\n\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        cell_footprint = (\n            self.input_data_objects[SilverCellFootprintDataObject.ID]\n            .df.filter(\n                (F.col(ColNames.year) == F.lit(self.current_date.year))\n                &amp; (F.col(ColNames.month) == F.lit(self.current_date.month))\n                &amp; (F.col(ColNames.day) == F.lit(self.current_date.day))\n            )\n            .select(ColNames.grid_id, ColNames.cell_id)\n        )\n\n        # First, we get the list of cells that provide coverage to a particular grid tile\n        # The ordered list of cells, turned into a string equal to their IDs separated by a comma,\n        # will represent the group IDs\n        # We use F.collect_list and not F.collect_set as we assume that the cell footprint does not have any repeated\n        # rows\n\n        cells_of_tile = (\n            cell_footprint.groupBy(ColNames.grid_id)\n            .agg(F.array_sort(F.collect_list(ColNames.cell_id)).alias(ColNames.cell_id))\n            .withColumn(ColNames.group_id, F.array_join(F.col(ColNames.cell_id), \",\"))\n        )\n\n        cells_of_tile.persist()\n\n        group_to_tile = cells_of_tile.select(ColNames.group_id, ColNames.grid_id).withColumns(\n            {\n                ColNames.year: F.lit(self.current_date.year),\n                ColNames.month: F.lit(self.current_date.month),\n                ColNames.day: F.lit(self.current_date.day),\n            }\n        )\n\n        cell_to_group = (\n            cells_of_tile.select(ColNames.cell_id, ColNames.group_id)\n            # we need distinct rows\n            .distinct()\n            .withColumn(ColNames.cell_id, F.explode(ColNames.cell_id))\n            .withColumns(\n                {\n                    ColNames.year: F.lit(self.current_date.year),\n                    ColNames.month: F.lit(self.current_date.month),\n                    ColNames.day: F.lit(self.current_date.day),\n                }\n            )\n        )\n\n        self.output_data_objects[SilverGroupToTileDataObject.ID].df = apply_schema_casting(\n            group_to_tile, SilverGroupToTileDataObject.SCHEMA\n        )\n        self.output_data_objects[SilverCellToGroupDataObject.ID].df = apply_schema_casting(\n            cell_to_group, SilverCellToGroupDataObject.SCHEMA\n        )\n</code></pre>"},{"location":"reference/components/execution/cell_proximity_estimation/","title":"cell_proximity_estimation","text":""},{"location":"reference/components/execution/cell_proximity_estimation/cell_proximity_estimation/","title":"cell_proximity_estimation","text":"<p>Module for calculating which cells have overlapping coverage areas, as well calculating distances between coverage areas.</p>"},{"location":"reference/components/execution/cell_proximity_estimation/cell_proximity_estimation/#components.execution.cell_proximity_estimation.cell_proximity_estimation.CellProximityEstimation","title":"<code>CellProximityEstimation</code>","text":"<p>               Bases: <code>Component</code></p> Source code in <code>multimno/components/execution/cell_proximity_estimation/cell_proximity_estimation.py</code> <pre><code>class CellProximityEstimation(Component):\n    COMPONENT_ID = \"CellProximityEstimation\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.max_nearby_cell_distance_m = self.config.getfloat(self.COMPONENT_ID, \"max_nearby_cell_distance_m\")\n        self.footprint_buffer_distance_m = self.config.getfloat(self.COMPONENT_ID, \"footprint_buffer_distance_m\")\n        self.n_output_partitions = self.config.getint(self.COMPONENT_ID, \"n_output_partitions\")\n        self.data_period_start = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n        self.data_period_end = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n\n        self.data_period_dates = [\n            self.data_period_start + timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n        self.grid_gen = InspireGridGenerator(self.spark)\n\n        self.current_date = None\n        self.prev_date = None\n\n    def initalize_data_objects(self):\n        # Input paths\n        input_cell_footprint_silver_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"cell_footprint_data_silver\")\n        # Output paths\n        output_cell_intersection_groups_silver_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"cell_intersection_groups_data_silver\"\n        )\n        output_cell_distance_silver_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"cell_distance_data_silver\")\n        clear_destination_directory = self.config.getboolean(self.COMPONENT_ID, \"clear_destination_directory\")\n\n        # Input data objects\n        silver_cell_footprint = SilverCellFootprintDataObject(self.spark, input_cell_footprint_silver_path)\n        self.input_data_objects = {SilverCellFootprintDataObject.ID: silver_cell_footprint}\n        # Output data objects\n        silver_cell_intersection_groups = SilverCellIntersectionGroupsDataObject(\n            self.spark, output_cell_intersection_groups_silver_path\n        )\n        silver_cell_distance = SilverCellDistanceDataObject(self.spark, output_cell_distance_silver_path)\n        self.output_data_objects = {\n            SilverCellIntersectionGroupsDataObject.ID: silver_cell_intersection_groups,\n            SilverCellDistanceDataObject.ID: silver_cell_distance,\n        }\n\n        if clear_destination_directory:\n            delete_file_or_folder(self.spark, output_cell_intersection_groups_silver_path)\n            delete_file_or_folder(self.spark, output_cell_distance_silver_path)\n\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n        for current_date in self.data_period_dates:\n            self.current_date = current_date\n            self.transform()\n            self.write()\n            self.prev_date = current_date\n            self.spark.catalog.clearCache()\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        current_date = self.current_date\n        self.logger.info(f\"Starting Transform method of {self.COMPONENT_ID} for date {current_date} ...\")\n        # Get cell grid_ids for current date.\n        cell_grid_ids_df = (\n            self.input_data_objects[SilverCellFootprintDataObject.ID]\n            .df.filter(\n                (F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(current_date))\n            )\n            .select(ColNames.cell_id, ColNames.grid_id)\n        )\n        # Shortcut to next date if no data is present.\n\n        if cell_grid_ids_df.isEmpty():\n            self.logger.info(f\"No input data for date {current_date}.\")\n            self.output_data_objects[SilverCellIntersectionGroupsDataObject.ID].df = self.spark.createDataFrame(\n                [], schema=SilverCellIntersectionGroupsDataObject.SCHEMA\n            )\n            self.output_data_objects[SilverCellDistanceDataObject.ID].df = self.spark.createDataFrame(\n                [], schema=SilverCellDistanceDataObject.SCHEMA\n            )\n            return\n\n        # For each cell, calculate the geometry from grid (concave hull of grid centroids, with buffer zone).\n        df_with_geom = self.calculate_cell_geometry_with_buffer(cell_grid_ids_df)\n        # Repartition to avoid exploding partition count on following self join.\n        # Note: using a partition count other than the output partition count may be desirable.\n        df_with_geom = df_with_geom.repartition(self.n_output_partitions, ColNames.cell_id)\n\n        # TODO: consider caching the dataframe at this point\n\n        # Perform dataframe self join to get cell pairs where distance is below max distance threshold.\n        df = self.calculate_nearby_cell_pairs(df_with_geom)\n        # Zero distance between two geoms indicates an overlap.\n        # We currently do not account for the amount of overlap, simply its existence.\n        df = df.withColumn(\"is_overlap\", F.col(ColNames.distance) &lt;= 0.0)\n        df = df.cache()\n\n        # Collect overlapping cell ids for output\n        cell_intersection_groups_df = self.create_cell_intersection_groups_dataframe(df, current_date=current_date)\n        cell_intersection_groups_df = cell_intersection_groups_df.repartition(\n            self.n_output_partitions, ColNames.cell_id\n        )\n        self.output_data_objects[SilverCellIntersectionGroupsDataObject.ID].df = cell_intersection_groups_df\n\n        # Collect cell distances for output\n        cell_distance_df = self.create_cell_distance_dataframe(df, current_date=current_date)\n        cell_distance_df = cell_distance_df.repartition(self.n_output_partitions, ColNames.cell_id_a)\n        self.output_data_objects[SilverCellDistanceDataObject.ID].df = cell_distance_df\n        return\n\n    def create_cell_distance_dataframe(self, df: DataFrame, current_date) -&gt; DataFrame:\n        \"\"\"Creates dataframe matching the cell distance data object structure.\n\n        Args:\n            df (DataFrame): (cell_id_a, cell_id_b, distance)\n            current_date: date of data\n\n        Returns:\n            DataFrame: (cell_id_a ,cell_id_b, distance, year, month, day)\n        \"\"\"\n        cell_distance_df = df.select(\n            ColNames.cell_id_a,\n            ColNames.cell_id_b,\n            F.col(ColNames.distance).cast(FloatType()),\n            F.lit(current_date.year).cast(ShortType()).alias(ColNames.year),\n            F.lit(current_date.month).cast(ByteType()).alias(ColNames.month),\n            F.lit(current_date.day).cast(ByteType()).alias(ColNames.day),\n        ).where(F.col(ColNames.cell_id_a) != F.col(ColNames.cell_id_b))\n\n        return cell_distance_df\n\n    def create_cell_intersection_groups_dataframe(self, df: DataFrame, current_date) -&gt; DataFrame:\n        \"\"\"Creates dataframe matching the cell intersection groups data object structure.\n\n        Args:\n            df (DataFrame): (cell_id_a, cell_id_b, is_overlap)\n            current_date: date of data\n\n        Returns:\n            DataFrame: (cell_id_a, overlapping_cell_ids, year, month, day)\n        \"\"\"\n        cell_intersection_groups_df = (\n            df.where(\"is_overlap\")\n            .groupBy(ColNames.cell_id_a)\n            .agg(F.array_sort(F.collect_list(ColNames.cell_id_b)).alias(ColNames.overlapping_cell_ids))\n        )\n        cell_intersection_groups_df = cell_intersection_groups_df.withColumnRenamed(\n            ColNames.cell_id_a, ColNames.cell_id\n        )\n        cell_intersection_groups_df = cell_intersection_groups_df.select(\n            ColNames.cell_id,\n            ColNames.overlapping_cell_ids,\n            F.lit(current_date.year).cast(ShortType()).alias(ColNames.year),\n            F.lit(current_date.month).cast(ByteType()).alias(ColNames.month),\n            F.lit(current_date.day).cast(ByteType()).alias(ColNames.day),\n        )\n        return cell_intersection_groups_df\n\n    def calculate_nearby_cell_pairs(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Calculates pairs of cells in the dataframe which are within parameter-defined distance of each other.\n        Pairs between the same cell_id have the cell_id_b set to None.\n        Args:\n            df (DataFrame): (cell_id, geometry)\n\n        Returns:\n            DataFrame: (cell_id_a, cell_id_b, distance)\n        \"\"\"\n        df = (\n            df.alias(\"df1\")\n            .join(\n                df.alias(\"df2\"),\n                on=STP.ST_DWithin(\n                    F.col(f\"df1.{ColNames.geometry}\"),\n                    F.col(f\"df2.{ColNames.geometry}\"),\n                    distance=self.max_nearby_cell_distance_m,\n                ),\n            )\n            .select(\n                F.col(f\"df1.{ColNames.cell_id}\").alias(ColNames.cell_id_a),\n                F.when((F.col(f\"df1.{ColNames.cell_id}\") == F.col(f\"df2.{ColNames.cell_id}\")), F.lit(None))\n                .otherwise(F.col(f\"df2.{ColNames.cell_id}\"))\n                .alias(ColNames.cell_id_b),\n                STF.ST_Distance(F.col(f\"df1.{ColNames.geometry}\"), F.col(f\"df2.{ColNames.geometry}\")).alias(\n                    ColNames.distance\n                ),\n            )\n        )\n        return df\n\n    def calculate_cell_geometry_with_buffer(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Calculates cell coverage geometries for cell rows, including added buffer zone.\n        Returns (cell_id, geometry) for each row.\n\n        Args:\n            df (DataFrame): (cell_id, is_new_geometry, grid_id_list)\n\n        Returns:\n            DataFrame: (cell_id, geometry)\n        \"\"\"\n        # In order to compute the centroids we need an origin for the 4-byte grid IDs. However, since we are only\n        # working with distances in this component, we can use any origin, so there is no need to read any\n        # grid data object to retrieve the origin\n        df_with_geom = self.grid_gen.grid_ids_to_grid_centroids(\n            sdf=df.withColumn(ColNames.origin, F.lit(0).cast(\"long\")),\n            grid_resolution=100,\n        ).drop(ColNames.origin)\n        df_with_geom = df_with_geom.groupBy([ColNames.cell_id]).agg(\n            STF.ST_ConcaveHull(STF.ST_Collect(F.collect_list(ColNames.geometry)), 0.5).alias(ColNames.geometry)\n        )\n        # Add buffer\n        df_with_geom = df_with_geom.withColumn(\n            ColNames.geometry, STF.ST_Buffer(ColNames.geometry, self.footprint_buffer_distance_m)\n        )\n\n        return df_with_geom\n</code></pre>"},{"location":"reference/components/execution/cell_proximity_estimation/cell_proximity_estimation/#components.execution.cell_proximity_estimation.cell_proximity_estimation.CellProximityEstimation.calculate_cell_geometry_with_buffer","title":"<code>calculate_cell_geometry_with_buffer(df)</code>","text":"<p>Calculates cell coverage geometries for cell rows, including added buffer zone. Returns (cell_id, geometry) for each row.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>(cell_id, is_new_geometry, grid_id_list)</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>(cell_id, geometry)</p> Source code in <code>multimno/components/execution/cell_proximity_estimation/cell_proximity_estimation.py</code> <pre><code>def calculate_cell_geometry_with_buffer(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Calculates cell coverage geometries for cell rows, including added buffer zone.\n    Returns (cell_id, geometry) for each row.\n\n    Args:\n        df (DataFrame): (cell_id, is_new_geometry, grid_id_list)\n\n    Returns:\n        DataFrame: (cell_id, geometry)\n    \"\"\"\n    # In order to compute the centroids we need an origin for the 4-byte grid IDs. However, since we are only\n    # working with distances in this component, we can use any origin, so there is no need to read any\n    # grid data object to retrieve the origin\n    df_with_geom = self.grid_gen.grid_ids_to_grid_centroids(\n        sdf=df.withColumn(ColNames.origin, F.lit(0).cast(\"long\")),\n        grid_resolution=100,\n    ).drop(ColNames.origin)\n    df_with_geom = df_with_geom.groupBy([ColNames.cell_id]).agg(\n        STF.ST_ConcaveHull(STF.ST_Collect(F.collect_list(ColNames.geometry)), 0.5).alias(ColNames.geometry)\n    )\n    # Add buffer\n    df_with_geom = df_with_geom.withColumn(\n        ColNames.geometry, STF.ST_Buffer(ColNames.geometry, self.footprint_buffer_distance_m)\n    )\n\n    return df_with_geom\n</code></pre>"},{"location":"reference/components/execution/cell_proximity_estimation/cell_proximity_estimation/#components.execution.cell_proximity_estimation.cell_proximity_estimation.CellProximityEstimation.calculate_nearby_cell_pairs","title":"<code>calculate_nearby_cell_pairs(df)</code>","text":"<p>Calculates pairs of cells in the dataframe which are within parameter-defined distance of each other. Pairs between the same cell_id have the cell_id_b set to None. Args:     df (DataFrame): (cell_id, geometry)</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>(cell_id_a, cell_id_b, distance)</p> Source code in <code>multimno/components/execution/cell_proximity_estimation/cell_proximity_estimation.py</code> <pre><code>def calculate_nearby_cell_pairs(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Calculates pairs of cells in the dataframe which are within parameter-defined distance of each other.\n    Pairs between the same cell_id have the cell_id_b set to None.\n    Args:\n        df (DataFrame): (cell_id, geometry)\n\n    Returns:\n        DataFrame: (cell_id_a, cell_id_b, distance)\n    \"\"\"\n    df = (\n        df.alias(\"df1\")\n        .join(\n            df.alias(\"df2\"),\n            on=STP.ST_DWithin(\n                F.col(f\"df1.{ColNames.geometry}\"),\n                F.col(f\"df2.{ColNames.geometry}\"),\n                distance=self.max_nearby_cell_distance_m,\n            ),\n        )\n        .select(\n            F.col(f\"df1.{ColNames.cell_id}\").alias(ColNames.cell_id_a),\n            F.when((F.col(f\"df1.{ColNames.cell_id}\") == F.col(f\"df2.{ColNames.cell_id}\")), F.lit(None))\n            .otherwise(F.col(f\"df2.{ColNames.cell_id}\"))\n            .alias(ColNames.cell_id_b),\n            STF.ST_Distance(F.col(f\"df1.{ColNames.geometry}\"), F.col(f\"df2.{ColNames.geometry}\")).alias(\n                ColNames.distance\n            ),\n        )\n    )\n    return df\n</code></pre>"},{"location":"reference/components/execution/cell_proximity_estimation/cell_proximity_estimation/#components.execution.cell_proximity_estimation.cell_proximity_estimation.CellProximityEstimation.create_cell_distance_dataframe","title":"<code>create_cell_distance_dataframe(df, current_date)</code>","text":"<p>Creates dataframe matching the cell distance data object structure.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>(cell_id_a, cell_id_b, distance)</p> required <code>current_date</code> <p>date of data</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>(cell_id_a ,cell_id_b, distance, year, month, day)</p> Source code in <code>multimno/components/execution/cell_proximity_estimation/cell_proximity_estimation.py</code> <pre><code>def create_cell_distance_dataframe(self, df: DataFrame, current_date) -&gt; DataFrame:\n    \"\"\"Creates dataframe matching the cell distance data object structure.\n\n    Args:\n        df (DataFrame): (cell_id_a, cell_id_b, distance)\n        current_date: date of data\n\n    Returns:\n        DataFrame: (cell_id_a ,cell_id_b, distance, year, month, day)\n    \"\"\"\n    cell_distance_df = df.select(\n        ColNames.cell_id_a,\n        ColNames.cell_id_b,\n        F.col(ColNames.distance).cast(FloatType()),\n        F.lit(current_date.year).cast(ShortType()).alias(ColNames.year),\n        F.lit(current_date.month).cast(ByteType()).alias(ColNames.month),\n        F.lit(current_date.day).cast(ByteType()).alias(ColNames.day),\n    ).where(F.col(ColNames.cell_id_a) != F.col(ColNames.cell_id_b))\n\n    return cell_distance_df\n</code></pre>"},{"location":"reference/components/execution/cell_proximity_estimation/cell_proximity_estimation/#components.execution.cell_proximity_estimation.cell_proximity_estimation.CellProximityEstimation.create_cell_intersection_groups_dataframe","title":"<code>create_cell_intersection_groups_dataframe(df, current_date)</code>","text":"<p>Creates dataframe matching the cell intersection groups data object structure.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>(cell_id_a, cell_id_b, is_overlap)</p> required <code>current_date</code> <p>date of data</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>(cell_id_a, overlapping_cell_ids, year, month, day)</p> Source code in <code>multimno/components/execution/cell_proximity_estimation/cell_proximity_estimation.py</code> <pre><code>def create_cell_intersection_groups_dataframe(self, df: DataFrame, current_date) -&gt; DataFrame:\n    \"\"\"Creates dataframe matching the cell intersection groups data object structure.\n\n    Args:\n        df (DataFrame): (cell_id_a, cell_id_b, is_overlap)\n        current_date: date of data\n\n    Returns:\n        DataFrame: (cell_id_a, overlapping_cell_ids, year, month, day)\n    \"\"\"\n    cell_intersection_groups_df = (\n        df.where(\"is_overlap\")\n        .groupBy(ColNames.cell_id_a)\n        .agg(F.array_sort(F.collect_list(ColNames.cell_id_b)).alias(ColNames.overlapping_cell_ids))\n    )\n    cell_intersection_groups_df = cell_intersection_groups_df.withColumnRenamed(\n        ColNames.cell_id_a, ColNames.cell_id\n    )\n    cell_intersection_groups_df = cell_intersection_groups_df.select(\n        ColNames.cell_id,\n        ColNames.overlapping_cell_ids,\n        F.lit(current_date.year).cast(ShortType()).alias(ColNames.year),\n        F.lit(current_date.month).cast(ByteType()).alias(ColNames.month),\n        F.lit(current_date.day).cast(ByteType()).alias(ColNames.day),\n    )\n    return cell_intersection_groups_df\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/","title":"daily_permanence_score","text":""},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/","title":"daily_permanence_score","text":"<p>Module that implements the Daily Permanence Score functionality</p>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore","title":"<code>DailyPermanenceScore</code>","text":"<p>               Bases: <code>Component</code></p> <p>A class to calculate the daily permanence score of each user per interval and grid tile.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>class DailyPermanenceScore(Component):\n    \"\"\"\n    A class to calculate the daily permanence score of each user per interval and grid tile.\n    \"\"\"\n\n    COMPONENT_ID = \"DailyPermanenceScore\"\n    # Only 24, 48, and 96 are allowed as values for the number of time slots. These values correspond to intervals\n    # of length 60, 30, and 15 minutes respectively.\n    ALLOWED_NUMBER_OF_TIME_SLOTS = [24, 48, 96]\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.data_period_start = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n        self.data_period_end = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n\n        self.time_slot_number = self.config.getint(self.COMPONENT_ID, \"time_slot_number\")\n        if self.time_slot_number not in self.ALLOWED_NUMBER_OF_TIME_SLOTS:\n            raise ValueError(\n                f\"Accepted values for `time_slot_number` are {str(self.ALLOWED_NUMBER_OF_TIME_SLOTS)} -- found {self.time_slot_number}\"\n            )\n\n        self.max_time_thresh = timedelta(seconds=self.config.getint(self.COMPONENT_ID, \"max_time_thresh\"))\n        self.max_time_thresh_day = timedelta(seconds=self.config.getint(self.COMPONENT_ID, \"max_time_thresh_day\"))\n        self.max_time_thresh_night = timedelta(seconds=self.config.getint(self.COMPONENT_ID, \"max_time_thresh_night\"))\n        self.max_time_thresh_abroad = timedelta(seconds=self.config.getint(self.COMPONENT_ID, \"max_time_thresh_abroad\"))\n        self.max_speed_thresh = self.config.getfloat(self.COMPONENT_ID, \"max_speed_thresh\")\n        self.broadcast_footprints = self.config.getboolean(self.COMPONENT_ID, \"broadcast_footprints\", fallback=False)\n        self.use_200m_grid = self.config.getboolean(self.COMPONENT_ID, \"use_200m_grid\", fallback=False)\n        self.local_mcc = self.config.getint(GENERAL_CONFIG_KEY, \"local_mcc\")\n        self.event_error_flags_to_include = self.config.geteval(self.COMPONENT_ID, \"event_error_flags_to_include\")\n\n        self.data_period_dates = [\n            self.data_period_start + timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n        self.grid_gen = InspireGridGenerator(self.spark)\n        self.events = None\n        self.cell_footprint = None\n        self.cell_to_group: DataFrame = None\n        self.group_to_tile: DataFrame = None\n        self.time_slots = None\n        self.current_date = None\n\n    def initalize_data_objects(self):\n        # Get paths\n        input_events_silver_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"event_data_silver_flagged\")\n        input_events_cache_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"event_cache\")\n        input_cell_footprint_silver_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"cell_footprint_data_silver\")\n        input_cell_to_group = self.config.get(CONFIG_SILVER_PATHS_KEY, \"cell_to_group_data_silver\")\n        input_group_to_tile = self.config.get(CONFIG_SILVER_PATHS_KEY, \"group_to_tile_data_silver\")\n        output_dps_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"daily_permanence_score_data_silver\")\n\n        # Clear destination directory if needed\n        clear_destination_directory = self.config.getboolean(\n            self.COMPONENT_ID, \"clear_destination_directory\", fallback=False\n        )\n        if clear_destination_directory:\n            self.logger.warning(f\"Deleting: {output_dps_path}\")\n            delete_file_or_folder(self.spark, output_dps_path)\n\n        # ------------------ Data objects ------------------\n        silver_events = SilverEventFlaggedDataObject(self.spark, input_events_silver_path)\n        silver_cell_footprint = SilverCellFootprintDataObject(self.spark, input_cell_footprint_silver_path)\n        events_cache = EventCacheDataObject(self.spark, input_events_cache_path)\n        cell_to_group = SilverCellToGroupDataObject(self.spark, input_cell_to_group)\n        group_to_tile = SilverGroupToTileDataObject(self.spark, input_group_to_tile)\n\n        silver_dps = SilverDailyPermanenceScoreDataObject(self.spark, output_dps_path)\n\n        self.input_data_objects = {\n            silver_events.ID: silver_events,\n            silver_cell_footprint.ID: silver_cell_footprint,\n            events_cache.ID: events_cache,\n            cell_to_group.ID: cell_to_group,\n            group_to_tile.ID: group_to_tile,\n        }\n\n        self.output_data_objects = {silver_dps.ID: silver_dps}\n\n    # ------------------ Assert existence of input data and read+filter it. ------------------\n\n    def check_needed_dates(self):\n        \"\"\"\n        Method that checks if both the dates of study and the dates necessary to generate\n        the daily permanence scores are present in the input data (events + cell footprint).\n\n        Raises:\n            ValueError: If there is no data for one or more of the needed dates.\n        \"\"\"\n        # needed dates: for each date D, we also need D-1 and D+1\n        # this is built this way so it would also support definition of study\n        # dates that are not consecutive\n        needed_dates = (\n            {d + timedelta(days=1) for d in self.data_period_dates}\n            | set(self.data_period_dates)\n            | {d - timedelta(days=1) for d in self.data_period_dates}\n        )\n        self.logger.info(needed_dates)\n        # Assert needed dates in event data:\n        self.assert_needed_dates_events()\n\n        # Assert needed dates in cell footprint data:\n        self.assert_needed_dates_data_object(SilverCellFootprintDataObject.ID, needed_dates)\n\n        # Assert needed dates in cell to group data:\n        self.assert_needed_dates_data_object(SilverCellToGroupDataObject.ID, needed_dates)\n\n        # Assert needed dates in group to tile data:\n        self.assert_needed_dates_data_object(SilverGroupToTileDataObject.ID, needed_dates)\n\n    def assert_needed_dates_events(self):\n        extended_dates = {d + timedelta(days=1) for d in self.data_period_dates} | {\n            d - timedelta(days=1) for d in self.data_period_dates\n        }\n\n        for dates_to_check, event_type_do in zip(\n            [self.data_period_dates, extended_dates], [SilverEventFlaggedDataObject, EventCacheDataObject]\n        ):\n            # Check event data for data_period_dates\n            missing_dates = [\n                date for date in dates_to_check if not self.input_data_objects[event_type_do.ID].is_data_available(date)\n            ]\n            # Report missing dates\n            if missing_dates:\n                error_msg = f\"Missing {event_type_do.ID} data for dates {sorted(list(missing_dates))}\"\n                self.logger.error(error_msg)\n                raise ValueError(error_msg)\n\n        # Check event cache data for extended_dates\n        missing_dates = [\n            date\n            for date in extended_dates\n            if not self.input_data_objects[EventCacheDataObject.ID].is_data_available(date)\n        ]\n\n    def assert_needed_dates_data_object(self, data_object_id: str, needed_dates: List[datetime]):\n        \"\"\"\n        Method that checks if data for a set of dates exists for a data object.\n\n        Args:\n            data_object_id (str): name of the data object to check.\n            needed_dates (List[datetime]): list of the dates for which data shall be available.\n\n        Raises:\n            ValueError: If there is no data for one or more of the needed dates.\n        \"\"\"\n        # Load data\n        df = self.input_data_objects[data_object_id].df\n\n        # Find dates that match the needed dates:\n        dates = (\n            df.withColumn(ColNames.date, F.make_date(ColNames.year, ColNames.month, ColNames.day))\n            .select(F.col(ColNames.date))\n            .filter(F.col(ColNames.date).isin(needed_dates))\n            .distinct()\n            .collect()\n        )\n        available_dates = {row[ColNames.date] for row in dates}\n\n        # If missing needed dates, raise error:\n        missing_dates = needed_dates.difference(available_dates)\n        if missing_dates:\n            error_msg = f\"Missing {data_object_id} data for dates {sorted(list(missing_dates))}\"\n            self.logger.error(error_msg)\n            raise ValueError(error_msg)\n\n    def filter_events(self, current_date: date, partition_chunk=None) -&gt; DataFrame:\n        \"\"\"\n        Load events with no errors for a specific date.\n\n        Args:\n            current_date (date): current date.\n\n        Returns:\n            DataFrame: filtered events dataframe.\n        \"\"\"\n        df = (\n            self.input_data_objects[SilverEventFlaggedDataObject.ID]\n            .df.filter(\n                (F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(current_date))\n                &amp; (F.col(ColNames.error_flag).isin(self.event_error_flags_to_include))\n            )\n            .select(\n                ColNames.user_id,\n                ColNames.cell_id,\n                ColNames.timestamp,\n                ColNames.plmn,\n                ColNames.year,\n                ColNames.month,\n                ColNames.day,\n                ColNames.user_id_modulo,\n            )\n        )\n\n        if partition_chunk is not None:\n            df = df.filter(F.col(ColNames.user_id_modulo).isin(partition_chunk))\n\n        return df\n\n    def get_cache_events(self, current_date: date, partition_chunk=None, last_event: bool = True) -&gt; DataFrame:\n        \"\"\"\n        Load cache events with for a specific date.\n\n        Args:\n            current_date (date): current date.\n            last_event (bool): flag to get last event or first.\n\n        Returns:\n            DataFrame: filtered events dataframe.\n        \"\"\"\n        df = (\n            self.input_data_objects[EventCacheDataObject.ID]\n            .df.filter(\n                (F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(current_date))\n            )\n            .filter(F.col(ColNames.is_last_event) == last_event)\n            .drop(ColNames.is_last_event)\n        ).select(\n            ColNames.user_id,\n            ColNames.cell_id,\n            ColNames.timestamp,\n            ColNames.plmn,\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            ColNames.user_id_modulo,\n        )\n\n        if partition_chunk is not None:\n            df = df.filter(F.col(ColNames.user_id_modulo).isin(partition_chunk))\n\n        return df\n\n    def filter_cell_footprint(self, current_date: date, cells: DataFrame = None) -&gt; DataFrame:\n        \"\"\"\n        Load cell footprints for a specific date.\n\n        Args:\n            current_date (date): current date.\n\n        Returns:\n            DataFrame: filtered cell footprint dataframe.\n        \"\"\"\n        df = self.input_data_objects[SilverCellFootprintDataObject.ID].df.filter(\n            (F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(current_date))\n        )\n\n        if cells is not None:\n            df = df.join(cells, ColNames.cell_id, \"inner\")\n\n        return df\n\n    def filter_cell_to_group(self, current_date: date) -&gt; DataFrame:\n        \"\"\"\n        Loads cell to group reference table for a specific date.\n\n        Args:\n            current_date (date): current date\n\n        Returns:\n            DataFrame: filtered cell to group dataframe\n        \"\"\"\n        df = self.input_data_objects[SilverCellToGroupDataObject.ID].df.filter(\n            (F.col(ColNames.year) == F.lit(current_date.year))\n            &amp; (F.col(ColNames.month) == F.lit(current_date.month))\n            &amp; (F.col(ColNames.day) == F.lit(current_date.day))\n        )\n\n        return df\n\n    def filter_group_to_tile(self, current_date: date) -&gt; DataFrame:\n        \"\"\"\n        Loads group to tile reference table for a specific date.\n\n        Args:\n            current_date (date): current date\n\n        Returns:\n            DataFrame: filtered group to tile dataframe\n        \"\"\"\n        df = self.input_data_objects[SilverGroupToTileDataObject.ID].df.filter(\n            (F.col(ColNames.year) == F.lit(current_date.year))\n            &amp; (F.col(ColNames.month) == F.lit(current_date.month))\n            &amp; (F.col(ColNames.day) == F.lit(current_date.day))\n        )\n\n        return df\n\n    # ------------------ Execute ------------------\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n        self.check_needed_dates()\n        partition_chunks = self._get_partition_chunks()\n\n        for current_date in self.data_period_dates:\n            self.current_date = current_date  # for use in other methods\n            self.logger.info(f\"Processing events for {current_date.strftime('%Y-%m-%d')}\")\n            self.build_time_slots_table(current_date)\n\n            for i, partition_chunk in enumerate(partition_chunks):\n                self.logger.info(f\"Processing partition chunk {i}\")\n                self.logger.debug(f\"Partition chunk: {partition_chunk}\")\n\n                # Build input data\n                self.build_day_data(current_date, partition_chunk)\n\n                # Transform\n                self.transform()\n\n                # Write\n                self.write()\n                self.spark.catalog.clearCache()\n                self.logger.info(f\"Finished partition chunk {i}\")\n\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def _get_partition_chunks(self) -&gt; List[List[int]]:\n        \"\"\"\n        Method that returns the partition chunks for the current date.\n\n        Returns:\n            List[List[int, int]]: list of partition chunks. If the partition_chunk_size is not defined in the config or\n                the number of partitions is less than the desired chunk size, it will return a list with a single None element.\n        \"\"\"\n        # Get partitions desired\n        partition_chunk_size = self.config.getint(self.COMPONENT_ID, \"partition_chunk_size\", fallback=None)\n        number_of_partitions = self.config.getint(self.COMPONENT_ID, \"number_of_partitions\", fallback=None)\n\n        if partition_chunk_size is None or number_of_partitions is None or partition_chunk_size &lt;= 0:\n            return [None]\n\n        if number_of_partitions &lt;= partition_chunk_size:\n            self.logger.warning(\n                f\"Available Partition number ({number_of_partitions}) is \"\n                f\"less than the desired chunk size ({partition_chunk_size}). \"\n                f\"Using all partitions.\"\n            )\n            return [None]\n        partition_chunks = [\n            list(range(i, min(i + partition_chunk_size, number_of_partitions)))\n            for i in range(0, number_of_partitions, partition_chunk_size)\n        ]\n        # NOTE: Generate chunks if partition_values were read for each day\n        # getting exactly the amount of partitions for that day\n\n        # partition_chunks = [\n        #     partition_values[i : i + partition_chunk_size]\n        #     for i in range(0, partition_values_size, partition_chunk_size)\n        # ]\n\n        return partition_chunks\n\n    def build_day_data(self, current_date, partition_chunk=None):\n        \"\"\"\n        Load events data for date D, also adding last event of each\n        user from date D-1 and first event of each user from D+1.\n\n        Args:\n            current_date (date): current date.\n\n        Returns:\n            DataFrame: events dataframe.\n        \"\"\"\n        previous_date = current_date - timedelta(days=1)\n        next_date = current_date + timedelta(days=1)\n\n        current_events = self.filter_events(current_date, partition_chunk)\n        previous_events = self.get_cache_events(previous_date, partition_chunk, last_event=True)\n        next_events = self.get_cache_events(next_date, partition_chunk, last_event=False)\n\n        # concat all events together (last of D-1 + all D + first of D+1 dummy outbound):\n        events = previous_events.union(current_events).union(next_events)\n\n        # Get distinct cell_ids for previous and next events for filtering footprints\n        previous_cells = previous_events.select(ColNames.cell_id).distinct()\n        next_cells = next_events.select(ColNames.cell_id).distinct()\n\n        current_cell_footprint = self.filter_cell_footprint(current_date)\n        previous_cell_footprint = self.filter_cell_footprint(previous_date, previous_cells)\n        next_cell_footprint = self.filter_cell_footprint(next_date, next_cells)\n\n        cell_footprint = previous_cell_footprint.union(current_cell_footprint).union(next_cell_footprint)\n\n        # We only use geometries to calculate distances, so we can use any origin for our 4-byte grid ID.\n        cell_footprint = self.grid_gen.grid_ids_to_grid_centroids(\n            cell_footprint.withColumn(ColNames.origin, F.lit(0).cast(LongType())),\n            grid_resolution=100,\n        ).drop(ColNames.origin)\n\n        if self.use_200m_grid:\n            self.logger.info(\"Using 200m grid for DPS calculation\")\n            cell_footprint = self.grid_gen.grid_id_to_coarser_resolution(\n                sdf=cell_footprint, coarse_resolution=200, coarse_grid_id_col=ColNames.grid_id\n            )\n\n        cell_footprint = cell_footprint.groupBy([ColNames.cell_id, ColNames.year, ColNames.month, ColNames.day]).agg(\n            F.collect_list(ColNames.geometry).alias(ColNames.geometry),\n            F.collect_set(ColNames.grid_id).alias(\"grid_ids\"),\n        )\n\n        cell_footprint = cell_footprint.withColumn(\n            ColNames.geometry,\n            STF.ST_ConcaveHull(STF.ST_Collect(F.col(ColNames.geometry)), 0.8),\n        )\n\n        # Load class attributes\n        self.events = events\n\n        # TODO: This is questionable, use with care\n        if self.broadcast_footprints:\n            cell_footprint = F.broadcast(cell_footprint)\n\n        self.cell_footprint = cell_footprint.persist(StorageLevel.MEMORY_AND_DISK)\n\n        # Load cell to group data\n        previous_cell_to_group = self.filter_cell_to_group(previous_date)\n        current_cell_to_group = self.filter_cell_to_group(current_date)\n        next_cell_to_group = self.filter_cell_to_group(next_date)\n\n        self.cell_to_group = previous_cell_to_group.union(current_cell_to_group).union(next_cell_to_group)\n\n        # Load group to tile data\n        previous_group_to_tile = self.filter_group_to_tile(previous_date)\n        current_group_to_tile = self.filter_group_to_tile(current_date)\n        next_group_to_tile = self.filter_group_to_tile(next_date)\n\n        self.group_to_tile = previous_group_to_tile.union(current_group_to_tile).union(next_group_to_tile)\n\n    def build_time_slots_table(self, current_date) -&gt; DataFrame:\n        \"\"\"\n        Build a dataframe with the specified time slots for the current date.\n\n        Returns:\n            DataFrame: time slots dataframe.\n        \"\"\"\n        time_slot_length = timedelta(days=1) / self.time_slot_number\n\n        time_slots_list = []\n        previous_end_time = datetime(\n            year=current_date.year,\n            month=current_date.month,\n            day=current_date.day,\n            hour=0,\n            minute=0,\n            second=0,\n        )\n\n        while previous_end_time.date() == current_date:\n            init_time = previous_end_time\n            end_time = init_time + time_slot_length\n            time_slot = (init_time, end_time)\n            time_slots_list.append(time_slot)\n            previous_end_time = end_time\n\n        schema = StructType(\n            [\n                StructField(ColNames.time_slot_initial_time, TimestampType(), True),\n                StructField(ColNames.time_slot_end_time, TimestampType(), True),\n            ]\n        )\n\n        self.time_slots = self.spark.createDataFrame(time_slots_list, schema=schema)\n\n    # ------------------ Main transformations ------------------\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n\n        current_events = self.events\n\n        current_events = self.enrich_events(current_events, self.cell_footprint)\n        # differentiate 'move' events:\n        current_events = self.detect_move_events(current_events)\n        # Determine stay durations:\n        stays = self.determine_stay_durations(current_events)\n        # generate time slots per user\n        # TODO: this might be an issue with big countries, probably will need to rethink\n        unique_users = stays.select(ColNames.user_id, ColNames.user_id_modulo).distinct()\n        unique_users = unique_users.repartition(ColNames.user_id_modulo)\n        user_time_slots = unique_users.crossJoin(F.broadcast(self.time_slots))\n\n        # Assign stay time slot, assign duration to time slots:\n        stays_slots = self.calculate_time_slots_durations(stays, user_time_slots)\n\n        # Filter out durations that would result in 0 DPS\n        stays_slots = self.filter_zero_dps_durations(stays_slots)\n        stays_slots = stays_slots.persist(StorageLevel.MEMORY_AND_DISK)\n        stays_slots.count()  # for some reason better to force computation\n        # collect grid_ids where DPS is 1\n        dps = self.calculate_dps(stays_slots, self.cell_footprint)\n\n        dps = (\n            dps\n            # since some stays may come from events in previous date, fix and always set current date:\n            .withColumn(ColNames.year, F.lit(self.current_date.year).cast(ShortType()))\n            .withColumn(ColNames.month, F.lit(self.current_date.month).cast(ByteType()))\n            .withColumn(ColNames.day, F.lit(self.current_date.day).cast(ByteType()))\n        )\n\n        dps = apply_schema_casting(dps, SilverDailyPermanenceScoreDataObject.SCHEMA)\n\n        dps = dps.repartition(*SilverDailyPermanenceScoreDataObject.PARTITION_COLUMNS)\n        self.output_data_objects[SilverDailyPermanenceScoreDataObject.ID].df = dps\n\n    def enrich_events(self, events: DataFrame, cell_footprint: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Enrich events with additional information\n\n        Args:\n            events (DataFrame): events dataframe.\n            cell_footprint (DataFrame): cells footprint dataframe.\n\n        Returns:\n            DataFrame: enriched events dataframe.\n        \"\"\"\n\n        events = self.join_footprints(events, cell_footprint, [ColNames.geometry])\n        # add abroad flag\n        events = (\n            events.withColumn(\"abroad_mcc\", F.substring(F.col(ColNames.plmn), 0, 3).cast(IntegerType()))\n            .withColumn(\n                \"is_abroad\",\n                F.when(\n                    (F.col(ColNames.plmn).isNotNull()) &amp; (F.col(\"abroad_mcc\") != self.local_mcc), F.lit(True)\n                ).otherwise(F.lit(False)),\n            )\n            .withColumn(  # add abroad country mcc as cell_id for abroad events\n                ColNames.cell_id,\n                F.when(F.col(\"is_abroad\") == True, F.col(\"abroad_mcc\")).otherwise(F.col(ColNames.cell_id)),\n            )\n            .drop(\"abroad_mcc\", ColNames.plmn)\n        )\n        # Add lags of timestamp, cell_id:\n        window = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id).orderBy(ColNames.timestamp)\n        lag_fields = [ColNames.timestamp, ColNames.cell_id, ColNames.geometry]\n        for lf in lag_fields:\n            events = events.withColumn(f\"{lf}_+1\", F.lag(lf, -1).over(window)).withColumn(\n                f\"{lf}_-1\", F.lag(lf, 1).over(window)\n            )\n\n        return events\n\n    def detect_move_events(self, events: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Detect which of the events are associated to moves according to the\n        distances/times from previous to posterior event and a speed threshold.\n\n        Args:\n            events (DataFrame): events dataframe.\n            cell_footprint (DataFrame): cells footprint dataframe.\n\n        Returns:\n            DataFrame: events dataframe, with an additional 'is_move' boolean column.\n        \"\"\"\n\n        window = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id).orderBy(ColNames.timestamp)\n        # Calculate distance between grid tiles associated to events -1, 0 and +1:\n        # Calculate speeds and determine which rows are moves:\n        events = (\n            events.withColumn(\n                \"dist_0_+1\",\n                F.when(\n                    F.col(ColNames.geometry).isNotNull() &amp; F.col(f\"{ColNames.geometry}_+1\").isNotNull(),\n                    STF.ST_Distance(F.col(ColNames.geometry), F.col(f\"{ColNames.geometry}_+1\")),\n                ).otherwise(F.lit(0)),\n            )\n            # .withColumn(\n            #     \"dist_-1_0\",\n            #     STF.ST_Distance(F.col(f\"{ColNames.geometry}_-1\"), F.col(ColNames.geometry)),\n            # )\n            .withColumn(  # repeating the distance calculation is not necessary, a lagged column works:\n                \"dist_-1_0\", F.lag(\"dist_0_+1\", 1).over(window)\n            )\n            .withColumn(\n                \"dist_-1_+1\",\n                F.when(\n                    F.col(f\"{ColNames.geometry}_-1\").isNotNull() &amp; F.col(f\"{ColNames.geometry}_+1\").isNotNull(),\n                    STF.ST_Distance(F.col(f\"{ColNames.geometry}_-1\"), F.col(f\"{ColNames.geometry}_+1\")),\n                ).otherwise(F.lit(0)),\n            )\n            .withColumn(\n                \"time_difference\",\n                F.unix_timestamp(events[f\"{ColNames.timestamp}_+1\"])\n                - F.unix_timestamp(events[f\"{ColNames.timestamp}_-1\"]),\n            )\n            .withColumn(\"max_dist\", F.greatest(F.col(\"dist_-1_0\") + F.col(\"dist_0_+1\"), F.col(\"dist_-1_+1\")))\n            .withColumn(\"speed\", F.col(\"max_dist\") / F.col(\"time_difference\"))\n            .withColumn(\"is_move\", F.when(F.col(\"speed\") &gt; self.max_speed_thresh, True).otherwise(False))\n            .drop(\n                \"dist_0_+1\",\n                \"dist_-1_0\",\n                \"dist_-1_+1\",\n                f\"{ColNames.geometry}_-1\",\n                f\"{ColNames.geometry}_+1\",\n                ColNames.geometry,\n                \"time_difference\",\n                \"max_dist\",\n                \"speed\",\n            )\n        )\n\n        return events\n\n    def determine_stay_durations(self, events: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Determine the start time and end time for each stay event.\n\n        Args:\n            events (DataFrame): events dataframe.\n\n        Returns:\n            DataFrame: stays dataframe (filtering out moves).\n        \"\"\"\n        current_datetime = datetime(self.current_date.year, self.current_date.month, self.current_date.day)\n        # TODO: night interval could be a config parameter instead of hardcoded!\n        night_start_time = current_datetime - timedelta(hours=1)\n        night_end_time = current_datetime + timedelta(hours=9)\n\n        stays = (\n            events\n            # Filter out 'move' events (keep only stays), and also drop unneeded columns:\n            .filter(F.col(\"is_move\") == False)\n            # Set applicable time thresholds:\n            # if prev, next and current events:\n            # - if the event is in the same cell as the previous one, and the previous event is within the night interval,\n            # - set the threshold to max_time_thresh_night, otherwise set it to max_time_thresh_day\n            # - if the event is not in the same cell as the previous one, set the threshold to max_time_thresh\n            # if prev or next event is missing and current event is abroad:\n            # - set missing timestamp to current timestamp +/- max_time_thresh_abroad\n            .withColumn(\n                \"threshold_-1\",\n                F.when(\n                    (F.col(\"cell_id\") == F.col(\"cell_id_-1\"))\n                    &amp; (F.col(\"timestamp\") &gt;= night_start_time)\n                    &amp; (F.col(\"timestamp_-1\") &lt;= night_end_time),\n                    self.max_time_thresh_night,\n                ).otherwise(\n                    F.when(F.col(\"cell_id\") == F.col(\"cell_id_-1\"), self.max_time_thresh_day).otherwise(\n                        self.max_time_thresh\n                    )\n                ),\n            )\n            .withColumn(\n                \"threshold_+1\",\n                F.when(\n                    (F.col(\"cell_id\") == F.col(\"cell_id_+1\"))\n                    &amp; (F.col(\"timestamp\") &gt;= night_start_time)\n                    &amp; (F.col(\"timestamp_+1\") &lt;= night_end_time),\n                    self.max_time_thresh_night,\n                ).otherwise(\n                    F.when(F.col(\"cell_id\") == F.col(\"cell_id_+1\"), self.max_time_thresh_day).otherwise(\n                        self.max_time_thresh\n                    )\n                ),\n            )\n            # Calculate init_time and end_time according to thresholds and time differences between events:\n            .withColumn(\n                \"init_time\",\n                F.when(\n                    (F.col(f\"{ColNames.timestamp}_-1\").isNull()) &amp; (F.col(\"is_abroad\") == True),\n                    F.col(ColNames.timestamp) - self.max_time_thresh_abroad,\n                )\n                .when(\n                    F.col(ColNames.timestamp) - F.col(f\"{ColNames.timestamp}_-1\") &lt;= F.col(\"threshold_-1\"),\n                    F.col(ColNames.timestamp) - (F.col(ColNames.timestamp) - F.col(f\"{ColNames.timestamp}_-1\")) / 2,\n                )\n                .otherwise(F.col(ColNames.timestamp) - self.max_time_thresh / 2),\n            )\n            .withColumn(\n                \"end_time\",\n                F.when(\n                    (F.col(f\"{ColNames.timestamp}_+1\").isNull()) &amp; (F.col(\"is_abroad\") == True),\n                    F.col(ColNames.timestamp) + self.max_time_thresh_abroad,\n                )\n                .when(\n                    F.col(f\"{ColNames.timestamp}_+1\") - F.col(ColNames.timestamp) &lt;= F.col(\"threshold_+1\"),\n                    F.col(f\"{ColNames.timestamp}_+1\")\n                    - (F.col(f\"{ColNames.timestamp}_+1\") - F.col(ColNames.timestamp)) / 2,\n                )\n                .otherwise(F.col(ColNames.timestamp) + self.max_time_thresh / 2),\n            )\n            .drop(\n                f\"{ColNames.cell_id}_-1\",\n                f\"{ColNames.cell_id}_+1\",\n                ColNames.timestamp,\n                f\"{ColNames.timestamp}_-1\",\n                f\"{ColNames.timestamp}_+1\",\n                ColNames.mcc,\n                \"is_move\",\n                \"threshold_-1\",\n                \"threshold_+1\",\n            )\n        )\n\n        return stays\n\n    def calculate_time_slots_durations(self, stays: DataFrame, user_time_slots: DataFrame) -&gt; DataFrame:\n        \"\"\"Calculates duration of user stays within defined time slots.\n\n        Joins stay records with time slot definitions and computes overlapping durations.\n        For time slots with no stays, assigns a default duration of time_slot_number (1/24th day).\n        Finally aggregates total stay duration per user, cell and time slot.\n\n        Args:\n            stays (DataFrame): User mobility stay records\n            user_time_slots (DataFrame): User time slot definitions\n\n        Returns:\n            DataFrame: Aggregated stay durations per time slot\n        \"\"\"\n        stays = (\n            stays.join(\n                user_time_slots.select(\n                    F.col(ColNames.user_id).alias(\"time_slot_user_id\"),\n                    F.col(ColNames.user_id_modulo).alias(\"time_slot_user_id_modulo\"),\n                    ColNames.time_slot_end_time,\n                    ColNames.time_slot_initial_time,\n                ),\n                (\n                    (F.col(\"init_time\") &lt; F.col(ColNames.time_slot_end_time))\n                    &amp; (\n                        F.col(\"end_time\")\n                        &gt; F.col(\n                            ColNames.time_slot_initial_time,\n                        )\n                    )\n                    &amp; (F.col(ColNames.user_id) == F.col(\"time_slot_user_id\"))\n                    &amp; (F.col(ColNames.user_id_modulo) == F.col(\"time_slot_user_id_modulo\"))\n                ),\n                how=\"right\",\n            )\n            .withColumn(\"init_time\", F.greatest(F.col(\"init_time\"), F.col(ColNames.time_slot_initial_time)))\n            .withColumn(\"end_time\", F.least(F.col(\"end_time\"), F.col(ColNames.time_slot_end_time)))\n            .withColumn(\n                ColNames.stay_duration,\n                F.when(\n                    F.col(ColNames.cell_id).isNotNull(),\n                    F.unix_timestamp(F.col(\"end_time\")) - F.unix_timestamp(F.col(\"init_time\")),\n                ).otherwise(F.lit((timedelta(days=1) / self.time_slot_number).total_seconds()).cast(IntegerType())),\n            )\n            .select(\n                F.coalesce(ColNames.user_id, \"time_slot_user_id\").alias(ColNames.user_id),\n                F.coalesce(ColNames.cell_id, F.lit(UeGridIdType.UNKNOWN)).alias(ColNames.cell_id),\n                ColNames.time_slot_initial_time,\n                ColNames.time_slot_end_time,\n                ColNames.stay_duration,\n                F.coalesce(F.col(\"is_abroad\"), F.lit(False)).alias(\"is_abroad\"),\n                ColNames.year,\n                ColNames.month,\n                ColNames.day,\n                F.coalesce(ColNames.user_id_modulo, \"time_slot_user_id_modulo\").alias(ColNames.user_id_modulo),\n            )\n        )\n\n        stays = stays.groupBy(\n            ColNames.user_id,\n            ColNames.cell_id,\n            ColNames.time_slot_initial_time,\n            ColNames.time_slot_end_time,\n            \"is_abroad\",\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            ColNames.user_id_modulo,\n        ).agg((F.sum(ColNames.stay_duration).cast(FloatType()).alias(ColNames.stay_duration)))\n\n        return stays\n\n    def filter_zero_dps_durations(self, stays: DataFrame) -&gt; DataFrame:\n        \"\"\"Filters out stay records that would result in zero Daily Permanence Score.\n\n        Removes records where the total stay duration is less than half of a time slot's\n        duration (1/24th of a day). This pre-filtering step optimizes performance by\n        eliminating records that would not contribute to the final score.\n\n        Args:\n            stays (DataFrame): DataFrame containing stay records with durations\n\n        Returns:\n            DataFrame: Filtered stay records above minimum duration threshold\n        \"\"\"\n        window_spec = Window.partitionBy(\n            ColNames.user_id,\n            ColNames.user_id_modulo,\n            ColNames.time_slot_initial_time,\n        )\n\n        # Count distinct cell_id within each window\n        stays = stays.withColumn(\n            \"distinct_cell_count\", F.size(F.collect_set(ColNames.cell_id).over(window_spec))\n        ).withColumn(\"duration_sum\", F.sum(ColNames.stay_duration).over(window_spec))\n\n        # remove all records where DPS will be 0\n        stays = stays.filter(\n            F.col(\"duration_sum\") &gt;= F.lit(int((timedelta(days=1) / self.time_slot_number).total_seconds() / 2))\n        ).drop(\"duration_sum\")\n\n        return stays\n\n    def calculate_dps(self, stay_intervals: DataFrame, cell_footprint: DataFrame) -&gt; DataFrame:\n        \"\"\"Calculate Daily Permanence Score (DPS) from user stay intervals.\n\n        Processes stay intervals to determine user's presence in grid locations:\n        - For multiple cell stays: explodes footprints and aggregates overlapping areas\n        - For single cell stays: directly maps to corresponding grid IDs\n        - For unknown locations: assigns special unknown identifier\n\n        Args:\n            stay_intervals (DataFrame): User stay intervals with duration and cell information\n            cell_footprint (DataFrame): Mapping between cells and their grid coverage areas\n\n        Returns:\n            DataFrame: Daily Permanence Score results with grid IDs and type identification\n        \"\"\"\n\n        # ----------- These are time slots to which contribute stays in different cells ----------\n        # We will join these cells with the corresponding cell footprint intersection groups, which represent sets\n        # of tiles covered by a particular combination of cells. All of the tiles in one of these groups will have the\n        # exact same total stay duration, and thus the same DPS. We can then calculate the DPS for the cell footprint\n        # intersection group by itself and then assign it to each of the tiles that it contains.\n        local_stay_intervals = stay_intervals.filter(F.col(\"is_abroad\") == False)\n        stay_intervals_multiple_cells = local_stay_intervals.filter(F.col(\"distinct_cell_count\") &gt; 1)\n\n        # Join with cell-to-group reference table\n        stay_intervals_multiple_cells = stay_intervals_multiple_cells.join(\n            self.cell_to_group, on=[ColNames.year, ColNames.month, ColNames.day, ColNames.cell_id], how=\"left\"\n        )\n        stay_intervals_multiple_cells = stay_intervals_multiple_cells.drop(ColNames.cell_id)\n\n        stay_intervals_multiple_cells = (\n            stay_intervals_multiple_cells.groupBy(\n                ColNames.year,\n                ColNames.month,\n                ColNames.day,\n                ColNames.user_id_modulo,\n                ColNames.user_id,\n                ColNames.time_slot_initial_time,\n                ColNames.time_slot_end_time,\n                ColNames.group_id,\n            )\n            .agg(F.sum(ColNames.stay_duration).alias(ColNames.stay_duration))\n            .filter(\n                F.col(ColNames.stay_duration)\n                &gt;= F.lit(int((timedelta(days=1) / self.time_slot_number).total_seconds() / 2))\n            )\n        )\n\n        # We have now determined what cell footprint intersection groups have DPS = 1, which are exactly the rows\n        # we have kept after the filter. The next step is to join each group with its grid tiles, and collect them\n        # into an array of tile IDs, which will be the DPS column.\n        stay_intervals_multiple_cells = stay_intervals_multiple_cells.join(\n            self.group_to_tile, on=[ColNames.year, ColNames.month, ColNames.day, ColNames.group_id], how=\"left\"\n        ).drop(ColNames.group_id)\n\n        stay_intervals_multiple_cells = (\n            stay_intervals_multiple_cells.groupBy(\n                ColNames.user_id,\n                ColNames.time_slot_initial_time,\n                ColNames.time_slot_end_time,\n                ColNames.user_id_modulo,\n            )\n            .agg(F.collect_set(\"grid_id\").alias(ColNames.dps))\n            .select(\n                ColNames.user_id,\n                ColNames.dps,\n                ColNames.time_slot_initial_time,\n                ColNames.time_slot_end_time,\n                ColNames.user_id_modulo,\n                F.lit(UeGridIdType.GRID_STR).alias(ColNames.id_type),\n            )\n        )\n\n        # ------------ These are time slots to which contribute stay(s) in a single cell ----------\n        # Here there is no need to add up the stay duration in different cells, so the calculation of the DPS is\n        # straightforward and, if it surpasses the threshold, we just keep the cell footprint of the cell\n\n        # For intervals with single cell no need to explode, just join grid ids as is\n        stay_intervals_single_cell = local_stay_intervals.filter(\n            (F.col(\"distinct_cell_count\") == 1)\n            &amp; (F.col(ColNames.cell_id) != F.lit(UeGridIdType.UNKNOWN).cast(StringType()))\n        )\n        stay_intervals_single_cell = self.join_footprints(stay_intervals_single_cell, cell_footprint, [\"grid_ids\"])\n\n        stay_intervals_single_cell = stay_intervals_single_cell.select(\n            ColNames.user_id,\n            F.col(\"grid_ids\").alias(ColNames.dps),\n            ColNames.time_slot_initial_time,\n            ColNames.time_slot_end_time,\n            ColNames.user_id_modulo,\n            F.lit(UeGridIdType.GRID_STR).alias(ColNames.id_type),\n        )\n\n        uknown_intervals = local_stay_intervals.filter(F.col(ColNames.cell_id) == F.lit(UeGridIdType.UNKNOWN))\n\n        uknown_intervals = uknown_intervals.withColumn(\n            ColNames.dps,\n            F.array(F.lit(UeGridIdType.UNKNOWN).cast(IntegerType())),\n        ).select(\n            ColNames.user_id,\n            ColNames.dps,\n            ColNames.time_slot_initial_time,\n            ColNames.time_slot_end_time,\n            ColNames.user_id_modulo,\n            F.lit(UeGridIdType.UKNOWN_STR).alias(ColNames.id_type),\n        )\n\n        abroad_intervals = stay_intervals.filter(\n            (F.col(\"is_abroad\") == True)\n            &amp; (\n                F.col(ColNames.stay_duration)\n                &gt;= F.lit(int((timedelta(days=1) / self.time_slot_number).total_seconds() / 2))\n            )\n        )\n        abroad_intervals = abroad_intervals.withColumn(\n            ColNames.dps,\n            F.array(F.col(ColNames.cell_id).cast(IntegerType())),\n        ).select(\n            ColNames.user_id,\n            ColNames.dps,\n            ColNames.time_slot_initial_time,\n            ColNames.time_slot_end_time,\n            ColNames.user_id_modulo,\n            F.lit(UeGridIdType.ABROAD_STR).alias(ColNames.id_type),\n        )\n\n        dps = (\n            stay_intervals_single_cell.union(stay_intervals_multiple_cells)\n            .union(uknown_intervals)\n            .union(abroad_intervals)\n        )\n\n        return dps\n\n    def join_footprints(self, events: DataFrame, cell_footprints: DataFrame, columns: List) -&gt; DataFrame:\n        \"\"\"\n        Join the events dataframe with the cell_footprint dataframe.\n\n        Args:\n            events (DataFrame): events dataframe.\n            cell_footprint (DataFrame): cell_footprint dataframe.\n\n        Returns:\n            DataFrame: events dataframe with the cell footprint information.\n        \"\"\"\n        events = events.join(\n            cell_footprints.select(\n                F.col(ColNames.cell_id).alias(\"footprints_cell_id\"),\n                F.col(ColNames.year).alias(\"footprints_year\"),\n                F.col(ColNames.month).alias(\"footprints_month\"),\n                F.col(ColNames.day).alias(\"footprints_day\"),\n                *columns,\n            ),\n            (F.col(ColNames.cell_id) == F.col(\"footprints_cell_id\"))\n            &amp; (F.col(ColNames.year) == F.col(\"footprints_year\"))\n            &amp; (F.col(ColNames.month) == F.col(\"footprints_month\"))\n            &amp; (F.col(ColNames.day) == F.col(\"footprints_day\")),\n            \"left\",\n        ).drop(\n            \"footprints_cell_id\",\n            \"footprints_year\",\n            \"footprints_month\",\n            \"footprints_day\",\n        )\n\n        return events\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.assert_needed_dates_data_object","title":"<code>assert_needed_dates_data_object(data_object_id, needed_dates)</code>","text":"<p>Method that checks if data for a set of dates exists for a data object.</p> <p>Parameters:</p> Name Type Description Default <code>data_object_id</code> <code>str</code> <p>name of the data object to check.</p> required <code>needed_dates</code> <code>List[datetime]</code> <p>list of the dates for which data shall be available.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If there is no data for one or more of the needed dates.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def assert_needed_dates_data_object(self, data_object_id: str, needed_dates: List[datetime]):\n    \"\"\"\n    Method that checks if data for a set of dates exists for a data object.\n\n    Args:\n        data_object_id (str): name of the data object to check.\n        needed_dates (List[datetime]): list of the dates for which data shall be available.\n\n    Raises:\n        ValueError: If there is no data for one or more of the needed dates.\n    \"\"\"\n    # Load data\n    df = self.input_data_objects[data_object_id].df\n\n    # Find dates that match the needed dates:\n    dates = (\n        df.withColumn(ColNames.date, F.make_date(ColNames.year, ColNames.month, ColNames.day))\n        .select(F.col(ColNames.date))\n        .filter(F.col(ColNames.date).isin(needed_dates))\n        .distinct()\n        .collect()\n    )\n    available_dates = {row[ColNames.date] for row in dates}\n\n    # If missing needed dates, raise error:\n    missing_dates = needed_dates.difference(available_dates)\n    if missing_dates:\n        error_msg = f\"Missing {data_object_id} data for dates {sorted(list(missing_dates))}\"\n        self.logger.error(error_msg)\n        raise ValueError(error_msg)\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.build_day_data","title":"<code>build_day_data(current_date, partition_chunk=None)</code>","text":"<p>Load events data for date D, also adding last event of each user from date D-1 and first event of each user from D+1.</p> <p>Parameters:</p> Name Type Description Default <code>current_date</code> <code>date</code> <p>current date.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <p>events dataframe.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def build_day_data(self, current_date, partition_chunk=None):\n    \"\"\"\n    Load events data for date D, also adding last event of each\n    user from date D-1 and first event of each user from D+1.\n\n    Args:\n        current_date (date): current date.\n\n    Returns:\n        DataFrame: events dataframe.\n    \"\"\"\n    previous_date = current_date - timedelta(days=1)\n    next_date = current_date + timedelta(days=1)\n\n    current_events = self.filter_events(current_date, partition_chunk)\n    previous_events = self.get_cache_events(previous_date, partition_chunk, last_event=True)\n    next_events = self.get_cache_events(next_date, partition_chunk, last_event=False)\n\n    # concat all events together (last of D-1 + all D + first of D+1 dummy outbound):\n    events = previous_events.union(current_events).union(next_events)\n\n    # Get distinct cell_ids for previous and next events for filtering footprints\n    previous_cells = previous_events.select(ColNames.cell_id).distinct()\n    next_cells = next_events.select(ColNames.cell_id).distinct()\n\n    current_cell_footprint = self.filter_cell_footprint(current_date)\n    previous_cell_footprint = self.filter_cell_footprint(previous_date, previous_cells)\n    next_cell_footprint = self.filter_cell_footprint(next_date, next_cells)\n\n    cell_footprint = previous_cell_footprint.union(current_cell_footprint).union(next_cell_footprint)\n\n    # We only use geometries to calculate distances, so we can use any origin for our 4-byte grid ID.\n    cell_footprint = self.grid_gen.grid_ids_to_grid_centroids(\n        cell_footprint.withColumn(ColNames.origin, F.lit(0).cast(LongType())),\n        grid_resolution=100,\n    ).drop(ColNames.origin)\n\n    if self.use_200m_grid:\n        self.logger.info(\"Using 200m grid for DPS calculation\")\n        cell_footprint = self.grid_gen.grid_id_to_coarser_resolution(\n            sdf=cell_footprint, coarse_resolution=200, coarse_grid_id_col=ColNames.grid_id\n        )\n\n    cell_footprint = cell_footprint.groupBy([ColNames.cell_id, ColNames.year, ColNames.month, ColNames.day]).agg(\n        F.collect_list(ColNames.geometry).alias(ColNames.geometry),\n        F.collect_set(ColNames.grid_id).alias(\"grid_ids\"),\n    )\n\n    cell_footprint = cell_footprint.withColumn(\n        ColNames.geometry,\n        STF.ST_ConcaveHull(STF.ST_Collect(F.col(ColNames.geometry)), 0.8),\n    )\n\n    # Load class attributes\n    self.events = events\n\n    # TODO: This is questionable, use with care\n    if self.broadcast_footprints:\n        cell_footprint = F.broadcast(cell_footprint)\n\n    self.cell_footprint = cell_footprint.persist(StorageLevel.MEMORY_AND_DISK)\n\n    # Load cell to group data\n    previous_cell_to_group = self.filter_cell_to_group(previous_date)\n    current_cell_to_group = self.filter_cell_to_group(current_date)\n    next_cell_to_group = self.filter_cell_to_group(next_date)\n\n    self.cell_to_group = previous_cell_to_group.union(current_cell_to_group).union(next_cell_to_group)\n\n    # Load group to tile data\n    previous_group_to_tile = self.filter_group_to_tile(previous_date)\n    current_group_to_tile = self.filter_group_to_tile(current_date)\n    next_group_to_tile = self.filter_group_to_tile(next_date)\n\n    self.group_to_tile = previous_group_to_tile.union(current_group_to_tile).union(next_group_to_tile)\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.build_time_slots_table","title":"<code>build_time_slots_table(current_date)</code>","text":"<p>Build a dataframe with the specified time slots for the current date.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>time slots dataframe.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def build_time_slots_table(self, current_date) -&gt; DataFrame:\n    \"\"\"\n    Build a dataframe with the specified time slots for the current date.\n\n    Returns:\n        DataFrame: time slots dataframe.\n    \"\"\"\n    time_slot_length = timedelta(days=1) / self.time_slot_number\n\n    time_slots_list = []\n    previous_end_time = datetime(\n        year=current_date.year,\n        month=current_date.month,\n        day=current_date.day,\n        hour=0,\n        minute=0,\n        second=0,\n    )\n\n    while previous_end_time.date() == current_date:\n        init_time = previous_end_time\n        end_time = init_time + time_slot_length\n        time_slot = (init_time, end_time)\n        time_slots_list.append(time_slot)\n        previous_end_time = end_time\n\n    schema = StructType(\n        [\n            StructField(ColNames.time_slot_initial_time, TimestampType(), True),\n            StructField(ColNames.time_slot_end_time, TimestampType(), True),\n        ]\n    )\n\n    self.time_slots = self.spark.createDataFrame(time_slots_list, schema=schema)\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.calculate_dps","title":"<code>calculate_dps(stay_intervals, cell_footprint)</code>","text":"<p>Calculate Daily Permanence Score (DPS) from user stay intervals.</p> <p>Processes stay intervals to determine user's presence in grid locations: - For multiple cell stays: explodes footprints and aggregates overlapping areas - For single cell stays: directly maps to corresponding grid IDs - For unknown locations: assigns special unknown identifier</p> <p>Parameters:</p> Name Type Description Default <code>stay_intervals</code> <code>DataFrame</code> <p>User stay intervals with duration and cell information</p> required <code>cell_footprint</code> <code>DataFrame</code> <p>Mapping between cells and their grid coverage areas</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Daily Permanence Score results with grid IDs and type identification</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def calculate_dps(self, stay_intervals: DataFrame, cell_footprint: DataFrame) -&gt; DataFrame:\n    \"\"\"Calculate Daily Permanence Score (DPS) from user stay intervals.\n\n    Processes stay intervals to determine user's presence in grid locations:\n    - For multiple cell stays: explodes footprints and aggregates overlapping areas\n    - For single cell stays: directly maps to corresponding grid IDs\n    - For unknown locations: assigns special unknown identifier\n\n    Args:\n        stay_intervals (DataFrame): User stay intervals with duration and cell information\n        cell_footprint (DataFrame): Mapping between cells and their grid coverage areas\n\n    Returns:\n        DataFrame: Daily Permanence Score results with grid IDs and type identification\n    \"\"\"\n\n    # ----------- These are time slots to which contribute stays in different cells ----------\n    # We will join these cells with the corresponding cell footprint intersection groups, which represent sets\n    # of tiles covered by a particular combination of cells. All of the tiles in one of these groups will have the\n    # exact same total stay duration, and thus the same DPS. We can then calculate the DPS for the cell footprint\n    # intersection group by itself and then assign it to each of the tiles that it contains.\n    local_stay_intervals = stay_intervals.filter(F.col(\"is_abroad\") == False)\n    stay_intervals_multiple_cells = local_stay_intervals.filter(F.col(\"distinct_cell_count\") &gt; 1)\n\n    # Join with cell-to-group reference table\n    stay_intervals_multiple_cells = stay_intervals_multiple_cells.join(\n        self.cell_to_group, on=[ColNames.year, ColNames.month, ColNames.day, ColNames.cell_id], how=\"left\"\n    )\n    stay_intervals_multiple_cells = stay_intervals_multiple_cells.drop(ColNames.cell_id)\n\n    stay_intervals_multiple_cells = (\n        stay_intervals_multiple_cells.groupBy(\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            ColNames.user_id_modulo,\n            ColNames.user_id,\n            ColNames.time_slot_initial_time,\n            ColNames.time_slot_end_time,\n            ColNames.group_id,\n        )\n        .agg(F.sum(ColNames.stay_duration).alias(ColNames.stay_duration))\n        .filter(\n            F.col(ColNames.stay_duration)\n            &gt;= F.lit(int((timedelta(days=1) / self.time_slot_number).total_seconds() / 2))\n        )\n    )\n\n    # We have now determined what cell footprint intersection groups have DPS = 1, which are exactly the rows\n    # we have kept after the filter. The next step is to join each group with its grid tiles, and collect them\n    # into an array of tile IDs, which will be the DPS column.\n    stay_intervals_multiple_cells = stay_intervals_multiple_cells.join(\n        self.group_to_tile, on=[ColNames.year, ColNames.month, ColNames.day, ColNames.group_id], how=\"left\"\n    ).drop(ColNames.group_id)\n\n    stay_intervals_multiple_cells = (\n        stay_intervals_multiple_cells.groupBy(\n            ColNames.user_id,\n            ColNames.time_slot_initial_time,\n            ColNames.time_slot_end_time,\n            ColNames.user_id_modulo,\n        )\n        .agg(F.collect_set(\"grid_id\").alias(ColNames.dps))\n        .select(\n            ColNames.user_id,\n            ColNames.dps,\n            ColNames.time_slot_initial_time,\n            ColNames.time_slot_end_time,\n            ColNames.user_id_modulo,\n            F.lit(UeGridIdType.GRID_STR).alias(ColNames.id_type),\n        )\n    )\n\n    # ------------ These are time slots to which contribute stay(s) in a single cell ----------\n    # Here there is no need to add up the stay duration in different cells, so the calculation of the DPS is\n    # straightforward and, if it surpasses the threshold, we just keep the cell footprint of the cell\n\n    # For intervals with single cell no need to explode, just join grid ids as is\n    stay_intervals_single_cell = local_stay_intervals.filter(\n        (F.col(\"distinct_cell_count\") == 1)\n        &amp; (F.col(ColNames.cell_id) != F.lit(UeGridIdType.UNKNOWN).cast(StringType()))\n    )\n    stay_intervals_single_cell = self.join_footprints(stay_intervals_single_cell, cell_footprint, [\"grid_ids\"])\n\n    stay_intervals_single_cell = stay_intervals_single_cell.select(\n        ColNames.user_id,\n        F.col(\"grid_ids\").alias(ColNames.dps),\n        ColNames.time_slot_initial_time,\n        ColNames.time_slot_end_time,\n        ColNames.user_id_modulo,\n        F.lit(UeGridIdType.GRID_STR).alias(ColNames.id_type),\n    )\n\n    uknown_intervals = local_stay_intervals.filter(F.col(ColNames.cell_id) == F.lit(UeGridIdType.UNKNOWN))\n\n    uknown_intervals = uknown_intervals.withColumn(\n        ColNames.dps,\n        F.array(F.lit(UeGridIdType.UNKNOWN).cast(IntegerType())),\n    ).select(\n        ColNames.user_id,\n        ColNames.dps,\n        ColNames.time_slot_initial_time,\n        ColNames.time_slot_end_time,\n        ColNames.user_id_modulo,\n        F.lit(UeGridIdType.UKNOWN_STR).alias(ColNames.id_type),\n    )\n\n    abroad_intervals = stay_intervals.filter(\n        (F.col(\"is_abroad\") == True)\n        &amp; (\n            F.col(ColNames.stay_duration)\n            &gt;= F.lit(int((timedelta(days=1) / self.time_slot_number).total_seconds() / 2))\n        )\n    )\n    abroad_intervals = abroad_intervals.withColumn(\n        ColNames.dps,\n        F.array(F.col(ColNames.cell_id).cast(IntegerType())),\n    ).select(\n        ColNames.user_id,\n        ColNames.dps,\n        ColNames.time_slot_initial_time,\n        ColNames.time_slot_end_time,\n        ColNames.user_id_modulo,\n        F.lit(UeGridIdType.ABROAD_STR).alias(ColNames.id_type),\n    )\n\n    dps = (\n        stay_intervals_single_cell.union(stay_intervals_multiple_cells)\n        .union(uknown_intervals)\n        .union(abroad_intervals)\n    )\n\n    return dps\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.calculate_time_slots_durations","title":"<code>calculate_time_slots_durations(stays, user_time_slots)</code>","text":"<p>Calculates duration of user stays within defined time slots.</p> <p>Joins stay records with time slot definitions and computes overlapping durations. For time slots with no stays, assigns a default duration of time_slot_number (1/24th day). Finally aggregates total stay duration per user, cell and time slot.</p> <p>Parameters:</p> Name Type Description Default <code>stays</code> <code>DataFrame</code> <p>User mobility stay records</p> required <code>user_time_slots</code> <code>DataFrame</code> <p>User time slot definitions</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Aggregated stay durations per time slot</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def calculate_time_slots_durations(self, stays: DataFrame, user_time_slots: DataFrame) -&gt; DataFrame:\n    \"\"\"Calculates duration of user stays within defined time slots.\n\n    Joins stay records with time slot definitions and computes overlapping durations.\n    For time slots with no stays, assigns a default duration of time_slot_number (1/24th day).\n    Finally aggregates total stay duration per user, cell and time slot.\n\n    Args:\n        stays (DataFrame): User mobility stay records\n        user_time_slots (DataFrame): User time slot definitions\n\n    Returns:\n        DataFrame: Aggregated stay durations per time slot\n    \"\"\"\n    stays = (\n        stays.join(\n            user_time_slots.select(\n                F.col(ColNames.user_id).alias(\"time_slot_user_id\"),\n                F.col(ColNames.user_id_modulo).alias(\"time_slot_user_id_modulo\"),\n                ColNames.time_slot_end_time,\n                ColNames.time_slot_initial_time,\n            ),\n            (\n                (F.col(\"init_time\") &lt; F.col(ColNames.time_slot_end_time))\n                &amp; (\n                    F.col(\"end_time\")\n                    &gt; F.col(\n                        ColNames.time_slot_initial_time,\n                    )\n                )\n                &amp; (F.col(ColNames.user_id) == F.col(\"time_slot_user_id\"))\n                &amp; (F.col(ColNames.user_id_modulo) == F.col(\"time_slot_user_id_modulo\"))\n            ),\n            how=\"right\",\n        )\n        .withColumn(\"init_time\", F.greatest(F.col(\"init_time\"), F.col(ColNames.time_slot_initial_time)))\n        .withColumn(\"end_time\", F.least(F.col(\"end_time\"), F.col(ColNames.time_slot_end_time)))\n        .withColumn(\n            ColNames.stay_duration,\n            F.when(\n                F.col(ColNames.cell_id).isNotNull(),\n                F.unix_timestamp(F.col(\"end_time\")) - F.unix_timestamp(F.col(\"init_time\")),\n            ).otherwise(F.lit((timedelta(days=1) / self.time_slot_number).total_seconds()).cast(IntegerType())),\n        )\n        .select(\n            F.coalesce(ColNames.user_id, \"time_slot_user_id\").alias(ColNames.user_id),\n            F.coalesce(ColNames.cell_id, F.lit(UeGridIdType.UNKNOWN)).alias(ColNames.cell_id),\n            ColNames.time_slot_initial_time,\n            ColNames.time_slot_end_time,\n            ColNames.stay_duration,\n            F.coalesce(F.col(\"is_abroad\"), F.lit(False)).alias(\"is_abroad\"),\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            F.coalesce(ColNames.user_id_modulo, \"time_slot_user_id_modulo\").alias(ColNames.user_id_modulo),\n        )\n    )\n\n    stays = stays.groupBy(\n        ColNames.user_id,\n        ColNames.cell_id,\n        ColNames.time_slot_initial_time,\n        ColNames.time_slot_end_time,\n        \"is_abroad\",\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n        ColNames.user_id_modulo,\n    ).agg((F.sum(ColNames.stay_duration).cast(FloatType()).alias(ColNames.stay_duration)))\n\n    return stays\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.check_needed_dates","title":"<code>check_needed_dates()</code>","text":"<p>Method that checks if both the dates of study and the dates necessary to generate the daily permanence scores are present in the input data (events + cell footprint).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there is no data for one or more of the needed dates.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def check_needed_dates(self):\n    \"\"\"\n    Method that checks if both the dates of study and the dates necessary to generate\n    the daily permanence scores are present in the input data (events + cell footprint).\n\n    Raises:\n        ValueError: If there is no data for one or more of the needed dates.\n    \"\"\"\n    # needed dates: for each date D, we also need D-1 and D+1\n    # this is built this way so it would also support definition of study\n    # dates that are not consecutive\n    needed_dates = (\n        {d + timedelta(days=1) for d in self.data_period_dates}\n        | set(self.data_period_dates)\n        | {d - timedelta(days=1) for d in self.data_period_dates}\n    )\n    self.logger.info(needed_dates)\n    # Assert needed dates in event data:\n    self.assert_needed_dates_events()\n\n    # Assert needed dates in cell footprint data:\n    self.assert_needed_dates_data_object(SilverCellFootprintDataObject.ID, needed_dates)\n\n    # Assert needed dates in cell to group data:\n    self.assert_needed_dates_data_object(SilverCellToGroupDataObject.ID, needed_dates)\n\n    # Assert needed dates in group to tile data:\n    self.assert_needed_dates_data_object(SilverGroupToTileDataObject.ID, needed_dates)\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.detect_move_events","title":"<code>detect_move_events(events)</code>","text":"<p>Detect which of the events are associated to moves according to the distances/times from previous to posterior event and a speed threshold.</p> <p>Parameters:</p> Name Type Description Default <code>events</code> <code>DataFrame</code> <p>events dataframe.</p> required <code>cell_footprint</code> <code>DataFrame</code> <p>cells footprint dataframe.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>events dataframe, with an additional 'is_move' boolean column.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def detect_move_events(self, events: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Detect which of the events are associated to moves according to the\n    distances/times from previous to posterior event and a speed threshold.\n\n    Args:\n        events (DataFrame): events dataframe.\n        cell_footprint (DataFrame): cells footprint dataframe.\n\n    Returns:\n        DataFrame: events dataframe, with an additional 'is_move' boolean column.\n    \"\"\"\n\n    window = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id).orderBy(ColNames.timestamp)\n    # Calculate distance between grid tiles associated to events -1, 0 and +1:\n    # Calculate speeds and determine which rows are moves:\n    events = (\n        events.withColumn(\n            \"dist_0_+1\",\n            F.when(\n                F.col(ColNames.geometry).isNotNull() &amp; F.col(f\"{ColNames.geometry}_+1\").isNotNull(),\n                STF.ST_Distance(F.col(ColNames.geometry), F.col(f\"{ColNames.geometry}_+1\")),\n            ).otherwise(F.lit(0)),\n        )\n        # .withColumn(\n        #     \"dist_-1_0\",\n        #     STF.ST_Distance(F.col(f\"{ColNames.geometry}_-1\"), F.col(ColNames.geometry)),\n        # )\n        .withColumn(  # repeating the distance calculation is not necessary, a lagged column works:\n            \"dist_-1_0\", F.lag(\"dist_0_+1\", 1).over(window)\n        )\n        .withColumn(\n            \"dist_-1_+1\",\n            F.when(\n                F.col(f\"{ColNames.geometry}_-1\").isNotNull() &amp; F.col(f\"{ColNames.geometry}_+1\").isNotNull(),\n                STF.ST_Distance(F.col(f\"{ColNames.geometry}_-1\"), F.col(f\"{ColNames.geometry}_+1\")),\n            ).otherwise(F.lit(0)),\n        )\n        .withColumn(\n            \"time_difference\",\n            F.unix_timestamp(events[f\"{ColNames.timestamp}_+1\"])\n            - F.unix_timestamp(events[f\"{ColNames.timestamp}_-1\"]),\n        )\n        .withColumn(\"max_dist\", F.greatest(F.col(\"dist_-1_0\") + F.col(\"dist_0_+1\"), F.col(\"dist_-1_+1\")))\n        .withColumn(\"speed\", F.col(\"max_dist\") / F.col(\"time_difference\"))\n        .withColumn(\"is_move\", F.when(F.col(\"speed\") &gt; self.max_speed_thresh, True).otherwise(False))\n        .drop(\n            \"dist_0_+1\",\n            \"dist_-1_0\",\n            \"dist_-1_+1\",\n            f\"{ColNames.geometry}_-1\",\n            f\"{ColNames.geometry}_+1\",\n            ColNames.geometry,\n            \"time_difference\",\n            \"max_dist\",\n            \"speed\",\n        )\n    )\n\n    return events\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.determine_stay_durations","title":"<code>determine_stay_durations(events)</code>","text":"<p>Determine the start time and end time for each stay event.</p> <p>Parameters:</p> Name Type Description Default <code>events</code> <code>DataFrame</code> <p>events dataframe.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>stays dataframe (filtering out moves).</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def determine_stay_durations(self, events: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Determine the start time and end time for each stay event.\n\n    Args:\n        events (DataFrame): events dataframe.\n\n    Returns:\n        DataFrame: stays dataframe (filtering out moves).\n    \"\"\"\n    current_datetime = datetime(self.current_date.year, self.current_date.month, self.current_date.day)\n    # TODO: night interval could be a config parameter instead of hardcoded!\n    night_start_time = current_datetime - timedelta(hours=1)\n    night_end_time = current_datetime + timedelta(hours=9)\n\n    stays = (\n        events\n        # Filter out 'move' events (keep only stays), and also drop unneeded columns:\n        .filter(F.col(\"is_move\") == False)\n        # Set applicable time thresholds:\n        # if prev, next and current events:\n        # - if the event is in the same cell as the previous one, and the previous event is within the night interval,\n        # - set the threshold to max_time_thresh_night, otherwise set it to max_time_thresh_day\n        # - if the event is not in the same cell as the previous one, set the threshold to max_time_thresh\n        # if prev or next event is missing and current event is abroad:\n        # - set missing timestamp to current timestamp +/- max_time_thresh_abroad\n        .withColumn(\n            \"threshold_-1\",\n            F.when(\n                (F.col(\"cell_id\") == F.col(\"cell_id_-1\"))\n                &amp; (F.col(\"timestamp\") &gt;= night_start_time)\n                &amp; (F.col(\"timestamp_-1\") &lt;= night_end_time),\n                self.max_time_thresh_night,\n            ).otherwise(\n                F.when(F.col(\"cell_id\") == F.col(\"cell_id_-1\"), self.max_time_thresh_day).otherwise(\n                    self.max_time_thresh\n                )\n            ),\n        )\n        .withColumn(\n            \"threshold_+1\",\n            F.when(\n                (F.col(\"cell_id\") == F.col(\"cell_id_+1\"))\n                &amp; (F.col(\"timestamp\") &gt;= night_start_time)\n                &amp; (F.col(\"timestamp_+1\") &lt;= night_end_time),\n                self.max_time_thresh_night,\n            ).otherwise(\n                F.when(F.col(\"cell_id\") == F.col(\"cell_id_+1\"), self.max_time_thresh_day).otherwise(\n                    self.max_time_thresh\n                )\n            ),\n        )\n        # Calculate init_time and end_time according to thresholds and time differences between events:\n        .withColumn(\n            \"init_time\",\n            F.when(\n                (F.col(f\"{ColNames.timestamp}_-1\").isNull()) &amp; (F.col(\"is_abroad\") == True),\n                F.col(ColNames.timestamp) - self.max_time_thresh_abroad,\n            )\n            .when(\n                F.col(ColNames.timestamp) - F.col(f\"{ColNames.timestamp}_-1\") &lt;= F.col(\"threshold_-1\"),\n                F.col(ColNames.timestamp) - (F.col(ColNames.timestamp) - F.col(f\"{ColNames.timestamp}_-1\")) / 2,\n            )\n            .otherwise(F.col(ColNames.timestamp) - self.max_time_thresh / 2),\n        )\n        .withColumn(\n            \"end_time\",\n            F.when(\n                (F.col(f\"{ColNames.timestamp}_+1\").isNull()) &amp; (F.col(\"is_abroad\") == True),\n                F.col(ColNames.timestamp) + self.max_time_thresh_abroad,\n            )\n            .when(\n                F.col(f\"{ColNames.timestamp}_+1\") - F.col(ColNames.timestamp) &lt;= F.col(\"threshold_+1\"),\n                F.col(f\"{ColNames.timestamp}_+1\")\n                - (F.col(f\"{ColNames.timestamp}_+1\") - F.col(ColNames.timestamp)) / 2,\n            )\n            .otherwise(F.col(ColNames.timestamp) + self.max_time_thresh / 2),\n        )\n        .drop(\n            f\"{ColNames.cell_id}_-1\",\n            f\"{ColNames.cell_id}_+1\",\n            ColNames.timestamp,\n            f\"{ColNames.timestamp}_-1\",\n            f\"{ColNames.timestamp}_+1\",\n            ColNames.mcc,\n            \"is_move\",\n            \"threshold_-1\",\n            \"threshold_+1\",\n        )\n    )\n\n    return stays\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.enrich_events","title":"<code>enrich_events(events, cell_footprint)</code>","text":"<p>Enrich events with additional information</p> <p>Parameters:</p> Name Type Description Default <code>events</code> <code>DataFrame</code> <p>events dataframe.</p> required <code>cell_footprint</code> <code>DataFrame</code> <p>cells footprint dataframe.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>enriched events dataframe.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def enrich_events(self, events: DataFrame, cell_footprint: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Enrich events with additional information\n\n    Args:\n        events (DataFrame): events dataframe.\n        cell_footprint (DataFrame): cells footprint dataframe.\n\n    Returns:\n        DataFrame: enriched events dataframe.\n    \"\"\"\n\n    events = self.join_footprints(events, cell_footprint, [ColNames.geometry])\n    # add abroad flag\n    events = (\n        events.withColumn(\"abroad_mcc\", F.substring(F.col(ColNames.plmn), 0, 3).cast(IntegerType()))\n        .withColumn(\n            \"is_abroad\",\n            F.when(\n                (F.col(ColNames.plmn).isNotNull()) &amp; (F.col(\"abroad_mcc\") != self.local_mcc), F.lit(True)\n            ).otherwise(F.lit(False)),\n        )\n        .withColumn(  # add abroad country mcc as cell_id for abroad events\n            ColNames.cell_id,\n            F.when(F.col(\"is_abroad\") == True, F.col(\"abroad_mcc\")).otherwise(F.col(ColNames.cell_id)),\n        )\n        .drop(\"abroad_mcc\", ColNames.plmn)\n    )\n    # Add lags of timestamp, cell_id:\n    window = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id).orderBy(ColNames.timestamp)\n    lag_fields = [ColNames.timestamp, ColNames.cell_id, ColNames.geometry]\n    for lf in lag_fields:\n        events = events.withColumn(f\"{lf}_+1\", F.lag(lf, -1).over(window)).withColumn(\n            f\"{lf}_-1\", F.lag(lf, 1).over(window)\n        )\n\n    return events\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.filter_cell_footprint","title":"<code>filter_cell_footprint(current_date, cells=None)</code>","text":"<p>Load cell footprints for a specific date.</p> <p>Parameters:</p> Name Type Description Default <code>current_date</code> <code>date</code> <p>current date.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>filtered cell footprint dataframe.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def filter_cell_footprint(self, current_date: date, cells: DataFrame = None) -&gt; DataFrame:\n    \"\"\"\n    Load cell footprints for a specific date.\n\n    Args:\n        current_date (date): current date.\n\n    Returns:\n        DataFrame: filtered cell footprint dataframe.\n    \"\"\"\n    df = self.input_data_objects[SilverCellFootprintDataObject.ID].df.filter(\n        (F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(current_date))\n    )\n\n    if cells is not None:\n        df = df.join(cells, ColNames.cell_id, \"inner\")\n\n    return df\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.filter_cell_to_group","title":"<code>filter_cell_to_group(current_date)</code>","text":"<p>Loads cell to group reference table for a specific date.</p> <p>Parameters:</p> Name Type Description Default <code>current_date</code> <code>date</code> <p>current date</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>filtered cell to group dataframe</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def filter_cell_to_group(self, current_date: date) -&gt; DataFrame:\n    \"\"\"\n    Loads cell to group reference table for a specific date.\n\n    Args:\n        current_date (date): current date\n\n    Returns:\n        DataFrame: filtered cell to group dataframe\n    \"\"\"\n    df = self.input_data_objects[SilverCellToGroupDataObject.ID].df.filter(\n        (F.col(ColNames.year) == F.lit(current_date.year))\n        &amp; (F.col(ColNames.month) == F.lit(current_date.month))\n        &amp; (F.col(ColNames.day) == F.lit(current_date.day))\n    )\n\n    return df\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.filter_events","title":"<code>filter_events(current_date, partition_chunk=None)</code>","text":"<p>Load events with no errors for a specific date.</p> <p>Parameters:</p> Name Type Description Default <code>current_date</code> <code>date</code> <p>current date.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>filtered events dataframe.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def filter_events(self, current_date: date, partition_chunk=None) -&gt; DataFrame:\n    \"\"\"\n    Load events with no errors for a specific date.\n\n    Args:\n        current_date (date): current date.\n\n    Returns:\n        DataFrame: filtered events dataframe.\n    \"\"\"\n    df = (\n        self.input_data_objects[SilverEventFlaggedDataObject.ID]\n        .df.filter(\n            (F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(current_date))\n            &amp; (F.col(ColNames.error_flag).isin(self.event_error_flags_to_include))\n        )\n        .select(\n            ColNames.user_id,\n            ColNames.cell_id,\n            ColNames.timestamp,\n            ColNames.plmn,\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            ColNames.user_id_modulo,\n        )\n    )\n\n    if partition_chunk is not None:\n        df = df.filter(F.col(ColNames.user_id_modulo).isin(partition_chunk))\n\n    return df\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.filter_group_to_tile","title":"<code>filter_group_to_tile(current_date)</code>","text":"<p>Loads group to tile reference table for a specific date.</p> <p>Parameters:</p> Name Type Description Default <code>current_date</code> <code>date</code> <p>current date</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>filtered group to tile dataframe</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def filter_group_to_tile(self, current_date: date) -&gt; DataFrame:\n    \"\"\"\n    Loads group to tile reference table for a specific date.\n\n    Args:\n        current_date (date): current date\n\n    Returns:\n        DataFrame: filtered group to tile dataframe\n    \"\"\"\n    df = self.input_data_objects[SilverGroupToTileDataObject.ID].df.filter(\n        (F.col(ColNames.year) == F.lit(current_date.year))\n        &amp; (F.col(ColNames.month) == F.lit(current_date.month))\n        &amp; (F.col(ColNames.day) == F.lit(current_date.day))\n    )\n\n    return df\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.filter_zero_dps_durations","title":"<code>filter_zero_dps_durations(stays)</code>","text":"<p>Filters out stay records that would result in zero Daily Permanence Score.</p> <p>Removes records where the total stay duration is less than half of a time slot's duration (1/24th of a day). This pre-filtering step optimizes performance by eliminating records that would not contribute to the final score.</p> <p>Parameters:</p> Name Type Description Default <code>stays</code> <code>DataFrame</code> <p>DataFrame containing stay records with durations</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered stay records above minimum duration threshold</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def filter_zero_dps_durations(self, stays: DataFrame) -&gt; DataFrame:\n    \"\"\"Filters out stay records that would result in zero Daily Permanence Score.\n\n    Removes records where the total stay duration is less than half of a time slot's\n    duration (1/24th of a day). This pre-filtering step optimizes performance by\n    eliminating records that would not contribute to the final score.\n\n    Args:\n        stays (DataFrame): DataFrame containing stay records with durations\n\n    Returns:\n        DataFrame: Filtered stay records above minimum duration threshold\n    \"\"\"\n    window_spec = Window.partitionBy(\n        ColNames.user_id,\n        ColNames.user_id_modulo,\n        ColNames.time_slot_initial_time,\n    )\n\n    # Count distinct cell_id within each window\n    stays = stays.withColumn(\n        \"distinct_cell_count\", F.size(F.collect_set(ColNames.cell_id).over(window_spec))\n    ).withColumn(\"duration_sum\", F.sum(ColNames.stay_duration).over(window_spec))\n\n    # remove all records where DPS will be 0\n    stays = stays.filter(\n        F.col(\"duration_sum\") &gt;= F.lit(int((timedelta(days=1) / self.time_slot_number).total_seconds() / 2))\n    ).drop(\"duration_sum\")\n\n    return stays\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.get_cache_events","title":"<code>get_cache_events(current_date, partition_chunk=None, last_event=True)</code>","text":"<p>Load cache events with for a specific date.</p> <p>Parameters:</p> Name Type Description Default <code>current_date</code> <code>date</code> <p>current date.</p> required <code>last_event</code> <code>bool</code> <p>flag to get last event or first.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>filtered events dataframe.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def get_cache_events(self, current_date: date, partition_chunk=None, last_event: bool = True) -&gt; DataFrame:\n    \"\"\"\n    Load cache events with for a specific date.\n\n    Args:\n        current_date (date): current date.\n        last_event (bool): flag to get last event or first.\n\n    Returns:\n        DataFrame: filtered events dataframe.\n    \"\"\"\n    df = (\n        self.input_data_objects[EventCacheDataObject.ID]\n        .df.filter(\n            (F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(current_date))\n        )\n        .filter(F.col(ColNames.is_last_event) == last_event)\n        .drop(ColNames.is_last_event)\n    ).select(\n        ColNames.user_id,\n        ColNames.cell_id,\n        ColNames.timestamp,\n        ColNames.plmn,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n        ColNames.user_id_modulo,\n    )\n\n    if partition_chunk is not None:\n        df = df.filter(F.col(ColNames.user_id_modulo).isin(partition_chunk))\n\n    return df\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.join_footprints","title":"<code>join_footprints(events, cell_footprints, columns)</code>","text":"<p>Join the events dataframe with the cell_footprint dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>events</code> <code>DataFrame</code> <p>events dataframe.</p> required <code>cell_footprint</code> <code>DataFrame</code> <p>cell_footprint dataframe.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>events dataframe with the cell footprint information.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def join_footprints(self, events: DataFrame, cell_footprints: DataFrame, columns: List) -&gt; DataFrame:\n    \"\"\"\n    Join the events dataframe with the cell_footprint dataframe.\n\n    Args:\n        events (DataFrame): events dataframe.\n        cell_footprint (DataFrame): cell_footprint dataframe.\n\n    Returns:\n        DataFrame: events dataframe with the cell footprint information.\n    \"\"\"\n    events = events.join(\n        cell_footprints.select(\n            F.col(ColNames.cell_id).alias(\"footprints_cell_id\"),\n            F.col(ColNames.year).alias(\"footprints_year\"),\n            F.col(ColNames.month).alias(\"footprints_month\"),\n            F.col(ColNames.day).alias(\"footprints_day\"),\n            *columns,\n        ),\n        (F.col(ColNames.cell_id) == F.col(\"footprints_cell_id\"))\n        &amp; (F.col(ColNames.year) == F.col(\"footprints_year\"))\n        &amp; (F.col(ColNames.month) == F.col(\"footprints_month\"))\n        &amp; (F.col(ColNames.day) == F.col(\"footprints_day\")),\n        \"left\",\n    ).drop(\n        \"footprints_cell_id\",\n        \"footprints_year\",\n        \"footprints_month\",\n        \"footprints_day\",\n    )\n\n    return events\n</code></pre>"},{"location":"reference/components/execution/device_activity_statistics/","title":"device_activity_statistics","text":""},{"location":"reference/components/execution/device_activity_statistics/device_activity_statistics/","title":"device_activity_statistics","text":"<p>Module that cleans RAW MNO Event data.</p>"},{"location":"reference/components/execution/device_activity_statistics/device_activity_statistics/#components.execution.device_activity_statistics.device_activity_statistics.DeviceActivityStatistics","title":"<code>DeviceActivityStatistics</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class that removes duplicates from clean MNO Event data</p> Source code in <code>multimno/components/execution/device_activity_statistics/device_activity_statistics.py</code> <pre><code>class DeviceActivityStatistics(Component):\n    \"\"\"\n    Class that removes duplicates from clean MNO Event data\n    \"\"\"\n\n    COMPONENT_ID = \"DeviceActivityStatistics\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n        self.current_date = None\n        self.current_input_events = None\n        self.current_input_network = None\n        self.statistics_df = None\n\n    def initalize_data_objects(self):\n        # Input\n        # TODO: update this to semantically cleaned files after merge\n        self.input_events_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"event_data_silver\")\n        # TODO: Figure out how this would work with coverage areas. We don't have location of cells in those cases\n        self.input_topology_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"network_data_silver\")\n        self.output_statistics_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"device_activity_statistics\")\n\n        self.data_period_start = self.config.get(DeviceActivityStatistics.COMPONENT_ID, \"data_period_start\")\n        self.data_period_end = self.config.get(DeviceActivityStatistics.COMPONENT_ID, \"data_period_end\")\n        self.clear_destination_directory = self.config.get(\n            DeviceActivityStatistics.COMPONENT_ID, \"clear_destination_directory\"\n        )\n        self.local_timezone_str = self.config.get(GENERAL_CONFIG_KEY, \"local_timezone\")\n\n        # Create all possible dates between start and end\n        sdate = pd.to_datetime(self.data_period_start)\n        edate = pd.to_datetime(self.data_period_end)\n        self.to_process_dates = list(pd.date_range(sdate, edate, freq=\"d\"))\n\n        # Create all input data objects\n        self.input_data_objects = {SilverEventDataObject.ID: None}\n        if check_if_data_path_exists(self.spark, self.input_events_path):\n            # Single input option, with a filter follwed by it\n            self.input_data_objects[SilverEventDataObject.ID] = SilverEventDataObject(\n                self.spark, self.input_events_path\n            )\n        else:\n            self.logger.warning(f\"Expected path {self.input_events_path} to exist but it does not\")\n\n        self.input_data_objects[SilverNetworkDataObject.ID] = None\n\n        if check_if_data_path_exists(self.spark, self.input_topology_path):\n            # Single input option, with a filter follwed by it\n            self.input_data_objects[SilverNetworkDataObject.ID] = SilverNetworkDataObject(\n                self.spark, self.input_topology_path\n            )\n        else:\n            self.logger.warning(f\"Expected path {self.input_topology_path} to exist but it does not\")\n\n        # Output data objects dictionary\n        self.output_data_objects = {}\n        self.output_data_objects[SilverDeviceActivityStatistics.ID] = SilverDeviceActivityStatistics(\n            self.spark, self.output_statistics_path\n        )\n\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, self.output_statistics_path)\n\n        # Create timezones for transformations\n        self.local_tz = pytz.timezone(self.local_timezone_str)\n        self.utc_tz = pytz.timezone(\"UTC\")\n\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n\n        self.read()\n\n        for current_date in self.to_process_dates:\n            self.current_date = current_date\n\n            start = datetime(year=current_date.year, month=current_date.month, day=current_date.day)\n            start_utc = self.local_tz.localize(start).astimezone(self.utc_tz)\n            end_utc = start_utc + timedelta(days=1) - timedelta(seconds=1)\n\n            # TODO: should this selection also be based on user_id_modulo?\n            self.current_input_events = self.input_data_objects[SilverEventDataObject.ID].df.filter(\n                (F.col(ColNames.timestamp).between(start_utc, end_utc))\n            )\n\n            self.current_input_network = self.input_data_objects[SilverNetworkDataObject.ID].df.filter(\n                (\n                    (F.col(ColNames.year) == current_date.year)\n                    &amp; (F.col(ColNames.month) == current_date.month)\n                    &amp; (F.col(ColNames.day) == current_date.day)\n                )\n            )\n\n            self.transform()\n\n            self.write()\n\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n        # Prepare everything for metrics calculations\n        df_events = self.preprocess_events(self.current_input_events, self.current_input_network)\n\n        # Calculate count of events per user\n        self.statistics_df = df_events.groupby(ColNames.user_id).count().withColumnRenamed(\"count\", ColNames.event_cnt)\n\n        # Calculate count of unique cells per user\n        unique_cell_counts = (\n            df_events.groupBy(ColNames.user_id)\n            .agg(F.countDistinct(ColNames.cell_id))\n            .withColumnRenamed(f\"count(DISTINCT {ColNames.cell_id})\", ColNames.unique_cell_cnt)\n        )\n        self.statistics_df = self.statistics_df.join(unique_cell_counts, on=\"user_id\")\n        # Calculate count of unique locations per user\n        unique_location_counts = df_events.groupBy(ColNames.user_id).agg(\n            F.countDistinct(ColNames.latitude, ColNames.longitude).alias(ColNames.unique_location_cnt)\n        )\n        self.statistics_df = self.statistics_df.join(unique_location_counts, on=\"user_id\")\n\n        # Calculate sum of distances between cells\n        distance_per_user = df_events.groupBy(ColNames.user_id).agg(\n            F.sum(\"distance\").cast(IntegerType()).alias(ColNames.sum_distance_m)\n        )\n        self.statistics_df = self.statistics_df.join(distance_per_user, on=\"user_id\")\n\n        # Calculate number of unique hours in data per user\n        hourly_events_df = df_events.withColumn(\n            ColNames.timestamp, F.date_trunc(\"hour\", F.col(ColNames.timestamp))\n        ).select([ColNames.user_id, ColNames.timestamp])\n        hourly_counts = hourly_events_df.groupBy(ColNames.user_id).agg(\n            F.countDistinct(ColNames.timestamp).alias(ColNames.unique_hour_cnt)\n        )\n        self.statistics_df = self.statistics_df.join(hourly_counts, on=\"user_id\")\n\n        # mean_time_gap\n        mean_time_gaps = df_events.groupBy(ColNames.user_id).agg(\n            F.mean(\"time_gap_s\").cast(IntegerType()).alias(ColNames.mean_time_gap)\n        )\n        self.statistics_df = self.statistics_df.join(mean_time_gaps, on=\"user_id\")\n\n        # stdev_time_gap\n        stddev_time_gaps = df_events.groupBy(ColNames.user_id).agg(\n            F.stddev(\"time_gap_s\").alias(ColNames.stdev_time_gap)\n        )\n        self.statistics_df = self.statistics_df.join(stddev_time_gaps, on=\"user_id\")\n        # Add date to statistics\n        self.statistics_df = self.statistics_df.withColumns(\n            {\n                ColNames.year: F.lit(self.current_date.year).cast(\"smallint\"),\n                ColNames.month: F.lit(self.current_date.month).cast(\"tinyint\"),\n                ColNames.day: F.lit(self.current_date.day).cast(\"tinyint\"),\n            }\n        )\n        # Reorder rows\n        self.statistics_df = self.statistics_df.select([col.name for col in SilverDeviceActivityStatistics.SCHEMA])\n        for col in SilverDeviceActivityStatistics.SCHEMA:\n            self.statistics_df = self.statistics_df.withColumn(\n                col.name, self.statistics_df[col.name].cast(col.dataType)\n            )\n        self.output_data_objects[SilverDeviceActivityStatistics.ID].df = self.statistics_df\n\n        # after each chunk processing clear all Cache to free memory and disk\n        self.spark.catalog.clearCache()\n\n    def preprocess_events(\n        self,\n        df_events: pyspark.sql.dataframe.DataFrame,\n        df_network: pyspark.sql.dataframe.DataFrame,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Preprocesses events dataframe to be able to calculate all metrics. Steps:\n        1. Converts timestamp from UTC to local\n        2. Fills latitude and longitude columns from the location of the cell\n        3. Gets location of next event for each event\n        4. Gets timestamp of next event for each event\n        5. Calculates time gap to next event\n        6. Calculates distance to next event\n\n        Args:\n            df_events (pyspark.sql.dataframe.DataFrame): Events relating to the day that is currently being processed\n            df_network (pyspark.sql.dataframe.DataFrame): Network relating to the day that is currently being processed\n\n        Returns:\n            df_events: Events relating to the day that is currently being processed with\n                extra columns for metric calculation\n        \"\"\"\n\n        # Convert timestamp to local\n        df_events.withColumn(\n            ColNames.timestamp,\n            F.to_utc_timestamp(F.from_utc_timestamp(ColNames.timestamp, self.local_timezone_str), \"UTC\"),\n        )\n        df_events = df_events.withColumns(\n            {\n                ColNames.year: F.year(ColNames.timestamp).cast(\"smallint\"),\n                ColNames.month: F.month(ColNames.timestamp).cast(\"tinyint\"),\n                ColNames.day: F.dayofmonth(ColNames.timestamp).cast(\"tinyint\"),\n            }\n        )\n\n        # Join events with topology data to enable checking unique locations and travelled distances\n        df_events = df_events.join(\n            df_network.select(\n                [\n                    F.col(ColNames.cell_id),\n                    F.col(ColNames.latitude).alias(\"cell_lat\"),\n                    F.col(ColNames.longitude).alias(\"cell_lon\"),\n                ]\n            ),\n            on=ColNames.cell_id,\n            how=\"left\",\n        )\n\n        # Use latitude and longitude if they exist, otherwise use cells location\n        df_events = df_events.withColumn(ColNames.latitude, F.coalesce(ColNames.latitude, \"cell_lat\"))\n        df_events = df_events.withColumn(ColNames.longitude, F.coalesce(ColNames.longitude, \"cell_lon\"))\n\n        # Add timestamp and location of next record\n        window = Window.partitionBy(*[ColNames.user_id_modulo, ColNames.user_id]).orderBy(ColNames.timestamp)\n        df_events = df_events.withColumn(\n            f\"next_{ColNames.timestamp}\",\n            F.lead(F.col(ColNames.timestamp), 1).over(window),\n        )\n        df_events = df_events.withColumn(\n            f\"next_{ColNames.latitude}\",\n            F.lead(F.col(ColNames.latitude), 1).over(window),\n        )\n        df_events = df_events.withColumn(\n            f\"next_{ColNames.longitude}\",\n            F.lead(F.col(ColNames.longitude), 1).over(window),\n        )\n\n        # Calculate time gap to next event\n        df_events = df_events.withColumn(\n            \"time_gap_s\",\n            F.unix_timestamp(f\"next_{ColNames.timestamp}\") - F.unix_timestamp(ColNames.timestamp),\n        )\n\n        # Calculate the distance between current and next event\n        # TODO: check if this is the correct way to calculate, got varying results\n        # There are many ways to calculate distance between points and all of them give different results\n        df_events = df_events.withColumn(\n            \"source_geom\",\n            STF.ST_SetSRID(\n                STC.ST_Point(df_events[ColNames.latitude], df_events[ColNames.longitude]),\n                4326,\n            ),\n        )\n        df_events = df_events.withColumn(\n            \"destination_geom\",\n            STF.ST_SetSRID(\n                STC.ST_Point(\n                    df_events[f\"next_{ColNames.latitude}\"],\n                    df_events[f\"next_{ColNames.longitude}\"],\n                ),\n                4326,\n            ),\n        )\n        df_events = df_events.withColumn(\n            \"distance\",\n            STF.ST_DistanceSphere(df_events[\"source_geom\"], df_events[\"destination_geom\"]),\n        )\n\n        return df_events\n</code></pre>"},{"location":"reference/components/execution/device_activity_statistics/device_activity_statistics/#components.execution.device_activity_statistics.device_activity_statistics.DeviceActivityStatistics.preprocess_events","title":"<code>preprocess_events(df_events, df_network)</code>","text":"<p>Preprocesses events dataframe to be able to calculate all metrics. Steps: 1. Converts timestamp from UTC to local 2. Fills latitude and longitude columns from the location of the cell 3. Gets location of next event for each event 4. Gets timestamp of next event for each event 5. Calculates time gap to next event 6. Calculates distance to next event</p> <p>Parameters:</p> Name Type Description Default <code>df_events</code> <code>DataFrame</code> <p>Events relating to the day that is currently being processed</p> required <code>df_network</code> <code>DataFrame</code> <p>Network relating to the day that is currently being processed</p> required <p>Returns:</p> Name Type Description <code>df_events</code> <code>DataFrame</code> <p>Events relating to the day that is currently being processed with extra columns for metric calculation</p> Source code in <code>multimno/components/execution/device_activity_statistics/device_activity_statistics.py</code> <pre><code>def preprocess_events(\n    self,\n    df_events: pyspark.sql.dataframe.DataFrame,\n    df_network: pyspark.sql.dataframe.DataFrame,\n) -&gt; DataFrame:\n    \"\"\"\n    Preprocesses events dataframe to be able to calculate all metrics. Steps:\n    1. Converts timestamp from UTC to local\n    2. Fills latitude and longitude columns from the location of the cell\n    3. Gets location of next event for each event\n    4. Gets timestamp of next event for each event\n    5. Calculates time gap to next event\n    6. Calculates distance to next event\n\n    Args:\n        df_events (pyspark.sql.dataframe.DataFrame): Events relating to the day that is currently being processed\n        df_network (pyspark.sql.dataframe.DataFrame): Network relating to the day that is currently being processed\n\n    Returns:\n        df_events: Events relating to the day that is currently being processed with\n            extra columns for metric calculation\n    \"\"\"\n\n    # Convert timestamp to local\n    df_events.withColumn(\n        ColNames.timestamp,\n        F.to_utc_timestamp(F.from_utc_timestamp(ColNames.timestamp, self.local_timezone_str), \"UTC\"),\n    )\n    df_events = df_events.withColumns(\n        {\n            ColNames.year: F.year(ColNames.timestamp).cast(\"smallint\"),\n            ColNames.month: F.month(ColNames.timestamp).cast(\"tinyint\"),\n            ColNames.day: F.dayofmonth(ColNames.timestamp).cast(\"tinyint\"),\n        }\n    )\n\n    # Join events with topology data to enable checking unique locations and travelled distances\n    df_events = df_events.join(\n        df_network.select(\n            [\n                F.col(ColNames.cell_id),\n                F.col(ColNames.latitude).alias(\"cell_lat\"),\n                F.col(ColNames.longitude).alias(\"cell_lon\"),\n            ]\n        ),\n        on=ColNames.cell_id,\n        how=\"left\",\n    )\n\n    # Use latitude and longitude if they exist, otherwise use cells location\n    df_events = df_events.withColumn(ColNames.latitude, F.coalesce(ColNames.latitude, \"cell_lat\"))\n    df_events = df_events.withColumn(ColNames.longitude, F.coalesce(ColNames.longitude, \"cell_lon\"))\n\n    # Add timestamp and location of next record\n    window = Window.partitionBy(*[ColNames.user_id_modulo, ColNames.user_id]).orderBy(ColNames.timestamp)\n    df_events = df_events.withColumn(\n        f\"next_{ColNames.timestamp}\",\n        F.lead(F.col(ColNames.timestamp), 1).over(window),\n    )\n    df_events = df_events.withColumn(\n        f\"next_{ColNames.latitude}\",\n        F.lead(F.col(ColNames.latitude), 1).over(window),\n    )\n    df_events = df_events.withColumn(\n        f\"next_{ColNames.longitude}\",\n        F.lead(F.col(ColNames.longitude), 1).over(window),\n    )\n\n    # Calculate time gap to next event\n    df_events = df_events.withColumn(\n        \"time_gap_s\",\n        F.unix_timestamp(f\"next_{ColNames.timestamp}\") - F.unix_timestamp(ColNames.timestamp),\n    )\n\n    # Calculate the distance between current and next event\n    # TODO: check if this is the correct way to calculate, got varying results\n    # There are many ways to calculate distance between points and all of them give different results\n    df_events = df_events.withColumn(\n        \"source_geom\",\n        STF.ST_SetSRID(\n            STC.ST_Point(df_events[ColNames.latitude], df_events[ColNames.longitude]),\n            4326,\n        ),\n    )\n    df_events = df_events.withColumn(\n        \"destination_geom\",\n        STF.ST_SetSRID(\n            STC.ST_Point(\n                df_events[f\"next_{ColNames.latitude}\"],\n                df_events[f\"next_{ColNames.longitude}\"],\n            ),\n            4326,\n        ),\n    )\n    df_events = df_events.withColumn(\n        \"distance\",\n        STF.ST_DistanceSphere(df_events[\"source_geom\"], df_events[\"destination_geom\"]),\n    )\n\n    return df_events\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/","title":"event_cleaning","text":""},{"location":"reference/components/execution/event_cleaning/event_cleaning/","title":"event_cleaning","text":"<p>Module that cleans RAW MNO Event data.</p>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning","title":"<code>EventCleaning</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class that cleans MNO Event data</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>class EventCleaning(Component):\n    \"\"\"\n    Class that cleans MNO Event data\n    \"\"\"\n\n    COMPONENT_ID = \"EventCleaning\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.do_same_location_deduplication = self.config.getboolean(\n            self.COMPONENT_ID,\n            \"do_same_location_deduplication\",\n            fallback=False,\n        )\n\n        self.do_bounding_box_filtering = self.config.getboolean(\n            self.COMPONENT_ID,\n            \"do_bounding_box_filtering\",\n            fallback=False,\n        )\n\n        self.do_cell_id_length_filtering = self.config.getboolean(\n            self.COMPONENT_ID,\n            \"do_cell_id_length_filtering\",\n            fallback=False,\n        )\n\n        self.do_timestamp_conversion_to_utc = self.config.getboolean(\n            self.COMPONENT_ID,\n            \"do_timestamp_conversion_to_utc\",\n            fallback=False,\n        )\n\n        self.timestamp_format = self.config.get(self.COMPONENT_ID, \"timestamp_format\")\n        self.local_timezone = self.config.get(GENERAL_CONFIG_KEY, \"local_timezone\")\n        self.local_mcc = self.config.getint(GENERAL_CONFIG_KEY, \"local_mcc\")\n        self.bbox = self.config.geteval(self.COMPONENT_ID, \"bounding_box\")\n\n    def initalize_data_objects(self):\n        # Input\n        self.bronze_event_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"event_data_bronze\")\n\n        self.clear_destination_directory = self.config.getboolean(self.COMPONENT_ID, \"clear_destination_directory\")\n        self.number_of_partitions = self.config.get(self.COMPONENT_ID, \"number_of_partitions\")\n\n        self.data_period_start = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n        self.data_period_end = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n\n        # List of datetime.dates for which to perform cleaning of the raw network input data.\n        # Notice that date_period_end is included\n        self.data_period_dates = [\n            self.data_period_start + datetime.timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n        # Create all input data objects\n        self.input_event_data_objects = []\n        self.dates_to_process = []\n        for date in self.data_period_dates:\n            path = f\"{self.bronze_event_path}/year={date.year}/month={date.month}/day={date.day}\"\n            if check_if_data_path_exists(self.spark, path):\n                self.dates_to_process.append(date)\n                self.input_event_data_objects.append(BronzeEventDataObject(self.spark, path))\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")\n\n        # Output\n        self.output_data_objects = {}\n\n        outputs = {\n            \"event_syntactic_quality_metrics_by_column\": SilverEventDataSyntacticQualityMetricsByColumn,\n            \"event_syntactic_quality_metrics_frequency_distribution\": SilverEventDataSyntacticQualityMetricsFrequencyDistribution,\n            \"event_data_silver\": SilverEventDataObject,\n        }\n\n        for key, value in outputs.items():\n            path = self.config.get(CONFIG_SILVER_PATHS_KEY, key)\n            if self.clear_destination_directory:\n                delete_file_or_folder(self.spark, path)\n            self.output_data_objects[value.ID] = value(self.spark, path)\n\n    def read(self):\n        self.current_input_do.read()\n\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        for input_do, current_date in zip(self.input_event_data_objects, self.dates_to_process):\n            self.current_date = current_date\n            self.logger.info(f\"Reading from path {input_do.default_path}\")\n            self.current_input_do = input_do\n            self.read()\n            self.transform()  # Transforms the input_df\n            self.write()\n            # after each chunk processing clear all Cache to free memory and disk\n            self.spark.catalog.clearCache()\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n        df_events = self.current_input_do.df\n\n        # check nulls\n        check_null_columns = [ColNames.user_id, ColNames.timestamp, ColNames.mcc, ColNames.mnc]\n\n        df_events = self.flag_nulls(df_events, check_null_columns)\n\n        # add user_id_modulo\n        # The concept of device demultiplex is implemented here\n        # 1) Creates a modulo column, 2) repartitions according to it 3) sorts data within partitions\n\n        df_events = self.calculate_user_id_modulo(df_events, self.number_of_partitions)\n        df_events = df_events.repartition(ColNames.user_id_modulo)\n        df_events = df_events.sortWithinPartitions(ColNames.user_id, ColNames.timestamp)\n\n        # parse timestamp\n        df_events = self.parse_timestamp(df_events, self.timestamp_format)\n\n        # flag out of range limits\n        cols_range_limits = {\n            ColNames.mcc: [100, 999],\n            ColNames.plmn: [10000, 99999],\n        }\n\n        df_events = self.flag_out_of_range_limits(df_events, cols_range_limits)\n\n        if self.do_bounding_box_filtering:\n            cols_range_limits = {\n                ColNames.latitude: [self.bbox[1], self.bbox[3]],\n                ColNames.longitude: [self.bbox[0], self.bbox[2]],\n            }\n            df_events = self.flag_out_of_range_limits(df_events, cols_range_limits)\n\n        # flag out of length limits\n        cols_length_limits = {ColNames.mnc: [2, 3]}\n\n        df_events = self.flag_out_of_length_limits(df_events, cols_length_limits)\n\n        if self.do_cell_id_length_filtering:\n            cols_length_limits = {ColNames.cell_id: [14, 15]}\n            df_events = self.flag_out_of_length_limits(df_events, cols_length_limits)\n\n        # flag timestamp out of bounds\n        df_events = self.flag_timestamp_out_of_bounds(\n            df_events, self.current_date, self.current_date + datetime.timedelta(days=1)\n        )\n\n        # flag missing network info\n        df_events = self.flag_missing_mno_info(df_events)\n\n        # assign domain\n        df_events = self.assign_domain(df_events, self.local_mcc)\n\n        # flag missing location\n        df_events = self.flag_missing_location(df_events)\n\n        # flag same location duplicates\n        if self.do_same_location_deduplication:\n            df_events = self.flag_same_location_duplicates(df_events)\n\n        # add no error flag columns\n        df_events = self.add_no_error_flag_columns(df_events)\n\n        df_events.persist(StorageLevel.MEMORY_AND_DISK)\n\n        # get metrics by flag column\n        output_qa_by_column = self.get_metrics_by_flag_column(df_events)\n\n        output_qa_by_column = output_qa_by_column.withColumns(\n            {\n                ColNames.result_timestamp: F.lit(F.current_timestamp()),\n                ColNames.date: F.to_date(F.lit(self.current_date)),\n            }\n        )\n\n        output_qa_by_column = utils.apply_schema_casting(\n            output_qa_by_column, SilverEventDataSyntacticQualityMetricsByColumn.SCHEMA\n        )\n        self.output_data_objects[SilverEventDataSyntacticQualityMetricsByColumn.ID].df = output_qa_by_column\n\n        # get frequency metrics\n        frequency_metrics = self.get_frequency_metrics(df_events)\n\n        frequency_metrics = frequency_metrics.withColumns(\n            {\n                ColNames.result_timestamp: F.lit(F.current_timestamp()),\n                ColNames.date: F.to_date(F.lit(self.current_date)),\n            }\n        )\n\n        frequency_metrics = utils.apply_schema_casting(\n            frequency_metrics, SilverEventDataSyntacticQualityMetricsFrequencyDistribution.SCHEMA\n        )\n        self.output_data_objects[SilverEventDataSyntacticQualityMetricsFrequencyDistribution.ID].df = frequency_metrics\n\n        df_events = df_events.filter(F.col(\"to_preserve\"))\n\n        if self.do_timestamp_conversion_to_utc:\n            df_events = self.convert_to_utc(df_events, self.local_timezone)\n\n        df_events = df_events.withColumns(\n            {\n                ColNames.year: F.year(ColNames.timestamp).cast(\"smallint\"),\n                ColNames.month: F.month(ColNames.timestamp).cast(\"tinyint\"),\n                ColNames.day: F.dayofmonth(ColNames.timestamp).cast(\"tinyint\"),\n            }\n        )\n\n        df_events = utils.apply_schema_casting(df_events, SilverEventDataObject.SCHEMA)\n        df_events = df_events.repartition(*SilverEventDataObject.PARTITION_COLUMNS)\n\n        self.output_data_objects[SilverEventDataObject.ID].df = df_events\n\n    @staticmethod\n    def flag_nulls(\n        sdf: DataFrame,\n        filter_columns: List[str],\n    ) -&gt; DataFrame:\n        \"\"\"\n        Marks rows which include nulls in the filter columns\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): df events with possible nulls values\n            filter_columns (List[str], optional): columns to check for nulls\n        Returns:\n            pyspark.sql.dataframe.DataFrame: df where rows with null values in filter columns are flagged\n        \"\"\"\n\n        sdf = sdf.withColumns({f\"{col}_flag_{ErrorTypes.NULL_VALUE}\": F.col(col).isNull() for col in filter_columns})\n\n        return sdf\n\n    @staticmethod\n    def parse_timestamp(\n        sdf: DataFrame,\n        timestamp_format: str,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Based on config params timestampt format and input timezone\n        convert timestamp column from string to timestamp type.\n        Flag succesful timestamp transformations and errors.\n\n        Args:\n            sdf (pyspark.sql.dataframe.DataFrame): df with timestamp column\n            timestamp_format (str): expected string format to use in time conversion\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: df with time column parsed to timestamp\n        \"\"\"\n\n        sdf = sdf.withColumn(\n            f\"{ColNames.timestamp}_parsed\",\n            F.to_timestamp(ColNames.timestamp, timestamp_format),\n        )\n\n        sdf = sdf.withColumn(\n            f\"{ColNames.timestamp}_flag_{ErrorTypes.CANNOT_PARSE}\",\n            F.when(\n                F.col(ColNames.timestamp).isNotNull() &amp; F.col(f\"{ColNames.timestamp}_parsed\").isNull(),\n                F.lit(True),\n            ).otherwise(F.lit(False)),\n        )\n\n        sdf = sdf.withColumn(\n            ColNames.timestamp,\n            F.col(f\"{ColNames.timestamp}_parsed\"),\n        ).drop(f\"{ColNames.timestamp}_parsed\")\n\n        return sdf\n\n    @staticmethod\n    def flag_out_of_range_limits(\n        sdf: DataFrame,\n        cols_range_limits: Dict[str, List[int]],\n    ) -&gt; DataFrame:\n        \"\"\"\n        Checks if values in columns are within the specified range\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): df with columns to check\n            cols_range_limits (Dict[str, List[int]]): dictionary with column names as keys and lists of min and max values as values\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: df with columns checked for range limits\n        \"\"\"\n\n        for col_name, limits in cols_range_limits.items():\n            sdf = sdf.withColumn(\n                f\"{col_name}_flag_{ErrorTypes.OUT_OF_RANGE}\",\n                F.when(\n                    F.col(col_name).isNotNull() &amp; ~F.col(col_name).between(limits[0], limits[1]),\n                    F.lit(True),\n                ).otherwise(F.lit(False)),\n            )\n        return sdf\n\n    @staticmethod\n    def flag_out_of_length_limits(\n        sdf: DataFrame,\n        cols_length_limits: Dict[str, List[int]],\n    ) -&gt; DataFrame:\n        \"\"\"\n        Checks if values in columns are within the specified characters length\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): df with columns to check\n            cols_length_limits (Dict[str, List[int]]): dictionary with column names as keys and lists of min and max values as values\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: df with columns checked for range limits\n        \"\"\"\n\n        for col_name, limits in cols_length_limits.items():\n            sdf = sdf.withColumn(\n                f\"{col_name}_flag_{ErrorTypes.OUT_OF_RANGE}\",\n                F.when(\n                    F.col(col_name).isNotNull() &amp; ~F.length(F.col(col_name)).between(limits[0], limits[1]),\n                    F.lit(True),\n                ).otherwise(F.lit(False)),\n            )\n\n        return sdf\n\n    @staticmethod\n    def flag_timestamp_out_of_bounds(\n        sdf: DataFrame,\n        start_date: str,\n        end_date: str,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Flags rows with timestamps outside of the specified date bounds\n\n        Args:\n            sdf (pyspark.sql.dataframe.DataFrame): df with timestamp column\n            start_date (str): start date of the data\n            end_date (str): end date of the data\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: df with rows flagged if timestamp is out of bounds\n        \"\"\"\n\n        sdf = sdf.withColumn(\n            f\"{ColNames.timestamp}_flag_{ErrorTypes.OUT_OF_RANGE}\",\n            F.when(\n                F.col(f\"{ColNames.timestamp}\").isNotNull()\n                &amp; ~F.col(f\"{ColNames.timestamp}\").between(start_date, end_date),\n                F.lit(True),\n            ).otherwise(F.lit(False)),\n        )\n\n        return sdf\n\n    @staticmethod\n    def flag_missing_mno_info(\n        sdf: DataFrame,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Flags rows with missing mno network information\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): df with network columns\n            network_columns (List[str]): columns with network information\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: df with rows flagged if network info is missing\n        \"\"\"\n\n        sdf = sdf.withColumn(\n            f\"no_mno_info_flag_{ErrorTypes.NO_MNO_INFO}\",\n            F.when(\n                F.col(ColNames.mcc).isNull() &amp; F.col(ColNames.mnc).isNull() &amp; F.col(ColNames.plmn).isNull(),\n                F.lit(True),\n            ).otherwise(F.lit(False)),\n        )\n\n        return sdf\n\n    @staticmethod\n    def assign_domain(\n        sdf: DataFrame,\n        local_mcc: int,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Assigns domain to rows\n\n        Args:\n            sdf (pyspark.sql.dataframe.DataFrame): df with domain column\n            local_mcc (int): local_mcc value\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: df with domain column assigned\n        \"\"\"\n        sdf = sdf.withColumn(\n            ColNames.domain,\n            F.when(\n                (F.col(ColNames.plmn).isNotNull()) &amp; (F.col(ColNames.plmn).substr(1, 3) != F.lit(local_mcc)),\n                Domains.OUTBOUND,\n            ).otherwise(F.when(F.col(ColNames.mcc) == local_mcc, Domains.DOMESTIC).otherwise(Domains.INBOUND)),\n        )\n        return sdf\n\n    @staticmethod\n    def flag_missing_location(\n        sdf: DataFrame,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Flags rows with missing location information\n\n        Args:\n            sdf (pyspark.sql.dataframe.DataFrame): df with location columns\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: df with rows flagged if location info is missing\n        \"\"\"\n\n        sdf = sdf.withColumn(\n            f\"no_location_flag_{ErrorTypes.NO_LOCATION_INFO}\",\n            F.when(\n                (\n                    (\n                        F.col(ColNames.latitude).isNull()\n                        &amp; F.col(ColNames.longitude).isNull()\n                        &amp; F.col(ColNames.cell_id).isNull()\n                        &amp; ((F.col(ColNames.domain) == Domains.DOMESTIC) | (F.col(ColNames.domain) == Domains.INBOUND))\n                    )\n                ),\n                F.lit(True),\n            ).otherwise(F.lit(False)),\n        )\n\n        return sdf\n\n    @staticmethod\n    def flag_same_location_duplicates(\n        df: DataFrame,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Flag rows that have identical records for\n        timestamp, cell_id, longitude, latitude, plmn and user_id.\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): df events with possible duplicates\n        Returns:\n            pyspark.sql.dataframe.DataFrame: df duplicate\n            in terms of user_id, cell_id, latitutde, longitude, plmn\n            and timestamp combination flagged\n        \"\"\"\n        window_spec = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id).orderBy(F.col(ColNames.timestamp))\n        # Use lag to get the previous row's values\n        df = df.withColumns(\n            {\n                \"prev_timestamp\": F.lag(ColNames.timestamp).over(window_spec),\n                \"prev_cell_id\": F.lag(ColNames.cell_id).over(window_spec),\n                \"prev_latitude\": F.lag(ColNames.latitude).over(window_spec),\n                \"prev_longitude\": F.lag(ColNames.longitude).over(window_spec),\n                \"prev_plmn\": F.lag(ColNames.plmn).over(window_spec),\n            }\n        )\n        # Flag the current row if the conditions are met\n        df = df.withColumn(\n            f\"duplicated_flag_{ErrorTypes.DUPLICATED}\",\n            F.when(\n                (F.col(ColNames.timestamp) == F.col(\"prev_timestamp\"))\n                &amp; (\n                    F.coalesce(F.col(ColNames.cell_id), F.lit(\"\")).cast(\"string\")\n                    == F.coalesce(F.col(\"prev_cell_id\"), F.lit(\"\")).cast(\"string\")\n                )\n                &amp; (\n                    F.coalesce(F.col(ColNames.latitude), F.lit(0.0)).cast(\"double\")\n                    == F.coalesce(F.col(\"prev_latitude\"), F.lit(0.0)).cast(\"double\")\n                )\n                &amp; (\n                    F.coalesce(F.col(ColNames.longitude), F.lit(0.0)).cast(\"double\")\n                    == F.coalesce(F.col(\"prev_longitude\"), F.lit(0.0)).cast(\"double\")\n                )\n                &amp; (\n                    F.coalesce(F.col(ColNames.plmn), F.lit(\"\")).cast(\"string\")\n                    == F.coalesce(F.col(\"prev_plmn\"), F.lit(\"\")).cast(\"string\")\n                ),\n                F.lit(True),\n            ).otherwise(F.lit(False)),\n        ).drop(\"prev_timestamp\", \"prev_cell_id\", \"prev_latitude\", \"prev_longitude\", \"prev_plmn\")\n\n        return df\n\n    @staticmethod\n    def add_no_error_flag_columns(sdf: DataFrame):\n        \"\"\"\n        Add columns with 'no error' flags for each column\n\n        Args:\n            sdf (pyspark.sql.dataframe.DataFrame): df with error flag columns\n\n        Returns:\n            List[str]: list of columns that are not flagged as errors\n        \"\"\"\n\n        flag_columns = [col for col in sdf.columns if \"flag\" in col]\n\n        base_columns = list(set(col.split(\"_flag\")[0] for col in flag_columns))\n\n        column_groups = dict()\n        for col in base_columns:\n            # for each auxiliar column\n            column_groups[col] = []\n            for cc in flag_columns:\n                # if it is related to the DO's column:\n                if col == cc.split(\"_flag\")[0]:\n                    column_groups[col].append(F.col(cc))\n\n        column_conditions = {col: reduce(lambda a, b: a | b, column_groups[col]) for col in column_groups}\n\n        field_without_errors = {col: ~column_conditions[col] for col in column_conditions}\n\n        sdf = sdf.withColumns(\n            {f\"{col}_flag_{ErrorTypes.NO_ERROR}\": field_without_errors[col] for col in field_without_errors}\n        )\n\n        sdf = sdf.withColumn(\n            \"to_preserve\", reduce(lambda a, b: a &amp; b, [field_without_errors[col] for col in field_without_errors])\n        )\n\n        return sdf\n\n    @staticmethod\n    def get_metrics_by_flag_column(sdf):\n        \"\"\"\n        Get the count of flagged errors for each column\n\n        Args:\n            sdf (pyspark.sql.dataframe.DataFrame): df with error columns\n\n        Returns:\n            dict: dictionary with column names as keys and error counts as values\n        \"\"\"\n        flag_columns = [col for col in sdf.columns if \"flag\" in col]\n\n        flag_counts = sdf.agg(*[F.sum(F.col(col).cast(\"int\")).alias(col) for col in flag_columns])\n\n        # Unpivot the true_counts DataFrame\n        metrics_sdf = flag_counts.unpivot([], flag_columns, \"flag_column\", ColNames.value)\n\n        metrics_sdf = (\n            metrics_sdf.withColumns(\n                {\n                    ColNames.variable: F.split(F.col(\"flag_column\"), \"_flag_\").getItem(0),\n                    ColNames.type_of_error: F.split(F.col(\"flag_column\"), \"_flag_\").getItem(1),\n                }\n            )\n            .drop(\"flag_column\")\n            .orderBy(ColNames.variable)\n        )\n\n        return metrics_sdf\n\n    @staticmethod\n    def get_frequency_metrics(sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Get total row counts per user_id and cell_id before\n        filtering and after filtering (rows without errors)\n\n        Args:\n            sdf (pyspark.sql.dataframe.DataFrame): df with columns to count\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: dataframe with frequency counts\n        \"\"\"\n\n        frequency_metrics = sdf.groupBy(ColNames.user_id, ColNames.cell_id).agg(\n            F.count(\"*\").alias(ColNames.initial_frequency),\n            F.sum(F.col(\"to_preserve\").cast(\"int\")).alias(ColNames.final_frequency),\n        )\n\n        return frequency_metrics\n\n    @staticmethod\n    def convert_to_utc(\n        sdf: DataFrame,\n        local_timezone: str,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Converts timestamp column to UTC timezone\n\n        Args:\n            sdf (pyspark.sql.dataframe.DataFrame): df with timestamp column\n            local_timezone (str): timezone of the input data\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: df with timestamp column converted to UTC\n        \"\"\"\n\n        sdf = sdf.withColumn(\n            ColNames.timestamp,\n            F.to_utc_timestamp(ColNames.timestamp, local_timezone),\n        )\n\n        return sdf\n\n    @staticmethod\n    def calculate_user_id_modulo(\n        df: DataFrame,\n        modulo_value: int,\n        hex_truncation_end: int = 12,\n    ) -&gt; DataFrame:\n        \"\"\"Calculates the extra column user_id_modulo, as the result of the modulo function\n        applied on the binary user id column. The modulo value will affect the number of\n        partitions in the final output.\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): df with user_id column\n            modulo_value (int): modulo value to be used when dividing user id.\n            hex_truncation_end (int): to which character truncate the hex, before sending it to conv function\n                and then to modulo. Anything upward of 13 is likely to result in distributional issues,\n                as the modulo value might not correspond to the number of final partitions.\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: dataframe with user_id_modulo column.\n        \"\"\"\n\n        # TODO make hex truncation (substring parameters) as configurable by user?\n\n        df = df.withColumn(\n            ColNames.user_id_modulo,\n            F.conv(\n                F.substring(F.hex(F.col(ColNames.user_id)), 1, hex_truncation_end),\n                16,\n                10,\n            ).cast(\"long\")\n            % F.lit(modulo_value).cast(\"bigint\"),\n        )\n\n        return df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.add_no_error_flag_columns","title":"<code>add_no_error_flag_columns(sdf)</code>  <code>staticmethod</code>","text":"<p>Add columns with 'no error' flags for each column</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>df with error flag columns</p> required <p>Returns:</p> Type Description <p>List[str]: list of columns that are not flagged as errors</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>@staticmethod\ndef add_no_error_flag_columns(sdf: DataFrame):\n    \"\"\"\n    Add columns with 'no error' flags for each column\n\n    Args:\n        sdf (pyspark.sql.dataframe.DataFrame): df with error flag columns\n\n    Returns:\n        List[str]: list of columns that are not flagged as errors\n    \"\"\"\n\n    flag_columns = [col for col in sdf.columns if \"flag\" in col]\n\n    base_columns = list(set(col.split(\"_flag\")[0] for col in flag_columns))\n\n    column_groups = dict()\n    for col in base_columns:\n        # for each auxiliar column\n        column_groups[col] = []\n        for cc in flag_columns:\n            # if it is related to the DO's column:\n            if col == cc.split(\"_flag\")[0]:\n                column_groups[col].append(F.col(cc))\n\n    column_conditions = {col: reduce(lambda a, b: a | b, column_groups[col]) for col in column_groups}\n\n    field_without_errors = {col: ~column_conditions[col] for col in column_conditions}\n\n    sdf = sdf.withColumns(\n        {f\"{col}_flag_{ErrorTypes.NO_ERROR}\": field_without_errors[col] for col in field_without_errors}\n    )\n\n    sdf = sdf.withColumn(\n        \"to_preserve\", reduce(lambda a, b: a &amp; b, [field_without_errors[col] for col in field_without_errors])\n    )\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.assign_domain","title":"<code>assign_domain(sdf, local_mcc)</code>  <code>staticmethod</code>","text":"<p>Assigns domain to rows</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>df with domain column</p> required <code>local_mcc</code> <code>int</code> <p>local_mcc value</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: df with domain column assigned</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>@staticmethod\ndef assign_domain(\n    sdf: DataFrame,\n    local_mcc: int,\n) -&gt; DataFrame:\n    \"\"\"\n    Assigns domain to rows\n\n    Args:\n        sdf (pyspark.sql.dataframe.DataFrame): df with domain column\n        local_mcc (int): local_mcc value\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: df with domain column assigned\n    \"\"\"\n    sdf = sdf.withColumn(\n        ColNames.domain,\n        F.when(\n            (F.col(ColNames.plmn).isNotNull()) &amp; (F.col(ColNames.plmn).substr(1, 3) != F.lit(local_mcc)),\n            Domains.OUTBOUND,\n        ).otherwise(F.when(F.col(ColNames.mcc) == local_mcc, Domains.DOMESTIC).otherwise(Domains.INBOUND)),\n    )\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.calculate_user_id_modulo","title":"<code>calculate_user_id_modulo(df, modulo_value, hex_truncation_end=12)</code>  <code>staticmethod</code>","text":"<p>Calculates the extra column user_id_modulo, as the result of the modulo function applied on the binary user id column. The modulo value will affect the number of partitions in the final output.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>df with user_id column</p> required <code>modulo_value</code> <code>int</code> <p>modulo value to be used when dividing user id.</p> required <code>hex_truncation_end</code> <code>int</code> <p>to which character truncate the hex, before sending it to conv function and then to modulo. Anything upward of 13 is likely to result in distributional issues, as the modulo value might not correspond to the number of final partitions.</p> <code>12</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: dataframe with user_id_modulo column.</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>@staticmethod\ndef calculate_user_id_modulo(\n    df: DataFrame,\n    modulo_value: int,\n    hex_truncation_end: int = 12,\n) -&gt; DataFrame:\n    \"\"\"Calculates the extra column user_id_modulo, as the result of the modulo function\n    applied on the binary user id column. The modulo value will affect the number of\n    partitions in the final output.\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): df with user_id column\n        modulo_value (int): modulo value to be used when dividing user id.\n        hex_truncation_end (int): to which character truncate the hex, before sending it to conv function\n            and then to modulo. Anything upward of 13 is likely to result in distributional issues,\n            as the modulo value might not correspond to the number of final partitions.\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: dataframe with user_id_modulo column.\n    \"\"\"\n\n    # TODO make hex truncation (substring parameters) as configurable by user?\n\n    df = df.withColumn(\n        ColNames.user_id_modulo,\n        F.conv(\n            F.substring(F.hex(F.col(ColNames.user_id)), 1, hex_truncation_end),\n            16,\n            10,\n        ).cast(\"long\")\n        % F.lit(modulo_value).cast(\"bigint\"),\n    )\n\n    return df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.convert_to_utc","title":"<code>convert_to_utc(sdf, local_timezone)</code>  <code>staticmethod</code>","text":"<p>Converts timestamp column to UTC timezone</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>df with timestamp column</p> required <code>local_timezone</code> <code>str</code> <p>timezone of the input data</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: df with timestamp column converted to UTC</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>@staticmethod\ndef convert_to_utc(\n    sdf: DataFrame,\n    local_timezone: str,\n) -&gt; DataFrame:\n    \"\"\"\n    Converts timestamp column to UTC timezone\n\n    Args:\n        sdf (pyspark.sql.dataframe.DataFrame): df with timestamp column\n        local_timezone (str): timezone of the input data\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: df with timestamp column converted to UTC\n    \"\"\"\n\n    sdf = sdf.withColumn(\n        ColNames.timestamp,\n        F.to_utc_timestamp(ColNames.timestamp, local_timezone),\n    )\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.flag_missing_location","title":"<code>flag_missing_location(sdf)</code>  <code>staticmethod</code>","text":"<p>Flags rows with missing location information</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>df with location columns</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: df with rows flagged if location info is missing</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>@staticmethod\ndef flag_missing_location(\n    sdf: DataFrame,\n) -&gt; DataFrame:\n    \"\"\"\n    Flags rows with missing location information\n\n    Args:\n        sdf (pyspark.sql.dataframe.DataFrame): df with location columns\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: df with rows flagged if location info is missing\n    \"\"\"\n\n    sdf = sdf.withColumn(\n        f\"no_location_flag_{ErrorTypes.NO_LOCATION_INFO}\",\n        F.when(\n            (\n                (\n                    F.col(ColNames.latitude).isNull()\n                    &amp; F.col(ColNames.longitude).isNull()\n                    &amp; F.col(ColNames.cell_id).isNull()\n                    &amp; ((F.col(ColNames.domain) == Domains.DOMESTIC) | (F.col(ColNames.domain) == Domains.INBOUND))\n                )\n            ),\n            F.lit(True),\n        ).otherwise(F.lit(False)),\n    )\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.flag_missing_mno_info","title":"<code>flag_missing_mno_info(sdf)</code>  <code>staticmethod</code>","text":"<p>Flags rows with missing mno network information</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>df with network columns</p> required <code>network_columns</code> <code>List[str]</code> <p>columns with network information</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: df with rows flagged if network info is missing</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>@staticmethod\ndef flag_missing_mno_info(\n    sdf: DataFrame,\n) -&gt; DataFrame:\n    \"\"\"\n    Flags rows with missing mno network information\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): df with network columns\n        network_columns (List[str]): columns with network information\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: df with rows flagged if network info is missing\n    \"\"\"\n\n    sdf = sdf.withColumn(\n        f\"no_mno_info_flag_{ErrorTypes.NO_MNO_INFO}\",\n        F.when(\n            F.col(ColNames.mcc).isNull() &amp; F.col(ColNames.mnc).isNull() &amp; F.col(ColNames.plmn).isNull(),\n            F.lit(True),\n        ).otherwise(F.lit(False)),\n    )\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.flag_nulls","title":"<code>flag_nulls(sdf, filter_columns)</code>  <code>staticmethod</code>","text":"<p>Marks rows which include nulls in the filter columns</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>df events with possible nulls values</p> required <code>filter_columns</code> <code>List[str]</code> <p>columns to check for nulls</p> required <p>Returns:     pyspark.sql.dataframe.DataFrame: df where rows with null values in filter columns are flagged</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>@staticmethod\ndef flag_nulls(\n    sdf: DataFrame,\n    filter_columns: List[str],\n) -&gt; DataFrame:\n    \"\"\"\n    Marks rows which include nulls in the filter columns\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): df events with possible nulls values\n        filter_columns (List[str], optional): columns to check for nulls\n    Returns:\n        pyspark.sql.dataframe.DataFrame: df where rows with null values in filter columns are flagged\n    \"\"\"\n\n    sdf = sdf.withColumns({f\"{col}_flag_{ErrorTypes.NULL_VALUE}\": F.col(col).isNull() for col in filter_columns})\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.flag_out_of_length_limits","title":"<code>flag_out_of_length_limits(sdf, cols_length_limits)</code>  <code>staticmethod</code>","text":"<p>Checks if values in columns are within the specified characters length</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>df with columns to check</p> required <code>cols_length_limits</code> <code>Dict[str, List[int]]</code> <p>dictionary with column names as keys and lists of min and max values as values</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: df with columns checked for range limits</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>@staticmethod\ndef flag_out_of_length_limits(\n    sdf: DataFrame,\n    cols_length_limits: Dict[str, List[int]],\n) -&gt; DataFrame:\n    \"\"\"\n    Checks if values in columns are within the specified characters length\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): df with columns to check\n        cols_length_limits (Dict[str, List[int]]): dictionary with column names as keys and lists of min and max values as values\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: df with columns checked for range limits\n    \"\"\"\n\n    for col_name, limits in cols_length_limits.items():\n        sdf = sdf.withColumn(\n            f\"{col_name}_flag_{ErrorTypes.OUT_OF_RANGE}\",\n            F.when(\n                F.col(col_name).isNotNull() &amp; ~F.length(F.col(col_name)).between(limits[0], limits[1]),\n                F.lit(True),\n            ).otherwise(F.lit(False)),\n        )\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.flag_out_of_range_limits","title":"<code>flag_out_of_range_limits(sdf, cols_range_limits)</code>  <code>staticmethod</code>","text":"<p>Checks if values in columns are within the specified range</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>df with columns to check</p> required <code>cols_range_limits</code> <code>Dict[str, List[int]]</code> <p>dictionary with column names as keys and lists of min and max values as values</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: df with columns checked for range limits</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>@staticmethod\ndef flag_out_of_range_limits(\n    sdf: DataFrame,\n    cols_range_limits: Dict[str, List[int]],\n) -&gt; DataFrame:\n    \"\"\"\n    Checks if values in columns are within the specified range\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): df with columns to check\n        cols_range_limits (Dict[str, List[int]]): dictionary with column names as keys and lists of min and max values as values\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: df with columns checked for range limits\n    \"\"\"\n\n    for col_name, limits in cols_range_limits.items():\n        sdf = sdf.withColumn(\n            f\"{col_name}_flag_{ErrorTypes.OUT_OF_RANGE}\",\n            F.when(\n                F.col(col_name).isNotNull() &amp; ~F.col(col_name).between(limits[0], limits[1]),\n                F.lit(True),\n            ).otherwise(F.lit(False)),\n        )\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.flag_same_location_duplicates","title":"<code>flag_same_location_duplicates(df)</code>  <code>staticmethod</code>","text":"<p>Flag rows that have identical records for timestamp, cell_id, longitude, latitude, plmn and user_id. Args:     df (pyspark.sql.dataframe.DataFrame): df events with possible duplicates Returns:     pyspark.sql.dataframe.DataFrame: df duplicate     in terms of user_id, cell_id, latitutde, longitude, plmn     and timestamp combination flagged</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>@staticmethod\ndef flag_same_location_duplicates(\n    df: DataFrame,\n) -&gt; DataFrame:\n    \"\"\"\n    Flag rows that have identical records for\n    timestamp, cell_id, longitude, latitude, plmn and user_id.\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): df events with possible duplicates\n    Returns:\n        pyspark.sql.dataframe.DataFrame: df duplicate\n        in terms of user_id, cell_id, latitutde, longitude, plmn\n        and timestamp combination flagged\n    \"\"\"\n    window_spec = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id).orderBy(F.col(ColNames.timestamp))\n    # Use lag to get the previous row's values\n    df = df.withColumns(\n        {\n            \"prev_timestamp\": F.lag(ColNames.timestamp).over(window_spec),\n            \"prev_cell_id\": F.lag(ColNames.cell_id).over(window_spec),\n            \"prev_latitude\": F.lag(ColNames.latitude).over(window_spec),\n            \"prev_longitude\": F.lag(ColNames.longitude).over(window_spec),\n            \"prev_plmn\": F.lag(ColNames.plmn).over(window_spec),\n        }\n    )\n    # Flag the current row if the conditions are met\n    df = df.withColumn(\n        f\"duplicated_flag_{ErrorTypes.DUPLICATED}\",\n        F.when(\n            (F.col(ColNames.timestamp) == F.col(\"prev_timestamp\"))\n            &amp; (\n                F.coalesce(F.col(ColNames.cell_id), F.lit(\"\")).cast(\"string\")\n                == F.coalesce(F.col(\"prev_cell_id\"), F.lit(\"\")).cast(\"string\")\n            )\n            &amp; (\n                F.coalesce(F.col(ColNames.latitude), F.lit(0.0)).cast(\"double\")\n                == F.coalesce(F.col(\"prev_latitude\"), F.lit(0.0)).cast(\"double\")\n            )\n            &amp; (\n                F.coalesce(F.col(ColNames.longitude), F.lit(0.0)).cast(\"double\")\n                == F.coalesce(F.col(\"prev_longitude\"), F.lit(0.0)).cast(\"double\")\n            )\n            &amp; (\n                F.coalesce(F.col(ColNames.plmn), F.lit(\"\")).cast(\"string\")\n                == F.coalesce(F.col(\"prev_plmn\"), F.lit(\"\")).cast(\"string\")\n            ),\n            F.lit(True),\n        ).otherwise(F.lit(False)),\n    ).drop(\"prev_timestamp\", \"prev_cell_id\", \"prev_latitude\", \"prev_longitude\", \"prev_plmn\")\n\n    return df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.flag_timestamp_out_of_bounds","title":"<code>flag_timestamp_out_of_bounds(sdf, start_date, end_date)</code>  <code>staticmethod</code>","text":"<p>Flags rows with timestamps outside of the specified date bounds</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>df with timestamp column</p> required <code>start_date</code> <code>str</code> <p>start date of the data</p> required <code>end_date</code> <code>str</code> <p>end date of the data</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: df with rows flagged if timestamp is out of bounds</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>@staticmethod\ndef flag_timestamp_out_of_bounds(\n    sdf: DataFrame,\n    start_date: str,\n    end_date: str,\n) -&gt; DataFrame:\n    \"\"\"\n    Flags rows with timestamps outside of the specified date bounds\n\n    Args:\n        sdf (pyspark.sql.dataframe.DataFrame): df with timestamp column\n        start_date (str): start date of the data\n        end_date (str): end date of the data\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: df with rows flagged if timestamp is out of bounds\n    \"\"\"\n\n    sdf = sdf.withColumn(\n        f\"{ColNames.timestamp}_flag_{ErrorTypes.OUT_OF_RANGE}\",\n        F.when(\n            F.col(f\"{ColNames.timestamp}\").isNotNull()\n            &amp; ~F.col(f\"{ColNames.timestamp}\").between(start_date, end_date),\n            F.lit(True),\n        ).otherwise(F.lit(False)),\n    )\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.get_frequency_metrics","title":"<code>get_frequency_metrics(sdf)</code>  <code>staticmethod</code>","text":"<p>Get total row counts per user_id and cell_id before filtering and after filtering (rows without errors)</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>df with columns to count</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: dataframe with frequency counts</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>@staticmethod\ndef get_frequency_metrics(sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Get total row counts per user_id and cell_id before\n    filtering and after filtering (rows without errors)\n\n    Args:\n        sdf (pyspark.sql.dataframe.DataFrame): df with columns to count\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: dataframe with frequency counts\n    \"\"\"\n\n    frequency_metrics = sdf.groupBy(ColNames.user_id, ColNames.cell_id).agg(\n        F.count(\"*\").alias(ColNames.initial_frequency),\n        F.sum(F.col(\"to_preserve\").cast(\"int\")).alias(ColNames.final_frequency),\n    )\n\n    return frequency_metrics\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.get_metrics_by_flag_column","title":"<code>get_metrics_by_flag_column(sdf)</code>  <code>staticmethod</code>","text":"<p>Get the count of flagged errors for each column</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>df with error columns</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>dictionary with column names as keys and error counts as values</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>@staticmethod\ndef get_metrics_by_flag_column(sdf):\n    \"\"\"\n    Get the count of flagged errors for each column\n\n    Args:\n        sdf (pyspark.sql.dataframe.DataFrame): df with error columns\n\n    Returns:\n        dict: dictionary with column names as keys and error counts as values\n    \"\"\"\n    flag_columns = [col for col in sdf.columns if \"flag\" in col]\n\n    flag_counts = sdf.agg(*[F.sum(F.col(col).cast(\"int\")).alias(col) for col in flag_columns])\n\n    # Unpivot the true_counts DataFrame\n    metrics_sdf = flag_counts.unpivot([], flag_columns, \"flag_column\", ColNames.value)\n\n    metrics_sdf = (\n        metrics_sdf.withColumns(\n            {\n                ColNames.variable: F.split(F.col(\"flag_column\"), \"_flag_\").getItem(0),\n                ColNames.type_of_error: F.split(F.col(\"flag_column\"), \"_flag_\").getItem(1),\n            }\n        )\n        .drop(\"flag_column\")\n        .orderBy(ColNames.variable)\n    )\n\n    return metrics_sdf\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.parse_timestamp","title":"<code>parse_timestamp(sdf, timestamp_format)</code>  <code>staticmethod</code>","text":"<p>Based on config params timestampt format and input timezone convert timestamp column from string to timestamp type. Flag succesful timestamp transformations and errors.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>df with timestamp column</p> required <code>timestamp_format</code> <code>str</code> <p>expected string format to use in time conversion</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: df with time column parsed to timestamp</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>@staticmethod\ndef parse_timestamp(\n    sdf: DataFrame,\n    timestamp_format: str,\n) -&gt; DataFrame:\n    \"\"\"\n    Based on config params timestampt format and input timezone\n    convert timestamp column from string to timestamp type.\n    Flag succesful timestamp transformations and errors.\n\n    Args:\n        sdf (pyspark.sql.dataframe.DataFrame): df with timestamp column\n        timestamp_format (str): expected string format to use in time conversion\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: df with time column parsed to timestamp\n    \"\"\"\n\n    sdf = sdf.withColumn(\n        f\"{ColNames.timestamp}_parsed\",\n        F.to_timestamp(ColNames.timestamp, timestamp_format),\n    )\n\n    sdf = sdf.withColumn(\n        f\"{ColNames.timestamp}_flag_{ErrorTypes.CANNOT_PARSE}\",\n        F.when(\n            F.col(ColNames.timestamp).isNotNull() &amp; F.col(f\"{ColNames.timestamp}_parsed\").isNull(),\n            F.lit(True),\n        ).otherwise(F.lit(False)),\n    )\n\n    sdf = sdf.withColumn(\n        ColNames.timestamp,\n        F.col(f\"{ColNames.timestamp}_parsed\"),\n    ).drop(f\"{ColNames.timestamp}_parsed\")\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/event_semantic_cleaning/","title":"event_semantic_cleaning","text":""},{"location":"reference/components/execution/event_semantic_cleaning/event_semantic_cleaning/","title":"event_semantic_cleaning","text":"<p>Module that computes semantic checks on event data and adds error flags</p>"},{"location":"reference/components/execution/event_semantic_cleaning/event_semantic_cleaning/#components.execution.event_semantic_cleaning.event_semantic_cleaning.SemanticCleaning","title":"<code>SemanticCleaning</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class that performs semantic checks on event data and adds error flags</p> Source code in <code>multimno/components/execution/event_semantic_cleaning/event_semantic_cleaning.py</code> <pre><code>class SemanticCleaning(Component):\n    \"\"\"\n    Class that performs semantic checks on event data and adds error flags\n    \"\"\"\n\n    COMPONENT_ID = \"SemanticCleaning\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n        self.timestamp = datetime.datetime.now()\n\n        # Read date of study\n        self.date_format = self.config.get(self.COMPONENT_ID, \"date_format\")\n        data_period_start = self.config.get(self.COMPONENT_ID, \"data_period_start\")\n        data_period_end = self.config.get(self.COMPONENT_ID, \"data_period_end\")\n        self.date_of_study: datetime.date = None\n\n        self.do_different_location_deduplication = self.config.getboolean(\n            self.COMPONENT_ID, \"do_different_location_deduplication\"\n        )\n\n        try:\n            self.data_period_start = datetime.datetime.strptime(data_period_start, self.date_format).date()\n        except ValueError as e:\n            self.logger.error(str(e), exc_info=True)\n            raise e\n\n        try:\n            self.data_period_end = datetime.datetime.strptime(data_period_end, self.date_format).date()\n        except ValueError as e:\n            self.logger.error(str(e), exc_info=True)\n            raise e\n\n        self.data_period_dates = [\n            self.data_period_start + datetime.timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n        # Unit: metre\n        self.semantic_min_distance = self.config.getfloat(self.COMPONENT_ID, \"semantic_min_distance_m\")\n\n        # Unit: metre / second\n        self.semantic_min_speed = self.config.getfloat(self.COMPONENT_ID, \"semantic_min_speed_m_s\")\n\n    def initalize_data_objects(self):\n        self.clear_destination_directory = self.config.getboolean(self.COMPONENT_ID, \"clear_destination_directory\")\n        input_silver_event_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"event_data_silver\")\n        input_silver_network_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"network_data_silver\")\n        output_silver_event_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"event_data_silver_flagged\")\n        output_silver_semantic_metrics_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"event_device_semantic_quality_metrics\"\n        )\n        output_event_cache_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"event_cache\")\n\n        input_silver_network = SilverNetworkDataObject(\n            self.spark,\n            input_silver_network_path,\n        )\n        input_silver_event = SilverEventDataObject(self.spark, input_silver_event_path)\n\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, output_silver_event_path)\n            delete_file_or_folder(self.spark, output_event_cache_path)\n\n        output_silver_event = SilverEventFlaggedDataObject(self.spark, output_silver_event_path)\n        output_silver_semantic_metrics = SilverEventSemanticQualityMetrics(\n            self.spark,\n            output_silver_semantic_metrics_path,\n        )\n        output_event_cache = EventCacheDataObject(self.spark, output_event_cache_path)\n\n        self.input_data_objects = {\n            SilverNetworkDataObject.ID: input_silver_network,\n            SilverEventDataObject.ID: input_silver_event,\n        }\n        self.output_data_objects = {\n            SilverEventFlaggedDataObject.ID: output_silver_event,\n            SilverEventSemanticQualityMetrics.ID: output_silver_semantic_metrics,\n            EventCacheDataObject.ID: output_event_cache,\n        }\n\n    def transform(self):\n        events_df = self.events_df\n        cells_df = self.cells_df\n\n        # Partition filter\n        events_df = events_df.filter(\n            F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(self.date_of_study)\n        )\n\n        cells_df = (\n            cells_df.filter(\n                F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n                == F.lit(self.date_of_study)\n            )\n            # Point geometry auxiliar column\n            .withColumn(\n                \"geometry\",\n                STC.ST_Point(F.col(ColNames.latitude), F.col(ColNames.latitude)),\n            ).select(\n                [\n                    F.col(ColNames.cell_id),\n                    F.col(ColNames.latitude).alias(\"cell_lat\"),\n                    F.col(ColNames.longitude).alias(\"cell_lon\"),\n                    F.col(ColNames.valid_date_start),\n                    F.col(ColNames.valid_date_end),\n                    F.col(\"geometry\"),\n                ]\n            )\n        )\n\n        # Perform a left join between events and cell IDs. Non-existent cell IDs will be matched\n        # with null values\n        df = events_df.join(cells_df, on=ColNames.cell_id, how=\"left\")\n        df = df.withColumn(ColNames.error_flag, F.lit(None))\n\n        # Flag outbound records as NO_ERROR to omit them from checks.\n        # NOTE: Different location duplicates checking is still performed as normal.\n        df = self._flag_outbound(df)\n\n        # Error flag: Check rows for cell ids with no matching cell entry\n        df = self._flag_non_existent_cell_ids(df)\n\n        # Optional flagging of duplicates with different location info\n        if self.do_different_location_deduplication:\n            df = self._flag_different_location_duplicates(df)\n\n        # Error flag: Check rows which have valid date start and/or valid date end, and flag when timestamp is incompatible\n        df = self._flag_invalid_cell_ids(df)\n\n        # Error flag: suspicious and incorrect events based on location change distance and speed\n        df = self._flag_by_event_location(df)\n\n        # Set NO_ERROR flag for all unmarked records\n        df = self._flag_non_errors(df)\n\n        # Keep only the necessary columns and remove auxiliar ones\n        df = utils.apply_schema_casting(df, SilverEventFlaggedDataObject.SCHEMA)\n\n        df = df.repartition(*SilverEventFlaggedDataObject.PARTITION_COLUMNS)\n\n        df.persist(StorageLevel.MEMORY_AND_DISK)\n\n        # Semantic metrics\n        metrics_df = self._compute_semantic_metrics(df)\n\n        # Event cache: Calculate first and last events\n        cache_events_df = self._mark_first_last_events(df)\n\n        self.output_data_objects[SilverEventFlaggedDataObject.ID].df = df\n        self.output_data_objects[SilverEventSemanticQualityMetrics.ID].df = metrics_df\n        self.output_data_objects[EventCacheDataObject.ID].df = cache_events_df\n\n    def _mark_first_last_events(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Marks first and last event per user per day.\n        - False for first event\n        - True for last event\n\n        Args:\n            df: Input DataFrame with user_id, timestamp, and date-related columns.\n\n        Returns:\n            DataFrame in EventCacheDataObject format.\n        \"\"\"\n        temp_column = \"temp\"\n\n        # Define window specification partitioned by user and day, ordered by timestamp\n        window_spec = Window.partitionBy(\n            [\n                ColNames.year,\n                ColNames.month,\n                ColNames.day,\n                ColNames.user_id_modulo,\n                ColNames.user_id,\n            ]\n        )\n        # Get the first and last event timestamps in the window\n        first_event_ts = F.first(ColNames.timestamp).over(window_spec)\n        last_event_ts = F.last(ColNames.timestamp).over(window_spec)\n\n        # Mark first and last events of event_error_flag = 0\n        df = df.filter(F.col(ColNames.error_flag) == SemanticErrorType.NO_ERROR).withColumn(\n            temp_column,\n            F.when(F.col(ColNames.timestamp) == first_event_ts, 1)  # First event\n            .when(F.col(ColNames.timestamp) == last_event_ts, 2)  # Last event\n            .otherwise(0),  # Other events\n        )\n\n        # Filter out events that are not first or last\n        df = df.filter(F.col(temp_column) &gt; 0)\n        df = df.withColumn(ColNames.is_last_event, F.when(F.col(temp_column) == 2, True).otherwise(False)).drop(\n            temp_column\n        )\n\n        return df\n\n    def _flag_outbound(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Method to mark all outbound records as valid.\n        Outbound records should have their domain marked as OUTBOUND in the previous event processing component.\n        Outbound records are exempt from further error checks (except same location duplicates) in this component.\n\n        Args:\n            df (DataFrame): Spark DataFrame of event records.\n\n        Returns:\n            DataFrame: Same DataFram with added error_flag column, with outbound records marked as NO_ERROR\n        \"\"\"\n        is_outbound_record_cond = F.col(ColNames.domain) == Domains.OUTBOUND\n\n        df = df.withColumn(\n            ColNames.error_flag,\n            F.when(F.col(ColNames.error_flag).isNotNull(), F.col(ColNames.error_flag)).when(\n                is_outbound_record_cond, F.lit(SemanticErrorType.NO_ERROR)\n            ),\n        )\n        return df\n\n    def _flag_non_existent_cell_ids(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Method that creates a new integer column with the name of ColNames.error_flag, and\n        sets the corresponding flags to events that refer to non-existent cell IDs. The rest of\n        the column's values are left as null.\n\n        Args:\n            df (DataFrame): PySpark DataFrame resulting from the left join of events and cells\n\n        Returns:\n            DataFrame: same DataFrame as the input with a new error flag column, containing\n                flags for the events with a non existent cell ID\n        \"\"\"\n        df = df.withColumn(\n            ColNames.error_flag,\n            F.when(F.col(ColNames.error_flag).isNotNull(), F.col(ColNames.error_flag)).when(\n                F.col(\"geometry\").isNull(), F.lit(SemanticErrorType.CELL_ID_NON_EXISTENT)\n            ),\n        )\n        return df\n\n    def _flag_invalid_cell_ids(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Method that finds and flags events which refer to an existent cell ID, but that happened outside the\n        time interval during which the cell was operationals. This flag cannot occur at the same time as a\n        non-existent cell. The rest of values (i.e., those with no previous flags) in the error flag column are left\n        as null.\n        The auxiliar Point geometry column will be set to null for these flagged events.\n\n        Args:\n            df (DataFrame): DataFrame in which invalid cells will be flagged\n\n        Returns:\n            DataFrame: DataFrame with flagged invalid cells\n        \"\"\"\n        df = df.withColumn(\n            ColNames.error_flag,\n            # Leave already flagged rows as is\n            F.when(\n                F.col(ColNames.error_flag).isNotNull(), F.col(ColNames.error_flag)\n            ).when(  # event happened before the cell was operational, or after the cell was operational\n                (\n                    (\n                        F.col(ColNames.valid_date_start).isNotNull()\n                        &amp; (F.col(ColNames.timestamp) &lt; F.col(ColNames.valid_date_start))\n                    )\n                    | (\n                        F.col(ColNames.valid_date_end).isNotNull()\n                        &amp; (F.col(ColNames.timestamp) &gt; F.col(ColNames.valid_date_end))\n                    )\n                ),\n                F.lit(SemanticErrorType.CELL_ID_NOT_VALID),\n            ),\n        )\n\n        df = df.withColumn(\n            \"geometry\",\n            F.when(F.col(ColNames.error_flag).isNotNull(), F.lit(None)).otherwise(F.col(\"geometry\")),\n        )\n        return df\n\n    def _flag_by_event_location(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Method that finds and flags events that are considered to be suspicious or incorrect in\n        terms of their timestamp and cell location with respect to their previous and/or following\n        events.\n        It is assumed that these are the last flags to be raised. Thus, non-flagged events are also\n        set to the no-error-flag value within this method.\n        Args:\n            df (DataFrame): DataFrame in which suspicious and/or incorrect events based on\n                location are to be found and flagged\n\n        Returns:\n            DataFrame: flagged DataFrame with suspicious and/or incorrect events\n        \"\"\"\n        # Windows that comprise all previous (following) rows ordered by time for each user.\n        # Partition pruning\n        # These windows have to be used, as all records have to be kept, and we skip them\n        forward_window = (\n            Window.partitionBy(\n                [\n                    ColNames.year,\n                    ColNames.month,\n                    ColNames.day,\n                    ColNames.user_id_modulo,\n                    ColNames.user_id,\n                ]\n            )\n            .orderBy(ColNames.timestamp)\n            .rowsBetween(Window.currentRow + 1, Window.unboundedFollowing)\n        )\n        backward_window = (\n            Window.partitionBy(\n                [\n                    ColNames.year,\n                    ColNames.month,\n                    ColNames.day,\n                    ColNames.user_id_modulo,\n                    ColNames.user_id,\n                ]\n            )\n            .orderBy(ColNames.timestamp)\n            .rowsBetween(Window.unboundedPreceding, Window.currentRow - 1)\n        )\n\n        # Columns to be evaluated for flags\n        # The order of the definition of these columns appears to affect the physical plan\n        # TODO: find best ordering\n        df = (\n            df\n            # auxiliar column containing timestamps of non-flagged events, and null for flagged events\n            .withColumn(\n                \"filtered_ts\",\n                F.when(F.col(ColNames.error_flag).isNull(), F.col(ColNames.timestamp)).otherwise(None),\n            )\n            .withColumn(\n                \"next_timediff\",  # time b/w curr event and first following non-flagged event timestamp\n                F.when(\n                    F.col(ColNames.error_flag).isNull(),\n                    (\n                        F.first(F.col(\"filtered_ts\"), ignorenulls=True).over(forward_window).cast(LongType())\n                        - F.col(ColNames.timestamp).cast(LongType())\n                    ),\n                ).otherwise(F.lit(None)),\n            )\n            .withColumn(\n                \"prev_timediff\",  # time b/w curr event and last previous non-flagged event timestamp\n                F.when(\n                    F.col(ColNames.error_flag).isNull(),\n                    (\n                        F.col(ColNames.timestamp).cast(LongType())\n                        - F.last(F.col(\"filtered_ts\"), ignorenulls=True).over(backward_window).cast(LongType())\n                    ),\n                ).otherwise(F.lit(None)),\n            )\n            .withColumn(\n                \"next_distance\",  # distance b/w curr location and first following non-flagged event location\n                F.when(\n                    F.col(ColNames.error_flag).isNull(),  # rows not flagged (yet)\n                    STF.ST_DistanceSpheroid(\n                        F.col(\"geometry\"),\n                        F.first(F.col(\"geometry\"), ignorenulls=True).over(forward_window),\n                    ),\n                ).otherwise(F.lit(None)),\n            )\n            .withColumn(\n                \"prev_distance\",  # distance b/w curr location and last previous non-flagged event location\n                F.when(\n                    F.col(ColNames.error_flag).isNull(),  # rows not flagged (yet)\n                    STF.ST_DistanceSpheroid(\n                        F.col(\"geometry\"),\n                        F.last(F.col(\"geometry\"), ignorenulls=True).over(backward_window),\n                    ),\n                ).otherwise(F.lit(None)),\n            )\n            .withColumn(  # mean speed in m/s for euclidean displacemente b/w curr and next non-flagged events\n                \"next_speed\", F.col(\"next_distance\") / F.col(\"next_timediff\")\n            )\n            .withColumn(  # mean speed in m/s for euclidean displacemente b/w curr and previous non-flagged events\n                \"prev_speed\", F.col(\"prev_distance\") / F.col(\"prev_timediff\")\n            )\n        )\n\n        # Conditions that must occur for the two location related error flags\n        incorrect_location_cond = (\n            (F.col(\"prev_speed\") &gt; self.semantic_min_speed)\n            &amp; (F.col(\"prev_distance\") &gt; self.semantic_min_distance)\n            &amp; (F.col(\"next_speed\") &gt; self.semantic_min_speed)\n            &amp; (F.col(\"next_distance\") &gt; self.semantic_min_distance)\n        )\n\n        suspicious_location_cond = F.coalesce(\n            (F.col(\"prev_speed\") &gt; self.semantic_min_speed) &amp; (F.col(\"prev_distance\") &gt; self.semantic_min_distance),\n            F.lit(False),\n        ) | F.coalesce(\n            (F.col(\"next_speed\") &gt; self.semantic_min_speed) &amp; (F.col(\"next_distance\") &gt; self.semantic_min_distance),\n            F.lit(False),\n        )\n        # Set the error flags.\n        # NOTE: it is assumed that this is the last flag to be computed. Thus, all non-flagged events\n        # will be set to the code corresponding to no error flags. If new flags are to be added, one might\n        # want to change this.\n        df = df.withColumn(\n            ColNames.error_flag,\n            F.when(F.col(ColNames.error_flag).isNotNull(), F.col(ColNames.error_flag))\n            .when(incorrect_location_cond, F.lit(SemanticErrorType.INCORRECT_EVENT_LOCATION))\n            .when(suspicious_location_cond, F.lit(SemanticErrorType.SUSPICIOUS_EVENT_LOCATION)),\n        )\n\n        return df\n\n    def _flag_non_errors(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Marks all rows where error type is null as NO_ERROR.\n\n        Args:\n            df (DataFrame): Dataframe of event records with error_flag column.\n\n        Returns:\n            DataFrame: Same dataframe with error_flag set to NO_ERROR code where it was null before.\n        \"\"\"\n        df = df.withColumn(\n            ColNames.error_flag,\n            F.when(F.col(ColNames.error_flag).isNull(), F.lit(SemanticErrorType.NO_ERROR)).otherwise(\n                F.col(ColNames.error_flag)\n            ),\n        )\n        return df\n\n    def _flag_different_location_duplicates(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Method that checkes for duplicates of a different location type and flags these rows with the corresponding error flag.\n        A different location duplicate is such where user_id and timestamp columns are identical,\n        but any of the cell_id, latitude or longitude columns are different.\n        In the current implementation, all column rows are counted for a given partition of user_id_modulo, user_id and timestamp.\n\n        Args:\n            df (DataFrame): PySpark DataFrame resulting from the left join of events and cells\n\n        Returns:\n            DataFrame: same DataFrame as the input with a new error flag column, containing\n                flags for the events with identical timestamps, but different cell_id or latitude or longitude column values\n        \"\"\"\n\n        # Hash the columns that define a unique location\n        df = df.withColumn(\n            \"location_hash\", F.hash(ColNames.cell_id, ColNames.latitude, ColNames.longitude, ColNames.plmn)\n        )\n\n        # Define a window partitioned by user_id only\n        window = Window.partitionBy(\n            ColNames.year, ColNames.month, ColNames.day, ColNames.user_id_modulo, ColNames.user_id\n        ).orderBy(ColNames.timestamp)\n\n        # Mark rows with the same user_id and timestamp but different location hashes as duplicates\n        df = df.withColumn(\n            ColNames.error_flag,\n            F.when(\n                (\n                    (F.lag(\"location_hash\", 1).over(window) != F.col(\"location_hash\"))\n                    &amp; (F.lag(ColNames.timestamp, 1).over(window) == F.col(ColNames.timestamp))\n                )\n                | (\n                    (F.lead(\"location_hash\", 1).over(window) != F.col(\"location_hash\"))\n                    &amp; (F.lead(ColNames.timestamp, 1).over(window) == F.col(ColNames.timestamp))\n                ),\n                F.lit(SemanticErrorType.DIFFERENT_LOCATION_DUPLICATE),\n            ).otherwise(F.col(ColNames.error_flag)),\n        )\n\n        return df\n\n    def _compute_semantic_metrics(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Method that computes the semantic quality metrics of the semantic checks.\n        This amounts to counting the number of flagged events after the semantic checks.\n\n        Args:\n            df (DataFrame): Flagged event DataFrame\n\n        Returns:\n            DataFrame: semantic metrics DataFrame\n        \"\"\"\n        metrics_df = (\n            df.groupby(ColNames.error_flag)\n            .agg(F.count(F.col(ColNames.error_flag)).alias(ColNames.value))\n            .withColumnRenamed(ColNames.error_flag, ColNames.type_of_error)\n            .withColumns(\n                {\n                    ColNames.variable: F.lit(ColNames.cell_id),  # currently, only cell_id here\n                    ColNames.year: F.lit(self.date_of_study.year).cast(ShortType()),\n                    ColNames.month: F.lit(self.date_of_study.month).cast(ByteType()),\n                    ColNames.day: F.lit(self.date_of_study.day).cast(ByteType()),\n                }\n            )\n        )\n\n        all_error_codes = [error_name for error_name in dir(SemanticErrorType) if not error_name.startswith(\"__\")]\n        all_error_codes = [\n            Row(\n                **{\n                    ColNames.type_of_error: getattr(SemanticErrorType, error_name),\n                }\n            )\n            for error_name in all_error_codes\n        ]\n\n        all_errors_df = self.spark.createDataFrame(\n            all_error_codes,\n            schema=StructType([SilverEventSemanticQualityMetrics.SCHEMA[ColNames.type_of_error]]),\n        )\n\n        metrics_df = (\n            metrics_df.join(all_errors_df, on=ColNames.type_of_error, how=\"right\")\n            .fillna(\n                {\n                    ColNames.variable: ColNames.cell_id,\n                    ColNames.value: 0,\n                    ColNames.year: self.date_of_study.year,\n                    ColNames.month: self.date_of_study.month,\n                    ColNames.day: self.date_of_study.day,\n                }\n            )\n            .withColumn(ColNames.result_timestamp, F.lit(self.timestamp))\n        )\n\n        return metrics_df\n\n    @get_execution_stats\n    def execute(self):\n        \"\"\"\n        Method that performs the read, transform and write methods of the component.\n        \"\"\"\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n\n        for date in self.data_period_dates:\n            self.date_of_study = date\n\n            self.events_df = self.input_data_objects[SilverEventDataObject.ID].df.filter(\n                F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n                == F.lit(self.date_of_study)\n            )\n            self.cells_df = self.input_data_objects[SilverNetworkDataObject.ID].df.filter(\n                F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n                == F.lit(self.date_of_study)\n            )\n\n            self.logger.info(f\"Processing data for {date}\")\n            self.transform()\n            self.write()\n            self.spark.catalog.clearCache()\n            self.logger.info(f\"Finished {date}\")\n\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n</code></pre>"},{"location":"reference/components/execution/event_semantic_cleaning/event_semantic_cleaning/#components.execution.event_semantic_cleaning.event_semantic_cleaning.SemanticCleaning.execute","title":"<code>execute()</code>","text":"<p>Method that performs the read, transform and write methods of the component.</p> Source code in <code>multimno/components/execution/event_semantic_cleaning/event_semantic_cleaning.py</code> <pre><code>@get_execution_stats\ndef execute(self):\n    \"\"\"\n    Method that performs the read, transform and write methods of the component.\n    \"\"\"\n    self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n    self.read()\n\n    for date in self.data_period_dates:\n        self.date_of_study = date\n\n        self.events_df = self.input_data_objects[SilverEventDataObject.ID].df.filter(\n            F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n            == F.lit(self.date_of_study)\n        )\n        self.cells_df = self.input_data_objects[SilverNetworkDataObject.ID].df.filter(\n            F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n            == F.lit(self.date_of_study)\n        )\n\n        self.logger.info(f\"Processing data for {date}\")\n        self.transform()\n        self.write()\n        self.spark.catalog.clearCache()\n        self.logger.info(f\"Finished {date}\")\n\n    self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n</code></pre>"},{"location":"reference/components/execution/geozones_grid_mapping/","title":"geozones_grid_mapping","text":""},{"location":"reference/components/execution/geozones_grid_mapping/geozones_grid_mapping/","title":"geozones_grid_mapping","text":"<p>This module is responsible for enrichment of the operational grid with elevation and landuse data.</p>"},{"location":"reference/components/execution/geozones_grid_mapping/geozones_grid_mapping/#components.execution.geozones_grid_mapping.geozones_grid_mapping.GeozonesGridMapping","title":"<code>GeozonesGridMapping</code>","text":"<p>               Bases: <code>Component</code></p> <p>This class is responsible for mapping of zoning data to the operational grid.</p> Source code in <code>multimno/components/execution/geozones_grid_mapping/geozones_grid_mapping.py</code> <pre><code>class GeozonesGridMapping(Component):\n    \"\"\"\n    This class is responsible for mapping of zoning data to the operational grid.\n    \"\"\"\n\n    COMPONENT_ID = \"GeozonesGridMapping\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.zoning_dataset_ids = self.config.geteval(GeozonesGridMapping.COMPONENT_ID, \"dataset_ids\")\n\n    def initalize_data_objects(self):\n\n        # inputs\n        self.clear_destination_directory = self.config.getboolean(\n            GeozonesGridMapping.COMPONENT_ID, \"clear_destination_directory\"\n        )\n        self.zoning_type = self.config.get(GeozonesGridMapping.COMPONENT_ID, \"zoning_type\")\n\n        if self.zoning_type == \"admin\":\n            zoning_data = {\"admin_units_data_bronze\": BronzeAdminUnitsDataObject}\n        elif self.zoning_type == \"other\":\n            zoning_data = {\"geographic_zones_data_bronze\": BronzeGeographicZonesDataObject}\n\n        self.input_data_objects = {}\n        inputs = {\n            \"grid_data_silver\": SilverGridDataObject,\n        } | zoning_data\n\n        for key, value in inputs.items():\n            if self.config.has_option(CONFIG_BRONZE_PATHS_KEY, key):\n                path = self.config.get(CONFIG_BRONZE_PATHS_KEY, key)\n            else:\n                path = self.config.get(CONFIG_SILVER_PATHS_KEY, key)\n            if check_if_data_path_exists(self.spark, path):\n                self.input_data_objects[value.ID] = value(self.spark, path)\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")\n                raise ValueError(f\"Invalid path for {value.ID}: {path}\")\n\n        # outputs\n\n        grid_do_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"geozones_grid_map_data_silver\")\n\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, grid_do_path)\n\n        self.output_data_objects = {}\n        self.output_data_objects[SilverGeozonesGridMapDataObject.ID] = SilverGeozonesGridMapDataObject(\n            self.spark,\n            grid_do_path,\n        )\n\n    @get_execution_stats\n    def execute(self):\n\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n\n        # iterate over each dataset_id and do mapping separately\n        for dataset_id in self.zoning_dataset_ids:\n            self.logger.info(f\"Starting mapping for {dataset_id} dataset...\")\n            self.current_dataset_id = dataset_id\n            self.read()\n            self.transform()\n            self.write()\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}...\")\n\n        current_zoning_sdf = self.input_data_objects[BronzeGeographicZonesDataObject.ID].df\n        current_zoning_sdf = current_zoning_sdf.filter(\n            current_zoning_sdf[ColNames.dataset_id].isin(self.current_dataset_id)\n        )\n        grid_sdf = self.input_data_objects[SilverGridDataObject.ID].df\n\n        zoning_levels = self.get_hierarchy_levels(current_zoning_sdf)\n\n        zone_grid_sdf = self.map_zoning_units_to_grid(grid_sdf, current_zoning_sdf, zoning_levels)\n\n        zone_grid_sdf = self.extract_hierarchy_ids(zone_grid_sdf, current_zoning_sdf, zoning_levels)\n\n        # get year, month, day from the current_zone_sdf year, month, day columns, assign to the zone_grid_sdf\n        first_row = current_zoning_sdf.first()\n\n        zone_grid_sdf = zone_grid_sdf.withColumn(ColNames.year, F.lit(first_row[ColNames.year]))\n        zone_grid_sdf = zone_grid_sdf.withColumn(ColNames.month, F.lit(first_row[ColNames.month]))\n        zone_grid_sdf = zone_grid_sdf.withColumn(ColNames.day, F.lit(first_row[ColNames.day]))\n\n        zone_grid_sdf = zone_grid_sdf.withColumn(ColNames.dataset_id, F.lit(self.current_dataset_id))\n\n        zone_grid_sdf = utils.apply_schema_casting(zone_grid_sdf, SilverGeozonesGridMapDataObject.SCHEMA)\n\n        self.output_data_objects[SilverGeozonesGridMapDataObject.ID].df = zone_grid_sdf\n\n    @staticmethod\n    def get_hierarchy_levels(zone_units_df: DataFrame) -&gt; Dict[str, int]:\n        \"\"\"\n        Returns the distinct hierarchy levels of the zoning units in a sorted order.\n\n        This method takes a DataFrame of zoning units, selects the distinct values in the 'level' column,\n        and returns these levels in a sorted list.\n\n        Args:\n            zone_units_df (DataFrame): A DataFrame containing zoning units data.\n                                    It is expected to have a column named 'level' which\n                                    indicates the hierarchy level of each zoning unit.\n\n        Returns:\n            list: A sorted list of distinct hierarchy levels of the zoning units.\n        \"\"\"\n        levels = [row[ColNames.level] for row in zone_units_df.select(ColNames.level).distinct().collect()]\n\n        return sorted(levels)\n\n    def map_zoning_units_to_grid(\n        self, grid_sdf: DataFrame, zoning_units_df: DataFrame, zoning_levels: list\n    ) -&gt; DataFrame:\n        \"\"\"\n        Maps zoning units to a grid.\n\n        This method takes a DataFrame of grid data and a DataFrame of zoning units data,\n        and maps the zoning units to the grid. The mapping is done based on the maximum zoning level.\n        The method returns a DataFrame that contains the grid data with an additional column for the zoning unit ID.\n        If a grid cell does not intersect with any zoning unit, the zoning unit ID for that cell is set to 'undefined'.\n\n        Args:\n            grid_sdf (DataFrame): A DataFrame containing grid data.\n                                  It is expected to have a column named 'geometry' which indicates the geometry of each grid cell.\n            zoning_units_df (DataFrame): A DataFrame containing zoning units data.\n                                         It is expected to have columns named 'level' and 'geometry'\n                                         which indicate the hierarchy level and the geometry of each zoning unit, respectively.\n            zoning_levels (list): A list of zoning levels.\n\n        Returns:\n            DataFrame: A DataFrame that contains the grid data with an additional column for the zoning unit ID.\n        \"\"\"\n        zoning_units_df = zoning_units_df.filter(F.col(ColNames.level) == max(zoning_levels))\n\n        intersection_sdf = (\n            grid_sdf.alias(\"a\")\n            .join(\n                F.broadcast(zoning_units_df.alias(\"b\")),\n                STP.ST_Intersects(f\"a.{ColNames.geometry}\", f\"b.{ColNames.geometry}\"),\n            )\n            .select(\"a.*\", f\"b.{ColNames.zone_id}\")\n        )\n\n        non_intersection_sdf = grid_sdf.alias(\"a\").join(\n            intersection_sdf.alias(\"b\").select(\n                ColNames.grid_id,\n            ),\n            F.col(f\"a.{ColNames.grid_id}\") == F.col(f\"b.{ColNames.grid_id}\"),\n            \"left_anti\",\n        )\n\n        non_intersection_sdf = non_intersection_sdf.withColumn(ColNames.zone_id, F.lit(\"undefined\"))\n\n        lowest_zone_grid_sdf = intersection_sdf.union(non_intersection_sdf)\n\n        return lowest_zone_grid_sdf\n\n    def extract_hierarchy_ids(\n        self, zone_grid_sdf: DataFrame, zone_units_sdf: DataFrame, zoning_levels: list\n    ) -&gt; DataFrame:\n        \"\"\"\n        Extracts the hierarchy IDs for all zoning levels.\n\n        This method takes a DataFrame of grid data mapped to zoning units and a DataFrame of zoning units data,\n        and extracts the hierarchy IDs for all zoning levels. The hierarchy ID for a grid tile is a string that\n        contains the IDs of the zoning units that the tile intersects with, ordered by the zoning level.\n\n        Args:\n            zone_grid_sdf (DataFrame): A DataFrame containing grid data mapped to zoning units.\n                                       It is expected to have column named 'zone_id' which represents zone id on the lowest level.\n            zone_units_sdf (DataFrame): A DataFrame containing zoning units data.\n                                        It is expected to have columns named 'level', 'zone_id', and 'parent_id'.\n            zoning_levels (int): The number of zoning levels.\n\n        Returns:\n            DataFrame: A DataFrame that contains the grid data with an additional column for the hierarchy ID.\n        \"\"\"\n\n        for level in reversed(zoning_levels):\n\n            current_level_zone_units_sdf = zone_units_sdf.filter(F.col(ColNames.level) == level)\n            if level == max(zoning_levels):\n\n                zone_grid_sdf = zone_grid_sdf.withColumn(ColNames.hierarchical_id, F.col(ColNames.zone_id))\n                zone_grid_sdf = zone_grid_sdf.alias(\"a\").join(\n                    current_level_zone_units_sdf.alias(\"b\"),\n                    F.col(f\"a.{ColNames.zone_id}\") == F.col(f\"b.{ColNames.zone_id}\"),\n                    \"left\",\n                )\n                zone_grid_sdf = zone_grid_sdf.withColumn(\"next_level_join_id\", F.col(f\"b.{ColNames.parent_id}\"))\n\n                zone_grid_sdf = zone_grid_sdf.select(\"a.*\", \"next_level_join_id\")\n\n            else:\n                zone_grid_sdf = zone_grid_sdf.alias(\"a\").join(\n                    current_level_zone_units_sdf.alias(\"b\"),\n                    F.col(f\"a.next_level_join_id\") == F.col(f\"b.{ColNames.zone_id}\"),\n                    \"left\",\n                )\n\n                zone_grid_sdf = zone_grid_sdf.withColumn(\n                    ColNames.hierarchical_id,\n                    F.when(\n                        F.col(f\"b.{ColNames.zone_id}\").isNotNull(),\n                        F.concat(\n                            F.col(f\"b.{ColNames.zone_id}\"),\n                            F.lit(\"|\"),\n                            F.col(f\"{ColNames.hierarchical_id}\"),\n                        ),\n                    ).otherwise(\n                        F.concat(\n                            F.lit(\"undefined\"),\n                            F.lit(\"|\"),\n                            F.col(f\"a.{ColNames.hierarchical_id}\"),\n                        )\n                    ),\n                )\n                zone_grid_sdf = zone_grid_sdf.withColumn(\"next_level_join_id\", F.col(f\"b.{ColNames.parent_id}\"))\n                zone_grid_sdf = zone_grid_sdf.select(\"a.*\", \"next_level_join_id\", ColNames.hierarchical_id)\n\n        return zone_grid_sdf\n</code></pre>"},{"location":"reference/components/execution/geozones_grid_mapping/geozones_grid_mapping/#components.execution.geozones_grid_mapping.geozones_grid_mapping.GeozonesGridMapping.extract_hierarchy_ids","title":"<code>extract_hierarchy_ids(zone_grid_sdf, zone_units_sdf, zoning_levels)</code>","text":"<p>Extracts the hierarchy IDs for all zoning levels.</p> <p>This method takes a DataFrame of grid data mapped to zoning units and a DataFrame of zoning units data, and extracts the hierarchy IDs for all zoning levels. The hierarchy ID for a grid tile is a string that contains the IDs of the zoning units that the tile intersects with, ordered by the zoning level.</p> <p>Parameters:</p> Name Type Description Default <code>zone_grid_sdf</code> <code>DataFrame</code> <p>A DataFrame containing grid data mapped to zoning units.                        It is expected to have column named 'zone_id' which represents zone id on the lowest level.</p> required <code>zone_units_sdf</code> <code>DataFrame</code> <p>A DataFrame containing zoning units data.                         It is expected to have columns named 'level', 'zone_id', and 'parent_id'.</p> required <code>zoning_levels</code> <code>int</code> <p>The number of zoning levels.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame that contains the grid data with an additional column for the hierarchy ID.</p> Source code in <code>multimno/components/execution/geozones_grid_mapping/geozones_grid_mapping.py</code> <pre><code>def extract_hierarchy_ids(\n    self, zone_grid_sdf: DataFrame, zone_units_sdf: DataFrame, zoning_levels: list\n) -&gt; DataFrame:\n    \"\"\"\n    Extracts the hierarchy IDs for all zoning levels.\n\n    This method takes a DataFrame of grid data mapped to zoning units and a DataFrame of zoning units data,\n    and extracts the hierarchy IDs for all zoning levels. The hierarchy ID for a grid tile is a string that\n    contains the IDs of the zoning units that the tile intersects with, ordered by the zoning level.\n\n    Args:\n        zone_grid_sdf (DataFrame): A DataFrame containing grid data mapped to zoning units.\n                                   It is expected to have column named 'zone_id' which represents zone id on the lowest level.\n        zone_units_sdf (DataFrame): A DataFrame containing zoning units data.\n                                    It is expected to have columns named 'level', 'zone_id', and 'parent_id'.\n        zoning_levels (int): The number of zoning levels.\n\n    Returns:\n        DataFrame: A DataFrame that contains the grid data with an additional column for the hierarchy ID.\n    \"\"\"\n\n    for level in reversed(zoning_levels):\n\n        current_level_zone_units_sdf = zone_units_sdf.filter(F.col(ColNames.level) == level)\n        if level == max(zoning_levels):\n\n            zone_grid_sdf = zone_grid_sdf.withColumn(ColNames.hierarchical_id, F.col(ColNames.zone_id))\n            zone_grid_sdf = zone_grid_sdf.alias(\"a\").join(\n                current_level_zone_units_sdf.alias(\"b\"),\n                F.col(f\"a.{ColNames.zone_id}\") == F.col(f\"b.{ColNames.zone_id}\"),\n                \"left\",\n            )\n            zone_grid_sdf = zone_grid_sdf.withColumn(\"next_level_join_id\", F.col(f\"b.{ColNames.parent_id}\"))\n\n            zone_grid_sdf = zone_grid_sdf.select(\"a.*\", \"next_level_join_id\")\n\n        else:\n            zone_grid_sdf = zone_grid_sdf.alias(\"a\").join(\n                current_level_zone_units_sdf.alias(\"b\"),\n                F.col(f\"a.next_level_join_id\") == F.col(f\"b.{ColNames.zone_id}\"),\n                \"left\",\n            )\n\n            zone_grid_sdf = zone_grid_sdf.withColumn(\n                ColNames.hierarchical_id,\n                F.when(\n                    F.col(f\"b.{ColNames.zone_id}\").isNotNull(),\n                    F.concat(\n                        F.col(f\"b.{ColNames.zone_id}\"),\n                        F.lit(\"|\"),\n                        F.col(f\"{ColNames.hierarchical_id}\"),\n                    ),\n                ).otherwise(\n                    F.concat(\n                        F.lit(\"undefined\"),\n                        F.lit(\"|\"),\n                        F.col(f\"a.{ColNames.hierarchical_id}\"),\n                    )\n                ),\n            )\n            zone_grid_sdf = zone_grid_sdf.withColumn(\"next_level_join_id\", F.col(f\"b.{ColNames.parent_id}\"))\n            zone_grid_sdf = zone_grid_sdf.select(\"a.*\", \"next_level_join_id\", ColNames.hierarchical_id)\n\n    return zone_grid_sdf\n</code></pre>"},{"location":"reference/components/execution/geozones_grid_mapping/geozones_grid_mapping/#components.execution.geozones_grid_mapping.geozones_grid_mapping.GeozonesGridMapping.get_hierarchy_levels","title":"<code>get_hierarchy_levels(zone_units_df)</code>  <code>staticmethod</code>","text":"<p>Returns the distinct hierarchy levels of the zoning units in a sorted order.</p> <p>This method takes a DataFrame of zoning units, selects the distinct values in the 'level' column, and returns these levels in a sorted list.</p> <p>Parameters:</p> Name Type Description Default <code>zone_units_df</code> <code>DataFrame</code> <p>A DataFrame containing zoning units data.                     It is expected to have a column named 'level' which                     indicates the hierarchy level of each zoning unit.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>Dict[str, int]</code> <p>A sorted list of distinct hierarchy levels of the zoning units.</p> Source code in <code>multimno/components/execution/geozones_grid_mapping/geozones_grid_mapping.py</code> <pre><code>@staticmethod\ndef get_hierarchy_levels(zone_units_df: DataFrame) -&gt; Dict[str, int]:\n    \"\"\"\n    Returns the distinct hierarchy levels of the zoning units in a sorted order.\n\n    This method takes a DataFrame of zoning units, selects the distinct values in the 'level' column,\n    and returns these levels in a sorted list.\n\n    Args:\n        zone_units_df (DataFrame): A DataFrame containing zoning units data.\n                                It is expected to have a column named 'level' which\n                                indicates the hierarchy level of each zoning unit.\n\n    Returns:\n        list: A sorted list of distinct hierarchy levels of the zoning units.\n    \"\"\"\n    levels = [row[ColNames.level] for row in zone_units_df.select(ColNames.level).distinct().collect()]\n\n    return sorted(levels)\n</code></pre>"},{"location":"reference/components/execution/geozones_grid_mapping/geozones_grid_mapping/#components.execution.geozones_grid_mapping.geozones_grid_mapping.GeozonesGridMapping.map_zoning_units_to_grid","title":"<code>map_zoning_units_to_grid(grid_sdf, zoning_units_df, zoning_levels)</code>","text":"<p>Maps zoning units to a grid.</p> <p>This method takes a DataFrame of grid data and a DataFrame of zoning units data, and maps the zoning units to the grid. The mapping is done based on the maximum zoning level. The method returns a DataFrame that contains the grid data with an additional column for the zoning unit ID. If a grid cell does not intersect with any zoning unit, the zoning unit ID for that cell is set to 'undefined'.</p> <p>Parameters:</p> Name Type Description Default <code>grid_sdf</code> <code>DataFrame</code> <p>A DataFrame containing grid data.                   It is expected to have a column named 'geometry' which indicates the geometry of each grid cell.</p> required <code>zoning_units_df</code> <code>DataFrame</code> <p>A DataFrame containing zoning units data.                          It is expected to have columns named 'level' and 'geometry'                          which indicate the hierarchy level and the geometry of each zoning unit, respectively.</p> required <code>zoning_levels</code> <code>list</code> <p>A list of zoning levels.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame that contains the grid data with an additional column for the zoning unit ID.</p> Source code in <code>multimno/components/execution/geozones_grid_mapping/geozones_grid_mapping.py</code> <pre><code>def map_zoning_units_to_grid(\n    self, grid_sdf: DataFrame, zoning_units_df: DataFrame, zoning_levels: list\n) -&gt; DataFrame:\n    \"\"\"\n    Maps zoning units to a grid.\n\n    This method takes a DataFrame of grid data and a DataFrame of zoning units data,\n    and maps the zoning units to the grid. The mapping is done based on the maximum zoning level.\n    The method returns a DataFrame that contains the grid data with an additional column for the zoning unit ID.\n    If a grid cell does not intersect with any zoning unit, the zoning unit ID for that cell is set to 'undefined'.\n\n    Args:\n        grid_sdf (DataFrame): A DataFrame containing grid data.\n                              It is expected to have a column named 'geometry' which indicates the geometry of each grid cell.\n        zoning_units_df (DataFrame): A DataFrame containing zoning units data.\n                                     It is expected to have columns named 'level' and 'geometry'\n                                     which indicate the hierarchy level and the geometry of each zoning unit, respectively.\n        zoning_levels (list): A list of zoning levels.\n\n    Returns:\n        DataFrame: A DataFrame that contains the grid data with an additional column for the zoning unit ID.\n    \"\"\"\n    zoning_units_df = zoning_units_df.filter(F.col(ColNames.level) == max(zoning_levels))\n\n    intersection_sdf = (\n        grid_sdf.alias(\"a\")\n        .join(\n            F.broadcast(zoning_units_df.alias(\"b\")),\n            STP.ST_Intersects(f\"a.{ColNames.geometry}\", f\"b.{ColNames.geometry}\"),\n        )\n        .select(\"a.*\", f\"b.{ColNames.zone_id}\")\n    )\n\n    non_intersection_sdf = grid_sdf.alias(\"a\").join(\n        intersection_sdf.alias(\"b\").select(\n            ColNames.grid_id,\n        ),\n        F.col(f\"a.{ColNames.grid_id}\") == F.col(f\"b.{ColNames.grid_id}\"),\n        \"left_anti\",\n    )\n\n    non_intersection_sdf = non_intersection_sdf.withColumn(ColNames.zone_id, F.lit(\"undefined\"))\n\n    lowest_zone_grid_sdf = intersection_sdf.union(non_intersection_sdf)\n\n    return lowest_zone_grid_sdf\n</code></pre>"},{"location":"reference/components/execution/grid_enrichment/","title":"grid_enrichment","text":""},{"location":"reference/components/execution/grid_enrichment/grid_enrichment/","title":"grid_enrichment","text":"<p>This module is responsible for enrichment of the operational grid with elevation and landuse data.</p>"},{"location":"reference/components/execution/grid_enrichment/grid_enrichment/#components.execution.grid_enrichment.grid_enrichment.GridEnrichment","title":"<code>GridEnrichment</code>","text":"<p>               Bases: <code>Component</code></p> <p>This class is responsible for enrichment of the operational grid with elevation and landuse data.</p> Source code in <code>multimno/components/execution/grid_enrichment/grid_enrichment.py</code> <pre><code>class GridEnrichment(Component):\n    \"\"\"\n    This class is responsible for enrichment of the operational grid with elevation and landuse data.\n    \"\"\"\n\n    COMPONENT_ID = \"GridEnrichment\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.do_landuse_enrichment = self.config.getboolean(self.COMPONENT_ID, \"do_landuse_enrichment\")\n        self.transportation_category_buffer_m = self.config.geteval(\n            self.COMPONENT_ID, \"transportation_category_buffer_m\"\n        )\n\n        self.do_elevation_enrichment = self.config.getboolean(self.COMPONENT_ID, \"do_elevation_enrichment\")\n\n        self.quadkey_batch_size = self.config.getint(self.COMPONENT_ID, \"quadkey_batch_size\")\n\n        # TODO: For now set to default 100, but can be dynamic from config in future\n        self.grid_resolution = 100\n\n        self.grid_generator = InspireGridGenerator(self.spark)\n\n    def initalize_data_objects(self):\n\n        # inputs\n        self.clear_destination_directory = self.config.getboolean(\n            GridEnrichment.COMPONENT_ID, \"clear_destination_directory\"\n        )\n\n        self.input_data_objects = {}\n        inputs = {\n            \"grid_data_silver\": SilverGridDataObject,\n            \"transportation_data_bronze\": BronzeTransportationDataObject,\n            \"landuse_data_bronze\": BronzeLanduseDataObject,\n        }\n\n        for key, value in inputs.items():\n            if self.config.has_option(CONFIG_BRONZE_PATHS_KEY, key):\n                path = self.config.get(CONFIG_BRONZE_PATHS_KEY, key)\n            else:\n                path = self.config.get(CONFIG_SILVER_PATHS_KEY, key)\n            if check_if_data_path_exists(self.spark, path):\n                self.input_data_objects[value.ID] = value(self.spark, path)\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")\n                raise ValueError(f\"Invalid path for {value.ID}: {path}\")\n\n        # outputs\n\n        grid_do_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"enriched_grid_data_silver\")\n\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, grid_do_path)\n\n        self.output_data_objects = {}\n        self.output_data_objects[SilverEnrichedGridDataObject.ID] = SilverEnrichedGridDataObject(\n            self.spark, grid_do_path\n        )\n\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n        # get quadkeys to process to list\n        self.quadkeys_to_process = sorted(\n            self.input_data_objects[SilverGridDataObject.ID]\n            .df.select(\"quadkey\")\n            .distinct()\n            .rdd.map(lambda x: x.quadkey)\n            .collect()\n        )\n        self.logger.info(\"Quadkeys to process: \")\n        self.logger.info(f\"{self.quadkeys_to_process}\")\n        # generate quadkey batches\n        self.quadkey_batches = self.generate_batches(self.quadkeys_to_process, self.quadkey_batch_size)\n        self.logger.info(f\"Will be processrd in: {len(self.quadkey_batches)} parts\")\n        processed = 0\n        self.origin = self.input_data_objects[SilverGridDataObject.ID].df.select(ColNames.origin).first().origin\n        for quadkey_batch in self.quadkey_batches:\n            self.logger.info(f\"Processing quadkeys {quadkey_batch}\")\n            self.current_quadkey_batch = quadkey_batch\n            self.transform()\n            self.write()\n            self.spark.catalog.clearCache()\n            processed += 1\n            self.logger.info(f\"Processed {processed} out of {len(self.quadkey_batches)} batches\")\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def generate_batches(self, elements_list, batch_size):\n        \"\"\"\n        Generates batches of elements from list.\n        \"\"\"\n        return [elements_list[i : i + batch_size] for i in range(0, len(elements_list), batch_size)]\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}...\")\n\n        current_grid_part_sdf = self.input_data_objects[SilverGridDataObject.ID].df.filter(\n            F.col(ColNames.quadkey).isin(self.current_quadkey_batch)\n        )\n\n        current_grid_part_sdf = current_grid_part_sdf.withColumn(ColNames.elevation, F.lit(None))\n        if self.do_elevation_enrichment:\n            current_grid_part_sdf = self.add_elevation_to_grid()\n\n        if self.do_landuse_enrichment:\n\n            # preapre transportation data\n            transportation_sdf = self.prepare_transportation_data()\n            transportation_sdf.persist()\n            transportation_count = transportation_sdf.count()\n            self.logger.info(f\"Transportation data prepared...\")\n            # # prepare landuse\n            landuse_sdf = self.input_data_objects[BronzeLanduseDataObject.ID].df.filter(\n                F.col(ColNames.quadkey).isin(self.current_quadkey_batch)\n            )\n\n            # merge landuse and transportation\n            if transportation_count &gt; 0:\n                landuse_sdf = utils.clip_polygons_with_mask_polygons(\n                    landuse_sdf,\n                    transportation_sdf,\n                    [\n                        ColNames.category,\n                        ColNames.geometry,\n                        ColNames.quadkey,\n                    ],\n                    False,\n                    ColNames.geometry,\n                )\n\n            landuse_roads_sdf = landuse_sdf.union(transportation_sdf)\n            landuse_roads_sdf.persist()\n            land_use_count = landuse_roads_sdf.count()\n            self.logger.info(f\"Landuse data prepared...\")\n\n            if land_use_count == 0:\n                self.logger.warning(\n                    f\"No landuse data found for quadkey {self.current_quadkey_batch}. Assigning default open_area\"\n                )\n                current_grid_part_sdf = current_grid_part_sdf.withColumn(\n                    ColNames.main_landuse_category, F.lit(\"open_area\")\n                ).withColumn(\n                    ColNames.landuse_area_ratios,\n                    F.create_map(F.lit(\"open_area\"), F.lit(1.0)),\n                )\n            else:\n                # count landuse types ratios in grid cells\n                current_grid_part_sdf = self.grid_generator.grid_ids_to_grid_tiles(\n                    current_grid_part_sdf, 100, origin=self.origin\n                )\n                current_grid_part_sdf = current_grid_part_sdf.withColumn(\"area\", STF.ST_Area(ColNames.geometry))\n\n                landuse_roads_sdf = landuse_roads_sdf.withColumn(\n                    ColNames.geometry, STF.ST_SubDivideExplode(ColNames.geometry, 1000)\n                )\n\n                tiles_with_landuse_sdf = self.calculate_grid_landuse_area_ratios(\n                    current_grid_part_sdf, landuse_roads_sdf\n                )\n\n                # get all other tiles that do not have landuse data\n                all_tiles_sdf = current_grid_part_sdf.join(\n                    tiles_with_landuse_sdf,\n                    on=[ColNames.grid_id, ColNames.quadkey],\n                    how=\"left\",\n                )\n\n                # and assign default open_area\n                all_tiles_sdf = all_tiles_sdf.withColumn(\n                    ColNames.category, F.coalesce(F.col(ColNames.category), F.lit(\"open_area\"))\n                ).withColumn(\"area_ratio\", F.coalesce(F.col(\"area_ratio\"), F.lit(1.0)))\n\n                # get main landuse category and landuse area ratios for each grid tile\n                current_grid_part_sdf = all_tiles_sdf.groupBy(\n                    ColNames.grid_id, ColNames.elevation, ColNames.quadkey\n                ).agg(\n                    F.map_from_entries(F.collect_list(F.struct(ColNames.category, \"area_ratio\"))).alias(\n                        ColNames.landuse_area_ratios\n                    ),\n                    F.max_by(\n                        F.when(F.col(\"area_ratio\") &gt; 0.05, F.col(\"category\")).otherwise(None), F.col(\"area_ratio\")\n                    ).alias(ColNames.main_landuse_category),\n                )\n\n                # Then also assign mauin landuse category to open_area if it is not assigned\n                current_grid_part_sdf = current_grid_part_sdf.withColumn(\n                    ColNames.main_landuse_category,\n                    F.coalesce(F.col(ColNames.main_landuse_category), F.lit(\"open_area\")),\n                )\n\n            current_grid_part_sdf = self.grid_generator.grid_ids_to_grid_centroids(\n                current_grid_part_sdf, 100, origin=self.origin\n            )\n\n        current_grid_part_sdf = utils.apply_schema_casting(current_grid_part_sdf, SilverEnrichedGridDataObject.SCHEMA)\n        current_grid_part_sdf = current_grid_part_sdf.repartition(*SilverEnrichedGridDataObject.PARTITION_COLUMNS)\n        self.output_data_objects[SilverEnrichedGridDataObject.ID].df = current_grid_part_sdf\n\n    def add_elevation_to_grid(self):\n        # TODO: implement elevation enrichment\n        pass\n\n    def prepare_transportation_data(self) -&gt; DataFrame:\n        \"\"\"\n        Prepares transportation data for mapping to a grid.\n\n        This function takes the transportation data DataFrame, filters it to the extent of the current quadkey,\n        and then calculates a buffer around each transportation feature based on its category.\n\n        Returns:\n            pyspark.sql.DataFrame: The prepared transportation data.\n        \"\"\"\n\n        transportation_sdf = self.input_data_objects[BronzeTransportationDataObject.ID].df\n        transportation_sdf = transportation_sdf.filter(F.col(ColNames.quadkey).isin(self.current_quadkey_batch))\n\n        if transportation_sdf.count() == 0:\n            self.logger.warning(f\"No transportation data found for quadkey {self.current_quadkey_batch}. Skipping\")\n\n            return self.spark.createDataFrame([], transportation_sdf.schema)\n\n        transportation_sdf = self.calculate_transportation_buffer(\n            transportation_sdf, self.transportation_category_buffer_m\n        )\n\n        # prepare quadkeys geometries to merge roads\n        child_quadkeys = [quadkey_utils.get_children_quadkeys(quadkey, 16) for quadkey in self.current_quadkey_batch]\n        child_quadkeys = reduce(operator.add, child_quadkeys, [])\n\n        quadkeys_mask_sdf = quadkey_utils.quadkeys_to_extent_dataframe(self.spark, child_quadkeys, 3035)\n        quadkeys_mask_sdf = quadkeys_mask_sdf.withColumnRenamed(ColNames.quadkey, \"mask_quadkey\")\n\n        transportation_sdf = utils.merge_geom_within_mask_geom(\n            transportation_sdf, quadkeys_mask_sdf, [ColNames.quadkey, \"mask_quadkey\"], ColNames.geometry\n        )\n\n        return transportation_sdf.select(F.lit(\"roads\").alias(ColNames.category), ColNames.geometry, ColNames.quadkey)\n\n    def calculate_transportation_buffer(\n        self, transportation_sdf: DataFrame, category_buffer_m: Dict[str, float]\n    ) -&gt; DataFrame:\n        \"\"\"\n        Calculates the buffer for each transportation feature based on its category.\n\n        This function takes a DataFrame of transportation features and a dictionary mapping categories to buffer distances.\n        It assigns the appropriate buffer distance to each feature based on its category and then calculates the buffer geometry.\n        The buffer geometry replaces the original geometry of each feature.\n\n        Args:\n            transportation_sdf (pyspark.sql.DataFrame): The DataFrame of transportation features.\n            category_buffer_m (dict): A dictionary mapping categories to buffer distances.\n\n        Returns:\n            pyspark.sql.DataFrame: The DataFrame of transportation features with the buffer geometries.\n        \"\"\"\n\n        transportation_sdf = self.assign_mapping_values(\n            transportation_sdf,\n            category_buffer_m,\n            ColNames.category,\n            \"buffer_dist\",\n            category_buffer_m[\"unknown\"],\n        )\n\n        transportation_sdf = transportation_sdf.filter(F.col(\"buffer_dist\") != 0.0)\n\n        transportation_sdf = transportation_sdf.withColumn(\n            ColNames.geometry,\n            STF.ST_Buffer(ColNames.geometry, F.col(\"buffer_dist\")),\n        ).drop(\"buffer_dist\")\n\n        return transportation_sdf\n\n    def calculate_grid_landuse_area_ratios(self, grid_tiles: DataFrame, landuse: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Finds the intersection ratio of a specific land use class for each tile in a grid.\n\n        This function takes a DataFrame of grid tiles, a DataFrame of land use data, and a land use class.\n        It finds the intersection of each grid tile with the land use data for the specified class,\n        calculates the area of the intersection, and divides this by the area of the tile to find the intersection ratio.\n        The intersection ratio is added as a new column to the grid DataFrame.\n\n        Args:\n            grid_tiles (pyspark.sql.DataFrame): The DataFrame of grid tiles.\n            landuse (pyspark.sql.DataFrame): The DataFrame of land use data.\n            landuse_class (str): The land use class to find the intersection ratio for.\n\n        Returns:\n            pyspark.sql.DataFrame: The DataFrame of grid tiles with the new column of intersection ratios.\n        \"\"\"\n\n        grid_tiles = grid_tiles.select(F.col(ColNames.geometry).alias(\"grid_geometry\"), ColNames.grid_id, \"area\").join(\n            landuse,\n            STP.ST_Intersects(\"grid_geometry\", f\"{ColNames.geometry}\"),\n        )\n\n        grid_tiles = grid_tiles.withColumn(\n            \"shared_geom\",\n            STF.ST_Intersection(\"grid_geometry\", f\"{ColNames.geometry}\"),\n        ).select(ColNames.category, ColNames.grid_id, ColNames.quadkey, \"area\", \"shared_geom\")\n\n        grid_tiles = grid_tiles.groupBy(ColNames.grid_id, ColNames.category, ColNames.quadkey, \"area\").agg(\n            F.sum(STF.ST_Area(\"shared_geom\")).alias(f\"category_area\")\n        )\n\n        grid_tiles = grid_tiles.withColumn(\"area_ratio\", F.col(f\"category_area\") / F.col(\"area\"))\n\n        # Define window spec for sum calculation per grid cell\n        window_spec = Window.partitionBy(ColNames.grid_id, ColNames.quadkey)\n\n        # Calculate total ratio and normalize areas if total ratio is greater than 1\n        grid_tiles = (\n            grid_tiles.withColumn(\"total_ratio\", F.sum(\"area_ratio\").over(window_spec))\n            .withColumn(\n                \"area_ratio\",\n                F.when(F.col(\"total_ratio\") &gt; 1.0, F.col(\"area_ratio\") / F.col(\"total_ratio\")).otherwise(\n                    F.col(\"area_ratio\")\n                ),\n            )\n            .drop(\"total_ratio\", \"category_area\", \"area\")\n        )  # Drop unnecessary columns\n\n        return grid_tiles.select(ColNames.grid_id, ColNames.category, ColNames.quadkey, \"area_ratio\")\n\n    @staticmethod\n    def assign_mapping_values(\n        sdf: DataFrame,\n        values_map: Dict[Any, Any],\n        map_column: str,\n        values_column: str,\n        default_value: Any = 2,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Assigns mapping values to a DataFrame based on a specified column.\n\n        This function takes a DataFrame, a dictionary of mapping values, a column to map from,\n        a column to map to, and a default value. It creates a new column in the DataFrame by mapping values\n        from the map column to the values column using the values map.\n        If a value in the map column is not found in the values map, the default value is used.\n\n        Args:\n            sdf (pyspark.sql.DataFrame): The DataFrame to assign mapping values to.\n            values_map (dict): The dictionary of mapping values.\n            map_column (str): The column to map from.\n            values_column (str): The column to map to.\n            default_value (any, optional): The default value to use if a value in the map column is not found in the values map. Defaults to 2.\n\n        Returns:\n            pyspark.sql.DataFrame: The DataFrame with the new column of mapped values.\n        \"\"\"\n\n        keys = list(values_map.keys())\n        reclass_expr = F.when(F.col(map_column) == (keys[0]), values_map[keys[0]])\n\n        for key in keys[1:]:\n            reclass_expr = reclass_expr.when(F.col(map_column) == (key), values_map[key])\n\n        reclass_expr = reclass_expr.otherwise(default_value)\n        sdf = sdf.withColumn(values_column, reclass_expr)\n\n        return sdf\n</code></pre>"},{"location":"reference/components/execution/grid_enrichment/grid_enrichment/#components.execution.grid_enrichment.grid_enrichment.GridEnrichment.assign_mapping_values","title":"<code>assign_mapping_values(sdf, values_map, map_column, values_column, default_value=2)</code>  <code>staticmethod</code>","text":"<p>Assigns mapping values to a DataFrame based on a specified column.</p> <p>This function takes a DataFrame, a dictionary of mapping values, a column to map from, a column to map to, and a default value. It creates a new column in the DataFrame by mapping values from the map column to the values column using the values map. If a value in the map column is not found in the values map, the default value is used.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The DataFrame to assign mapping values to.</p> required <code>values_map</code> <code>dict</code> <p>The dictionary of mapping values.</p> required <code>map_column</code> <code>str</code> <p>The column to map from.</p> required <code>values_column</code> <code>str</code> <p>The column to map to.</p> required <code>default_value</code> <code>any</code> <p>The default value to use if a value in the map column is not found in the values map. Defaults to 2.</p> <code>2</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: The DataFrame with the new column of mapped values.</p> Source code in <code>multimno/components/execution/grid_enrichment/grid_enrichment.py</code> <pre><code>@staticmethod\ndef assign_mapping_values(\n    sdf: DataFrame,\n    values_map: Dict[Any, Any],\n    map_column: str,\n    values_column: str,\n    default_value: Any = 2,\n) -&gt; DataFrame:\n    \"\"\"\n    Assigns mapping values to a DataFrame based on a specified column.\n\n    This function takes a DataFrame, a dictionary of mapping values, a column to map from,\n    a column to map to, and a default value. It creates a new column in the DataFrame by mapping values\n    from the map column to the values column using the values map.\n    If a value in the map column is not found in the values map, the default value is used.\n\n    Args:\n        sdf (pyspark.sql.DataFrame): The DataFrame to assign mapping values to.\n        values_map (dict): The dictionary of mapping values.\n        map_column (str): The column to map from.\n        values_column (str): The column to map to.\n        default_value (any, optional): The default value to use if a value in the map column is not found in the values map. Defaults to 2.\n\n    Returns:\n        pyspark.sql.DataFrame: The DataFrame with the new column of mapped values.\n    \"\"\"\n\n    keys = list(values_map.keys())\n    reclass_expr = F.when(F.col(map_column) == (keys[0]), values_map[keys[0]])\n\n    for key in keys[1:]:\n        reclass_expr = reclass_expr.when(F.col(map_column) == (key), values_map[key])\n\n    reclass_expr = reclass_expr.otherwise(default_value)\n    sdf = sdf.withColumn(values_column, reclass_expr)\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/grid_enrichment/grid_enrichment/#components.execution.grid_enrichment.grid_enrichment.GridEnrichment.calculate_grid_landuse_area_ratios","title":"<code>calculate_grid_landuse_area_ratios(grid_tiles, landuse)</code>","text":"<p>Finds the intersection ratio of a specific land use class for each tile in a grid.</p> <p>This function takes a DataFrame of grid tiles, a DataFrame of land use data, and a land use class. It finds the intersection of each grid tile with the land use data for the specified class, calculates the area of the intersection, and divides this by the area of the tile to find the intersection ratio. The intersection ratio is added as a new column to the grid DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>grid_tiles</code> <code>DataFrame</code> <p>The DataFrame of grid tiles.</p> required <code>landuse</code> <code>DataFrame</code> <p>The DataFrame of land use data.</p> required <code>landuse_class</code> <code>str</code> <p>The land use class to find the intersection ratio for.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: The DataFrame of grid tiles with the new column of intersection ratios.</p> Source code in <code>multimno/components/execution/grid_enrichment/grid_enrichment.py</code> <pre><code>def calculate_grid_landuse_area_ratios(self, grid_tiles: DataFrame, landuse: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Finds the intersection ratio of a specific land use class for each tile in a grid.\n\n    This function takes a DataFrame of grid tiles, a DataFrame of land use data, and a land use class.\n    It finds the intersection of each grid tile with the land use data for the specified class,\n    calculates the area of the intersection, and divides this by the area of the tile to find the intersection ratio.\n    The intersection ratio is added as a new column to the grid DataFrame.\n\n    Args:\n        grid_tiles (pyspark.sql.DataFrame): The DataFrame of grid tiles.\n        landuse (pyspark.sql.DataFrame): The DataFrame of land use data.\n        landuse_class (str): The land use class to find the intersection ratio for.\n\n    Returns:\n        pyspark.sql.DataFrame: The DataFrame of grid tiles with the new column of intersection ratios.\n    \"\"\"\n\n    grid_tiles = grid_tiles.select(F.col(ColNames.geometry).alias(\"grid_geometry\"), ColNames.grid_id, \"area\").join(\n        landuse,\n        STP.ST_Intersects(\"grid_geometry\", f\"{ColNames.geometry}\"),\n    )\n\n    grid_tiles = grid_tiles.withColumn(\n        \"shared_geom\",\n        STF.ST_Intersection(\"grid_geometry\", f\"{ColNames.geometry}\"),\n    ).select(ColNames.category, ColNames.grid_id, ColNames.quadkey, \"area\", \"shared_geom\")\n\n    grid_tiles = grid_tiles.groupBy(ColNames.grid_id, ColNames.category, ColNames.quadkey, \"area\").agg(\n        F.sum(STF.ST_Area(\"shared_geom\")).alias(f\"category_area\")\n    )\n\n    grid_tiles = grid_tiles.withColumn(\"area_ratio\", F.col(f\"category_area\") / F.col(\"area\"))\n\n    # Define window spec for sum calculation per grid cell\n    window_spec = Window.partitionBy(ColNames.grid_id, ColNames.quadkey)\n\n    # Calculate total ratio and normalize areas if total ratio is greater than 1\n    grid_tiles = (\n        grid_tiles.withColumn(\"total_ratio\", F.sum(\"area_ratio\").over(window_spec))\n        .withColumn(\n            \"area_ratio\",\n            F.when(F.col(\"total_ratio\") &gt; 1.0, F.col(\"area_ratio\") / F.col(\"total_ratio\")).otherwise(\n                F.col(\"area_ratio\")\n            ),\n        )\n        .drop(\"total_ratio\", \"category_area\", \"area\")\n    )  # Drop unnecessary columns\n\n    return grid_tiles.select(ColNames.grid_id, ColNames.category, ColNames.quadkey, \"area_ratio\")\n</code></pre>"},{"location":"reference/components/execution/grid_enrichment/grid_enrichment/#components.execution.grid_enrichment.grid_enrichment.GridEnrichment.calculate_transportation_buffer","title":"<code>calculate_transportation_buffer(transportation_sdf, category_buffer_m)</code>","text":"<p>Calculates the buffer for each transportation feature based on its category.</p> <p>This function takes a DataFrame of transportation features and a dictionary mapping categories to buffer distances. It assigns the appropriate buffer distance to each feature based on its category and then calculates the buffer geometry. The buffer geometry replaces the original geometry of each feature.</p> <p>Parameters:</p> Name Type Description Default <code>transportation_sdf</code> <code>DataFrame</code> <p>The DataFrame of transportation features.</p> required <code>category_buffer_m</code> <code>dict</code> <p>A dictionary mapping categories to buffer distances.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: The DataFrame of transportation features with the buffer geometries.</p> Source code in <code>multimno/components/execution/grid_enrichment/grid_enrichment.py</code> <pre><code>def calculate_transportation_buffer(\n    self, transportation_sdf: DataFrame, category_buffer_m: Dict[str, float]\n) -&gt; DataFrame:\n    \"\"\"\n    Calculates the buffer for each transportation feature based on its category.\n\n    This function takes a DataFrame of transportation features and a dictionary mapping categories to buffer distances.\n    It assigns the appropriate buffer distance to each feature based on its category and then calculates the buffer geometry.\n    The buffer geometry replaces the original geometry of each feature.\n\n    Args:\n        transportation_sdf (pyspark.sql.DataFrame): The DataFrame of transportation features.\n        category_buffer_m (dict): A dictionary mapping categories to buffer distances.\n\n    Returns:\n        pyspark.sql.DataFrame: The DataFrame of transportation features with the buffer geometries.\n    \"\"\"\n\n    transportation_sdf = self.assign_mapping_values(\n        transportation_sdf,\n        category_buffer_m,\n        ColNames.category,\n        \"buffer_dist\",\n        category_buffer_m[\"unknown\"],\n    )\n\n    transportation_sdf = transportation_sdf.filter(F.col(\"buffer_dist\") != 0.0)\n\n    transportation_sdf = transportation_sdf.withColumn(\n        ColNames.geometry,\n        STF.ST_Buffer(ColNames.geometry, F.col(\"buffer_dist\")),\n    ).drop(\"buffer_dist\")\n\n    return transportation_sdf\n</code></pre>"},{"location":"reference/components/execution/grid_enrichment/grid_enrichment/#components.execution.grid_enrichment.grid_enrichment.GridEnrichment.generate_batches","title":"<code>generate_batches(elements_list, batch_size)</code>","text":"<p>Generates batches of elements from list.</p> Source code in <code>multimno/components/execution/grid_enrichment/grid_enrichment.py</code> <pre><code>def generate_batches(self, elements_list, batch_size):\n    \"\"\"\n    Generates batches of elements from list.\n    \"\"\"\n    return [elements_list[i : i + batch_size] for i in range(0, len(elements_list), batch_size)]\n</code></pre>"},{"location":"reference/components/execution/grid_enrichment/grid_enrichment/#components.execution.grid_enrichment.grid_enrichment.GridEnrichment.prepare_transportation_data","title":"<code>prepare_transportation_data()</code>","text":"<p>Prepares transportation data for mapping to a grid.</p> <p>This function takes the transportation data DataFrame, filters it to the extent of the current quadkey, and then calculates a buffer around each transportation feature based on its category.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: The prepared transportation data.</p> Source code in <code>multimno/components/execution/grid_enrichment/grid_enrichment.py</code> <pre><code>def prepare_transportation_data(self) -&gt; DataFrame:\n    \"\"\"\n    Prepares transportation data for mapping to a grid.\n\n    This function takes the transportation data DataFrame, filters it to the extent of the current quadkey,\n    and then calculates a buffer around each transportation feature based on its category.\n\n    Returns:\n        pyspark.sql.DataFrame: The prepared transportation data.\n    \"\"\"\n\n    transportation_sdf = self.input_data_objects[BronzeTransportationDataObject.ID].df\n    transportation_sdf = transportation_sdf.filter(F.col(ColNames.quadkey).isin(self.current_quadkey_batch))\n\n    if transportation_sdf.count() == 0:\n        self.logger.warning(f\"No transportation data found for quadkey {self.current_quadkey_batch}. Skipping\")\n\n        return self.spark.createDataFrame([], transportation_sdf.schema)\n\n    transportation_sdf = self.calculate_transportation_buffer(\n        transportation_sdf, self.transportation_category_buffer_m\n    )\n\n    # prepare quadkeys geometries to merge roads\n    child_quadkeys = [quadkey_utils.get_children_quadkeys(quadkey, 16) for quadkey in self.current_quadkey_batch]\n    child_quadkeys = reduce(operator.add, child_quadkeys, [])\n\n    quadkeys_mask_sdf = quadkey_utils.quadkeys_to_extent_dataframe(self.spark, child_quadkeys, 3035)\n    quadkeys_mask_sdf = quadkeys_mask_sdf.withColumnRenamed(ColNames.quadkey, \"mask_quadkey\")\n\n    transportation_sdf = utils.merge_geom_within_mask_geom(\n        transportation_sdf, quadkeys_mask_sdf, [ColNames.quadkey, \"mask_quadkey\"], ColNames.geometry\n    )\n\n    return transportation_sdf.select(F.lit(\"roads\").alias(ColNames.category), ColNames.geometry, ColNames.quadkey)\n</code></pre>"},{"location":"reference/components/execution/internal_migration/","title":"internal_migration","text":""},{"location":"reference/components/execution/internal_migration/internal_migration/","title":"internal_migration","text":"<p>Module that implements the InternalMigration component</p>"},{"location":"reference/components/execution/internal_migration/internal_migration/#components.execution.internal_migration.internal_migration.InternalMigration","title":"<code>InternalMigration</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class responsible for calculating the internal home location changes produced between two long-term periods and for a given zoning system. First, devices with a significant change in home location tiles are found. Then, based on tile weights, weights for migration between different zones where the home tiles are contained are computed for each device, and these weights are summed up to result in a final weighted device count of devices that migrated from one zone to another</p> Source code in <code>multimno/components/execution/internal_migration/internal_migration.py</code> <pre><code>class InternalMigration(Component):\n    \"\"\"\n    Class responsible for calculating the internal home location changes produced between two long-term periods and for\n    a given zoning system. First, devices with a significant change in home location tiles are found. Then, based on\n    tile weights, weights for migration between different zones where the home tiles are contained are computed\n    for each device, and these weights are summed up to result in a final weighted device count of devices that migrated\n    from one zone to another\n    \"\"\"\n\n    COMPONENT_ID = \"InternalMigration\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        # Read and validate migration threshold\n        self.migration_threshold = self.config.getfloat(self.COMPONENT_ID, \"migration_threshold\")\n        if self.migration_threshold &lt; 0 or self.migration_threshold &gt; 1:\n            msg = f\"migration_threshold should be a value between 0 and 1, found {self.migration_threshold}\"\n            self.logger.error(msg)\n            raise ValueError(msg)\n\n        # Read zoning dataset and hierarchical levels\n        self.zoning_dataset = self.config.get(self.COMPONENT_ID, \"zoning_dataset_id\")\n        levels = self.config.get(self.COMPONENT_ID, \"hierarchical_levels\")\n        try:\n            levels = list(int(x.strip()) for x in levels.split(\",\"))\n        except BaseException as e:\n            msg = f\"hierarchical_levels expected a comma-separated list of integers, but found {levels}\"\n            self.logger.error(msg)\n            raise e(msg)\n        self.levels = levels\n\n        # Read and validate identifying values for the first period's home labels\n        self.start_date_prev = dt.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"start_month_previous\"), \"%Y-%m\"\n        ).date()\n        end_date_prev = dt.datetime.strptime(self.config.get(self.COMPONENT_ID, \"end_month_previous\"), \"%Y-%m\")\n        self.end_date_prev = (\n            end_date_prev + dt.timedelta(days=cal.monthrange(end_date_prev.year, end_date_prev.month)[1] - 1)\n        ).date()\n        if self.start_date_prev &gt; self.end_date_prev:\n            msg = f\"start_month_previous {self.start_date_prev.strftime('%Y-%m')} must be earlier than end_month_previous {end_date_prev.strftime('%Y-%m')}\"\n            self.logger.error(msg)\n            raise ValueError(msg)\n\n        self.season_prev = self.config.get(self.COMPONENT_ID, \"season_previous\")\n        if not Seasons.is_valid_type(self.season_prev):\n            msg = f\"Unknown season_previous {self.season_prev} -- valid values are {Seasons.values()}\"\n            self.logger.error(msg)\n            raise ValueError(msg)\n\n        # Read and validate identifying values for the second period's home labels\n        self.start_date_new = dt.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"start_month_new\"), \"%Y-%m\"\n        ).date()\n        end_date_new = dt.datetime.strptime(self.config.get(self.COMPONENT_ID, \"end_month_new\"), \"%Y-%m\")\n        self.end_date_new = (\n            end_date_new + dt.timedelta(days=cal.monthrange(end_date_new.year, end_date_new.month)[1] - 1)\n        ).date()\n        if self.start_date_new &gt; self.end_date_new:\n            msg = f\"start_month_new {self.start_date_new.strftime('%Y-%m')} must be earlier than end_month_new {end_date_new.strftime('%Y-%m')}\"\n            self.logger.error(msg)\n            raise ValueError(msg)\n\n        self.season_new = self.config.get(self.COMPONENT_ID, \"season_new\")\n        if not Seasons.is_valid_type(self.season_new):\n            msg = f\"Unknown season_new {self.season_new} -- valid values are {Seasons.values()}\"\n            self.logger.error(msg)\n            raise ValueError(msg)\n\n        self.landuse_weights = self.config.geteval(self.COMPONENT_ID, \"landuse_weights\")\n\n        # Initialise other variables\n        self.current_level: int = None\n        self.quality_metrics_computed: bool = False  # quality metrics need only be computed once\n        self.prev_home_tiles: DataFrame = None  # reuse dataframe if already computed for one hierarchical level\n        self.new_home_tiles: DataFrame = None  # reuse dataframe if already computed for one hierarchical level\n        self.migrating_users: DataFrame = None  # reuse dataframe if already computed for one hierarchical level\n\n    def initalize_data_objects(self):\n        # Input paths\n        input_enriched_grid_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"enriched_grid_data_silver\")\n        input_grid_map_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"geozones_grid_map_data_silver\")\n        input_ue_labels_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"aggregated_usual_environments_silver\")\n\n        # Output paths\n        output_do_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"internal_migration_silver\")\n        quality_metrics_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"internal_migration_quality_metrics\")\n\n        # Check uniform for getting the grid or the enriched grid data\n        self.uniform_tile_weights = self.config.getboolean(self.COMPONENT_ID, \"uniform_tile_weights\")\n        if not self.uniform_tile_weights:\n            if not check_if_data_path_exists(self.spark, input_enriched_grid_path):\n                self.logger.warning(f\"Expected path {input_enriched_grid_path} to exist but it does not\")\n                raise ValueError(f\"Invalid path for {SilverEnrichedGridDataObject.ID}: {input_enriched_grid_path}\")\n\n        if not check_if_data_path_exists(self.spark, input_grid_map_path):\n            self.logger.warning(f\"Expected path {input_grid_map_path} to exist but it does not\")\n            raise ValueError(f\"Invalid path for {SilverGeozonesGridMapDataObject.ID}: {input_grid_map_path}\")\n\n        if not check_if_data_path_exists(self.spark, input_ue_labels_path):\n            self.logger.warning(f\"Expected path {input_ue_labels_path} to exist but it does not\")\n            raise ValueError(f\"Invalid path for {SilverUsualEnvironmentLabelsDataObject.ID}: {input_ue_labels_path}\")\n\n        clear_destination_directory = self.config.getboolean(self.COMPONENT_ID, \"clear_destination_directory\")\n        if clear_destination_directory:\n            delete_file_or_folder(self.spark, output_do_path)\n\n        clear_quality_metrics_directory = self.config.getboolean(self.COMPONENT_ID, \"clear_quality_metrics_directory\")\n        if clear_quality_metrics_directory:\n            delete_file_or_folder(self.spark, quality_metrics_path)\n\n        ue_labels = SilverUsualEnvironmentLabelsDataObject(self.spark, input_ue_labels_path)\n        grid_map_zones = SilverGeozonesGridMapDataObject(self.spark, input_grid_map_path)\n        output_do = SilverInternalMigrationDataObject(self.spark, output_do_path)\n        quality_metrics_do = SilverInternalMigrationQualityMetricsDataObject(self.spark, quality_metrics_path)\n\n        self.input_data_objects = {\n            SilverUsualEnvironmentLabelsDataObject.ID: ue_labels,\n            SilverGeozonesGridMapDataObject.ID: grid_map_zones,\n        }\n        if not self.uniform_tile_weights:\n            self.input_data_objects[SilverEnrichedGridDataObject.ID] = SilverEnrichedGridDataObject(\n                self.spark, input_enriched_grid_path\n            )\n\n        self.output_data_objects = {output_do.ID: output_do, quality_metrics_do.ID: quality_metrics_do}\n\n    def calculate_landuse_tile_weights(\n        self,\n        enriched_grid_sdf: DataFrame,\n        landuse_prior_weights: Dict[str, float],\n    ) -&gt; DataFrame:\n        \"\"\"\n        Calculates the tile weights based on landuse information in the enriched grid DataFrame.\n        \"\"\"\n\n        grid_sdf = self.input_data_objects[SilverEnrichedGridDataObject.ID].df\n        grid_sdf = grid_sdf.select(\n            ColNames.grid_id,\n            F.col(ColNames.main_landuse_category),\n        )\n\n        # Create a DataFrame from the weights dictionary\n        weights_list = [(k, float(v)) for k, v in landuse_prior_weights.items()]\n        weights_df = self.spark.createDataFrame(weights_list, [ColNames.main_landuse_category, \"weight\"])\n        weighted_df = enriched_grid_sdf.join(weights_df, on=ColNames.main_landuse_category, how=\"left\").withColumn(\n            ColNames.tile_weight, F.coalesce(F.col(\"weight\"), F.lit(0.0))\n        )  # Default 0.0 for missing weights\n\n        return weighted_df\n\n    @staticmethod\n    def get_migrating_users(\n        prev_home_tiles: DataFrame, new_home_tiles: DataFrame, migration_threshold: float\n    ) -&gt; tuple[DataFrame, DataFrame]:\n        \"\"\"Computes the overlap index for each device in order to determine if it will be used as a migrating device\n        or not. This method also computes the number of unique devices that have home tiles in the first long-term\n        period, in the second long-term period (quality metrics).\n\n        Args:\n            prev_home_tiles (DataFrame): home tiles of devices in the first long-term period\n            new_home_tiles (DataFrame): home tiles of devices in the second long-term period\n            migration_threshold (float): threshold to classify a device as migrating or not\n\n        Returns:\n            tuple[DataFrame, DataFrame]:\n                - First dataframe is the list of devices that have been classified as migrating\n                - Second dataframe contains the quality metrics\n        \"\"\"\n        prev_home_tiles_alias = prev_home_tiles.alias(\"prev\")\n        new_home_tiles_alias = new_home_tiles.alias(\"new\")\n\n        joint_home_tiles = (\n            prev_home_tiles_alias.join(\n                new_home_tiles_alias,\n                how=\"full\",\n                on=(\n                    (prev_home_tiles_alias[ColNames.user_id_modulo] == new_home_tiles_alias[ColNames.user_id_modulo])\n                    &amp; (prev_home_tiles_alias[ColNames.user_id] == new_home_tiles_alias[ColNames.user_id])\n                    &amp; (prev_home_tiles_alias[ColNames.grid_id] == new_home_tiles_alias[ColNames.grid_id])\n                ),\n            )\n            .withColumn(\"coalesced_user_id\", F.coalesce(\"prev.\" + ColNames.user_id, \"new.\" + ColNames.user_id))\n            .withColumn(\n                \"coalesced_modulo\",\n                F.coalesce(\"prev.\" + ColNames.user_id_modulo, \"new.\" + ColNames.user_id_modulo),\n            )\n        )\n        joint_home_tiles.cache()\n\n        quality_metrics = joint_home_tiles.groupBy().agg(\n            F.count_distinct(\"prev.\" + ColNames.user_id).alias(\"previous_home_users\"),  # users in first LT\n            F.count_distinct(\"new.\" + ColNames.user_id).alias(\"new_home_users\"),  # users in second LT\n            F.count_distinct(\"coalesced_user_id\").alias(\"common_home_users\"),  # users in both LTs\n        )\n\n        migrating_users = (\n            joint_home_tiles.groupBy(\"coalesced_modulo\", \"coalesced_user_id\")\n            .agg(\n                (F.count(\"prev.\" + ColNames.grid_id) + F.count(\"new.\" + ColNames.grid_id)).alias(\"total_count\"),\n                F.count(\n                    F.when(\n                        F.col(\"prev.\" + ColNames.grid_id) == F.col(\"new.\" + ColNames.grid_id),\n                        F.col(\"prev.\" + ColNames.grid_id),\n                    )\n                ).alias(\"common_count\"),\n            )\n            .select(\n                F.col(\"coalesced_modulo\").alias(ColNames.user_id_modulo),\n                F.col(\"coalesced_user_id\").alias(ColNames.user_id),\n                (F.lit(2) * F.col(\"common_count\") / F.col(\"total_count\")).alias(\"overlap_index\"),\n            )\n            .where(F.col(\"overlap_index\") &lt; migration_threshold)\n            .select(ColNames.user_id_modulo, ColNames.user_id)\n        )\n        migrating_users.cache()\n\n        return migrating_users, quality_metrics\n\n    def get_zone_home_weights(\n        self, migrating_prev_home_tiles: DataFrame, migrating_new_home_tiles: DataFrame\n    ) -&gt; tuple[DataFrame, DataFrame]:\n        \"\"\"Computes the home weight that each user has from a set of zones to a different set of zones, based\n        on the weights assigned to its home tiles.\n\n        Args:\n            migrating_prev_home_tiles (DataFrame): home tiles of the first long-term period mapped to their resp. zones\n            migrating_new_home_tiles (DataFrame): home tiles of the second long-term period mapped to their resp. zones\n\n        Returns:\n            tuple[DataFrame, DataFrame]: dataframes that contain the device-level home weights for the first and\n                second long-term periods respectively.\n        \"\"\"\n        # Compute tile weight of each user-tile, and add up weight to zones\n        window = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id)\n\n        # If we are not using uniform weights, get land-use or prior probabilities to use as tile weights\n        if not self.uniform_tile_weights:\n            grid_sdf = self.input_data_objects[SilverEnrichedGridDataObject.ID].df\n            grid_sdf = self.calculate_landuse_tile_weights(grid_sdf, self.landuse_weights)\n            grid_sdf = grid_sdf.select(ColNames.grid_id, ColNames.tile_weight)\n            prev_weights = migrating_prev_home_tiles.join(grid_sdf, on=ColNames.grid_id, how=\"inner\").withColumn(\n                ColNames.tile_weight, ColNames.tile_weight / F.sum(ColNames.tile_weight).over(window)\n            )\n\n            new_weights = migrating_new_home_tiles.join(grid_sdf, on=ColNames.grid_id, how=\"inner\").withColumn(\n                ColNames.tile_weight, ColNames.tile_weight / F.sum(ColNames.tile_weight).over(window)\n            )\n        else:  # using uniform tile weights\n            prev_weights = migrating_prev_home_tiles.withColumn(\n                ColNames.tile_weight, F.lit(1) / F.count(\"*\").over(window)\n            )\n\n            new_weights = migrating_new_home_tiles.withColumn(\n                ColNames.tile_weight, F.lit(1) / F.count(\"*\").over(window)\n            )\n\n        prev_zones = (\n            prev_weights.groupBy(ColNames.user_id_modulo, ColNames.user_id, ColNames.zone_id)\n            .agg(F.sum(ColNames.tile_weight).alias(\"prev_weight\"))\n            .withColumnRenamed(ColNames.zone_id, ColNames.previous_zone)\n        )\n        new_zones = (\n            new_weights.groupBy(ColNames.user_id_modulo, ColNames.user_id, ColNames.zone_id)\n            .agg(F.sum(ColNames.tile_weight).alias(\"new_weight\"))\n            .withColumnRenamed(ColNames.zone_id, ColNames.new_zone)\n        )\n        return prev_zones, new_zones\n\n    def transform(self):\n        grid_to_zone = self.input_data_objects[SilverGeozonesGridMapDataObject.ID].df\n        zone_to_grid_map_sdf = grid_to_zone.withColumn(\n            ColNames.zone_id,\n            F.element_at(F.split(F.col(ColNames.hierarchical_id), pattern=\"\\\\|\"), self.current_level),\n        )\n\n        # Read home labels for first long-term period\n        if self.prev_home_tiles is None:\n            self.prev_home_tiles = (\n                self.input_data_objects[SilverUsualEnvironmentLabelsDataObject.ID]\n                .df.where(F.col(ColNames.start_date) == F.lit(self.start_date_prev))\n                .where(F.col(ColNames.end_date) == F.lit(self.end_date_prev))\n                .where(F.col(ColNames.season) == F.lit(self.season_prev))\n                .where(F.col(ColNames.label) == F.lit(\"home\"))\n                .select(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id)\n            )\n            self.prev_home_tiles.cache()\n        # Read home labels for second long-term period\n        if self.new_home_tiles is None:\n            self.new_home_tiles = (\n                self.input_data_objects[SilverUsualEnvironmentLabelsDataObject.ID]\n                .df.where(F.col(ColNames.start_date) == F.lit(self.start_date_new))\n                .where(F.col(ColNames.end_date) == F.lit(self.end_date_new))\n                .where(F.col(ColNames.season) == F.lit(self.season_new))\n                .where(F.col(ColNames.label) == F.lit(\"home\"))\n                .select(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id)\n            )\n            self.new_home_tiles.cache()\n\n        # Get list of users that are under the migration threshold, plus quality metrics\n        if self.migrating_users is None:\n            self.migrating_users, quality_metrics = self.get_migrating_users(\n                self.prev_home_tiles, self.new_home_tiles, self.migration_threshold\n            )\n            self.migrating_users.cache()\n\n        # Join with list of migrating users, and then join with the grid-to-zone mapping\n        migrating_prev_home_tiles = (\n            self.prev_home_tiles.withColumnRenamed(\"prev_grid_id\", ColNames.grid_id)\n            .join(self.migrating_users, on=[ColNames.user_id_modulo, ColNames.user_id])\n            .join(zone_to_grid_map_sdf, on=ColNames.grid_id)\n        )\n        migrating_new_home_tiles = (\n            self.new_home_tiles.withColumnRenamed(\"new_grid_id\", ColNames.grid_id)\n            .join(self.migrating_users, on=[ColNames.user_id_modulo, ColNames.user_id])\n            .join(zone_to_grid_map_sdf, on=ColNames.grid_id)\n        )\n\n        prev_zone_weights, new_zone_weights = self.get_zone_home_weights(\n            migrating_prev_home_tiles, migrating_new_home_tiles\n        )\n\n        # Join before and after zones with their weights\n        migration = (\n            prev_zone_weights.join(new_zone_weights, on=[ColNames.user_id_modulo, ColNames.user_id])\n            .where(F.col(ColNames.previous_zone) != F.col(ColNames.new_zone))  # we don't care about same-zone migration\n            .withColumn(ColNames.migration, F.col(\"prev_weight\") * F.col(\"new_weight\"))\n            .groupBy(ColNames.previous_zone, ColNames.new_zone)\n            .agg(F.sum(ColNames.migration).alias(ColNames.migration))\n        )\n\n        # Add additional partition key columns\n        migration = migration.withColumns(\n            {\n                ColNames.dataset_id: F.lit(self.zoning_dataset),\n                ColNames.level: F.lit(self.current_level),\n                ColNames.start_date_previous: F.lit(self.start_date_prev),\n                ColNames.end_date_previous: F.lit(self.end_date_prev),\n                ColNames.season_previous: F.lit(self.season_prev),\n                ColNames.start_date_new: F.lit(self.start_date_new),\n                ColNames.end_date_new: F.lit(self.end_date_new),\n                ColNames.season_new: F.lit(self.season_new),\n            }\n        )\n\n        migration = apply_schema_casting(migration, SilverInternalMigrationDataObject.SCHEMA)\n        self.output_data_objects[SilverInternalMigrationDataObject.ID].df = migration\n\n        # If quality metrics have not been written yet\n        if not self.quality_metrics_computed:\n            self.quality_metrics_computed = True\n            quality_metrics = quality_metrics.withColumns(\n                {\n                    ColNames.result_timestamp: F.current_timestamp(),\n                    ColNames.dataset_id: F.lit(self.zoning_dataset),\n                    ColNames.level: F.lit(self.current_level),\n                    ColNames.start_date_previous: F.lit(self.start_date_prev),\n                    ColNames.end_date_previous: F.lit(self.end_date_prev),\n                    ColNames.season_previous: F.lit(self.season_prev),\n                    ColNames.start_date_new: F.lit(self.start_date_new),\n                    ColNames.end_date_new: F.lit(self.end_date_new),\n                    ColNames.season_new: F.lit(self.season_new),\n                }\n            )\n            quality_metrics = apply_schema_casting(\n                quality_metrics, SilverInternalMigrationQualityMetricsDataObject.SCHEMA\n            )\n            self.output_data_objects[SilverInternalMigrationQualityMetricsDataObject.ID].df = quality_metrics\n        else:\n            # If it has already been written, we remove the key from the output DO dictionary so it is not written\n            # in the self.write() method\n            if SilverInternalMigrationQualityMetricsDataObject.ID in self.output_data_objects:\n                del self.output_data_objects[SilverInternalMigrationQualityMetricsDataObject.ID]\n\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n\n        for level in self.levels:\n            self.logger.info(f\"Starting migration estimation for hierarchical level {level}...\")\n            self.current_level = level\n            self.transform()\n            self.write()\n\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n</code></pre>"},{"location":"reference/components/execution/internal_migration/internal_migration/#components.execution.internal_migration.internal_migration.InternalMigration.calculate_landuse_tile_weights","title":"<code>calculate_landuse_tile_weights(enriched_grid_sdf, landuse_prior_weights)</code>","text":"<p>Calculates the tile weights based on landuse information in the enriched grid DataFrame.</p> Source code in <code>multimno/components/execution/internal_migration/internal_migration.py</code> <pre><code>def calculate_landuse_tile_weights(\n    self,\n    enriched_grid_sdf: DataFrame,\n    landuse_prior_weights: Dict[str, float],\n) -&gt; DataFrame:\n    \"\"\"\n    Calculates the tile weights based on landuse information in the enriched grid DataFrame.\n    \"\"\"\n\n    grid_sdf = self.input_data_objects[SilverEnrichedGridDataObject.ID].df\n    grid_sdf = grid_sdf.select(\n        ColNames.grid_id,\n        F.col(ColNames.main_landuse_category),\n    )\n\n    # Create a DataFrame from the weights dictionary\n    weights_list = [(k, float(v)) for k, v in landuse_prior_weights.items()]\n    weights_df = self.spark.createDataFrame(weights_list, [ColNames.main_landuse_category, \"weight\"])\n    weighted_df = enriched_grid_sdf.join(weights_df, on=ColNames.main_landuse_category, how=\"left\").withColumn(\n        ColNames.tile_weight, F.coalesce(F.col(\"weight\"), F.lit(0.0))\n    )  # Default 0.0 for missing weights\n\n    return weighted_df\n</code></pre>"},{"location":"reference/components/execution/internal_migration/internal_migration/#components.execution.internal_migration.internal_migration.InternalMigration.get_migrating_users","title":"<code>get_migrating_users(prev_home_tiles, new_home_tiles, migration_threshold)</code>  <code>staticmethod</code>","text":"<p>Computes the overlap index for each device in order to determine if it will be used as a migrating device or not. This method also computes the number of unique devices that have home tiles in the first long-term period, in the second long-term period (quality metrics).</p> <p>Parameters:</p> Name Type Description Default <code>prev_home_tiles</code> <code>DataFrame</code> <p>home tiles of devices in the first long-term period</p> required <code>new_home_tiles</code> <code>DataFrame</code> <p>home tiles of devices in the second long-term period</p> required <code>migration_threshold</code> <code>float</code> <p>threshold to classify a device as migrating or not</p> required <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame]</code> <p>tuple[DataFrame, DataFrame]: - First dataframe is the list of devices that have been classified as migrating - Second dataframe contains the quality metrics</p> Source code in <code>multimno/components/execution/internal_migration/internal_migration.py</code> <pre><code>@staticmethod\ndef get_migrating_users(\n    prev_home_tiles: DataFrame, new_home_tiles: DataFrame, migration_threshold: float\n) -&gt; tuple[DataFrame, DataFrame]:\n    \"\"\"Computes the overlap index for each device in order to determine if it will be used as a migrating device\n    or not. This method also computes the number of unique devices that have home tiles in the first long-term\n    period, in the second long-term period (quality metrics).\n\n    Args:\n        prev_home_tiles (DataFrame): home tiles of devices in the first long-term period\n        new_home_tiles (DataFrame): home tiles of devices in the second long-term period\n        migration_threshold (float): threshold to classify a device as migrating or not\n\n    Returns:\n        tuple[DataFrame, DataFrame]:\n            - First dataframe is the list of devices that have been classified as migrating\n            - Second dataframe contains the quality metrics\n    \"\"\"\n    prev_home_tiles_alias = prev_home_tiles.alias(\"prev\")\n    new_home_tiles_alias = new_home_tiles.alias(\"new\")\n\n    joint_home_tiles = (\n        prev_home_tiles_alias.join(\n            new_home_tiles_alias,\n            how=\"full\",\n            on=(\n                (prev_home_tiles_alias[ColNames.user_id_modulo] == new_home_tiles_alias[ColNames.user_id_modulo])\n                &amp; (prev_home_tiles_alias[ColNames.user_id] == new_home_tiles_alias[ColNames.user_id])\n                &amp; (prev_home_tiles_alias[ColNames.grid_id] == new_home_tiles_alias[ColNames.grid_id])\n            ),\n        )\n        .withColumn(\"coalesced_user_id\", F.coalesce(\"prev.\" + ColNames.user_id, \"new.\" + ColNames.user_id))\n        .withColumn(\n            \"coalesced_modulo\",\n            F.coalesce(\"prev.\" + ColNames.user_id_modulo, \"new.\" + ColNames.user_id_modulo),\n        )\n    )\n    joint_home_tiles.cache()\n\n    quality_metrics = joint_home_tiles.groupBy().agg(\n        F.count_distinct(\"prev.\" + ColNames.user_id).alias(\"previous_home_users\"),  # users in first LT\n        F.count_distinct(\"new.\" + ColNames.user_id).alias(\"new_home_users\"),  # users in second LT\n        F.count_distinct(\"coalesced_user_id\").alias(\"common_home_users\"),  # users in both LTs\n    )\n\n    migrating_users = (\n        joint_home_tiles.groupBy(\"coalesced_modulo\", \"coalesced_user_id\")\n        .agg(\n            (F.count(\"prev.\" + ColNames.grid_id) + F.count(\"new.\" + ColNames.grid_id)).alias(\"total_count\"),\n            F.count(\n                F.when(\n                    F.col(\"prev.\" + ColNames.grid_id) == F.col(\"new.\" + ColNames.grid_id),\n                    F.col(\"prev.\" + ColNames.grid_id),\n                )\n            ).alias(\"common_count\"),\n        )\n        .select(\n            F.col(\"coalesced_modulo\").alias(ColNames.user_id_modulo),\n            F.col(\"coalesced_user_id\").alias(ColNames.user_id),\n            (F.lit(2) * F.col(\"common_count\") / F.col(\"total_count\")).alias(\"overlap_index\"),\n        )\n        .where(F.col(\"overlap_index\") &lt; migration_threshold)\n        .select(ColNames.user_id_modulo, ColNames.user_id)\n    )\n    migrating_users.cache()\n\n    return migrating_users, quality_metrics\n</code></pre>"},{"location":"reference/components/execution/internal_migration/internal_migration/#components.execution.internal_migration.internal_migration.InternalMigration.get_zone_home_weights","title":"<code>get_zone_home_weights(migrating_prev_home_tiles, migrating_new_home_tiles)</code>","text":"<p>Computes the home weight that each user has from a set of zones to a different set of zones, based on the weights assigned to its home tiles.</p> <p>Parameters:</p> Name Type Description Default <code>migrating_prev_home_tiles</code> <code>DataFrame</code> <p>home tiles of the first long-term period mapped to their resp. zones</p> required <code>migrating_new_home_tiles</code> <code>DataFrame</code> <p>home tiles of the second long-term period mapped to their resp. zones</p> required <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame]</code> <p>tuple[DataFrame, DataFrame]: dataframes that contain the device-level home weights for the first and second long-term periods respectively.</p> Source code in <code>multimno/components/execution/internal_migration/internal_migration.py</code> <pre><code>def get_zone_home_weights(\n    self, migrating_prev_home_tiles: DataFrame, migrating_new_home_tiles: DataFrame\n) -&gt; tuple[DataFrame, DataFrame]:\n    \"\"\"Computes the home weight that each user has from a set of zones to a different set of zones, based\n    on the weights assigned to its home tiles.\n\n    Args:\n        migrating_prev_home_tiles (DataFrame): home tiles of the first long-term period mapped to their resp. zones\n        migrating_new_home_tiles (DataFrame): home tiles of the second long-term period mapped to their resp. zones\n\n    Returns:\n        tuple[DataFrame, DataFrame]: dataframes that contain the device-level home weights for the first and\n            second long-term periods respectively.\n    \"\"\"\n    # Compute tile weight of each user-tile, and add up weight to zones\n    window = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id)\n\n    # If we are not using uniform weights, get land-use or prior probabilities to use as tile weights\n    if not self.uniform_tile_weights:\n        grid_sdf = self.input_data_objects[SilverEnrichedGridDataObject.ID].df\n        grid_sdf = self.calculate_landuse_tile_weights(grid_sdf, self.landuse_weights)\n        grid_sdf = grid_sdf.select(ColNames.grid_id, ColNames.tile_weight)\n        prev_weights = migrating_prev_home_tiles.join(grid_sdf, on=ColNames.grid_id, how=\"inner\").withColumn(\n            ColNames.tile_weight, ColNames.tile_weight / F.sum(ColNames.tile_weight).over(window)\n        )\n\n        new_weights = migrating_new_home_tiles.join(grid_sdf, on=ColNames.grid_id, how=\"inner\").withColumn(\n            ColNames.tile_weight, ColNames.tile_weight / F.sum(ColNames.tile_weight).over(window)\n        )\n    else:  # using uniform tile weights\n        prev_weights = migrating_prev_home_tiles.withColumn(\n            ColNames.tile_weight, F.lit(1) / F.count(\"*\").over(window)\n        )\n\n        new_weights = migrating_new_home_tiles.withColumn(\n            ColNames.tile_weight, F.lit(1) / F.count(\"*\").over(window)\n        )\n\n    prev_zones = (\n        prev_weights.groupBy(ColNames.user_id_modulo, ColNames.user_id, ColNames.zone_id)\n        .agg(F.sum(ColNames.tile_weight).alias(\"prev_weight\"))\n        .withColumnRenamed(ColNames.zone_id, ColNames.previous_zone)\n    )\n    new_zones = (\n        new_weights.groupBy(ColNames.user_id_modulo, ColNames.user_id, ColNames.zone_id)\n        .agg(F.sum(ColNames.tile_weight).alias(\"new_weight\"))\n        .withColumnRenamed(ColNames.zone_id, ColNames.new_zone)\n    )\n    return prev_zones, new_zones\n</code></pre>"},{"location":"reference/components/execution/longterm_permanence_score/","title":"longterm_permanence_score","text":""},{"location":"reference/components/execution/longterm_permanence_score/longterm_permanence_score/","title":"longterm_permanence_score","text":"<p>Module that computes the Long-term Permanence Score</p>"},{"location":"reference/components/execution/longterm_permanence_score/longterm_permanence_score/#components.execution.longterm_permanence_score.longterm_permanence_score.LongtermPermanenceScore","title":"<code>LongtermPermanenceScore</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class that computes the long term permanence score and related metrics, for different combinations of seasons, day types and time intervals.</p> Source code in <code>multimno/components/execution/longterm_permanence_score/longterm_permanence_score.py</code> <pre><code>class LongtermPermanenceScore(Component):\n    \"\"\"\n    Class that computes the long term permanence score and related metrics, for different combinations of\n    seasons, day types and time intervals.\n    \"\"\"\n\n    COMPONENT_ID = \"LongtermPermanenceScore\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        # Months that compose the long-term period, at least one\n        start_month = self.config.get(self.COMPONENT_ID, \"start_month\")\n        try:\n            self.start_date = dt.datetime.strptime(start_month, \"%Y-%m\").date()\n        except ValueError as e:\n            self.logger.error(f\"Could not parse start_month = `{start_month}`. Expected format: YYYY-MM\")\n            raise e\n\n        end_month = self.config.get(self.COMPONENT_ID, \"end_month\")\n        try:\n            self.end_date = dt.datetime.strptime(end_month, \"%Y-%m\").date()\n        except ValueError as e:\n            self.logger.error(f\"Could not parse end_month = `{end_month}`. Expected format: YYYY-MM\")\n            raise e\n        self.end_date = self.end_date.replace(day=cal.monthrange(self.end_date.year, self.end_date.month)[1])\n\n        if self.end_date &lt; self.start_date:\n            raise ValueError(f\"End month `{end_month}` should not be an earlier than start month `{start_month}`\")\n\n        # Get months of the year in each season\n        self.winter_months = self._get_month_list(self.config.get(self.COMPONENT_ID, \"winter_months\"), \"winter\")\n        self.spring_months = self._get_month_list(self.config.get(self.COMPONENT_ID, \"spring_months\"), \"spring\")\n        self.summer_months = self._get_month_list(self.config.get(self.COMPONENT_ID, \"summer_months\"), \"summer\")\n        self.autumn_months = self._get_month_list(self.config.get(self.COMPONENT_ID, \"autumn_months\"), \"autumn\")\n\n        # Check for possible repeated months\n        all_season_months = self.winter_months + self.spring_months + self.summer_months + self.autumn_months\n        if len(all_season_months) != len(set(all_season_months)):\n            raise ValueError(\"at least one month belongs to more than one season -- please correct input parameters\")\n\n        # Read all subyearly-submonthly-subdaily combinations\n        period_combinations = self.config.geteval(self.COMPONENT_ID, \"period_combinations\")\n        self.period_combinations = {}\n        for season, day_types_dict in period_combinations.items():\n            if not Seasons.is_valid_type(season.lower()):\n                raise ValueError(f\"Unknown season `{season}` in period_combinations\")\n            self.period_combinations[season.lower()] = {}\n            for day_type, time_intervals in day_types_dict.items():\n                if not DayTypes.is_valid_type(day_type.lower()):\n                    raise ValueError(f\"Uknown day_type `{day_type}` under `{season}` in period_combinations\")\n                self.period_combinations[season.lower()][day_type.lower()] = []\n                if len(time_intervals) != len(set(time_intervals)):\n                    raise ValueError(\n                        f\"Repeated values for time_interval in under `{season}` and `{day_type}`:\",\n                        str(period_combinations[season][day_type]),\n                    )\n                for time_interval in time_intervals:\n                    if not TimeIntervals.is_valid_type(time_interval):\n                        raise ValueError(f\"Unknown time_interval `{time_interval}` under `{season}` and `{day_type}`\")\n                    self.period_combinations[season.lower()][day_type.lower()].append(time_interval)\n\n        # Check that all seasons for which an analysis is to be performed have been assigned at least one month\n        for season, periods in self.period_combinations.items():\n            if season == Seasons.ALL:\n                continue\n            if len(periods) &gt; 0 and len(getattr(self, f\"{season}_months\")) == 0:\n                raise ValueError(\n                    f\"Some period combinations have been requested for season `{season}` but no \"\n                    \"month has been included in this season -- please correct the configuration \"\n                    \"parameters\"\n                )\n\n        # Get list of all the months that will be used, together with the seasons they belong to\n        self.longterm_months = []\n        self.season_months = {}\n\n        for season in self.period_combinations:\n            self.season_months[season] = []\n\n        month_start_date = self.start_date\n        while month_start_date &lt; self.end_date:\n            self.longterm_months.append(month_start_date)\n            if Seasons.ALL in self.season_months:\n                self.season_months[Seasons.ALL].append(month_start_date)\n            if Seasons.WINTER in self.season_months and month_start_date.month in self.winter_months:\n                self.season_months[Seasons.WINTER].append(month_start_date)\n            if Seasons.SPRING in self.season_months and month_start_date.month in self.spring_months:\n                self.season_months[Seasons.SPRING].append(month_start_date)\n            if Seasons.SUMMER in self.season_months and month_start_date.month in self.summer_months:\n                self.season_months[Seasons.SUMMER].append(month_start_date)\n            if Seasons.AUTUMN in self.season_months and month_start_date.month in self.autumn_months:\n                self.season_months[Seasons.AUTUMN].append(month_start_date)\n\n            month_start_date += dt.timedelta(days=cal.monthrange(month_start_date.year, month_start_date.month)[1])\n\n        # Initialise variables for working with each longterm analysis\n        self.current_lt_analysis = None\n        self.longterm_analyses = None\n\n    def _get_month_list(self, months_input: str, context: str) -&gt; List[int]:\n        \"\"\"Read and parse a comma-separated list of months that will be assigned to a particular season. Months are\n        represented by an integer from 1 to 12 and must not be repeated within the list\n\n\n        Args:\n            months_input (str): comma-separated list of months to be parsed\n            context (str): name of the season, used for error tracking\n\n        Raises:\n            e: could not parse month to integer\n            ValueError: integer is not one between 1 and 12, inclusive\n            ValueError: repeated integers\n\n        Returns:\n            List[int]: list of integers representing the months of the year that will constitute a season\n        \"\"\"\n        months_input = months_input.replace(\" \", \"\").replace(\"\\t\", \"\")\n        if months_input == \"\":\n            return []\n\n        months_input = months_input.split(\",\")\n        months = []\n\n        for mm in months_input:\n            try:\n                mm = int(mm)\n            except ValueError as e:\n                self.logger.error(f\"expected integer as a month for {context} season, but found `{mm}`\")\n                raise e\n            if mm &lt; 1 or mm &gt; 12:\n                raise ValueError(\n                    f\"expected integer between 1 and 12 to represent a month for {context} season, \" f\"but found `{mm}`\"\n                )\n            months.append(mm)\n\n        if len(months) != len(set(months)):\n            raise ValueError(f\"found repeated month {months} in season {context} -- please remove any duplicates\")\n\n        return months\n\n    def initalize_data_objects(self):\n        input_silver_midterm_ps_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"midterm_permanence_score_data_silver\")\n        output_silver_longterm_ps_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"longterm_permanence_score_data_silver\"\n        )\n\n        midterm_ps = SilverMidtermPermanenceScoreDataObject(self.spark, input_silver_midterm_ps_path)\n        longterm_ps = SilverLongtermPermanenceScoreDataObject(self.spark, output_silver_longterm_ps_path)\n\n        self.input_data_objects = {midterm_ps.ID: midterm_ps}\n\n        self.output_data_objects = {longterm_ps.ID: longterm_ps}\n\n    def _check_midterm_data_exist(self) -&gt; List[dict]:\n        \"\"\"Checks that the mid-term permanence score data necessary for carrying out all long-term analyses exist,\n        based on the day_types and time_intervals requested for each analysis. Returns a list of dictionaries\n        containing the information necessary to filter the required mid-term analysis data of each analysis.\n\n        Raises:\n            FileNotFoundError: Whenever a certain (day_type, time_interval) combination has not been found in the\n                data of a particular month, required to compute some long-term analysis.\n\n        Returns:\n            List[dict]: information of each long-term analysis to be performed\n        \"\"\"\n        midterm_df = self.input_data_objects[SilverMidtermPermanenceScoreDataObject.ID].df\n        longterm_analyses = []\n        already_checked = set()\n        # For each season\n        for season, months in self.season_months.items():\n            # Check that all months of that season have the mid-term PS data of the required day type and time interval\n            for day_type, time_intervals in self.period_combinations[season].items():\n                for time_interval in time_intervals:\n                    for month in months:\n                        if (month, day_type, time_interval) in already_checked:\n                            continue\n\n                        partition_filter = (\n                            (F.col(ColNames.year) == F.lit(month.year))\n                            &amp; (F.col(ColNames.month) == F.lit(month.month))\n                            &amp; (F.col(ColNames.day_type) == F.lit(day_type))\n                            &amp; (F.col(ColNames.time_interval) == F.lit(time_interval))\n                        )\n\n                        data_exists = midterm_df.where(partition_filter).limit(1).count() &gt; 0\n                        if not data_exists:\n                            raise FileNotFoundError(\n                                \"No Mid-term Permanence Score data has been found for month \"\n                                f\"{month.strftime('%Y-%m')}, day_type `{day_type}` and time_interval `{time_interval}`\"\n                            )\n                        already_checked.add((month, day_type, time_interval))\n\n                    longterm_analyses.append(\n                        {\n                            \"season\": season,\n                            \"months\": months,\n                            \"day_type\": day_type,\n                            \"time_interval\": time_interval,\n                        }\n                    )\n\n        return longterm_analyses\n\n    def filter_longterm_analysis_data(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Applies the push-up filters on the Mid-term Permanence Score data to only consider the data from the\n        months, day_type, and time_interval required for the current long-term analysis combination.\n\n        Args:\n            df (DataFrame): Mid-term Permanence Score DataFrame before filtering\n\n        Returns:\n            DataFrame: Mid-term Permanence Score DataFrame after filtering\n        \"\"\"\n        month_filters = [\n            (F.col(ColNames.year) == F.lit(month.year)) &amp; (F.col(ColNames.month) == F.lit(month.month))\n            for month in self.current_lt_analysis[\"months\"]\n        ]\n\n        def logical_or(x, y):\n            return x | y\n\n        month_filter = reduce(logical_or, month_filters)\n\n        day_type_filter = F.col(ColNames.day_type) == F.lit(self.current_lt_analysis[\"day_type\"])\n\n        time_interval_filter = F.col(ColNames.time_interval) == F.lit(self.current_lt_analysis[\"time_interval\"])\n\n        filtered_df = df.filter(month_filter &amp; day_type_filter &amp; time_interval_filter)\n\n        return filtered_df\n\n    def compute_longterm_metrics(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Compute the Long-term Permanence Score and metrics from the filtered Mid-term Permanence Score data\n\n        Args:\n            df (DataFrame): Mid-term Permanence Score DataFrame\n\n        Returns:\n            DataFrame: dataframe with the long-term permanence score aend metrics\n        \"\"\"\n\n        df = df.groupby(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id, ColNames.id_type).agg(\n            F.sum(ColNames.mps).cast(IntegerType()).alias(ColNames.lps),\n            F.sum(ColNames.frequency).cast(IntegerType()).alias(ColNames.total_frequency),\n            F.mean(ColNames.frequency).cast(FloatType()).alias(ColNames.frequency_mean),\n            F.stddev_samp(ColNames.frequency).cast(FloatType()).alias(ColNames.frequency_std),\n            F.mean(ColNames.regularity_mean).cast(FloatType()).alias(ColNames.regularity_mean),\n            F.stddev_samp(ColNames.regularity_mean).cast(FloatType()).alias(ColNames.regularity_std),\n        )\n\n        return df\n\n    def transform(self):\n        midterm_df = self.input_data_objects[SilverMidtermPermanenceScoreDataObject.ID].df\n\n        df = self.filter_longterm_analysis_data(midterm_df)\n\n        # Filter partition chunk\n        if self.partition_chunk is not None:\n            df = df.filter(F.col(ColNames.user_id_modulo).isin(self.partition_chunk))\n\n        longterm_df = self.compute_longterm_metrics(df)\n\n        start_date = min(self.current_lt_analysis[\"months\"])\n        end_date = max(self.current_lt_analysis[\"months\"])\n        end_date = end_date.replace(day=cal.monthrange(end_date.year, end_date.month)[1])\n\n        longterm_df = longterm_df.withColumns(\n            {\n                ColNames.start_date: F.lit(start_date),\n                ColNames.end_date: F.lit(end_date),\n                ColNames.season: F.lit(self.current_lt_analysis[\"season\"]),\n                ColNames.day_type: F.lit(self.current_lt_analysis[\"day_type\"]),\n                ColNames.time_interval: F.lit(self.current_lt_analysis[\"time_interval\"]),\n            }\n        )\n\n        longterm_df = longterm_df.repartition(*SilverLongtermPermanenceScoreDataObject.PARTITION_COLUMNS)\n\n        self.output_data_objects[SilverLongtermPermanenceScoreDataObject.ID].df = longterm_df\n\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(\"Reading data objects...\")\n        self.read()\n        self.logger.info(\"... data objects read!\")\n        self.logger.info(\"Checking that all required mid-term permanence score data exists...\")\n        self.longterm_analyses = self._check_midterm_data_exist()\n        self.logger.info(\"... check successful!\")\n        self.logger.info(\"Starting long-term analyses...\")\n\n        partition_chunks = self._get_partition_chunks()\n        for i, partition_chunk in enumerate(partition_chunks):\n            self.partition_chunk = partition_chunk\n            self.logger.info(f\"Processing partition chunk {i}\")\n            self.logger.debug(f\"Partition chunk {partition_chunk}\")\n            for lt_analysis in self.longterm_analyses:\n                self.current_lt_analysis = lt_analysis\n                self.logger.info(\n                    f\"Starting analysis for season `{self.current_lt_analysis['season']}`, \"\n                    f\"{min(self.current_lt_analysis['months']).strftime('%Y-%m')} to \"\n                    f\"{max(self.current_lt_analysis['months']).strftime('%Y-%m')}, \"\n                    f\"day_type `{self.current_lt_analysis['day_type']}` and \"\n                    f\"time_interval `{self.current_lt_analysis['time_interval']}`...\"\n                )\n                self.transform()\n                self.write()\n                self.logger.info(\"... results saved\")\n\n        self.logger.info(\"... all analyses finished!\")\n\n    # TODO: Refactor iterative processing as an Interface\n    def _get_partition_chunks(self) -&gt; List[List[int]]:\n        \"\"\"\n        Method that returns the partition chunks for the current date.\n\n        Returns:\n            List[List[int, int]]: list of partition chunks. If the partition_chunk_size is not defined in the config or\n                the number of partitions is less than the desired chunk size, it will return a list with a single None element.\n        \"\"\"\n        # Get partitions desired\n        partition_chunk_size = self.config.getint(self.COMPONENT_ID, \"partition_chunk_size\", fallback=None)\n        number_of_partitions = self.config.getint(self.COMPONENT_ID, \"number_of_partitions\", fallback=None)\n\n        if partition_chunk_size is None or number_of_partitions is None or partition_chunk_size &lt;= 0:\n            return [None]\n\n        if number_of_partitions &lt;= partition_chunk_size:\n            self.logger.warning(\n                f\"Available Partition number ({number_of_partitions}) is \"\n                f\"less than the desired chunk size ({partition_chunk_size}). \"\n                f\"Using all partitions.\"\n            )\n            return [None]\n        partition_chunks = [\n            list(range(i, min(i + partition_chunk_size, number_of_partitions)))\n            for i in range(0, number_of_partitions, partition_chunk_size)\n        ]\n        # NOTE: Generate chunks if partition_values were read for each day\n        # getting exactly the amount of partitions for that day\n\n        # partition_chunks = [\n        #     partition_values[i : i + partition_chunk_size]\n        #     for i in range(0, partition_values_size, partition_chunk_size)\n        # ]\n\n        return partition_chunks\n</code></pre>"},{"location":"reference/components/execution/longterm_permanence_score/longterm_permanence_score/#components.execution.longterm_permanence_score.longterm_permanence_score.LongtermPermanenceScore.compute_longterm_metrics","title":"<code>compute_longterm_metrics(df)</code>","text":"<p>Compute the Long-term Permanence Score and metrics from the filtered Mid-term Permanence Score data</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Mid-term Permanence Score DataFrame</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>dataframe with the long-term permanence score aend metrics</p> Source code in <code>multimno/components/execution/longterm_permanence_score/longterm_permanence_score.py</code> <pre><code>def compute_longterm_metrics(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Compute the Long-term Permanence Score and metrics from the filtered Mid-term Permanence Score data\n\n    Args:\n        df (DataFrame): Mid-term Permanence Score DataFrame\n\n    Returns:\n        DataFrame: dataframe with the long-term permanence score aend metrics\n    \"\"\"\n\n    df = df.groupby(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id, ColNames.id_type).agg(\n        F.sum(ColNames.mps).cast(IntegerType()).alias(ColNames.lps),\n        F.sum(ColNames.frequency).cast(IntegerType()).alias(ColNames.total_frequency),\n        F.mean(ColNames.frequency).cast(FloatType()).alias(ColNames.frequency_mean),\n        F.stddev_samp(ColNames.frequency).cast(FloatType()).alias(ColNames.frequency_std),\n        F.mean(ColNames.regularity_mean).cast(FloatType()).alias(ColNames.regularity_mean),\n        F.stddev_samp(ColNames.regularity_mean).cast(FloatType()).alias(ColNames.regularity_std),\n    )\n\n    return df\n</code></pre>"},{"location":"reference/components/execution/longterm_permanence_score/longterm_permanence_score/#components.execution.longterm_permanence_score.longterm_permanence_score.LongtermPermanenceScore.filter_longterm_analysis_data","title":"<code>filter_longterm_analysis_data(df)</code>","text":"<p>Applies the push-up filters on the Mid-term Permanence Score data to only consider the data from the months, day_type, and time_interval required for the current long-term analysis combination.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Mid-term Permanence Score DataFrame before filtering</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Mid-term Permanence Score DataFrame after filtering</p> Source code in <code>multimno/components/execution/longterm_permanence_score/longterm_permanence_score.py</code> <pre><code>def filter_longterm_analysis_data(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Applies the push-up filters on the Mid-term Permanence Score data to only consider the data from the\n    months, day_type, and time_interval required for the current long-term analysis combination.\n\n    Args:\n        df (DataFrame): Mid-term Permanence Score DataFrame before filtering\n\n    Returns:\n        DataFrame: Mid-term Permanence Score DataFrame after filtering\n    \"\"\"\n    month_filters = [\n        (F.col(ColNames.year) == F.lit(month.year)) &amp; (F.col(ColNames.month) == F.lit(month.month))\n        for month in self.current_lt_analysis[\"months\"]\n    ]\n\n    def logical_or(x, y):\n        return x | y\n\n    month_filter = reduce(logical_or, month_filters)\n\n    day_type_filter = F.col(ColNames.day_type) == F.lit(self.current_lt_analysis[\"day_type\"])\n\n    time_interval_filter = F.col(ColNames.time_interval) == F.lit(self.current_lt_analysis[\"time_interval\"])\n\n    filtered_df = df.filter(month_filter &amp; day_type_filter &amp; time_interval_filter)\n\n    return filtered_df\n</code></pre>"},{"location":"reference/components/execution/midterm_permanence_score/","title":"midterm_permanence_score","text":""},{"location":"reference/components/execution/midterm_permanence_score/midterm_permanence_score/","title":"midterm_permanence_score","text":"<p>Module that computes the Mid-term Permanence Score.</p>"},{"location":"reference/components/execution/midterm_permanence_score/midterm_permanence_score/#components.execution.midterm_permanence_score.midterm_permanence_score.MidtermPermanenceScore","title":"<code>MidtermPermanenceScore</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class that computes the mid term permanence score and related metrics, for different combinations of day types and time intervals in the day</p> Source code in <code>multimno/components/execution/midterm_permanence_score/midterm_permanence_score.py</code> <pre><code>class MidtermPermanenceScore(Component):\n    \"\"\"\n    Class that computes the mid term permanence score and related metrics, for different\n    combinations of day types and time intervals in the day\n    \"\"\"\n\n    COMPONENT_ID = \"MidtermPermanenceScore\"\n\n    night_time_start, night_time_end = None, None\n    working_hours_start, working_hours_end = None, None\n    evening_time_start, evening_time_end = None, None\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        # Months to process as each mid-term period\n        start_month = self.config.get(self.COMPONENT_ID, \"start_month\")\n        try:\n            self.start_date = dt.datetime.strptime(start_month, \"%Y-%m\").date()\n        except ValueError as e:\n            self.logger.error(f\"Could not parse start_month = `{start_month}`. Expected format: YYYY-MM\")\n            raise e\n\n        end_month = self.config.get(self.COMPONENT_ID, \"end_month\")\n        try:\n            self.end_date = dt.datetime.strptime(end_month, \"%Y-%m\").date()\n        except ValueError as e:\n            self.logger.error(f\"Could not parse end_month = `{end_month}`. Expected format: YYYY-MM\")\n            raise e\n        self.end_date = self.end_date.replace(day=cal.monthrange(self.end_date.year, self.end_date.month)[1])\n\n        if end_month &lt; start_month:\n            raise ValueError(f\"End month `{end_month}` should not be an earlier than start month `{start_month}`\")\n\n        # Additional days before and after each month to use for calculating regularity metrics\n        self.before_reg_days = self.config.getint(self.COMPONENT_ID, \"before_regularity_days\")\n        if self.before_reg_days &lt; 0:\n            raise ValueError(f\"`before_reg_days` must be a non-negative integer, found {self.before_reg_days}\")\n\n        self.after_reg_days = self.config.getint(self.COMPONENT_ID, \"after_regularity_days\")\n        if self.after_reg_days &lt; 0:\n            raise ValueError(f\"`after_reg_days` must be a non-negative integer, found {self.after_reg_days}\")\n\n        # list of dictionaries with each mid-term to be analysed\n        self.midterm_periods = self._get_midterm_periods()\n        # Hour used to define the start of a day, e.g. 4 means that a Monday starts at 4AM Monday and ends at\n        # 4AM Tuesday\n        self.day_start_hour = self.config.getint(self.COMPONENT_ID, \"day_start_hour\")\n        if self.day_start_hour &lt; 0 or self.day_start_hour &gt;= 24:\n            raise ValueError(f\"`day_start_hour` must be between 0 and 23 inclusive, found {self.day_start_hour} \")\n\n        # Read the definition of each sub-daily period (or time interval) to be studied.\n        # Set to keep track of the minutes in each the intervals start/end, which must be compared with\n        # daily permanence score to verify compatibility\n        self.midterm_minutes = set()\n        for time_interval in TimeIntervals.values():  # night, work, evening, all\n            if time_interval == TimeIntervals.ALL:\n                continue\n            interval_start = self.config.get(self.COMPONENT_ID, f\"{time_interval}_start\")\n            interval_start = self._check_time_interval(interval_start, name=f\"{time_interval}_start\")\n            setattr(self, f\"{time_interval}_start\", interval_start)\n\n            interval_end = self.config.get(self.COMPONENT_ID, f\"{time_interval}_end\")\n            interval_end = self._check_time_interval(interval_end, name=f\"{time_interval}_end\")\n            setattr(self, f\"{time_interval}_end\", interval_end)\n\n            if interval_start == interval_end:\n                raise ValueError(\n                    f\"{time_interval}_start and {time_interval}_end are equal, when they must be strictly \"\n                    \"different -- please provide a valid time interval\"\n                )\n\n            # Non-allowed time interval limits. Example:\n            # self.day_start_hour = 4 (4AM)\n            # interval_start = 03:30, interval_end = 01:00\n            # The time interval starts at 03:30 of day D-1 and ends at 01:00 of day D, but would belong to day D-1\n            if (\n                interval_start.hour &lt; self.day_start_hour\n                and interval_end != dt.time(0, 0)\n                and (interval_end &lt; interval_start)\n            ):\n                raise ValueError(\n                    \"Invalid configuration: the following order of of parameters is not allowed:\\n\"\n                    f\"\\t {time_interval}_end ({interval_end}) &lt; {time_interval}_start ({interval_start}) &lt; \"\n                    f\"day_start_hour ({self.day_start_hour})\"\n                )\n\n            # Additional prohibited time interval (except for nights): time interval must not cross the self.dat_start_hour\n            if time_interval != \"night_time\":\n                if interval_start.hour &lt; self.day_start_hour and (\n                    interval_end.hour &gt; self.day_start_hour\n                    or (interval_end.hour == self.day_start_hour and interval_end.minute != 0)\n                ):\n                    raise ValueError(\n                        \"Invalid configuration: the following order of parameters is not allowed:\\n\"\n                        f\"{time_interval}_start ({interval_start}) &lt; day_start_hour ({self.day_start_hour}) &lt; {time_interval}_end ({interval_end})\"\n                    )\n\n            self.midterm_minutes.add(interval_start.minute)\n            self.midterm_minutes.add(interval_end.minute)\n\n        # Day of the week marking the start of the weekend, (starting in self.day_start_hour)\n        weekend_start_str = self.config.get(self.COMPONENT_ID, \"weekend_start\")\n        self.weekend_start_day = self._check_weekday_number(weekend_start_str, context=weekend_start_str)\n\n        # Day of the week marking the end of the weekend, date included, (ending right before self.day_start_hour)\n        weekend_end_str = self.config.get(self.COMPONENT_ID, \"weekend_end\")\n        self.weekend_end_day = self._check_weekday_number(weekend_end_str, context=weekend_end_str)\n\n        # List of days of the week composing the weekend\n        self.weekend_days = []\n        dd = self.weekend_start_day\n        while dd != self.weekend_end_day:\n            self.weekend_days.append(dd)\n            dd = (dd) % 7 + 1\n        self.weekend_days.append(dd)\n\n        # Work days are those that are not part of the weekend (also excluding holidays later on)\n        self.work_days = sorted(list({1, 2, 3, 4, 5, 6, 7}.difference(self.weekend_days)))\n\n        # Read from configuration the combination of sub-monthly and sub-daily pairs, i.e. day types and time intervals,\n        # to compute\n        period_combinations = self.config.geteval(self.COMPONENT_ID, \"period_combinations\")\n        self.period_combinations = {}\n        for key, vals in period_combinations.items():\n            if not DayTypes.is_valid_type(key.lower()):\n                raise ValueError(f\"Unknown day type `{key}` in period_combinations\")\n            self.period_combinations[key.lower()] = []\n            if len(vals) != len(set(vals)):\n                raise ValueError(\n                    f\"Repeated values for time interval in period_combinations under `{key}`:\",\n                    str(period_combinations[key]),\n                )\n            for val in vals:\n                if not TimeIntervals.is_valid_type(val.lower()):\n                    raise ValueError(f\"Unknown time interval `{val}` in period_combinations under `{key}`\")\n                self.period_combinations[key.lower()].append(val)\n\n        # Country of study, used to load its holidays\n        self.country_of_study = self.config.get(self.COMPONENT_ID, \"country_of_study\")\n\n        # Initialise variable for working in each midterm_period\n        self.day_type = None\n        self.time_interval = None\n        self.current_mt_period = None\n        self.current_dps_data = None\n        self.current_dps_data_chunk = None\n\n    def _get_midterm_periods(self) -&gt; List[dict]:\n        \"\"\"Computes the date limits of each mid-term period, together with the limits of the regularity metrics' extra\n        dates\n\n        Returns:\n            List[dict]: list of dictionaries with the information of dates of each mid-term period\n        \"\"\"\n        midterm_periods = []\n        start_of_the_month = self.start_date\n\n        while True:\n            end_of_the_month = start_of_the_month.replace(\n                day=cal.monthrange(start_of_the_month.year, start_of_the_month.month)[1]\n            )\n            before_reg_date = start_of_the_month - dt.timedelta(days=self.before_reg_days)\n            after_reg_date = end_of_the_month + dt.timedelta(days=self.after_reg_days)\n\n            midterm_periods.append(\n                {\n                    \"month_start\": start_of_the_month,\n                    \"month_end\": end_of_the_month,\n                    \"extended_month_start\": before_reg_date,\n                    \"extended_month_end\": after_reg_date,\n                }\n            )\n\n            if end_of_the_month == self.end_date:\n                return midterm_periods\n\n            start_of_the_month = end_of_the_month + dt.timedelta(days=1)\n\n    def _check_weekday_number(self, num: str, context: str) -&gt; int:\n        \"\"\"Parses and validates a day of the week\n\n        Args:\n            num (str): string to be parsed to integer between 1 and 7\n            context (str): string for error tracking\n\n        Raises:\n            e: Error in parsing num to int\n            ValueError: num is not a valid day of the week (between 1 and 7 inclusive)\n\n        Returns:\n            int: integer representing a day of the week\n        \"\"\"\n        try:\n            num = int(num)\n        except Exception as e:\n            self.logger.error(f\"Must specify a day as an integer between 1 and 7, but found `{num}` in `{context}`\")\n            raise e\n        if num &lt; 1 or num &gt; 7:\n            raise ValueError(\n                f\"Days must take a value between 1 for Monday and 7 for Sunday, found {num} in `{context}`\"\n            )\n        return num\n\n    def _check_time_interval(self, interval: str, name: str) -&gt; dt.time:\n        \"\"\"Tries to parse time interval's start/end time from configuration file and check if it has\n        valid minutes (00, 15, 30, or 45). If so, returns the corresponding dt.time object\n\n        Args:\n            interval (str): interval string to be parsed to dt.datetime\n            name (str): name of the interval being parsed, used for error tracking\n\n        Raises:\n            e: Formatting error, cannot parse time as HH:MM (24h format)\n            ValueError: interval ends in non-allowed minutes\n\n        Returns:\n            dt.time: time of the start or end of the time interval\n        \"\"\"\n        try:\n            interval = dt.datetime.strptime(interval, \"%H:%M\")\n        except ValueError as e:\n            self.logger.error(f\"Could not parse {name}, expected HH:MM format, found {interval}\")\n            raise e\n\n        interval = interval.time()\n        if interval.minute not in [0, 15, 30, 45]:\n            raise ValueError(f\"Time interval {name} must have :00, :15, :30, or :45 minutes, found :{interval.minute}\")\n        return interval\n\n    def initalize_data_objects(self):\n        # Initialize data objects\n        input_silver_daily_ps_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"daily_permanence_score_data_silver\")\n        input_bronze_holiday_calendar_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"holiday_calendar_data_bronze\")\n        output_silver_midterm_ps_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"midterm_permanence_score_data_silver\")\n\n        # Clear destination directory if needed\n        clear_destination_directory = self.config.getboolean(\n            self.COMPONENT_ID, \"clear_destination_directory\", fallback=False\n        )\n        if clear_destination_directory:\n            self.logger.warning(f\"Deleting: {output_silver_midterm_ps_path}\")\n            delete_file_or_folder(self.spark, output_silver_midterm_ps_path)\n\n        daily_ps = SilverDailyPermanenceScoreDataObject(self.spark, input_silver_daily_ps_path)\n        holiday_calendar = BronzeHolidayCalendarDataObject(self.spark, input_bronze_holiday_calendar_path)\n        midterm_ps = SilverMidtermPermanenceScoreDataObject(self.spark, output_silver_midterm_ps_path)\n\n        self.input_data_objects = {\n            holiday_calendar.ID: holiday_calendar,\n            daily_ps.ID: daily_ps,\n        }\n        self.output_data_objects = {midterm_ps.ID: midterm_ps}\n\n    def _validate_and_load_daily_permanence_score(self, mt_period: dict) -&gt; DataFrame:\n        \"\"\"Loads the Daily Permanence Score data to be used for the calculation of the Mid-term Permanence Score metrics\n        of a particular mid-term period. Filters out DPS values equal to zero and checks that the time slots are\n        compatible with the configuration-provided time intervals.\n\n        Raises:\n            ValueError: If DPS data has a time slot duration different from 15, 30, or 60 minutes.\n            ValueError: If DPS data has 60-min slots but 15- or 30-min lengths are required.\n            ValueError: If DPS data has 30-min slots but 15-min lengths are required.\n\n        Returns:\n            dps: DataFrame of all DPS data necessary to calcualte the mid-term permanence score &amp; metrics of\n                self.current_mt_period\n        \"\"\"\n        dps = self.input_data_objects[SilverDailyPermanenceScoreDataObject.ID].df\n\n        # Add a one-day buffer, as later on the definition of a day does not match the midnight definition\n        dps = dps.filter(\n            F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n            &gt;= F.lit(mt_period[\"extended_month_start\"] - dt.timedelta(days=1))\n        ).filter(\n            F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n            &lt;= F.lit(mt_period[\"extended_month_end\"]) + dt.timedelta(days=1)\n        )\n\n        # If all time_intervals match a whole hour, no check needs to be done here.\n        if self.midterm_minutes == {0}:\n            return dps\n\n        # If the previous set was not disjoint, then the user has entered some time_interval that does not match a\n        # whole hour. We check, for all loaded dates, what their time slot length was. We only need to check the length\n        # of one time slot per DPS date.\n        time_slot_duration = (\n            dps.groupby([ColNames.year, ColNames.month, ColNames.day])\n            .agg(\n                F.first(F.col(ColNames.time_slot_end_time) - F.col(ColNames.time_slot_initial_time)).alias(\n                    \"time_slot_duration\"\n                )\n            )\n            .collect()\n        )\n        daily_duration = {\n            dt.date(row[\"year\"], row[\"month\"], row[\"day\"]): row[\"time_slot_duration\"].seconds // 60\n            for row in time_slot_duration\n        }\n\n        # If disjoint, user has specified intervals ending in :00 or :30. Then we admit 15- or 30-min time slot\n        # durations, but not whole hours.\n        # If not disjoint, user has specified some :15 or :45 intervals, and we can only admit 15-min time slots\n        check_for_half_hour_only = {15, 45}.isdisjoint(self.midterm_minutes)\n\n        for date, duration in daily_duration.items():\n            if duration not in {15, 30, 60}:\n                raise ValueError(\n                    f\"Found time_slot duration of {duration} min in DailyPermanenceScore of {date}, when \"\n                    \"accepted values are 15, 30, or 60 minutes.\"\n                )\n            if duration == 60:\n                if check_for_half_hour_only:\n                    msg = (\n                        f\"Found time_slot duration of 60 min in DailyPermanenceScore of {date}, when only accepted \"\n                        \"values are 15 and 30\"\n                    )\n                else:\n                    msg = (\n                        f\"Found time_slot duration of 60 min in DailyPermanenceScore of {date}, when only accepted \"\n                        \"value is 15\"\n                    )\n                raise ValueError(msg)\n            if duration == 30 and not check_for_half_hour_only:\n                raise ValueError(\n                    f\"Found time_slot duration of 30 min in DailyPermanenceScore of {date}, when only accepted \"\n                    \"value is 15\"\n                )\n        return dps\n\n    def filter_dps_by_time_interval(\n        self, df: DataFrame, subdaily_period: str, start: dt.time, end: dt.time\n    ) -&gt; DataFrame:\n        \"\"\"Filter DPS matrix by keeping the time slots that belong to the subdaily period or time interval\n        specified. Also create a new column, `date`, which contains to which day the time slot belongs to, as it\n        does not necessarily match the day the time slot belongs to (i.e. the `midnight` definition)\n\n        Args:\n            df (DataFrame): DPS dataframe to be filtered.\n            subdaily_period (str): name of the time interval or subdaily period. Must be one of the values in\n                MidtermPermanenceScore.TIME_INTERVALS.\n            start (dt.time): earliest time of accepted time slots that will not be filtered out\n            end (dt.time): latest time of accepted time slots that will not be filtered out\n\n        Raises:\n            ValueError: Whenever an unknown subdaily period is specified\n\n        Returns:\n            DataFrame: filtered DPS dataframe\n        \"\"\"\n        if not TimeIntervals.is_valid_type(subdaily_period):\n            raise ValueError(f\"Unknown subdaily/time_interval {subdaily_period}\")\n\n        # Auxiliary variables\n        start_hour = F.lit(start.hour)\n        start_min = F.lit(start.minute)\n        end_hour = end.hour\n        if end_hour == 0:\n            end_hour = F.lit(24)\n        else:\n            end_hour = F.lit(end_hour)\n        end_min = F.lit(end.minute)\n\n        slot_initial_hour = F.hour(ColNames.time_slot_initial_time)\n        slot_initial_min = F.minute(ColNames.time_slot_initial_time)\n        slot_end_hour = F.hour(ColNames.time_slot_end_time)\n        slot_end_hour = F.when(slot_end_hour == F.lit(0), F.lit(24)).otherwise(slot_end_hour)\n        slot_end_min = F.minute(ColNames.time_slot_end_time)\n\n        # Global time interval, taking all time slots\n        if subdaily_period == TimeIntervals.ALL:\n            if start != end:\n                raise ValueError(\n                    \"`all` time interval must have matching start and end times to not overlap with \"\n                    f\"different dates, found start={start} and end={end}\"\n                )\n            # If the day starts and ends at midnight, no additional logic needs to be done for assigning the correct\n            # date\n            if start == dt.time(0, 0):\n                df = df.withColumn(ColNames.date, F.col(ColNames.time_slot_initial_time).cast(DateType()))\n                return df\n            # If not: the hour that defines the day always belongs to that day.\n            # e.g., if subdaily_start=4, then day D is defined as 4:00AM of day D to 4:00 AM of day D+1\n            df = df.withColumn(\n                ColNames.date,\n                F.when(\n                    F.lit(60) * (slot_initial_hour - start_hour) &gt;= (start_min - slot_initial_min),\n                    F.col(ColNames.time_slot_initial_time),\n                )\n                .otherwise(F.date_add(F.col(ColNames.time_slot_initial_time), -1))\n                .cast(DateType()),\n            )\n            return df\n\n        # Rest of time intervals: night_time, evening_time, working_hours.\n        # Filter out time slots not contained in the time interval\n        if start &lt; end or end == dt.time(0, 0):\n            df = df.filter(\n                (F.lit(60) * (slot_initial_hour - start_hour) &gt;= (start_min - slot_initial_min))\n                &amp; (F.lit(60) * (slot_end_hour - end_hour) &lt;= (end_min - slot_end_min))\n            )\n        else:\n            df = df.filter(\n                (F.lit(60) * (slot_initial_hour - start_hour) &gt;= (start_min - slot_initial_min))\n                | (F.lit(60) * (slot_end_hour - end_hour) &lt;= (end_min - slot_end_min))\n            )\n\n        # consider self.day_start_hour = 4 for the following examples\n        if subdaily_period == TimeIntervals.NIGHT_TIME:\n            if start.hour &gt;= self.day_start_hour and (end &gt; start or end == dt.time(0, 0)):\n                # night-interval stays in the day, example [20:15 to 23:30] or [19:45 to 00:00]\n                df = df.withColumn(ColNames.date, F.col(ColNames.time_slot_initial_time).cast(DateType()))\n            elif start.hour &gt;= self.day_start_hour and end &lt; start and end != dt.time(0, 0):\n                # night-interval starts in the day and crosses midnight, example [20:15 to 03:30] or [19:00 to 18:45]\n                df = df.withColumn(\n                    ColNames.date,\n                    F.when(\n                        F.lit(60) * (slot_initial_hour - start_hour) &gt;= (start_min - slot_initial_min),\n                        F.col(ColNames.time_slot_initial_time),\n                    )\n                    .otherwise(F.date_add(F.col(ColNames.time_slot_initial_time), -1))\n                    .cast(DateType()),\n                )\n            elif start.hour &lt; self.day_start_hour and (end &gt; start or end == dt.time(0, 0)):\n                # night-interval starts in the next day and does not cross midnight, example [3:15 to 6:00] or [2:30 to 10:45]\n                df = df.withColumn(\n                    ColNames.date, F.date_add(F.col(ColNames.time_slot_initial_time), -1).cast(DateType())\n                )\n            return df\n\n        # if subdaily_period in (\"working_hours\", \"evening_time\"):\n        if start &gt;= end and end != dt.time(0, 0):\n            self.logger.log(\n                msg=f\"'inverted' start ({start}) and end ({end}) times found for subdaily period {subdaily_period}\"\n                \" -- the whole period will belong to the day of start of the interval\",\n                level=logging.INFO,\n            )\n        df = df.withColumn(\n            ColNames.date,\n            F.when(\n                F.lit(60) * (slot_initial_hour - start_hour) &gt;= (start_min - slot_initial_min),\n                F.col(ColNames.time_slot_initial_time),\n            )\n            .otherwise(F.date_add(F.col(ColNames.time_slot_initial_time), -1))\n            .cast(DateType()),\n        )\n        return df\n\n    def filter_dps_by_day_type(self, df: DataFrame, submonthly_period: str) -&gt; DataFrame:\n        \"\"\"Filter DPS matrix, with an assigned date column, keeping the time slots that belong to the day type or\n        submonthly period specified.\n\n        Args:\n            df (DataFrame): DPS dataframe, with assigned `date` column\n            submonthly_period (str): submonthly period or day type. Must be one of the values in\n                MidtermPermanenceScore.DAY_TYPE\n\n        Raises:\n            ValueError: Whenever an unknown submonthly period is specified\n\n        Returns:\n            DataFrame: Filtered dataframe\n        \"\"\"\n        # Now that we have the date column, we filter unneeded dates again, as we took one extra day before and after\n        df = df.filter(\n            F.col(ColNames.date).between(\n                lowerBound=self.current_mt_period[\"extended_month_start\"],\n                upperBound=self.current_mt_period[\"extended_month_end\"],\n            )\n        )\n\n        if not DayTypes.is_valid_type(submonthly_period):\n            raise ValueError(f\"Unknown submonthly period/day type `{submonthly_period}`\")\n        if submonthly_period == DayTypes.ALL:\n            return df\n\n        if submonthly_period == DayTypes.WEEKENDS:\n            df = df.filter((F.weekday(ColNames.date) + F.lit(1)).isin(self.weekend_days))\n            return df\n\n        # Handle specific weekdays\n        if submonthly_period in DayTypes.WEEKDAY_MAP.keys():\n            return df.filter(F.weekday(ColNames.date) == F.lit(DayTypes.WEEKDAY_MAP[submonthly_period]))\n\n        holidays = F.broadcast(self.input_data_objects[\"BronzeHolidayCalendarInfoDO\"].df)\n        holidays = holidays.filter(F.col(ColNames.iso2) == F.lit(self.country_of_study))\n        if submonthly_period == DayTypes.HOLIDAYS:\n            df = df.join(holidays.select(ColNames.date), on=ColNames.date, how=\"inner\")\n        # Workdays are all days falling in one of self.work_days and not being a holiday\n        if submonthly_period == DayTypes.WORKDAYS:\n            df_local = df.filter(F.col(ColNames.id_type) != UeGridIdType.ABROAD_STR).join(\n                holidays.select(ColNames.date), on=ColNames.date, how=\"left_anti\"\n            )\n            df_abroad = df.filter(F.col(ColNames.id_type) == UeGridIdType.ABROAD_STR)\n            df = df_local.unionByName(df_abroad).filter((F.weekday(ColNames.date) + F.lit(1)).isin(self.work_days))\n        return df\n\n    def compute_midterm_metrics(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Compute the mid-term permanence score and metrics of the current mid-term period, submonthly (i.e. day type)\n        and subdaily (i.e. time interval) combination.\n\n        Args:\n            df (DataFrame): filtered DPS DataFrame with added `date` column\n\n        Returns:\n            DataFrame: resulting DataFrame\n        \"\"\"\n        # Find latest stay of the user in each grid tile during the lookback period for regularity metrics\n        # (if it exists)\n        before_reg = (\n            df.filter(F.col(ColNames.date) &lt; F.lit(self.current_mt_period[\"month_start\"]))\n            .withColumn(ColNames.grid_id, F.explode(ColNames.dps))\n            .groupBy(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id, ColNames.id_type)\n            .agg(F.max(ColNames.date).alias(ColNames.date))\n        )\n\n        # Find earliest stay of the user in each grid tile during the lookback period for regularity metrics\n        # (if it exists)\n        after_reg = (\n            df.filter(F.col(ColNames.date) &gt; F.lit(self.current_mt_period[\"month_end\"]))\n            .withColumn(ColNames.grid_id, F.explode(ColNames.dps))\n            .groupBy(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id, ColNames.id_type)\n            .agg(F.min(ColNames.date).alias(ColNames.date))\n        )\n\n        # Current month data\n        study_df = df.filter(\n            F.col(ColNames.date).between(\n                lowerBound=self.current_mt_period[\"month_start\"], upperBound=self.current_mt_period[\"month_end\"]\n            )\n        )\n\n        # Device observation metric\n        observation_df = self._generate_device_observation_df(study_df)\n        # Calculate regularity metrics\n        regularity_df = self._generate_midterm_metrics_df(study_df, before_reg, after_reg)\n\n        combined_df = regularity_df.unionByName(observation_df)\n\n        return combined_df\n\n    def _generate_device_observation_df(self, study_df: DataFrame) -&gt; DataFrame:\n        return (\n            study_df.filter(F.col(ColNames.id_type) != F.lit(UeGridIdType.UKNOWN_STR))\n            .groupby(ColNames.user_id_modulo, ColNames.user_id, ColNames.date)\n            .agg(F.count_distinct(ColNames.time_slot_initial_time).alias(\"observed_day_dps\"))\n            .groupby(ColNames.user_id_modulo, ColNames.user_id)\n            .agg(\n                F.sum(\"observed_day_dps\").alias(ColNames.mps),\n                F.count_distinct(ColNames.date).cast(IntegerType()).alias(ColNames.frequency),\n            )\n            .select(\n                ColNames.user_id_modulo,\n                ColNames.user_id,\n                F.lit(UeGridIdType.DEVICE_OBSERVATION).alias(ColNames.grid_id),\n                F.col(ColNames.mps).cast(IntegerType()).alias(ColNames.mps),\n                ColNames.frequency,\n                F.lit(None).cast(FloatType()).alias(ColNames.regularity_mean),\n                F.lit(None).cast(FloatType()).alias(ColNames.regularity_std),\n                F.lit(UeGridIdType.DEVICE_OBSERVATION_STR).alias(ColNames.id_type),\n            )\n        )\n\n    def _generate_midterm_metrics_df(\n        self, study_df: DataFrame, before_reg: DataFrame, after_reg: DataFrame\n    ) -&gt; DataFrame:\n        df = (\n            study_df.withColumn(ColNames.grid_id, F.explode(ColNames.dps))\n            .withColumn(ColNames.dps, F.lit(1))\n            .select(\n                ColNames.user_id_modulo,\n                ColNames.user_id,\n                ColNames.grid_id,\n                ColNames.id_type,\n                ColNames.date,\n                ColNames.dps,\n            )\n            .union(before_reg.withColumn(ColNames.dps, F.lit(0)))\n            .union(after_reg.withColumn(ColNames.dps, F.lit(0)))\n            .groupby(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id, ColNames.id_type)\n            .agg(\n                F.sum(ColNames.dps).cast(IntegerType()).alias(ColNames.mps),\n                F.array_sort(F.collect_set(ColNames.date)).alias(\"dates\"),\n            )\n            .filter(F.col(ColNames.mps) &gt; 0)\n        )\n\n        return self._calculate_midterm_metrics(df)\n\n    def _calculate_midterm_metrics(self, study_df: DataFrame) -&gt; DataFrame:\n        # Temp cols\n        size_days_colname = \"size_days\"\n        dates_col = \"dates\"\n\n        # First, calculate frequency as size of dates col\n        df = study_df.withColumn(ColNames.frequency, F.size(F.col(dates_col)))\n\n        # --------------- Handle Extended start/end buffer ---------------\n\n        # Add extended_start and extended_end to dates if no date found in buffer\n        month_start = self.current_mt_period[\"month_start\"]\n        month_end = self.current_mt_period[\"month_end\"]\n        extended_start = self.current_mt_period[\"extended_month_start\"]\n        extended_end = self.current_mt_period[\"extended_month_end\"]\n\n        # Add extended_start and extended_end to dates if no date found in buffer\n        df = (\n            df\n            # --- Start buffer ---\n            .withColumn(\n                dates_col,\n                # If earliest date is not in start buffer, add buffer start bound\n                F.when(\n                    F.element_at(F.col(dates_col), 1) &gt;= F.lit(month_start),\n                    F.array_union(F.array(F.lit(extended_start)), F.col(dates_col)),\n                ).otherwise(F.col(dates_col)),\n            )\n            # --- End buffer ---\n            .withColumn(\n                dates_col,\n                # If latest date is not in end buffer, add buffer end bound\n                F.when(\n                    F.element_at(F.col(dates_col), -1) &lt;= F.lit(month_end),\n                    F.array_union(F.col(dates_col), F.array(F.lit(extended_end))),\n                ).otherwise(F.col(dates_col)),\n            )\n        )\n\n        # --------------- Date distances calculation ---------------\n        day_distances_col = \"day_distances\"\n        df = df.withColumn(\n            day_distances_col,\n            F.expr(\n                f\"\"\"\n                    transform(\n                        slice({dates_col}, 2, size({dates_col})),\n                        (current_day, idx) -&gt; datediff(current_day, element_at({dates_col}, idx + 1))\n                    )\n                \"\"\"\n            ),\n        )\n\n        # --------------- Metrics calculation ---------------\n        df = (\n            df\n            # --- Frequency ---\n            .withColumn(size_days_colname, F.size(dates_col))\n            # --- Regularity mean ---\n            .withColumn(\n                # Optimized way to calculate regularity mean\n                # (latest_date - earliest_date).days / (array_length - 1)\n                ColNames.regularity_mean,\n                F.when(\n                    F.col(size_days_colname) &gt; 0,\n                    F.date_diff(F.element_at(dates_col, -1), F.element_at(dates_col, 1))\n                    / (F.col(size_days_colname) - 1),\n                ).otherwise(0.0),\n            )\n            .drop(dates_col)\n            # --- Regularity deviation ---\n            .withColumn(\n                ColNames.regularity_std,\n                F.when(\n                    F.col(size_days_colname) &gt; 1,  # Due to interval array being 1 less than the size of the dates array\n                    # (sum((dd - mean) ** 2 for dd in diffs) / (array_length - 2)) ** 0.5\n                    F.sqrt(\n                        F.expr(\n                            f\"\"\"\n                            aggregate(\n                                {day_distances_col},\n                                CAST(0 AS DOUBLE),\n                                (acc, x) -&gt; acc + POWER(x - {ColNames.regularity_mean}, 2)\n                            ) / (size({day_distances_col}) - 1)\n                        \"\"\"\n                        )\n                    ),\n                ).otherwise(0.0),\n            )\n            # Remove temp columns\n            .drop(day_distances_col)\n            .drop(size_days_colname)\n        )\n\n        return df\n\n    def transform(self):\n        # Load all needed dps\n        if self.time_interval == TimeIntervals.ALL:\n            time_interval_start = dt.time(hour=self.day_start_hour)\n            time_interval_end = dt.time(hour=self.day_start_hour)\n        else:\n            time_interval_start = getattr(self, f\"{self.time_interval}_start\")\n            time_interval_end = getattr(self, f\"{self.time_interval}_end\")\n\n        # Keep only time slots belonging to the time interval\n        filtered = self.filter_dps_by_time_interval(\n            self.current_dps_data_chunk, self.time_interval, time_interval_start, time_interval_end\n        )\n\n        # Keep only time slots belonging to the day type\n        filtered = self.filter_dps_by_day_type(filtered, self.day_type)\n\n        # Compute metrics\n        mps = self.compute_midterm_metrics(filtered)\n\n        mps = (\n            mps.withColumn(ColNames.day_type, F.lit(self.day_type))\n            .withColumn(ColNames.time_interval, F.lit(self.time_interval))\n            .withColumn(ColNames.year, F.lit(self.current_mt_period[\"month_start\"].year).cast(ShortType()))\n            .withColumn(ColNames.month, F.lit(self.current_mt_period[\"month_start\"].month).cast(ByteType()))\n        )\n        mps = apply_schema_casting(mps, SilverMidtermPermanenceScoreDataObject.SCHEMA)\n\n        mps = mps.repartition(*SilverMidtermPermanenceScoreDataObject.PARTITION_COLUMNS)\n\n        self.output_data_objects[SilverMidtermPermanenceScoreDataObject.ID].df = mps\n\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(\"Reading data objects...\")\n        self.read()\n        self.logger.info(\"... data objects read!\")\n\n        partition_chunks = self._get_partition_chunks()\n\n        midterm_daily_data = []\n\n        self.logger.info(\"Validating DPS data for each mid-term period...\")\n        for i, mt_period in enumerate(self.midterm_periods):\n            self.logger.info(\n                f\"... validating {mt_period['extended_month_start']} to {mt_period['extended_month_end']} ...\"\n            )\n            midterm_daily_data.append(self._validate_and_load_daily_permanence_score(mt_period))\n        self.logger.info(\"... all mid-term periods validated!\")\n\n        self.logger.info(\"Starting mid-term permanece score &amp; metrics computation...\")\n        for i, mt_period in enumerate(self.midterm_periods):\n            self.current_mt_period = mt_period\n            self.current_dps_data = midterm_daily_data[i]\n            self.logger.info(f\"... working on month {mt_period['month_start']} to {mt_period['month_end']}\")\n\n            for day_type, time_intervals in self.period_combinations.items():\n                self.day_type = day_type\n\n                for time_interval in time_intervals:\n                    self.time_interval = time_interval\n\n                    for i, partition_chunk in enumerate(partition_chunks):\n                        self.logger.info(f\"Processing partition chunk {i}\")\n                        self.logger.debug(f\"Partition chunk: {partition_chunk}\")\n                        if partition_chunk is not None:\n                            self.current_dps_data_chunk = self.current_dps_data.filter(\n                                F.col(ColNames.user_id_modulo).isin(partition_chunk)\n                            )\n                        else:\n                            self.current_dps_data_chunk = self.current_dps_data\n\n                        self.transform()\n                        self.write()\n                        self.logger.info(f\"Finished processing partition chunk {i}\")\n                    self.logger.info(\n                        f\"... finished saving results for day_type `{self.day_type}` and time_interval `{self.time_interval}`\"\n                    )\n            self.logger.info(\n                f\"... finished saving results for month {mt_period['month_start']} to {mt_period['month_end']}\"\n            )\n\n        self.logger.info(\"... Finished!\")\n\n    def _get_partition_chunks(self) -&gt; List[List[int]]:\n        \"\"\"\n        Method that returns the partition chunks for the current date.\n\n        Returns:\n            List[List[int, int]]: list of partition chunks. If the partition_chunk_size is not defined in the config or\n                the number of partitions is less than the desired chunk size, it will return a list with a single None element.\n        \"\"\"\n        # Get partitions desired\n        partition_chunk_size = self.config.getint(self.COMPONENT_ID, \"partition_chunk_size\", fallback=None)\n        number_of_partitions = self.config.getint(self.COMPONENT_ID, \"number_of_partitions\", fallback=None)\n\n        if partition_chunk_size is None or number_of_partitions is None or partition_chunk_size &lt;= 0:\n            return [None]\n\n        if number_of_partitions &lt;= partition_chunk_size:\n            self.logger.warning(\n                f\"Available Partition number ({number_of_partitions}) is \"\n                f\"less than the desired chunk size ({partition_chunk_size}). \"\n                f\"Using all partitions.\"\n            )\n            return [None]\n        partition_chunks = [\n            list(range(i, min(i + partition_chunk_size, number_of_partitions)))\n            for i in range(0, number_of_partitions, partition_chunk_size)\n        ]\n        # NOTE: Generate chunks if partition_values were read for each day\n        # getting exactly the amount of partitions for that day\n\n        # partition_chunks = [\n        #     partition_values[i : i + partition_chunk_size]\n        #     for i in range(0, partition_values_size, partition_chunk_size)\n        # ]\n\n        return partition_chunks\n</code></pre>"},{"location":"reference/components/execution/midterm_permanence_score/midterm_permanence_score/#components.execution.midterm_permanence_score.midterm_permanence_score.MidtermPermanenceScore.compute_midterm_metrics","title":"<code>compute_midterm_metrics(df)</code>","text":"<p>Compute the mid-term permanence score and metrics of the current mid-term period, submonthly (i.e. day type) and subdaily (i.e. time interval) combination.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>filtered DPS DataFrame with added <code>date</code> column</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>resulting DataFrame</p> Source code in <code>multimno/components/execution/midterm_permanence_score/midterm_permanence_score.py</code> <pre><code>def compute_midterm_metrics(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Compute the mid-term permanence score and metrics of the current mid-term period, submonthly (i.e. day type)\n    and subdaily (i.e. time interval) combination.\n\n    Args:\n        df (DataFrame): filtered DPS DataFrame with added `date` column\n\n    Returns:\n        DataFrame: resulting DataFrame\n    \"\"\"\n    # Find latest stay of the user in each grid tile during the lookback period for regularity metrics\n    # (if it exists)\n    before_reg = (\n        df.filter(F.col(ColNames.date) &lt; F.lit(self.current_mt_period[\"month_start\"]))\n        .withColumn(ColNames.grid_id, F.explode(ColNames.dps))\n        .groupBy(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id, ColNames.id_type)\n        .agg(F.max(ColNames.date).alias(ColNames.date))\n    )\n\n    # Find earliest stay of the user in each grid tile during the lookback period for regularity metrics\n    # (if it exists)\n    after_reg = (\n        df.filter(F.col(ColNames.date) &gt; F.lit(self.current_mt_period[\"month_end\"]))\n        .withColumn(ColNames.grid_id, F.explode(ColNames.dps))\n        .groupBy(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id, ColNames.id_type)\n        .agg(F.min(ColNames.date).alias(ColNames.date))\n    )\n\n    # Current month data\n    study_df = df.filter(\n        F.col(ColNames.date).between(\n            lowerBound=self.current_mt_period[\"month_start\"], upperBound=self.current_mt_period[\"month_end\"]\n        )\n    )\n\n    # Device observation metric\n    observation_df = self._generate_device_observation_df(study_df)\n    # Calculate regularity metrics\n    regularity_df = self._generate_midterm_metrics_df(study_df, before_reg, after_reg)\n\n    combined_df = regularity_df.unionByName(observation_df)\n\n    return combined_df\n</code></pre>"},{"location":"reference/components/execution/midterm_permanence_score/midterm_permanence_score/#components.execution.midterm_permanence_score.midterm_permanence_score.MidtermPermanenceScore.filter_dps_by_day_type","title":"<code>filter_dps_by_day_type(df, submonthly_period)</code>","text":"<p>Filter DPS matrix, with an assigned date column, keeping the time slots that belong to the day type or submonthly period specified.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DPS dataframe, with assigned <code>date</code> column</p> required <code>submonthly_period</code> <code>str</code> <p>submonthly period or day type. Must be one of the values in MidtermPermanenceScore.DAY_TYPE</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Whenever an unknown submonthly period is specified</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered dataframe</p> Source code in <code>multimno/components/execution/midterm_permanence_score/midterm_permanence_score.py</code> <pre><code>def filter_dps_by_day_type(self, df: DataFrame, submonthly_period: str) -&gt; DataFrame:\n    \"\"\"Filter DPS matrix, with an assigned date column, keeping the time slots that belong to the day type or\n    submonthly period specified.\n\n    Args:\n        df (DataFrame): DPS dataframe, with assigned `date` column\n        submonthly_period (str): submonthly period or day type. Must be one of the values in\n            MidtermPermanenceScore.DAY_TYPE\n\n    Raises:\n        ValueError: Whenever an unknown submonthly period is specified\n\n    Returns:\n        DataFrame: Filtered dataframe\n    \"\"\"\n    # Now that we have the date column, we filter unneeded dates again, as we took one extra day before and after\n    df = df.filter(\n        F.col(ColNames.date).between(\n            lowerBound=self.current_mt_period[\"extended_month_start\"],\n            upperBound=self.current_mt_period[\"extended_month_end\"],\n        )\n    )\n\n    if not DayTypes.is_valid_type(submonthly_period):\n        raise ValueError(f\"Unknown submonthly period/day type `{submonthly_period}`\")\n    if submonthly_period == DayTypes.ALL:\n        return df\n\n    if submonthly_period == DayTypes.WEEKENDS:\n        df = df.filter((F.weekday(ColNames.date) + F.lit(1)).isin(self.weekend_days))\n        return df\n\n    # Handle specific weekdays\n    if submonthly_period in DayTypes.WEEKDAY_MAP.keys():\n        return df.filter(F.weekday(ColNames.date) == F.lit(DayTypes.WEEKDAY_MAP[submonthly_period]))\n\n    holidays = F.broadcast(self.input_data_objects[\"BronzeHolidayCalendarInfoDO\"].df)\n    holidays = holidays.filter(F.col(ColNames.iso2) == F.lit(self.country_of_study))\n    if submonthly_period == DayTypes.HOLIDAYS:\n        df = df.join(holidays.select(ColNames.date), on=ColNames.date, how=\"inner\")\n    # Workdays are all days falling in one of self.work_days and not being a holiday\n    if submonthly_period == DayTypes.WORKDAYS:\n        df_local = df.filter(F.col(ColNames.id_type) != UeGridIdType.ABROAD_STR).join(\n            holidays.select(ColNames.date), on=ColNames.date, how=\"left_anti\"\n        )\n        df_abroad = df.filter(F.col(ColNames.id_type) == UeGridIdType.ABROAD_STR)\n        df = df_local.unionByName(df_abroad).filter((F.weekday(ColNames.date) + F.lit(1)).isin(self.work_days))\n    return df\n</code></pre>"},{"location":"reference/components/execution/midterm_permanence_score/midterm_permanence_score/#components.execution.midterm_permanence_score.midterm_permanence_score.MidtermPermanenceScore.filter_dps_by_time_interval","title":"<code>filter_dps_by_time_interval(df, subdaily_period, start, end)</code>","text":"<p>Filter DPS matrix by keeping the time slots that belong to the subdaily period or time interval specified. Also create a new column, <code>date</code>, which contains to which day the time slot belongs to, as it does not necessarily match the day the time slot belongs to (i.e. the <code>midnight</code> definition)</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DPS dataframe to be filtered.</p> required <code>subdaily_period</code> <code>str</code> <p>name of the time interval or subdaily period. Must be one of the values in MidtermPermanenceScore.TIME_INTERVALS.</p> required <code>start</code> <code>time</code> <p>earliest time of accepted time slots that will not be filtered out</p> required <code>end</code> <code>time</code> <p>latest time of accepted time slots that will not be filtered out</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Whenever an unknown subdaily period is specified</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>filtered DPS dataframe</p> Source code in <code>multimno/components/execution/midterm_permanence_score/midterm_permanence_score.py</code> <pre><code>def filter_dps_by_time_interval(\n    self, df: DataFrame, subdaily_period: str, start: dt.time, end: dt.time\n) -&gt; DataFrame:\n    \"\"\"Filter DPS matrix by keeping the time slots that belong to the subdaily period or time interval\n    specified. Also create a new column, `date`, which contains to which day the time slot belongs to, as it\n    does not necessarily match the day the time slot belongs to (i.e. the `midnight` definition)\n\n    Args:\n        df (DataFrame): DPS dataframe to be filtered.\n        subdaily_period (str): name of the time interval or subdaily period. Must be one of the values in\n            MidtermPermanenceScore.TIME_INTERVALS.\n        start (dt.time): earliest time of accepted time slots that will not be filtered out\n        end (dt.time): latest time of accepted time slots that will not be filtered out\n\n    Raises:\n        ValueError: Whenever an unknown subdaily period is specified\n\n    Returns:\n        DataFrame: filtered DPS dataframe\n    \"\"\"\n    if not TimeIntervals.is_valid_type(subdaily_period):\n        raise ValueError(f\"Unknown subdaily/time_interval {subdaily_period}\")\n\n    # Auxiliary variables\n    start_hour = F.lit(start.hour)\n    start_min = F.lit(start.minute)\n    end_hour = end.hour\n    if end_hour == 0:\n        end_hour = F.lit(24)\n    else:\n        end_hour = F.lit(end_hour)\n    end_min = F.lit(end.minute)\n\n    slot_initial_hour = F.hour(ColNames.time_slot_initial_time)\n    slot_initial_min = F.minute(ColNames.time_slot_initial_time)\n    slot_end_hour = F.hour(ColNames.time_slot_end_time)\n    slot_end_hour = F.when(slot_end_hour == F.lit(0), F.lit(24)).otherwise(slot_end_hour)\n    slot_end_min = F.minute(ColNames.time_slot_end_time)\n\n    # Global time interval, taking all time slots\n    if subdaily_period == TimeIntervals.ALL:\n        if start != end:\n            raise ValueError(\n                \"`all` time interval must have matching start and end times to not overlap with \"\n                f\"different dates, found start={start} and end={end}\"\n            )\n        # If the day starts and ends at midnight, no additional logic needs to be done for assigning the correct\n        # date\n        if start == dt.time(0, 0):\n            df = df.withColumn(ColNames.date, F.col(ColNames.time_slot_initial_time).cast(DateType()))\n            return df\n        # If not: the hour that defines the day always belongs to that day.\n        # e.g., if subdaily_start=4, then day D is defined as 4:00AM of day D to 4:00 AM of day D+1\n        df = df.withColumn(\n            ColNames.date,\n            F.when(\n                F.lit(60) * (slot_initial_hour - start_hour) &gt;= (start_min - slot_initial_min),\n                F.col(ColNames.time_slot_initial_time),\n            )\n            .otherwise(F.date_add(F.col(ColNames.time_slot_initial_time), -1))\n            .cast(DateType()),\n        )\n        return df\n\n    # Rest of time intervals: night_time, evening_time, working_hours.\n    # Filter out time slots not contained in the time interval\n    if start &lt; end or end == dt.time(0, 0):\n        df = df.filter(\n            (F.lit(60) * (slot_initial_hour - start_hour) &gt;= (start_min - slot_initial_min))\n            &amp; (F.lit(60) * (slot_end_hour - end_hour) &lt;= (end_min - slot_end_min))\n        )\n    else:\n        df = df.filter(\n            (F.lit(60) * (slot_initial_hour - start_hour) &gt;= (start_min - slot_initial_min))\n            | (F.lit(60) * (slot_end_hour - end_hour) &lt;= (end_min - slot_end_min))\n        )\n\n    # consider self.day_start_hour = 4 for the following examples\n    if subdaily_period == TimeIntervals.NIGHT_TIME:\n        if start.hour &gt;= self.day_start_hour and (end &gt; start or end == dt.time(0, 0)):\n            # night-interval stays in the day, example [20:15 to 23:30] or [19:45 to 00:00]\n            df = df.withColumn(ColNames.date, F.col(ColNames.time_slot_initial_time).cast(DateType()))\n        elif start.hour &gt;= self.day_start_hour and end &lt; start and end != dt.time(0, 0):\n            # night-interval starts in the day and crosses midnight, example [20:15 to 03:30] or [19:00 to 18:45]\n            df = df.withColumn(\n                ColNames.date,\n                F.when(\n                    F.lit(60) * (slot_initial_hour - start_hour) &gt;= (start_min - slot_initial_min),\n                    F.col(ColNames.time_slot_initial_time),\n                )\n                .otherwise(F.date_add(F.col(ColNames.time_slot_initial_time), -1))\n                .cast(DateType()),\n            )\n        elif start.hour &lt; self.day_start_hour and (end &gt; start or end == dt.time(0, 0)):\n            # night-interval starts in the next day and does not cross midnight, example [3:15 to 6:00] or [2:30 to 10:45]\n            df = df.withColumn(\n                ColNames.date, F.date_add(F.col(ColNames.time_slot_initial_time), -1).cast(DateType())\n            )\n        return df\n\n    # if subdaily_period in (\"working_hours\", \"evening_time\"):\n    if start &gt;= end and end != dt.time(0, 0):\n        self.logger.log(\n            msg=f\"'inverted' start ({start}) and end ({end}) times found for subdaily period {subdaily_period}\"\n            \" -- the whole period will belong to the day of start of the interval\",\n            level=logging.INFO,\n        )\n    df = df.withColumn(\n        ColNames.date,\n        F.when(\n            F.lit(60) * (slot_initial_hour - start_hour) &gt;= (start_min - slot_initial_min),\n            F.col(ColNames.time_slot_initial_time),\n        )\n        .otherwise(F.date_add(F.col(ColNames.time_slot_initial_time), -1))\n        .cast(DateType()),\n    )\n    return df\n</code></pre>"},{"location":"reference/components/execution/multimno_aggregation/","title":"multimno_aggregation","text":""},{"location":"reference/components/execution/multimno_aggregation/multimno_aggregation/","title":"multimno_aggregation","text":"<p>Module that implements the MultiMNO aggregation component</p>"},{"location":"reference/components/execution/multimno_aggregation/multimno_aggregation/#components.execution.multimno_aggregation.multimno_aggregation.MultiMNOAggregation","title":"<code>MultiMNOAggregation</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class responsible for the aggregation of indicators computed in different MNOs into a single, aggregate MultiMNO indicator.</p> Source code in <code>multimno/components/execution/multimno_aggregation/multimno_aggregation.py</code> <pre><code>class MultiMNOAggregation(Component):\n    \"\"\"\n    Class responsible for the aggregation of indicators computed in different MNOs into a single, aggregate MultiMNO\n    indicator.\n    \"\"\"\n\n    COMPONENT_ID = \"MultiMNOAggregation\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n        # self.use_case, self.number_of_single_mnos already defined\n\n        self.single_mno_factors = [\n            self.config.getfloat(self.COMPONENT_ID, f\"single_mno_{i}_factor\")\n            for i in range(1, self.number_of_single_mnos + 1)\n        ]\n\n        # Initialise use-case specific partition/segmentation values read from config\n        if self.use_case == InternalMigration.COMPONENT_ID:\n            self.init_internal_migration()\n        elif self.use_case == PresentPopulationEstimation.COMPONENT_ID:\n            self.init_present_population()\n        elif self.use_case == UsualEnvironmentAggregation.COMPONENT_ID:\n            self.init_usual_environment()\n        elif self.use_case == TourismStatisticsCalculation.COMPONENT_ID:\n            self.init_inbound_tourism()\n        elif self.use_case == TourismOutboundStatisticsCalculation.COMPONENT_ID:\n            self.init_outbound_tourism()\n\n        if self.zoning_dataset in ReservedDatasetIDs():\n            if self.use_case == InternalMigration.COMPONENT_ID:\n                raise ValueError(\n                    f\"Reserved dataset {self.zoning_dataset} execution not implemented for\",\n                    InternalMigration.COMPONENT_ID,\n                )\n            # force level to 1 for reserved datasets\n            self.logger.info(f\"zoning_dataset_id is {self.zoning_dataset} -- forcing hierarchical levels to `[1]`\")\n            self.levels = [1]\n        elif self.zoning_dataset is None:  # no zoning or level needed, e.g. for outbound use case\n            pass\n        else:\n            levels = self.config.get(f\"{self.COMPONENT_ID}.{self.use_case}\", \"hierarchical_levels\")\n            self.levels = list(int(x.strip()) for x in levels.split(\",\"))\n            if len(levels) == 0:\n                raise ValueError(f\"Provide at least one hierarchical level -- encountered an empty list\")\n\n    def initalize_data_objects(self):\n        self.use_case = self.config.get(self.COMPONENT_ID, \"use_case\")\n        self.section = f\"{self.COMPONENT_ID}.{self.use_case}\"\n        self.number_of_single_mnos = self.config.getint(self.COMPONENT_ID, \"number_of_single_mnos\")\n\n        if self.number_of_single_mnos &lt;= 1:\n            raise ValueError(\n                f\"Number of single MNOs to aggregate must be 2 or greater -- got {self.number_of_single_mnos}\"\n            )\n\n        clear_destination_directory = self.config.getboolean(self.COMPONENT_ID, \"clear_destination_directory\")\n\n        self.input_data_objects = {}\n        self.output_data_objects = {}\n\n        for do_info in USE_CASE_DATA_OBJECTS[self.use_case]:\n            input_do_path_maker = do_info[\"input_path_config_key\"]\n            output_do_path = self.config.get(CONFIG_GOLD_PATHS_KEY, do_info[\"output_path_config_key\"])\n\n            for i in range(1, self.number_of_single_mnos + 1):\n                input_do_path = self.config.get(CONFIG_GOLD_PATHS_KEY, input_do_path_maker(i))\n                if not check_if_data_path_exists(self.spark, input_do_path):\n                    self.logger.warning(f\"Expected path {input_do_path} to exist but it does not\")\n                    raise ValueError(f\"Invalid path for {do_info['constructor'].ID}: {input_do_path}\")\n\n                self.input_data_objects[f\"{do_info['constructor'].ID}_{str(i)}\"] = do_info[\"constructor\"](\n                    self.spark, input_do_path\n                )\n\n            if clear_destination_directory:\n                delete_file_or_folder(self.spark, output_do_path)\n\n            self.output_data_objects[do_info[\"constructor\"].ID] = do_info[\"constructor\"](self.spark, output_do_path)\n\n    def _parse_validate_month(self, config_key: str) -&gt; dt.datetime:\n        \"\"\"Parse and validate month string from config file\n\n        Args:\n            config_key (str): config key of the parameter to parse and validate\n\n        Raises:\n            ValueError: parameter could not be parsed using YYYY-MM format\n\n        Returns:\n            dt.date: first day of the specified month\n        \"\"\"\n        config_input = self.config.get(self.section, config_key)\n        try:\n            out = dt.datetime.strptime(config_input, \"%Y-%m\").date()\n        except ValueError as e:\n            err_msg = f\"Could not parse parameter {config_key} = `{config_input}` -- expected format: YYYY-MM\"\n            self.logger.error(err_msg)\n            raise e(err_msg)\n        return out\n\n    def _parse_validate_season(self, config_key: str) -&gt; str:\n        \"\"\"Parse and validate season string from config file\n\n        Args:\n            config_key (str): config key of the parameter to parse and validate\n\n        Raises:\n            ValueError: parameter was not one of the valid season values\n\n        Returns:\n            str: season value\n        \"\"\"\n        config_input = self.config.get(self.section, config_key)\n        if not Seasons.is_valid_type(config_input):\n            err_msg = f\"Unknown season {config_input} -- valid values are {Seasons.values()}\"\n            self.logger.error(err_msg)\n            raise ValueError(err_msg)\n\n        return config_input\n\n    def _parse_validate_date(self, config_key: str) -&gt; dt.date:\n        \"\"\"Parse and validate date string from config file\n\n        Args:\n            config_key (str): config key of the parameter to parse and validate\n\n        Raises:\n            ValueError: could not parse parameter using YYYY-MM-DD format\n\n        Returns:\n            dt.date: specified date in the config\n        \"\"\"\n        config_input = self.config.get(self.section, config_key)\n        try:\n            out = dt.datetime.strptime(config_input, \"%Y-%m-%d\").date()\n        except ValueError as e:\n            err_msg = f\"Could not parse parameter {config_key} = `{config_input}` -- expected format: YYYY-MM-DD\"\n            self.logger.error(err_msg)\n            raise e(err_msg)\n        return out\n\n    def init_internal_migration(self):\n        \"\"\"\n        Initialises parameters to filter internal migration data for this execution\n        \"\"\"\n        self.start_date_prev = self._parse_validate_month(\"start_month_previous\")\n        self.end_date_prev = self._parse_validate_month(\"end_month_previous\")\n        self.end_date_prev = self.end_date_prev + dt.timedelta(\n            days=cal.monthrange(self.end_date_prev.year, self.end_date_prev.month)[1] - 1\n        )\n        self.season_prev = self._parse_validate_season(\"season_previous\")\n\n        self.start_date_new = self._parse_validate_month(\"start_month_new\")\n        self.end_date_new = self._parse_validate_month(\"end_month_new\")\n        self.end_date_new = self.end_date_new + dt.timedelta(\n            days=cal.monthrange(self.end_date_new.year, self.end_date_new.month)[1] - 1\n        )\n        self.season_new = self._parse_validate_season(\"season_new\")\n\n        self.zoning_dataset = self.config.get(f\"{self.COMPONENT_ID}.{self.use_case}\", \"zoning_dataset_id\")\n\n    def init_present_population(self):\n        \"\"\"\n        Initialises parameters to filter present population for this execution\n        \"\"\"\n        self.start_date = self._parse_validate_date(\"start_date\")\n        self.end_date = self._parse_validate_date(\"end_date\")\n\n        self.zoning_dataset = self.config.get(f\"{self.COMPONENT_ID}.{self.use_case}\", \"zoning_dataset_id\")\n\n    def init_inbound_tourism(self):\n        \"\"\"\n        Initialises parameters to filter inbound tourism for this execution\n        \"\"\"\n        self.start_month = self._parse_validate_month(\"start_month\")\n        self.end_month = self._parse_validate_month(\"end_month\")\n\n        self.zoning_dataset = self.config.get(f\"{self.COMPONENT_ID}.{self.use_case}\", \"zoning_dataset_id\")\n\n    def init_outbound_tourism(self):\n        \"\"\"\n        Initialises parameters to filter outbound tourism for this execution\n        \"\"\"\n        self.start_month = self._parse_validate_month(\"start_month\")\n        self.end_month = self._parse_validate_month(\"end_month\")\n\n        self.zoning_dataset = None\n\n    def init_usual_environment(self):\n        \"\"\"\n        Initialises parameters to filter usual environment for this execution\n\n        Raises:\n            ValueError: If no UE label was specified\n        \"\"\"\n        labels = self.config.get(f\"{self.COMPONENT_ID}.{self.use_case}\", \"labels\")\n        labels = list(x.strip() for x in labels.split(\",\"))\n\n        if len(labels) == 0:\n            raise ValueError(f\"Provide at least one usual environment label -- encountered an empty list\")\n\n        self.labels = labels\n        self.start_date = self._parse_validate_month(\"start_month\")\n        self.end_date = self._parse_validate_month(\"end_month\")\n        self.end_date = self.end_date + dt.timedelta(\n            days=cal.monthrange(self.end_date.year, self.end_date.month)[1] - 1\n        )\n        self.season = self.config.get(f\"{self.COMPONENT_ID}.{self.use_case}\", \"season\")\n        if not Seasons.is_valid_type(self.season):\n            err_msg = f\"Unknown season `{self.season}` specified -- must be one of {Seasons.values()}\"\n            self.logger.error(err_msg)\n            raise ValueError(err_msg)\n\n        self.zoning_dataset = self.config.get(f\"{self.COMPONENT_ID}.{self.use_case}\", \"zoning_dataset_id\")\n\n    def filter_dataframe(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Filters dataframe according to the config-specified values for the data objects belonging to a specific\n        use case\n\n        Args:\n            df (DataFrame): dataframe to be filtered\n\n        Raises:\n            NotImplementedError: use case that has not been implemented for this function\n\n        Returns:\n            DataFrame: filtered dataframe\n        \"\"\"\n        if self.use_case == InternalMigration.COMPONENT_ID:\n            df = (\n                df.where(F.col(ColNames.dataset_id) == self.zoning_dataset)\n                .where(F.col(ColNames.level).isin(self.levels))\n                .where(F.col(ColNames.start_date_previous) == self.start_date_prev)\n                .where(F.col(ColNames.end_date_previous) == self.end_date_prev)\n                .where(F.col(ColNames.season_previous) == self.season_prev)\n                .where(F.col(ColNames.start_date_new) == self.start_date_new)\n                .where(F.col(ColNames.end_date_new) == self.end_date_new)\n                .where(F.col(ColNames.season_new) == self.season_new)\n            )\n        elif self.use_case == PresentPopulationEstimation.COMPONENT_ID:\n            df = df.where(\n                F.make_date(ColNames.year, ColNames.month, ColNames.day).between(self.start_date, self.end_date)\n            )\n        elif self.use_case == UsualEnvironmentAggregation.COMPONENT_ID:\n            df = (\n                df.where(F.col(ColNames.label).isin(self.labels))\n                .where(F.col(ColNames.start_date) == self.start_date)\n                .where(F.col(ColNames.end_date) == self.end_date)\n                .where(F.col(ColNames.season) == self.season)\n            )\n        elif self.use_case == TourismStatisticsCalculation.COMPONENT_ID:\n            # Same filter applies to both data objects\n            df = (\n                df.where(F.col(ColNames.dataset_id) == self.zoning_dataset)\n                .where(F.col(ColNames.level).isin(self.levels))\n                .where(\n                    (F.col(ColNames.year) &gt; self.start_month.year)\n                    | (\n                        (F.col(ColNames.year) == self.start_month.year)\n                        &amp; (F.col(ColNames.month) &gt;= self.start_month.month)\n                    )\n                )\n                .where(\n                    (F.col(ColNames.year) &lt; self.end_month.year)\n                    | ((F.col(ColNames.year) == self.end_month.year) &amp; (F.col(ColNames.month) &lt;= self.end_month.month))\n                )\n            )\n        elif self.use_case == TourismOutboundStatisticsCalculation.COMPONENT_ID:\n            df = df.where(\n                (F.col(ColNames.year) &gt; self.start_month.year)\n                | ((F.col(ColNames.year) == self.start_month.year) &amp; (F.col(ColNames.month) &gt;= self.start_month.month))\n            ).where(\n                (F.col(ColNames.year) &lt; self.end_month.year)\n                | ((F.col(ColNames.year) == self.end_month.year) &amp; (F.col(ColNames.month) &lt;= self.end_month.month))\n            )\n        else:\n            raise NotImplementedError(f\"use case {self.use_case}\")\n\n        return df\n\n    @staticmethod\n    def filter_out_obfuscated_records(dfs: list[DataFrame], target_columns: list[str]) -&gt; list[DataFrame]:\n        \"\"\"Filter out obfuscated records, which are records flagged by the k-anonymity process to have values lower\n        than k, for some value k. Specific treatment for these obfuscated values is not yet defined, so we simply\n        remove them in this version of the code.\n\n        Args:\n            dfs (list[DataFrame]): list of single MNO dataframes with possibly obfuscated values\n            target_column (str): name of the target column containig the value of the indicator\n\n        Returns:\n            list[DataFrame]: list of single MNO dataframes without obfuscated values\n        \"\"\"\n        output_dfs = []\n\n        for df in dfs:\n            for col in target_columns:\n                df = df.where(F.col(col) &gt;= F.lit(0))\n            output_dfs.append(df)\n        return output_dfs\n\n    def aggregate_single_mno_indicators(self, dfs: list[DataFrame], do_info: dict) -&gt; DataFrame:\n        \"\"\"Aggregate the indicator over all single MNOs' data. First, the value of each MNO is multiplied by some\n        weight, and then these values are added up.\n\n        Args:\n            dfs (list[DataFrame]): list of single MNO dataframes\n\n        Returns:\n            DataFrame: final dataframe with the weighted sum indicator of all MNOs\n        \"\"\"\n        weighted_dfs = []\n\n        for i, df in enumerate(dfs):\n            for col in do_info[\"target_columns\"]:\n                df = df.withColumn(col, F.col(col) * F.lit(self.single_mno_factors[i]))\n            weighted_dfs.append(df)\n\n        groupby_cols = [\n            column for column in do_info[\"constructor\"].SCHEMA.names if column not in do_info[\"target_columns\"]\n        ]\n\n        agg_df = (\n            reduce(DataFrame.union, weighted_dfs)\n            .groupBy(*groupby_cols)\n            .agg(*[F.sum(col).alias(col) for col in do_info[\"target_columns\"]])\n        )\n\n        agg_df = apply_schema_casting(agg_df, do_info[\"constructor\"].SCHEMA)\n\n        return agg_df\n\n    def transform(self):\n        # Get list of input single MNO indicators\n        for do_info in USE_CASE_DATA_OBJECTS[self.use_case]:\n\n            single_mno_dfs = [\n                self.input_data_objects[f\"{do_info['constructor'].ID}_{str(i)}\"].df\n                for i in range(1, self.number_of_single_mnos + 1)\n            ]\n\n            single_mno_dfs = [self.filter_dataframe(df) for df in single_mno_dfs]\n\n            # Filter out obfuscated records, if they exist. This is a first-version operation where we don't deal with\n            # obfuscated values and just ignore them.\n            filtered_dfs = self.filter_out_obfuscated_records(single_mno_dfs, do_info[\"target_columns\"])\n\n            # Aggregate values across MNOs\n            aggregated_df = self.aggregate_single_mno_indicators(filtered_dfs, do_info)\n\n            self.output_data_objects[do_info[\"constructor\"].ID].df = aggregated_df\n\n    def check_output_format(self):\n        for id_, output_do in self.output_data_objects.items():\n            self.logger.info(f\"Checking output data object {id_}...\")\n            output_do.read()\n\n            df = output_do.df\n            schema = output_do.SCHEMA\n\n            if len(df.schema) != len(schema):\n                raise ValueError(f\"Dataset schema has `{len(df.schema)}` fields -- expected {len(schema)}\")\n\n            check_for_nulls = []\n\n            for i, (df_field, expected_field) in enumerate(zip(df.schema, schema)):\n                if df_field.name != expected_field.name:\n                    raise ValueError(\n                        f\"Dataset field number {i+1} has name {df_field.name} -- expected {expected_field.name}\"\n                    )\n\n                if df_field.dataType != expected_field.dataType:\n                    raise TypeError(\n                        f\"Dataset field {df_field.name} has type {df_field.dataType} -- expected {expected_field.dataType}\"\n                    )\n\n                if not expected_field.nullable:\n                    check_for_nulls.append(expected_field.name)\n\n            # Count null values\n            if check_for_nulls:\n                null_counts = (\n                    df.select([F.count(F.when(F.isnull(c), True)).alias(c) for c in check_for_nulls])\n                    .collect()[0]\n                    .asDict()\n                )\n\n                columns_with_nulls = {col: count for col, count in null_counts.items() if count &gt; 0}\n                if columns_with_nulls:\n                    raise ValueError(f\"Unexpected null values detected: {columns_with_nulls}\")\n\n            self.logger.info(f\"... verified {id_}\")\n\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID} for {self.use_case} data...\")\n\n        self.read()\n        self.transform()\n        self.write()\n\n        self.logger.info(f\"Finished writing output of {self.COMPONENT_ID}!\")\n        self.logger.info(f\"Checking fields and format of output data objects...\")\n        self.check_output_format()\n        self.logger.info(\"Finished!\")\n</code></pre>"},{"location":"reference/components/execution/multimno_aggregation/multimno_aggregation/#components.execution.multimno_aggregation.multimno_aggregation.MultiMNOAggregation.aggregate_single_mno_indicators","title":"<code>aggregate_single_mno_indicators(dfs, do_info)</code>","text":"<p>Aggregate the indicator over all single MNOs' data. First, the value of each MNO is multiplied by some weight, and then these values are added up.</p> <p>Parameters:</p> Name Type Description Default <code>dfs</code> <code>list[DataFrame]</code> <p>list of single MNO dataframes</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>final dataframe with the weighted sum indicator of all MNOs</p> Source code in <code>multimno/components/execution/multimno_aggregation/multimno_aggregation.py</code> <pre><code>def aggregate_single_mno_indicators(self, dfs: list[DataFrame], do_info: dict) -&gt; DataFrame:\n    \"\"\"Aggregate the indicator over all single MNOs' data. First, the value of each MNO is multiplied by some\n    weight, and then these values are added up.\n\n    Args:\n        dfs (list[DataFrame]): list of single MNO dataframes\n\n    Returns:\n        DataFrame: final dataframe with the weighted sum indicator of all MNOs\n    \"\"\"\n    weighted_dfs = []\n\n    for i, df in enumerate(dfs):\n        for col in do_info[\"target_columns\"]:\n            df = df.withColumn(col, F.col(col) * F.lit(self.single_mno_factors[i]))\n        weighted_dfs.append(df)\n\n    groupby_cols = [\n        column for column in do_info[\"constructor\"].SCHEMA.names if column not in do_info[\"target_columns\"]\n    ]\n\n    agg_df = (\n        reduce(DataFrame.union, weighted_dfs)\n        .groupBy(*groupby_cols)\n        .agg(*[F.sum(col).alias(col) for col in do_info[\"target_columns\"]])\n    )\n\n    agg_df = apply_schema_casting(agg_df, do_info[\"constructor\"].SCHEMA)\n\n    return agg_df\n</code></pre>"},{"location":"reference/components/execution/multimno_aggregation/multimno_aggregation/#components.execution.multimno_aggregation.multimno_aggregation.MultiMNOAggregation.filter_dataframe","title":"<code>filter_dataframe(df)</code>","text":"<p>Filters dataframe according to the config-specified values for the data objects belonging to a specific use case</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe to be filtered</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>use case that has not been implemented for this function</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>filtered dataframe</p> Source code in <code>multimno/components/execution/multimno_aggregation/multimno_aggregation.py</code> <pre><code>def filter_dataframe(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Filters dataframe according to the config-specified values for the data objects belonging to a specific\n    use case\n\n    Args:\n        df (DataFrame): dataframe to be filtered\n\n    Raises:\n        NotImplementedError: use case that has not been implemented for this function\n\n    Returns:\n        DataFrame: filtered dataframe\n    \"\"\"\n    if self.use_case == InternalMigration.COMPONENT_ID:\n        df = (\n            df.where(F.col(ColNames.dataset_id) == self.zoning_dataset)\n            .where(F.col(ColNames.level).isin(self.levels))\n            .where(F.col(ColNames.start_date_previous) == self.start_date_prev)\n            .where(F.col(ColNames.end_date_previous) == self.end_date_prev)\n            .where(F.col(ColNames.season_previous) == self.season_prev)\n            .where(F.col(ColNames.start_date_new) == self.start_date_new)\n            .where(F.col(ColNames.end_date_new) == self.end_date_new)\n            .where(F.col(ColNames.season_new) == self.season_new)\n        )\n    elif self.use_case == PresentPopulationEstimation.COMPONENT_ID:\n        df = df.where(\n            F.make_date(ColNames.year, ColNames.month, ColNames.day).between(self.start_date, self.end_date)\n        )\n    elif self.use_case == UsualEnvironmentAggregation.COMPONENT_ID:\n        df = (\n            df.where(F.col(ColNames.label).isin(self.labels))\n            .where(F.col(ColNames.start_date) == self.start_date)\n            .where(F.col(ColNames.end_date) == self.end_date)\n            .where(F.col(ColNames.season) == self.season)\n        )\n    elif self.use_case == TourismStatisticsCalculation.COMPONENT_ID:\n        # Same filter applies to both data objects\n        df = (\n            df.where(F.col(ColNames.dataset_id) == self.zoning_dataset)\n            .where(F.col(ColNames.level).isin(self.levels))\n            .where(\n                (F.col(ColNames.year) &gt; self.start_month.year)\n                | (\n                    (F.col(ColNames.year) == self.start_month.year)\n                    &amp; (F.col(ColNames.month) &gt;= self.start_month.month)\n                )\n            )\n            .where(\n                (F.col(ColNames.year) &lt; self.end_month.year)\n                | ((F.col(ColNames.year) == self.end_month.year) &amp; (F.col(ColNames.month) &lt;= self.end_month.month))\n            )\n        )\n    elif self.use_case == TourismOutboundStatisticsCalculation.COMPONENT_ID:\n        df = df.where(\n            (F.col(ColNames.year) &gt; self.start_month.year)\n            | ((F.col(ColNames.year) == self.start_month.year) &amp; (F.col(ColNames.month) &gt;= self.start_month.month))\n        ).where(\n            (F.col(ColNames.year) &lt; self.end_month.year)\n            | ((F.col(ColNames.year) == self.end_month.year) &amp; (F.col(ColNames.month) &lt;= self.end_month.month))\n        )\n    else:\n        raise NotImplementedError(f\"use case {self.use_case}\")\n\n    return df\n</code></pre>"},{"location":"reference/components/execution/multimno_aggregation/multimno_aggregation/#components.execution.multimno_aggregation.multimno_aggregation.MultiMNOAggregation.filter_out_obfuscated_records","title":"<code>filter_out_obfuscated_records(dfs, target_columns)</code>  <code>staticmethod</code>","text":"<p>Filter out obfuscated records, which are records flagged by the k-anonymity process to have values lower than k, for some value k. Specific treatment for these obfuscated values is not yet defined, so we simply remove them in this version of the code.</p> <p>Parameters:</p> Name Type Description Default <code>dfs</code> <code>list[DataFrame]</code> <p>list of single MNO dataframes with possibly obfuscated values</p> required <code>target_column</code> <code>str</code> <p>name of the target column containig the value of the indicator</p> required <p>Returns:</p> Type Description <code>list[DataFrame]</code> <p>list[DataFrame]: list of single MNO dataframes without obfuscated values</p> Source code in <code>multimno/components/execution/multimno_aggregation/multimno_aggregation.py</code> <pre><code>@staticmethod\ndef filter_out_obfuscated_records(dfs: list[DataFrame], target_columns: list[str]) -&gt; list[DataFrame]:\n    \"\"\"Filter out obfuscated records, which are records flagged by the k-anonymity process to have values lower\n    than k, for some value k. Specific treatment for these obfuscated values is not yet defined, so we simply\n    remove them in this version of the code.\n\n    Args:\n        dfs (list[DataFrame]): list of single MNO dataframes with possibly obfuscated values\n        target_column (str): name of the target column containig the value of the indicator\n\n    Returns:\n        list[DataFrame]: list of single MNO dataframes without obfuscated values\n    \"\"\"\n    output_dfs = []\n\n    for df in dfs:\n        for col in target_columns:\n            df = df.where(F.col(col) &gt;= F.lit(0))\n        output_dfs.append(df)\n    return output_dfs\n</code></pre>"},{"location":"reference/components/execution/multimno_aggregation/multimno_aggregation/#components.execution.multimno_aggregation.multimno_aggregation.MultiMNOAggregation.init_inbound_tourism","title":"<code>init_inbound_tourism()</code>","text":"<p>Initialises parameters to filter inbound tourism for this execution</p> Source code in <code>multimno/components/execution/multimno_aggregation/multimno_aggregation.py</code> <pre><code>def init_inbound_tourism(self):\n    \"\"\"\n    Initialises parameters to filter inbound tourism for this execution\n    \"\"\"\n    self.start_month = self._parse_validate_month(\"start_month\")\n    self.end_month = self._parse_validate_month(\"end_month\")\n\n    self.zoning_dataset = self.config.get(f\"{self.COMPONENT_ID}.{self.use_case}\", \"zoning_dataset_id\")\n</code></pre>"},{"location":"reference/components/execution/multimno_aggregation/multimno_aggregation/#components.execution.multimno_aggregation.multimno_aggregation.MultiMNOAggregation.init_internal_migration","title":"<code>init_internal_migration()</code>","text":"<p>Initialises parameters to filter internal migration data for this execution</p> Source code in <code>multimno/components/execution/multimno_aggregation/multimno_aggregation.py</code> <pre><code>def init_internal_migration(self):\n    \"\"\"\n    Initialises parameters to filter internal migration data for this execution\n    \"\"\"\n    self.start_date_prev = self._parse_validate_month(\"start_month_previous\")\n    self.end_date_prev = self._parse_validate_month(\"end_month_previous\")\n    self.end_date_prev = self.end_date_prev + dt.timedelta(\n        days=cal.monthrange(self.end_date_prev.year, self.end_date_prev.month)[1] - 1\n    )\n    self.season_prev = self._parse_validate_season(\"season_previous\")\n\n    self.start_date_new = self._parse_validate_month(\"start_month_new\")\n    self.end_date_new = self._parse_validate_month(\"end_month_new\")\n    self.end_date_new = self.end_date_new + dt.timedelta(\n        days=cal.monthrange(self.end_date_new.year, self.end_date_new.month)[1] - 1\n    )\n    self.season_new = self._parse_validate_season(\"season_new\")\n\n    self.zoning_dataset = self.config.get(f\"{self.COMPONENT_ID}.{self.use_case}\", \"zoning_dataset_id\")\n</code></pre>"},{"location":"reference/components/execution/multimno_aggregation/multimno_aggregation/#components.execution.multimno_aggregation.multimno_aggregation.MultiMNOAggregation.init_outbound_tourism","title":"<code>init_outbound_tourism()</code>","text":"<p>Initialises parameters to filter outbound tourism for this execution</p> Source code in <code>multimno/components/execution/multimno_aggregation/multimno_aggregation.py</code> <pre><code>def init_outbound_tourism(self):\n    \"\"\"\n    Initialises parameters to filter outbound tourism for this execution\n    \"\"\"\n    self.start_month = self._parse_validate_month(\"start_month\")\n    self.end_month = self._parse_validate_month(\"end_month\")\n\n    self.zoning_dataset = None\n</code></pre>"},{"location":"reference/components/execution/multimno_aggregation/multimno_aggregation/#components.execution.multimno_aggregation.multimno_aggregation.MultiMNOAggregation.init_present_population","title":"<code>init_present_population()</code>","text":"<p>Initialises parameters to filter present population for this execution</p> Source code in <code>multimno/components/execution/multimno_aggregation/multimno_aggregation.py</code> <pre><code>def init_present_population(self):\n    \"\"\"\n    Initialises parameters to filter present population for this execution\n    \"\"\"\n    self.start_date = self._parse_validate_date(\"start_date\")\n    self.end_date = self._parse_validate_date(\"end_date\")\n\n    self.zoning_dataset = self.config.get(f\"{self.COMPONENT_ID}.{self.use_case}\", \"zoning_dataset_id\")\n</code></pre>"},{"location":"reference/components/execution/multimno_aggregation/multimno_aggregation/#components.execution.multimno_aggregation.multimno_aggregation.MultiMNOAggregation.init_usual_environment","title":"<code>init_usual_environment()</code>","text":"<p>Initialises parameters to filter usual environment for this execution</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no UE label was specified</p> Source code in <code>multimno/components/execution/multimno_aggregation/multimno_aggregation.py</code> <pre><code>def init_usual_environment(self):\n    \"\"\"\n    Initialises parameters to filter usual environment for this execution\n\n    Raises:\n        ValueError: If no UE label was specified\n    \"\"\"\n    labels = self.config.get(f\"{self.COMPONENT_ID}.{self.use_case}\", \"labels\")\n    labels = list(x.strip() for x in labels.split(\",\"))\n\n    if len(labels) == 0:\n        raise ValueError(f\"Provide at least one usual environment label -- encountered an empty list\")\n\n    self.labels = labels\n    self.start_date = self._parse_validate_month(\"start_month\")\n    self.end_date = self._parse_validate_month(\"end_month\")\n    self.end_date = self.end_date + dt.timedelta(\n        days=cal.monthrange(self.end_date.year, self.end_date.month)[1] - 1\n    )\n    self.season = self.config.get(f\"{self.COMPONENT_ID}.{self.use_case}\", \"season\")\n    if not Seasons.is_valid_type(self.season):\n        err_msg = f\"Unknown season `{self.season}` specified -- must be one of {Seasons.values()}\"\n        self.logger.error(err_msg)\n        raise ValueError(err_msg)\n\n    self.zoning_dataset = self.config.get(f\"{self.COMPONENT_ID}.{self.use_case}\", \"zoning_dataset_id\")\n</code></pre>"},{"location":"reference/components/execution/network_cleaning/","title":"network_cleaning","text":""},{"location":"reference/components/execution/network_cleaning/network_cleaning/","title":"network_cleaning","text":"<p>Module that cleans raw MNO Network Topology data.</p>"},{"location":"reference/components/execution/network_cleaning/network_cleaning/#components.execution.network_cleaning.network_cleaning.NetworkCleaning","title":"<code>NetworkCleaning</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class that cleans MNO Network Topology Data (based on physical properties of the cell)</p> Source code in <code>multimno/components/execution/network_cleaning/network_cleaning.py</code> <pre><code>class NetworkCleaning(Component):\n    \"\"\"\n    Class that cleans MNO Network Topology Data (based on physical properties of the cell)\n    \"\"\"\n\n    COMPONENT_ID = \"NetworkCleaning\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n        self.latitude_max = self.config.getfloat(self.COMPONENT_ID, \"latitude_max\")\n        self.latitude_min = self.config.getfloat(self.COMPONENT_ID, \"latitude_min\")\n        self.longitude_max = self.config.getfloat(self.COMPONENT_ID, \"longitude_max\")\n        self.longitude_min = self.config.getfloat(self.COMPONENT_ID, \"longitude_min\")\n\n        # list of possible technologies\n        self.tech = self.config.get(self.COMPONENT_ID, \"technology_options\").strip().replace(\" \", \"\").split(\",\")\n\n        # list of possible cell types\n        self.cell_type_options = (\n            self.config.get(self.COMPONENT_ID, \"cell_type_options\").strip().replace(\" \", \"\").split(\",\")\n        )\n\n        self.data_period_start = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n        self.data_period_end = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n\n        # List of datetime.dates for which to perform cleaning of the raw network input data.\n        # Notice that date_period_end is included\n        self.data_period_dates = [\n            self.data_period_start + datetime.timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n        # Timestamp format that the function pyspark.sql.functions.to_timestamp expects.\n        # Format must follow guidelines in https://spark.apache.org/docs/3.4.2/sql-ref-datetime-pattern.html\n        self.valid_date_timestamp_format = self.config.get(self.COMPONENT_ID, \"valid_date_timestamp_format\")\n\n        self.frequent_error_criterion = self.config.get(self.COMPONENT_ID, \"frequent_error_criterion\")\n        if self.frequent_error_criterion not in (\"absolute\", \"percentage\"):\n            raise ValueError(\n                \"unexpected value in frequent_error_criterion: expected `absolute` or `percentage`, got\",\n                self.frequent_error_criterion,\n            )\n\n        if self.frequent_error_criterion == \"absolute\":\n            self.top_k_errors = self.config.getint(self.COMPONENT_ID, \"top_k_errors\")\n        else:  # percentage\n            self.top_k_errors = self.config.getfloat(self.COMPONENT_ID, \"top_k_errors\")\n\n        self.timestamp = datetime.datetime.now()\n        self.current_date: datetime.date = None\n        self.cells_df: DataFrame = None\n        self.accdf: DataFrame = None\n\n    def initalize_data_objects(self):\n        input_bronze_network_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"network_data_bronze\")\n        output_silver_network_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"network_data_silver\")\n        output_silver_network_syntactic_quality_metrics_by_column = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"network_syntactic_quality_metrics_by_column\"\n        )\n        output_silver_network_top_errors_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"network_top_frequent_errors\")\n        output_silver_network_row_error_metrics_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"network_row_error_metrics\"\n        )\n\n        bronze_network = BronzeNetworkDataObject(self.spark, input_bronze_network_path)\n        silver_network = SilverNetworkDataObject(self.spark, output_silver_network_path)\n\n        silver_network_quality_metrics_by_column = SilverNetworkDataQualityMetricsByColumn(\n            self.spark,\n            output_silver_network_syntactic_quality_metrics_by_column,\n        )\n\n        silver_network_row_error_metrics = SilverNetworkRowErrorMetrics(\n            self.spark,\n            output_silver_network_row_error_metrics_path,\n        )\n\n        silver_network_top_errors = SilverNetworkDataTopFrequentErrors(\n            self.spark,\n            output_silver_network_top_errors_path,\n        )\n\n        self.input_data_objects = {bronze_network.ID: bronze_network}\n        self.output_data_objects = {\n            silver_network.ID: silver_network,\n            silver_network_quality_metrics_by_column.ID: silver_network_quality_metrics_by_column,\n            silver_network_top_errors.ID: silver_network_top_errors,\n            silver_network_row_error_metrics.ID: silver_network_row_error_metrics,\n        }\n\n    def transform(self):\n        # Raw/Bronze Network Topology DF\n        self.cells_df = self.input_data_objects[BronzeNetworkDataObject.ID].df\n\n        # Read only desired dates, specified via config\n        self.cells_df = self.cells_df.filter(\n            F.make_date(ColNames.year, ColNames.month, ColNames.day) == F.lit(self.current_date)\n        )\n\n        # List that will contain all the columns created to keep track of every kind of error\n        auxiliar_columns = []\n\n        # Columns for which we will check for null values\n        # Notice that currently, valid_date_end can have null values by definition as long as for the current date the tower is\n        # still operational. Thus, it is not taken into account for the deletion of rows/records\n        check_for_null_columns = [\n            ColNames.cell_id,\n            ColNames.valid_date_start,\n            ColNames.valid_date_end,  # should not be counted for discarding rows!!\n            ColNames.latitude,\n            ColNames.longitude,\n            ColNames.altitude,\n            ColNames.antenna_height,\n            ColNames.directionality,\n            ColNames.elevation_angle,\n            ColNames.horizontal_beam_width,\n            ColNames.vertical_beam_width,\n            ColNames.power,\n            ColNames.range,\n            ColNames.frequency,\n            ColNames.technology,\n            ColNames.cell_type,\n        ]\n\n        # Add auxiliar columns to track instances where a row has a null value\n        # Note that currently valid_date_end has a permited null value, as well as\n        # azimith_angle when directionality is 0\n        self.cells_df = self.cells_df.withColumns(\n            {f\"{col}_{NetworkErrorType.NULL_VALUE}\": F.col(col).isNull() for col in check_for_null_columns}\n        ).withColumn(\n            f\"{ColNames.azimuth_angle}_{NetworkErrorType.NULL_VALUE}\",\n            F.when(F.col(ColNames.directionality) == F.lit(1), F.col(ColNames.azimuth_angle).isNull()).otherwise(\n                F.lit(False)\n            ),\n        )\n\n        auxiliar_columns.extend([f\"{col}_{NetworkErrorType.NULL_VALUE}\" for col in check_for_null_columns])\n        auxiliar_columns.append(f\"{ColNames.azimuth_angle}_{NetworkErrorType.NULL_VALUE}\")\n\n        # Now, we try to parse the valid_date_start and valid_date_end columns, from a string to a timestamp\n        self.cells_df = (\n            self.cells_df.withColumn(\n                f\"{ColNames.valid_date_start}_parsed\",\n                F.to_timestamp(F.col(ColNames.valid_date_start), self.valid_date_timestamp_format),\n            )\n            .withColumn(\n                f\"{ColNames.valid_date_end}_parsed\",\n                F.to_timestamp(F.col(ColNames.valid_date_end), self.valid_date_timestamp_format),\n            )\n            # Check when parsing failed, excluding the cases where the field was null to begi with\n            .withColumn(\n                f\"{ColNames.valid_date_start}_{NetworkErrorType.CANNOT_PARSE}\",\n                F.when(\n                    F.col(ColNames.valid_date_start).isNotNull()\n                    &amp; F.col(f\"{ColNames.valid_date_start}_parsed\").isNull(),\n                    F.lit(True),\n                ).otherwise(F.lit(False)),\n            )\n            .withColumn(\n                f\"{ColNames.valid_date_end}_{NetworkErrorType.CANNOT_PARSE}\",\n                F.when(\n                    F.col(ColNames.valid_date_end).isNotNull() &amp; F.col(f\"{ColNames.valid_date_end}_parsed\").isNull(),\n                    F.lit(True),\n                ).otherwise(F.lit(False)),\n            )\n        )\n\n        auxiliar_columns.extend(\n            [\n                f\"{ColNames.valid_date_start}_{NetworkErrorType.CANNOT_PARSE}\",\n                f\"{ColNames.valid_date_end}_{NetworkErrorType.CANNOT_PARSE}\",\n            ]\n        )\n\n        # Now, we check for incoherent dates (valid_date_end is earlier in time than valid_date_start)\n        self.cells_df = self.cells_df.withColumn(\n            f\"dates_{NetworkErrorType.OUT_OF_RANGE}\",\n            F.when(\n                F.col(f\"{ColNames.valid_date_start}_parsed\").isNotNull()\n                &amp; F.col(f\"{ColNames.valid_date_end}_parsed\").isNotNull(),\n                F.col(f\"{ColNames.valid_date_start}_parsed\") &gt; F.col(f\"{ColNames.valid_date_end}_parsed\"),\n            ).otherwise(F.lit(False)),\n        )\n\n        auxiliar_columns.append(f\"dates_{NetworkErrorType.OUT_OF_RANGE}\")\n\n        # Now we check for invalid values that are outside of the range defined for the data object.\n\n        # TODO: correct check for CGI in cell ids\n        do_cgi_check = self.config.getboolean(self.COMPONENT_ID, \"do_cell_cgi_check\", fallback=False)\n        if do_cgi_check:\n            cgi_condition = (F.length(F.col(ColNames.cell_id)) != F.lit(14)) &amp; (\n                F.length(F.col(ColNames.cell_id)) != F.lit(15)\n            )\n        else:\n            cgi_condition = F.lit(False)\n        self.cells_df = self.cells_df.withColumn(\n            f\"{ColNames.cell_id}_{NetworkErrorType.OUT_OF_RANGE}\",\n            cgi_condition,\n        )\n\n        # TODO: cover case where bounding box crosses the -180/180 longitude\n        self.cells_df = (\n            self.cells_df.withColumn(\n                f\"{ColNames.latitude}_{NetworkErrorType.OUT_OF_RANGE}\",\n                (F.col(ColNames.latitude) &gt; F.lit(self.latitude_max))\n                | (F.lit(ColNames.latitude) &lt; F.lit(self.latitude_min)),\n            )\n            .withColumn(\n                f\"{ColNames.longitude}_{NetworkErrorType.OUT_OF_RANGE}\",\n                (F.col(ColNames.longitude) &gt; F.lit(self.longitude_max))\n                | (F.lit(ColNames.longitude) &lt; F.lit(self.longitude_min)),\n            )\n            # altitude: must only be float, no checks\n            # antenna height: must be positive\n            .withColumn(\n                f\"{ColNames.antenna_height}_{NetworkErrorType.OUT_OF_RANGE}\",\n                F.col(ColNames.antenna_height) &lt;= F.lit(0),\n            )\n            # directionality: 0 or 1\n            .withColumn(\n                f\"{ColNames.directionality}_{NetworkErrorType.OUT_OF_RANGE}\",\n                (F.col(ColNames.directionality) != F.lit(0)) &amp; (F.col(ColNames.directionality) != F.lit(1)),\n            )\n            .withColumn(\n                f\"{ColNames.azimuth_angle}_{NetworkErrorType.OUT_OF_RANGE}\",\n                F.when(\n                    (F.col(ColNames.directionality) == F.lit(1)),  # &amp; F.col(ColNames.azimuth_angle).isNotNull(),\n                    (F.col(ColNames.azimuth_angle) &lt; F.lit(0)) | (F.col(ColNames.azimuth_angle) &gt; F.lit(360)),\n                ).otherwise(F.lit(False)),\n            )\n            .withColumn(\n                f\"{ColNames.elevation_angle}_{NetworkErrorType.OUT_OF_RANGE}\",\n                (F.col(ColNames.elevation_angle) &lt; F.lit(-90)) | (F.col(ColNames.elevation_angle) &gt; F.lit(90)),\n            )\n            .withColumn(\n                f\"{ColNames.horizontal_beam_width}_{NetworkErrorType.OUT_OF_RANGE}\",\n                (F.col(ColNames.horizontal_beam_width) &lt; F.lit(0))\n                | (F.col(ColNames.horizontal_beam_width) &gt; F.lit(360)),\n            )\n            .withColumn(\n                f\"{ColNames.vertical_beam_width}_{NetworkErrorType.OUT_OF_RANGE}\",\n                (F.col(ColNames.vertical_beam_width) &lt; F.lit(0)) | (F.col(ColNames.vertical_beam_width) &gt; F.lit(360)),\n            )\n            .withColumn(\n                f\"{ColNames.power}_{NetworkErrorType.OUT_OF_RANGE}\",\n                F.col(ColNames.power) &lt; F.lit(0),\n            )\n            .withColumn(\n                f\"{ColNames.range}_{NetworkErrorType.OUT_OF_RANGE}\",\n                F.col(ColNames.range) &lt; F.lit(0),\n            )\n            .withColumn(\n                f\"{ColNames.frequency}_{NetworkErrorType.OUT_OF_RANGE}\",\n                F.col(ColNames.frequency) &lt; F.lit(0),\n            )\n            .withColumn(\n                f\"{ColNames.technology}_{NetworkErrorType.OUT_OF_RANGE}\",\n                ~F.col(ColNames.technology).isin(self.tech),\n            )\n            .withColumn(\n                f\"{ColNames.cell_type}_{NetworkErrorType.OUT_OF_RANGE}\",\n                ~F.col(ColNames.cell_type).isin(self.cell_type_options),\n            )\n        )\n\n        # Null values will appear for the above checks when the raw data was null. Thus, for these columns\n        # we change null for False by using the .fillna() method\n        self.cells_df = self.cells_df.fillna(\n            False,\n            [\n                f\"{ColNames.cell_id}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.latitude}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.longitude}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.antenna_height}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.directionality}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.azimuth_angle}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.elevation_angle}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.horizontal_beam_width}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.vertical_beam_width}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.power}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.range}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.frequency}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.technology}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.cell_type}_{NetworkErrorType.OUT_OF_RANGE}\",\n            ],\n        )\n\n        auxiliar_columns.extend(\n            [\n                f\"{ColNames.cell_id}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.latitude}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.longitude}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.antenna_height}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.directionality}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.azimuth_angle}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.elevation_angle}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.horizontal_beam_width}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.vertical_beam_width}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.power}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.range}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.frequency}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.technology}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.cell_type}_{NetworkErrorType.OUT_OF_RANGE}\",\n            ]\n        )\n\n        # Auxiliar dict, relating the DO columns with its auxiliar columns\n        column_groups = dict()\n        for col in self.output_data_objects[\"SilverNetworkDO\"].SCHEMA.names:\n            # for each auxiliar column\n            column_groups[col] = []\n            for cc in auxiliar_columns:\n                # if it is related to the DO's column:\n                if col == \"_\".join(cc.split(\"_\")[:-1]):\n                    # Ignore nulls for valid date end\n                    if cc == f\"{ColNames.valid_date_end}_{NetworkErrorType.NULL_VALUE}\":\n                        continue\n                    column_groups[col].append(F.col(cc))\n\n        # For each column, create an abstract conditional whenever a value of that column has ANY type of error.\n        # Example: latitude can have two types of error: a) being null, or b) being out of range.\n        # The coniditonal for this column is then&gt; (isNull(latitude) OR isOutOfRange(latitude))\n        column_conditions = {\n            col: reduce(lambda a, b: a | b, column_groups[col]) for col in column_groups if len(column_groups[col]) &gt; 0\n        }\n\n        # Negate the conditionals above to get those records without ANY type of error\n        field_without_errors = {col: ~column_conditions[col] for col in column_conditions}\n\n        auxiliar_columns.extend([f\"{col}_{NetworkErrorType.NO_ERROR}\" for col in field_without_errors])\n\n        # Abstract conditional ,indicating those records that do not have any type of error in any\n        # mandatory column, i.e. all accepted values.\n        mandatory_columns = [\n            ColNames.cell_id,\n            ColNames.latitude,\n            ColNames.longitude,\n            ColNames.directionality,\n            ColNames.azimuth_angle,\n        ]\n\n        # Rows to be preserved are those without errors in their mandatory fields\n        preserve_row = reduce(lambda a, b: a &amp; b, [field_without_errors[col] for col in mandatory_columns])\n\n        # Rows to be deleted\n\n        # Rows with any type of error\n        any_error_row = reduce(lambda a, b: a | b, column_conditions.values())\n\n        self.cells_df = self.cells_df.withColumns(\n            {f\"{col}_{NetworkErrorType.NO_ERROR}\": field_without_errors[col] for col in field_without_errors}\n        )\n\n        self.cells_df = self.cells_df.withColumn(\"to_preserve\", preserve_row)\n\n        self.cells_df.cache()\n\n        rows_to_be_deleted = (\n            self.cells_df.select((~preserve_row).cast(ByteType()).alias(\"to_be_deleted\")).withColumn(\n                \"to_be_deleted\", F.sum(\"to_be_deleted\")\n            )\n        ).collect()[0][\"to_be_deleted\"]\n\n        rows_with_any_error = (\n            self.cells_df.select((any_error_row).cast(ByteType()).alias(\"row_with_some_error\")).withColumn(\n                \"row_with_some_error\", F.sum(\"row_with_some_error\")\n            )\n        ).collect()[0][\"row_with_some_error\"]\n\n        row_error_metrics = []\n        row_error_metrics.append(\n            Row(\n                **{\n                    ColNames.result_timestamp: self.timestamp,\n                    ColNames.variable: \"rows_deleted\",\n                    ColNames.value: rows_to_be_deleted,\n                    ColNames.year: self.current_date.year,\n                    ColNames.month: self.current_date.month,\n                    ColNames.day: self.current_date.day,\n                }\n            )\n        )\n\n        row_error_metrics.append(\n            Row(\n                **{\n                    ColNames.result_timestamp: self.timestamp,\n                    ColNames.variable: \"rows_with_some_error\",\n                    ColNames.value: rows_with_any_error,\n                    ColNames.year: self.current_date.year,\n                    ColNames.month: self.current_date.month,\n                    ColNames.day: self.current_date.day,\n                }\n            )\n        )\n\n        row_error_df = self.spark.createDataFrame(row_error_metrics, schema=SilverNetworkRowErrorMetrics.SCHEMA)\n\n        self.output_data_objects[SilverNetworkRowErrorMetrics.ID].df = row_error_df\n\n        # Collect the number of True values in each auxiliar column, that counts the number of each error type, or\n        # any error, in each of the columns of the data object.\n        # TODO: possible improvement if pyspark.sql.GroupedData.pivot can be used instead.\n        metrics = (\n            self.cells_df.withColumns({col: F.col(col).cast(IntegerType()) for col in auxiliar_columns})\n            .groupBy([F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)])\n            .agg({col: \"sum\" for col in auxiliar_columns})\n            .withColumnsRenamed({f\"sum({col})\": col for col in auxiliar_columns})\n            .collect()\n        )\n\n        # Extract the collected values and reformat them into the shape of the metrics DO dataframe.\n        metrics_long_format = []\n\n        for row in metrics:\n            row = row.asDict()\n            year = row[ColNames.year]\n            month = row[ColNames.month]\n            day = row[ColNames.day]\n            date = datetime.date(year=year, month=month, day=day)\n            row_dict = dict()\n\n            for col in row:\n                if col in [ColNames.year, ColNames.month, ColNames.day]:\n                    continue\n\n                row_dict = {\n                    ColNames.field_name: \"_\".join(col.split(\"_\")[:-1]),\n                    ColNames.type_code: int(col.split(\"_\")[-1]),\n                    ColNames.value: row[col],\n                    ColNames.date: date,\n                    ColNames.year: year,\n                    ColNames.month: month,\n                    ColNames.day: day,\n                }\n\n                metrics_long_format.append(Row(**row_dict))\n\n        # Initial records (before cleaning)\n        initial_records = (\n            self.cells_df.groupBy([F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)]).count().collect()\n        )\n\n        for row in initial_records:\n            row = row.asDict()\n            year = row[ColNames.year]\n            month = row[ColNames.month]\n            day = row[ColNames.day]\n            date = datetime.date(year=year, month=month, day=day)\n\n            row_dict = {\n                ColNames.field_name: None,\n                ColNames.type_code: NetworkErrorType.INITIAL_ROWS,\n                ColNames.value: row[\"count\"],\n                ColNames.date: date,\n                ColNames.year: year,\n                ColNames.month: month,\n                ColNames.day: day,\n            }\n\n            metrics_long_format.append(Row(**row_dict))\n\n        # Final records (after cleaning)\n        final_records = (\n            self.cells_df.withColumn(\"to_preserve\", F.col(\"to_preserve\").cast(IntegerType()))\n            .groupBy([F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)])\n            .sum(\"to_preserve\")\n            .withColumnRenamed(\"sum(to_preserve)\", \"count\")\n            .collect()\n        )\n        for row in final_records:\n            row = row.asDict()\n            year = row[ColNames.year]\n            month = row[ColNames.month]\n            day = row[ColNames.day]\n            date = datetime.date(year=year, month=month, day=day)\n\n            row_dict = {\n                ColNames.field_name: None,\n                ColNames.type_code: NetworkErrorType.FINAL_ROWS,\n                ColNames.value: row[\"count\"],\n                ColNames.date: date,\n                ColNames.year: year,\n                ColNames.month: month,\n                ColNames.day: day,\n            }\n\n            metrics_long_format.append(Row(**row_dict))\n\n        # Final result\n        metrics_df = self.spark.createDataFrame(\n            metrics_long_format,\n            schema=StructType(\n                [\n                    StructField(ColNames.field_name, StringType(), nullable=True),\n                    StructField(ColNames.type_code, IntegerType(), nullable=False),\n                    StructField(ColNames.value, IntegerType(), nullable=False),\n                    StructField(ColNames.date, DateType(), nullable=False),\n                    StructField(ColNames.year, ShortType(), nullable=False),\n                    StructField(ColNames.month, ByteType(), nullable=False),\n                    StructField(ColNames.day, ByteType(), nullable=False),\n                ]\n            ),\n        )\n        metrics_df = metrics_df.withColumn(ColNames.result_timestamp, F.lit(self.timestamp).cast(TimestampType()))\n\n        self.output_data_objects[SilverNetworkDataQualityMetricsByColumn.ID].df = metrics_df\n\n        silver_cells_df = self.cells_df.filter(F.col(\"to_preserve\"))\n        # Prepare the clean, silver network data by imputing null values in invalid optional fields\n        for auxcol in auxiliar_columns:\n            # do not impute fields without error or that are already null\n            if int(auxcol.split(\"_\")[-1]) not in (NetworkErrorType.NO_ERROR, NetworkErrorType.NULL_VALUE):\n                split_col = auxcol.split(\"_\")\n                variable = \"_\".join(split_col[:-1])\n\n                if variable == \"dates\":\n                    silver_cells_df = silver_cells_df.withColumn(\n                        ColNames.valid_date_start,\n                        F.when(F.col(auxcol), F.lit(None)).otherwise(F.col(ColNames.valid_date_start)),\n                    ).withColumn(\n                        ColNames.valid_date_end,\n                        F.when(F.col(auxcol), F.lit(None)).otherwise(F.col(ColNames.valid_date_end)),\n                    )\n                    continue\n\n                silver_cells_df = silver_cells_df.withColumn(\n                    variable, F.when(F.col(auxcol), F.lit(None)).otherwise(F.col(variable))\n                )\n\n        silver_cells_df = (\n            silver_cells_df.withColumn(ColNames.valid_date_start, F.col(f\"{ColNames.valid_date_start}_parsed\"))\n            .withColumn(ColNames.valid_date_end, F.col(f\"{ColNames.valid_date_end}_parsed\"))\n            .select(SilverNetworkDataObject.SCHEMA.names)\n        )\n\n        self.output_data_objects[SilverNetworkDataObject.ID].df = silver_cells_df\n\n        # Top Frequent Error Metrics\n        error_counts_df = []\n        for field_name, cols in column_groups.items():\n            if len(cols) == 0:\n                continue\n\n            # Get name of the column and its error code\n            col_name = cols[0]._jc.toString()\n            type_code_column = F.when(F.col(col_name), F.lit(int(col_name.split(\"_\")[-1])))\n\n            # if len(cols) == 1, the loop is not entered\n            for i in range(1, len(cols)):\n                col_name = cols[i]._jc.toString()\n                type_code_column = type_code_column.when(F.col(col_name), F.lit(int(col_name.split(\"_\")[-1])))\n            type_code_column = type_code_column.otherwise(None)\n\n            error_counts_df.append(\n                self.cells_df.filter(~field_without_errors[field_name])  # rows with errors in this field\n                .select(field_name, *cols)  # select field and its aux columns\n                .withColumn(ColNames.type_code, type_code_column)  # new column with error code\n                .groupBy(field_name, ColNames.type_code)\n                .count()  # count frequency of each particular error value\n                .withColumnsRenamed(\n                    {\n                        \"count\": ColNames.error_count,\n                        field_name: ColNames.error_value,\n                    }\n                )\n                .withColumn(ColNames.error_count, F.col(ColNames.error_count).cast(IntegerType()))\n                .withColumn(  # cast values as strings\n                    ColNames.error_value, F.col(ColNames.error_value).cast(StringType())\n                )\n                .withColumn(ColNames.field_name, F.lit(field_name))\n            )\n\n        # Join all error count dataframes\n        errors_df = reduce(lambda x, y: DataFrame.union(x, y), error_counts_df)\n\n        errors_df.cache()\n\n        total_errors = errors_df.select(F.sum(ColNames.error_count).alias(ColNames.error_count)).collect()[0][\n            ColNames.error_count\n        ]\n\n        if total_errors is None:\n            self.output_data_objects[SilverNetworkDataTopFrequentErrors.ID].df = self.spark.createDataFrame(\n                [], schema=SilverNetworkDataTopFrequentErrors.SCHEMA\n            )\n            return\n\n        window = (\n            Window()\n            .orderBy(F.col(ColNames.error_count).desc())\n            .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n        )\n\n        self.accdf = errors_df.withColumn(\n            ColNames.accumulated_percentage,\n            (F.lit(100 / total_errors) * F.sum(F.col(ColNames.error_count)).over(window)).cast(FloatType()),\n        ).withColumn(\"id\", F.row_number().over(window))\n\n        if self.frequent_error_criterion == \"absolute\":\n            self.accdf = self.accdf.filter(F.col(\"id\") &lt;= F.lit(self.top_k_errors)).drop(\"id\")\n        else:  # percentage\n            self.accdf.cache()\n            prev_id = (\n                self.accdf.filter(F.col(ColNames.accumulated_percentage) &lt;= F.lit(self.top_k_errors)).select(\n                    F.max(\"id\")\n                )\n            ).collect()[0][\"max(id)\"]\n\n            if prev_id is None:\n                prev_id = 0\n\n            self.accdf = self.accdf.filter(F.col(\"id\") &lt;= F.lit(prev_id + 1)).drop(\"id\")\n\n        self.accdf = (\n            self.accdf.withColumn(ColNames.result_timestamp, F.lit(self.timestamp).cast(TimestampType()))\n            .withColumn(ColNames.year, F.lit(self.current_date.year).cast(ShortType()))\n            .withColumn(ColNames.month, F.lit(self.current_date.month).cast(ByteType()))\n            .withColumn(ColNames.day, F.lit(self.current_date.day).cast(ByteType()))\n            .select(SilverNetworkDataTopFrequentErrors.SCHEMA.fieldNames())\n        )\n\n        self.output_data_objects[SilverNetworkDataTopFrequentErrors.ID].df = self.accdf\n\n    @get_execution_stats\n    def execute(self):\n        self.read()\n        for date in self.data_period_dates:\n            self.logger.info(f\"Processing {date}...\")\n            self.current_date = date\n            self.accdf = None\n            self.transform()\n            self.write()\n            self.cells_df.unpersist()\n            if self.accdf is not None:\n                self.accdf.unpersist()\n            else:\n                self.logger.info(f\"No errors found for {date} -- no error frequency metrics generated\")\n            self.logger.info(f\"... {date} finished\")\n\n    def write(self):\n        \"\"\"\n        Method that performs the write operation of the output data objects.\n        \"\"\"\n        for data_object in self.output_data_objects.values():\n            self.logger.info(f\"Writing {data_object.ID}...\")\n            data_object.write()\n            self.logger.info(\"... finished\")\n\n        for data_object in self.output_data_objects.values():\n            data_object.df.unpersist()\n</code></pre>"},{"location":"reference/components/execution/network_cleaning/network_cleaning/#components.execution.network_cleaning.network_cleaning.NetworkCleaning.write","title":"<code>write()</code>","text":"<p>Method that performs the write operation of the output data objects.</p> Source code in <code>multimno/components/execution/network_cleaning/network_cleaning.py</code> <pre><code>def write(self):\n    \"\"\"\n    Method that performs the write operation of the output data objects.\n    \"\"\"\n    for data_object in self.output_data_objects.values():\n        self.logger.info(f\"Writing {data_object.ID}...\")\n        data_object.write()\n        self.logger.info(\"... finished\")\n\n    for data_object in self.output_data_objects.values():\n        data_object.df.unpersist()\n</code></pre>"},{"location":"reference/components/execution/output_indicators/","title":"output_indicators","text":""},{"location":"reference/components/execution/output_indicators/output_indicators/","title":"output_indicators","text":""},{"location":"reference/components/execution/output_indicators/output_indicators/#components.execution.output_indicators.output_indicators.OutputIndicators","title":"<code>OutputIndicators</code>","text":"<p>               Bases: <code>Component</code></p> Source code in <code>multimno/components/execution/output_indicators/output_indicators.py</code> <pre><code>class OutputIndicators(Component):\n    \"\"\" \"\"\"\n\n    COMPONENT_ID = \"OutputIndicators\"\n    USE_CASES = [\n        InternalMigration.COMPONENT_ID,\n        PresentPopulationEstimation.COMPONENT_ID,\n        UsualEnvironmentAggregation.COMPONENT_ID,\n        TourismStatisticsCalculation.COMPONENT_ID,\n        TourismOutboundStatisticsCalculation.COMPONENT_ID,\n    ]\n\n    def __init__(self, general_config_path, component_config_path):\n        super().__init__(general_config_path, component_config_path)\n\n        # self.use_case and self.zoning_dataset have been defined in self.initialize_data_objects()\n\n        self.section = f\"{self.COMPONENT_ID}.{self.use_case}\"\n\n        # Check incompatibility of reserved datasets with some use cases\n        if self.zoning_dataset in ReservedDatasetIDs():\n            if self.use_case == InternalMigration.COMPONENT_ID:\n                raise ValueError(\n                    f\"Reserved dataset {self.zoning_dataset} execution not implemented for\",\n                    InternalMigration.COMPONENT_ID,\n                )\n            # force level to 1 for reserved datasets\n            self.logger.info(f\"zoning_dataset_id is {self.zoning_dataset} -- forcing hierarchical levels to `[1]`\")\n            self.levels = [1]\n        elif self.zoning_dataset is None:  # no zoning or level needed, e.g. for outbound use case\n            pass\n        else:\n            levels = self.config.get(f\"{self.COMPONENT_ID}.{self.use_case}\", \"hierarchical_levels\")\n            self.levels = list(int(x.strip()) for x in levels.split(\",\"))\n            if len(levels) == 0:\n                raise ValueError(f\"Provide at least one hierarchical level -- encountered an empty list\")\n\n        self.deduplication_factor_local = self.config.getfloat(self.COMPONENT_ID, \"deduplication_factor_local\")\n        self.deduplication_factor_default_inbound = self.config.getfloat(\n            self.COMPONENT_ID, \"deduplication_factor_default_inbound\"\n        )\n        self.mno_to_target_population_factor_local = self.config.getfloat(\n            self.COMPONENT_ID, \"mno_to_target_population_factor_local\"\n        )\n        self.mno_to_target_population_factor_default_inbound = self.config.getfloat(\n            self.COMPONENT_ID, \"mno_to_target_population_factor_default_inbound\"\n        )\n        self.k = self.config.getint(self.COMPONENT_ID, \"k\")\n        self.anonymity_type = self.config.get(self.COMPONENT_ID, \"anonymity_type\")\n        if self.anonymity_type not in KANONYMITY_TYPES:\n            raise ValueError(f\"unknown anonymity type `{self.anonymity_type}` -- must be one of {KANONYMITY_TYPES}\")\n\n        # Initialise use-case specific partition/segmentation values read from config\n        if self.use_case == InternalMigration.COMPONENT_ID:\n            self.init_internal_migration()\n        elif self.use_case == PresentPopulationEstimation.COMPONENT_ID:\n            self.init_present_population()\n        elif self.use_case == UsualEnvironmentAggregation.COMPONENT_ID:\n            self.init_usual_environment()\n        elif self.use_case == TourismStatisticsCalculation.COMPONENT_ID:\n            self.init_inbound_tourism()\n        elif self.use_case == TourismOutboundStatisticsCalculation.COMPONENT_ID:\n            self.init_outbound_tourism()\n\n        # Grid ID origin\n        self.origin = None\n        # Working df\n        self.dfs = None\n\n    def initalize_data_objects(self):\n        self.use_case = self.config.get(self.COMPONENT_ID, \"use_case\")\n\n        # zoning_dataset is not used in outbound tourism, so it will be None for that use case\n        self.zoning_dataset = None\n        if self.use_case in [\n            InternalMigration.COMPONENT_ID,\n            UsualEnvironmentAggregation.COMPONENT_ID,\n            PresentPopulationEstimation.COMPONENT_ID,\n            TourismStatisticsCalculation.COMPONENT_ID,\n        ]:\n            self.zoning_dataset = self.config.get(f\"{self.COMPONENT_ID}.{self.use_case}\", \"zoning_dataset_id\")\n\n        if self.use_case not in self.USE_CASES:\n            raise ValueError(f\"Unknown use_case `{self.use_case}` -- must be one of {', '.join(self.USE_CASES)}\")\n\n        self.input_data_objects = {}\n        self.output_data_objects = {}\n\n        for do_info in USE_CASE_DATA_OBJECTS[self.use_case]:\n            input_do_path = self.config.get(CONFIG_SILVER_PATHS_KEY, do_info[\"input_path_config_key\"])\n            output_do_path = self.config.get(CONFIG_GOLD_PATHS_KEY, do_info[\"output_path_config_key\"])\n\n            if not check_if_data_path_exists(self.spark, input_do_path):\n                self.logger.warning(f\"Expected path {input_do_path} to exist but it does not\")\n                raise ValueError(f\"Invalid path for {do_info['input_constructor'].ID}: {input_do_path}\")\n\n            clear_destination_directory = self.config.getboolean(self.COMPONENT_ID, \"clear_destination_directory\")\n            if clear_destination_directory:\n                delete_file_or_folder(self.spark, output_do_path)\n\n            input_do = do_info[\"input_constructor\"](self.spark, input_do_path)\n            output_do = do_info[\"output_constructor\"](self.spark, output_do_path)\n\n            self.input_data_objects[input_do.ID] = input_do\n            self.output_data_objects[output_do.ID] = output_do\n\n        # Present population and UE aggregation might have INSPIRE 100m or INSPIRE 1km grid as zoning dataset, and they\n        # have to be mapped and aggregated for them in this component, so load geozones grid map data object\n        if self.use_case in [\n            PresentPopulationEstimation.COMPONENT_ID,\n            UsualEnvironmentAggregation.COMPONENT_ID,\n        ]:\n            if self.zoning_dataset not in ReservedDatasetIDs():\n                input_zone_grid_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"geozones_grid_map_data_silver\")\n\n                if not check_if_data_path_exists(self.spark, input_zone_grid_path):\n                    self.logger.warning(f\"Expected path {input_zone_grid_path} to exist but it does not\")\n                    raise ValueError(f\"Invalid path for {SilverGeozonesGridMapDataObject.ID}: {input_zone_grid_path}\")\n\n                self.input_data_objects[SilverGeozonesGridMapDataObject.ID] = SilverGeozonesGridMapDataObject(\n                    self.spark, input_zone_grid_path\n                )\n            else:\n                grid_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"grid_data_silver\")\n                self.input_data_objects[SilverGridDataObject.ID] = SilverGridDataObject(self.spark, grid_path)\n\n        # inbound tourism use case also loads estimation factors (deduplication and mno-to-target-pop factors) for\n        # different countries of origin. Missing factors or countries in the data object will use default factors\n        # specified in configuration\n        if self.use_case == TourismStatisticsCalculation.COMPONENT_ID:\n            input_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"inbound_estimation_factors_bronze\")\n            if check_if_data_path_exists(self.spark, input_path):\n                self.input_data_objects[BronzeInboundEstimationFactorsDataObject.ID] = (\n                    BronzeInboundEstimationFactorsDataObject(self.spark, input_path)\n                )\n            else:\n                self.logger.warning(\n                    f\"Could not find inbound estimation factors at {input_path} -- using default factors\"\n                )\n\n    def _parse_validate_month(self, config_key: str) -&gt; dt.date:\n        \"\"\"Parse and validate month string from config file\n\n        Args:\n            config_key (str): config key of the parameter to parse and validate\n\n        Raises:\n            ValueError: parameter could not be parsed using YYYY-MM format\n\n        Returns:\n            dt.date: first day of the specified month\n        \"\"\"\n        config_input = self.config.get(self.section, config_key)\n        try:\n            out = dt.datetime.strptime(config_input, \"%Y-%m\").date()\n        except ValueError as e:\n            err_msg = f\"Could not parse parameter {config_key} = `{config_input}` -- expected format: YYYY-MM\"\n            self.logger.error(err_msg)\n            raise ValueError(err_msg) from e\n        return out\n\n    def _parse_validate_season(self, config_key: str) -&gt; str:\n        \"\"\"Parse and validate season string from config file\n\n        Args:\n            config_key (str): config key of the parameter to parse and validate\n\n        Raises:\n            ValueError: parameter was not one of the valid season values\n\n        Returns:\n            str: season value\n        \"\"\"\n        config_input = self.config.get(self.section, config_key)\n        if not Seasons.is_valid_type(config_input):\n            err_msg = f\"Unknown season {config_input} -- valid values are {Seasons.values()}\"\n            self.logger.error(err_msg)\n            raise ValueError(err_msg)\n\n        return config_input\n\n    def _parse_validate_date(self, config_key: str) -&gt; dt.date:\n        \"\"\"Parse and validate date string from config file\n\n        Args:\n            config_key (str): config key of the parameter to parse and validate\n\n        Raises:\n            ValueError: could not parse parameter using YYYY-MM-DD format\n\n        Returns:\n            dt.date: specified date in the config\n        \"\"\"\n        config_input = self.config.get(self.section, config_key)\n        try:\n            out = dt.datetime.strptime(config_input, \"%Y-%m-%d\").date()\n        except ValueError as e:\n            err_msg = f\"Could not parse parameter {config_key} = `{config_input}` -- expected format: YYYY-MM-DD\"\n            self.logger.error(err_msg)\n            raise ValueError(err_msg) from e\n        return out\n\n    def init_internal_migration(self):\n        \"\"\"\n        Initialises parameters to filter internal migration data for this execution\n        \"\"\"\n        self.start_date_prev = self._parse_validate_month(\"start_month_previous\")\n        self.end_date_prev = self._parse_validate_month(\"end_month_previous\")\n        self.end_date_prev = self.end_date_prev + dt.timedelta(\n            days=cal.monthrange(self.end_date_prev.year, self.end_date_prev.month)[1] - 1\n        )\n        self.season_prev = self._parse_validate_season(\"season_previous\")\n\n        self.start_date_new = self._parse_validate_month(\"start_month_new\")\n        self.end_date_new = self._parse_validate_month(\"end_month_new\")\n        self.end_date_new = self.end_date_new + dt.timedelta(\n            days=cal.monthrange(self.end_date_new.year, self.end_date_new.month)[1] - 1\n        )\n        self.season_new = self._parse_validate_season(\"season_new\")\n\n    def init_present_population(self):\n        \"\"\"\n        Initialises parameters to filter present population for this execution\n        \"\"\"\n        self.start_date = self._parse_validate_date(\"start_date\")\n        self.end_date = self._parse_validate_date(\"end_date\")\n\n    def init_inbound_tourism(self):\n        \"\"\"\n        Initialises parameters to filter inbound tourism for this execution\n        \"\"\"\n        self.start_month = self._parse_validate_month(\"start_month\")\n        self.end_month = self._parse_validate_month(\"end_month\")\n\n    def init_outbound_tourism(self):\n        \"\"\"\n        Initialises parameters to filter outbound tourism for this execution\n        \"\"\"\n        self.start_month = self._parse_validate_month(\"start_month\")\n        self.end_month = self._parse_validate_month(\"end_month\")\n\n    def init_usual_environment(self):\n        \"\"\"\n        Initialises parameters to filter usual environment for this execution\n\n        Raises:\n            ValueError: If no UE label was specified\n        \"\"\"\n        labels = self.config.get(f\"{self.COMPONENT_ID}.{self.use_case}\", \"labels\")\n        labels = list(x.strip() for x in labels.split(\",\"))\n\n        if len(labels) == 0:\n            raise ValueError(f\"Provide at least one usual environment label -- encountered an empty list\")\n\n        self.labels = labels\n        self.start_date = self._parse_validate_month(\"start_month\")\n        self.end_date = self._parse_validate_month(\"end_month\")\n        self.end_date = self.end_date + dt.timedelta(\n            days=cal.monthrange(self.end_date.year, self.end_date.month)[1] - 1\n        )\n        self.season = self._parse_validate_season(\"season\")\n\n    def filter_dataframe(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Filters dataframe according to the config-specified values for the data objects belonging to a specific\n        use case\n\n        Args:\n            df (DataFrame): dataframe to be filtered\n\n        Raises:\n            NotImplementedError: use case that has not been implemented for this function\n\n        Returns:\n            DataFrame: filtered dataframe\n        \"\"\"\n        if self.use_case == InternalMigration.COMPONENT_ID:\n            df = (\n                df.where(F.col(ColNames.dataset_id) == self.zoning_dataset)\n                .where(F.col(ColNames.level).isin(self.levels))\n                .where(F.col(ColNames.start_date_previous) == self.start_date_prev)\n                .where(F.col(ColNames.end_date_previous) == self.end_date_prev)\n                .where(F.col(ColNames.season_previous) == self.season_prev)\n                .where(F.col(ColNames.start_date_new) == self.start_date_new)\n                .where(F.col(ColNames.end_date_new) == self.end_date_new)\n                .where(F.col(ColNames.season_new) == self.season_new)\n            )\n        elif self.use_case == PresentPopulationEstimation.COMPONENT_ID:\n            df = df.where(\n                F.make_date(ColNames.year, ColNames.month, ColNames.day).between(self.start_date, self.end_date)\n            )\n        elif self.use_case == UsualEnvironmentAggregation.COMPONENT_ID:\n            df = (\n                df.where(F.col(ColNames.label).isin(self.labels))\n                .where(F.col(ColNames.start_date) == self.start_date)\n                .where(F.col(ColNames.end_date) == self.end_date)\n                .where(F.col(ColNames.season) == self.season)\n            )\n        elif self.use_case == TourismStatisticsCalculation.COMPONENT_ID:\n            # Same filter applies to both data objects\n            df = (\n                df.where(F.col(ColNames.dataset_id) == self.zoning_dataset)\n                .where(F.col(ColNames.level).isin(self.levels))\n                .where(\n                    (F.col(ColNames.year) &gt; self.start_month.year)\n                    | (\n                        (F.col(ColNames.year) == self.start_month.year)\n                        &amp; (F.col(ColNames.month) &gt;= self.start_month.month)\n                    )\n                )\n                .where(\n                    (F.col(ColNames.year) &lt; self.end_month.year)\n                    | ((F.col(ColNames.year) == self.end_month.year) &amp; (F.col(ColNames.month) &lt;= self.end_month.month))\n                )\n            )\n        elif self.use_case == TourismOutboundStatisticsCalculation.COMPONENT_ID:\n            df = df.where(\n                (F.col(ColNames.year) &gt; self.start_month.year)\n                | ((F.col(ColNames.year) == self.start_month.year) &amp; (F.col(ColNames.month) &gt;= self.start_month.month))\n            ).where(\n                (F.col(ColNames.year) &lt; self.end_month.year)\n                | ((F.col(ColNames.year) == self.end_month.year) &amp; (F.col(ColNames.month) &lt;= self.end_month.month))\n            )\n        else:\n            raise NotImplementedError(f\"use case {self.use_case}\")\n\n        return df\n\n    def spatial_aggregation(self):\n        \"\"\"\n        Aggregates the indicators of present population and usual environment aggregation to a zoning custom system,\n        to the INSPIRE 1km grid, or keep indicators at the INSPIRE 100m grid, depending on what was specified in\n        configuration.\n\n        This is a no-op for internal migration, inbound tourism, and outbound tourism use cases.\n        \"\"\"\n        # no-op\n        if self.use_case in [\n            InternalMigration.COMPONENT_ID,\n            TourismStatisticsCalculation.COMPONENT_ID,\n            TourismOutboundStatisticsCalculation.COMPONENT_ID,\n        ]:\n            return\n\n        # If dataset is one of INSPIRE 100m or 1km grid, initialise grid generator object\n        if self.zoning_dataset in ReservedDatasetIDs():\n            grid_gen = InspireGridGenerator(spark=self.spark)\n\n            # We need to get the origin in order to correctly obtain the INSPIRE IDs\n            self.origin = self.input_data_objects[SilverGridDataObject.ID].df.first()[\"origin\"]\n        else:  # if not, prepare grid-to-zone dataset\n            zoning_df = self.input_data_objects[SilverGeozonesGridMapDataObject.ID].df\n            zoning_df = zoning_df.filter(F.col(ColNames.dataset_id) == self.zoning_dataset).select(\n                ColNames.grid_id, ColNames.hierarchical_id, ColNames.zone_id, ColNames.dataset_id\n            )\n\n        for do_info in USE_CASE_DATA_OBJECTS[self.use_case]:\n            dfID = do_info[\"output_constructor\"].ID\n            segment_columns = [\n                x\n                for x in do_info[\"output_constructor\"].AGGREGATION_COLUMNS\n                if x not in [ColNames.level, ColNames.dataset_id]\n            ]\n            value_expressions = [F.sum(F.col(col)).alias(col) for col in do_info[\"spatial_agg_columns\"]]\n            df = self.dfs[dfID]\n\n            # INSPIRE 100m grid\n            if self.zoning_dataset == ReservedDatasetIDs.INSPIRE_100m:\n                # impute dataset ID and hierarchical leve equal to 1\n                df = df.withColumn(ColNames.dataset_id, F.lit(ReservedDatasetIDs.INSPIRE_100m)).withColumn(\n                    ColNames.level, F.lit(1)\n                )\n                # convert internal grid ID to INSPIRE specification, and rename column to zone_id\n                df = grid_gen.grid_id_to_inspire_id(\n                    sdf=df, inspire_resolution=100, grid_id_col=ColNames.grid_id, origin=self.origin\n                )\n                df = df.withColumnRenamed(ColNames.inspire_id, ColNames.zone_id).drop(ColNames.grid_id)\n\n                self.dfs[dfID] = df\n                continue\n            # INSPIRE 1km grid\n            elif self.zoning_dataset == ReservedDatasetIDs.INSPIRE_1km:\n                # Transform to INSPIRE representation\n                df = grid_gen.grid_id_to_inspire_id(\n                    sdf=df, inspire_resolution=1000, grid_id_col=ColNames.grid_id, origin=self.origin\n                )\n                df = df.withColumnRenamed(ColNames.inspire_id, ColNames.zone_id).drop(ColNames.grid_id)\n                # Aggregate\n                agg_df = df.groupBy(*segment_columns).agg(*value_expressions)\n                # Add hierarchical level 1 and dataset ID columns\n                agg_df = agg_df.withColumn(ColNames.level, F.lit(1)).withColumn(\n                    ColNames.dataset_id, F.lit(self.zoning_dataset)\n                )\n\n                self.dfs[dfID] = agg_df\n                continue\n            else:  # Using a non-reserved zoning dataset\n                for level in self.levels:\n                    # Replace zone_id column, which contains the smallest zone ID of the hierarchy, with the zone\n                    # of the specific level. The greater zones correspond to lower values of level, and viceversa\n                    # Notice that the coarsest level is 1, not 0. The method `F.element_at` has 1-based indexing,\n                    # instead of 0-based indexing, so the first level is 1.\n                    zone_to_grid_df = zoning_df.withColumn(\n                        ColNames.zone_id, F.element_at(F.split(F.col(ColNames.hierarchical_id), pattern=\"\\\\|\"), level)\n                    )\n\n                    # Join grid IDs with the zone they correspond to, aggregate, and add level and dataset ID columns\n                    agg_df = df.join(zone_to_grid_df, on=ColNames.grid_id)\n                    agg_df = agg_df.groupBy(*segment_columns).agg(*value_expressions)\n                    agg_df = agg_df.withColumn(ColNames.level, F.lit(level)).withColumn(\n                        ColNames.dataset_id, F.lit(self.zoning_dataset)\n                    )\n\n                    # If computing more than one level, we construct more than one data object, so handle this\n                    # appropriately in the self.output_data_objects dictionary.\n                    if len(self.levels) &gt; 1:\n                        output_do = do_info[\"output_constructor\"](\n                            self.spark, self.config.get(CONFIG_GOLD_PATHS_KEY, do_info[\"output_path_config_key\"])\n                        )\n                        self.dfs[dfID + \"_\" + str(level)] = agg_df\n                        self.output_data_objects[dfID + \"_\" + str(level)] = output_do\n                    else:\n                        self.dfs[dfID] = agg_df\n\n                # If computing more than one level, all IDs are of the form `dfID_X` where X is one of the levels.\n                # We remove the \"original\" output DO by the new ones we just created.\n                if len(self.levels) &gt; 1:\n                    del self.dfs[dfID]\n                    del self.output_data_objects[dfID]\n\n    def apply_deduplication_factor(self):\n        \"\"\"Applies the deduplication factor to the appropriate value columns of a dataframe. This consists in\n        multiplying these values by a factor in order to take into account the fact that some people might carry more\n        than one device.\n\n        Raises:\n            NotImplementedError: this function does not implement a specific use case\n        \"\"\"\n        for do_info in USE_CASE_DATA_OBJECTS[self.use_case]:\n            # no-op for some data objects\n            if \"estimation_columns\" not in do_info:\n                continue\n\n            dfID = do_info[\"output_constructor\"].ID\n            # filter the data objects that correspond to this do_info\n            for id_ in self.dfs:\n                if not id_.startswith(dfID):\n                    continue\n                df = self.dfs[id_]\n\n                # These use cases just multiply by the same constant factor\n                if self.use_case in [\n                    UsualEnvironmentAggregation.COMPONENT_ID,\n                    PresentPopulationEstimation.COMPONENT_ID,\n                    InternalMigration.COMPONENT_ID,\n                    TourismOutboundStatisticsCalculation.COMPONENT_ID,\n                ]:\n                    for col in do_info[\"estimation_columns\"]:\n                        df = df.withColumn(col, F.col(col) * F.lit(self.deduplication_factor_local))\n\n                elif self.use_case == TourismStatisticsCalculation.COMPONENT_ID:\n                    # check if we were able to find Inbound Estimation Factors data object in the specified directory\n                    if BronzeInboundEstimationFactorsDataObject.ID not in self.input_data_objects:\n                        # if no factors DO found, just multiply by default value\n                        for col in do_info[\"estimation_columns\"]:\n                            df = df.withColumn(col, F.col(col) * self.deduplication_factor_default_inbound)\n                        continue\n\n                    # If the DO was loaded, load the data object and rename the country column\n                    estimation_factors_df = F.broadcast(\n                        self.input_data_objects[BronzeInboundEstimationFactorsDataObject.ID].df\n                    )\n                    estimation_factors_df = estimation_factors_df.withColumnRenamed(\n                        ColNames.iso2, ColNames.country_of_origin\n                    )\n                    # Left join to get the factors for any countries that appear in the factors DO\n                    df = df.join(estimation_factors_df, on=ColNames.country_of_origin, how=\"left\")\n                    # For missing factors or countries, impute with default values\n                    # we also impute here the mno-to-target-population factor\n                    df = df.fillna(\n                        {\n                            ColNames.deduplication_factor: self.deduplication_factor_default_inbound,\n                            ColNames.mno_to_target_population_factor: self.mno_to_target_population_factor_default_inbound,\n                        }\n                    )\n                    # mutiply by deduplication factor\n                    for col in do_info[\"estimation_columns\"]:\n                        df = df.withColumn(col, F.col(col) * F.col(ColNames.deduplication_factor))\n\n                else:\n                    raise NotImplementedError(self.use_case)\n\n                self.dfs[id_] = df\n\n    def apply_mno_to_target_population_factor(self):\n        \"\"\"Apply the MNO-to-target population to the value columns of the data objects of the specified use case. This\n        is done to estimate the total target population from the number of devices that this particular MNO has.\n\n        Raises:\n            NotImplementedError: this function does not implement a specific use case\n        \"\"\"\n        for do_info in USE_CASE_DATA_OBJECTS[self.use_case]:\n            # no-op for some data objects\n            if \"estimation_columns\" not in do_info:\n                continue\n\n            dfID = do_info[\"output_constructor\"].ID\n            # filter the data objects that correspond to this do_info\n            for id_ in self.dfs:\n                if not id_.startswith(dfID):\n                    continue\n                df = self.dfs[id_]\n\n                # These use cases just multiply by the same constant factor\n                if self.use_case in [\n                    UsualEnvironmentAggregation.COMPONENT_ID,\n                    PresentPopulationEstimation.COMPONENT_ID,\n                    InternalMigration.COMPONENT_ID,\n                    TourismOutboundStatisticsCalculation.COMPONENT_ID,\n                ]:\n                    for col in do_info[\"estimation_columns\"]:\n                        df = df.withColumn(col, F.col(col) * F.lit(self.mno_to_target_population_factor_local))\n                elif self.use_case == TourismStatisticsCalculation.COMPONENT_ID:\n                    # If inbound factors loaded, we can multiply by the factor loaded previously\n                    if BronzeInboundEstimationFactorsDataObject.ID in self.input_data_objects:\n                        for col in do_info[\"estimation_columns\"]:\n                            df = df.withColumn(col, F.col(col) * F.col(ColNames.mno_to_target_population_factor))\n                    else:  # if not, multiply by default factor\n                        for col in do_info[\"estimation_columns\"]:\n                            df = df.withColumn(col, F.col(col) * self.deduplication_factor_default_inbound)\n                else:\n                    raise NotImplementedError(self.use_case)\n\n                # If columns don't exist this is a no-op\n                df = df.drop(ColNames.deduplication_factor, ColNames.mno_to_target_population_factor)\n                self.dfs[id_] = df\n\n    def apply_anonymity_obfuscation(self):\n        \"\"\"Apply k-anonymity obfuscation to the output data objects. Any value in the specified columns that is strictly\n        lower than `k` is replaced by `-1`.\n        \"\"\"\n        for do_info in USE_CASE_DATA_OBJECTS[self.use_case]:\n            # no-op for some use cases\n            if \"kanonymity_columns\" not in do_info:\n                continue\n\n            dfID = do_info[\"output_constructor\"].ID\n            for id_ in self.dfs:\n                if not id_.startswith(dfID):\n                    continue\n                df = self.dfs[id_]\n\n                for col in do_info[\"kanonymity_columns\"]:\n                    df = df.withColumn(\n                        col,\n                        F.when(\n                            F.col(col) &lt; F.lit(self.k),\n                            F.lit(-1).cast(do_info[\"output_constructor\"].SCHEMA[col].dataType),\n                        ).otherwise(F.col(col)),\n                    )\n\n                self.dfs[id_] = df\n\n    def apply_anonymity_deletion(self):\n        \"\"\"Apply k-anonymity deletion to the output data objects.  Any value in the specified columns that is strictly\n        lower than `k` is removed along with its corresponding row.\n        \"\"\"\n        for do_info in USE_CASE_DATA_OBJECTS[self.use_case]:\n            # no-op for some data objects\n            if \"kanonymity_columns\" not in do_info:\n                continue\n            dfID = do_info[\"output_constructor\"].ID\n            # filter the data objects that correspond to this do_info\n            for id_ in self.dfs:\n                if not id_.startswith(dfID):\n                    continue\n                df = self.dfs[id_]\n\n                for col in do_info[\"kanonymity_columns\"]:\n                    df = df.where(F.col(col) &gt;= F.lit(self.k))\n\n                self.dfs[id_] = df\n\n    def transform(self):\n        # Dictionary that will contain the dataframes as we modify them\n        self.dfs = {}\n        for do_info in USE_CASE_DATA_OBJECTS[self.use_case]:\n            input_do_ID = do_info[\"input_constructor\"].ID\n            output_do_ID = do_info[\"output_constructor\"].ID\n            self.dfs[output_do_ID] = self.filter_dataframe(self.input_data_objects[input_do_ID].df)\n\n        # Spatial aggregation\n        self.spatial_aggregation()\n\n        # Deduplication factor\n        self.apply_deduplication_factor()\n\n        # Device to target population\n        self.apply_mno_to_target_population_factor()\n\n        # Apply k-anonymity\n        if self.anonymity_type == \"obfuscate\":\n            self.apply_anonymity_obfuscation()\n        elif self.anonymity_type == \"delete\":\n            self.apply_anonymity_deletion()\n        else:\n            raise ValueError(f\"Unknown anonymity type {self.anonymity_type}\")  # should not happen\n\n        for do_info in USE_CASE_DATA_OBJECTS[self.use_case]:\n            output_do_ID = do_info[\"output_constructor\"].ID\n\n            for id_ in self.dfs:\n                if not id_.startswith(output_do_ID):\n                    continue\n\n                self.output_data_objects[id_].df = apply_schema_casting(\n                    self.dfs[id_], do_info[\"output_constructor\"].SCHEMA\n                )\n\n    def check_output_format(self):\n        for id_, output_do in self.output_data_objects.items():\n            self.logger.info(f\"Checking output data object {id_}...\")\n            output_do.read()\n\n            df = output_do.df\n            schema = output_do.SCHEMA\n\n            if len(df.schema) != len(schema):\n                self.logger.warning(f\"Dataset schema has `{len(df.schema)}` fields -- expected {len(schema)}\")\n\n            check_for_nulls = []\n\n            for i, (df_field, expected_field) in enumerate(zip(df.schema, schema)):\n                if df_field.name != expected_field.name:\n                    self.logger.warning(\n                        f\"Dataset field number {i+1} has name {df_field.name} -- expected {expected_field.name}\"\n                    )\n\n                if df_field.dataType != expected_field.dataType:\n                    self.logger.warning(\n                        f\"Dataset field {df_field.name} has type {df_field.dataType} -- expected {expected_field.dataType}\"\n                    )\n\n                if not expected_field.nullable:\n                    check_for_nulls.append(expected_field.name)\n\n            # Count null values\n            if check_for_nulls:\n                null_counts = (\n                    df.select([F.count(F.when(F.isnull(c), True)).alias(c) for c in check_for_nulls])\n                    .collect()[0]\n                    .asDict()\n                )\n\n                columns_with_nulls = {col: count for col, count in null_counts.items() if count &gt; 0}\n                if columns_with_nulls:\n                    self.logger.warning(f\"Unexpected null values detected: {columns_with_nulls}\")\n\n            self.logger.info(f\"... checked {id_}\")\n\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID} for {self.use_case} data...\")\n\n        self.read()\n        self.transform()\n        self.write()\n\n        self.logger.info(f\"Finished writing output of {self.COMPONENT_ID}!\")\n        self.logger.info(f\"Checking fields and format of output data objects...\")\n        self.check_output_format()\n        self.logger.info(\"Finished!\")\n</code></pre>"},{"location":"reference/components/execution/output_indicators/output_indicators/#components.execution.output_indicators.output_indicators.OutputIndicators.apply_anonymity_deletion","title":"<code>apply_anonymity_deletion()</code>","text":"<p>Apply k-anonymity deletion to the output data objects.  Any value in the specified columns that is strictly lower than <code>k</code> is removed along with its corresponding row.</p> Source code in <code>multimno/components/execution/output_indicators/output_indicators.py</code> <pre><code>def apply_anonymity_deletion(self):\n    \"\"\"Apply k-anonymity deletion to the output data objects.  Any value in the specified columns that is strictly\n    lower than `k` is removed along with its corresponding row.\n    \"\"\"\n    for do_info in USE_CASE_DATA_OBJECTS[self.use_case]:\n        # no-op for some data objects\n        if \"kanonymity_columns\" not in do_info:\n            continue\n        dfID = do_info[\"output_constructor\"].ID\n        # filter the data objects that correspond to this do_info\n        for id_ in self.dfs:\n            if not id_.startswith(dfID):\n                continue\n            df = self.dfs[id_]\n\n            for col in do_info[\"kanonymity_columns\"]:\n                df = df.where(F.col(col) &gt;= F.lit(self.k))\n\n            self.dfs[id_] = df\n</code></pre>"},{"location":"reference/components/execution/output_indicators/output_indicators/#components.execution.output_indicators.output_indicators.OutputIndicators.apply_anonymity_obfuscation","title":"<code>apply_anonymity_obfuscation()</code>","text":"<p>Apply k-anonymity obfuscation to the output data objects. Any value in the specified columns that is strictly lower than <code>k</code> is replaced by <code>-1</code>.</p> Source code in <code>multimno/components/execution/output_indicators/output_indicators.py</code> <pre><code>def apply_anonymity_obfuscation(self):\n    \"\"\"Apply k-anonymity obfuscation to the output data objects. Any value in the specified columns that is strictly\n    lower than `k` is replaced by `-1`.\n    \"\"\"\n    for do_info in USE_CASE_DATA_OBJECTS[self.use_case]:\n        # no-op for some use cases\n        if \"kanonymity_columns\" not in do_info:\n            continue\n\n        dfID = do_info[\"output_constructor\"].ID\n        for id_ in self.dfs:\n            if not id_.startswith(dfID):\n                continue\n            df = self.dfs[id_]\n\n            for col in do_info[\"kanonymity_columns\"]:\n                df = df.withColumn(\n                    col,\n                    F.when(\n                        F.col(col) &lt; F.lit(self.k),\n                        F.lit(-1).cast(do_info[\"output_constructor\"].SCHEMA[col].dataType),\n                    ).otherwise(F.col(col)),\n                )\n\n            self.dfs[id_] = df\n</code></pre>"},{"location":"reference/components/execution/output_indicators/output_indicators/#components.execution.output_indicators.output_indicators.OutputIndicators.apply_deduplication_factor","title":"<code>apply_deduplication_factor()</code>","text":"<p>Applies the deduplication factor to the appropriate value columns of a dataframe. This consists in multiplying these values by a factor in order to take into account the fact that some people might carry more than one device.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>this function does not implement a specific use case</p> Source code in <code>multimno/components/execution/output_indicators/output_indicators.py</code> <pre><code>def apply_deduplication_factor(self):\n    \"\"\"Applies the deduplication factor to the appropriate value columns of a dataframe. This consists in\n    multiplying these values by a factor in order to take into account the fact that some people might carry more\n    than one device.\n\n    Raises:\n        NotImplementedError: this function does not implement a specific use case\n    \"\"\"\n    for do_info in USE_CASE_DATA_OBJECTS[self.use_case]:\n        # no-op for some data objects\n        if \"estimation_columns\" not in do_info:\n            continue\n\n        dfID = do_info[\"output_constructor\"].ID\n        # filter the data objects that correspond to this do_info\n        for id_ in self.dfs:\n            if not id_.startswith(dfID):\n                continue\n            df = self.dfs[id_]\n\n            # These use cases just multiply by the same constant factor\n            if self.use_case in [\n                UsualEnvironmentAggregation.COMPONENT_ID,\n                PresentPopulationEstimation.COMPONENT_ID,\n                InternalMigration.COMPONENT_ID,\n                TourismOutboundStatisticsCalculation.COMPONENT_ID,\n            ]:\n                for col in do_info[\"estimation_columns\"]:\n                    df = df.withColumn(col, F.col(col) * F.lit(self.deduplication_factor_local))\n\n            elif self.use_case == TourismStatisticsCalculation.COMPONENT_ID:\n                # check if we were able to find Inbound Estimation Factors data object in the specified directory\n                if BronzeInboundEstimationFactorsDataObject.ID not in self.input_data_objects:\n                    # if no factors DO found, just multiply by default value\n                    for col in do_info[\"estimation_columns\"]:\n                        df = df.withColumn(col, F.col(col) * self.deduplication_factor_default_inbound)\n                    continue\n\n                # If the DO was loaded, load the data object and rename the country column\n                estimation_factors_df = F.broadcast(\n                    self.input_data_objects[BronzeInboundEstimationFactorsDataObject.ID].df\n                )\n                estimation_factors_df = estimation_factors_df.withColumnRenamed(\n                    ColNames.iso2, ColNames.country_of_origin\n                )\n                # Left join to get the factors for any countries that appear in the factors DO\n                df = df.join(estimation_factors_df, on=ColNames.country_of_origin, how=\"left\")\n                # For missing factors or countries, impute with default values\n                # we also impute here the mno-to-target-population factor\n                df = df.fillna(\n                    {\n                        ColNames.deduplication_factor: self.deduplication_factor_default_inbound,\n                        ColNames.mno_to_target_population_factor: self.mno_to_target_population_factor_default_inbound,\n                    }\n                )\n                # mutiply by deduplication factor\n                for col in do_info[\"estimation_columns\"]:\n                    df = df.withColumn(col, F.col(col) * F.col(ColNames.deduplication_factor))\n\n            else:\n                raise NotImplementedError(self.use_case)\n\n            self.dfs[id_] = df\n</code></pre>"},{"location":"reference/components/execution/output_indicators/output_indicators/#components.execution.output_indicators.output_indicators.OutputIndicators.apply_mno_to_target_population_factor","title":"<code>apply_mno_to_target_population_factor()</code>","text":"<p>Apply the MNO-to-target population to the value columns of the data objects of the specified use case. This is done to estimate the total target population from the number of devices that this particular MNO has.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>this function does not implement a specific use case</p> Source code in <code>multimno/components/execution/output_indicators/output_indicators.py</code> <pre><code>def apply_mno_to_target_population_factor(self):\n    \"\"\"Apply the MNO-to-target population to the value columns of the data objects of the specified use case. This\n    is done to estimate the total target population from the number of devices that this particular MNO has.\n\n    Raises:\n        NotImplementedError: this function does not implement a specific use case\n    \"\"\"\n    for do_info in USE_CASE_DATA_OBJECTS[self.use_case]:\n        # no-op for some data objects\n        if \"estimation_columns\" not in do_info:\n            continue\n\n        dfID = do_info[\"output_constructor\"].ID\n        # filter the data objects that correspond to this do_info\n        for id_ in self.dfs:\n            if not id_.startswith(dfID):\n                continue\n            df = self.dfs[id_]\n\n            # These use cases just multiply by the same constant factor\n            if self.use_case in [\n                UsualEnvironmentAggregation.COMPONENT_ID,\n                PresentPopulationEstimation.COMPONENT_ID,\n                InternalMigration.COMPONENT_ID,\n                TourismOutboundStatisticsCalculation.COMPONENT_ID,\n            ]:\n                for col in do_info[\"estimation_columns\"]:\n                    df = df.withColumn(col, F.col(col) * F.lit(self.mno_to_target_population_factor_local))\n            elif self.use_case == TourismStatisticsCalculation.COMPONENT_ID:\n                # If inbound factors loaded, we can multiply by the factor loaded previously\n                if BronzeInboundEstimationFactorsDataObject.ID in self.input_data_objects:\n                    for col in do_info[\"estimation_columns\"]:\n                        df = df.withColumn(col, F.col(col) * F.col(ColNames.mno_to_target_population_factor))\n                else:  # if not, multiply by default factor\n                    for col in do_info[\"estimation_columns\"]:\n                        df = df.withColumn(col, F.col(col) * self.deduplication_factor_default_inbound)\n            else:\n                raise NotImplementedError(self.use_case)\n\n            # If columns don't exist this is a no-op\n            df = df.drop(ColNames.deduplication_factor, ColNames.mno_to_target_population_factor)\n            self.dfs[id_] = df\n</code></pre>"},{"location":"reference/components/execution/output_indicators/output_indicators/#components.execution.output_indicators.output_indicators.OutputIndicators.filter_dataframe","title":"<code>filter_dataframe(df)</code>","text":"<p>Filters dataframe according to the config-specified values for the data objects belonging to a specific use case</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe to be filtered</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>use case that has not been implemented for this function</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>filtered dataframe</p> Source code in <code>multimno/components/execution/output_indicators/output_indicators.py</code> <pre><code>def filter_dataframe(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Filters dataframe according to the config-specified values for the data objects belonging to a specific\n    use case\n\n    Args:\n        df (DataFrame): dataframe to be filtered\n\n    Raises:\n        NotImplementedError: use case that has not been implemented for this function\n\n    Returns:\n        DataFrame: filtered dataframe\n    \"\"\"\n    if self.use_case == InternalMigration.COMPONENT_ID:\n        df = (\n            df.where(F.col(ColNames.dataset_id) == self.zoning_dataset)\n            .where(F.col(ColNames.level).isin(self.levels))\n            .where(F.col(ColNames.start_date_previous) == self.start_date_prev)\n            .where(F.col(ColNames.end_date_previous) == self.end_date_prev)\n            .where(F.col(ColNames.season_previous) == self.season_prev)\n            .where(F.col(ColNames.start_date_new) == self.start_date_new)\n            .where(F.col(ColNames.end_date_new) == self.end_date_new)\n            .where(F.col(ColNames.season_new) == self.season_new)\n        )\n    elif self.use_case == PresentPopulationEstimation.COMPONENT_ID:\n        df = df.where(\n            F.make_date(ColNames.year, ColNames.month, ColNames.day).between(self.start_date, self.end_date)\n        )\n    elif self.use_case == UsualEnvironmentAggregation.COMPONENT_ID:\n        df = (\n            df.where(F.col(ColNames.label).isin(self.labels))\n            .where(F.col(ColNames.start_date) == self.start_date)\n            .where(F.col(ColNames.end_date) == self.end_date)\n            .where(F.col(ColNames.season) == self.season)\n        )\n    elif self.use_case == TourismStatisticsCalculation.COMPONENT_ID:\n        # Same filter applies to both data objects\n        df = (\n            df.where(F.col(ColNames.dataset_id) == self.zoning_dataset)\n            .where(F.col(ColNames.level).isin(self.levels))\n            .where(\n                (F.col(ColNames.year) &gt; self.start_month.year)\n                | (\n                    (F.col(ColNames.year) == self.start_month.year)\n                    &amp; (F.col(ColNames.month) &gt;= self.start_month.month)\n                )\n            )\n            .where(\n                (F.col(ColNames.year) &lt; self.end_month.year)\n                | ((F.col(ColNames.year) == self.end_month.year) &amp; (F.col(ColNames.month) &lt;= self.end_month.month))\n            )\n        )\n    elif self.use_case == TourismOutboundStatisticsCalculation.COMPONENT_ID:\n        df = df.where(\n            (F.col(ColNames.year) &gt; self.start_month.year)\n            | ((F.col(ColNames.year) == self.start_month.year) &amp; (F.col(ColNames.month) &gt;= self.start_month.month))\n        ).where(\n            (F.col(ColNames.year) &lt; self.end_month.year)\n            | ((F.col(ColNames.year) == self.end_month.year) &amp; (F.col(ColNames.month) &lt;= self.end_month.month))\n        )\n    else:\n        raise NotImplementedError(f\"use case {self.use_case}\")\n\n    return df\n</code></pre>"},{"location":"reference/components/execution/output_indicators/output_indicators/#components.execution.output_indicators.output_indicators.OutputIndicators.init_inbound_tourism","title":"<code>init_inbound_tourism()</code>","text":"<p>Initialises parameters to filter inbound tourism for this execution</p> Source code in <code>multimno/components/execution/output_indicators/output_indicators.py</code> <pre><code>def init_inbound_tourism(self):\n    \"\"\"\n    Initialises parameters to filter inbound tourism for this execution\n    \"\"\"\n    self.start_month = self._parse_validate_month(\"start_month\")\n    self.end_month = self._parse_validate_month(\"end_month\")\n</code></pre>"},{"location":"reference/components/execution/output_indicators/output_indicators/#components.execution.output_indicators.output_indicators.OutputIndicators.init_internal_migration","title":"<code>init_internal_migration()</code>","text":"<p>Initialises parameters to filter internal migration data for this execution</p> Source code in <code>multimno/components/execution/output_indicators/output_indicators.py</code> <pre><code>def init_internal_migration(self):\n    \"\"\"\n    Initialises parameters to filter internal migration data for this execution\n    \"\"\"\n    self.start_date_prev = self._parse_validate_month(\"start_month_previous\")\n    self.end_date_prev = self._parse_validate_month(\"end_month_previous\")\n    self.end_date_prev = self.end_date_prev + dt.timedelta(\n        days=cal.monthrange(self.end_date_prev.year, self.end_date_prev.month)[1] - 1\n    )\n    self.season_prev = self._parse_validate_season(\"season_previous\")\n\n    self.start_date_new = self._parse_validate_month(\"start_month_new\")\n    self.end_date_new = self._parse_validate_month(\"end_month_new\")\n    self.end_date_new = self.end_date_new + dt.timedelta(\n        days=cal.monthrange(self.end_date_new.year, self.end_date_new.month)[1] - 1\n    )\n    self.season_new = self._parse_validate_season(\"season_new\")\n</code></pre>"},{"location":"reference/components/execution/output_indicators/output_indicators/#components.execution.output_indicators.output_indicators.OutputIndicators.init_outbound_tourism","title":"<code>init_outbound_tourism()</code>","text":"<p>Initialises parameters to filter outbound tourism for this execution</p> Source code in <code>multimno/components/execution/output_indicators/output_indicators.py</code> <pre><code>def init_outbound_tourism(self):\n    \"\"\"\n    Initialises parameters to filter outbound tourism for this execution\n    \"\"\"\n    self.start_month = self._parse_validate_month(\"start_month\")\n    self.end_month = self._parse_validate_month(\"end_month\")\n</code></pre>"},{"location":"reference/components/execution/output_indicators/output_indicators/#components.execution.output_indicators.output_indicators.OutputIndicators.init_present_population","title":"<code>init_present_population()</code>","text":"<p>Initialises parameters to filter present population for this execution</p> Source code in <code>multimno/components/execution/output_indicators/output_indicators.py</code> <pre><code>def init_present_population(self):\n    \"\"\"\n    Initialises parameters to filter present population for this execution\n    \"\"\"\n    self.start_date = self._parse_validate_date(\"start_date\")\n    self.end_date = self._parse_validate_date(\"end_date\")\n</code></pre>"},{"location":"reference/components/execution/output_indicators/output_indicators/#components.execution.output_indicators.output_indicators.OutputIndicators.init_usual_environment","title":"<code>init_usual_environment()</code>","text":"<p>Initialises parameters to filter usual environment for this execution</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no UE label was specified</p> Source code in <code>multimno/components/execution/output_indicators/output_indicators.py</code> <pre><code>def init_usual_environment(self):\n    \"\"\"\n    Initialises parameters to filter usual environment for this execution\n\n    Raises:\n        ValueError: If no UE label was specified\n    \"\"\"\n    labels = self.config.get(f\"{self.COMPONENT_ID}.{self.use_case}\", \"labels\")\n    labels = list(x.strip() for x in labels.split(\",\"))\n\n    if len(labels) == 0:\n        raise ValueError(f\"Provide at least one usual environment label -- encountered an empty list\")\n\n    self.labels = labels\n    self.start_date = self._parse_validate_month(\"start_month\")\n    self.end_date = self._parse_validate_month(\"end_month\")\n    self.end_date = self.end_date + dt.timedelta(\n        days=cal.monthrange(self.end_date.year, self.end_date.month)[1] - 1\n    )\n    self.season = self._parse_validate_season(\"season\")\n</code></pre>"},{"location":"reference/components/execution/output_indicators/output_indicators/#components.execution.output_indicators.output_indicators.OutputIndicators.spatial_aggregation","title":"<code>spatial_aggregation()</code>","text":"<p>Aggregates the indicators of present population and usual environment aggregation to a zoning custom system, to the INSPIRE 1km grid, or keep indicators at the INSPIRE 100m grid, depending on what was specified in configuration.</p> <p>This is a no-op for internal migration, inbound tourism, and outbound tourism use cases.</p> Source code in <code>multimno/components/execution/output_indicators/output_indicators.py</code> <pre><code>def spatial_aggregation(self):\n    \"\"\"\n    Aggregates the indicators of present population and usual environment aggregation to a zoning custom system,\n    to the INSPIRE 1km grid, or keep indicators at the INSPIRE 100m grid, depending on what was specified in\n    configuration.\n\n    This is a no-op for internal migration, inbound tourism, and outbound tourism use cases.\n    \"\"\"\n    # no-op\n    if self.use_case in [\n        InternalMigration.COMPONENT_ID,\n        TourismStatisticsCalculation.COMPONENT_ID,\n        TourismOutboundStatisticsCalculation.COMPONENT_ID,\n    ]:\n        return\n\n    # If dataset is one of INSPIRE 100m or 1km grid, initialise grid generator object\n    if self.zoning_dataset in ReservedDatasetIDs():\n        grid_gen = InspireGridGenerator(spark=self.spark)\n\n        # We need to get the origin in order to correctly obtain the INSPIRE IDs\n        self.origin = self.input_data_objects[SilverGridDataObject.ID].df.first()[\"origin\"]\n    else:  # if not, prepare grid-to-zone dataset\n        zoning_df = self.input_data_objects[SilverGeozonesGridMapDataObject.ID].df\n        zoning_df = zoning_df.filter(F.col(ColNames.dataset_id) == self.zoning_dataset).select(\n            ColNames.grid_id, ColNames.hierarchical_id, ColNames.zone_id, ColNames.dataset_id\n        )\n\n    for do_info in USE_CASE_DATA_OBJECTS[self.use_case]:\n        dfID = do_info[\"output_constructor\"].ID\n        segment_columns = [\n            x\n            for x in do_info[\"output_constructor\"].AGGREGATION_COLUMNS\n            if x not in [ColNames.level, ColNames.dataset_id]\n        ]\n        value_expressions = [F.sum(F.col(col)).alias(col) for col in do_info[\"spatial_agg_columns\"]]\n        df = self.dfs[dfID]\n\n        # INSPIRE 100m grid\n        if self.zoning_dataset == ReservedDatasetIDs.INSPIRE_100m:\n            # impute dataset ID and hierarchical leve equal to 1\n            df = df.withColumn(ColNames.dataset_id, F.lit(ReservedDatasetIDs.INSPIRE_100m)).withColumn(\n                ColNames.level, F.lit(1)\n            )\n            # convert internal grid ID to INSPIRE specification, and rename column to zone_id\n            df = grid_gen.grid_id_to_inspire_id(\n                sdf=df, inspire_resolution=100, grid_id_col=ColNames.grid_id, origin=self.origin\n            )\n            df = df.withColumnRenamed(ColNames.inspire_id, ColNames.zone_id).drop(ColNames.grid_id)\n\n            self.dfs[dfID] = df\n            continue\n        # INSPIRE 1km grid\n        elif self.zoning_dataset == ReservedDatasetIDs.INSPIRE_1km:\n            # Transform to INSPIRE representation\n            df = grid_gen.grid_id_to_inspire_id(\n                sdf=df, inspire_resolution=1000, grid_id_col=ColNames.grid_id, origin=self.origin\n            )\n            df = df.withColumnRenamed(ColNames.inspire_id, ColNames.zone_id).drop(ColNames.grid_id)\n            # Aggregate\n            agg_df = df.groupBy(*segment_columns).agg(*value_expressions)\n            # Add hierarchical level 1 and dataset ID columns\n            agg_df = agg_df.withColumn(ColNames.level, F.lit(1)).withColumn(\n                ColNames.dataset_id, F.lit(self.zoning_dataset)\n            )\n\n            self.dfs[dfID] = agg_df\n            continue\n        else:  # Using a non-reserved zoning dataset\n            for level in self.levels:\n                # Replace zone_id column, which contains the smallest zone ID of the hierarchy, with the zone\n                # of the specific level. The greater zones correspond to lower values of level, and viceversa\n                # Notice that the coarsest level is 1, not 0. The method `F.element_at` has 1-based indexing,\n                # instead of 0-based indexing, so the first level is 1.\n                zone_to_grid_df = zoning_df.withColumn(\n                    ColNames.zone_id, F.element_at(F.split(F.col(ColNames.hierarchical_id), pattern=\"\\\\|\"), level)\n                )\n\n                # Join grid IDs with the zone they correspond to, aggregate, and add level and dataset ID columns\n                agg_df = df.join(zone_to_grid_df, on=ColNames.grid_id)\n                agg_df = agg_df.groupBy(*segment_columns).agg(*value_expressions)\n                agg_df = agg_df.withColumn(ColNames.level, F.lit(level)).withColumn(\n                    ColNames.dataset_id, F.lit(self.zoning_dataset)\n                )\n\n                # If computing more than one level, we construct more than one data object, so handle this\n                # appropriately in the self.output_data_objects dictionary.\n                if len(self.levels) &gt; 1:\n                    output_do = do_info[\"output_constructor\"](\n                        self.spark, self.config.get(CONFIG_GOLD_PATHS_KEY, do_info[\"output_path_config_key\"])\n                    )\n                    self.dfs[dfID + \"_\" + str(level)] = agg_df\n                    self.output_data_objects[dfID + \"_\" + str(level)] = output_do\n                else:\n                    self.dfs[dfID] = agg_df\n\n            # If computing more than one level, all IDs are of the form `dfID_X` where X is one of the levels.\n            # We remove the \"original\" output DO by the new ones we just created.\n            if len(self.levels) &gt; 1:\n                del self.dfs[dfID]\n                del self.output_data_objects[dfID]\n</code></pre>"},{"location":"reference/components/execution/present_population/","title":"present_population","text":""},{"location":"reference/components/execution/present_population/present_population_estimation/","title":"present_population_estimation","text":"<p>Module for estimating the present population of a geographical area at a given time.</p>"},{"location":"reference/components/execution/present_population/present_population_estimation/#components.execution.present_population.present_population_estimation.PresentPopulationEstimation","title":"<code>PresentPopulationEstimation</code>","text":"<p>               Bases: <code>Component</code></p> <p>This component calculates the estimated actual population (number of people spatially present) for a specified spatial area (country, municipality, grid).</p> <p>NOTE: In the current variant 1 of implementation, this module implements only the counting of one MNO's users instead of extrapolating to the entire population.</p> Source code in <code>multimno/components/execution/present_population/present_population_estimation.py</code> <pre><code>class PresentPopulationEstimation(Component):\n    \"\"\"This component calculates the estimated actual population (number of people spatially present)\n    for a specified spatial area (country, municipality, grid).\n\n    NOTE: In the current variant 1 of implementation, this module implements only the counting of one\n    MNO's users instead of extrapolating to the entire population.\n    \"\"\"\n\n    COMPONENT_ID = \"PresentPopulationEstimation\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        # Maximum allowed time difference for an event to be included in a time point.\n        self.tolerance_period_s = self.config.getint(self.COMPONENT_ID, \"tolerance_period_s\")\n\n        # Time boundaries for result calculation.\n        self.data_period_start = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d %H:%M:%S\"\n        )\n        self.data_period_end = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d  %H:%M:%S\"\n        )\n\n        # Time gap (time distance in seconds between time points).\n        self.time_point_gap_s = timedelta(seconds=self.config.getint(self.COMPONENT_ID, \"time_point_gap_s\"))\n\n        # Maximum number of iterations for the Bayesian process.\n        self.max_iterations = self.config.getint(self.COMPONENT_ID, \"max_iterations\")\n\n        # Minimum difference threshold between prior and posterior to continue iterating the Bayesian process.\n        # Compares sum of absolute differences of each row.\n        self.min_difference_threshold = self.config.getfloat(self.COMPONENT_ID, \"min_difference_threshold\")\n\n        self.event_error_flags_to_include = self.config.geteval(self.COMPONENT_ID, \"event_error_flags_to_include\")\n        self.time_point = None\n        self.partition_number = self.config.getint(self.COMPONENT_ID, \"partition_number\")\n\n    def initalize_data_objects(self):\n        input_silver_event_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"event_data_silver_flagged\")\n        clear_destination_directory = self.config.getboolean(self.COMPONENT_ID, \"clear_destination_directory\")\n        input_silver_cell_connection_prob_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"cell_connection_probabilities_data_silver\"\n        )\n        input_silver_grid_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"grid_data_silver\")\n        input_silver_event = SilverEventFlaggedDataObject(self.spark, input_silver_event_path)\n        input_silver_cell_connection_prob = SilverCellConnectionProbabilitiesDataObject(\n            self.spark, input_silver_cell_connection_prob_path\n        )\n        input_silver_grid = SilverGridDataObject(self.spark, input_silver_grid_path)\n        self.input_data_objects = {\n            SilverEventFlaggedDataObject.ID: input_silver_event,\n            SilverCellConnectionProbabilitiesDataObject.ID: input_silver_cell_connection_prob,\n            SilverGridDataObject.ID: input_silver_grid,\n        }\n\n        # Output\n        # Output data object depends on whether results are aggregated per grid or per zone.\n        silver_present_population_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"present_population_silver\")\n        output_present_population = SilverPresentPopulationDataObject(\n            self.spark,\n            silver_present_population_path,\n        )\n        self.output_data_objects = {SilverPresentPopulationDataObject.ID: output_present_population}\n\n        if clear_destination_directory:\n            delete_file_or_folder(self.spark, silver_present_population_path)\n\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(\"STARTING: Present Population Estimation\")\n\n        self.read()\n\n        # Generate desired time points.\n        time_points = generate_time_points(self.data_period_start, self.data_period_end, self.time_point_gap_s)\n\n        # Processing logic: handle time points independently one at a time. Write results after each time point.\n        for time_point in time_points:\n            try:\n                self.logger.info(f\"Present Population: Starting time point {time_point}\")\n                self.time_point = time_point\n                self.transform()\n                self.write()\n            except PpNoDevicesException as e:\n                self.logger.warning(e.error_msg(time_point))\n                continue\n            finally:\n                # Cleanup\n                self.spark.catalog.clearCache()\n                tmp_pp_path = self.config.get(GENERAL_CONFIG_KEY, \"tmp_present_population_path\")\n                delete_file_or_folder(self.spark, tmp_pp_path)\n                self.logger.info(f\"Present Population: Finished time point {time_point}\")\n        self.logger.info(\"FINISHED: Present Population Estimation\")\n\n    def transform(self):\n        time_point = self.time_point\n        # Filter event data to dates within allowed time bounds.\n        events_df = self.input_data_objects[SilverEventFlaggedDataObject.ID].df\n        events_df = events_df.filter(F.col(ColNames.error_flag).isin(self.event_error_flags_to_include))\n\n        # Apply date-level filtering to omit events from dates unrelated to this time point.\n        events_df = select_where_dates_include_time_point_window(time_point, self.tolerance_period_s, events_df)\n\n        # Number of devices connected to each cell, taking only their event closest to the time_point\n        count_per_cell_df = self.calculate_devices_per_cell(events_df, time_point)\n\n        cell_conn_prob_df = self.get_cell_connection_probabilities(time_point)\n\n        # calculate population estimates per grid tile. It will write the results in tmp path\n        population_per_grid_df = self.calculate_population_per_grid(count_per_cell_df, cell_conn_prob_df)\n\n        # Prepare the results.\n        tmp_pp_path = self.config.get(GENERAL_CONFIG_KEY, \"tmp_present_population_path\")\n        population_per_grid_df = self.spark.read.parquet(tmp_pp_path)\n        population_per_grid_df = (\n            population_per_grid_df\n            # .withColumn(ColNames.population, F.col(ColNames.population).cast(FloatType()))\n            .withColumn(ColNames.timestamp, F.lit(time_point))\n            .withColumn(ColNames.year, F.lit(time_point.year))\n            .withColumn(ColNames.month, F.lit(time_point.month))\n            .withColumn(ColNames.day, F.lit(time_point.day))\n        )\n        # Set results data object\n        population_per_grid_df = apply_schema_casting(population_per_grid_df, SilverPresentPopulationDataObject.SCHEMA)\n        population_per_grid_df = population_per_grid_df.coalesce(self.partition_number)\n\n        self.output_data_objects[SilverPresentPopulationDataObject.ID].df = population_per_grid_df\n\n    def get_cell_connection_probabilities(self, time_point: datetime) -&gt; DataFrame:\n        \"\"\"\n        Filter the cell connection probabilities of the dates needed for the time_point provided.\n        Args:\n            time_point (datetime.datetime): timestamp of time point\n\n        Returns:\n            DataFrame: (grid_id, cell_id, cell_connection_probability) dataframe\n        \"\"\"\n        time_bound_lower = time_point - timedelta(seconds=self.tolerance_period_s)\n        time_bound_upper = time_point + timedelta(seconds=self.tolerance_period_s)\n\n        cell_conn_prob_df = (\n            self.input_data_objects[SilverCellConnectionProbabilitiesDataObject.ID]\n            .df.select(\n                ColNames.year,\n                ColNames.month,\n                ColNames.day,\n                ColNames.cell_id,\n                ColNames.grid_id,\n                ColNames.cell_connection_probability,\n            )\n            .filter(\n                F.make_date(ColNames.year, ColNames.month, ColNames.day).between(\n                    lowerBound=F.lit(time_bound_lower.date()), upperBound=F.lit(time_bound_upper.date())\n                )\n            )\n        )\n\n        return cell_conn_prob_df\n\n    def calculate_devices_per_cell(\n        self,\n        events_df: DataFrame,\n        time_point: datetime,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Calculates the number of unique users/devices per cell for one time point based on the events inside the\n        interval around the time_point. If a device has multiple events inside the interval, the one closest to the\n        time_point is selected. In case of a tie, the earliest event is chosen.\n\n        Args:\n            events_df (DataFrame): Event data. For each user, expected to contain all of that user's events that can be\n                included in this time point.\n            time_point (datetime): The timestamp for which the population counts are calculated for.\n\n        Returns:\n            DataFrame: Count of devices per cell\n        \"\"\"\n        # Filter to include only events within the time window of the time point.\n        time_bound_lower = time_point - timedelta(seconds=self.tolerance_period_s)\n        time_bound_upper = time_point + timedelta(seconds=self.tolerance_period_s)\n        events_df = events_df.where(\n            (time_bound_lower &lt;= F.col(ColNames.timestamp)) &amp; (F.col(ColNames.timestamp) &lt;= time_bound_upper)\n        )\n\n        # For each user, order events by time distance from time point. If tied, prefer earlier timestamp.\n        window = Window.partitionBy(ColNames.user_id).orderBy(\n            F.abs(F.col(ColNames.timestamp) - time_point).asc(),\n            F.col(ColNames.timestamp),\n        )\n\n        events_df = (\n            events_df.withColumn(\"rank\", F.row_number().over(window))\n            .filter(F.col(\"rank\") == F.lit(1))  # event closes to the time_point\n            .drop(\"rank\")\n        )\n\n        counts_df = events_df.groupBy(ColNames.year, ColNames.month, ColNames.day, ColNames.cell_id).agg(\n            F.count(ColNames.user_id).alias(ColNames.device_count)\n        )\n\n        return counts_df\n\n    def calculate_population_per_grid(self, devices_per_cell_df: DataFrame, cell_conn_prob_df: DataFrame):\n        \"\"\"\n        Calculates population estimates for each grid tile Using an iterative Bayesian process.\n\n        Args:\n            devices_per_cell_df (DataFrame): (cell_id, device_count) dataframe\n            cell_conn_prob_df (DataFrame): (grid_id, cell_id, cell_connection_probability) dataframe\n        Returns:\n            None\n        Raises:\n            PpNoDevicesException: If no devices are found at the time point being calculated.\n        \"\"\"\n        devices_per_cell_df.persist()\n\n        # First, calculate total number of devices and grid tiles to initialise the prior\n        total_devices_row = devices_per_cell_df.agg(F.sum(ColNames.device_count).alias(\"total_devices\")).first()\n        total_devices = total_devices_row[\"total_devices\"] if total_devices_row else 0\n\n        if total_devices is None or total_devices == 0:\n            raise PpNoDevicesException()\n\n        grid_df = self.input_data_objects[SilverGridDataObject.ID].df.select(ColNames.grid_id)\n\n        # persist the grid dataframe as it will be used two times\n        # 1- Count grid tiles\n        # 2- Initial population per grid tile using prior\n        grid_df.persist()\n\n        # TODO: total number of grid tiles, or total number of grid tiles covered by the cells our events refer to?\n        # (Action)\n        total_tiles = grid_df.count()\n\n        # Initial prior value of population per tile\n        initial_prior_value = float(total_devices / total_tiles)\n\n        # Create Initial Population Dataframe\n        pop_df = grid_df.select(ColNames.grid_id).withColumn(ColNames.population, F.lit(initial_prior_value))\n\n        # Create master dataframe\n        # Fields: year, month, day, cell_id, grid_id, cell_connection_probability, device_count(in the cell)\n        master_df = cell_conn_prob_df.join(\n            devices_per_cell_df, on=[ColNames.year, ColNames.month, ColNames.day, ColNames.cell_id]\n        )\n\n        # Persist static information\n        master_df.persist()\n\n        # (Action) Action to persist master_df\n        master_df.count()\n\n        niter = 0\n        diff = float(\"inf\")\n        has_converged = False\n\n        normalisation_window = Window().partitionBy(ColNames.year, ColNames.month, ColNames.day, ColNames.cell_id)\n\n        # (Action) Persist the population dataframe in disk\n        tmp_pp_path = self.config.get(GENERAL_CONFIG_KEY, \"tmp_present_population_path\")\n        pop_df.write.parquet(tmp_pp_path, mode=\"overwrite\")\n\n        # un-persist the grid dataframe\n        grid_df.unpersist()\n        devices_per_cell_df.unpersist()\n\n        # Start the iterative process\n        while niter &lt; self.max_iterations:\n            # Load the population dataframe from disk\n            pop_df = self.spark.read.parquet(tmp_pp_path)\n\n            # Calculate the new population dataframe using the cached master dataframe\n            new_pop_df = (\n                master_df.drop(ColNames.population)\n                .join(pop_df, on=ColNames.grid_id)\n                .withColumn(\"previous_population\", F.col(ColNames.population))\n                .withColumn(\n                    ColNames.population, F.col(ColNames.population) * F.col(ColNames.cell_connection_probability)\n                )\n                .withColumn(\n                    ColNames.population,\n                    (\n                        F.col(ColNames.device_count)\n                        * F.col(ColNames.population)\n                        / F.sum(ColNames.population).over(normalisation_window)\n                    ),\n                )\n                .groupby(ColNames.grid_id)\n                .agg(\n                    F.sum(ColNames.population).alias(ColNames.population),\n                    F.first(\"previous_population\").alias(\"previous_population\"),\n                )\n            )\n\n            # Cache the new population dataframe\n            new_pop_df.persist()\n\n            # (Action) Calculate the difference between the new population dataframe and the previous one\n            diff = (\n                new_pop_df.select(\n                    F.sum(F.abs(F.col(\"previous_population\") - F.col(ColNames.population))).alias(\"difference\")\n                )\n            ).first()\n            diff = diff[\"difference\"] if diff else 0\n\n            # (Action) Overwrite in disk the new population dataframe &amp; Unpersist the new population dataframe\n            new_pop_df.write.parquet(tmp_pp_path, mode=\"overwrite\")\n            new_pop_df.unpersist()\n\n            niter += 1\n            self.logger.info(f\"Finished iteration {niter}, diff {diff} vs threshold {self.min_difference_threshold}\")\n\n            # Check exit condition\n            if diff &lt; self.min_difference_threshold:\n                has_converged = True\n                break\n\n        if has_converged:\n            self.logger.info(f\"Algorithm convergence for tolerance {self.min_difference_threshold}!\")\n        else:\n            self.logger.info(f\"Stopped iterations after reaching max iterations {self.max_iterations}\")\n\n        # Cleanup\n        master_df.unpersist()\n\n        # At the end of the iteration, we have our population estimation over the grid tiles written in disk\n        return\n</code></pre>"},{"location":"reference/components/execution/present_population/present_population_estimation/#components.execution.present_population.present_population_estimation.PresentPopulationEstimation.calculate_devices_per_cell","title":"<code>calculate_devices_per_cell(events_df, time_point)</code>","text":"<p>Calculates the number of unique users/devices per cell for one time point based on the events inside the interval around the time_point. If a device has multiple events inside the interval, the one closest to the time_point is selected. In case of a tie, the earliest event is chosen.</p> <p>Parameters:</p> Name Type Description Default <code>events_df</code> <code>DataFrame</code> <p>Event data. For each user, expected to contain all of that user's events that can be included in this time point.</p> required <code>time_point</code> <code>datetime</code> <p>The timestamp for which the population counts are calculated for.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Count of devices per cell</p> Source code in <code>multimno/components/execution/present_population/present_population_estimation.py</code> <pre><code>def calculate_devices_per_cell(\n    self,\n    events_df: DataFrame,\n    time_point: datetime,\n) -&gt; DataFrame:\n    \"\"\"\n    Calculates the number of unique users/devices per cell for one time point based on the events inside the\n    interval around the time_point. If a device has multiple events inside the interval, the one closest to the\n    time_point is selected. In case of a tie, the earliest event is chosen.\n\n    Args:\n        events_df (DataFrame): Event data. For each user, expected to contain all of that user's events that can be\n            included in this time point.\n        time_point (datetime): The timestamp for which the population counts are calculated for.\n\n    Returns:\n        DataFrame: Count of devices per cell\n    \"\"\"\n    # Filter to include only events within the time window of the time point.\n    time_bound_lower = time_point - timedelta(seconds=self.tolerance_period_s)\n    time_bound_upper = time_point + timedelta(seconds=self.tolerance_period_s)\n    events_df = events_df.where(\n        (time_bound_lower &lt;= F.col(ColNames.timestamp)) &amp; (F.col(ColNames.timestamp) &lt;= time_bound_upper)\n    )\n\n    # For each user, order events by time distance from time point. If tied, prefer earlier timestamp.\n    window = Window.partitionBy(ColNames.user_id).orderBy(\n        F.abs(F.col(ColNames.timestamp) - time_point).asc(),\n        F.col(ColNames.timestamp),\n    )\n\n    events_df = (\n        events_df.withColumn(\"rank\", F.row_number().over(window))\n        .filter(F.col(\"rank\") == F.lit(1))  # event closes to the time_point\n        .drop(\"rank\")\n    )\n\n    counts_df = events_df.groupBy(ColNames.year, ColNames.month, ColNames.day, ColNames.cell_id).agg(\n        F.count(ColNames.user_id).alias(ColNames.device_count)\n    )\n\n    return counts_df\n</code></pre>"},{"location":"reference/components/execution/present_population/present_population_estimation/#components.execution.present_population.present_population_estimation.PresentPopulationEstimation.calculate_population_per_grid","title":"<code>calculate_population_per_grid(devices_per_cell_df, cell_conn_prob_df)</code>","text":"<p>Calculates population estimates for each grid tile Using an iterative Bayesian process.</p> <p>Parameters:</p> Name Type Description Default <code>devices_per_cell_df</code> <code>DataFrame</code> <p>(cell_id, device_count) dataframe</p> required <code>cell_conn_prob_df</code> <code>DataFrame</code> <p>(grid_id, cell_id, cell_connection_probability) dataframe</p> required <p>Returns:     None Raises:     PpNoDevicesException: If no devices are found at the time point being calculated.</p> Source code in <code>multimno/components/execution/present_population/present_population_estimation.py</code> <pre><code>def calculate_population_per_grid(self, devices_per_cell_df: DataFrame, cell_conn_prob_df: DataFrame):\n    \"\"\"\n    Calculates population estimates for each grid tile Using an iterative Bayesian process.\n\n    Args:\n        devices_per_cell_df (DataFrame): (cell_id, device_count) dataframe\n        cell_conn_prob_df (DataFrame): (grid_id, cell_id, cell_connection_probability) dataframe\n    Returns:\n        None\n    Raises:\n        PpNoDevicesException: If no devices are found at the time point being calculated.\n    \"\"\"\n    devices_per_cell_df.persist()\n\n    # First, calculate total number of devices and grid tiles to initialise the prior\n    total_devices_row = devices_per_cell_df.agg(F.sum(ColNames.device_count).alias(\"total_devices\")).first()\n    total_devices = total_devices_row[\"total_devices\"] if total_devices_row else 0\n\n    if total_devices is None or total_devices == 0:\n        raise PpNoDevicesException()\n\n    grid_df = self.input_data_objects[SilverGridDataObject.ID].df.select(ColNames.grid_id)\n\n    # persist the grid dataframe as it will be used two times\n    # 1- Count grid tiles\n    # 2- Initial population per grid tile using prior\n    grid_df.persist()\n\n    # TODO: total number of grid tiles, or total number of grid tiles covered by the cells our events refer to?\n    # (Action)\n    total_tiles = grid_df.count()\n\n    # Initial prior value of population per tile\n    initial_prior_value = float(total_devices / total_tiles)\n\n    # Create Initial Population Dataframe\n    pop_df = grid_df.select(ColNames.grid_id).withColumn(ColNames.population, F.lit(initial_prior_value))\n\n    # Create master dataframe\n    # Fields: year, month, day, cell_id, grid_id, cell_connection_probability, device_count(in the cell)\n    master_df = cell_conn_prob_df.join(\n        devices_per_cell_df, on=[ColNames.year, ColNames.month, ColNames.day, ColNames.cell_id]\n    )\n\n    # Persist static information\n    master_df.persist()\n\n    # (Action) Action to persist master_df\n    master_df.count()\n\n    niter = 0\n    diff = float(\"inf\")\n    has_converged = False\n\n    normalisation_window = Window().partitionBy(ColNames.year, ColNames.month, ColNames.day, ColNames.cell_id)\n\n    # (Action) Persist the population dataframe in disk\n    tmp_pp_path = self.config.get(GENERAL_CONFIG_KEY, \"tmp_present_population_path\")\n    pop_df.write.parquet(tmp_pp_path, mode=\"overwrite\")\n\n    # un-persist the grid dataframe\n    grid_df.unpersist()\n    devices_per_cell_df.unpersist()\n\n    # Start the iterative process\n    while niter &lt; self.max_iterations:\n        # Load the population dataframe from disk\n        pop_df = self.spark.read.parquet(tmp_pp_path)\n\n        # Calculate the new population dataframe using the cached master dataframe\n        new_pop_df = (\n            master_df.drop(ColNames.population)\n            .join(pop_df, on=ColNames.grid_id)\n            .withColumn(\"previous_population\", F.col(ColNames.population))\n            .withColumn(\n                ColNames.population, F.col(ColNames.population) * F.col(ColNames.cell_connection_probability)\n            )\n            .withColumn(\n                ColNames.population,\n                (\n                    F.col(ColNames.device_count)\n                    * F.col(ColNames.population)\n                    / F.sum(ColNames.population).over(normalisation_window)\n                ),\n            )\n            .groupby(ColNames.grid_id)\n            .agg(\n                F.sum(ColNames.population).alias(ColNames.population),\n                F.first(\"previous_population\").alias(\"previous_population\"),\n            )\n        )\n\n        # Cache the new population dataframe\n        new_pop_df.persist()\n\n        # (Action) Calculate the difference between the new population dataframe and the previous one\n        diff = (\n            new_pop_df.select(\n                F.sum(F.abs(F.col(\"previous_population\") - F.col(ColNames.population))).alias(\"difference\")\n            )\n        ).first()\n        diff = diff[\"difference\"] if diff else 0\n\n        # (Action) Overwrite in disk the new population dataframe &amp; Unpersist the new population dataframe\n        new_pop_df.write.parquet(tmp_pp_path, mode=\"overwrite\")\n        new_pop_df.unpersist()\n\n        niter += 1\n        self.logger.info(f\"Finished iteration {niter}, diff {diff} vs threshold {self.min_difference_threshold}\")\n\n        # Check exit condition\n        if diff &lt; self.min_difference_threshold:\n            has_converged = True\n            break\n\n    if has_converged:\n        self.logger.info(f\"Algorithm convergence for tolerance {self.min_difference_threshold}!\")\n    else:\n        self.logger.info(f\"Stopped iterations after reaching max iterations {self.max_iterations}\")\n\n    # Cleanup\n    master_df.unpersist()\n\n    # At the end of the iteration, we have our population estimation over the grid tiles written in disk\n    return\n</code></pre>"},{"location":"reference/components/execution/present_population/present_population_estimation/#components.execution.present_population.present_population_estimation.PresentPopulationEstimation.get_cell_connection_probabilities","title":"<code>get_cell_connection_probabilities(time_point)</code>","text":"<p>Filter the cell connection probabilities of the dates needed for the time_point provided. Args:     time_point (datetime.datetime): timestamp of time point</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>(grid_id, cell_id, cell_connection_probability) dataframe</p> Source code in <code>multimno/components/execution/present_population/present_population_estimation.py</code> <pre><code>def get_cell_connection_probabilities(self, time_point: datetime) -&gt; DataFrame:\n    \"\"\"\n    Filter the cell connection probabilities of the dates needed for the time_point provided.\n    Args:\n        time_point (datetime.datetime): timestamp of time point\n\n    Returns:\n        DataFrame: (grid_id, cell_id, cell_connection_probability) dataframe\n    \"\"\"\n    time_bound_lower = time_point - timedelta(seconds=self.tolerance_period_s)\n    time_bound_upper = time_point + timedelta(seconds=self.tolerance_period_s)\n\n    cell_conn_prob_df = (\n        self.input_data_objects[SilverCellConnectionProbabilitiesDataObject.ID]\n        .df.select(\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            ColNames.cell_id,\n            ColNames.grid_id,\n            ColNames.cell_connection_probability,\n        )\n        .filter(\n            F.make_date(ColNames.year, ColNames.month, ColNames.day).between(\n                lowerBound=F.lit(time_bound_lower.date()), upperBound=F.lit(time_bound_upper.date())\n            )\n        )\n    )\n\n    return cell_conn_prob_df\n</code></pre>"},{"location":"reference/components/execution/present_population/present_population_estimation/#components.execution.present_population.present_population_estimation.generate_time_points","title":"<code>generate_time_points(period_start, period_end, time_point_gap_s)</code>","text":"<p>Generates time points within the specified period with the specified spacing.</p> <p>Parameters:</p> Name Type Description Default <code>period_start</code> <code>datetime</code> <p>Start timestamp of generation.</p> required <code>period_end</code> <code>datetime</code> <p>End timestamp of generation.</p> required <code>time_point_gap_s</code> <code>timedelta</code> <p>Time delta object defining the space between consectuive time points.</p> required <p>Returns:</p> Type Description <code>List[datetime]</code> <p>[datetime]: List of time point timestamps.</p> Source code in <code>multimno/components/execution/present_population/present_population_estimation.py</code> <pre><code>def generate_time_points(period_start: datetime, period_end: datetime, time_point_gap_s: timedelta) -&gt; List[datetime]:\n    \"\"\"\n    Generates time points within the specified period with the specified spacing.\n\n    Args:\n        period_start (datetime): Start timestamp of generation.\n        period_end (datetime): End timestamp of generation.\n        time_point_gap_s (timedelta): Time delta object defining the space between consectuive time points.\n\n    Returns:\n        [datetime]: List of time point timestamps.\n    \"\"\"\n    # TODO this might be reusable across components.\n    time_points = []\n    one_time_point = period_start\n    while one_time_point &lt;= period_end:\n        time_points.append(one_time_point)\n        one_time_point = one_time_point + time_point_gap_s\n    return time_points\n</code></pre>"},{"location":"reference/components/execution/present_population/present_population_estimation/#components.execution.present_population.present_population_estimation.select_where_dates_include_time_point_window","title":"<code>select_where_dates_include_time_point_window(time_point, tolerance_period_s, df)</code>","text":"<p>Applies filtering to the DataFrame to omit rows from dates which are outside time point boundaries. The purpose is to leverage our Parquet partitioning schema (partitioned by year, month, day) and use predicate pushdown to avoid reading event data from irrelevant days.</p> <p>Parameters:</p> Name Type Description Default <code>time_point</code> <code>datetime</code> <p>Fixed timestamp to calculate results for.</p> required <code>tolerance_period_s</code> <code>int</code> <p>Time window size. Time in seconds before and after the time point</p> required <code>df</code> <code>DataFrame</code> <p>DataFrame of event data storage partitioned by year, month, day.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>df including only data from dates which include some part of the time point's window.</p> Source code in <code>multimno/components/execution/present_population/present_population_estimation.py</code> <pre><code>def select_where_dates_include_time_point_window(\n    time_point: datetime, tolerance_period_s: int, df: DataFrame\n) -&gt; DataFrame:\n    \"\"\"\n    Applies filtering to the DataFrame to omit rows from dates which are outside time point boundaries.\n    The purpose is to leverage our Parquet partitioning schema (partitioned by year, month, day) and\n    use predicate pushdown to avoid reading event data from irrelevant days.\n\n    Args:\n        time_point (datetime): Fixed timestamp to calculate results for.\n        tolerance_period_s (int): Time window size. Time in seconds before and after the time point\n        within which the event data is included.\n        df (DataFrame): DataFrame of event data storage partitioned by year, month, day.\n\n    Returns:\n        DataFrame: df including only data from dates which include some part of the time point's window.\n    \"\"\"\n    time_bound_lower = time_point - timedelta(seconds=tolerance_period_s)\n    date_lower = time_bound_lower.date()\n    time_bound_upper = time_point + timedelta(seconds=tolerance_period_s)\n    date_upper = time_bound_upper.date()\n    return df.where(\n        (F.make_date(ColNames.year, ColNames.month, ColNames.day) &gt;= date_lower)\n        &amp; (F.make_date(ColNames.year, ColNames.month, ColNames.day) &lt;= date_upper)\n    )\n</code></pre>"},{"location":"reference/components/execution/time_segments/","title":"time_segments","text":""},{"location":"reference/components/execution/time_segments/continuous_time_segmentation/","title":"continuous_time_segmentation","text":"<p>Module that implements the Continuous Time Segmentations functionality</p>"},{"location":"reference/components/execution/time_segments/continuous_time_segmentation/#components.execution.time_segments.continuous_time_segmentation.ContinuousTimeSegmentation","title":"<code>ContinuousTimeSegmentation</code>","text":"<p>               Bases: <code>Component</code></p> <p>A class to aggregate events into time segments.</p> Source code in <code>multimno/components/execution/time_segments/continuous_time_segmentation.py</code> <pre><code>class ContinuousTimeSegmentation(Component):\n    \"\"\"\n    A class to aggregate events into time segments.\n    \"\"\"\n\n    COMPONENT_ID = \"ContinuousTimeSegmentation\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.data_period_start = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n        self.data_period_end = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n\n        self.min_time_stay = timedelta(seconds=self.config.getint(self.COMPONENT_ID, \"min_time_stay_s\"))\n        self.max_time_missing_stay = timedelta(seconds=self.config.getint(self.COMPONENT_ID, \"max_time_missing_stay_s\"))\n        self.max_time_missing_move = timedelta(seconds=self.config.getint(self.COMPONENT_ID, \"max_time_missing_move_s\"))\n        self.max_time_missing_abroad = timedelta(\n            seconds=self.config.getint(self.COMPONENT_ID, \"max_time_missing_abroad_s\")\n        )\n        self.pad_time = timedelta(seconds=self.config.getint(self.COMPONENT_ID, \"pad_time_s\"))\n\n        self.event_error_flags_to_include = self.config.geteval(self.COMPONENT_ID, \"event_error_flags_to_include\")\n        self.domains_to_include = ContinuousTimeSegmentation._get_domains_to_include(\n            self.config.geteval(self.COMPONENT_ID, \"domains_to_include\")\n        )\n\n        # When only outbound results are requested, we still want to use domestic events to determine outbound segment end times.\n        # Thus we read them, but omit the domestic (stay and move) segments from the output.\n        if (Domains.OUTBOUND in self.domains_to_include) &amp; (Domains.DOMESTIC not in self.domains_to_include):\n            self.do_outbound_only = True\n        else:\n            self.do_outbound_only = False\n\n        self.local_mcc = self.config.getint(self.COMPONENT_ID, \"local_mcc\")\n        # this is for UDF\n        self.segmentation_return_schema = StructType(\n            [\n                StructField(ColNames.start_timestamp, TimestampType()),\n                StructField(ColNames.end_timestamp, TimestampType()),\n                StructField(ColNames.last_event_timestamp, TimestampType()),\n                StructField(ColNames.cells, ArrayType(StringType())),\n                StructField(ColNames.state, ByteType()),\n                StructField(ColNames.is_last, BooleanType()),\n                StructField(ColNames.time_segment_id, StringType()),\n                StructField(ColNames.user_id, StringType()),\n                StructField(ColNames.mcc, ShortType()),\n                StructField(ColNames.mnc, StringType()),\n                StructField(ColNames.plmn, IntegerType()),\n                StructField(ColNames.user_id_modulo, IntegerType()),\n            ]\n        )\n\n        self.data_period_dates = [\n            self.data_period_start + timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n        self.last_time_segments = None\n        self.current_date = None\n\n    def initalize_data_objects(self):\n\n        self.output_silver_time_segments_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"time_segments_silver\")\n        # Output clearing\n        # Done first, since the time segment existence is checked on the same directory\n        clear_time_segments_directory = self.config.getboolean(\n            self.COMPONENT_ID, \"clear_time_segments_directory\", fallback=False\n        )\n        if clear_time_segments_directory:\n            delete_file_or_folder(self.spark, self.output_silver_time_segments_path)\n        # TODO add optional date-limited deletion when not first run,\n        # but consider that segments get generated not only for D but at least D+1 as well\n\n        # Input\n        self.input_data_objects = {}\n        inputs = {\n            \"event_data_silver_flagged\": SilverEventFlaggedDataObject,\n            \"cell_intersection_groups_data_silver\": SilverCellIntersectionGroupsDataObject,\n            \"event_cache\": EventCacheDataObject,\n        }\n        for key, value in inputs.items():\n            path = self.config.get(CONFIG_SILVER_PATHS_KEY, key)\n            if check_if_data_path_exists(self.spark, path):\n                self.input_data_objects[value.ID] = value(self.spark, path)\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")\n                raise ValueError(f\"Invalid path for {value.ID}: {path}\")\n\n        # SilverTimeSegmentsDataObject input data can exist when initializing, but may not.\n        path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"time_segments_silver\")\n        if check_if_data_path_exists(self.spark, path):\n            self.input_data_objects[SilverTimeSegmentsDataObject.ID] = SilverTimeSegmentsDataObject(self.spark, path)\n        else:\n            self.logger.info(f\"No existing time segments found at {path}.\")\n\n        # Output\n        self.output_data_objects = {}\n        self.output_data_objects[SilverTimeSegmentsDataObject.ID] = SilverTimeSegmentsDataObject(\n            self.spark,\n            self.output_silver_time_segments_path,\n        )\n\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n\n        # for every date in the data period, get the events and the intersection groups\n        #  for that date + get first event of each user for the following date, calculate the time segments\n        for current_date in self.data_period_dates:\n            self.logger.info(f\"Processing events for {current_date.strftime('%Y-%m-%d')}\")\n            self.current_date = current_date\n            self.read()\n\n            next_date = current_date + timedelta(days=1)\n\n            self.current_input_events_sdf = (\n                self.input_data_objects[SilverEventFlaggedDataObject.ID]\n                .df.filter(\n                    (\n                        F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n                        == F.lit(current_date)\n                    )\n                    &amp; (F.col(ColNames.error_flag).isin(self.event_error_flags_to_include))\n                    &amp; (F.col(ColNames.domain).isin(self.domains_to_include))\n                )\n                .unionByName(  # Add first event of each user from the next date\n                    apply_schema_casting(\n                        self.input_data_objects[EventCacheDataObject.ID].df.filter(\n                            (\n                                F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n                                == F.lit(next_date)\n                            )\n                            &amp; (F.col(ColNames.is_last_event) == False)  # Matches first event of date\n                        ),\n                        SilverEventFlaggedDataObject.SCHEMA,\n                    )\n                )\n            )\n\n            # Get cell intersection groups for the current date.\n            # Note: Currently the events in D+1 are assigned the cell intersection groups of D, not D+1.\n            self.current_interesection_groups_sdf = (\n                self.input_data_objects[SilverCellIntersectionGroupsDataObject.ID]\n                .df.filter(\n                    (\n                        F.make_date(\n                            F.col(ColNames.year),\n                            F.col(ColNames.month),\n                            F.col(ColNames.day),\n                        )\n                        == F.lit(current_date)\n                    )\n                )\n                .select(ColNames.cell_id, ColNames.overlapping_cell_ids, ColNames.year, ColNames.month, ColNames.day)\n            )\n\n            # If there are existing time segments, retrieve each is_last time segment in D-1.\n            # If not, the starting state of time segments will be initialized later.\n            if SilverTimeSegmentsDataObject.ID in self.input_data_objects.keys():\n                previous_date = current_date - timedelta(days=1)\n                last_time_segments_selection = self.input_data_objects[SilverTimeSegmentsDataObject.ID].df.filter(\n                    (\n                        (\n                            F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n                            == F.lit(previous_date)\n                        )\n                        &amp; (F.col(ColNames.is_last) == True)\n                    )\n                    | (\n                        F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n                        == F.lit(current_date)\n                    )\n                )\n                # The segments selection can provide zero or more segments. Select the latest one among them as the latest previous segment.\n                self.last_time_segments = (\n                    last_time_segments_selection.withColumn(\n                        \"max_end_timestamp\", F.max(ColNames.end_timestamp).over(Window.partitionBy(ColNames.user_id))\n                    )\n                    .where(F.col(\"end_timestamp\") == F.col(\"max_end_timestamp\"))\n                    .drop(\"max_end_timestamp\")\n                )\n            else:\n                self.last_time_segments = None\n\n            # If only outbound domain is requested, we additionally include the domestic events of outbound users (if any are present).\n            if self.do_outbound_only:\n                # Select distinct users in current outbound events.\n                # If previous date time segments are present, also include distinct users with ABROAD state in previous date.\n                if self.last_time_segments is not None:\n                    distinct_outbound_user_ids_df = (\n                        self.current_input_events_sdf.select(F.col(ColNames.user_id))\n                        .union(\n                            self.last_time_segments.where(F.col(ColNames.state) == F.lit(SegmentStates.ABROAD)).select(\n                                F.col(ColNames.user_id)\n                            )\n                        )\n                        .distinct()\n                    )\n                else:\n                    distinct_outbound_user_ids_df = self.current_input_events_sdf.select(\n                        F.col(ColNames.user_id)\n                    ).distinct()\n                # Retrieve domestic events of these users.\n                additional_domestic_events_df = (\n                    self.input_data_objects[SilverEventFlaggedDataObject.ID]\n                    .df.filter(\n                        (\n                            F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n                            == F.lit(current_date)\n                        )\n                        &amp; (F.col(ColNames.error_flag).isin(self.event_error_flags_to_include))\n                        &amp; (F.col(ColNames.domain) == F.lit(Domains.DOMESTIC)).alias(\"df1\")\n                    )\n                    .join(distinct_outbound_user_ids_df.alias(\"df2\"), on=[ColNames.user_id], how=\"left_semi\")\n                    .select(\"df1.*\")\n                )\n                self.current_input_events_sdf = self.current_input_events_sdf.unionByName(additional_domestic_events_df)\n\n            self.transform()\n            self.write()\n            self.input_data_objects[SilverTimeSegmentsDataObject.ID] = self.output_data_objects[\n                SilverTimeSegmentsDataObject.ID\n            ]\n\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n\n        current_events_sdf = self.current_input_events_sdf\n        last_time_segments_sdf = self.last_time_segments\n        intersections_groups_df = self.current_interesection_groups_sdf\n\n        # Add overlapping_cell_ids list to each current date event\n        current_events_sdf = (\n            current_events_sdf.alias(\"df1\")\n            .join(\n                intersections_groups_df.alias(\"df2\"),\n                on=[ColNames.cell_id],\n                how=\"left\",\n            )\n            .select(\n                f\"df1.{ColNames.user_id}\",\n                f\"df1.{ColNames.timestamp}\",\n                f\"df1.{ColNames.mcc}\",\n                f\"df1.{ColNames.mnc}\",\n                f\"df1.{ColNames.plmn}\",\n                f\"df1.{ColNames.domain}\",\n                f\"df1.{ColNames.cell_id}\",\n                f\"df1.{ColNames.user_id_modulo}\",\n                ColNames.overlapping_cell_ids,\n            )\n        )\n        if last_time_segments_sdf is None:\n            # If there are no previous time segments, initialize the time segment columns of the dataframe\n            current_events_sdf = (\n                current_events_sdf.withColumn(ColNames.end_timestamp, F.lit(None).cast(TimestampType()))\n                .withColumn(ColNames.last_event_timestamp, F.lit(None))\n                .withColumn(ColNames.cells, F.lit(None))\n                .withColumn(ColNames.state, F.lit(None))\n                .withColumn(\"segment_mcc\", F.lit(None))\n                .withColumn(\"segment_mnc\", F.lit(None))\n                .withColumn(\"segment_plmn\", F.lit(None))\n            )\n        else:\n            # If previous time segments are present, join and set the time segment columns (D-1 is_last segment values)\n            last_time_segments_sdf = last_time_segments_sdf.select(\n                ColNames.end_timestamp,\n                ColNames.last_event_timestamp,\n                ColNames.cells,\n                ColNames.state,\n                ColNames.user_id,\n                F.col(ColNames.mcc).alias(\"segment_mcc\"),\n                F.col(ColNames.mnc).alias(\"segment_mnc\"),\n                F.col(ColNames.plmn).alias(\"segment_plmn\"),\n                ColNames.user_id_modulo,\n            )\n            current_events_sdf = current_events_sdf.join(\n                F.broadcast(last_time_segments_sdf),\n                on=[ColNames.user_id_modulo, ColNames.user_id],\n                how=\"outer\",\n            )\n\n            current_events_sdf = (\n                current_events_sdf.withColumn(\n                    ColNames.mcc, F.coalesce(F.col(ColNames.mcc), F.col(\"segment_mcc\"))\n                ).withColumn(ColNames.mnc, F.coalesce(F.col(ColNames.mnc), F.col(\"segment_mnc\")))\n            ).drop(\"segment_mcc\", \"segment_mnc\")\n\n        # TODO: This conversion is needed for Pandas serialisation/deserialisation,\n        # to remove it when user_id will be stored as string, not as binary\n        current_events_sdf = current_events_sdf.withColumn(ColNames.user_id, F.hex(F.col(ColNames.user_id)))\n\n        current_events_sdf = current_events_sdf.withColumn(\n            \"is_abroad_event\", F.col(ColNames.domain) == F.lit(Domains.OUTBOUND)\n        )\n\n        # Partial function to pass the current date and other parameters to the aggregation function\n        aggregate_segments_partial = partial(\n            self.aggregate_segments,\n            current_date=self.current_date,\n            min_time_stay=self.min_time_stay,\n            max_time_missing_stay=self.max_time_missing_stay,\n            max_time_missing_move=self.max_time_missing_move,\n            max_time_missing_abroad=self.max_time_missing_abroad,\n            pad_time=self.pad_time,\n        )\n\n        # TODO: To test this approach with large datasets, might not be feasible\n        current_segments_sdf = current_events_sdf.groupby(ColNames.user_id_modulo, ColNames.user_id).applyInPandas(\n            aggregate_segments_partial, self.segmentation_return_schema\n        )\n\n        current_segments_sdf = current_segments_sdf.withColumns(\n            {\n                ColNames.year: F.year(ColNames.start_timestamp).cast(\"smallint\"),\n                ColNames.month: F.month(ColNames.start_timestamp).cast(\"tinyint\"),\n                ColNames.day: F.dayofmonth(ColNames.start_timestamp).cast(\"tinyint\"),\n            }\n        )\n\n        # TODO: This conversion is needed to get back to binary after Pandas serialisation/deserialisation,\n        # to remove it when user_id will be stored as string, not as binary\n        current_segments_sdf = current_segments_sdf.withColumn(ColNames.user_id, F.unhex(F.col(ColNames.user_id)))\n\n        current_segments_sdf = apply_schema_casting(current_segments_sdf, SilverTimeSegmentsDataObject.SCHEMA)\n        current_segments_sdf = current_segments_sdf.repartition(\n            *SilverTimeSegmentsDataObject.PARTITION_COLUMNS\n        ).sortWithinPartitions(ColNames.user_id, ColNames.start_timestamp)\n\n        self.output_data_objects[SilverTimeSegmentsDataObject.ID].df = current_segments_sdf\n\n    @staticmethod\n    def aggregate_segments(\n        pdf: pd.DataFrame,\n        current_date: date,\n        min_time_stay: timedelta,\n        max_time_missing_stay: timedelta,\n        max_time_missing_move: timedelta,\n        max_time_missing_abroad: timedelta,\n        pad_time: timedelta,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Aggregates user stays into continuous time segments based on given parameters.\n        This function processes user location data and creates continuous time segments,\n        taking into account various time-based parameters to determine segment boundaries and types.\n        Args:\n            pdf: DataFrame containing user location events.\n            current_date: Date for which to generate segments.\n            min_time_stay: Minimum duration required to consider a period as a stay.\n            max_time_missing_stay: Maximum allowed gap in data while maintaining a stay segment.\n            max_time_missing_move: Maximum allowed gap in data while maintaining a move segment.\n            max_time_missing_abroad: Maximum allowed gap in data for abroad segments.\n            pad_time: Time padding to add around segments.\n        Returns:\n            DataFrame containing aggregated time segments.\n        \"\"\"\n        user_id, user_mod, mcc, mnc = ContinuousTimeSegmentation._get_user_metadata(pdf)\n\n        # Prepare date boundaries\n        current_date_start = datetime.combine(current_date, time(0, 0, 0))\n        current_date_end = datetime.combine(current_date, time(23, 59, 59))\n\n        # Check if there are any events for this date\n        # D+1 events are not included in this check\n        no_events_for_current_date = pdf[pdf[ColNames.timestamp] &lt;= current_date_end][ColNames.timestamp].isna().all()\n        no_previous_segments = pdf[ColNames.end_timestamp].isna().all()\n\n        if no_events_for_current_date and no_previous_segments:\n            # If both no events in D and no previous segments, then this is the first date before this user has any events.\n            # Return empty DataFrame.\n            return pd.DataFrame()\n        elif no_events_for_current_date:\n            # If no events in D but previous segments exist, create a single UNKNOWN segment for date D\n            segments = ContinuousTimeSegmentation._handle_no_events_for_current_date(\n                pdf, no_previous_segments, user_id, current_date_start, current_date_end, max_time_missing_abroad\n            )\n        else:\n            # If events in D exist, process them along with data of latest existing time segment to generate segments.\n\n            # Determine if the existing previous time segment is in D or D-1.\n            # If in D, we want to omit the inital time segment from output later since it has already been written.\n            is_previous_segment_in_current_date = pdf[ColNames.end_timestamp].iloc[0] &gt;= current_date_start\n\n            # Create the initial time segment for this day\n            current_ts = ContinuousTimeSegmentation._create_initial_time_segment(\n                pdf,\n                no_previous_segments,\n                current_date_start,\n                pad_time,\n                user_id,\n                max_time_missing_stay,\n                max_time_missing_move,\n                max_time_missing_abroad,\n            )\n\n            # Limit columns we actually need\n            pdf_for_events = pdf[\n                [ColNames.timestamp, ColNames.cell_id, ColNames.overlapping_cell_ids, \"is_abroad_event\", ColNames.plmn]\n            ]\n\n            # Build segments from each event\n            segments = ContinuousTimeSegmentation._iterate_events(\n                pdf_for_events,\n                current_ts,\n                user_id,\n                min_time_stay,\n                max_time_missing_stay,\n                max_time_missing_move,\n                max_time_missing_abroad,\n                pad_time,\n                current_date_end,\n            )\n\n            # Omit first time segment if it has already been written.\n            if is_previous_segment_in_current_date:\n                segments = segments[1:]\n\n        # Convert list of segments to DataFrame\n        segments_df = pd.DataFrame(segments)\n        segments_df[ColNames.user_id] = user_id\n        segments_df[ColNames.mcc] = mcc\n        segments_df[ColNames.mnc] = mnc\n        segments_df[ColNames.user_id_modulo] = user_mod\n\n        return segments_df\n\n    # ---------------------  No-Events Helper  ---------------------\n    @staticmethod\n    def _handle_no_events_for_current_date(\n        pdf: pd.DataFrame,\n        no_previous_segments: bool,\n        user_id: str,\n        day_start: datetime,\n        day_end: datetime,\n        max_time_missing_abroad: timedelta,\n    ) -&gt; List[Dict]:\n        \"\"\"Handles cases where there are no events for the current date.\n        This method creates a time segment for a day without events. If there were previous segments\n        and the last segment was abroad within the maximum allowed time gap, it continues the abroad state.\n        Otherwise, it creates an unknown state segment.\n        Args:\n            pdf (pd.DataFrame): DataFrame containing previous segments information\n            no_previous_segments (bool): Flag indicating if there are previous segments\n            user_id (str): Identifier for the user\n            day_start (datetime): Start timestamp of the day\n            day_end (datetime): End timestamp of the day\n            max_time_missing_abroad (timedelta): Maximum allowed time gap for continuing abroad state\n        Returns:\n            List[Dict]: List containing a single time segment dictionary with the appropriate state\n        \"\"\"\n        if not no_previous_segments:\n            previous_segment_state = pdf[ColNames.state].iloc[0]\n            previous_segment_plmn = pdf[\"segment_plmn\"].iloc[0]\n            previous_segment_last_event_timestamp = pdf[ColNames.last_event_timestamp].iloc[0]\n\n            # If time since the last event timestamp is below threshold, create whole-day ABROAD segment\n            if (previous_segment_state == SegmentStates.ABROAD) and (\n                day_end - previous_segment_last_event_timestamp &lt;= max_time_missing_abroad\n            ):\n                seg = ContinuousTimeSegmentation._create_time_segment(\n                    day_start,\n                    day_end,\n                    previous_segment_last_event_timestamp,\n                    [],\n                    previous_segment_plmn,\n                    SegmentStates.ABROAD,\n                    user_id,\n                )\n            # If time since the last event timestamp is above threshold, create whole-day UNKNOWN segment\n            else:\n                seg = ContinuousTimeSegmentation._create_time_segment(\n                    day_start, day_end, None, [], None, SegmentStates.UNKNOWN, user_id\n                )\n        # If no previous time segment, create whole-day UNKNOWN segment\n        else:\n            seg = ContinuousTimeSegmentation._create_time_segment(\n                day_start, day_end, None, [], None, SegmentStates.UNKNOWN, user_id\n            )\n\n        seg[ColNames.is_last] = True\n        return [seg]\n\n    # ---------------------  Initial Segment Helper  ---------------------\n    @staticmethod\n    def _create_initial_time_segment(\n        pdf: pd.DataFrame,\n        no_previous_segments: bool,\n        day_start: datetime,\n        pad_time: timedelta,\n        user_id: str,\n        max_time_missing_stay: timedelta,\n        max_time_missing_move: timedelta,\n        max_time_missing_abroad: timedelta,\n    ) -&gt; Dict:\n        \"\"\"Create initial time segment based on first event and previous day information.\n        Creates a time segment from the start of the day until the first event of the day,\n        considering any existing segments from the previous day to maintain continuity.\n        Args:\n            pdf: DataFrame containing the first event data\n            no_previous_segments: Boolean indicating if there are segments from previous day\n            day_start: DateTime marking the start of the current day\n            pad_time: TimeDelta for padding unknown segments\n            user_id: String identifier for the user\n            max_time_missing_stay: Maximum allowed gap for stay segments\n            max_time_missing_move: Maximum allowed gap for move segments\n            max_time_missing_abroad: Maximum allowed gap for abroad segments\n        Returns:\n            Dict containing the created time segment\n        \"\"\"\n        first_event_time = pdf[ColNames.timestamp].iloc[0]\n        previous_segment_end_time = pdf[ColNames.end_timestamp].iloc[0]\n        previous_segment_last_event_timestamp = pdf[ColNames.last_event_timestamp].iloc[0]\n        previous_segment_state = pdf[ColNames.state].iloc[0]\n        previous_segment_plmn = pdf[\"segment_plmn\"].iloc[0]\n        previous_segment_cells = pdf[ColNames.cells].iloc[0]\n        if previous_segment_cells is not None:\n            previous_segment_cells = list(previous_segment_cells)  # cast to list from pandas array\n\n        time_to_first_event = first_event_time - day_start\n        adjusted_pad = min(pad_time, time_to_first_event / 2)\n\n        if no_previous_segments:\n            # No segment from previous or current day =&gt; unknown until first event\n            return ContinuousTimeSegmentation._create_time_segment(\n                day_start,\n                first_event_time - adjusted_pad,\n                None,\n                [],\n                None,\n                SegmentStates.UNKNOWN,\n                user_id,\n            )\n\n        # The previous segment can be either in D-1 or in D.\n        # If D, we want to omit the first event since it has already been used to generate the segment.\n        if previous_segment_end_time &gt; day_start:\n            return ContinuousTimeSegmentation._create_time_segment(\n                day_start,\n                previous_segment_end_time,\n                previous_segment_last_event_timestamp,\n                previous_segment_cells,\n                previous_segment_plmn,\n                previous_segment_state,\n                user_id,\n            )\n\n        # Otherwise if the previous segment is in D-1, try to continue from the previous day\n        gap = first_event_time - previous_segment_last_event_timestamp\n\n        if (previous_segment_state == SegmentStates.STAY) and (gap &lt;= max_time_missing_stay):\n            return ContinuousTimeSegmentation._create_time_segment(\n                day_start,\n                first_event_time,\n                previous_segment_last_event_timestamp,\n                previous_segment_cells,\n                previous_segment_plmn,\n                SegmentStates.STAY,\n                user_id,\n            )\n        elif (previous_segment_state == SegmentStates.MOVE) and (gap &lt;= max_time_missing_move):\n            return ContinuousTimeSegmentation._create_time_segment(\n                day_start,\n                first_event_time,\n                previous_segment_last_event_timestamp,\n                previous_segment_cells,\n                previous_segment_plmn,\n                SegmentStates.MOVE,\n                user_id,\n            )\n        elif (previous_segment_state == SegmentStates.ABROAD) and (gap &lt;= max_time_missing_abroad):\n            return ContinuousTimeSegmentation._create_time_segment(\n                day_start,\n                first_event_time,\n                previous_segment_last_event_timestamp,\n                [],\n                previous_segment_plmn,\n                SegmentStates.ABROAD,\n                user_id,\n            )\n        else:\n            # Large gap or incompatible =&gt; unknown until first event\n            return ContinuousTimeSegmentation._create_time_segment(\n                day_start,\n                first_event_time - adjusted_pad,\n                None,\n                [],\n                None,\n                SegmentStates.UNKNOWN,\n                user_id,\n            )\n\n    # ---------------------  Iteration Over Events ---------------------\n    @staticmethod\n    def _iterate_events(\n        pdf_events: pd.DataFrame,\n        current_ts: Dict,\n        user_id: str,\n        min_time_stay: timedelta,\n        max_time_missing_stay: timedelta,\n        max_time_missing_move: timedelta,\n        max_time_missing_abroad: timedelta,\n        pad_time: timedelta,\n        current_date_end: datetime,\n    ) -&gt; List[Dict]:\n        \"\"\"Iterates through events and constructs time segments based on continuous time segmentation rules.\n\n        Processes a sequence of events (both abroad and local) and creates time segments according to\n        specified time constraints. Each event updates the current time segment state and may generate\n        new segments when conditions are met.\n\n        Args:\n            pdf_events: DataFrame containing events with timestamp, location, and other relevant information.\n            current_ts: Dictionary representing the current time segment state.\n            user_id: String identifier for the user.\n            min_time_stay: Minimum duration required for a stay segment.\n            max_time_missing_stay: Maximum allowed gap in stay segments.\n            max_time_missing_move: Maximum allowed gap in movement segments.\n            max_time_missing_abroad: Maximum allowed gap in abroad segments.\n            pad_time: Time padding added to segments.\n            current_date_end: Midnight timestamp at the end of the current date.\n\n        Returns:\n            List of dictionaries representing time segments\n        \"\"\"\n        all_segments: List[Dict] = []\n\n        for event in pdf_events.itertuples(index=False):\n            if event.is_abroad_event:\n                # Process abroad logic\n                new_segments, new_current = ContinuousTimeSegmentation._process_abroad_event(\n                    current_ts,\n                    user_id,\n                    event.timestamp,\n                    event.plmn,\n                    max_time_missing_abroad,\n                )\n            else:\n                # Process local logic\n                new_segments, new_current = ContinuousTimeSegmentation._process_local_event(\n                    current_ts,\n                    user_id,\n                    event.timestamp,\n                    event.cell_id,\n                    event.overlapping_cell_ids,\n                    event.plmn,\n                    min_time_stay,\n                    max_time_missing_stay,\n                    max_time_missing_move,\n                    pad_time,\n                )\n\n            all_segments.extend(new_segments)\n            current_ts = new_current\n\n        # Add the last segment to segments list\n        all_segments.append(current_ts)\n\n        # If the last event is from D+1, then we have up to one segment which is in both D and D+1. We want to split that segment to D and D+1 parts.\n        # It is possible for such a segment to not exist if the generated segment ends exactly at midnight.\n        if current_ts[ColNames.end_timestamp] &gt; current_date_end:\n            all_segments = ContinuousTimeSegmentation._handle_multi_day_segment(all_segments, current_date_end, user_id)\n        else:\n            # If there is no D+1 event, then this user has no events in D+1.\n            # Apply separate logic to generate a segment that ends at midnight of D.\n            all_segments = ContinuousTimeSegmentation._handle_last_segment_if_no_next_date_events(\n                all_segments, current_date_end, user_id\n            )\n\n        return all_segments\n\n    @staticmethod\n    def _extend_segment(current_ts: Dict, new_end_time: datetime, new_cells: List[Any] = None) -&gt; Dict:\n        \"\"\"\n        Returns a brand new segment dictionary with an extended_ts end_time\n        and optionally merged cells. Does not mutate the original.\n        \"\"\"\n        updated_ts = current_ts.copy()\n        updated_ts[ColNames.end_timestamp] = new_end_time\n\n        if new_cells is not None:\n            merged_cells = list(set(updated_ts[ColNames.cells] + new_cells))\n            updated_ts[ColNames.cells] = merged_cells\n\n        return updated_ts\n\n    # ---------------------  Processing Each Event ---------------------\n    @staticmethod\n    def _process_abroad_event(\n        current_ts: Dict,\n        user_id: str,\n        event_timestamp: datetime,\n        event_plmn: str,\n        max_time_missing_abroad: timedelta,\n    ) -&gt; Tuple[List[Dict], Dict]:\n        \"\"\"\n        Decide whether to extend current ABROAD segment, create a new one,\n        or start bridging with UNKNOWN if the gap is too large.\n        Returns (finalized_segments, new_current_ts).\n        \"\"\"\n        segments_to_add: List[Dict] = []\n\n        abroad_mcc = str(event_plmn)[:3]\n        current_mcc = str(current_ts.get(ColNames.plmn) or \"\")[:3]\n        is_mcc_matched = abroad_mcc == current_mcc\n\n        gap = event_timestamp - current_ts[ColNames.end_timestamp]\n\n        if current_ts[ColNames.state] != SegmentStates.ABROAD:\n            # Transition from a different state to ABROAD\n            segments_to_add.append(current_ts)\n            current_ts = ContinuousTimeSegmentation._create_time_segment(\n                current_ts[ColNames.end_timestamp],\n                event_timestamp,\n                current_ts[ColNames.last_event_timestamp],\n                [],\n                event_plmn,\n                SegmentStates.ABROAD,\n                user_id,\n            )\n\n        elif is_mcc_matched and (gap &lt;= max_time_missing_abroad):\n            # Extend existing ABROAD\n            current_ts = ContinuousTimeSegmentation._extend_segment(current_ts, event_timestamp)\n            current_ts[ColNames.last_event_timestamp] = event_timestamp\n\n        elif (not is_mcc_matched) and (gap &lt;= max_time_missing_abroad):\n            # Different MCC but within the gap =&gt; new ABROAD segment\n            segments_to_add.append(current_ts)\n            current_ts = ContinuousTimeSegmentation._create_time_segment(\n                current_ts[ColNames.end_timestamp],\n                event_timestamp,\n                event_timestamp,\n                [],\n                event_plmn,\n                SegmentStates.ABROAD,\n                user_id,\n            )\n\n        else:\n            # Gap too large =&gt; bridging with UNKNOWN\n            segments_to_add.append(current_ts)\n            current_ts = ContinuousTimeSegmentation._create_time_segment(\n                current_ts[ColNames.end_timestamp],\n                event_timestamp,\n                None,\n                [],\n                None,\n                SegmentStates.UNKNOWN,\n                user_id,\n            )\n\n        return segments_to_add, current_ts\n\n    @staticmethod\n    def _process_local_event(\n        current_ts: Dict,\n        user_id: str,\n        event_timestamp: datetime,\n        event_cell: Any,\n        overlapping_cell_ids: Any,\n        event_plmn: Any,\n        min_time_stay: timedelta,\n        max_time_missing_stay: timedelta,\n        max_time_missing_move: timedelta,\n        pad_time: timedelta,\n    ) -&gt; Tuple[List[Dict], Dict]:\n        \"\"\"\n        Decide whether to continue a STAY/UNDETERMINED, transition to MOVE,\n        or insert UNKNOWN bridging based on the local event.\n        Returns (finalized_segments, updated_current_ts).\n        \"\"\"\n        segments_to_add: List[Dict] = []\n        # TODO can use last_event_timestamp in some conditions instead of time since prev segment end\n        gap = event_timestamp - current_ts[ColNames.end_timestamp]\n        if overlapping_cell_ids is None:\n            overlapping_cell_ids = []\n        new_cells = list(overlapping_cell_ids)\n        new_cells.append(event_cell)\n\n        is_intersected = ContinuousTimeSegmentation._check_intersection(\n            current_ts[ColNames.cells],\n            new_cells,\n        )\n\n        # Case 1: UNKNOWN/ABROAD =&gt; UNDETERMINED transition\n        if current_ts[ColNames.state] in [SegmentStates.UNKNOWN, SegmentStates.ABROAD]:\n            segments_to_add.append(current_ts)\n            current_ts = ContinuousTimeSegmentation._create_time_segment(\n                current_ts[ColNames.end_timestamp],\n                event_timestamp,\n                event_timestamp,\n                [event_cell],\n                event_plmn,\n                SegmentStates.UNDETERMINED,\n                user_id,\n            )\n\n        # Case 2: Intersection =&gt; STAY or UNDETERMINED extension\n        elif is_intersected and (gap &lt;= max_time_missing_stay):\n            if current_ts[ColNames.state] in [SegmentStates.UNDETERMINED, SegmentStates.STAY]:\n                # Extend in place\n                current_ts = ContinuousTimeSegmentation._extend_segment(current_ts, event_timestamp, [event_cell])\n                current_ts[ColNames.last_event_timestamp] = event_timestamp\n                duration = current_ts[ColNames.end_timestamp] - current_ts[ColNames.start_timestamp]\n                if duration &gt; min_time_stay:\n                    current_ts[ColNames.state] = SegmentStates.STAY\n\n            elif current_ts[ColNames.state] == SegmentStates.MOVE:\n                # End MOVE =&gt; start UNDETERMINED\n                segments_to_add.append(current_ts)\n                current_ts = ContinuousTimeSegmentation._create_time_segment(\n                    current_ts[ColNames.end_timestamp],\n                    event_timestamp,\n                    event_timestamp,\n                    [event_cell],\n                    event_plmn,\n                    SegmentStates.UNDETERMINED,\n                    user_id,\n                )\n        # Case 3: No intersection but gap &lt;= max_time_missing_move =&gt; 'move'\n        elif (not is_intersected) and (gap &lt;= max_time_missing_move):\n\n            midpoint = current_ts[ColNames.end_timestamp] + gap / 2\n            move_ts_1 = ContinuousTimeSegmentation._create_time_segment(\n                current_ts[ColNames.end_timestamp],\n                midpoint,\n                current_ts[ColNames.last_event_timestamp],\n                current_ts[ColNames.cells],\n                event_plmn,\n                SegmentStates.MOVE,\n                user_id,\n            )\n            segments_to_add.extend([current_ts, move_ts_1])\n\n            current_ts = ContinuousTimeSegmentation._create_time_segment(\n                midpoint,\n                event_timestamp,\n                event_timestamp,\n                [event_cell],\n                event_plmn,\n                SegmentStates.MOVE,\n                user_id,\n            )\n\n        # Case 4: Gap too large =&gt; bridging with UNKNOWN\n        else:\n            # First, artificially extend current_ts by pad_time\n            extended_ts = ContinuousTimeSegmentation._extend_segment(\n                current_ts, current_ts[ColNames.end_timestamp] + pad_time\n            )\n\n            unknown_segment = ContinuousTimeSegmentation._create_time_segment(\n                extended_ts[ColNames.end_timestamp],\n                event_timestamp - pad_time,\n                None,\n                [],\n                None,\n                SegmentStates.UNKNOWN,\n                user_id,\n            )\n\n            segments_to_add.extend([extended_ts, unknown_segment])\n\n            current_ts = ContinuousTimeSegmentation._create_time_segment(\n                event_timestamp - pad_time,\n                event_timestamp,\n                event_timestamp,\n                [event_cell],\n                event_plmn,\n                SegmentStates.UNDETERMINED,\n                user_id,\n            )\n\n        return segments_to_add, current_ts\n\n    @staticmethod\n    def _create_time_segment(\n        start_timestamp: datetime,\n        end_timestamp: datetime,\n        last_event_timestamp: datetime,\n        cells: List[str],\n        plmn: int,\n        state: str,\n        user_id: str,\n    ) -&gt; Dict:\n        \"\"\"\n        Creates a new time segment.\n\n        It creates a new time segment with these values, incrementing the segment ID by 1\n        if a previous segment ID is provided, or setting it to 1 if not.\n\n        Parameters:\n        start_timestamp (datetime): The start timestamp of the time segment.\n        end_timestamp (datetime): The end timestamp of the time segment.\n        last_event_timestamp (datetime): The timestamp of the last event of the time segment.\n        cells (List[str]): The cells of the time segment.\n        state (str): The state of the time segment.\n        previous_segment_id (Optional[int]): The ID of the previous time segment, if any.\n\n        Returns:\n        Dict: The new time segment.\n        \"\"\"\n        segment_id_string = f\"{user_id}{start_timestamp}\"\n        return {\n            ColNames.time_segment_id: hashlib.md5(segment_id_string.encode()).hexdigest(),\n            ColNames.start_timestamp: start_timestamp,\n            ColNames.end_timestamp: end_timestamp,\n            ColNames.last_event_timestamp: last_event_timestamp,\n            ColNames.cells: cells,\n            ColNames.plmn: plmn,\n            ColNames.state: state,\n            ColNames.is_last: False,\n        }\n\n    @staticmethod\n    def _handle_multi_day_segment(all_segments: list[dict], current_date_end: datetime, user_id: str) -&gt; list[dict]:\n        \"\"\"\n        Handles segments which cross date bounds (midnight).\n        These segments are split into two halves on midnight, one segment in D and one in D+1.\n        At most one such segment can exist per user per date.\n\n        Args:\n            all_segments (list[dict]): list of segments\n            current_date_end (datetime): midnight timestamp of current date end\n            user_id (str): user identifier\n\n        Returns:\n            list[dict]: list of segments with midnight-crossing segment replaced by its two halves\n        \"\"\"\n        # Reverse-order iterate over segments.\n        for i in reversed(range(0, len(all_segments))):\n            seg = all_segments[i]\n            # If current segment ends within D, it is the last segment of D.\n            # Mark it as such. End iterating.\n            if seg[ColNames.end_timestamp] &lt;= current_date_end:\n                all_segments[i][ColNames.is_last] = True\n                return all_segments\n            # If current segment crosses midnight, split it into two segments.\n            # Mark the half in D as last segment of D. End iterating.\n            if (seg[ColNames.start_timestamp] &lt;= current_date_end) &amp; (seg[ColNames.end_timestamp] &gt; current_date_end):\n                # Split the segment into two on midnight\n                ts1 = ContinuousTimeSegmentation._create_time_segment(\n                    start_timestamp=seg[ColNames.start_timestamp],\n                    end_timestamp=current_date_end,\n                    last_event_timestamp=seg[ColNames.last_event_timestamp],\n                    cells=seg[ColNames.cells],\n                    plmn=seg[ColNames.plmn],\n                    state=seg[ColNames.state],\n                    user_id=user_id,\n                )\n                ts1[ColNames.is_last] = True\n\n                ts2 = ContinuousTimeSegmentation._create_time_segment(\n                    start_timestamp=current_date_end + timedelta(seconds=1),\n                    end_timestamp=seg[ColNames.end_timestamp],\n                    last_event_timestamp=seg[ColNames.last_event_timestamp],\n                    cells=seg[ColNames.cells],\n                    plmn=seg[ColNames.plmn],\n                    state=seg[ColNames.state],\n                    user_id=user_id,\n                )\n                # Remove existing segment, add two new segments, end processing\n                all_segments = all_segments[:i] + [ts1, ts2] + all_segments[i + 1 :]\n                return all_segments\n\n    @staticmethod\n    def _handle_last_segment_if_no_next_date_events(\n        all_segments: list[dict], current_date_end: datetime, user_id: str\n    ) -&gt; list[dict]:\n        \"\"\"\n        Handles last segment generation if there are no events for the next date.\n        Generates an UNKNOWN segment which starts at the end of the existing segment and ends at midnight.\n\n        Args:\n            all_segments (list[dict]): list of current date segments\n            current_date_end (datetime): midnight timestamp of the current date end\n            user_id (str): user identifier\n        Returns:\n            list[dict]: list of current date segments with added last segment\n        \"\"\"\n        # Generate segment from last segment end until D midight\n        # TODO what type? UNKNOWN? Extend last segment?\n        seg = all_segments[-1]\n        ts = ContinuousTimeSegmentation._create_time_segment(\n            start_timestamp=seg[ColNames.end_timestamp],\n            end_timestamp=current_date_end,\n            last_event_timestamp=None,\n            cells=[],\n            plmn=None,\n            state=SegmentStates.UNKNOWN,\n            user_id=user_id,\n        )\n        all_segments.append(ts)\n\n        # Mark final segment as is_last\n        all_segments[-1][ColNames.is_last] = True\n\n        return all_segments\n\n    @staticmethod\n    def _get_user_metadata(pdf: pdDataFrame) -&gt; Tuple[str, int, str]:\n        \"\"\"\n        Gets user_id, user_id_modulo, mcc, mnc from Pandas DataFrame containing columns with the corresponding names.\n        Values from the first row of the dataframe are used.\n\n        Args:\n            pdf (pdDataFrame): Pandas DataFrame\n\n        Returns:\n            Tuple[str, int, str]: user_id, user_id_modulo, mcc, mnc\n        \"\"\"\n        user_id = pdf[ColNames.user_id][0]\n        user_id_mod = pdf[ColNames.user_id_modulo][0]\n        mcc = pdf[ColNames.mcc][0]\n        mnc = pdf[ColNames.mnc][0]\n        return user_id, user_id_mod, mcc, mnc\n\n    @staticmethod\n    def _check_intersection(\n        previous_ts_cells: List[str],\n        current_event_overlapping_cell_ids: List[str],\n    ) -&gt; bool:\n        \"\"\"\n        Checks if there is an intersection between the existing time segment and the current event.\n\n        This method takes two lists of cells, one for the cells included in the existing time segment and the other for\n        the overlapping cell ids of the current event's cell.\n        The time segment intersects with the event if each of the time segment's cells are included in the event's overlapping cell ids list.\n\n        A segment with no cells cannot intersect and returns False.\n\n        Parameters:\n        previous_ts_cells (List[str]): The cells of the existing time segment.\n        current_event_overlapping_cell_ids (List[str]): Cells the current event's cell overlaps with, including itself.\n\n        Returns:\n        bool: True if there is an intersection, False otherwise.\n        \"\"\"\n        if len(previous_ts_cells) == 0:\n            is_intersected = False\n        else:\n            is_intersected = set(previous_ts_cells).issubset(set(current_event_overlapping_cell_ids))\n        return is_intersected\n\n    @staticmethod\n    def _get_domains_to_include(\n        config_domains_to_include: List[str],\n    ) -&gt; List[str]:\n        \"\"\"\n        Returns a list of domains to include in the component.\n\n        Parses the configuration for domain names (inbound, domestic, outbound) and maps to the corresponding Domains constants.\n\n        Parameters:\n        config_domains_to_include (List[str]): List of domains to include from the configuration file.\n\n        Returns:\n        List[str]: List of Domains to include in the segmentation.\n        \"\"\"\n        domains_to_include = []\n        for val in config_domains_to_include:\n            if val == \"outbound\":\n                domains_to_include.append(Domains.OUTBOUND)\n            elif val == \"inbound\":\n                domains_to_include.append(Domains.INBOUND)\n            elif val == \"domestic\":\n                domains_to_include.append(Domains.DOMESTIC)\n            else:\n                raise ValueError(f\"Value {val} does not match any domain name (inbound, domestic, outbound)\")\n\n        if len(domains_to_include) == 0:\n            raise ValueError(\"No domain names specfied in configuration\")\n\n        return domains_to_include\n</code></pre>"},{"location":"reference/components/execution/time_segments/continuous_time_segmentation/#components.execution.time_segments.continuous_time_segmentation.ContinuousTimeSegmentation.aggregate_segments","title":"<code>aggregate_segments(pdf, current_date, min_time_stay, max_time_missing_stay, max_time_missing_move, max_time_missing_abroad, pad_time)</code>  <code>staticmethod</code>","text":"<p>Aggregates user stays into continuous time segments based on given parameters. This function processes user location data and creates continuous time segments, taking into account various time-based parameters to determine segment boundaries and types. Args:     pdf: DataFrame containing user location events.     current_date: Date for which to generate segments.     min_time_stay: Minimum duration required to consider a period as a stay.     max_time_missing_stay: Maximum allowed gap in data while maintaining a stay segment.     max_time_missing_move: Maximum allowed gap in data while maintaining a move segment.     max_time_missing_abroad: Maximum allowed gap in data for abroad segments.     pad_time: Time padding to add around segments. Returns:     DataFrame containing aggregated time segments.</p> Source code in <code>multimno/components/execution/time_segments/continuous_time_segmentation.py</code> <pre><code>@staticmethod\ndef aggregate_segments(\n    pdf: pd.DataFrame,\n    current_date: date,\n    min_time_stay: timedelta,\n    max_time_missing_stay: timedelta,\n    max_time_missing_move: timedelta,\n    max_time_missing_abroad: timedelta,\n    pad_time: timedelta,\n) -&gt; pd.DataFrame:\n    \"\"\"Aggregates user stays into continuous time segments based on given parameters.\n    This function processes user location data and creates continuous time segments,\n    taking into account various time-based parameters to determine segment boundaries and types.\n    Args:\n        pdf: DataFrame containing user location events.\n        current_date: Date for which to generate segments.\n        min_time_stay: Minimum duration required to consider a period as a stay.\n        max_time_missing_stay: Maximum allowed gap in data while maintaining a stay segment.\n        max_time_missing_move: Maximum allowed gap in data while maintaining a move segment.\n        max_time_missing_abroad: Maximum allowed gap in data for abroad segments.\n        pad_time: Time padding to add around segments.\n    Returns:\n        DataFrame containing aggregated time segments.\n    \"\"\"\n    user_id, user_mod, mcc, mnc = ContinuousTimeSegmentation._get_user_metadata(pdf)\n\n    # Prepare date boundaries\n    current_date_start = datetime.combine(current_date, time(0, 0, 0))\n    current_date_end = datetime.combine(current_date, time(23, 59, 59))\n\n    # Check if there are any events for this date\n    # D+1 events are not included in this check\n    no_events_for_current_date = pdf[pdf[ColNames.timestamp] &lt;= current_date_end][ColNames.timestamp].isna().all()\n    no_previous_segments = pdf[ColNames.end_timestamp].isna().all()\n\n    if no_events_for_current_date and no_previous_segments:\n        # If both no events in D and no previous segments, then this is the first date before this user has any events.\n        # Return empty DataFrame.\n        return pd.DataFrame()\n    elif no_events_for_current_date:\n        # If no events in D but previous segments exist, create a single UNKNOWN segment for date D\n        segments = ContinuousTimeSegmentation._handle_no_events_for_current_date(\n            pdf, no_previous_segments, user_id, current_date_start, current_date_end, max_time_missing_abroad\n        )\n    else:\n        # If events in D exist, process them along with data of latest existing time segment to generate segments.\n\n        # Determine if the existing previous time segment is in D or D-1.\n        # If in D, we want to omit the inital time segment from output later since it has already been written.\n        is_previous_segment_in_current_date = pdf[ColNames.end_timestamp].iloc[0] &gt;= current_date_start\n\n        # Create the initial time segment for this day\n        current_ts = ContinuousTimeSegmentation._create_initial_time_segment(\n            pdf,\n            no_previous_segments,\n            current_date_start,\n            pad_time,\n            user_id,\n            max_time_missing_stay,\n            max_time_missing_move,\n            max_time_missing_abroad,\n        )\n\n        # Limit columns we actually need\n        pdf_for_events = pdf[\n            [ColNames.timestamp, ColNames.cell_id, ColNames.overlapping_cell_ids, \"is_abroad_event\", ColNames.plmn]\n        ]\n\n        # Build segments from each event\n        segments = ContinuousTimeSegmentation._iterate_events(\n            pdf_for_events,\n            current_ts,\n            user_id,\n            min_time_stay,\n            max_time_missing_stay,\n            max_time_missing_move,\n            max_time_missing_abroad,\n            pad_time,\n            current_date_end,\n        )\n\n        # Omit first time segment if it has already been written.\n        if is_previous_segment_in_current_date:\n            segments = segments[1:]\n\n    # Convert list of segments to DataFrame\n    segments_df = pd.DataFrame(segments)\n    segments_df[ColNames.user_id] = user_id\n    segments_df[ColNames.mcc] = mcc\n    segments_df[ColNames.mnc] = mnc\n    segments_df[ColNames.user_id_modulo] = user_mod\n\n    return segments_df\n</code></pre>"},{"location":"reference/components/execution/tourism_outbound_statistics/","title":"tourism_outbound_statistics","text":""},{"location":"reference/components/execution/tourism_outbound_statistics/tourism_outbound_statistics_calculation/","title":"tourism_outbound_statistics_calculation","text":"<p>Module that implements the Outbound Tourism use case.</p>"},{"location":"reference/components/execution/tourism_outbound_statistics/tourism_outbound_statistics_calculation/#components.execution.tourism_outbound_statistics.tourism_outbound_statistics_calculation.TourismOutboundStatisticsCalculation","title":"<code>TourismOutboundStatisticsCalculation</code>","text":"<p>               Bases: <code>Component</code></p> <p>A class to calculate outbound tourism statistics per time period.</p> Source code in <code>multimno/components/execution/tourism_outbound_statistics/tourism_outbound_statistics_calculation.py</code> <pre><code>class TourismOutboundStatisticsCalculation(Component):\n    \"\"\"\n    A class to calculate outbound tourism statistics per time period.\n    \"\"\"\n\n    COMPONENT_ID = \"TourismOutboundStatisticsCalculation\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.data_period_start = datetime.strptime(self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m\")\n        self.data_period_end = datetime.strptime(self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m\")\n        # Generate (month_first_date, month_last_date) pairs\n        self.data_period_bounds_list = self.get_period_month_bounds_list(self.data_period_start, self.data_period_end)\n\n        self.min_duration_segment_m = self.config.getint(self.COMPONENT_ID, \"min_duration_segment_m\")\n        self.functional_midnight_h = self.config.getint(self.COMPONENT_ID, \"functional_midnight_h\")\n        self.min_duration_segment_night_m = self.config.getint(self.COMPONENT_ID, \"min_duration_segment_night_m\")\n\n        self.max_trip_gap_h = self.config.getint(self.COMPONENT_ID, \"max_outbound_trip_gap_h\")\n\n    def initalize_data_objects(self):\n\n        # Delete existing trips if needed\n        self.delete_existing_trips = self.config.getboolean(self.COMPONENT_ID, \"delete_existing_trips\")\n        if self.delete_existing_trips:\n            output_tourism_trip_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"tourism_outbound_trips_silver\")\n            delete_file_or_folder(self.spark, output_tourism_trip_path)\n        # Input\n        self.input_data_objects = {}\n\n        inputs = {\n            \"time_segments_silver\": SilverTimeSegmentsDataObject,\n            \"mcc_iso_timezones_data_bronze\": BronzeMccIsoTzMap,\n            # tourism trips are handled separately\n        }\n\n        for key, value in inputs.items():\n            if self.config.has_option(CONFIG_BRONZE_PATHS_KEY, key):\n                path = self.config.get(CONFIG_BRONZE_PATHS_KEY, key)\n            else:\n                path = self.config.get(CONFIG_SILVER_PATHS_KEY, key)\n            if check_if_data_path_exists(self.spark, path):\n                self.input_data_objects[value.ID] = value(self.spark, path)\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")\n                raise ValueError(f\"Invalid path for {value.ID}: {path}\")\n\n        # Separate handling for tourism trips: data existence is optional.\n        # If data does not exist, create empty dataframe with proper schema.\n        outbound_tourism_trip_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"tourism_outbound_trips_silver\")\n        self.input_data_objects[SilverTourismTripDataObject.ID] = SilverTourismTripDataObject(\n            self.spark, outbound_tourism_trip_path\n        )\n        if not check_if_data_path_exists(self.spark, outbound_tourism_trip_path):\n            self.input_data_objects[SilverTourismTripDataObject.ID].df = self.spark.createDataFrame(\n                [], schema=SilverTourismTripDataObject.SCHEMA\n            )\n            self.input_data_objects[SilverTourismTripDataObject.ID].write()  # Write empty dataframe to path\n\n        # Output\n        self.output_data_objects = {}\n\n        self.output_tourism_trip_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"tourism_outbound_trips_silver\")\n        self.output_data_objects[SilverTourismTripDataObject.ID] = SilverTourismTripDataObject(\n            self.spark,\n            self.output_tourism_trip_path,\n        )\n\n        self.output_tourism_outbound_aggregation_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"tourism_outbound_aggregations_silver\"\n        )\n        self.output_data_objects[SilverTourismOutboundNightsSpentDataObject.ID] = (\n            SilverTourismOutboundNightsSpentDataObject(\n                self.spark,\n                self.output_tourism_outbound_aggregation_path,\n            )\n        )\n\n        # Output clearing\n        clear_destination_directory = self.config.getboolean(self.COMPONENT_ID, \"clear_destination_directory\")\n        if clear_destination_directory:\n            delete_file_or_folder(self.spark, self.output_tourism_outbound_aggregation_path)\n\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n\n        # Handle mcc to iso timezone mapping data.\n        self.mcc_iso_tz_map_df = self.input_data_objects[BronzeMccIsoTzMap.ID].df.select(\n            ColNames.mcc, ColNames.iso2, ColNames.timezone\n        )\n\n        # for every month within the data period, process stays and calculate aggregations.\n        for current_month_date_min, current_month_date_max in self.data_period_bounds_list:\n\n            self.read()\n            self.current_month_date_min, self.current_month_date_max = current_month_date_min, current_month_date_max\n            self.current_month_time_period_string = f\"{current_month_date_min.year}-{current_month_date_min.month:0&gt;2}\"\n            # Increase date_max to include stays from the next month within max_trip_gap_h\n            input_date_max = current_month_date_max + timedelta(days=ceil(self.max_trip_gap_h / 24.0))\n            input_timestamp_max = current_month_date_max + timedelta(hours=self.max_trip_gap_h)\n            self.logger.info(\n                f\"Processing segments in date range {current_month_date_min.strftime('%Y-%m-%d')} - {input_date_max.strftime('%Y-%m-%d')}, look-forward timestamp max: {input_timestamp_max}\"\n            )\n\n            # Select segments of the current month plus those within the look-forward window\n            current_month_stays_df = (\n                self.input_data_objects[SilverTimeSegmentsDataObject.ID]\n                .df.filter(\n                    (\n                        F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n                        &gt;= F.lit(current_month_date_min)\n                    )\n                    &amp; (\n                        F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n                        &lt;= F.lit(input_date_max)\n                    )\n                    &amp; (F.col(ColNames.start_timestamp) &lt;= input_timestamp_max)\n                    &amp; (F.col(ColNames.state) == SegmentStates.ABROAD)  # Only consider abroad segments\n                    &amp; (\n                        (F.col(ColNames.end_timestamp).cast(\"long\") - F.col(ColNames.start_timestamp).cast(\"long\"))\n                        / 60.0\n                        &gt;= self.min_duration_segment_m\n                    )  # Filter out segments shorter than min_duration_segment_m\n                )\n                .select(\n                    F.col(ColNames.user_id),\n                    F.col(ColNames.time_segment_id),\n                    F.lit(None).alias(ColNames.trip_id),\n                    F.lit(None).alias(ColNames.trip_start_timestamp),\n                    F.col(ColNames.start_timestamp),\n                    F.col(ColNames.end_timestamp),\n                    F.col(ColNames.user_id_modulo),\n                    F.col(ColNames.plmn),\n                    F.col(ColNames.year),\n                    F.col(ColNames.month),\n                )\n            )\n\n            # Get previous ongoing trips\n            ongoing_trips_df = self.input_data_objects[SilverTourismTripDataObject.ID].df.filter(\n                (F.col(ColNames.is_trip_finished) == False)\n                &amp; (F.col(ColNames.year) == (current_month_date_min - relativedelta(months=1)).year)\n                &amp; (F.col(ColNames.month) == (current_month_date_min - relativedelta(months=1)).month)\n            )\n            if ongoing_trips_df.count() == 0:\n                self.relevant_stays_df = current_month_stays_df\n                self.logger.info(\"No ongoing trips from the previous month.\")\n            else:\n                # get earliest start timestamp of the trip\n                earliest_start_timestamp = ongoing_trips_df.select(F.min(ColNames.trip_start_timestamp)).collect()[0][0]\n\n                # get all stays starting from earliest date of ongoing trips\n                last_month_stays_df = (\n                    self.input_data_objects[SilverTimeSegmentsDataObject.ID]\n                    .df.filter(\n                        (F.col(ColNames.start_timestamp) &gt;= earliest_start_timestamp)\n                        &amp; (F.col(ColNames.end_timestamp) &lt; current_month_date_min)\n                    )\n                    .select(\n                        F.col(ColNames.user_id),\n                        F.col(ColNames.time_segment_id),\n                        F.col(ColNames.start_timestamp),\n                        F.col(ColNames.end_timestamp),\n                        F.col(ColNames.user_id_modulo),\n                        F.col(ColNames.plmn),\n                        F.col(ColNames.year),\n                        F.col(ColNames.month),\n                    )\n                )\n\n                ongoing_trips_stays_df = ongoing_trips_df.withColumn(\n                    ColNames.time_segment_id, F.explode(ColNames.time_segment_ids_list)\n                ).select(\n                    ColNames.user_id,\n                    ColNames.trip_id,\n                    ColNames.trip_start_timestamp,\n                    ColNames.time_segment_id,\n                    ColNames.user_id_modulo,\n                )\n\n                ongoing_trips_stays_df = ongoing_trips_stays_df.join(\n                    last_month_stays_df,\n                    [ColNames.user_id, ColNames.time_segment_id, ColNames.user_id_modulo],\n                    \"inner\",\n                )\n                self.relevant_stays_df = current_month_stays_df.unionByName(ongoing_trips_stays_df)\n\n            self.transform()\n            self.write()\n            self.spark.catalog.clearCache()\n\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n\n        relevant_stays_df = self.relevant_stays_df\n        mcc_iso_tz_map_df = self.mcc_iso_tz_map_df\n\n        # Process abroad segments:\n        # 1. Join with MCC_ISO_TZ_MAP to get iso2 and timezone\n        # 2. Mark segments as overnight or not based on local time zone\n\n        relevant_stays_df = relevant_stays_df.join(\n            mcc_iso_tz_map_df,\n            F.col(ColNames.plmn).substr(1, 3) == F.col(ColNames.mcc),\n            \"left\",\n        )\n\n        relevant_stays_df = self.mark_overnight_segments(relevant_stays_df)\n\n        # Calculate trips\n        relevant_stays_df = self.calculate_trips(relevant_stays_df)\n        relevant_stays_df = relevant_stays_df.cache()\n        relevant_stays_df.count()\n        # NOTE: Count is used to force dataframe calculation to prevent aggregation calculations from reading the\n        #  new trips as input due to cache behaviour. Not certain if this is the best solution.\n\n        # group stays to trips collect list of segment ids.\n        trips_df = relevant_stays_df.groupBy(ColNames.user_id, ColNames.trip_id).agg(\n            F.first(ColNames.is_trip_finished).alias(ColNames.is_trip_finished),\n            F.first(ColNames.start_timestamp).alias(ColNames.trip_start_timestamp),\n            F.collect_list(F.col(ColNames.time_segment_id)).alias(\n                ColNames.time_segment_ids_list\n            ),  # NOTE: ordering is not guaranteed\n            F.lit(self.current_month_date_min.year).alias(ColNames.year),\n            F.lit(self.current_month_date_min.month).alias(ColNames.month),\n            F.first(ColNames.user_id_modulo).alias(ColNames.user_id_modulo),\n            F.lit(ReservedDatasetIDs.ABROAD).alias(ColNames.dataset_id),\n        )\n\n        # Omit trips that start in the lookahead window.\n        trips_df = trips_df.filter(F.col(ColNames.trip_start_timestamp) &lt;= self.current_month_date_max)\n        trips_df = apply_schema_casting(trips_df, SilverTourismTripDataObject.SCHEMA)\n        trips_df = trips_df.repartition(*SilverTourismTripDataObject.PARTITION_COLUMNS)\n\n        self.output_data_objects[SilverTourismTripDataObject.ID].df = trips_df\n\n        relevant_stays_df = self.calculate_visits(relevant_stays_df)\n\n        aggregations_df = relevant_stays_df.groupBy(ColNames.iso2).agg(\n            # Sum of overnight stays in countries for the current month\n            F.sum(F.when((F.col(\"is_visit_finished\")) &amp; (F.col(ColNames.is_overnight)), 1).otherwise(0)).alias(\n                ColNames.nights_spent\n            ),\n        )\n\n        aggregations_df = (\n            aggregations_df.withColumn(ColNames.time_period, F.lit(self.current_month_time_period_string))\n            .withColumn(ColNames.year, F.lit(self.current_month_date_min.year))\n            .withColumn(ColNames.month, F.lit(self.current_month_date_min.month))\n            .withColumnRenamed(ColNames.iso2, ColNames.country_of_destination)\n        )\n\n        aggregations_df = apply_schema_casting(aggregations_df, SilverTourismOutboundNightsSpentDataObject.SCHEMA)\n        aggregations_df = aggregations_df.repartition(*SilverTourismOutboundNightsSpentDataObject.PARTITION_COLUMNS)\n        self.output_data_objects[SilverTourismOutboundNightsSpentDataObject.ID].df = aggregations_df\n\n    def mark_overnight_segments(self, stays_df):\n        \"\"\"Marks segments as overnight or not based on local time zone.\n\n        Args:\n            stays_df (pyspark.sql.DataFrame): DataFrame containing stay records\n\n        Returns:\n            pyspark.sql.DataFrame: DataFrame with `is_overnight` column added.\n        \"\"\"\n\n        # Calculate local start and end timestamps\n        stays_df = (\n            stays_df.withColumn(\n                \"local_start_timestamp\",\n                F.from_utc_timestamp(F.col(ColNames.start_timestamp), F.col(ColNames.timezone)),\n            )\n            .withColumn(\n                \"local_end_timestamp\",\n                F.from_utc_timestamp(F.col(ColNames.end_timestamp), F.col(ColNames.timezone)),\n            )\n            .withColumn(  # Midnight timestamp step 1: midnight hour of first date\n                \"first_midnight\",\n                F.make_timestamp(\n                    F.year(F.col(\"local_start_timestamp\")),\n                    F.month(F.col(\"local_start_timestamp\")),\n                    F.dayofmonth(F.col(\"local_start_timestamp\")),\n                    F.lit(self.functional_midnight_h),\n                    F.lit(0),\n                    F.lit(0),\n                ),\n            )\n            .withColumn(  # Midnight timestamp step 2: shift first midnight timestamp to the next day if it is before the start timestamp\n                \"first_midnight\",\n                F.when(\n                    F.col(\"first_midnight\") &lt; F.col(\"local_start_timestamp\"),\n                    F.col(\"first_midnight\") + timedelta(days=1),\n                ).otherwise(F.col(\"first_midnight\")),\n            )\n        )\n\n        # Mark segments that contain the functional midnight hour and are sufficiently long as overnight segments.\n        stays_df = stays_df.withColumn(\n            ColNames.is_overnight,\n            F.when(\n                (\n                    (F.col(\"local_start_timestamp\") &lt; F.col(\"first_midnight\"))\n                    &amp; (F.col(\"local_end_timestamp\") &gt;= F.col(\"first_midnight\"))\n                    &amp;\n                    # Duration check\n                    (\n                        (F.col(\"local_end_timestamp\").cast(\"long\") - F.col(\"local_start_timestamp\").cast(\"long\")) / 60.0\n                        &gt;= self.min_duration_segment_night_m\n                    )\n                ),\n                F.lit(True),\n            ).otherwise(F.lit(False)),\n        ).drop(\"local_start_timestamp\", \"local_end_timestamp\", \"first_midnight\")\n\n        return stays_df\n\n    def calculate_trips(self, stays_df):\n        \"\"\"Calculates trip information for segments.\n\n        This function processes stay records to identify and mark distinct trips based on temporal gaps\n        between consecutive stays for each user. It assigns trip IDs, determines trip start timestamps,\n        and marks whether trips are finished in the current month.\n\n        Args:\n            stays_df (pyspark.sql.DataFrame): DataFrame containing stay records\n\n        Returns:\n            pyspark.sql.DataFrame: DataFrame with trip information added.\n\n        Notes:\n            - A new trip is created when the time gap between consecutive stays exceeds max_trip_gap_h\n            - Existing trip_ids are preserved if present in the input DataFrame\n            - Trip completion is determined based on the current_month_date_min\n        \"\"\"\n\n        # Define window for sorting stays within each user\n        user_window = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id).orderBy(ColNames.start_timestamp)\n\n        # Identify new trips based on the maximum allowed time gap between stays\n        # and if there is no previous ongoing trip\n        stays_df = stays_df.withColumn(\n            \"prev_end_timestamp\", F.lag(ColNames.end_timestamp).over(user_window)\n        ).withColumn(\n            \"is_new_trip\",\n            F.when((F.col(\"prev_end_timestamp\").isNull()) &amp; (F.col(ColNames.trip_id).isNull()), F.lit(True))\n            .when((F.col(ColNames.trip_id).isNotNull()), F.lit(False))\n            .otherwise(\n                (\n                    (F.col(ColNames.start_timestamp).cast(LongType()) - F.col(\"prev_end_timestamp\").cast(LongType()))\n                    / 3600\n                    &gt; self.max_trip_gap_h\n                )\n            ),\n        )\n\n        # assign start timestamp of the trip\n        stays_df = stays_df.withColumn(\n            ColNames.trip_start_timestamp,\n            F.when(\n                F.col(\"is_new_trip\"),\n                F.col(ColNames.start_timestamp),\n            ).otherwise(F.col(ColNames.trip_start_timestamp)),\n        ).withColumn(  # forward fill trip start timestamp\n            ColNames.trip_start_timestamp,\n            F.last(ColNames.trip_start_timestamp, ignorenulls=True).over(user_window),\n        )\n        # Generate trip ID as a hash of user ID and trip start time, but reuse existing trip IDs if available\n        stays_df = stays_df.withColumn(\n            ColNames.trip_id,\n            F.when(\n                F.col(ColNames.trip_id).isNull(),  # Generate new trip_id only if it does not already exist\n                F.md5(\n                    F.concat(\n                        F.col(ColNames.user_id).cast(StringType()),\n                        F.col(ColNames.trip_start_timestamp).cast(StringType()),\n                    )\n                ),\n            ).otherwise(F.col(ColNames.trip_id)),\n        ).drop(\"prev_end_timestamp\")\n\n        # Determine if a trip is finished based on the maximum `end_timestamp` in each trip\n        trip_window = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id, ColNames.trip_id)\n        stays_df = stays_df.withColumn(\n            \"is_trip_finished\",\n            F.when(\n                F.max(ColNames.end_timestamp).over(trip_window).cast(\"date\") &lt;= F.lit(self.current_month_date_max),\n                F.lit(True),\n            ).otherwise(F.lit(False)),\n        )\n\n        return stays_df\n\n    def calculate_visits(self, stays_df):\n        \"\"\"Calculates visit IDs for stays data and determines if visits are finished in the current month.\n\n        A new visit is created when:\n        1. It's the first record in the trip\n        2. There's a change in the country visited\n\n        Args:\n            stays_df (pyspark.sql.DataFrame): DataFrame containing stay records\n\n        Returns:\n            pyspark.sql.DataFrame: DataFrame with visit information added.\n\n        \"\"\"\n\n        # Define window for trip level sorting\n        trip_window = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id, ColNames.trip_id).orderBy(\n            ColNames.start_timestamp\n        )\n\n        stays_df = stays_df.withColumn(\n            \"prev_end_timestamp\", F.lag(ColNames.end_timestamp).over(trip_window)\n        ).withColumn(\"prev_iso2\", F.lag(ColNames.iso2).over(trip_window))\n\n        # Flag new visits based on country change\n        stays_df = stays_df.withColumn(\n            ColNames.visit_id,\n            F.sum(\n                F.when(\n                    (F.col(\"prev_end_timestamp\").isNull())  # First record in the trip\n                    | (F.col(ColNames.iso2) != F.col(\"prev_iso2\")),  # New country visited\n                    F.lit(1),\n                ).otherwise(F.lit(0))\n            ).over(trip_window),\n        ).drop(\"prev_end_timestamp\", \"prev_iso2\")\n\n        # Define a window for visit level sorting\n        visit_window = Window.partitionBy(\n            ColNames.user_id_modulo, ColNames.user_id, ColNames.trip_id, ColNames.visit_id\n        )\n\n        # Determine if a visit is finished in the current month\n        stays_df = stays_df.withColumn(\n            \"is_visit_finished\",\n            F.when(\n                F.month(F.max(ColNames.end_timestamp).over(visit_window).cast(\"date\"))\n                == F.lit(self.current_month_date_max.month),\n                F.lit(True),\n            ).otherwise(F.lit(False)),\n        )\n\n        return stays_df\n\n    @staticmethod\n    def get_period_month_bounds_list(\n        data_period_start: datetime, data_period_end: datetime\n    ) -&gt; list[tuple[datetime.date, datetime.date]]:\n        \"\"\"Generate the first and last date of each month that is within the calculation period.\"\"\"\n        return_list = []\n        current_month_start = data_period_start\n        while current_month_start &lt;= data_period_end:\n            current_month_end = current_month_start + relativedelta(months=1) - timedelta(seconds=1)\n            return_list.append((current_month_start.date(), current_month_end.date()))\n            current_month_start += relativedelta(months=1)\n        return return_list\n</code></pre>"},{"location":"reference/components/execution/tourism_outbound_statistics/tourism_outbound_statistics_calculation/#components.execution.tourism_outbound_statistics.tourism_outbound_statistics_calculation.TourismOutboundStatisticsCalculation.calculate_trips","title":"<code>calculate_trips(stays_df)</code>","text":"<p>Calculates trip information for segments.</p> <p>This function processes stay records to identify and mark distinct trips based on temporal gaps between consecutive stays for each user. It assigns trip IDs, determines trip start timestamps, and marks whether trips are finished in the current month.</p> <p>Parameters:</p> Name Type Description Default <code>stays_df</code> <code>DataFrame</code> <p>DataFrame containing stay records</p> required <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: DataFrame with trip information added.</p> Notes <ul> <li>A new trip is created when the time gap between consecutive stays exceeds max_trip_gap_h</li> <li>Existing trip_ids are preserved if present in the input DataFrame</li> <li>Trip completion is determined based on the current_month_date_min</li> </ul> Source code in <code>multimno/components/execution/tourism_outbound_statistics/tourism_outbound_statistics_calculation.py</code> <pre><code>def calculate_trips(self, stays_df):\n    \"\"\"Calculates trip information for segments.\n\n    This function processes stay records to identify and mark distinct trips based on temporal gaps\n    between consecutive stays for each user. It assigns trip IDs, determines trip start timestamps,\n    and marks whether trips are finished in the current month.\n\n    Args:\n        stays_df (pyspark.sql.DataFrame): DataFrame containing stay records\n\n    Returns:\n        pyspark.sql.DataFrame: DataFrame with trip information added.\n\n    Notes:\n        - A new trip is created when the time gap between consecutive stays exceeds max_trip_gap_h\n        - Existing trip_ids are preserved if present in the input DataFrame\n        - Trip completion is determined based on the current_month_date_min\n    \"\"\"\n\n    # Define window for sorting stays within each user\n    user_window = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id).orderBy(ColNames.start_timestamp)\n\n    # Identify new trips based on the maximum allowed time gap between stays\n    # and if there is no previous ongoing trip\n    stays_df = stays_df.withColumn(\n        \"prev_end_timestamp\", F.lag(ColNames.end_timestamp).over(user_window)\n    ).withColumn(\n        \"is_new_trip\",\n        F.when((F.col(\"prev_end_timestamp\").isNull()) &amp; (F.col(ColNames.trip_id).isNull()), F.lit(True))\n        .when((F.col(ColNames.trip_id).isNotNull()), F.lit(False))\n        .otherwise(\n            (\n                (F.col(ColNames.start_timestamp).cast(LongType()) - F.col(\"prev_end_timestamp\").cast(LongType()))\n                / 3600\n                &gt; self.max_trip_gap_h\n            )\n        ),\n    )\n\n    # assign start timestamp of the trip\n    stays_df = stays_df.withColumn(\n        ColNames.trip_start_timestamp,\n        F.when(\n            F.col(\"is_new_trip\"),\n            F.col(ColNames.start_timestamp),\n        ).otherwise(F.col(ColNames.trip_start_timestamp)),\n    ).withColumn(  # forward fill trip start timestamp\n        ColNames.trip_start_timestamp,\n        F.last(ColNames.trip_start_timestamp, ignorenulls=True).over(user_window),\n    )\n    # Generate trip ID as a hash of user ID and trip start time, but reuse existing trip IDs if available\n    stays_df = stays_df.withColumn(\n        ColNames.trip_id,\n        F.when(\n            F.col(ColNames.trip_id).isNull(),  # Generate new trip_id only if it does not already exist\n            F.md5(\n                F.concat(\n                    F.col(ColNames.user_id).cast(StringType()),\n                    F.col(ColNames.trip_start_timestamp).cast(StringType()),\n                )\n            ),\n        ).otherwise(F.col(ColNames.trip_id)),\n    ).drop(\"prev_end_timestamp\")\n\n    # Determine if a trip is finished based on the maximum `end_timestamp` in each trip\n    trip_window = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id, ColNames.trip_id)\n    stays_df = stays_df.withColumn(\n        \"is_trip_finished\",\n        F.when(\n            F.max(ColNames.end_timestamp).over(trip_window).cast(\"date\") &lt;= F.lit(self.current_month_date_max),\n            F.lit(True),\n        ).otherwise(F.lit(False)),\n    )\n\n    return stays_df\n</code></pre>"},{"location":"reference/components/execution/tourism_outbound_statistics/tourism_outbound_statistics_calculation/#components.execution.tourism_outbound_statistics.tourism_outbound_statistics_calculation.TourismOutboundStatisticsCalculation.calculate_visits","title":"<code>calculate_visits(stays_df)</code>","text":"<p>Calculates visit IDs for stays data and determines if visits are finished in the current month.</p> <p>A new visit is created when: 1. It's the first record in the trip 2. There's a change in the country visited</p> <p>Parameters:</p> Name Type Description Default <code>stays_df</code> <code>DataFrame</code> <p>DataFrame containing stay records</p> required <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: DataFrame with visit information added.</p> Source code in <code>multimno/components/execution/tourism_outbound_statistics/tourism_outbound_statistics_calculation.py</code> <pre><code>def calculate_visits(self, stays_df):\n    \"\"\"Calculates visit IDs for stays data and determines if visits are finished in the current month.\n\n    A new visit is created when:\n    1. It's the first record in the trip\n    2. There's a change in the country visited\n\n    Args:\n        stays_df (pyspark.sql.DataFrame): DataFrame containing stay records\n\n    Returns:\n        pyspark.sql.DataFrame: DataFrame with visit information added.\n\n    \"\"\"\n\n    # Define window for trip level sorting\n    trip_window = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id, ColNames.trip_id).orderBy(\n        ColNames.start_timestamp\n    )\n\n    stays_df = stays_df.withColumn(\n        \"prev_end_timestamp\", F.lag(ColNames.end_timestamp).over(trip_window)\n    ).withColumn(\"prev_iso2\", F.lag(ColNames.iso2).over(trip_window))\n\n    # Flag new visits based on country change\n    stays_df = stays_df.withColumn(\n        ColNames.visit_id,\n        F.sum(\n            F.when(\n                (F.col(\"prev_end_timestamp\").isNull())  # First record in the trip\n                | (F.col(ColNames.iso2) != F.col(\"prev_iso2\")),  # New country visited\n                F.lit(1),\n            ).otherwise(F.lit(0))\n        ).over(trip_window),\n    ).drop(\"prev_end_timestamp\", \"prev_iso2\")\n\n    # Define a window for visit level sorting\n    visit_window = Window.partitionBy(\n        ColNames.user_id_modulo, ColNames.user_id, ColNames.trip_id, ColNames.visit_id\n    )\n\n    # Determine if a visit is finished in the current month\n    stays_df = stays_df.withColumn(\n        \"is_visit_finished\",\n        F.when(\n            F.month(F.max(ColNames.end_timestamp).over(visit_window).cast(\"date\"))\n            == F.lit(self.current_month_date_max.month),\n            F.lit(True),\n        ).otherwise(F.lit(False)),\n    )\n\n    return stays_df\n</code></pre>"},{"location":"reference/components/execution/tourism_outbound_statistics/tourism_outbound_statistics_calculation/#components.execution.tourism_outbound_statistics.tourism_outbound_statistics_calculation.TourismOutboundStatisticsCalculation.get_period_month_bounds_list","title":"<code>get_period_month_bounds_list(data_period_start, data_period_end)</code>  <code>staticmethod</code>","text":"<p>Generate the first and last date of each month that is within the calculation period.</p> Source code in <code>multimno/components/execution/tourism_outbound_statistics/tourism_outbound_statistics_calculation.py</code> <pre><code>@staticmethod\ndef get_period_month_bounds_list(\n    data_period_start: datetime, data_period_end: datetime\n) -&gt; list[tuple[datetime.date, datetime.date]]:\n    \"\"\"Generate the first and last date of each month that is within the calculation period.\"\"\"\n    return_list = []\n    current_month_start = data_period_start\n    while current_month_start &lt;= data_period_end:\n        current_month_end = current_month_start + relativedelta(months=1) - timedelta(seconds=1)\n        return_list.append((current_month_start.date(), current_month_end.date()))\n        current_month_start += relativedelta(months=1)\n    return return_list\n</code></pre>"},{"location":"reference/components/execution/tourism_outbound_statistics/tourism_outbound_statistics_calculation/#components.execution.tourism_outbound_statistics.tourism_outbound_statistics_calculation.TourismOutboundStatisticsCalculation.mark_overnight_segments","title":"<code>mark_overnight_segments(stays_df)</code>","text":"<p>Marks segments as overnight or not based on local time zone.</p> <p>Parameters:</p> Name Type Description Default <code>stays_df</code> <code>DataFrame</code> <p>DataFrame containing stay records</p> required <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: DataFrame with <code>is_overnight</code> column added.</p> Source code in <code>multimno/components/execution/tourism_outbound_statistics/tourism_outbound_statistics_calculation.py</code> <pre><code>def mark_overnight_segments(self, stays_df):\n    \"\"\"Marks segments as overnight or not based on local time zone.\n\n    Args:\n        stays_df (pyspark.sql.DataFrame): DataFrame containing stay records\n\n    Returns:\n        pyspark.sql.DataFrame: DataFrame with `is_overnight` column added.\n    \"\"\"\n\n    # Calculate local start and end timestamps\n    stays_df = (\n        stays_df.withColumn(\n            \"local_start_timestamp\",\n            F.from_utc_timestamp(F.col(ColNames.start_timestamp), F.col(ColNames.timezone)),\n        )\n        .withColumn(\n            \"local_end_timestamp\",\n            F.from_utc_timestamp(F.col(ColNames.end_timestamp), F.col(ColNames.timezone)),\n        )\n        .withColumn(  # Midnight timestamp step 1: midnight hour of first date\n            \"first_midnight\",\n            F.make_timestamp(\n                F.year(F.col(\"local_start_timestamp\")),\n                F.month(F.col(\"local_start_timestamp\")),\n                F.dayofmonth(F.col(\"local_start_timestamp\")),\n                F.lit(self.functional_midnight_h),\n                F.lit(0),\n                F.lit(0),\n            ),\n        )\n        .withColumn(  # Midnight timestamp step 2: shift first midnight timestamp to the next day if it is before the start timestamp\n            \"first_midnight\",\n            F.when(\n                F.col(\"first_midnight\") &lt; F.col(\"local_start_timestamp\"),\n                F.col(\"first_midnight\") + timedelta(days=1),\n            ).otherwise(F.col(\"first_midnight\")),\n        )\n    )\n\n    # Mark segments that contain the functional midnight hour and are sufficiently long as overnight segments.\n    stays_df = stays_df.withColumn(\n        ColNames.is_overnight,\n        F.when(\n            (\n                (F.col(\"local_start_timestamp\") &lt; F.col(\"first_midnight\"))\n                &amp; (F.col(\"local_end_timestamp\") &gt;= F.col(\"first_midnight\"))\n                &amp;\n                # Duration check\n                (\n                    (F.col(\"local_end_timestamp\").cast(\"long\") - F.col(\"local_start_timestamp\").cast(\"long\")) / 60.0\n                    &gt;= self.min_duration_segment_night_m\n                )\n            ),\n            F.lit(True),\n        ).otherwise(F.lit(False)),\n    ).drop(\"local_start_timestamp\", \"local_end_timestamp\", \"first_midnight\")\n\n    return stays_df\n</code></pre>"},{"location":"reference/components/execution/tourism_statistics/","title":"tourism_statistics","text":""},{"location":"reference/components/execution/tourism_statistics/tourism_statistics_calculation/","title":"tourism_statistics_calculation","text":"<p>Module that implements the Tourism Stays Estimation functionality.</p>"},{"location":"reference/components/execution/tourism_statistics/tourism_statistics_calculation/#components.execution.tourism_statistics.tourism_statistics_calculation.TourismStatisticsCalculation","title":"<code>TourismStatisticsCalculation</code>","text":"<p>               Bases: <code>Component</code></p> <p>A class to calculate tourism statistics of geozones per time period.</p> Source code in <code>multimno/components/execution/tourism_statistics/tourism_statistics_calculation.py</code> <pre><code>class TourismStatisticsCalculation(Component):\n    \"\"\"\n    A class to calculate tourism statistics of geozones per time period.\n    \"\"\"\n\n    COMPONENT_ID = \"TourismStatisticsCalculation\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.data_period_start = datetime.strptime(self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m\")\n        self.data_period_end = datetime.strptime(self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m\")\n        # Generate (month_first_date, month_last_date) pairs\n        self.data_period_bounds_list = self.get_period_month_bounds_list(self.data_period_start, self.data_period_end)\n\n        self.zoning_dataset_ids_and_levels_list = self.config.geteval(\n            self.COMPONENT_ID, \"zoning_dataset_ids_and_levels_list\"\n        )\n        self.max_trip_gap_h = self.config.getint(self.COMPONENT_ID, \"max_trip_gap_h\")\n        self.max_visit_gap_h = self.config.getint(self.COMPONENT_ID, \"max_visit_gap_h\")\n\n    def initalize_data_objects(self):\n        # Handle optional deletion of existing trips\n        self.output_tourism_trip_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"tourism_trips_silver\")\n        self.delete_existing_trips = self.config.getboolean(self.COMPONENT_ID, \"delete_existing_trips\")\n        if self.delete_existing_trips:\n            delete_file_or_folder(self.spark, self.output_tourism_trip_path)\n\n        # Input\n        self.input_data_objects = {}\n\n        inputs = {\n            \"tourism_stays_silver\": SilverTourismStaysDataObject,\n            \"mcc_iso_timezones_data_bronze\": BronzeMccIsoTzMap,\n            # \"tourism_trips_silver\": SilverTourismTripDataObject\n        }\n\n        for key, value in inputs.items():\n            if self.config.has_option(CONFIG_BRONZE_PATHS_KEY, key):\n                path = self.config.get(CONFIG_BRONZE_PATHS_KEY, key)\n            else:\n                path = self.config.get(CONFIG_SILVER_PATHS_KEY, key)\n            if check_if_data_path_exists(self.spark, path):\n                self.input_data_objects[value.ID] = value(self.spark, path)\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")\n                raise ValueError(f\"Invalid path for {value.ID}: {path}\")\n\n        # Separate handling for tourism trips: data existence is optional.\n        # If data does not exist, create empty dataframe with proper schema.\n        # Also write empty dataframe to path (workaround for read() having no target path).\n        path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"tourism_trips_silver\")\n        self.input_data_objects[SilverTourismTripDataObject.ID] = SilverTourismTripDataObject(self.spark, path)\n        if not check_if_data_path_exists(self.spark, path):\n            self.input_data_objects[SilverTourismTripDataObject.ID].df = self.spark.createDataFrame(\n                [], schema=SilverTourismTripDataObject.SCHEMA\n            )\n            self.input_data_objects[SilverTourismTripDataObject.ID].write()\n\n        # Output\n        self.output_data_objects = {}\n\n        self.output_data_objects[SilverTourismTripDataObject.ID] = SilverTourismTripDataObject(\n            self.spark,\n            self.output_tourism_trip_path,\n        )\n\n        self.output_tourism_geozone_aggregations_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"tourism_geozone_aggregations_silver\"\n        )\n        self.output_data_objects[SilverTourismZoneDeparturesNightsSpentDataObject.ID] = (\n            SilverTourismZoneDeparturesNightsSpentDataObject(\n                self.spark,\n                self.output_tourism_geozone_aggregations_path,\n            )\n        )\n\n        self.output_tourism_trip_aggregations_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"tourism_trip_aggregations_silver\"\n        )\n        self.output_data_objects[SilverTourismTripAvgDestinationsNightsSpentDataObject.ID] = (\n            SilverTourismTripAvgDestinationsNightsSpentDataObject(\n                self.spark,\n                self.output_tourism_trip_aggregations_path,\n            )\n        )\n\n        # Output clearing\n        clear_destination_directory = self.config.getboolean(self.COMPONENT_ID, \"clear_destination_directory\")\n        if clear_destination_directory:\n            delete_file_or_folder(self.spark, self.output_tourism_geozone_aggregations_path)\n            delete_file_or_folder(self.spark, self.output_tourism_trip_aggregations_path)\n\n    def write(self, data_object_id: str):\n        self.logger.info(f\"Writing {data_object_id} output...\")\n        self.output_data_objects[data_object_id].write()\n\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n\n        # for every month within the data period, process stays and calculate aggregations.\n        for dataset_id_and_hierarchical_levels_pair in self.zoning_dataset_ids_and_levels_list:\n            self.current_zoning_dataset_id = dataset_id_and_hierarchical_levels_pair[0]\n            self.hierarchical_levels_to_calculate = dataset_id_and_hierarchical_levels_pair[1]\n            for current_month_date_min, current_month_date_max in self.data_period_bounds_list:\n                self.read()\n\n                # Handle mcc to iso timezone mapping data.\n                self.mcc_iso_tz_map_df = self.input_data_objects[BronzeMccIsoTzMap.ID].df.select(\n                    F.col(ColNames.mcc).alias(\"mcc_map\"), ColNames.iso2\n                )\n\n                self.current_month_date_min, self.current_month_date_max = (\n                    current_month_date_min,\n                    current_month_date_max,\n                )\n                self.current_month_time_period_string = (\n                    f\"{current_month_date_min.year}-{current_month_date_min.month:0&gt;2}\"\n                )\n                # Increase date_max to include stays from the next month within max_trip_gap_h\n                input_date_max = current_month_date_max + timedelta(days=ceil(self.max_trip_gap_h / 24.0))\n                input_timestamp_max = input_date_max + timedelta(hours=self.max_trip_gap_h)\n                self.logger.info(\n                    f\"Processing stays in date range {current_month_date_min.strftime('%Y-%m-%d')} - {current_month_date_max.strftime('%Y-%m-%d')}, look-forward timestamp max: {input_timestamp_max}\"\n                )\n\n                # Select stays of the current month plus those within the look-forward window\n                current_month_stays_df = (\n                    self.input_data_objects[SilverTourismStaysDataObject.ID]\n                    .df.filter(\n                        (\n                            F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n                            &gt;= F.lit(current_month_date_min)\n                        )\n                        &amp; (\n                            F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n                            &lt;= F.lit(input_date_max)\n                        )\n                        &amp; (F.col(ColNames.start_timestamp) &lt;= input_timestamp_max)\n                        &amp; (F.col(ColNames.dataset_id) == self.current_zoning_dataset_id)\n                    )\n                    .select(\n                        F.col(ColNames.user_id),\n                        F.col(ColNames.time_segment_id),\n                        F.lit(None).alias(ColNames.trip_id),\n                        F.lit(None).alias(ColNames.trip_start_timestamp),\n                        F.col(ColNames.zone_ids_list),\n                        F.col(ColNames.zone_weights_list),\n                        F.col(ColNames.start_timestamp),\n                        F.col(ColNames.end_timestamp),\n                        F.col(ColNames.is_overnight),\n                        F.col(ColNames.user_id_modulo),\n                        F.col(ColNames.mcc),\n                        F.col(ColNames.year),\n                        F.col(ColNames.month),\n                    )\n                )\n\n                # Get previous ongoing trips. Select trips that were unfinished in the previous month.\n                ongoing_trips_df = self.input_data_objects[SilverTourismTripDataObject.ID].df.filter(\n                    (F.col(ColNames.is_trip_finished) == False)\n                    &amp; (F.col(ColNames.year) == (current_month_date_min - relativedelta(months=1)).year)\n                    &amp; (F.col(ColNames.month) == (current_month_date_min - relativedelta(months=1)).month)\n                    &amp; (F.col(ColNames.dataset_id) == self.current_zoning_dataset_id)\n                )\n                if ongoing_trips_df.count() == 0:\n                    self.relevant_stays_df = current_month_stays_df\n                    self.logger.info(\"No ongoing trips from the previous month.\")\n                else:\n                    # get earliest start timestamp of the trip\n                    earliest_start_timestamp = ongoing_trips_df.select(F.min(ColNames.trip_start_timestamp)).collect()[\n                        0\n                    ][0]\n\n                    # get all stays starting from earliest date of ongoing trips\n                    last_month_stays_df = (\n                        self.input_data_objects[SilverTourismStaysDataObject.ID]\n                        .df.filter(\n                            (F.col(ColNames.start_timestamp) &gt;= earliest_start_timestamp)\n                            &amp; (F.col(ColNames.end_timestamp) &lt; current_month_date_min)\n                        )\n                        .select(\n                            F.col(ColNames.user_id),\n                            F.col(ColNames.time_segment_id),\n                            F.col(ColNames.zone_ids_list),\n                            F.col(ColNames.zone_weights_list),\n                            F.col(ColNames.start_timestamp),\n                            F.col(ColNames.end_timestamp),\n                            F.col(ColNames.is_overnight),\n                            F.col(ColNames.user_id_modulo),\n                            F.col(ColNames.mcc),\n                            F.col(ColNames.year),\n                            F.col(ColNames.month),\n                        )\n                    )\n\n                    ongoing_trips_stays_df = ongoing_trips_df.withColumn(\n                        ColNames.time_segment_id, F.explode(ColNames.time_segment_ids_list)\n                    ).select(\n                        ColNames.user_id,\n                        ColNames.trip_id,\n                        ColNames.trip_start_timestamp,\n                        ColNames.time_segment_id,\n                        ColNames.user_id_modulo,\n                    )\n\n                    ongoing_trips_stays_df = ongoing_trips_stays_df.join(\n                        last_month_stays_df,\n                        [ColNames.user_id, ColNames.time_segment_id, ColNames.user_id_modulo],\n                        \"inner\",\n                    )\n                    self.relevant_stays_df = current_month_stays_df.unionByName(ongoing_trips_stays_df)\n\n                self.transform()\n                self.spark.catalog.clearCache()\n\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n\n        relevant_stays_df = self.relevant_stays_df\n        mcc_iso_tz_map_df = self.mcc_iso_tz_map_df\n\n        # Calculate trips\n        relevant_stays_df = self.calculate_trips(relevant_stays_df)\n        relevant_stays_df = relevant_stays_df.cache()\n\n        # group stays to trips collect list of segment ids.\n        trips_df = relevant_stays_df.groupBy(ColNames.user_id, ColNames.trip_id).agg(\n            F.first(ColNames.is_trip_finished).alias(ColNames.is_trip_finished),\n            F.first(ColNames.start_timestamp).alias(ColNames.trip_start_timestamp),\n            F.collect_list(F.col(ColNames.time_segment_id)).alias(\n                ColNames.time_segment_ids_list\n            ),  # NOTE: order is not deterministic\n            F.lit(self.current_month_date_min.year).alias(ColNames.year),\n            F.lit(self.current_month_date_min.month).alias(ColNames.month),\n            F.min(ColNames.user_id_modulo).alias(ColNames.user_id_modulo),\n            F.lit(self.current_zoning_dataset_id).alias(ColNames.dataset_id),\n        )\n\n        trips_df = apply_schema_casting(trips_df, SilverTourismTripDataObject.SCHEMA)\n        trips_df = trips_df.repartition(*SilverTourismTripDataObject.PARTITION_COLUMNS)\n        # NOTE: This will be written as output at the end. It is not written right away to avoid doubling rows in the stays dataframe due to cache() behaviour.\n\n        # Join stays with mcc_iso_tz_map to get iso2 for each mcc\n        relevant_stays_df = relevant_stays_df.join(\n            mcc_iso_tz_map_df,\n            F.col(ColNames.mcc) == F.col(\"mcc_map\"),\n            \"left\",\n        ).drop(\"mcc_map\")\n\n        relevant_stays_df = relevant_stays_df.withColumn(\n            ColNames.country_of_origin, F.coalesce(F.col(ColNames.iso2), F.lit(\"XX\"))\n        ).drop(\"iso2\", ColNames.mcc)\n\n        # Explode zone arrays and weights\n        relevant_stays_df = (\n            relevant_stays_df.withColumn(\n                \"zipped_arrays\",\n                F.explode(F.arrays_zip(F.col(ColNames.zone_ids_list), F.col(ColNames.zone_weights_list))),\n            )\n            .withColumn(ColNames.hierarchical_id, F.col(f\"zipped_arrays.{ColNames.zone_ids_list}\"))\n            .withColumn(ColNames.zone_weight, F.col(f\"zipped_arrays.{ColNames.zone_weights_list}\"))\n            .drop(\"zipped_arrays\", ColNames.zone_ids_list, ColNames.zone_weights_list)\n        )\n\n        for hierarchical_level in self.hierarchical_levels_to_calculate:\n            self.logger.info(f\"Calculating aggregations for hierarchical level {hierarchical_level}...\")\n\n            # get current level zone ids\n            hierarchical_level_stays_df = relevant_stays_df.withColumn(\n                ColNames.zone_id,\n                F.element_at(F.split(F.col(ColNames.hierarchical_id), pattern=\"\\\\|\"), hierarchical_level),\n            )\n\n            # calculate visits to current level zones\n            hierarchical_level_stays_df = self.calculate_visits(hierarchical_level_stays_df)\n\n            # Perform aggregations I (nights spent and departures from geographical unit)\n            # I.1 Calculate nights spent per zone, applying zone weights per stay.\n            # I.2 Calculate departures per zone for overnight and non-overnight visits\n            # TODO: Potentially calculate departures based on weights of zones visited\n            aggregations_i_df = hierarchical_level_stays_df.groupBy(\n                ColNames.country_of_origin, ColNames.is_overnight, ColNames.zone_id\n            ).agg(\n                # Sum of zone_weights for finished visits in the current month\n                F.sum(\n                    F.when(\n                        (F.col(\"is_visit_finished\")) &amp; (F.col(ColNames.is_overnight)), F.col(ColNames.zone_weight)\n                    ).otherwise(0)\n                ).alias(ColNames.nights_spent),\n                # Count of unique zone_ids for finished trips\n                F.countDistinct(F.when(F.col(ColNames.is_trip_finished), F.col(ColNames.trip_id))).alias(\n                    ColNames.num_of_departures\n                ),\n            )\n\n            aggregations_i_df = (\n                aggregations_i_df.withColumn(ColNames.time_period, F.lit(self.current_month_time_period_string))\n                .withColumn(ColNames.level, F.lit(hierarchical_level))\n                .withColumn(ColNames.year, F.lit(self.current_month_date_min.year))\n                .withColumn(ColNames.month, F.lit(self.current_month_date_min.month))\n                .withColumn(ColNames.dataset_id, F.lit(self.current_zoning_dataset_id))\n            )\n\n            aggregations_i_df = apply_schema_casting(\n                aggregations_i_df, SilverTourismZoneDeparturesNightsSpentDataObject.SCHEMA\n            )\n\n            aggregations_i_df = aggregations_i_df.repartition(\n                *SilverTourismZoneDeparturesNightsSpentDataObject.PARTITION_COLUMNS\n            )\n\n            # Write output. Append to existing if previous hierarchical level exists.\n            self.output_data_objects[SilverTourismZoneDeparturesNightsSpentDataObject.ID].df = aggregations_i_df\n\n            # Perform aggregations II\n            # These apply only to trips completed in the current month.\n            current_month_relevant_stays_df = hierarchical_level_stays_df.where(F.col(ColNames.is_trip_finished))\n            # II.1 Average number of destinations per trip completed this month.\n            avg_destination_cnt_df = self.calculate_avg_destination_cnt(current_month_relevant_stays_df)\n\n            # II.2 Average number of nights spent per destination.\n            avg_number_of_nights_spent_per_destination_df = self.calculate_avg_nights_spent_per_destination(\n                current_month_relevant_stays_df\n            )\n\n            # Combine average destination count per trip and average number of nights spent per destination.\n            aggregations_ii_df = (\n                avg_destination_cnt_df.alias(\"df1\")\n                .join(\n                    avg_number_of_nights_spent_per_destination_df.alias(\"df2\"),\n                    on=F.col(f\"df1.{ColNames.country_of_origin}\") == F.col(f\"df2.{ColNames.country_of_origin}\"),\n                )\n                .select(\n                    F.lit(self.current_month_time_period_string).alias(ColNames.time_period),\n                    F.col(f\"df1.{ColNames.country_of_origin}\"),\n                    F.col(ColNames.avg_destinations),\n                    F.col(ColNames.avg_nights_spent_per_destination),\n                    F.lit(self.current_month_date_min.year).alias(ColNames.year),\n                    F.lit(self.current_month_date_min.month).alias(ColNames.month),\n                    F.lit(hierarchical_level).alias(ColNames.level),\n                    F.lit(self.current_zoning_dataset_id).alias(ColNames.dataset_id),\n                )\n            )\n\n            aggregations_ii_df = apply_schema_casting(\n                aggregations_ii_df, SilverTourismTripAvgDestinationsNightsSpentDataObject.SCHEMA\n            )\n\n            aggregations_ii_df = aggregations_ii_df.repartition(\n                *SilverTourismTripAvgDestinationsNightsSpentDataObject.PARTITION_COLUMNS\n            )\n\n            # Write output. Append to existing if previous hierarchical level exists.\n            self.output_data_objects[SilverTourismTripAvgDestinationsNightsSpentDataObject.ID].df = aggregations_ii_df\n\n            self.write(SilverTourismZoneDeparturesNightsSpentDataObject.ID)\n            self.write(SilverTourismTripAvgDestinationsNightsSpentDataObject.ID)\n\n        # Write current month trips to output\n        # Careful when you execute this, since it messes with cached stays.\n        self.output_data_objects[SilverTourismTripDataObject.ID].df = trips_df\n        self.write(SilverTourismTripDataObject.ID)\n\n    def calculate_trips(self, stays_df):\n        \"\"\"Calculates trip information for stays data using PySpark DataFrame operations.\n\n        This function processes stay records to identify and mark distinct trips based on temporal gaps\n        between consecutive stays for each user. It assigns trip IDs, determines trip start timestamps,\n        and marks whether trips are finished in the current month.\n\n        Args:\n            stays_df (pyspark.sql.DataFrame): DataFrame containing stay records\n\n        Returns:\n            pyspark.sql.DataFrame: DataFrame with trip information added.\n\n        Notes:\n            - A new trip is created when the time gap between consecutive stays exceeds max_trip_gap_h\n            - Existing trip_ids are preserved if present in the input DataFrame\n            - Trip completion is determined based on the current_month_date_min\n        \"\"\"\n\n        # Define window for sorting stays within each user\n        user_window = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id).orderBy(ColNames.start_timestamp)\n\n        # Identify new trips based on the maximum allowed time gap between stays\n        # and if there is no previous ongoing trip\n        stays_df = stays_df.withColumn(\n            \"prev_end_timestamp\", F.lag(ColNames.end_timestamp).over(user_window)\n        ).withColumn(\n            \"is_new_trip\",\n            F.when((F.col(\"prev_end_timestamp\").isNull()) &amp; (F.col(ColNames.trip_id).isNull()), F.lit(True))\n            .when((F.col(ColNames.trip_id).isNotNull()), F.lit(False))\n            .otherwise(\n                (\n                    (F.col(ColNames.start_timestamp).cast(LongType()) - F.col(\"prev_end_timestamp\").cast(LongType()))\n                    / 3600\n                    &gt; self.max_trip_gap_h\n                )\n            ),\n        )\n\n        # assign start timestamp of the trip\n        stays_df = stays_df.withColumn(\n            ColNames.trip_start_timestamp,\n            F.when(\n                F.col(\"is_new_trip\"),\n                F.col(ColNames.start_timestamp),\n            ).otherwise(F.col(ColNames.trip_start_timestamp)),\n        ).withColumn(  # forward fill trip start timestamp\n            ColNames.trip_start_timestamp,\n            F.last(ColNames.trip_start_timestamp, ignorenulls=True).over(user_window),\n        )\n\n        # Generate trip ID as a hash of user ID and trip start time, but reuse existing trip IDs if available\n        stays_df = stays_df.withColumn(\n            ColNames.trip_id,\n            F.when(\n                F.col(ColNames.trip_id).isNull(),  # Generate new trip_id only if it does not already exist\n                F.md5(\n                    F.concat(\n                        F.col(ColNames.user_id).cast(StringType()),\n                        F.col(ColNames.trip_start_timestamp).cast(StringType()),\n                    )\n                ),\n            ).otherwise(F.col(ColNames.trip_id)),\n        ).drop(\"prev_end_timestamp\")\n\n        # Determine if a trip is finished based on the maximum `end_timestamp` in each trip\n        trip_window = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id, ColNames.trip_id)\n        stays_df = stays_df.withColumn(\n            \"is_trip_finished\",\n            F.when(\n                F.max(ColNames.end_timestamp).over(trip_window).cast(\"date\") &lt;= F.lit(self.current_month_date_max),\n                F.lit(True),\n            ).otherwise(F.lit(False)),\n        )\n\n        return stays_df\n\n    def calculate_visits(self, stays_df):\n        \"\"\"Calculates visit IDs for stays data and determines if visits are finished in the current month.\n\n        A new visit is created when:\n        1. It's the first record in the trip\n        2. There's a change in the visited zone\n        3. Time gap between consecutive stays exceeds max_visit_gap_h\n\n        Args:\n            stays_df (pyspark.sql.DataFrame): DataFrame containing stay records\n\n        Returns:\n            pyspark.sql.DataFrame: DataFrame with visit information added.\n\n        Notes:\n            The function uses window functions to partition data by user and trip for proper\n            visit calculation and utilizes self.max_visit_gap_h and self.current_month_date_min\n            class attributes for gap threshold and current month validation respectively.\n        \"\"\"\n\n        # Define window for trip level sorting\n        # Sorting by zone_id first allows aggregating by zone regardless of stay sequentiality\n        trip_window = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id, ColNames.trip_id).orderBy(\n            ColNames.zone_id, ColNames.start_timestamp\n        )\n\n        stays_df = stays_df.withColumn(\n            \"prev_end_timestamp\", F.lag(ColNames.end_timestamp).over(trip_window)\n        ).withColumn(\"prev_zone_id\", F.lag(ColNames.zone_id).over(trip_window))\n\n        # Flag new visits based on time gap or zone change\n        stays_df = stays_df.withColumn(\n            ColNames.visit_id,\n            F.sum(\n                F.when(\n                    (F.col(\"prev_end_timestamp\").isNull())  # First record in the trip\n                    | (F.col(ColNames.zone_id) != F.col(\"prev_zone_id\"))  # New zone visited\n                    | (\n                        (\n                            F.col(ColNames.start_timestamp).cast(LongType())\n                            - F.col(\"prev_end_timestamp\").cast(LongType())\n                        )\n                        / 3600\n                        &gt; self.max_visit_gap_h  # Time gap exceeds threshold\n                    ),\n                    F.lit(1),\n                ).otherwise(F.lit(0))\n            ).over(trip_window),\n        ).drop(\"prev_end_timestamp\", \"prev_zone_id\")\n\n        # Define a window for visit level sorting\n        visit_window = Window.partitionBy(\n            ColNames.user_id_modulo, ColNames.user_id, ColNames.trip_id, ColNames.visit_id\n        )\n\n        # Determine if a visit is finished in the current month\n        stays_df = stays_df.withColumn(\n            \"is_visit_finished\",\n            F.when(\n                F.month(F.max(ColNames.end_timestamp).over(visit_window)) == F.lit(self.current_month_date_max.month),\n                F.lit(True),\n            ).otherwise(F.lit(False)),\n        )\n\n        return stays_df\n\n    def calculate_avg_nights_spent_per_destination(self, current_month_finished_trip_stays_df):\n        \"\"\"Calculates the average number of nights spent per destination grouped by MCC.\n\n        This function processes trip stays data to compute the average number of overnight stays\n        for each destination category (MCC). It first aggregates nights spent per user-trip-zone\n        combination, then calculates the average across all users for each MCC.\n\n        Args:\n            current_month_finished_trip_stays_df (DataFrame): Spark DataFrame containing finished trip stays data\n\n        Returns:\n            DataFrame: A Spark DataFrame with columns for MCC and the average number of nights spent per destination.\n\n        Note:\n            The function uses zone_weight to account for partial stays when calculating nights spent.\n            Only records where is_overnight is True are considered for the nights calculation.\n        \"\"\"\n\n        avg_number_of_nights_spent_per_destination_df = (\n            current_month_finished_trip_stays_df.groupBy(ColNames.user_id, ColNames.trip_id, ColNames.zone_id)\n            .agg(\n                F.sum(F.when(F.col(ColNames.is_overnight), F.col(ColNames.zone_weight)).otherwise(F.lit(0.0))).alias(\n                    \"nights_spent\"\n                ),\n                F.first(F.col(ColNames.country_of_origin)).alias(ColNames.country_of_origin),\n            )\n            .groupBy(ColNames.country_of_origin)\n            .agg(F.avg(F.col(\"nights_spent\")).alias(ColNames.avg_nights_spent_per_destination))\n        )\n\n        return avg_number_of_nights_spent_per_destination_df\n\n    def calculate_avg_destination_cnt(self, current_month_finished_trip_stays_df):\n        \"\"\"Calculate average number of unique destinations per trip grouped by MCC.\n\n        Args:\n            current_month_finished_trip_stays_df (DataFrame): Spark DataFrame containing trip stays data\n\n        Returns:\n            DataFrame: A Spark DataFrame with the average number of unique destinations per trip.\n        \"\"\"\n\n        avg_destination_cnt_df = (\n            current_month_finished_trip_stays_df.groupBy(ColNames.user_id, ColNames.trip_id)\n            .agg(\n                F.count_distinct(F.col(ColNames.zone_id)).alias(\"nr_of_unique_destinations\"),\n                F.first(F.col(ColNames.country_of_origin)).alias(ColNames.country_of_origin),\n            )\n            .groupBy(ColNames.country_of_origin)\n            .agg(F.avg(F.col(\"nr_of_unique_destinations\")).alias(ColNames.avg_destinations))\n        )\n\n        return avg_destination_cnt_df\n\n    @staticmethod\n    def get_period_month_bounds_list(\n        data_period_start: datetime, data_period_end: datetime\n    ) -&gt; list[tuple[datetime.date, datetime.date]]:\n        \"\"\"Generate the first and last date of each month that is within the calculation period.\"\"\"\n        return_list = []\n        current_month_start = data_period_start\n        while current_month_start &lt;= data_period_end:\n            current_month_end = current_month_start + relativedelta(months=1) - timedelta(seconds=1)\n            return_list.append((current_month_start.date(), current_month_end.date()))\n            current_month_start += relativedelta(months=1)\n        return return_list\n</code></pre>"},{"location":"reference/components/execution/tourism_statistics/tourism_statistics_calculation/#components.execution.tourism_statistics.tourism_statistics_calculation.TourismStatisticsCalculation.calculate_avg_destination_cnt","title":"<code>calculate_avg_destination_cnt(current_month_finished_trip_stays_df)</code>","text":"<p>Calculate average number of unique destinations per trip grouped by MCC.</p> <p>Parameters:</p> Name Type Description Default <code>current_month_finished_trip_stays_df</code> <code>DataFrame</code> <p>Spark DataFrame containing trip stays data</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <p>A Spark DataFrame with the average number of unique destinations per trip.</p> Source code in <code>multimno/components/execution/tourism_statistics/tourism_statistics_calculation.py</code> <pre><code>def calculate_avg_destination_cnt(self, current_month_finished_trip_stays_df):\n    \"\"\"Calculate average number of unique destinations per trip grouped by MCC.\n\n    Args:\n        current_month_finished_trip_stays_df (DataFrame): Spark DataFrame containing trip stays data\n\n    Returns:\n        DataFrame: A Spark DataFrame with the average number of unique destinations per trip.\n    \"\"\"\n\n    avg_destination_cnt_df = (\n        current_month_finished_trip_stays_df.groupBy(ColNames.user_id, ColNames.trip_id)\n        .agg(\n            F.count_distinct(F.col(ColNames.zone_id)).alias(\"nr_of_unique_destinations\"),\n            F.first(F.col(ColNames.country_of_origin)).alias(ColNames.country_of_origin),\n        )\n        .groupBy(ColNames.country_of_origin)\n        .agg(F.avg(F.col(\"nr_of_unique_destinations\")).alias(ColNames.avg_destinations))\n    )\n\n    return avg_destination_cnt_df\n</code></pre>"},{"location":"reference/components/execution/tourism_statistics/tourism_statistics_calculation/#components.execution.tourism_statistics.tourism_statistics_calculation.TourismStatisticsCalculation.calculate_avg_nights_spent_per_destination","title":"<code>calculate_avg_nights_spent_per_destination(current_month_finished_trip_stays_df)</code>","text":"<p>Calculates the average number of nights spent per destination grouped by MCC.</p> <p>This function processes trip stays data to compute the average number of overnight stays for each destination category (MCC). It first aggregates nights spent per user-trip-zone combination, then calculates the average across all users for each MCC.</p> <p>Parameters:</p> Name Type Description Default <code>current_month_finished_trip_stays_df</code> <code>DataFrame</code> <p>Spark DataFrame containing finished trip stays data</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <p>A Spark DataFrame with columns for MCC and the average number of nights spent per destination.</p> Note <p>The function uses zone_weight to account for partial stays when calculating nights spent. Only records where is_overnight is True are considered for the nights calculation.</p> Source code in <code>multimno/components/execution/tourism_statistics/tourism_statistics_calculation.py</code> <pre><code>def calculate_avg_nights_spent_per_destination(self, current_month_finished_trip_stays_df):\n    \"\"\"Calculates the average number of nights spent per destination grouped by MCC.\n\n    This function processes trip stays data to compute the average number of overnight stays\n    for each destination category (MCC). It first aggregates nights spent per user-trip-zone\n    combination, then calculates the average across all users for each MCC.\n\n    Args:\n        current_month_finished_trip_stays_df (DataFrame): Spark DataFrame containing finished trip stays data\n\n    Returns:\n        DataFrame: A Spark DataFrame with columns for MCC and the average number of nights spent per destination.\n\n    Note:\n        The function uses zone_weight to account for partial stays when calculating nights spent.\n        Only records where is_overnight is True are considered for the nights calculation.\n    \"\"\"\n\n    avg_number_of_nights_spent_per_destination_df = (\n        current_month_finished_trip_stays_df.groupBy(ColNames.user_id, ColNames.trip_id, ColNames.zone_id)\n        .agg(\n            F.sum(F.when(F.col(ColNames.is_overnight), F.col(ColNames.zone_weight)).otherwise(F.lit(0.0))).alias(\n                \"nights_spent\"\n            ),\n            F.first(F.col(ColNames.country_of_origin)).alias(ColNames.country_of_origin),\n        )\n        .groupBy(ColNames.country_of_origin)\n        .agg(F.avg(F.col(\"nights_spent\")).alias(ColNames.avg_nights_spent_per_destination))\n    )\n\n    return avg_number_of_nights_spent_per_destination_df\n</code></pre>"},{"location":"reference/components/execution/tourism_statistics/tourism_statistics_calculation/#components.execution.tourism_statistics.tourism_statistics_calculation.TourismStatisticsCalculation.calculate_trips","title":"<code>calculate_trips(stays_df)</code>","text":"<p>Calculates trip information for stays data using PySpark DataFrame operations.</p> <p>This function processes stay records to identify and mark distinct trips based on temporal gaps between consecutive stays for each user. It assigns trip IDs, determines trip start timestamps, and marks whether trips are finished in the current month.</p> <p>Parameters:</p> Name Type Description Default <code>stays_df</code> <code>DataFrame</code> <p>DataFrame containing stay records</p> required <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: DataFrame with trip information added.</p> Notes <ul> <li>A new trip is created when the time gap between consecutive stays exceeds max_trip_gap_h</li> <li>Existing trip_ids are preserved if present in the input DataFrame</li> <li>Trip completion is determined based on the current_month_date_min</li> </ul> Source code in <code>multimno/components/execution/tourism_statistics/tourism_statistics_calculation.py</code> <pre><code>def calculate_trips(self, stays_df):\n    \"\"\"Calculates trip information for stays data using PySpark DataFrame operations.\n\n    This function processes stay records to identify and mark distinct trips based on temporal gaps\n    between consecutive stays for each user. It assigns trip IDs, determines trip start timestamps,\n    and marks whether trips are finished in the current month.\n\n    Args:\n        stays_df (pyspark.sql.DataFrame): DataFrame containing stay records\n\n    Returns:\n        pyspark.sql.DataFrame: DataFrame with trip information added.\n\n    Notes:\n        - A new trip is created when the time gap between consecutive stays exceeds max_trip_gap_h\n        - Existing trip_ids are preserved if present in the input DataFrame\n        - Trip completion is determined based on the current_month_date_min\n    \"\"\"\n\n    # Define window for sorting stays within each user\n    user_window = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id).orderBy(ColNames.start_timestamp)\n\n    # Identify new trips based on the maximum allowed time gap between stays\n    # and if there is no previous ongoing trip\n    stays_df = stays_df.withColumn(\n        \"prev_end_timestamp\", F.lag(ColNames.end_timestamp).over(user_window)\n    ).withColumn(\n        \"is_new_trip\",\n        F.when((F.col(\"prev_end_timestamp\").isNull()) &amp; (F.col(ColNames.trip_id).isNull()), F.lit(True))\n        .when((F.col(ColNames.trip_id).isNotNull()), F.lit(False))\n        .otherwise(\n            (\n                (F.col(ColNames.start_timestamp).cast(LongType()) - F.col(\"prev_end_timestamp\").cast(LongType()))\n                / 3600\n                &gt; self.max_trip_gap_h\n            )\n        ),\n    )\n\n    # assign start timestamp of the trip\n    stays_df = stays_df.withColumn(\n        ColNames.trip_start_timestamp,\n        F.when(\n            F.col(\"is_new_trip\"),\n            F.col(ColNames.start_timestamp),\n        ).otherwise(F.col(ColNames.trip_start_timestamp)),\n    ).withColumn(  # forward fill trip start timestamp\n        ColNames.trip_start_timestamp,\n        F.last(ColNames.trip_start_timestamp, ignorenulls=True).over(user_window),\n    )\n\n    # Generate trip ID as a hash of user ID and trip start time, but reuse existing trip IDs if available\n    stays_df = stays_df.withColumn(\n        ColNames.trip_id,\n        F.when(\n            F.col(ColNames.trip_id).isNull(),  # Generate new trip_id only if it does not already exist\n            F.md5(\n                F.concat(\n                    F.col(ColNames.user_id).cast(StringType()),\n                    F.col(ColNames.trip_start_timestamp).cast(StringType()),\n                )\n            ),\n        ).otherwise(F.col(ColNames.trip_id)),\n    ).drop(\"prev_end_timestamp\")\n\n    # Determine if a trip is finished based on the maximum `end_timestamp` in each trip\n    trip_window = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id, ColNames.trip_id)\n    stays_df = stays_df.withColumn(\n        \"is_trip_finished\",\n        F.when(\n            F.max(ColNames.end_timestamp).over(trip_window).cast(\"date\") &lt;= F.lit(self.current_month_date_max),\n            F.lit(True),\n        ).otherwise(F.lit(False)),\n    )\n\n    return stays_df\n</code></pre>"},{"location":"reference/components/execution/tourism_statistics/tourism_statistics_calculation/#components.execution.tourism_statistics.tourism_statistics_calculation.TourismStatisticsCalculation.calculate_visits","title":"<code>calculate_visits(stays_df)</code>","text":"<p>Calculates visit IDs for stays data and determines if visits are finished in the current month.</p> <p>A new visit is created when: 1. It's the first record in the trip 2. There's a change in the visited zone 3. Time gap between consecutive stays exceeds max_visit_gap_h</p> <p>Parameters:</p> Name Type Description Default <code>stays_df</code> <code>DataFrame</code> <p>DataFrame containing stay records</p> required <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: DataFrame with visit information added.</p> Notes <p>The function uses window functions to partition data by user and trip for proper visit calculation and utilizes self.max_visit_gap_h and self.current_month_date_min class attributes for gap threshold and current month validation respectively.</p> Source code in <code>multimno/components/execution/tourism_statistics/tourism_statistics_calculation.py</code> <pre><code>def calculate_visits(self, stays_df):\n    \"\"\"Calculates visit IDs for stays data and determines if visits are finished in the current month.\n\n    A new visit is created when:\n    1. It's the first record in the trip\n    2. There's a change in the visited zone\n    3. Time gap between consecutive stays exceeds max_visit_gap_h\n\n    Args:\n        stays_df (pyspark.sql.DataFrame): DataFrame containing stay records\n\n    Returns:\n        pyspark.sql.DataFrame: DataFrame with visit information added.\n\n    Notes:\n        The function uses window functions to partition data by user and trip for proper\n        visit calculation and utilizes self.max_visit_gap_h and self.current_month_date_min\n        class attributes for gap threshold and current month validation respectively.\n    \"\"\"\n\n    # Define window for trip level sorting\n    # Sorting by zone_id first allows aggregating by zone regardless of stay sequentiality\n    trip_window = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id, ColNames.trip_id).orderBy(\n        ColNames.zone_id, ColNames.start_timestamp\n    )\n\n    stays_df = stays_df.withColumn(\n        \"prev_end_timestamp\", F.lag(ColNames.end_timestamp).over(trip_window)\n    ).withColumn(\"prev_zone_id\", F.lag(ColNames.zone_id).over(trip_window))\n\n    # Flag new visits based on time gap or zone change\n    stays_df = stays_df.withColumn(\n        ColNames.visit_id,\n        F.sum(\n            F.when(\n                (F.col(\"prev_end_timestamp\").isNull())  # First record in the trip\n                | (F.col(ColNames.zone_id) != F.col(\"prev_zone_id\"))  # New zone visited\n                | (\n                    (\n                        F.col(ColNames.start_timestamp).cast(LongType())\n                        - F.col(\"prev_end_timestamp\").cast(LongType())\n                    )\n                    / 3600\n                    &gt; self.max_visit_gap_h  # Time gap exceeds threshold\n                ),\n                F.lit(1),\n            ).otherwise(F.lit(0))\n        ).over(trip_window),\n    ).drop(\"prev_end_timestamp\", \"prev_zone_id\")\n\n    # Define a window for visit level sorting\n    visit_window = Window.partitionBy(\n        ColNames.user_id_modulo, ColNames.user_id, ColNames.trip_id, ColNames.visit_id\n    )\n\n    # Determine if a visit is finished in the current month\n    stays_df = stays_df.withColumn(\n        \"is_visit_finished\",\n        F.when(\n            F.month(F.max(ColNames.end_timestamp).over(visit_window)) == F.lit(self.current_month_date_max.month),\n            F.lit(True),\n        ).otherwise(F.lit(False)),\n    )\n\n    return stays_df\n</code></pre>"},{"location":"reference/components/execution/tourism_statistics/tourism_statistics_calculation/#components.execution.tourism_statistics.tourism_statistics_calculation.TourismStatisticsCalculation.get_period_month_bounds_list","title":"<code>get_period_month_bounds_list(data_period_start, data_period_end)</code>  <code>staticmethod</code>","text":"<p>Generate the first and last date of each month that is within the calculation period.</p> Source code in <code>multimno/components/execution/tourism_statistics/tourism_statistics_calculation.py</code> <pre><code>@staticmethod\ndef get_period_month_bounds_list(\n    data_period_start: datetime, data_period_end: datetime\n) -&gt; list[tuple[datetime.date, datetime.date]]:\n    \"\"\"Generate the first and last date of each month that is within the calculation period.\"\"\"\n    return_list = []\n    current_month_start = data_period_start\n    while current_month_start &lt;= data_period_end:\n        current_month_end = current_month_start + relativedelta(months=1) - timedelta(seconds=1)\n        return_list.append((current_month_start.date(), current_month_end.date()))\n        current_month_start += relativedelta(months=1)\n    return return_list\n</code></pre>"},{"location":"reference/components/execution/tourism_stays_estimation/","title":"tourism_stays_estimation","text":""},{"location":"reference/components/execution/tourism_stays_estimation/tourism_stays_estimation/","title":"tourism_stays_estimation","text":"<p>Module that implements the Tourism Stays Estimation functionality.</p>"},{"location":"reference/components/execution/tourism_stays_estimation/tourism_stays_estimation/#components.execution.tourism_stays_estimation.tourism_stays_estimation.TourismStaysEstimation","title":"<code>TourismStaysEstimation</code>","text":"<p>               Bases: <code>Component</code></p> <p>A class to calculate geozones for inbound time segments.</p> Source code in <code>multimno/components/execution/tourism_stays_estimation/tourism_stays_estimation.py</code> <pre><code>class TourismStaysEstimation(Component):\n    \"\"\"\n    A class to calculate geozones for inbound time segments.\n    \"\"\"\n\n    COMPONENT_ID = \"TourismStaysEstimation\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.data_period_start = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n        self.data_period_end = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n\n        self.data_period_dates = [\n            self.data_period_start + timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n        # get state codes from string\n        self.segment_states_to_include = [\"stay\"]\n        # Convert string states to indices\n        self.segment_states_to_include = [SegmentStates.STR_TO_INDEX[state] for state in self.segment_states_to_include]\n\n        self.local_mcc = self.config.getint(GENERAL_CONFIG_KEY, \"local_mcc\")\n        self.zoning_dataset_ids_list = self.config.geteval(self.COMPONENT_ID, \"zoning_dataset_ids_list\")\n        self.min_duration_segment_m = self.config.geteval(self.COMPONENT_ID, \"min_duration_segment_m\")\n        self.functional_midnight_h = self.config.geteval(self.COMPONENT_ID, \"functional_midnight_h\")\n        self.min_duration_segment_night_m = self.config.geteval(self.COMPONENT_ID, \"min_duration_segment_night_m\")\n\n        # Origin of the 4-byte grid IDs, to be used with reserved datasets 100m and 1km INSPIRE grids.\n        self.origin = None\n\n    def initalize_data_objects(self):\n\n        self.filter_ue_segments = self.config.getboolean(self.COMPONENT_ID, \"filter_ue_segments\")\n\n        # Input\n        self.input_data_objects = {}\n\n        inputs = {\n            \"time_segments_silver\": SilverTimeSegmentsDataObject,\n            \"cell_connection_probabilities_data_silver\": SilverCellConnectionProbabilitiesDataObject,\n            \"geozones_grid_map_data_silver\": SilverGeozonesGridMapDataObject,\n            \"grid_data_silver\": SilverGridDataObject,\n        }\n\n        # If releveant, add usual environment labels data object to filter all devices that have a usual environment label\n        if self.filter_ue_segments:\n            inputs[\"usual_environment_labels_data_silver\"] = SilverUsualEnvironmentLabelsDataObject\n\n        for key, value in inputs.items():\n            path = self.config.get(CONFIG_SILVER_PATHS_KEY, key)\n            if check_if_data_path_exists(self.spark, path):\n                self.input_data_objects[value.ID] = value(self.spark, path)\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")\n                raise ValueError(f\"Invalid path for {value.ID}: {path}\")\n\n        # Output\n        self.output_data_objects = {}\n        self.output_daily_silver_tourism_stays_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"tourism_stays_silver\")\n        self.output_data_objects[SilverTourismStaysDataObject.ID] = SilverTourismStaysDataObject(\n            self.spark,\n            self.output_daily_silver_tourism_stays_path,\n        )\n\n        # Output clearing\n        clear_destination_directory = self.config.getboolean(self.COMPONENT_ID, \"clear_destination_directory\")\n        if clear_destination_directory:\n            delete_file_or_folder(self.spark, self.output_daily_silver_tourism_stays_path)\n\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n\n        # for each zoning system, for every date in the data period, process segments and map them to zone ids\n        for current_zoning_dataset_id in self.zoning_dataset_ids_list:\n            self.current_zoning_dataset_id = current_zoning_dataset_id\n            # Handle reserved zoning dataset inputs\n            if current_zoning_dataset_id in ReservedDatasetIDs():\n                self.current_geozones_grid_mapping_df = None\n                # We need to get the origin in order to correctly obtain the INSPIRE IDs\n                self.origin = self.input_data_objects[SilverGridDataObject.ID].df.first()[\"origin\"]\n            else:\n                self.current_geozones_grid_mapping_df = self.input_data_objects[\n                    SilverGeozonesGridMapDataObject.ID\n                ].df.filter(F.col(ColNames.dataset_id) == F.lit(current_zoning_dataset_id))\n\n            for current_date in self.data_period_dates:\n                self.logger.info(\n                    f\"Processing events for {current_date.strftime('%Y-%m-%d')}, zoning dataset {current_zoning_dataset_id}\"\n                )\n\n                # get list of inbound roamers from usual environment labels\n                if self.filter_ue_segments:\n                    inbound_residents_df = (\n                        self.input_data_objects[SilverUsualEnvironmentLabelsDataObject.ID]\n                        .df.filter(\n                            ((F.col(ColNames.label) == \"ue\"))\n                            &amp; (F.col(ColNames.start_date) &lt;= current_date)\n                            &amp; (F.col(ColNames.end_date) &gt;= current_date)\n                        )\n                        .select(ColNames.user_id)\n                        .distinct()\n                    )\n\n                self.current_date = current_date\n                self.current_time_segments_df = self.input_data_objects[SilverTimeSegmentsDataObject.ID].df.filter(\n                    (\n                        F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n                        == F.lit(current_date)\n                    )\n                    &amp; (F.col(ColNames.state).isin(self.segment_states_to_include))\n                    &amp; (F.col(ColNames.mcc) != self.local_mcc)\n                    &amp; (\n                        (F.col(ColNames.end_timestamp).cast(\"long\") - F.col(ColNames.start_timestamp).cast(\"long\"))\n                        / 60.0\n                        &gt;= self.min_duration_segment_m\n                    )\n                )\n\n                # filter out segments that are inbound residents\n                if self.filter_ue_segments:\n                    self.current_time_segments_df = self.current_time_segments_df.join(\n                        inbound_residents_df, on=ColNames.user_id, how=\"left_anti\"\n                    )\n\n                self.current_cell_connection_probabilities_df = self.input_data_objects[\n                    SilverCellConnectionProbabilitiesDataObject.ID\n                ].df.filter(\n                    (\n                        F.make_date(\n                            F.col(ColNames.year),\n                            F.col(ColNames.month),\n                            F.col(ColNames.day),\n                        )\n                        == F.lit(current_date)\n                    )\n                )\n\n                self.transform()\n                self.write()\n\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n\n        current_time_segments_df = self.current_time_segments_df\n        current_cell_connection_probabilities_df = self.current_cell_connection_probabilities_df\n        current_geozones_grid_mapping_df = self.current_geozones_grid_mapping_df\n        current_zoning_dataset_id = self.current_zoning_dataset_id\n\n        # TODO cache current_time_segments_df?\n        # Extract unique cells arrays among segments.\n        unique_cells_df = current_time_segments_df.select(F.col(ColNames.cells)).distinct()\n\n        # For each unique cells array, join to cell-grid dataframe to get grids with posterior probabilities.\n        # Normalize grid probabilities (divide probabilities by cell count) to sum to 1.\n        cells_arr_grid_prob_df = (\n            unique_cells_df.withColumn(ColNames.cell_id, F.explode(F.col(ColNames.cells)))\n            .alias(\"df1\")\n            .join(\n                current_cell_connection_probabilities_df.alias(\"df2\"),\n                on=ColNames.cell_id,\n                how=\"inner\",\n            )\n            .select(\n                \"df1.*\",\n                f\"df2.{ColNames.grid_id}\",\n                (F.col(f\"df2.{ColNames.posterior_probability}\") / F.size(f\"df1.{ColNames.cells}\")).alias(\"grid_weight\"),\n            )\n        )\n\n        # For each unique cells array, map to the lowest level of zone hierarchy and calculate zone weight as sum of grid weights in that zone.\n        # For reserved zoning datasets, instead use grid generator to map grid id to desired resolution and use its string form instead of hierarhical id.\n        if current_zoning_dataset_id in ReservedDatasetIDs():\n            resolution = ReservedDatasetIDs.get_resolution_m(current_zoning_dataset_id)\n            inspire_grid_generator = InspireGridGenerator(spark=self.spark)\n\n            cells_arr_zone_df = (\n                inspire_grid_generator.grid_id_to_inspire_id(\n                    sdf=cells_arr_grid_prob_df,\n                    inspire_resolution=resolution,\n                    origin=self.origin,\n                )\n                .withColumnRenamed(ColNames.inspire_id, ColNames.hierarchical_id)\n                .groupBy(ColNames.cells, ColNames.hierarchical_id)\n                .agg(\n                    F.sum(F.col(\"grid_weight\")).cast(\"float\").alias(ColNames.zone_weight),\n                )\n            ).withColumn(ColNames.dataset_id, F.lit(current_zoning_dataset_id))\n\n        else:\n            cells_arr_zone_df = (\n                cells_arr_grid_prob_df.alias(\"df1\")\n                .join(current_geozones_grid_mapping_df, on=ColNames.grid_id, how=\"inner\")\n                .groupBy(ColNames.cells, ColNames.hierarchical_id)\n                .agg(\n                    F.sum(F.col(\"grid_weight\")).cast(\"float\").alias(ColNames.zone_weight),\n                )\n            ).withColumn(ColNames.dataset_id, F.lit(current_zoning_dataset_id))\n\n        # Join unique cells array with zone mappings back to the original segments.\n        segments_with_zone_weights_df = (\n            current_time_segments_df.alias(\"df1\")\n            .join(cells_arr_zone_df.alias(\"df2\"), on=ColNames.cells, how=\"inner\")\n            .select(\n                \"df1.*\",\n                ColNames.hierarchical_id,\n                ColNames.zone_weight,\n                ColNames.dataset_id,\n            )\n        )\n\n        # Repartition and sort\n        segments_with_zone_weights_df = segments_with_zone_weights_df.repartition(\n            *SilverTourismStaysDataObject.PARTITION_COLUMNS\n        ).sortWithinPartitions(ColNames.start_timestamp, ColNames.hierarchical_id)\n\n        # Mark segments that contain the functional midnight hour and are sufficiently long as overnight segments.\n        segments_with_zone_weights_df = segments_with_zone_weights_df.withColumn(\n            ColNames.is_overnight,\n            F.when(\n                (F.hour(F.col(f\"df1.{ColNames.start_timestamp}\")) &lt;= self.functional_midnight_h)\n                &amp; (F.hour(F.col(f\"df1.{ColNames.end_timestamp}\")) &gt; self.functional_midnight_h)\n                &amp; (\n                    ((F.col(ColNames.end_timestamp).cast(\"long\") - F.col(ColNames.start_timestamp).cast(\"long\")) / 60.0)\n                    &gt;= self.min_duration_segment_night_m\n                ),\n                F.lit(True),\n            ).otherwise(F.lit(False)),\n        )\n\n        # Keep one row per segment, aggregating zone ids and weights.\n        segments_with_zone_weights_df = segments_with_zone_weights_df.groupBy(\n            ColNames.user_id,\n            ColNames.time_segment_id,\n            ColNames.start_timestamp,\n            ColNames.end_timestamp,\n            ColNames.is_overnight,\n            ColNames.mcc,\n            ColNames.mnc,\n            ColNames.plmn,\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            ColNames.user_id_modulo,\n            ColNames.dataset_id,\n        ).agg(  # TODO collect_list order is not deterministic\n            F.collect_list(ColNames.hierarchical_id).alias(ColNames.zone_ids_list),\n            F.collect_list(ColNames.zone_weight).alias(ColNames.zone_weights_list),\n        )\n\n        segments_with_zone_weights_df = apply_schema_casting(\n            segments_with_zone_weights_df, SilverTourismStaysDataObject.SCHEMA\n        )\n\n        # Prepare output\n        self.output_data_objects[SilverTourismStaysDataObject.ID].df = segments_with_zone_weights_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_aggregation/","title":"usual_environment_aggregation","text":""},{"location":"reference/components/execution/usual_environment_aggregation/usual_environment_aggregation/","title":"usual_environment_aggregation","text":"<p>This module is responsible for usual environment and location labels aggregation to reference grid</p>"},{"location":"reference/components/execution/usual_environment_aggregation/usual_environment_aggregation/#components.execution.usual_environment_aggregation.usual_environment_aggregation.UsualEnvironmentAggregation","title":"<code>UsualEnvironmentAggregation</code>","text":"<p>               Bases: <code>Component</code></p> <p>A class to aggregate devices usual environment and location labels to reference grid.</p> Source code in <code>multimno/components/execution/usual_environment_aggregation/usual_environment_aggregation.py</code> <pre><code>class UsualEnvironmentAggregation(Component):\n    \"\"\"\n    A class to aggregate devices usual environment and location labels to reference grid.\n    \"\"\"\n\n    COMPONENT_ID = \"UsualEnvironmentAggregation\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        # Months that compose the long-term period, at least one\n        start_month = self.config.get(self.COMPONENT_ID, \"start_month\")\n        try:\n            self.start_date = dt.datetime.strptime(start_month, \"%Y-%m\").date()\n        except ValueError as e:\n            self.logger.error(f\"Could not parse start_month = `{start_month}`. Expected format: YYYY-MM\")\n            raise e\n\n        end_month = self.config.get(self.COMPONENT_ID, \"end_month\")\n        try:\n            self.end_date = dt.datetime.strptime(end_month, \"%Y-%m\").date()\n        except ValueError as e:\n            self.logger.error(f\"Could not parse end_month = `{end_month}`. Expected format: YYYY-MM\")\n            raise e\n        self.end_date = self.end_date.replace(day=cal.monthrange(self.end_date.year, self.end_date.month)[1])\n\n        if self.end_date &lt; self.start_date:\n            raise ValueError(f\"End month `{end_month}` should not be an earlier than start month `{start_month}`\")\n\n        self.uniform_tile_weights = self.config.getboolean(\n            UsualEnvironmentAggregation.COMPONENT_ID, \"uniform_tile_weights\"\n        )\n\n        self.landuse_weights = self.config.geteval(self.COMPONENT_ID, \"landuse_weights\")\n\n        self.season = self.config.get(self.COMPONENT_ID, \"season\")\n        if not Seasons.is_valid_type(self.season):\n            error_msg = f\"season: expected one of: {', '.join(Seasons.values())} - found: {self.season}\"\n            self.logger.error(error_msg)\n            raise ValueError(error_msg)\n\n    def initalize_data_objects(self):\n\n        # inputs\n        self.clear_destination_directory = self.config.getboolean(\n            UsualEnvironmentAggregation.COMPONENT_ID, \"clear_destination_directory\"\n        )\n\n        self.input_data_objects = {}\n        # Check uniform for getting the grid or the enriched grid data\n        uniform_tile_weights = self.config.getboolean(UsualEnvironmentAggregation.COMPONENT_ID, \"uniform_tile_weights\")\n\n        if uniform_tile_weights:\n            inputs = {\n                \"grid_data_silver\": SilverGridDataObject,\n                \"usual_environment_labels_data_silver\": SilverUsualEnvironmentLabelsDataObject,\n            }\n        else:\n            inputs = {\n                \"enriched_grid_data_silver\": SilverEnrichedGridDataObject,\n                \"usual_environment_labels_data_silver\": SilverUsualEnvironmentLabelsDataObject,\n            }\n\n        for key, value in inputs.items():\n            path = self.config.get(CONFIG_SILVER_PATHS_KEY, key)\n            if check_if_data_path_exists(self.spark, path):\n                self.input_data_objects[value.ID] = value(self.spark, path)\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")\n                raise ValueError(f\"Invalid path for {value.ID}: {path}\")\n\n        # outputs\n\n        output_do_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"aggregated_usual_environments_silver\")\n\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, output_do_path)\n\n        self.output_data_objects = {}\n        self.output_data_objects[SilverAggregatedUsualEnvironmentsDataObject.ID] = (\n            SilverAggregatedUsualEnvironmentsDataObject(\n                self.spark, output_do_path, [ColNames.start_date, ColNames.end_date, ColNames.season]\n            )\n        )\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}...\")\n\n        # prepare grid with tile weights\n        if self.uniform_tile_weights:\n            grid_sdf = self.input_data_objects[SilverGridDataObject.ID].df\n            grid_sdf = grid_sdf.withColumn(ColNames.tile_weight, F.lit(1.0))\n        else:\n            grid_sdf = self.calculate_landuse_tile_weights(\n                self.input_data_objects[SilverEnrichedGridDataObject.ID].df, self.landuse_weights\n            )\n\n        # prepare usual environment labels\n        ue_labels_sdf = self.input_data_objects[SilverUsualEnvironmentLabelsDataObject.ID].df\n        ue_labels_sdf = ue_labels_sdf.filter(\n            (F.col(ColNames.start_date) == F.lit(self.start_date))\n            &amp; (F.col(ColNames.end_date) == F.lit(self.end_date))\n            &amp; (F.col(ColNames.season) == F.lit(self.season))\n        )\n        ue_labels_sdf = ue_labels_sdf.select(\n            ColNames.user_id, ColNames.grid_id, ColNames.label, ColNames.user_id_modulo\n        )\n\n        # aggregate ue and meaningful locations\n        aggregated_labels_sdf = self.aggregate_labels(ue_labels_sdf, grid_sdf)\n\n        # Cast column types to DO schema, add missing columns manually\n        aggregated_labels_sdf = (\n            aggregated_labels_sdf.withColumn(ColNames.start_date, F.lit(self.start_date))\n            .withColumn(ColNames.end_date, F.lit(self.end_date))\n            .withColumn(ColNames.season, F.lit(self.season))\n        )\n\n        aggregated_labels_sdf = utils.apply_schema_casting(\n            aggregated_labels_sdf, SilverAggregatedUsualEnvironmentsDataObject.SCHEMA\n        )\n\n        aggregated_labels_sdf = aggregated_labels_sdf.repartition(\n            *SilverAggregatedUsualEnvironmentsDataObject.PARTITION_COLUMNS\n        )\n\n        self.output_data_objects[SilverAggregatedUsualEnvironmentsDataObject.ID].df = aggregated_labels_sdf\n\n    def calculate_landuse_tile_weights(\n        self,\n        enriched_grid_sdf: DataFrame,\n        landuse_prior_weights: Dict[str, float],\n    ) -&gt; DataFrame:\n        \"\"\"\n        Calculates the tile weights based on landuse information in the enriched grid DataFrame.\n        \"\"\"\n\n        grid_sdf = self.input_data_objects[SilverEnrichedGridDataObject.ID].df\n        grid_sdf = grid_sdf.select(\n            ColNames.grid_id,\n            F.col(ColNames.main_landuse_category),\n        )\n\n        # Create a DataFrame from the weights dictionary\n        weights_list = [(k, float(v)) for k, v in landuse_prior_weights.items()]\n        weights_df = self.spark.createDataFrame(weights_list, [ColNames.main_landuse_category, \"weight\"])\n        weighted_df = enriched_grid_sdf.join(weights_df, on=ColNames.main_landuse_category, how=\"left\").withColumn(\n            ColNames.tile_weight, F.coalesce(F.col(\"weight\"), F.lit(0.0))\n        )  # Default 0.0 for missing weights\n\n        return weighted_df\n\n    def get_device_tile_weights(self, ue_labels_sdf: DataFrame, grid_sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Calculates and assigns weights to each device UE tiles based on the tile weights in the grid DataFrame.\n\n        This method performs a join operation between ue labels DataFrame and a grid DataFrame based on grid IDs.\n        It then calculates the weight of each device tile as the ratio of the tile's weight to the sum of all tile weights\n        for the same user for a given label.\n\n        If a label is specified, the method first filters the UE labels DataFrame to include only rows with the matching label\n        before proceeding with the join and weight calculation.\n\n        If a label is not specified, the method uses all tiles of a device to calculate the weights.\n\n        Parameters:\n        - ue_labels_sdf (DataFrame): The DataFrame containing usual environment and location labels for each device.\n        - grid_sdf (DataFrame): The DataFrame containing grid information, including grid IDs and tile weights.\n        - label (str, optional): A specific label to filter the UE labels DataFrame.\n\n        Returns:\n        - DataFrame: A DataFrame containing the calculated weights for each device tile.\n        \"\"\"\n\n        ue_labels_sdf = ue_labels_sdf.join(grid_sdf, on=ColNames.grid_id, how=\"left\")\n        # Assign a default tile weight of 0.001 to missing tile weights to avoid division by zero\n        ue_labels_sdf = ue_labels_sdf.withColumn(ColNames.tile_weight, F.coalesce(ColNames.tile_weight, F.lit(0.001)))\n        window_spec = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id, ColNames.label)\n        ue_labels_sdf = ue_labels_sdf.withColumn(\n            ColNames.device_tile_weight, F.col(ColNames.tile_weight) / F.sum(ColNames.tile_weight).over(window_spec)\n        )\n\n        return ue_labels_sdf\n\n    def aggregate_labels(self, ue_labels_sdf: DataFrame, grid_sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Aggregates location labels by grid ID and calculates the sum of weighted device count.\n\n        This method first calculates device tile weights for location label tiles.\n        It then aggregates these weights by grid ID to compute the total weighted device count for each grid for each label.\n        Finally, it assigns corresponding label name to all aggregated entries to indicate their association with usual environments.\n\n        Parameters:\n        - ue_labels_sdf (DataFrame): The DataFrame containing ue labels.\n        - grid_sdf (DataFrame): The DataFrame containing grid information with tile weights.\n\n        Returns:\n        - DataFrame: A DataFrame with sum of weighted device count per grid tile.\n        \"\"\"\n\n        loc_label_with_device_weights_sdf = self.get_device_tile_weights(ue_labels_sdf, grid_sdf)\n\n        aggregated_labels_sdf = loc_label_with_device_weights_sdf.groupBy(ColNames.grid_id, ColNames.label).agg(\n            F.sum(ColNames.device_tile_weight).alias(ColNames.weighted_device_count)\n        )\n\n        return aggregated_labels_sdf\n</code></pre>"},{"location":"reference/components/execution/usual_environment_aggregation/usual_environment_aggregation/#components.execution.usual_environment_aggregation.usual_environment_aggregation.UsualEnvironmentAggregation.aggregate_labels","title":"<code>aggregate_labels(ue_labels_sdf, grid_sdf)</code>","text":"<p>Aggregates location labels by grid ID and calculates the sum of weighted device count.</p> <p>This method first calculates device tile weights for location label tiles. It then aggregates these weights by grid ID to compute the total weighted device count for each grid for each label. Finally, it assigns corresponding label name to all aggregated entries to indicate their association with usual environments.</p> <p>Parameters: - ue_labels_sdf (DataFrame): The DataFrame containing ue labels. - grid_sdf (DataFrame): The DataFrame containing grid information with tile weights.</p> <p>Returns: - DataFrame: A DataFrame with sum of weighted device count per grid tile.</p> Source code in <code>multimno/components/execution/usual_environment_aggregation/usual_environment_aggregation.py</code> <pre><code>def aggregate_labels(self, ue_labels_sdf: DataFrame, grid_sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Aggregates location labels by grid ID and calculates the sum of weighted device count.\n\n    This method first calculates device tile weights for location label tiles.\n    It then aggregates these weights by grid ID to compute the total weighted device count for each grid for each label.\n    Finally, it assigns corresponding label name to all aggregated entries to indicate their association with usual environments.\n\n    Parameters:\n    - ue_labels_sdf (DataFrame): The DataFrame containing ue labels.\n    - grid_sdf (DataFrame): The DataFrame containing grid information with tile weights.\n\n    Returns:\n    - DataFrame: A DataFrame with sum of weighted device count per grid tile.\n    \"\"\"\n\n    loc_label_with_device_weights_sdf = self.get_device_tile_weights(ue_labels_sdf, grid_sdf)\n\n    aggregated_labels_sdf = loc_label_with_device_weights_sdf.groupBy(ColNames.grid_id, ColNames.label).agg(\n        F.sum(ColNames.device_tile_weight).alias(ColNames.weighted_device_count)\n    )\n\n    return aggregated_labels_sdf\n</code></pre>"},{"location":"reference/components/execution/usual_environment_aggregation/usual_environment_aggregation/#components.execution.usual_environment_aggregation.usual_environment_aggregation.UsualEnvironmentAggregation.calculate_landuse_tile_weights","title":"<code>calculate_landuse_tile_weights(enriched_grid_sdf, landuse_prior_weights)</code>","text":"<p>Calculates the tile weights based on landuse information in the enriched grid DataFrame.</p> Source code in <code>multimno/components/execution/usual_environment_aggregation/usual_environment_aggregation.py</code> <pre><code>def calculate_landuse_tile_weights(\n    self,\n    enriched_grid_sdf: DataFrame,\n    landuse_prior_weights: Dict[str, float],\n) -&gt; DataFrame:\n    \"\"\"\n    Calculates the tile weights based on landuse information in the enriched grid DataFrame.\n    \"\"\"\n\n    grid_sdf = self.input_data_objects[SilverEnrichedGridDataObject.ID].df\n    grid_sdf = grid_sdf.select(\n        ColNames.grid_id,\n        F.col(ColNames.main_landuse_category),\n    )\n\n    # Create a DataFrame from the weights dictionary\n    weights_list = [(k, float(v)) for k, v in landuse_prior_weights.items()]\n    weights_df = self.spark.createDataFrame(weights_list, [ColNames.main_landuse_category, \"weight\"])\n    weighted_df = enriched_grid_sdf.join(weights_df, on=ColNames.main_landuse_category, how=\"left\").withColumn(\n        ColNames.tile_weight, F.coalesce(F.col(\"weight\"), F.lit(0.0))\n    )  # Default 0.0 for missing weights\n\n    return weighted_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_aggregation/usual_environment_aggregation/#components.execution.usual_environment_aggregation.usual_environment_aggregation.UsualEnvironmentAggregation.get_device_tile_weights","title":"<code>get_device_tile_weights(ue_labels_sdf, grid_sdf)</code>","text":"<p>Calculates and assigns weights to each device UE tiles based on the tile weights in the grid DataFrame.</p> <p>This method performs a join operation between ue labels DataFrame and a grid DataFrame based on grid IDs. It then calculates the weight of each device tile as the ratio of the tile's weight to the sum of all tile weights for the same user for a given label.</p> <p>If a label is specified, the method first filters the UE labels DataFrame to include only rows with the matching label before proceeding with the join and weight calculation.</p> <p>If a label is not specified, the method uses all tiles of a device to calculate the weights.</p> <p>Parameters: - ue_labels_sdf (DataFrame): The DataFrame containing usual environment and location labels for each device. - grid_sdf (DataFrame): The DataFrame containing grid information, including grid IDs and tile weights. - label (str, optional): A specific label to filter the UE labels DataFrame.</p> <p>Returns: - DataFrame: A DataFrame containing the calculated weights for each device tile.</p> Source code in <code>multimno/components/execution/usual_environment_aggregation/usual_environment_aggregation.py</code> <pre><code>def get_device_tile_weights(self, ue_labels_sdf: DataFrame, grid_sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Calculates and assigns weights to each device UE tiles based on the tile weights in the grid DataFrame.\n\n    This method performs a join operation between ue labels DataFrame and a grid DataFrame based on grid IDs.\n    It then calculates the weight of each device tile as the ratio of the tile's weight to the sum of all tile weights\n    for the same user for a given label.\n\n    If a label is specified, the method first filters the UE labels DataFrame to include only rows with the matching label\n    before proceeding with the join and weight calculation.\n\n    If a label is not specified, the method uses all tiles of a device to calculate the weights.\n\n    Parameters:\n    - ue_labels_sdf (DataFrame): The DataFrame containing usual environment and location labels for each device.\n    - grid_sdf (DataFrame): The DataFrame containing grid information, including grid IDs and tile weights.\n    - label (str, optional): A specific label to filter the UE labels DataFrame.\n\n    Returns:\n    - DataFrame: A DataFrame containing the calculated weights for each device tile.\n    \"\"\"\n\n    ue_labels_sdf = ue_labels_sdf.join(grid_sdf, on=ColNames.grid_id, how=\"left\")\n    # Assign a default tile weight of 0.001 to missing tile weights to avoid division by zero\n    ue_labels_sdf = ue_labels_sdf.withColumn(ColNames.tile_weight, F.coalesce(ColNames.tile_weight, F.lit(0.001)))\n    window_spec = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id, ColNames.label)\n    ue_labels_sdf = ue_labels_sdf.withColumn(\n        ColNames.device_tile_weight, F.col(ColNames.tile_weight) / F.sum(ColNames.tile_weight).over(window_spec)\n    )\n\n    return ue_labels_sdf\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/","title":"usual_environment_labeling","text":""},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/","title":"usual_environment_labeling","text":"<p>Module that implements the Usual Environment Labeling functionality</p>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling","title":"<code>UsualEnvironmentLabeling</code>","text":"<p>               Bases: <code>Component</code></p> <p>A class to calculate the grid tiles that conform the usual environment, home and work locations of each user.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>class UsualEnvironmentLabeling(Component):\n    \"\"\"\n    A class to calculate the grid tiles that conform the usual environment, home and work locations of each user.\n    \"\"\"\n\n    COMPONENT_ID = \"UsualEnvironmentLabeling\"\n    LABEL_TO_LABELNAMES = {\"ue\": \"no_label\", \"home\": \"home\", \"work\": \"work\"}\n    LABEL_TO_SHORT_LABELNAMES = {\"ue\": \"ue\", \"home\": \"h\", \"work\": \"w\"}\n\n    TOTAL_PERMANENCE_THRESHOLD = \"total_ps_threshold\"\n    TOTAL_FREQUENCY_THRESHOLD = \"total_freq_threshold\"\n\n    CHECK_TO_COLUMN = {\n        TOTAL_PERMANENCE_THRESHOLD: ColNames.lps,\n        TOTAL_FREQUENCY_THRESHOLD: ColNames.total_frequency,\n    }\n\n    UNLABELED_TILES = \"unlabeled_tiles\"\n    UNLABELED_DEVICES = \"unlabeled_devices\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.gap_ps_thresholds = {\n            \"ue\": self.config.getfloat(self.COMPONENT_ID, \"ue_gap_ps_threshold\"),\n            \"home\": self.config.getint(self.COMPONENT_ID, \"home_gap_ps_threshold\"),\n            \"work\": self.config.getint(self.COMPONENT_ID, \"work_gap_ps_threshold\"),\n        }\n\n        self.gap_ps_threshold_is_absolute = {\n            \"ue\": False,\n            \"home\": True,\n            \"work\": True,\n        }  # TODO:\n\n        self.ps_thresholds = {\n            \"ue\": self.config.getfloat(self.COMPONENT_ID, \"ue_ps_threshold\"),\n            \"home\": self.config.getfloat(self.COMPONENT_ID, \"home_ps_threshold\"),\n            \"work\": self.config.getfloat(self.COMPONENT_ID, \"work_ps_threshold\"),\n        }\n\n        self.freq_thresholds = {\n            \"ue\": self.config.getfloat(self.COMPONENT_ID, \"ue_ndays_threshold\"),\n            \"home\": self.config.getfloat(self.COMPONENT_ID, \"home_ndays_threshold\"),\n            \"work\": self.config.getfloat(self.COMPONENT_ID, \"work_ndays_threshold\"),\n        }\n\n        self.ps_threshold_for_rare_devices = self.config.getfloat(self.COMPONENT_ID, \"total_ps_threshold\")\n        self.freq_threshold_for_discontinuous_devices = self.config.getfloat(self.COMPONENT_ID, \"total_ndays_threshold\")\n\n        self.day_and_interval_type_combinations = {\n            \"ue\": [(\"all\", \"all\"), (\"all\", \"night_time\"), (\"workdays\", \"working_hours\")],\n            \"home\": [(\"all\", \"all\"), (\"all\", \"night_time\")],\n            \"work\": [(\"workdays\", \"working_hours\")],\n        }\n\n        self.season = self.config.get(self.COMPONENT_ID, \"season\")\n        if not Seasons.is_valid_type(self.season):\n            error_msg = f\"season: expected one of: {', '.join(Seasons.values)} - found: {self.season}\"\n            self.logger.error(error_msg)\n            raise ValueError(error_msg)\n\n        # Months that compose the long-term period, at least one\n        start_month = self.config.get(self.COMPONENT_ID, \"start_month\")\n        try:\n            self.start_date = dt.datetime.strptime(start_month, \"%Y-%m\").date()\n        except ValueError as e:\n            self.logger.error(f\"Could not parse start_month = `{start_month}`. Expected format: YYYY-MM\")\n            raise e\n\n        end_month = self.config.get(self.COMPONENT_ID, \"end_month\")\n        try:\n            self.end_date = dt.datetime.strptime(end_month, \"%Y-%m\").date()\n        except ValueError as e:\n            self.logger.error(f\"Could not parse end_month = `{end_month}`. Expected format: YYYY-MM\")\n            raise e\n        self.end_date = self.end_date.replace(day=cal.monthrange(self.end_date.year, self.end_date.month)[1])\n\n        if self.end_date &lt; self.start_date:\n            raise ValueError(f\"End month `{end_month}` should not be earlier than start month `{start_month}`\")\n\n        self.ltps_df: DataFrame = None\n        self.rare_devices_count = 0\n        self.discontinuous_devices_count = 0\n        self.disaggregate_to_100m_grid = self.config.getboolean(\n            self.COMPONENT_ID, \"disaggregate_to_100m_grid\", fallback=False\n        )\n        self.grid_gen = InspireGridGenerator(self.spark)\n        self.label_home = self.config.getboolean(self.COMPONENT_ID, \"label_home\", fallback=False)\n        self.label_work = self.config.getboolean(self.COMPONENT_ID, \"label_work\", fallback=False)\n\n    def initalize_data_objects(self):\n        # Load paths from configuration file:\n        input_ltps_silver_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"longterm_permanence_score_data_silver\")\n        output_uelabels_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"usual_environment_labels_data_silver\")\n        self.ue_labels_cache_path = f\"{output_uelabels_path}_cache\"\n        output_quality_metrics_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"usual_environment_labeling_quality_metrics_data_silver\"\n        )\n\n        # Clear destination directory if needed\n        clear_destination_directory = self.config.getboolean(\n            self.COMPONENT_ID, \"clear_destination_directory\", fallback=False\n        )\n        if clear_destination_directory:\n            self.logger.warning(f\"Deleting: {output_uelabels_path}\")\n            delete_file_or_folder(self.spark, output_uelabels_path)\n\n            self.logger.warning(f\"Deleting: {output_quality_metrics_path}\")\n            delete_file_or_folder(self.spark, output_quality_metrics_path)\n\n        # Initialise input and output data objects:\n        silver_ltps = SilverLongtermPermanenceScoreDataObject(self.spark, input_ltps_silver_path)\n        ue_labels = SilverUsualEnvironmentLabelsDataObject(self.spark, output_uelabels_path)\n        ue_quality_metrics = SilverUsualEnvironmentLabelingQualityMetricsDataObject(\n            self.spark, output_quality_metrics_path\n        )\n\n        # Store data objects in the corresponding attributes:\n        self.input_data_objects = {silver_ltps.ID: silver_ltps}\n        self.output_data_objects = {ue_labels.ID: ue_labels, ue_quality_metrics.ID: ue_quality_metrics}\n\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        # read input data object:\n        self.read()\n        full_ltps_df = self.input_data_objects[SilverLongtermPermanenceScoreDataObject.ID].df\n        # filtering to obtain the main dataset which this method will work with:\n        self.ltps_df = self.filter_ltps_by_target_dates(full_ltps_df, self.start_date, self.end_date, self.season)\n\n        # get label configuration:\n        ue_labeling_config, meaningful_location_labeling_config = self.get_labeling_config()\n\n        # check if meaningful location labeling is enabled and remove the corresponding label configurations:\n        if not self.label_home:\n            meaningful_location_labeling_config = [\n                label for label in meaningful_location_labeling_config if label[\"label_code\"] != \"home\"\n            ]\n        if not self.label_work:\n            meaningful_location_labeling_config = [\n                label for label in meaningful_location_labeling_config if label[\"label_code\"] != \"work\"\n            ]\n\n        # extract relevant intervals:\n        relevant_periods = self.extract_relevant_periods(ue_labeling_config)\n        for label in meaningful_location_labeling_config:\n            relevant_periods.update(self.extract_relevant_periods(label))\n        # assert that all the needed day type and interval times are available in the main dataset:\n        self.check_needed_day_and_interval_types(self.ltps_df, relevant_periods)\n        self.ue_labeling_config = ue_labeling_config\n        self.meaningful_location_labeling_config = meaningful_location_labeling_config\n\n        partition_chunks = self._get_partition_chunks()\n        for i, partition_chunk in enumerate(partition_chunks):\n            self.logger.info(f\"Processing partition chunk: {i}\")\n            self.logger.debug(f\"Partition chunk: {partition_chunk}\")\n            self.partition_chunk = partition_chunk\n            # main transformations of this method:\n            self.transform()\n            self.spark.catalog.clearCache()\n\n        self.logger.info(f\"Generating quality metrics...\")\n        quality_metrics_df = self.compute_quality_metrics()\n        self.output_data_objects[SilverUsualEnvironmentLabelingQualityMetricsDataObject.ID].df = quality_metrics_df\n        self.output_data_objects[SilverUsualEnvironmentLabelingQualityMetricsDataObject.ID].write()\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def get_labeling_config(self):\n        \"\"\"\n        Get the configuration of the labeling process.\n\n        Returns:\n            (dict): configuration of the labeling process.\n        \"\"\"\n        ue_labels = {\n            \"label_code\": \"ue\",\n            \"ps_difference_gap_filter\": {\n                \"relevant_periods\": [PeriodCombinations.ALL_PERIODS],\n                \"gap_permanence_threshold\": self.gap_ps_thresholds[\"ue\"],\n                \"is_absolute\": self.gap_ps_threshold_is_absolute[\"ue\"],\n            },\n            \"labeling_rules\": [\n                {\n                    \"rule_code\": \"ue_1\",\n                    \"check_type\": self.TOTAL_PERMANENCE_THRESHOLD,\n                    \"threshold_value\": self.ps_thresholds[\"ue\"],\n                    \"relevant_periods\": [PeriodCombinations.ALL_PERIODS],\n                    \"apply_condition\": self.UNLABELED_TILES,\n                },\n                {\n                    \"rule_code\": \"ue_2\",\n                    \"check_type\": self.TOTAL_PERMANENCE_THRESHOLD,\n                    \"threshold_value\": self.ps_thresholds[\"ue\"],\n                    \"relevant_periods\": [\n                        PeriodCombinations.NIGHT_TIME_ALL,\n                    ],\n                    \"apply_condition\": self.UNLABELED_TILES,\n                },\n                {\n                    \"rule_code\": \"ue_2\",\n                    \"check_type\": self.TOTAL_PERMANENCE_THRESHOLD,\n                    \"threshold_value\": self.ps_thresholds[\"ue\"],\n                    \"relevant_periods\": [\n                        PeriodCombinations.WORKING_HOURS_WORKDAYS,\n                    ],\n                    \"apply_condition\": self.UNLABELED_TILES,\n                },\n                {\n                    \"rule_code\": \"ue_3\",\n                    \"check_type\": self.TOTAL_FREQUENCY_THRESHOLD,\n                    \"threshold_value\": self.freq_thresholds[\"ue\"],\n                    \"relevant_periods\": [PeriodCombinations.ALL_PERIODS],\n                    \"apply_condition\": self.UNLABELED_TILES,\n                },\n            ],\n        }\n\n        meaningful_location_labels = [\n            {\n                \"label_code\": \"home\",\n                \"ps_difference_gap_filter\": {\n                    \"relevant_periods\": [PeriodCombinations.ALL_PERIODS],\n                    \"gap_permanence_threshold\": self.gap_ps_thresholds[\"home\"],\n                    \"is_absolute\": self.gap_ps_threshold_is_absolute[\"home\"],\n                },\n                \"exclude_label_codes\": [],\n                \"labeling_rules\": [\n                    {\n                        \"rule_code\": \"h_1\",\n                        \"check_type\": self.TOTAL_PERMANENCE_THRESHOLD,\n                        \"threshold_value\": self.ps_thresholds[\"home\"],\n                        \"relevant_periods\": [PeriodCombinations.ALL_PERIODS],\n                        \"apply_condition\": self.UNLABELED_DEVICES,\n                    },\n                    {\n                        \"rule_code\": \"h_2\",\n                        \"check_type\": self.TOTAL_PERMANENCE_THRESHOLD,\n                        \"threshold_value\": self.ps_thresholds[\"home\"],\n                        \"relevant_periods\": [PeriodCombinations.NIGHT_TIME_ALL],\n                        \"apply_condition\": self.UNLABELED_DEVICES,\n                    },\n                    {\n                        \"rule_code\": \"h_3\",\n                        \"check_type\": self.TOTAL_FREQUENCY_THRESHOLD,\n                        \"threshold_value\": self.freq_thresholds[\"home\"],\n                        \"relevant_periods\": [PeriodCombinations.NIGHT_TIME_ALL],\n                        \"apply_condition\": self.UNLABELED_DEVICES,\n                    },\n                ],\n            },\n            {\n                \"label_code\": \"work\",\n                \"ps_difference_gap_filter\": {\n                    \"relevant_periods\": [PeriodCombinations.WORKING_HOURS_WORKDAYS],\n                    \"gap_permanence_threshold\": self.gap_ps_thresholds[\"work\"],\n                    \"is_absolute\": self.gap_ps_threshold_is_absolute[\"work\"],\n                },\n                \"exclude_label_codes\": [],\n                \"labeling_rules\": [\n                    {\n                        \"rule_code\": \"w_1\",\n                        \"check_type\": self.TOTAL_PERMANENCE_THRESHOLD,\n                        \"threshold_value\": self.ps_thresholds[\"work\"],\n                        \"relevant_periods\": [PeriodCombinations.WORKING_HOURS_WORKDAYS],\n                        \"apply_condition\": self.UNLABELED_DEVICES,\n                    },\n                    {\n                        \"rule_code\": \"w_2\",\n                        \"check_type\": self.TOTAL_FREQUENCY_THRESHOLD,\n                        \"threshold_value\": self.freq_thresholds[\"work\"],\n                        \"relevant_periods\": [PeriodCombinations.WORKING_HOURS_WORKDAYS],\n                        \"apply_condition\": self.UNLABELED_DEVICES,\n                    },\n                ],\n            },\n        ]\n\n        return ue_labels, meaningful_location_labels\n\n    def extract_relevant_periods(self, labeling_config: Dict) -&gt; List[List[str]]:\n        all_intervals = set()\n\n        for rule in labeling_config[\"labeling_rules\"]:\n            for interval in rule[\"relevant_periods\"]:\n                all_intervals.add(tuple(interval))\n\n        for interval in labeling_config[\"ps_difference_gap_filter\"][\"relevant_periods\"]:\n            all_intervals.add(tuple(interval))\n\n        return all_intervals\n\n    @staticmethod\n    def filter_ltps_by_target_dates(\n        full_ltps_df: DataFrame, start_date: dt.date, end_date: dt.date, season: str\n    ) -&gt; DataFrame:\n        \"\"\"\n        Keep only the rows of the input SilverLongtermPermanenceScoreDataObject in which the start\n        date, end date and season match the ones specified for the processing of this method.\n\n        Args:\n            full_ltps_df (DataFrame): full dataset.\n            start_date (dt.date): specified target start date for the execution of the method.\n            end_date (dt.date): specified target end date for the execution of the method.\n            season (str): specified target season for the execution of the method.\n\n        Returns:\n            DataFrame: filtered dataset.\n        \"\"\"\n        filtered_ltps_df = full_ltps_df.filter(\n            (F.col(ColNames.start_date) == start_date)\n            &amp; (F.col(ColNames.end_date) == end_date)\n            &amp; (F.col(ColNames.season) == season)\n        ).select(\n            ColNames.user_id,\n            ColNames.grid_id,\n            ColNames.lps,\n            ColNames.total_frequency,\n            ColNames.season,\n            ColNames.day_type,\n            ColNames.time_interval,\n            ColNames.user_id_modulo,\n            ColNames.id_type,\n        )\n        return filtered_ltps_df\n\n    @staticmethod\n    def check_needed_day_and_interval_types(\n        ltps_df: DataFrame, day_and_interval_type_combinations: Set[Tuple[str, str, str]]\n    ):\n        \"\"\"\n        Method that checks if the needed combinations of day type and interval type are available\n        in the provided dataset.\n\n        Args:\n            ltps_df (DataFrame): provided Long-Term Permanence Score dataset\n            day_and_interval_type_combinations (Dict[str,List[Tuple[str,str]]]): day type and interval type\n                combinations that are needed for the execution of the method.\n\n        Raises:\n            FileNotFoundError: If there is no data for one or more of the needed combinations of day type\n                and interval type.\n        \"\"\"\n        # Assert that these combinations appear at least once in the input Long-Term Permanence\n        # Score data object:\n        for season, day_type, time_interval in day_and_interval_type_combinations:\n            filtered_ltps_df = ltps_df.filter(\n                (F.col(ColNames.season) == season)\n                &amp; (F.col(ColNames.day_type) == day_type)\n                &amp; (F.col(ColNames.time_interval) == time_interval)\n            )\n\n            data_exists = filtered_ltps_df.count() &gt; 0\n            if not data_exists:\n                raise FileNotFoundError(\n                    \"No Long-term Permanence Score data has been found for \"\n                    f\"day_type `{day_type}` and time_interval `{time_interval}`\"\n                )\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n\n        if self.partition_chunk is not None:\n            self.ltps_df = self.ltps_df.filter(F.col(ColNames.user_id_modulo).isin(self.partition_chunk))\n        # discard devices that will not be analysed ('rarely observed' or 'discontinuously observed'):\n        self.logger.info(\"Discarding devices...\")\n        self.ltps_df = self.discard_devices(self.ltps_df)\n\n        # calculate tiles that belong to the ue (usual environment) of each device:\n        self.logger.info(\"Calculating usual environment...\")\n        ue_tiles_df = self.compute_generic_labeling(self.ue_labeling_config, is_location_labeling=False)\n        self.write_label(ue_tiles_df)\n\n        for label_config in self.meaningful_location_labeling_config:\n            self.logger.info(f\"Calculating {label_config['label_code']} for ue tiles...\")\n\n            self.output_data_objects[SilverUsualEnvironmentLabelsDataObject.ID].read()\n            ue_tiles_df = self.output_data_objects[SilverUsualEnvironmentLabelsDataObject.ID].df\n            self.ue_tiles_df = ue_tiles_df.filter(\n                (F.col(ColNames.season) == self.season)\n                &amp; (F.col(ColNames.start_date) == self.start_date)\n                &amp; (F.col(ColNames.end_date) == self.end_date)\n                &amp; (F.col(ColNames.label) == \"ue\")\n            ).select(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id)\n\n            labeled_tiles_df = self.compute_generic_labeling(label_config, is_location_labeling=True)\n            self.write_label(labeled_tiles_df)\n\n    def write_label(self, labeled_tiles_df: DataFrame):\n        \"\"\"\n        Write the labeled tiles to the output data object.\n\n        Args:\n            labeled_tiles_df (DataFrame): labeled tiles dataset.\n        \"\"\"\n        labeled_tiles_df = labeled_tiles_df.withColumn(ColNames.season, F.lit(self.season))\n        labeled_tiles_df = labeled_tiles_df.withColumn(ColNames.start_date, F.lit(self.start_date))\n        labeled_tiles_df = labeled_tiles_df.withColumn(ColNames.end_date, F.lit(self.end_date))\n\n        # Repartition\n        if self.disaggregate_to_100m_grid:\n            self.logger.info(\"Dissagregating 200m to 100m grid\")\n            labeled_tiles_df = self.grid_gen.grid_id_from_coarser_resolution(\n                sdf=labeled_tiles_df,\n                coarse_resolution=200,\n                coarse_grid_id_col=ColNames.grid_id,\n                new_grid_id_col=ColNames.grid_id,\n            )\n        labeled_tiles_df = labeled_tiles_df.repartition(*SilverUsualEnvironmentLabelsDataObject.PARTITION_COLUMNS)\n\n        labeled_tiles_df = apply_schema_casting(labeled_tiles_df, SilverUsualEnvironmentLabelsDataObject.SCHEMA)\n\n        # save the output data object:\n        self.output_data_objects[SilverUsualEnvironmentLabelsDataObject.ID].df = labeled_tiles_df\n        self.output_data_objects[SilverUsualEnvironmentLabelsDataObject.ID].write()\n\n        # clear cache\n        self.spark.catalog.clearCache()\n\n    @staticmethod\n    def find_rarely_observed_devices(\n        total_observations_df: DataFrame, total_device_threshold: int, threshold_col: str\n    ) -&gt; DataFrame:\n        \"\"\"\n        Find devices (user ids) which match the condition for being considered \"rarely observed\".\n        This condition consists in having a total device observation of: lps &gt; total_ps_threshold.\n\n        Args:\n            total_observations_df (DataFrame): Long-Term Permanence Score dataset, filtered by the\n                corresponding start and end dates, and filtered by id_type == 'device_observation'.\n            total_device_threshold (int): ps threshold.\n            threshold_col (str): column to use for the threshold.\n\n        Returns:\n            DataFrame: two-column dataframe with the ids of the devices to discard and the corresponding user modulos.\n        \"\"\"\n        rarely_observed_user_ids = total_observations_df.filter(F.col(threshold_col) &lt; total_device_threshold).select(\n            ColNames.user_id_modulo, ColNames.user_id\n        )\n        return rarely_observed_user_ids\n\n    def discard_devices(self, ltps_df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Given a Long-Term Permanence Score dataset (filtered for the corresponding start and end dates),\n        discard the rows corresponding to some devices.\n\n        There are 2 type of devices to discard:\n            - rarely observed devices, based on LPS (long-term permanence score).\n            - discontinuously observed devices, based on frequency.\n\n        The user ids that are classified in any of these 2 groups are discarded from the Long-Term\n        Permanence Score dataset, and the number of discarded users of each kind is saved to the\n        corresponding attributes.\n\n        Args:\n            ltps_df (DataFrame): Long-Term Permanence Score dataset (filtered for the corresponding\n                start and end dates).\n\n        Returns:\n            DataFrame: Long-Term Permanence Score dataset (filtered for the corresponding start and\n                end dates), without the rows associated to rarely or discontinuously observed\n                devices.\n        \"\"\"\n        # Initial filter of ltps dataset to keep total device observation values:\n        total_observations_df = ltps_df.filter(\n            (F.col(ColNames.id_type) == UeGridIdType.DEVICE_OBSERVATION_STR)\n            &amp; (F.col(ColNames.season) == PeriodCombinations.ALL_PERIODS[0])\n            &amp; (F.col(ColNames.day_type) == PeriodCombinations.ALL_PERIODS[1])\n            &amp; (F.col(ColNames.time_interval) == PeriodCombinations.ALL_PERIODS[2])\n        )\n\n        # Rarely observed:\n        rarely_observed_user_ids = self.find_rarely_observed_devices(\n            total_observations_df, self.ps_threshold_for_rare_devices, ColNames.lps\n        )\n        self.rare_devices_count = self.rare_devices_count + rarely_observed_user_ids.count()\n        # Discontinuously observed:\n        discontinuously_observed_user_ids = self.find_rarely_observed_devices(\n            total_observations_df, self.freq_threshold_for_discontinuous_devices, ColNames.total_frequency\n        )\n        self.discontinuous_devices_count = self.discontinuous_devices_count + discontinuously_observed_user_ids.count()\n        # All user ids to discard:\n        discardable_user_ids = rarely_observed_user_ids.union(discontinuously_observed_user_ids).distinct()\n\n        # Filter dataset:\n        filtered_ltps_df = ltps_df.join(\n            discardable_user_ids, on=[ColNames.user_id_modulo, ColNames.user_id], how=\"left_anti\"\n        )\n\n        return filtered_ltps_df\n\n    def compute_generic_labeling(\n        self,\n        labeling_config,\n        is_location_labeling,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Generate the labeled tiles dataset for the specified label type. This function is generic and works for all\n        label types.\n\n        Args:\n            label_type (str): label type to compute: 'ue', 'home' or 'work'.\n            apply_ndays_filter (bool): Indicates if the final ndays frequency filter shall be applied in the computation\n                of the specified label type.\n\n        Returns:\n            DataFrame: labeled tiles dataset for the specified label type.\n        \"\"\"\n        label_code = labeling_config[\"label_code\"]\n        gap_ps_threshold = labeling_config[\"ps_difference_gap_filter\"][\"gap_permanence_threshold\"]\n        gap_ps_threshold_is_absolute = labeling_config[\"ps_difference_gap_filter\"][\"is_absolute\"]\n        gap_period_combinations = labeling_config[\"ps_difference_gap_filter\"][\"relevant_periods\"][0]\n\n        # filter ltps df for the current day type and time interval combination:\n        ltps_df = self.ltps_df.filter(\n            (F.col(ColNames.season) == gap_period_combinations[0])\n            &amp; (F.col(ColNames.day_type) == gap_period_combinations[1])\n            &amp; (F.col(ColNames.time_interval) == gap_period_combinations[2])\n        )\n\n        # if this is meaningful location labeling, use only tiles labeled as usual environment\n        if is_location_labeling:\n            ltps_df = ltps_df.join(self.ue_tiles_df, on=[ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id])\n\n        # cut tiles at gap to generate preselected tiles\n        preselected_tiles_df = self.cut_tiles_at_gap(ltps_df, gap_ps_threshold, gap_ps_threshold_is_absolute)\n\n        labeled_tiles_dfs_list = []\n        is_first_rule = True\n        for rule in labeling_config[\"labeling_rules\"]:\n            self.logger.info(f\"Applying rule: {rule['rule_code']}\")\n            rule_code = rule[\"rule_code\"]\n            rule_threshold = rule[\"threshold_value\"]\n            rule_check_type = rule[\"check_type\"]\n            rule_check_column = self.CHECK_TO_COLUMN[rule_check_type]\n            rule_apply_condition = rule[\"apply_condition\"]\n            rule_period_combinations = rule[\"relevant_periods\"][0]\n            # use only tiles which are still unlabeled after the previous rule\n            if not is_first_rule:\n                preselected_tiles_df = labeled_tiles_df.filter(F.col(\"is_labeled\") == False).select(\n                    ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id\n                )\n\n            # filter ltps df for the current day type and time interval combination:\n            ltps_df = self.ltps_df.filter(\n                (F.col(ColNames.season) == rule_period_combinations[0])\n                &amp; (F.col(ColNames.day_type) == rule_period_combinations[1])\n                &amp; (F.col(ColNames.time_interval) == rule_period_combinations[2])\n            )\n\n            device_observation_df = ltps_df.filter(\n                F.col(ColNames.id_type) == UeGridIdType.DEVICE_OBSERVATION_STR\n            ).select(ColNames.user_id_modulo, ColNames.user_id, rule_check_column)\n\n            # keep only the tiles that left after gap filtering and previous rules\n            labeled_tiles_df = ltps_df.join(\n                preselected_tiles_df.select(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id),\n                on=[ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id],\n                how=\"right\",\n            )\n            # might be the case that there are no inputs for current day type and time interval combination\n            # for tiles that were left from previous rules. To keep them for the next rule, need to coallesce\n            labeled_tiles_df = labeled_tiles_df.fillna({rule_check_column: 0})\n\n            # apply rule to preselected tiles\n            labeled_tiles_df = self.calculate_device_abs_threshold(\n                device_observation_df, labeled_tiles_df, rule_threshold, rule_check_column\n            )\n\n            labeled_tiles_df = labeled_tiles_df.withColumn(\n                \"is_labeled\", F.col(rule_check_column) &gt;= F.col(\"abs_threshold\")\n            )\n\n            labeled_tiles_df = labeled_tiles_df.withColumn(ColNames.label_rule, F.lit(rule_code))\n\n            labeled_tiles_df = labeled_tiles_df.select(\n                ColNames.user_id_modulo,\n                ColNames.user_id,\n                ColNames.grid_id,\n                ColNames.id_type,\n                F.col(\"is_labeled\"),\n                F.col(ColNames.label_rule),\n            )\n\n            labeled_tiles_df = labeled_tiles_df.persist()\n            labeled_tiles_df.count()\n            labeled_tiles_dfs_list.append(labeled_tiles_df.filter(F.col(\"is_labeled\")))\n            is_first_rule = False\n\n            # if apply condition is unlabeled device and at least some tiles are labeled stop the process\n            if rule_apply_condition == self.UNLABELED_DEVICES:\n                if labeled_tiles_df.filter(F.col(\"is_labeled\")).count() &gt; 0:\n                    break\n\n        labeled_tiles_df = reduce(DataFrame.unionAll, labeled_tiles_dfs_list)\n        labeled_tiles_df = labeled_tiles_df.withColumn(ColNames.label, F.lit(label_code))\n\n        return labeled_tiles_df\n\n    def cut_tiles_at_gap(self, ltps_df: DataFrame, gap_ps_threshold: float, threshold_is_absolute: bool) -&gt; DataFrame:\n        \"\"\"\n        Preprocessing function. Given a Long-Term Permanence Metrics dataset, for each user:\n        - sort the grid tiles of the user by LPS\n        - find a high difference (gap) in the LPS values between one grid tile and the next one,\n          where what is a high difference is defined through the gap_ps_threshold argument.\n        - for each agent, filter out all tiles after this high difference: the remaining tiles are\n          the \"pre-selected tiles\", which are the output of this function.\n\n        Args:\n            ltps_df (DataFrame): Long-Term Permanence Metrics dataset, for a specific day type and\n                time interval combination.\n            gap_ps_threshold (int/float): absolute/relative lps threshold.\n            threshold_is_absolute (bool): indicates if gap_ps_threshold is a relative/absolute value.\n\n        Returns:\n            DataFrame: dataset with the pre-selected tiles.\n        \"\"\"\n        ltps_df = ltps_df.filter(\n            (F.col(ColNames.id_type) == UeGridIdType.GRID_STR) | (F.col(ColNames.id_type) == UeGridIdType.ABROAD_STR)\n        )\n\n        window = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id).orderBy(F.desc(ColNames.lps))\n        cumulative_window = window.rowsBetween(Window.unboundedPreceding, Window.currentRow)\n\n        ltps_df = self.add_abs_ps_threshold(ltps_df, window, gap_ps_threshold, threshold_is_absolute)\n        pre_selected_tiles_df = (\n            ltps_df.withColumn(\"previous_lps\", F.lag(ColNames.lps, 1).over(window))\n            .withColumn(\"lps_difference\", F.col(\"previous_lps\") - F.col(ColNames.lps))\n            .fillna({\"lps_difference\": 0})\n            .withColumn(\"high_difference\", F.when(F.col(\"lps_difference\") &gt;= F.col(\"abs_ps_threshold\"), 1).otherwise(0))\n            .withColumn(\"cumulative_condition\", F.sum(F.col(\"high_difference\")).over(cumulative_window))\n            .filter(F.col(\"cumulative_condition\") == F.lit(0))\n            .drop(\"lps_difference\", \"previous_lps\", \"high_difference\", \"cumulative_condition\", \"abs_ps_threshold\")\n        )\n        return pre_selected_tiles_df.select(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id)\n\n    @staticmethod\n    def add_abs_ps_threshold(\n        ltps_df: DataFrame, window: Window, gap_ps_threshold: Union[int, float], threshold_is_absolute: bool\n    ) -&gt; DataFrame:\n        \"\"\"\n        Add \"abs_ps_threshold\" field to the ltps dataset.\n\n        If \"threshold_is_absolute\" is True, then just assign \"abs_ps_threshold\" = \"gap_ps_threshold\"\n        value to all registers.\n\n        If threshold_is_absolute is False, then reach the maximum value of lps for a grid tile for\n        each device (ps_max) and calculate \"abs_ps_threshold\" = \"gap_ps_threshold\" * \"ps_max\".\n\n        Args:\n            ltps_df (DataFrame): Long-Term Permanence Score dataset.\n            window (Window): window, partitioned by user id and ordered by lps.\n            gap_ps_threshold (int|float): absolute/relative lps threshold.\n            threshold_is_absolute (bool): indicates if gap_ps_threshold is a relative/absolute value.\n\n        Returns:\n            DataFrame: Long-Term Permanence Score dataset with the additional \"abs_ps_threshold\"\n                column.\n        \"\"\"\n        if threshold_is_absolute is True:\n            ltps_df = ltps_df.withColumn(\"abs_ps_threshold\", F.lit(gap_ps_threshold))\n        else:\n            ltps_df = (\n                ltps_df.withColumn(\"ps_max\", F.first(ColNames.lps).over(window))\n                .withColumn(\"abs_ps_threshold\", F.col(\"ps_max\") * F.lit(gap_ps_threshold / 100))\n                .drop(\"ps_max\")\n            )\n        return ltps_df\n\n    @staticmethod\n    def calculate_device_abs_threshold(\n        device_observation: DataFrame, target_rows_ltps_df: DataFrame, perc_threshold: float, threshold_col: str\n    ) -&gt; DataFrame:\n        \"\"\"\n        Calculate the total assigned long-term metric score for each device (total_device_ps) for a given day type\n        and time interval and add this information to an additional column of the provided dataset. Then, based on\n        this column, generate the absolute metric threshold to consider for each device by applying the corresponding\n        configured percentage (perc_ps_threshold).\n\n        Args:\n            device_observation (DataFrame): Long-Term Permanence Metrics dataset, for a specific day type and time interval\n                combination.\n            target_rows_ltps_df (DataFrame): Long-Term Permanence Metrics dataset, for a specific day type and time\n                interval combination, with one 'id_type' == 'grid'.\n            perc_threshold (float): specified ps threshold (in percentage).\n            threshold_col (str): column name of the total device value.\n\n        Returns:\n            DataFrame: Long-Term Permanence Metrics dataset, for a specific day type and time interval combination,\n                with one 'id_type' == 'grid', with an additional column named \"abs_ps_threshold\".\n        \"\"\"\n        device_observation = device_observation.withColumnRenamed(threshold_col, \"total_device_value\")\n        target_rows_ltps_df = (\n            target_rows_ltps_df.join(device_observation, on=[ColNames.user_id_modulo, ColNames.user_id])\n            .withColumn(\"abs_threshold\", F.col(\"total_device_value\") * F.lit(perc_threshold / 100))\n            .drop(\"total_device_value\")\n        )\n\n        return target_rows_ltps_df\n\n    def compute_quality_metrics(self) -&gt; DataFrame:\n        \"\"\"\n        Build usual environment labeling quality metrics dataframe.\n\n        Returns:\n            DataFrame: quality metrics dataframe.\n        \"\"\"\n        self.output_data_objects[SilverUsualEnvironmentLabelsDataObject.ID].read()\n        labeled_tiles_df = self.output_data_objects[SilverUsualEnvironmentLabelsDataObject.ID].df\n        labeled_tiles_df = labeled_tiles_df.filter(\n            (F.col(ColNames.season) == self.season)\n            &amp; (F.col(ColNames.start_date) == self.start_date)\n            &amp; (F.col(ColNames.end_date) == self.end_date)\n        ).select(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id, ColNames.label, ColNames.label_rule)\n\n        # To get number of tiles not labeled as ue and meanigful locations, we need to get count of all tiles per user\n        all_tiles_df = self.ltps_df.filter(\n            (F.col(ColNames.day_type) == PeriodCombinations.ALL_PERIODS[1])\n            &amp; (F.col(ColNames.time_interval) == PeriodCombinations.ALL_PERIODS[2])\n            &amp; (F.col(ColNames.id_type) != UeGridIdType.DEVICE_OBSERVATION_STR)\n        ).select(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id)\n\n        all_tiles_count_df = (\n            all_tiles_df.groupBy(ColNames.user_id_modulo, ColNames.user_id)\n            .count()\n            .withColumnRenamed(\"count\", \"all_tiles\")\n        )\n\n        # Mark all ue and meaningful locations rules as loc_na and ue_na\n        grouped_df_location = labeled_tiles_df.withColumn(\n            ColNames.label_rule, F.when(F.col(ColNames.label) != \"ue\", \"loc_na\").otherwise(\"ue_na\")\n        )\n\n        # Count number of tiles per user per ue and meaningful locations\n        grouped_df_location = grouped_df_location.groupBy(ColNames.user_id_modulo, ColNames.user_id, \"label_rule\").agg(\n            F.count_distinct(ColNames.grid_id).alias(\"location_count\")\n        )\n\n        # Join all tiles count with location count to get count of tiles not labeled as ue and meaningful locations\n        grouped_df_location = grouped_df_location.join(\n            all_tiles_count_df, on=[ColNames.user_id_modulo, ColNames.user_id], how=\"outer\"\n        )\n        grouped_df_location = grouped_df_location.withColumn(\n            \"count\", F.col(\"all_tiles\") - F.col(\"location_count\")\n        ).drop(\"all_tiles\", \"location_count\")\n\n        # Count labeled tiles for separate rules\n        grouped_df = labeled_tiles_df.groupby(\n            ColNames.user_id_modulo, ColNames.user_id, ColNames.label, ColNames.label_rule\n        ).count()\n\n        device_quality_metrics_df = grouped_df.drop(\"label\").unionByName(grouped_df_location)\n\n        device_quality_metrics_df = device_quality_metrics_df.groupBy(ColNames.label_rule).agg(\n            F.sum(\"count\").alias(ColNames.labeling_quality_count),\n            F.min(\"count\").alias(ColNames.labeling_quality_min),\n            F.max(\"count\").alias(ColNames.labeling_quality_max),\n            F.avg(\"count\").alias(ColNames.labeling_quality_avg),\n        )\n\n        # Add device observation and ue abroad metrics\n        abroad_ue_count = (\n            labeled_tiles_df.filter(F.col(ColNames.id_type) == UeGridIdType.ABROAD_STR)\n            .select(ColNames.user_id)\n            .distinct()\n            .count()\n        )\n        other_rule_metrics_df = self.spark.createDataFrame(\n            [\n                (\"device_filter_1\", self.rare_devices_count, 0, 0, 0),\n                (\"device_filter_2\", self.discontinuous_devices_count, 0, 0, 0),\n                (\"ue_abroad\", abroad_ue_count, 0, 0, 0),\n            ],\n            [\n                ColNames.label_rule,\n                ColNames.labeling_quality_count,\n                ColNames.labeling_quality_min,\n                ColNames.labeling_quality_max,\n                ColNames.labeling_quality_avg,\n            ],\n        )\n\n        device_quality_metrics_df = device_quality_metrics_df.unionByName(other_rule_metrics_df)\n\n        device_quality_metrics_df = device_quality_metrics_df.withColumns(\n            {ColNames.start_date: F.lit(self.start_date), ColNames.end_date: F.lit(self.end_date)}\n        ).withColumnRenamed(ColNames.label_rule, ColNames.labeling_quality_metric)\n\n        device_quality_metrics_df = apply_schema_casting(\n            device_quality_metrics_df, SilverUsualEnvironmentLabelingQualityMetricsDataObject.SCHEMA\n        )\n\n        return device_quality_metrics_df\n\n    def _get_partition_chunks(self) -&gt; List[List[int]]:\n        \"\"\"\n        Method that returns the partition chunks for the current date.\n\n        Returns:\n            List[List[int, int]]: list of partition chunks. If the partition_chunk_size is not defined in the config or\n                the number of partitions is less than the desired chunk size, it will return a list with a single None element.\n        \"\"\"\n        # Get partitions desired\n        partition_chunk_size = self.config.getint(self.COMPONENT_ID, \"partition_chunk_size\", fallback=None)\n        number_of_partitions = self.config.getint(self.COMPONENT_ID, \"number_of_partitions\", fallback=None)\n\n        if partition_chunk_size is None or number_of_partitions is None or partition_chunk_size &lt;= 0:\n            return [None]\n\n        if number_of_partitions &lt;= partition_chunk_size:\n            self.logger.warning(\n                f\"Available Partition number ({number_of_partitions}) is \"\n                f\"less than the desired chunk size ({partition_chunk_size}). \"\n                f\"Using all partitions.\"\n            )\n            return [None]\n        partition_chunks = [\n            list(range(i, min(i + partition_chunk_size, number_of_partitions)))\n            for i in range(0, number_of_partitions, partition_chunk_size)\n        ]\n        # NOTE: Generate chunks if partition_values were read for each day\n        # getting exactly the amount of partitions for that day\n\n        # partition_chunks = [\n        #     partition_values[i : i + partition_chunk_size]\n        #     for i in range(0, partition_values_size, partition_chunk_size)\n        # ]\n\n        return partition_chunks\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.add_abs_ps_threshold","title":"<code>add_abs_ps_threshold(ltps_df, window, gap_ps_threshold, threshold_is_absolute)</code>  <code>staticmethod</code>","text":"<p>Add \"abs_ps_threshold\" field to the ltps dataset.</p> <p>If \"threshold_is_absolute\" is True, then just assign \"abs_ps_threshold\" = \"gap_ps_threshold\" value to all registers.</p> <p>If threshold_is_absolute is False, then reach the maximum value of lps for a grid tile for each device (ps_max) and calculate \"abs_ps_threshold\" = \"gap_ps_threshold\" * \"ps_max\".</p> <p>Parameters:</p> Name Type Description Default <code>ltps_df</code> <code>DataFrame</code> <p>Long-Term Permanence Score dataset.</p> required <code>window</code> <code>Window</code> <p>window, partitioned by user id and ordered by lps.</p> required <code>gap_ps_threshold</code> <code>int | float</code> <p>absolute/relative lps threshold.</p> required <code>threshold_is_absolute</code> <code>bool</code> <p>indicates if gap_ps_threshold is a relative/absolute value.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Long-Term Permanence Score dataset with the additional \"abs_ps_threshold\" column.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>@staticmethod\ndef add_abs_ps_threshold(\n    ltps_df: DataFrame, window: Window, gap_ps_threshold: Union[int, float], threshold_is_absolute: bool\n) -&gt; DataFrame:\n    \"\"\"\n    Add \"abs_ps_threshold\" field to the ltps dataset.\n\n    If \"threshold_is_absolute\" is True, then just assign \"abs_ps_threshold\" = \"gap_ps_threshold\"\n    value to all registers.\n\n    If threshold_is_absolute is False, then reach the maximum value of lps for a grid tile for\n    each device (ps_max) and calculate \"abs_ps_threshold\" = \"gap_ps_threshold\" * \"ps_max\".\n\n    Args:\n        ltps_df (DataFrame): Long-Term Permanence Score dataset.\n        window (Window): window, partitioned by user id and ordered by lps.\n        gap_ps_threshold (int|float): absolute/relative lps threshold.\n        threshold_is_absolute (bool): indicates if gap_ps_threshold is a relative/absolute value.\n\n    Returns:\n        DataFrame: Long-Term Permanence Score dataset with the additional \"abs_ps_threshold\"\n            column.\n    \"\"\"\n    if threshold_is_absolute is True:\n        ltps_df = ltps_df.withColumn(\"abs_ps_threshold\", F.lit(gap_ps_threshold))\n    else:\n        ltps_df = (\n            ltps_df.withColumn(\"ps_max\", F.first(ColNames.lps).over(window))\n            .withColumn(\"abs_ps_threshold\", F.col(\"ps_max\") * F.lit(gap_ps_threshold / 100))\n            .drop(\"ps_max\")\n        )\n    return ltps_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.calculate_device_abs_threshold","title":"<code>calculate_device_abs_threshold(device_observation, target_rows_ltps_df, perc_threshold, threshold_col)</code>  <code>staticmethod</code>","text":"<p>Calculate the total assigned long-term metric score for each device (total_device_ps) for a given day type and time interval and add this information to an additional column of the provided dataset. Then, based on this column, generate the absolute metric threshold to consider for each device by applying the corresponding configured percentage (perc_ps_threshold).</p> <p>Parameters:</p> Name Type Description Default <code>device_observation</code> <code>DataFrame</code> <p>Long-Term Permanence Metrics dataset, for a specific day type and time interval combination.</p> required <code>target_rows_ltps_df</code> <code>DataFrame</code> <p>Long-Term Permanence Metrics dataset, for a specific day type and time interval combination, with one 'id_type' == 'grid'.</p> required <code>perc_threshold</code> <code>float</code> <p>specified ps threshold (in percentage).</p> required <code>threshold_col</code> <code>str</code> <p>column name of the total device value.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Long-Term Permanence Metrics dataset, for a specific day type and time interval combination, with one 'id_type' == 'grid', with an additional column named \"abs_ps_threshold\".</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>@staticmethod\ndef calculate_device_abs_threshold(\n    device_observation: DataFrame, target_rows_ltps_df: DataFrame, perc_threshold: float, threshold_col: str\n) -&gt; DataFrame:\n    \"\"\"\n    Calculate the total assigned long-term metric score for each device (total_device_ps) for a given day type\n    and time interval and add this information to an additional column of the provided dataset. Then, based on\n    this column, generate the absolute metric threshold to consider for each device by applying the corresponding\n    configured percentage (perc_ps_threshold).\n\n    Args:\n        device_observation (DataFrame): Long-Term Permanence Metrics dataset, for a specific day type and time interval\n            combination.\n        target_rows_ltps_df (DataFrame): Long-Term Permanence Metrics dataset, for a specific day type and time\n            interval combination, with one 'id_type' == 'grid'.\n        perc_threshold (float): specified ps threshold (in percentage).\n        threshold_col (str): column name of the total device value.\n\n    Returns:\n        DataFrame: Long-Term Permanence Metrics dataset, for a specific day type and time interval combination,\n            with one 'id_type' == 'grid', with an additional column named \"abs_ps_threshold\".\n    \"\"\"\n    device_observation = device_observation.withColumnRenamed(threshold_col, \"total_device_value\")\n    target_rows_ltps_df = (\n        target_rows_ltps_df.join(device_observation, on=[ColNames.user_id_modulo, ColNames.user_id])\n        .withColumn(\"abs_threshold\", F.col(\"total_device_value\") * F.lit(perc_threshold / 100))\n        .drop(\"total_device_value\")\n    )\n\n    return target_rows_ltps_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.check_needed_day_and_interval_types","title":"<code>check_needed_day_and_interval_types(ltps_df, day_and_interval_type_combinations)</code>  <code>staticmethod</code>","text":"<p>Method that checks if the needed combinations of day type and interval type are available in the provided dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ltps_df</code> <code>DataFrame</code> <p>provided Long-Term Permanence Score dataset</p> required <code>day_and_interval_type_combinations</code> <code>Dict[str, List[Tuple[str, str]]]</code> <p>day type and interval type combinations that are needed for the execution of the method.</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If there is no data for one or more of the needed combinations of day type and interval type.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>@staticmethod\ndef check_needed_day_and_interval_types(\n    ltps_df: DataFrame, day_and_interval_type_combinations: Set[Tuple[str, str, str]]\n):\n    \"\"\"\n    Method that checks if the needed combinations of day type and interval type are available\n    in the provided dataset.\n\n    Args:\n        ltps_df (DataFrame): provided Long-Term Permanence Score dataset\n        day_and_interval_type_combinations (Dict[str,List[Tuple[str,str]]]): day type and interval type\n            combinations that are needed for the execution of the method.\n\n    Raises:\n        FileNotFoundError: If there is no data for one or more of the needed combinations of day type\n            and interval type.\n    \"\"\"\n    # Assert that these combinations appear at least once in the input Long-Term Permanence\n    # Score data object:\n    for season, day_type, time_interval in day_and_interval_type_combinations:\n        filtered_ltps_df = ltps_df.filter(\n            (F.col(ColNames.season) == season)\n            &amp; (F.col(ColNames.day_type) == day_type)\n            &amp; (F.col(ColNames.time_interval) == time_interval)\n        )\n\n        data_exists = filtered_ltps_df.count() &gt; 0\n        if not data_exists:\n            raise FileNotFoundError(\n                \"No Long-term Permanence Score data has been found for \"\n                f\"day_type `{day_type}` and time_interval `{time_interval}`\"\n            )\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.compute_generic_labeling","title":"<code>compute_generic_labeling(labeling_config, is_location_labeling)</code>","text":"<p>Generate the labeled tiles dataset for the specified label type. This function is generic and works for all label types.</p> <p>Parameters:</p> Name Type Description Default <code>label_type</code> <code>str</code> <p>label type to compute: 'ue', 'home' or 'work'.</p> required <code>apply_ndays_filter</code> <code>bool</code> <p>Indicates if the final ndays frequency filter shall be applied in the computation of the specified label type.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>labeled tiles dataset for the specified label type.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>def compute_generic_labeling(\n    self,\n    labeling_config,\n    is_location_labeling,\n) -&gt; DataFrame:\n    \"\"\"\n    Generate the labeled tiles dataset for the specified label type. This function is generic and works for all\n    label types.\n\n    Args:\n        label_type (str): label type to compute: 'ue', 'home' or 'work'.\n        apply_ndays_filter (bool): Indicates if the final ndays frequency filter shall be applied in the computation\n            of the specified label type.\n\n    Returns:\n        DataFrame: labeled tiles dataset for the specified label type.\n    \"\"\"\n    label_code = labeling_config[\"label_code\"]\n    gap_ps_threshold = labeling_config[\"ps_difference_gap_filter\"][\"gap_permanence_threshold\"]\n    gap_ps_threshold_is_absolute = labeling_config[\"ps_difference_gap_filter\"][\"is_absolute\"]\n    gap_period_combinations = labeling_config[\"ps_difference_gap_filter\"][\"relevant_periods\"][0]\n\n    # filter ltps df for the current day type and time interval combination:\n    ltps_df = self.ltps_df.filter(\n        (F.col(ColNames.season) == gap_period_combinations[0])\n        &amp; (F.col(ColNames.day_type) == gap_period_combinations[1])\n        &amp; (F.col(ColNames.time_interval) == gap_period_combinations[2])\n    )\n\n    # if this is meaningful location labeling, use only tiles labeled as usual environment\n    if is_location_labeling:\n        ltps_df = ltps_df.join(self.ue_tiles_df, on=[ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id])\n\n    # cut tiles at gap to generate preselected tiles\n    preselected_tiles_df = self.cut_tiles_at_gap(ltps_df, gap_ps_threshold, gap_ps_threshold_is_absolute)\n\n    labeled_tiles_dfs_list = []\n    is_first_rule = True\n    for rule in labeling_config[\"labeling_rules\"]:\n        self.logger.info(f\"Applying rule: {rule['rule_code']}\")\n        rule_code = rule[\"rule_code\"]\n        rule_threshold = rule[\"threshold_value\"]\n        rule_check_type = rule[\"check_type\"]\n        rule_check_column = self.CHECK_TO_COLUMN[rule_check_type]\n        rule_apply_condition = rule[\"apply_condition\"]\n        rule_period_combinations = rule[\"relevant_periods\"][0]\n        # use only tiles which are still unlabeled after the previous rule\n        if not is_first_rule:\n            preselected_tiles_df = labeled_tiles_df.filter(F.col(\"is_labeled\") == False).select(\n                ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id\n            )\n\n        # filter ltps df for the current day type and time interval combination:\n        ltps_df = self.ltps_df.filter(\n            (F.col(ColNames.season) == rule_period_combinations[0])\n            &amp; (F.col(ColNames.day_type) == rule_period_combinations[1])\n            &amp; (F.col(ColNames.time_interval) == rule_period_combinations[2])\n        )\n\n        device_observation_df = ltps_df.filter(\n            F.col(ColNames.id_type) == UeGridIdType.DEVICE_OBSERVATION_STR\n        ).select(ColNames.user_id_modulo, ColNames.user_id, rule_check_column)\n\n        # keep only the tiles that left after gap filtering and previous rules\n        labeled_tiles_df = ltps_df.join(\n            preselected_tiles_df.select(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id),\n            on=[ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id],\n            how=\"right\",\n        )\n        # might be the case that there are no inputs for current day type and time interval combination\n        # for tiles that were left from previous rules. To keep them for the next rule, need to coallesce\n        labeled_tiles_df = labeled_tiles_df.fillna({rule_check_column: 0})\n\n        # apply rule to preselected tiles\n        labeled_tiles_df = self.calculate_device_abs_threshold(\n            device_observation_df, labeled_tiles_df, rule_threshold, rule_check_column\n        )\n\n        labeled_tiles_df = labeled_tiles_df.withColumn(\n            \"is_labeled\", F.col(rule_check_column) &gt;= F.col(\"abs_threshold\")\n        )\n\n        labeled_tiles_df = labeled_tiles_df.withColumn(ColNames.label_rule, F.lit(rule_code))\n\n        labeled_tiles_df = labeled_tiles_df.select(\n            ColNames.user_id_modulo,\n            ColNames.user_id,\n            ColNames.grid_id,\n            ColNames.id_type,\n            F.col(\"is_labeled\"),\n            F.col(ColNames.label_rule),\n        )\n\n        labeled_tiles_df = labeled_tiles_df.persist()\n        labeled_tiles_df.count()\n        labeled_tiles_dfs_list.append(labeled_tiles_df.filter(F.col(\"is_labeled\")))\n        is_first_rule = False\n\n        # if apply condition is unlabeled device and at least some tiles are labeled stop the process\n        if rule_apply_condition == self.UNLABELED_DEVICES:\n            if labeled_tiles_df.filter(F.col(\"is_labeled\")).count() &gt; 0:\n                break\n\n    labeled_tiles_df = reduce(DataFrame.unionAll, labeled_tiles_dfs_list)\n    labeled_tiles_df = labeled_tiles_df.withColumn(ColNames.label, F.lit(label_code))\n\n    return labeled_tiles_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.compute_quality_metrics","title":"<code>compute_quality_metrics()</code>","text":"<p>Build usual environment labeling quality metrics dataframe.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>quality metrics dataframe.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>def compute_quality_metrics(self) -&gt; DataFrame:\n    \"\"\"\n    Build usual environment labeling quality metrics dataframe.\n\n    Returns:\n        DataFrame: quality metrics dataframe.\n    \"\"\"\n    self.output_data_objects[SilverUsualEnvironmentLabelsDataObject.ID].read()\n    labeled_tiles_df = self.output_data_objects[SilverUsualEnvironmentLabelsDataObject.ID].df\n    labeled_tiles_df = labeled_tiles_df.filter(\n        (F.col(ColNames.season) == self.season)\n        &amp; (F.col(ColNames.start_date) == self.start_date)\n        &amp; (F.col(ColNames.end_date) == self.end_date)\n    ).select(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id, ColNames.label, ColNames.label_rule)\n\n    # To get number of tiles not labeled as ue and meanigful locations, we need to get count of all tiles per user\n    all_tiles_df = self.ltps_df.filter(\n        (F.col(ColNames.day_type) == PeriodCombinations.ALL_PERIODS[1])\n        &amp; (F.col(ColNames.time_interval) == PeriodCombinations.ALL_PERIODS[2])\n        &amp; (F.col(ColNames.id_type) != UeGridIdType.DEVICE_OBSERVATION_STR)\n    ).select(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id)\n\n    all_tiles_count_df = (\n        all_tiles_df.groupBy(ColNames.user_id_modulo, ColNames.user_id)\n        .count()\n        .withColumnRenamed(\"count\", \"all_tiles\")\n    )\n\n    # Mark all ue and meaningful locations rules as loc_na and ue_na\n    grouped_df_location = labeled_tiles_df.withColumn(\n        ColNames.label_rule, F.when(F.col(ColNames.label) != \"ue\", \"loc_na\").otherwise(\"ue_na\")\n    )\n\n    # Count number of tiles per user per ue and meaningful locations\n    grouped_df_location = grouped_df_location.groupBy(ColNames.user_id_modulo, ColNames.user_id, \"label_rule\").agg(\n        F.count_distinct(ColNames.grid_id).alias(\"location_count\")\n    )\n\n    # Join all tiles count with location count to get count of tiles not labeled as ue and meaningful locations\n    grouped_df_location = grouped_df_location.join(\n        all_tiles_count_df, on=[ColNames.user_id_modulo, ColNames.user_id], how=\"outer\"\n    )\n    grouped_df_location = grouped_df_location.withColumn(\n        \"count\", F.col(\"all_tiles\") - F.col(\"location_count\")\n    ).drop(\"all_tiles\", \"location_count\")\n\n    # Count labeled tiles for separate rules\n    grouped_df = labeled_tiles_df.groupby(\n        ColNames.user_id_modulo, ColNames.user_id, ColNames.label, ColNames.label_rule\n    ).count()\n\n    device_quality_metrics_df = grouped_df.drop(\"label\").unionByName(grouped_df_location)\n\n    device_quality_metrics_df = device_quality_metrics_df.groupBy(ColNames.label_rule).agg(\n        F.sum(\"count\").alias(ColNames.labeling_quality_count),\n        F.min(\"count\").alias(ColNames.labeling_quality_min),\n        F.max(\"count\").alias(ColNames.labeling_quality_max),\n        F.avg(\"count\").alias(ColNames.labeling_quality_avg),\n    )\n\n    # Add device observation and ue abroad metrics\n    abroad_ue_count = (\n        labeled_tiles_df.filter(F.col(ColNames.id_type) == UeGridIdType.ABROAD_STR)\n        .select(ColNames.user_id)\n        .distinct()\n        .count()\n    )\n    other_rule_metrics_df = self.spark.createDataFrame(\n        [\n            (\"device_filter_1\", self.rare_devices_count, 0, 0, 0),\n            (\"device_filter_2\", self.discontinuous_devices_count, 0, 0, 0),\n            (\"ue_abroad\", abroad_ue_count, 0, 0, 0),\n        ],\n        [\n            ColNames.label_rule,\n            ColNames.labeling_quality_count,\n            ColNames.labeling_quality_min,\n            ColNames.labeling_quality_max,\n            ColNames.labeling_quality_avg,\n        ],\n    )\n\n    device_quality_metrics_df = device_quality_metrics_df.unionByName(other_rule_metrics_df)\n\n    device_quality_metrics_df = device_quality_metrics_df.withColumns(\n        {ColNames.start_date: F.lit(self.start_date), ColNames.end_date: F.lit(self.end_date)}\n    ).withColumnRenamed(ColNames.label_rule, ColNames.labeling_quality_metric)\n\n    device_quality_metrics_df = apply_schema_casting(\n        device_quality_metrics_df, SilverUsualEnvironmentLabelingQualityMetricsDataObject.SCHEMA\n    )\n\n    return device_quality_metrics_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.cut_tiles_at_gap","title":"<code>cut_tiles_at_gap(ltps_df, gap_ps_threshold, threshold_is_absolute)</code>","text":"<p>Preprocessing function. Given a Long-Term Permanence Metrics dataset, for each user: - sort the grid tiles of the user by LPS - find a high difference (gap) in the LPS values between one grid tile and the next one,   where what is a high difference is defined through the gap_ps_threshold argument. - for each agent, filter out all tiles after this high difference: the remaining tiles are   the \"pre-selected tiles\", which are the output of this function.</p> <p>Parameters:</p> Name Type Description Default <code>ltps_df</code> <code>DataFrame</code> <p>Long-Term Permanence Metrics dataset, for a specific day type and time interval combination.</p> required <code>gap_ps_threshold</code> <code>int / float</code> <p>absolute/relative lps threshold.</p> required <code>threshold_is_absolute</code> <code>bool</code> <p>indicates if gap_ps_threshold is a relative/absolute value.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>dataset with the pre-selected tiles.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>def cut_tiles_at_gap(self, ltps_df: DataFrame, gap_ps_threshold: float, threshold_is_absolute: bool) -&gt; DataFrame:\n    \"\"\"\n    Preprocessing function. Given a Long-Term Permanence Metrics dataset, for each user:\n    - sort the grid tiles of the user by LPS\n    - find a high difference (gap) in the LPS values between one grid tile and the next one,\n      where what is a high difference is defined through the gap_ps_threshold argument.\n    - for each agent, filter out all tiles after this high difference: the remaining tiles are\n      the \"pre-selected tiles\", which are the output of this function.\n\n    Args:\n        ltps_df (DataFrame): Long-Term Permanence Metrics dataset, for a specific day type and\n            time interval combination.\n        gap_ps_threshold (int/float): absolute/relative lps threshold.\n        threshold_is_absolute (bool): indicates if gap_ps_threshold is a relative/absolute value.\n\n    Returns:\n        DataFrame: dataset with the pre-selected tiles.\n    \"\"\"\n    ltps_df = ltps_df.filter(\n        (F.col(ColNames.id_type) == UeGridIdType.GRID_STR) | (F.col(ColNames.id_type) == UeGridIdType.ABROAD_STR)\n    )\n\n    window = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id).orderBy(F.desc(ColNames.lps))\n    cumulative_window = window.rowsBetween(Window.unboundedPreceding, Window.currentRow)\n\n    ltps_df = self.add_abs_ps_threshold(ltps_df, window, gap_ps_threshold, threshold_is_absolute)\n    pre_selected_tiles_df = (\n        ltps_df.withColumn(\"previous_lps\", F.lag(ColNames.lps, 1).over(window))\n        .withColumn(\"lps_difference\", F.col(\"previous_lps\") - F.col(ColNames.lps))\n        .fillna({\"lps_difference\": 0})\n        .withColumn(\"high_difference\", F.when(F.col(\"lps_difference\") &gt;= F.col(\"abs_ps_threshold\"), 1).otherwise(0))\n        .withColumn(\"cumulative_condition\", F.sum(F.col(\"high_difference\")).over(cumulative_window))\n        .filter(F.col(\"cumulative_condition\") == F.lit(0))\n        .drop(\"lps_difference\", \"previous_lps\", \"high_difference\", \"cumulative_condition\", \"abs_ps_threshold\")\n    )\n    return pre_selected_tiles_df.select(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id)\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.discard_devices","title":"<code>discard_devices(ltps_df)</code>","text":"<p>Given a Long-Term Permanence Score dataset (filtered for the corresponding start and end dates), discard the rows corresponding to some devices.</p> There are 2 type of devices to discard <ul> <li>rarely observed devices, based on LPS (long-term permanence score).</li> <li>discontinuously observed devices, based on frequency.</li> </ul> <p>The user ids that are classified in any of these 2 groups are discarded from the Long-Term Permanence Score dataset, and the number of discarded users of each kind is saved to the corresponding attributes.</p> <p>Parameters:</p> Name Type Description Default <code>ltps_df</code> <code>DataFrame</code> <p>Long-Term Permanence Score dataset (filtered for the corresponding start and end dates).</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Long-Term Permanence Score dataset (filtered for the corresponding start and end dates), without the rows associated to rarely or discontinuously observed devices.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>def discard_devices(self, ltps_df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Given a Long-Term Permanence Score dataset (filtered for the corresponding start and end dates),\n    discard the rows corresponding to some devices.\n\n    There are 2 type of devices to discard:\n        - rarely observed devices, based on LPS (long-term permanence score).\n        - discontinuously observed devices, based on frequency.\n\n    The user ids that are classified in any of these 2 groups are discarded from the Long-Term\n    Permanence Score dataset, and the number of discarded users of each kind is saved to the\n    corresponding attributes.\n\n    Args:\n        ltps_df (DataFrame): Long-Term Permanence Score dataset (filtered for the corresponding\n            start and end dates).\n\n    Returns:\n        DataFrame: Long-Term Permanence Score dataset (filtered for the corresponding start and\n            end dates), without the rows associated to rarely or discontinuously observed\n            devices.\n    \"\"\"\n    # Initial filter of ltps dataset to keep total device observation values:\n    total_observations_df = ltps_df.filter(\n        (F.col(ColNames.id_type) == UeGridIdType.DEVICE_OBSERVATION_STR)\n        &amp; (F.col(ColNames.season) == PeriodCombinations.ALL_PERIODS[0])\n        &amp; (F.col(ColNames.day_type) == PeriodCombinations.ALL_PERIODS[1])\n        &amp; (F.col(ColNames.time_interval) == PeriodCombinations.ALL_PERIODS[2])\n    )\n\n    # Rarely observed:\n    rarely_observed_user_ids = self.find_rarely_observed_devices(\n        total_observations_df, self.ps_threshold_for_rare_devices, ColNames.lps\n    )\n    self.rare_devices_count = self.rare_devices_count + rarely_observed_user_ids.count()\n    # Discontinuously observed:\n    discontinuously_observed_user_ids = self.find_rarely_observed_devices(\n        total_observations_df, self.freq_threshold_for_discontinuous_devices, ColNames.total_frequency\n    )\n    self.discontinuous_devices_count = self.discontinuous_devices_count + discontinuously_observed_user_ids.count()\n    # All user ids to discard:\n    discardable_user_ids = rarely_observed_user_ids.union(discontinuously_observed_user_ids).distinct()\n\n    # Filter dataset:\n    filtered_ltps_df = ltps_df.join(\n        discardable_user_ids, on=[ColNames.user_id_modulo, ColNames.user_id], how=\"left_anti\"\n    )\n\n    return filtered_ltps_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.filter_ltps_by_target_dates","title":"<code>filter_ltps_by_target_dates(full_ltps_df, start_date, end_date, season)</code>  <code>staticmethod</code>","text":"<p>Keep only the rows of the input SilverLongtermPermanenceScoreDataObject in which the start date, end date and season match the ones specified for the processing of this method.</p> <p>Parameters:</p> Name Type Description Default <code>full_ltps_df</code> <code>DataFrame</code> <p>full dataset.</p> required <code>start_date</code> <code>date</code> <p>specified target start date for the execution of the method.</p> required <code>end_date</code> <code>date</code> <p>specified target end date for the execution of the method.</p> required <code>season</code> <code>str</code> <p>specified target season for the execution of the method.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>filtered dataset.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>@staticmethod\ndef filter_ltps_by_target_dates(\n    full_ltps_df: DataFrame, start_date: dt.date, end_date: dt.date, season: str\n) -&gt; DataFrame:\n    \"\"\"\n    Keep only the rows of the input SilverLongtermPermanenceScoreDataObject in which the start\n    date, end date and season match the ones specified for the processing of this method.\n\n    Args:\n        full_ltps_df (DataFrame): full dataset.\n        start_date (dt.date): specified target start date for the execution of the method.\n        end_date (dt.date): specified target end date for the execution of the method.\n        season (str): specified target season for the execution of the method.\n\n    Returns:\n        DataFrame: filtered dataset.\n    \"\"\"\n    filtered_ltps_df = full_ltps_df.filter(\n        (F.col(ColNames.start_date) == start_date)\n        &amp; (F.col(ColNames.end_date) == end_date)\n        &amp; (F.col(ColNames.season) == season)\n    ).select(\n        ColNames.user_id,\n        ColNames.grid_id,\n        ColNames.lps,\n        ColNames.total_frequency,\n        ColNames.season,\n        ColNames.day_type,\n        ColNames.time_interval,\n        ColNames.user_id_modulo,\n        ColNames.id_type,\n    )\n    return filtered_ltps_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.find_rarely_observed_devices","title":"<code>find_rarely_observed_devices(total_observations_df, total_device_threshold, threshold_col)</code>  <code>staticmethod</code>","text":"<p>Find devices (user ids) which match the condition for being considered \"rarely observed\". This condition consists in having a total device observation of: lps &gt; total_ps_threshold.</p> <p>Parameters:</p> Name Type Description Default <code>total_observations_df</code> <code>DataFrame</code> <p>Long-Term Permanence Score dataset, filtered by the corresponding start and end dates, and filtered by id_type == 'device_observation'.</p> required <code>total_device_threshold</code> <code>int</code> <p>ps threshold.</p> required <code>threshold_col</code> <code>str</code> <p>column to use for the threshold.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>two-column dataframe with the ids of the devices to discard and the corresponding user modulos.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>@staticmethod\ndef find_rarely_observed_devices(\n    total_observations_df: DataFrame, total_device_threshold: int, threshold_col: str\n) -&gt; DataFrame:\n    \"\"\"\n    Find devices (user ids) which match the condition for being considered \"rarely observed\".\n    This condition consists in having a total device observation of: lps &gt; total_ps_threshold.\n\n    Args:\n        total_observations_df (DataFrame): Long-Term Permanence Score dataset, filtered by the\n            corresponding start and end dates, and filtered by id_type == 'device_observation'.\n        total_device_threshold (int): ps threshold.\n        threshold_col (str): column to use for the threshold.\n\n    Returns:\n        DataFrame: two-column dataframe with the ids of the devices to discard and the corresponding user modulos.\n    \"\"\"\n    rarely_observed_user_ids = total_observations_df.filter(F.col(threshold_col) &lt; total_device_threshold).select(\n        ColNames.user_id_modulo, ColNames.user_id\n    )\n    return rarely_observed_user_ids\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.get_labeling_config","title":"<code>get_labeling_config()</code>","text":"<p>Get the configuration of the labeling process.</p> <p>Returns:</p> Type Description <code>dict</code> <p>configuration of the labeling process.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>def get_labeling_config(self):\n    \"\"\"\n    Get the configuration of the labeling process.\n\n    Returns:\n        (dict): configuration of the labeling process.\n    \"\"\"\n    ue_labels = {\n        \"label_code\": \"ue\",\n        \"ps_difference_gap_filter\": {\n            \"relevant_periods\": [PeriodCombinations.ALL_PERIODS],\n            \"gap_permanence_threshold\": self.gap_ps_thresholds[\"ue\"],\n            \"is_absolute\": self.gap_ps_threshold_is_absolute[\"ue\"],\n        },\n        \"labeling_rules\": [\n            {\n                \"rule_code\": \"ue_1\",\n                \"check_type\": self.TOTAL_PERMANENCE_THRESHOLD,\n                \"threshold_value\": self.ps_thresholds[\"ue\"],\n                \"relevant_periods\": [PeriodCombinations.ALL_PERIODS],\n                \"apply_condition\": self.UNLABELED_TILES,\n            },\n            {\n                \"rule_code\": \"ue_2\",\n                \"check_type\": self.TOTAL_PERMANENCE_THRESHOLD,\n                \"threshold_value\": self.ps_thresholds[\"ue\"],\n                \"relevant_periods\": [\n                    PeriodCombinations.NIGHT_TIME_ALL,\n                ],\n                \"apply_condition\": self.UNLABELED_TILES,\n            },\n            {\n                \"rule_code\": \"ue_2\",\n                \"check_type\": self.TOTAL_PERMANENCE_THRESHOLD,\n                \"threshold_value\": self.ps_thresholds[\"ue\"],\n                \"relevant_periods\": [\n                    PeriodCombinations.WORKING_HOURS_WORKDAYS,\n                ],\n                \"apply_condition\": self.UNLABELED_TILES,\n            },\n            {\n                \"rule_code\": \"ue_3\",\n                \"check_type\": self.TOTAL_FREQUENCY_THRESHOLD,\n                \"threshold_value\": self.freq_thresholds[\"ue\"],\n                \"relevant_periods\": [PeriodCombinations.ALL_PERIODS],\n                \"apply_condition\": self.UNLABELED_TILES,\n            },\n        ],\n    }\n\n    meaningful_location_labels = [\n        {\n            \"label_code\": \"home\",\n            \"ps_difference_gap_filter\": {\n                \"relevant_periods\": [PeriodCombinations.ALL_PERIODS],\n                \"gap_permanence_threshold\": self.gap_ps_thresholds[\"home\"],\n                \"is_absolute\": self.gap_ps_threshold_is_absolute[\"home\"],\n            },\n            \"exclude_label_codes\": [],\n            \"labeling_rules\": [\n                {\n                    \"rule_code\": \"h_1\",\n                    \"check_type\": self.TOTAL_PERMANENCE_THRESHOLD,\n                    \"threshold_value\": self.ps_thresholds[\"home\"],\n                    \"relevant_periods\": [PeriodCombinations.ALL_PERIODS],\n                    \"apply_condition\": self.UNLABELED_DEVICES,\n                },\n                {\n                    \"rule_code\": \"h_2\",\n                    \"check_type\": self.TOTAL_PERMANENCE_THRESHOLD,\n                    \"threshold_value\": self.ps_thresholds[\"home\"],\n                    \"relevant_periods\": [PeriodCombinations.NIGHT_TIME_ALL],\n                    \"apply_condition\": self.UNLABELED_DEVICES,\n                },\n                {\n                    \"rule_code\": \"h_3\",\n                    \"check_type\": self.TOTAL_FREQUENCY_THRESHOLD,\n                    \"threshold_value\": self.freq_thresholds[\"home\"],\n                    \"relevant_periods\": [PeriodCombinations.NIGHT_TIME_ALL],\n                    \"apply_condition\": self.UNLABELED_DEVICES,\n                },\n            ],\n        },\n        {\n            \"label_code\": \"work\",\n            \"ps_difference_gap_filter\": {\n                \"relevant_periods\": [PeriodCombinations.WORKING_HOURS_WORKDAYS],\n                \"gap_permanence_threshold\": self.gap_ps_thresholds[\"work\"],\n                \"is_absolute\": self.gap_ps_threshold_is_absolute[\"work\"],\n            },\n            \"exclude_label_codes\": [],\n            \"labeling_rules\": [\n                {\n                    \"rule_code\": \"w_1\",\n                    \"check_type\": self.TOTAL_PERMANENCE_THRESHOLD,\n                    \"threshold_value\": self.ps_thresholds[\"work\"],\n                    \"relevant_periods\": [PeriodCombinations.WORKING_HOURS_WORKDAYS],\n                    \"apply_condition\": self.UNLABELED_DEVICES,\n                },\n                {\n                    \"rule_code\": \"w_2\",\n                    \"check_type\": self.TOTAL_FREQUENCY_THRESHOLD,\n                    \"threshold_value\": self.freq_thresholds[\"work\"],\n                    \"relevant_periods\": [PeriodCombinations.WORKING_HOURS_WORKDAYS],\n                    \"apply_condition\": self.UNLABELED_DEVICES,\n                },\n            ],\n        },\n    ]\n\n    return ue_labels, meaningful_location_labels\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.write_label","title":"<code>write_label(labeled_tiles_df)</code>","text":"<p>Write the labeled tiles to the output data object.</p> <p>Parameters:</p> Name Type Description Default <code>labeled_tiles_df</code> <code>DataFrame</code> <p>labeled tiles dataset.</p> required Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>def write_label(self, labeled_tiles_df: DataFrame):\n    \"\"\"\n    Write the labeled tiles to the output data object.\n\n    Args:\n        labeled_tiles_df (DataFrame): labeled tiles dataset.\n    \"\"\"\n    labeled_tiles_df = labeled_tiles_df.withColumn(ColNames.season, F.lit(self.season))\n    labeled_tiles_df = labeled_tiles_df.withColumn(ColNames.start_date, F.lit(self.start_date))\n    labeled_tiles_df = labeled_tiles_df.withColumn(ColNames.end_date, F.lit(self.end_date))\n\n    # Repartition\n    if self.disaggregate_to_100m_grid:\n        self.logger.info(\"Dissagregating 200m to 100m grid\")\n        labeled_tiles_df = self.grid_gen.grid_id_from_coarser_resolution(\n            sdf=labeled_tiles_df,\n            coarse_resolution=200,\n            coarse_grid_id_col=ColNames.grid_id,\n            new_grid_id_col=ColNames.grid_id,\n        )\n    labeled_tiles_df = labeled_tiles_df.repartition(*SilverUsualEnvironmentLabelsDataObject.PARTITION_COLUMNS)\n\n    labeled_tiles_df = apply_schema_casting(labeled_tiles_df, SilverUsualEnvironmentLabelsDataObject.SCHEMA)\n\n    # save the output data object:\n    self.output_data_objects[SilverUsualEnvironmentLabelsDataObject.ID].df = labeled_tiles_df\n    self.output_data_objects[SilverUsualEnvironmentLabelsDataObject.ID].write()\n\n    # clear cache\n    self.spark.catalog.clearCache()\n</code></pre>"},{"location":"reference/components/ingestion/","title":"ingestion","text":""},{"location":"reference/components/ingestion/data_filtering/","title":"data_filtering","text":""},{"location":"reference/components/ingestion/data_filtering/data_filtering/","title":"data_filtering","text":"<p>Module that cleans RAW MNO Event and Network Topology data.</p>"},{"location":"reference/components/ingestion/data_filtering/data_filtering/#components.ingestion.data_filtering.data_filtering.DataFiltering","title":"<code>DataFiltering</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class that filters MNO Event and network topology data</p> Source code in <code>multimno/components/ingestion/data_filtering/data_filtering.py</code> <pre><code>class DataFiltering(Component):\n    \"\"\"\n    Class that filters MNO Event and network topology data\n    \"\"\"\n\n    COMPONENT_ID = \"DataFiltering\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.data_period_start = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n        self.data_period_end = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n\n        self.data_period_dates = [\n            self.data_period_start + timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n        self.do_spatial_filtering = self.config.getboolean(\n            self.COMPONENT_ID,\n            \"do_spatial_filtering\",\n        )\n\n        self.spatial_filtering_variant = self.config.getint(\n            self.COMPONENT_ID,\n            \"spatial_filtering_variant\",\n        )\n\n        self.extent = self.config.geteval(self.COMPONENT_ID, \"extent\")\n\n        self.do_device_sampling = self.config.getboolean(\n            self.COMPONENT_ID,\n            \"do_device_sampling\",\n        )\n\n        self.sample_size = self.config.getfloat(\n            self.COMPONENT_ID,\n            \"sample_size\",\n        )\n\n        self.reference_polygon = self.config.get(self.COMPONENT_ID, \"reference_polygon\")\n\n        self.current_date = None\n\n        self.repartition_num = self.config.getint(self.COMPONENT_ID, \"repartition_num\")\n        self.sample_seed = self.config.getint(self.COMPONENT_ID, \"sample_seed\")\n        self.devices_subset = None\n\n    def initalize_data_objects(self):\n        # Input\n        self.input_data_objects = {}\n\n        inputs = {\n            \"event_data_bronze\": BronzeEventDataObject,\n            \"network_data_bronze\": BronzeNetworkDataObject,\n        }\n        self.spatial_filtering_mask = self.config.get(self.COMPONENT_ID, \"spatial_filtering_mask\")\n        for key, value in inputs.items():\n            path = self.config.get(CONFIG_BRONZE_PATHS_KEY, key)\n            if check_if_data_path_exists(self.spark, path):\n                self.input_data_objects[value.ID] = value(self.spark, path)\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")\n                raise ValueError(f\"Invalid path for {value.ID}: {path}\")\n\n        if self.spatial_filtering_mask == \"polygon\":\n            self.input_data_objects[BronzeCountriesDataObject.ID] = BronzeCountriesDataObject(\n                self.spark,\n                self.config.get(CONFIG_BRONZE_PATHS_KEY, \"countries_data_bronze\"),\n            )\n\n        # Output\n        bronze_dir = self.config.get(CONFIG_PATHS_KEY, \"bronze_dir\")\n        bronze_dir_sample = self.config.get(CONFIG_PATHS_KEY, \"bronze_dir_sample\")\n        self.clear_destination_directory = self.config.getboolean(self.COMPONENT_ID, \"clear_destination_directory\")\n        self.output_data_objects = {}\n\n        outputs = {\n            \"event_data_bronze\": BronzeEventDataObject,\n            \"network_data_bronze\": BronzeNetworkDataObject,\n        }\n\n        for key, value in outputs.items():\n            path = self.config.get(CONFIG_BRONZE_PATHS_KEY, key)\n            path = path.replace(bronze_dir, bronze_dir_sample)\n            if self.clear_destination_directory:\n                delete_file_or_folder(self.spark, path)\n            self.output_data_objects[value.ID] = value(self.spark, path)\n\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        if self.do_device_sampling:\n            self.logger.info(f\"Sampling devices for data period with sample size {self.sample_size}\")\n            self.input_data_objects[BronzeEventDataObject.ID].read()\n            events_sdf = self.input_data_objects[BronzeEventDataObject.ID].df\n            # Get users in first day\n            events_sdf = events_sdf.filter(\n                F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n                == F.lit(self.data_period_dates[0])\n            )\n            self.devices_subset = self.sample_devices(events_sdf, self.sample_size, self.sample_seed)\n            self.devices_subset.cache()\n            self.logger.info(f\"Devices sampled: {self.devices_subset.count()}\")\n        for current_date in self.data_period_dates:\n            self.current_date = current_date\n            self.logger.info(f\"Processing {current_date}\")\n            self.read()\n            self.transform()\n            self.write()\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n        events_sdf = self.input_data_objects[BronzeEventDataObject.ID].df.filter(\n            F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(self.current_date)\n        )\n        cells_sdf = self.input_data_objects[BronzeNetworkDataObject.ID].df.filter(\n            F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(self.current_date)\n        )\n        if self.spatial_filtering_mask == \"polygon\":\n            polygons = self.input_data_objects[BronzeCountriesDataObject.ID].df.filter(\n                F.col(ColNames.iso2) == self.reference_polygon\n            )\n            polygons = utils.project_to_crs(polygons, 3035, 4326)\n\n        if self.do_spatial_filtering:\n            if self.spatial_filtering_variant == 1:\n                if self.spatial_filtering_mask == \"polygon\":\n                    events_sdf, cells_sdf = self.filter_by_spatial_area(events_sdf, cells_sdf, polygons)\n                elif self.spatial_filtering_mask == \"extent\":\n                    events_sdf, cells_sdf = self.filter_by_spatial_area(events_sdf, cells_sdf, bbox=self.extent)\n            elif self.spatial_filtering_variant == 2:\n                if self.spatial_filtering_mask == \"polygon\":\n                    events_sdf, cells_sdf = self.filter_by_devices_in_spatial_area(events_sdf, cells_sdf, polygons)\n                elif self.spatial_filtering_mask == \"extent\":\n                    events_sdf, cells_sdf = self.filter_by_devices_in_spatial_area(\n                        events_sdf, cells_sdf, bbox=self.extent\n                    )\n\n        if self.do_device_sampling:\n            events_sdf = self.sample_events(events_sdf, self.devices_subset)\n\n        # --- Export actions ---\n        # Schema casting\n        events_sdf = utils.apply_schema_casting(events_sdf, BronzeEventDataObject.SCHEMA)\n        cells_sdf = utils.apply_schema_casting(cells_sdf, BronzeNetworkDataObject.SCHEMA)\n\n        # Partition output data (Avoid generating thousands of small files)\n        events_sdf = events_sdf.repartition(self.repartition_num)\n        cells_sdf = cells_sdf.repartition(self.repartition_num)\n\n        self.output_data_objects[BronzeNetworkDataObject.ID].df = cells_sdf\n        self.output_data_objects[BronzeEventDataObject.ID].df = events_sdf\n\n    @staticmethod\n    def filter_cells_bbox(cells_sdf: DataFrame, bbox: List) -&gt; DataFrame:\n        \"\"\"\n        Filters cells DataFrame based on bounding box.\n        \"\"\"\n        bbox_filter = (\n            (F.col(ColNames.longitude) &gt;= bbox[0])\n            &amp; (F.col(ColNames.longitude) &lt;= bbox[2])\n            &amp; (F.col(ColNames.latitude) &gt;= bbox[1])\n            &amp; (F.col(ColNames.latitude) &lt;= bbox[3])\n        )\n        return cells_sdf.filter(bbox_filter)\n\n    @staticmethod\n    def filter_cells_polygon(cells_sdf: DataFrame, polygons_sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Filters cells DataFrame based on polygon.\n        \"\"\"\n        cells_sdf = cells_sdf.withColumn(\n            ColNames.geometry, STC.ST_Point(ColNames.longitude, ColNames.latitude)\n        ).withColumn(ColNames.geometry, STF.ST_SetSRID(ColNames.geometry, 4326))\n\n        cells_sdf = cells_sdf.join(\n            polygons_sdf, STP.ST_Intersects(cells_sdf[ColNames.geometry], polygons_sdf[ColNames.geometry]), \"inner\"\n        ).drop(polygons_sdf[ColNames.geometry])\n\n        return cells_sdf\n\n    @staticmethod\n    def filter_by_spatial_area(\n        events_sdf: DataFrame, cells_sdf: DataFrame, polygons_sdf: DataFrame = None, bbox: List = None\n    ) -&gt; DataFrame:\n        \"\"\"\n        Filters events DataFrame based on spatial area.\n        \"\"\"\n        if polygons_sdf is not None:\n            cells_sdf = DataFiltering.filter_cells_polygon(cells_sdf, polygons_sdf)\n        elif bbox is not None:\n            cells_sdf = DataFiltering.filter_cells_bbox(cells_sdf, bbox)\n\n        events_sdf = events_sdf.join(cells_sdf.select(ColNames.cell_id), on=ColNames.cell_id, how=\"inner\")\n\n        return events_sdf, cells_sdf\n\n    @staticmethod\n    def filter_by_devices_in_spatial_area(\n        events_sdf: DataFrame, cells_sdf: DataFrame, polygons_sdf: DataFrame = None, bbox: List = None\n    ) -&gt; DataFrame:\n        \"\"\"\n        Filters events DataFrame based on devices in bounding box.\n        \"\"\"\n        # Get cells in the bounding box\n        if polygons_sdf is not None:\n            cells_in_area_sdf = DataFiltering.filter_cells_polygon(cells_sdf, polygons_sdf)\n        elif bbox is not None:\n            cells_in_area_sdf = DataFiltering.filter_cells_bbox(cells_sdf, bbox)\n\n        # Filter events based on cell_id being in the cells from the bounding box\n        events_in_bbox_sdf = events_sdf.join(\n            cells_in_area_sdf.select(ColNames.cell_id), on=ColNames.cell_id, how=\"inner\"\n        )\n\n        # Get distinct devices in the filtered events\n        devices_in_bbox_sdf = events_in_bbox_sdf.select(ColNames.user_id).distinct()\n\n        # Filter original events by the devices in the bounding box\n        events_sdf = events_sdf.join(devices_in_bbox_sdf, on=ColNames.user_id, how=\"inner\")\n\n        # Filter cells based on the distinct cell IDs in the filtered events\n        all_cells_sdf = events_sdf.select(ColNames.cell_id).distinct()\n\n        cells_sdf = cells_sdf.join(all_cells_sdf, on=ColNames.cell_id, how=\"inner\")\n\n        return events_sdf, cells_sdf\n\n    @staticmethod\n    def sample_devices(events_sdf: DataFrame, sample_size: float, sample_seed: int) -&gt; DataFrame:\n\n        devices_sdf = events_sdf.select(ColNames.user_id).distinct()\n        devices_sdf = devices_sdf.sample(False, sample_size, sample_seed)\n\n        return devices_sdf\n\n    @staticmethod\n    def sample_events(events_sdf: DataFrame, devices_sdf: DataFrame) -&gt; DataFrame:\n\n        events_sdf = events_sdf.join(devices_sdf, on=ColNames.user_id, how=\"inner\")\n\n        return events_sdf\n</code></pre>"},{"location":"reference/components/ingestion/data_filtering/data_filtering/#components.ingestion.data_filtering.data_filtering.DataFiltering.filter_by_devices_in_spatial_area","title":"<code>filter_by_devices_in_spatial_area(events_sdf, cells_sdf, polygons_sdf=None, bbox=None)</code>  <code>staticmethod</code>","text":"<p>Filters events DataFrame based on devices in bounding box.</p> Source code in <code>multimno/components/ingestion/data_filtering/data_filtering.py</code> <pre><code>@staticmethod\ndef filter_by_devices_in_spatial_area(\n    events_sdf: DataFrame, cells_sdf: DataFrame, polygons_sdf: DataFrame = None, bbox: List = None\n) -&gt; DataFrame:\n    \"\"\"\n    Filters events DataFrame based on devices in bounding box.\n    \"\"\"\n    # Get cells in the bounding box\n    if polygons_sdf is not None:\n        cells_in_area_sdf = DataFiltering.filter_cells_polygon(cells_sdf, polygons_sdf)\n    elif bbox is not None:\n        cells_in_area_sdf = DataFiltering.filter_cells_bbox(cells_sdf, bbox)\n\n    # Filter events based on cell_id being in the cells from the bounding box\n    events_in_bbox_sdf = events_sdf.join(\n        cells_in_area_sdf.select(ColNames.cell_id), on=ColNames.cell_id, how=\"inner\"\n    )\n\n    # Get distinct devices in the filtered events\n    devices_in_bbox_sdf = events_in_bbox_sdf.select(ColNames.user_id).distinct()\n\n    # Filter original events by the devices in the bounding box\n    events_sdf = events_sdf.join(devices_in_bbox_sdf, on=ColNames.user_id, how=\"inner\")\n\n    # Filter cells based on the distinct cell IDs in the filtered events\n    all_cells_sdf = events_sdf.select(ColNames.cell_id).distinct()\n\n    cells_sdf = cells_sdf.join(all_cells_sdf, on=ColNames.cell_id, how=\"inner\")\n\n    return events_sdf, cells_sdf\n</code></pre>"},{"location":"reference/components/ingestion/data_filtering/data_filtering/#components.ingestion.data_filtering.data_filtering.DataFiltering.filter_by_spatial_area","title":"<code>filter_by_spatial_area(events_sdf, cells_sdf, polygons_sdf=None, bbox=None)</code>  <code>staticmethod</code>","text":"<p>Filters events DataFrame based on spatial area.</p> Source code in <code>multimno/components/ingestion/data_filtering/data_filtering.py</code> <pre><code>@staticmethod\ndef filter_by_spatial_area(\n    events_sdf: DataFrame, cells_sdf: DataFrame, polygons_sdf: DataFrame = None, bbox: List = None\n) -&gt; DataFrame:\n    \"\"\"\n    Filters events DataFrame based on spatial area.\n    \"\"\"\n    if polygons_sdf is not None:\n        cells_sdf = DataFiltering.filter_cells_polygon(cells_sdf, polygons_sdf)\n    elif bbox is not None:\n        cells_sdf = DataFiltering.filter_cells_bbox(cells_sdf, bbox)\n\n    events_sdf = events_sdf.join(cells_sdf.select(ColNames.cell_id), on=ColNames.cell_id, how=\"inner\")\n\n    return events_sdf, cells_sdf\n</code></pre>"},{"location":"reference/components/ingestion/data_filtering/data_filtering/#components.ingestion.data_filtering.data_filtering.DataFiltering.filter_cells_bbox","title":"<code>filter_cells_bbox(cells_sdf, bbox)</code>  <code>staticmethod</code>","text":"<p>Filters cells DataFrame based on bounding box.</p> Source code in <code>multimno/components/ingestion/data_filtering/data_filtering.py</code> <pre><code>@staticmethod\ndef filter_cells_bbox(cells_sdf: DataFrame, bbox: List) -&gt; DataFrame:\n    \"\"\"\n    Filters cells DataFrame based on bounding box.\n    \"\"\"\n    bbox_filter = (\n        (F.col(ColNames.longitude) &gt;= bbox[0])\n        &amp; (F.col(ColNames.longitude) &lt;= bbox[2])\n        &amp; (F.col(ColNames.latitude) &gt;= bbox[1])\n        &amp; (F.col(ColNames.latitude) &lt;= bbox[3])\n    )\n    return cells_sdf.filter(bbox_filter)\n</code></pre>"},{"location":"reference/components/ingestion/data_filtering/data_filtering/#components.ingestion.data_filtering.data_filtering.DataFiltering.filter_cells_polygon","title":"<code>filter_cells_polygon(cells_sdf, polygons_sdf)</code>  <code>staticmethod</code>","text":"<p>Filters cells DataFrame based on polygon.</p> Source code in <code>multimno/components/ingestion/data_filtering/data_filtering.py</code> <pre><code>@staticmethod\ndef filter_cells_polygon(cells_sdf: DataFrame, polygons_sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Filters cells DataFrame based on polygon.\n    \"\"\"\n    cells_sdf = cells_sdf.withColumn(\n        ColNames.geometry, STC.ST_Point(ColNames.longitude, ColNames.latitude)\n    ).withColumn(ColNames.geometry, STF.ST_SetSRID(ColNames.geometry, 4326))\n\n    cells_sdf = cells_sdf.join(\n        polygons_sdf, STP.ST_Intersects(cells_sdf[ColNames.geometry], polygons_sdf[ColNames.geometry]), \"inner\"\n    ).drop(polygons_sdf[ColNames.geometry])\n\n    return cells_sdf\n</code></pre>"},{"location":"reference/components/ingestion/grid_generation/","title":"grid_generation","text":""},{"location":"reference/components/ingestion/grid_generation/inspire_grid_generation/","title":"inspire_grid_generation","text":"<p>This module contains the InspireGridGeneration class which is responsible for generating the INSPIRE grid and enrich it with elevation and landuse data.</p>"},{"location":"reference/components/ingestion/grid_generation/inspire_grid_generation/#components.ingestion.grid_generation.inspire_grid_generation.InspireGridGeneration","title":"<code>InspireGridGeneration</code>","text":"<p>               Bases: <code>Component</code></p> <p>This class is responsible for generating the INSPIRE grid for given extent or polygon and enrich it with elevation and landuse data.</p> Source code in <code>multimno/components/ingestion/grid_generation/inspire_grid_generation.py</code> <pre><code>class InspireGridGeneration(Component):\n    \"\"\"\n    This class is responsible for generating the INSPIRE grid for given extent or polygon\n    and enrich it with elevation and landuse data.\n    \"\"\"\n\n    COMPONENT_ID = \"InspireGridGeneration\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.grid_partition_size = self.config.get(InspireGridGeneration.COMPONENT_ID, \"grid_generation_partition_size\")\n\n        self.grid_quadkey_level = self.config.getint(\n            InspireGridGeneration.COMPONENT_ID,\n            \"grid_processing_partition_quadkey_level\",\n        )\n\n        self.reference_country = self.config.get(InspireGridGeneration.COMPONENT_ID, \"reference_country\")\n\n        self.country_buffer = self.config.get(InspireGridGeneration.COMPONENT_ID, \"country_buffer\")\n\n        self.grid_generator = InspireGridGenerator(\n            self.spark,\n            ColNames.geometry,\n            ColNames.grid_id,\n            self.grid_partition_size,\n        )\n\n        # Attributes that will hold the shared common origin if creating INSPIRE grid using polygon(s) instead of\n        # an extent\n        self.n_origin: float = None\n        self.e_origin: float = None\n\n    def initalize_data_objects(self):\n\n        self.clear_destination_directory = self.config.getboolean(\n            InspireGridGeneration.COMPONENT_ID, \"clear_destination_directory\"\n        )\n\n        self.grid_mask = self.config.get(InspireGridGeneration.COMPONENT_ID, \"grid_mask\")\n\n        if self.grid_mask == \"polygon\":\n            # inputs\n            self.input_data_objects = {}\n            self.input_data_objects[BronzeCountriesDataObject.ID] = BronzeCountriesDataObject(\n                self.spark,\n                self.config.get(CONFIG_BRONZE_PATHS_KEY, \"countries_data_bronze\"),\n            )\n\n        # outputs\n\n        grid_do_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"grid_data_silver\")\n\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, grid_do_path)\n\n        self.output_data_objects = {}\n        self.output_data_objects[SilverGridDataObject.ID] = SilverGridDataObject(\n            self.spark, grid_do_path, [ColNames.quadkey]\n        )\n\n    @get_execution_stats\n    def execute(self):\n\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n\n        if self.grid_mask == \"polygon\":\n            self.read()\n\n            countries, country_extent = self.get_country_mask(self.reference_country, self.country_buffer)\n            proj_extent, _ = self.grid_generator.process_latlon_extent(country_extent)\n            self.n_origin = proj_extent[0]\n            self.e_origin = proj_extent[1]\n\n            ids = [row[\"temp_id\"] for row in countries.select(\"temp_id\").collect()]\n\n            self.logger.info(f\"Processing {len(ids)} parts of the country\")\n            processed_parts = 0\n            for id in ids:\n                self.current_country_part = countries.filter(F.col(\"temp_id\") == id)\n                self.transform()\n                self.write()\n                processed_parts += 1\n                self.logger.info(f\"Finished processing {processed_parts} parts\")\n        else:\n            self.transform()\n            self.write()\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}...\")\n\n        if self.grid_mask == \"polygon\":\n            grid_sdf = self.grid_generator.cover_polygon_with_grid_centroids(\n                self.current_country_part, n_origin=self.n_origin, e_origin=self.e_origin\n            )\n        else:\n            grid_extent = self.config.geteval(InspireGridGeneration.COMPONENT_ID, \"extent\")\n            grid_sdf = self.grid_generator.cover_extent_with_grid_centroids(grid_extent)\n        grid_sdf = quadkey_utils.assign_quadkey(grid_sdf, 3035, self.grid_quadkey_level)\n\n        grid_sdf = grid_sdf.orderBy(ColNames.quadkey)\n        grid_sdf = grid_sdf.repartition(ColNames.quadkey)\n\n        grid_sdf = utils.apply_schema_casting(grid_sdf, SilverGridDataObject.SCHEMA)\n\n        self.output_data_objects[SilverGridDataObject.ID].df = grid_sdf\n\n    def get_country_mask(self, reference_country, country_buffer):\n\n        countries = self.input_data_objects[BronzeCountriesDataObject.ID].df\n        countries = (\n            countries.filter(F.col(ColNames.iso2) == reference_country)\n            .withColumn(\n                ColNames.geometry,\n                STF.ST_Buffer(ColNames.geometry, F.lit(country_buffer)),\n            )\n            .groupBy()\n            .agg(STA.ST_Union_Aggr(ColNames.geometry).alias(ColNames.geometry))\n        )\n\n        countries = utils.project_to_crs(countries, 3035, 4326)\n\n        country_extent = countries.select(\n            STF.ST_XMin(\"geometry\").alias(\"longitude_min\"),\n            STF.ST_YMin(\"geometry\").alias(\"latitude_min\"),\n            STF.ST_XMax(\"geometry\").alias(\"longitude_max\"),\n            STF.ST_YMax(\"geometry\").alias(\"latitude_max\"),\n        ).collect()\n\n        if len(country_extent) == 0:\n            raise ValueError(f\"BronzeCountriesDataObject for {reference_country} seems to have no geometries\")\n        country_extent = list(country_extent[0][:])\n\n        countries = countries.withColumn(ColNames.geometry, F.explode(STF.ST_Dump(ColNames.geometry))).withColumn(\n            \"temp_id\", F.monotonically_increasing_id()\n        )\n\n        return countries, country_extent\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/","title":"spatial_data_ingestion","text":""},{"location":"reference/components/ingestion/spatial_data_ingestion/gisco_data_ingestion/","title":"gisco_data_ingestion","text":""},{"location":"reference/components/ingestion/spatial_data_ingestion/gisco_data_ingestion/#components.ingestion.spatial_data_ingestion.gisco_data_ingestion.GiscoDataIngestion","title":"<code>GiscoDataIngestion</code>","text":"<p>               Bases: <code>Component</code></p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/gisco_data_ingestion.py</code> <pre><code>class GiscoDataIngestion(Component):\n    \"\"\" \"\"\"\n\n    COMPONENT_ID = \"GiscoDataIngestion\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n    def initalize_data_objects(self):\n\n        base_url = self.config.get(GiscoDataIngestion.COMPONENT_ID, \"base_url\")\n\n        self.get_countries = self.config.getboolean(GiscoDataIngestion.COMPONENT_ID, \"get_countries\")\n\n        self.get_nuts = self.config.getboolean(GiscoDataIngestion.COMPONENT_ID, \"get_nuts\")\n\n        self.default_crs = self.config.getint(GiscoDataIngestion.COMPONENT_ID, \"default_crs\")\n\n        self.input_data_objects = {}\n        self.output_data_objects = {}\n        if self.get_countries:\n\n            countries_resolution = self.config.get(GiscoDataIngestion.COMPONENT_ID, \"countries_resolution\")\n            countries_year = self.config.get(GiscoDataIngestion.COMPONENT_ID, \"countries_year\")\n\n            countries_url = (\n                f\"{base_url}/countries/geojson/CNTR_RG_{countries_resolution}M_{countries_year}_4326.geojson\"\n            )\n\n            self.input_data_objects[\"countries\"] = LandingHttpGeoJsonDataObject(self.spark, countries_url, 240, 3)\n\n            countries_do_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"countries_data_bronze\")\n\n            self.output_data_objects[BronzeCountriesDataObject.ID] = BronzeCountriesDataObject(\n                self.spark,\n                countries_do_path,\n            )\n\n        if self.get_nuts:\n\n            nuts_resolution = self.config.get(GiscoDataIngestion.COMPONENT_ID, \"nuts_resolution\")\n            self.nuts_year = self.config.get(GiscoDataIngestion.COMPONENT_ID, \"nuts_year\")\n            nuts_levels = self.config.get(GiscoDataIngestion.COMPONENT_ID, \"nuts_levels\")\n            self.nuts_levels = nuts_levels.split(\",\")\n\n            self.reference_country = self.config.get(GiscoDataIngestion.COMPONENT_ID, \"reference_country\")\n            for level in self.nuts_levels:\n                nuts_url = (\n                    f\"{base_url}/nuts/geojson/NUTS_RG_{nuts_resolution}M_{self.nuts_year}_4326_LEVL_{level}.geojson\"\n                )\n                self.input_data_objects[f\"nuts_{level}\"] = LandingHttpGeoJsonDataObject(self.spark, nuts_url, 300, 5)\n\n            geographic_zones_do_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"geographic_zones_data_bronze\")\n\n            self.output_data_objects[BronzeGeographicZonesDataObject.ID] = BronzeGeographicZonesDataObject(\n                self.spark,\n                geographic_zones_do_path,\n            )\n\n        self.clear_destination_directory = self.config.getboolean(\n            GiscoDataIngestion.COMPONENT_ID, \"clear_destination_directory\"\n        )\n\n        if self.clear_destination_directory:\n            for do in self.output_data_objects.values():\n                self.logger.info(f\"Clearing {do.default_path}\")\n                delete_file_or_folder(self.spark, do.default_path)\n\n    def read(self):\n        # need to read the data from the input data objects separately\n        pass\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n\n        if self.get_countries:\n\n            countries = self.get_countries_data()\n            countries = utils.apply_schema_casting(countries, BronzeCountriesDataObject.SCHEMA)\n            self.output_data_objects[BronzeCountriesDataObject.ID].df = countries\n\n        if self.get_nuts:\n            self.output_data_objects[BronzeGeographicZonesDataObject.ID].df = self.spark.createDataFrame(\n                [], BronzeGeographicZonesDataObject.SCHEMA\n            )\n            for level in self.nuts_levels:\n\n                nuts = self.get_nuts_data(level)\n                nuts = nuts.withColumn(ColNames.dataset_id, F.lit(\"nuts\"))\n                nuts = utils.apply_schema_casting(nuts, BronzeGeographicZonesDataObject.SCHEMA)\n                self.output_data_objects[BronzeGeographicZonesDataObject.ID].df = self.output_data_objects[\n                    BronzeGeographicZonesDataObject.ID\n                ].df.union(nuts)\n\n    def get_countries_data(self) -&gt; DataFrame:\n        \"\"\"\n        Retrieves and processes country data.\n\n        This method reads country data from the GISCO portal\n        and processes the data by renaming columns, exploding the 'geometry' column,\n        and projecting the data to a default CRS.\n        The processed data is returned as a DataFrame.\n\n        Returns:\n            DataFrame: A DataFrame containing processed country data.\n                    The DataFrame has columns for the ISO 2 country code, country name, and geometry.\n        \"\"\"\n\n        self.input_data_objects[\"countries\"].read()\n        self.logger.info(f\"got countries data\")\n        countries_sdf = self.input_data_objects[\"countries\"].df\n        countries_sdf = countries_sdf.withColumnRenamed(\"CNTR_ID\", ColNames.iso2).withColumnRenamed(\n            \"NAME_ENGL\", ColNames.name\n        )\n\n        countries_sdf = countries_sdf.withColumn(\"geometry\", F.explode(STF.ST_Dump(\"geometry\")))\n\n        countries_sdf = utils.project_to_crs(countries_sdf, 4326, self.default_crs)\n        return countries_sdf\n\n    def get_nuts_data(self, level: str) -&gt; DataFrame:\n        \"\"\"\n        Retrieves and processes NUTS (Nomenclature of Territorial Units for Statistics) data for a specific level.\n\n        This method reads NUTS data from GISCO portal and processes the data by\n        renaming columns, filtering by reference country, and projecting the data to a default CRS.\n        It also adds a 'parent_id' column that contains the ID of the parent NUTS unit for each NUTS unit,\n        and adds columns for the year, month, and day with fixed values.\n        The processed data is returned as a DataFrame.\n\n        Args:\n            level (str): The NUTS level for which to retrieve and process data.\n\n        Returns:\n            DataFrame: A DataFrame containing processed NUTS data.\n                    The DataFrame has columns for the NUTS ID, name, ISO 2 country code, level,\n                    geometry, parent ID, year, month, and day.\n        \"\"\"\n\n        self.input_data_objects[f\"nuts_{level}\"].read()\n        self.logger.info(f\"got NUTS data for level {level}\")\n        nuts_sdf = self.input_data_objects[f\"nuts_{level}\"].df\n        nuts_sdf = nuts_sdf.drop(\"id\")\n        nuts_sdf = nuts_sdf.filter(F.col(\"CNTR_CODE\") == self.reference_country)\n\n        nuts_sdf = (\n            nuts_sdf.withColumnRenamed(\"NUTS_ID\", ColNames.zone_id)\n            .withColumnRenamed(\"NAME_LATN\", ColNames.name)\n            .withColumnRenamed(\"CNTR_CODE\", ColNames.iso2)\n            .withColumnRenamed(\"LEVL_CODE\", ColNames.level)\n        )\n\n        nuts_sdf = nuts_sdf.select(\n            ColNames.zone_id,\n            ColNames.name,\n            ColNames.iso2,\n            ColNames.level,\n            ColNames.geometry,\n        )\n\n        if level == 0:\n            nuts_sdf = nuts_sdf.withColumn(\"parent_id\", F.lit(None))\n        else:\n            nuts_sdf = nuts_sdf.withColumn(\"parent_id\", F.expr(\"substring(zone_id, 1, length(zone_id)-1)\"))\n\n        nuts_sdf = utils.project_to_crs(nuts_sdf, 4326, self.default_crs)\n\n        nuts_sdf = nuts_sdf.withColumns(\n            {\n                ColNames.year: F.lit(self.nuts_year),\n                ColNames.month: F.lit(1),\n                ColNames.day: F.lit(1),\n            }\n        )\n        return nuts_sdf\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/gisco_data_ingestion/#components.ingestion.spatial_data_ingestion.gisco_data_ingestion.GiscoDataIngestion.get_countries_data","title":"<code>get_countries_data()</code>","text":"<p>Retrieves and processes country data.</p> <p>This method reads country data from the GISCO portal and processes the data by renaming columns, exploding the 'geometry' column, and projecting the data to a default CRS. The processed data is returned as a DataFrame.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing processed country data.     The DataFrame has columns for the ISO 2 country code, country name, and geometry.</p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/gisco_data_ingestion.py</code> <pre><code>def get_countries_data(self) -&gt; DataFrame:\n    \"\"\"\n    Retrieves and processes country data.\n\n    This method reads country data from the GISCO portal\n    and processes the data by renaming columns, exploding the 'geometry' column,\n    and projecting the data to a default CRS.\n    The processed data is returned as a DataFrame.\n\n    Returns:\n        DataFrame: A DataFrame containing processed country data.\n                The DataFrame has columns for the ISO 2 country code, country name, and geometry.\n    \"\"\"\n\n    self.input_data_objects[\"countries\"].read()\n    self.logger.info(f\"got countries data\")\n    countries_sdf = self.input_data_objects[\"countries\"].df\n    countries_sdf = countries_sdf.withColumnRenamed(\"CNTR_ID\", ColNames.iso2).withColumnRenamed(\n        \"NAME_ENGL\", ColNames.name\n    )\n\n    countries_sdf = countries_sdf.withColumn(\"geometry\", F.explode(STF.ST_Dump(\"geometry\")))\n\n    countries_sdf = utils.project_to_crs(countries_sdf, 4326, self.default_crs)\n    return countries_sdf\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/gisco_data_ingestion/#components.ingestion.spatial_data_ingestion.gisco_data_ingestion.GiscoDataIngestion.get_nuts_data","title":"<code>get_nuts_data(level)</code>","text":"<p>Retrieves and processes NUTS (Nomenclature of Territorial Units for Statistics) data for a specific level.</p> <p>This method reads NUTS data from GISCO portal and processes the data by renaming columns, filtering by reference country, and projecting the data to a default CRS. It also adds a 'parent_id' column that contains the ID of the parent NUTS unit for each NUTS unit, and adds columns for the year, month, and day with fixed values. The processed data is returned as a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>str</code> <p>The NUTS level for which to retrieve and process data.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing processed NUTS data.     The DataFrame has columns for the NUTS ID, name, ISO 2 country code, level,     geometry, parent ID, year, month, and day.</p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/gisco_data_ingestion.py</code> <pre><code>def get_nuts_data(self, level: str) -&gt; DataFrame:\n    \"\"\"\n    Retrieves and processes NUTS (Nomenclature of Territorial Units for Statistics) data for a specific level.\n\n    This method reads NUTS data from GISCO portal and processes the data by\n    renaming columns, filtering by reference country, and projecting the data to a default CRS.\n    It also adds a 'parent_id' column that contains the ID of the parent NUTS unit for each NUTS unit,\n    and adds columns for the year, month, and day with fixed values.\n    The processed data is returned as a DataFrame.\n\n    Args:\n        level (str): The NUTS level for which to retrieve and process data.\n\n    Returns:\n        DataFrame: A DataFrame containing processed NUTS data.\n                The DataFrame has columns for the NUTS ID, name, ISO 2 country code, level,\n                geometry, parent ID, year, month, and day.\n    \"\"\"\n\n    self.input_data_objects[f\"nuts_{level}\"].read()\n    self.logger.info(f\"got NUTS data for level {level}\")\n    nuts_sdf = self.input_data_objects[f\"nuts_{level}\"].df\n    nuts_sdf = nuts_sdf.drop(\"id\")\n    nuts_sdf = nuts_sdf.filter(F.col(\"CNTR_CODE\") == self.reference_country)\n\n    nuts_sdf = (\n        nuts_sdf.withColumnRenamed(\"NUTS_ID\", ColNames.zone_id)\n        .withColumnRenamed(\"NAME_LATN\", ColNames.name)\n        .withColumnRenamed(\"CNTR_CODE\", ColNames.iso2)\n        .withColumnRenamed(\"LEVL_CODE\", ColNames.level)\n    )\n\n    nuts_sdf = nuts_sdf.select(\n        ColNames.zone_id,\n        ColNames.name,\n        ColNames.iso2,\n        ColNames.level,\n        ColNames.geometry,\n    )\n\n    if level == 0:\n        nuts_sdf = nuts_sdf.withColumn(\"parent_id\", F.lit(None))\n    else:\n        nuts_sdf = nuts_sdf.withColumn(\"parent_id\", F.expr(\"substring(zone_id, 1, length(zone_id)-1)\"))\n\n    nuts_sdf = utils.project_to_crs(nuts_sdf, 4326, self.default_crs)\n\n    nuts_sdf = nuts_sdf.withColumns(\n        {\n            ColNames.year: F.lit(self.nuts_year),\n            ColNames.month: F.lit(1),\n            ColNames.day: F.lit(1),\n        }\n    )\n    return nuts_sdf\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/overture_data_ingestion/","title":"overture_data_ingestion","text":""},{"location":"reference/components/ingestion/spatial_data_ingestion/overture_data_ingestion/#components.ingestion.spatial_data_ingestion.overture_data_ingestion.OvertureDataIngestion","title":"<code>OvertureDataIngestion</code>","text":"<p>               Bases: <code>Component</code></p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/overture_data_ingestion.py</code> <pre><code>class OvertureDataIngestion(Component):\n    \"\"\" \"\"\"\n\n    COMPONENT_ID = \"OvertureDataIngestion\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.extent = self.config.geteval(self.COMPONENT_ID, \"extent\")\n        self.extraction_quadkey_level = self.config.getint(self.COMPONENT_ID, \"extraction_quadkey_level\")\n        self.quadkeys_to_process = self.config.geteval(self.COMPONENT_ID, \"quadkeys_to_process\")\n\n    def initalize_data_objects(self):\n\n        self.transportation = \"transportation\"\n        self.landuse = \"landuse\"\n        self.landcover = \"landcover\"\n        self.buildings = \"buildings\"\n        self.water = \"water\"\n\n        overture_url = self.config.get(self.COMPONENT_ID, \"overture_url\")\n\n        transportation_url = overture_url + \"/theme=transportation/type=segment\"\n        buildings_url = overture_url + \"/theme=buildings/type=building\"\n        landcover_url = overture_url + \"/theme=base/type=land\"\n        landuse_url = overture_url + \"/theme=base/type=land_use\"\n        water_url = overture_url + \"/theme=base/type=water\"\n\n        self.input_data_objects = {}\n        self.input_data_objects[self.transportation] = LandingGeoParquetDataObject(\n            self.spark, transportation_url, set_crs=False\n        )\n        self.input_data_objects[self.buildings] = LandingGeoParquetDataObject(self.spark, buildings_url, set_crs=False)\n        self.input_data_objects[self.landcover] = LandingGeoParquetDataObject(self.spark, landcover_url, set_crs=False)\n        self.input_data_objects[self.landuse] = LandingGeoParquetDataObject(self.spark, landuse_url, set_crs=False)\n        self.input_data_objects[self.water] = LandingGeoParquetDataObject(self.spark, water_url, set_crs=False)\n\n        self.clear_destination_directory = self.config.getboolean(\n            OvertureDataIngestion.COMPONENT_ID, \"clear_destination_directory\"\n        )\n\n        transportation_do_path = self.config.get(CONFIG_LANDING_PATHS_KEY, \"transportation_data_landing\")\n        landuse_do_path = self.config.get(CONFIG_LANDING_PATHS_KEY, \"landuse_data_landing\")\n        landcover_do_path = self.config.get(CONFIG_LANDING_PATHS_KEY, \"landcover_data_landing\")\n        buildings_do_path = self.config.get(CONFIG_LANDING_PATHS_KEY, \"buildings_data_landing\")\n        water_do_path = self.config.get(CONFIG_LANDING_PATHS_KEY, \"water_data_landing\")\n\n        self.output_data_objects = {}\n\n        self.output_data_objects[self.transportation] = LandingGeoParquetDataObject(\n            self.spark, transportation_do_path, [ColNames.quadkey]\n        )\n        self.output_data_objects[self.landuse] = LandingGeoParquetDataObject(\n            self.spark, landuse_do_path, [ColNames.quadkey]\n        )\n        self.output_data_objects[self.landcover] = LandingGeoParquetDataObject(\n            self.spark, landcover_do_path, [ColNames.quadkey]\n        )\n        self.output_data_objects[self.buildings] = LandingGeoParquetDataObject(\n            self.spark, buildings_do_path, [ColNames.quadkey]\n        )\n        self.output_data_objects[self.water] = LandingGeoParquetDataObject(\n            self.spark, water_do_path, [ColNames.quadkey]\n        )\n\n        if self.clear_destination_directory:\n            for do in self.output_data_objects.values():\n                self.logger.info(f\"Clearing {do.default_path}\")\n                delete_file_or_folder(self.spark, do.default_path)\n\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n\n        if len(self.quadkeys_to_process) == 0:\n            self.quadkeys_to_process = quadkey_utils.get_quadkeys_for_bbox(self.extent, self.extraction_quadkey_level)\n        self.logger.info(f\"Extraction will be done in {len(self.quadkeys_to_process)} parts.\")\n\n        self.logger.info(\"quadkeys to process: \")\n        self.logger.info(f\"{self.quadkeys_to_process}\")\n        processed = 0\n        for quadkey in self.quadkeys_to_process:\n            self.logger.info(f\"Processing quadkey {quadkey}\")\n            self.current_extent = quadkey_utils.quadkey_to_extent(quadkey)\n            self.current_quadkey = quadkey\n            self.transform()\n            processed += 1\n            self.logger.info(f\"Processed {processed} out of {len(self.quadkeys_to_process)} quadkeys\")\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def write(self, data_object_id: str):\n        self.logger.info(f\"Writing {data_object_id} data object\")\n        self.output_data_objects[data_object_id].write()\n        return None\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n\n        # process transportaion\n        transportation_sdf = self.get_raw_overture_data(self.transportation, [\"class\", \"geometry\", \"subtype\"], 2)\n        self.output_data_objects[self.transportation].df = transportation_sdf\n\n        self.write(self.transportation)\n\n        # process landuse\n        landuse_cols_to_select = [\"subtype\", \"geometry\"]\n\n        landuse_sdf = self.get_raw_overture_data(self.landuse, landuse_cols_to_select, 3)\n        self.output_data_objects[self.landuse].df = landuse_sdf\n\n        self.write(self.landuse)\n\n        # process landcover\n\n        landcover_sdf = self.get_raw_overture_data(self.landcover, landuse_cols_to_select, 3)\n        self.output_data_objects[self.landcover].df = landcover_sdf\n\n        self.write(self.landcover)\n\n        # process water\n        water_sdf = self.get_raw_overture_data(self.water, landuse_cols_to_select, 3)\n        water_sdf = water_sdf.withColumn(\"subtype\", F.lit(\"water\"))\n        self.output_data_objects[self.water].df = water_sdf\n\n        self.write(self.water)\n\n        # process buildings\n        buildings_sdf = self.get_raw_overture_data(self.buildings, landuse_cols_to_select, 3)\n        self.output_data_objects[self.buildings].df = buildings_sdf\n\n        self.write(self.buildings)\n\n    def get_raw_overture_data(self, data_type: str, cols_to_select, geometry_type: int) -&gt; DataFrame:\n        \"\"\"\n        Retrieves and processes Overture Maps raw data for a specific land use type.\n\n        This function filters input data objects based on the specified data type and optional filter types,\n        fixes the polygon geometry, and projects the data to a specific CRS.\n        If the persist parameter is set to True, the resulting DataFrame is persisted in memory and disk.\n\n        Args:\n            data_type (str): The type of land use data to retrieve.\n            filter_types (list, optional): A list of subtypes to filter the data by. Each subtype is a string.\n                If None, no filtering is performed. Defaults to None.\n            persist (bool, optional): Whether to persist the resulting DataFrame in memory and disk. Defaults to True.\n\n        Returns:\n            DataFrame: A DataFrame containing the processed land use data.\n            The DataFrame includes a subtype column and a geometry column.\n        \"\"\"\n\n        sdf = self.filter_input_data_objects(data_type, cols_to_select)\n\n        sdf = utils.project_to_crs(sdf, 4326, 3035)\n        sdf = utils.fix_geometry(sdf, geometry_type)\n        sdf = sdf.withColumn(ColNames.quadkey, F.lit(self.current_quadkey))\n\n        return sdf\n\n    def filter_input_data_objects(self, data_type: str, required_columns: List[str]) -&gt; DataFrame:\n        \"\"\"\n        Filters and processes input Overture Maps data based on the specified data type and columns.\n\n        This function selects the required columns from the input data objects,\n        filters the data to the current processing iteration extent, and cuts the data to the general extent.\n        If the data type is \"landcover\", \"landuse\", or \"transportation\", it further filters the data by the specified subtypes.\n        If the data type is not \"transportation\", it filters out invalid polygons.\n\n        Args:\n            data_type (str): The type of data to filter and process. \"landcover\", \"landuse\", \"transportation\", \"buildings\", or \"water\".\n            required_columns (list): A list of column names to select from the data. Each column name is a string.\n            category_col (str): The name of the category column to filter by when the data type is \"landcover\", \"landuse\", or \"transportation\".\n            subtypes (list, optional): A list of subtypes to filter the data by when the data type is \"landcover\", \"landuse\", or \"transportation\".\n                Each subtype is a string. If None, no subtype filtering is performed. Defaults to None.\n\n        Returns:\n            DataFrame: A DataFrame containing the filtered and processed data.\n        \"\"\"\n\n        do_sdf = self.input_data_objects[data_type].df\n        do_sdf = do_sdf.select(*required_columns)\n        do_sdf = utils.cut_geodata_to_extent(do_sdf, self.current_extent, 4326)\n\n        return do_sdf\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/overture_data_ingestion/#components.ingestion.spatial_data_ingestion.overture_data_ingestion.OvertureDataIngestion.filter_input_data_objects","title":"<code>filter_input_data_objects(data_type, required_columns)</code>","text":"<p>Filters and processes input Overture Maps data based on the specified data type and columns.</p> <p>This function selects the required columns from the input data objects, filters the data to the current processing iteration extent, and cuts the data to the general extent. If the data type is \"landcover\", \"landuse\", or \"transportation\", it further filters the data by the specified subtypes. If the data type is not \"transportation\", it filters out invalid polygons.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>str</code> <p>The type of data to filter and process. \"landcover\", \"landuse\", \"transportation\", \"buildings\", or \"water\".</p> required <code>required_columns</code> <code>list</code> <p>A list of column names to select from the data. Each column name is a string.</p> required <code>category_col</code> <code>str</code> <p>The name of the category column to filter by when the data type is \"landcover\", \"landuse\", or \"transportation\".</p> required <code>subtypes</code> <code>list</code> <p>A list of subtypes to filter the data by when the data type is \"landcover\", \"landuse\", or \"transportation\". Each subtype is a string. If None, no subtype filtering is performed. Defaults to None.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing the filtered and processed data.</p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/overture_data_ingestion.py</code> <pre><code>def filter_input_data_objects(self, data_type: str, required_columns: List[str]) -&gt; DataFrame:\n    \"\"\"\n    Filters and processes input Overture Maps data based on the specified data type and columns.\n\n    This function selects the required columns from the input data objects,\n    filters the data to the current processing iteration extent, and cuts the data to the general extent.\n    If the data type is \"landcover\", \"landuse\", or \"transportation\", it further filters the data by the specified subtypes.\n    If the data type is not \"transportation\", it filters out invalid polygons.\n\n    Args:\n        data_type (str): The type of data to filter and process. \"landcover\", \"landuse\", \"transportation\", \"buildings\", or \"water\".\n        required_columns (list): A list of column names to select from the data. Each column name is a string.\n        category_col (str): The name of the category column to filter by when the data type is \"landcover\", \"landuse\", or \"transportation\".\n        subtypes (list, optional): A list of subtypes to filter the data by when the data type is \"landcover\", \"landuse\", or \"transportation\".\n            Each subtype is a string. If None, no subtype filtering is performed. Defaults to None.\n\n    Returns:\n        DataFrame: A DataFrame containing the filtered and processed data.\n    \"\"\"\n\n    do_sdf = self.input_data_objects[data_type].df\n    do_sdf = do_sdf.select(*required_columns)\n    do_sdf = utils.cut_geodata_to_extent(do_sdf, self.current_extent, 4326)\n\n    return do_sdf\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/overture_data_ingestion/#components.ingestion.spatial_data_ingestion.overture_data_ingestion.OvertureDataIngestion.get_raw_overture_data","title":"<code>get_raw_overture_data(data_type, cols_to_select, geometry_type)</code>","text":"<p>Retrieves and processes Overture Maps raw data for a specific land use type.</p> <p>This function filters input data objects based on the specified data type and optional filter types, fixes the polygon geometry, and projects the data to a specific CRS. If the persist parameter is set to True, the resulting DataFrame is persisted in memory and disk.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>str</code> <p>The type of land use data to retrieve.</p> required <code>filter_types</code> <code>list</code> <p>A list of subtypes to filter the data by. Each subtype is a string. If None, no filtering is performed. Defaults to None.</p> required <code>persist</code> <code>bool</code> <p>Whether to persist the resulting DataFrame in memory and disk. Defaults to True.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing the processed land use data.</p> <code>DataFrame</code> <p>The DataFrame includes a subtype column and a geometry column.</p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/overture_data_ingestion.py</code> <pre><code>def get_raw_overture_data(self, data_type: str, cols_to_select, geometry_type: int) -&gt; DataFrame:\n    \"\"\"\n    Retrieves and processes Overture Maps raw data for a specific land use type.\n\n    This function filters input data objects based on the specified data type and optional filter types,\n    fixes the polygon geometry, and projects the data to a specific CRS.\n    If the persist parameter is set to True, the resulting DataFrame is persisted in memory and disk.\n\n    Args:\n        data_type (str): The type of land use data to retrieve.\n        filter_types (list, optional): A list of subtypes to filter the data by. Each subtype is a string.\n            If None, no filtering is performed. Defaults to None.\n        persist (bool, optional): Whether to persist the resulting DataFrame in memory and disk. Defaults to True.\n\n    Returns:\n        DataFrame: A DataFrame containing the processed land use data.\n        The DataFrame includes a subtype column and a geometry column.\n    \"\"\"\n\n    sdf = self.filter_input_data_objects(data_type, cols_to_select)\n\n    sdf = utils.project_to_crs(sdf, 4326, 3035)\n    sdf = utils.fix_geometry(sdf, geometry_type)\n    sdf = sdf.withColumn(ColNames.quadkey, F.lit(self.current_quadkey))\n\n    return sdf\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/overture_data_transformation/","title":"overture_data_transformation","text":""},{"location":"reference/components/ingestion/spatial_data_ingestion/overture_data_transformation/#components.ingestion.spatial_data_ingestion.overture_data_transformation.OvertureDataTransformation","title":"<code>OvertureDataTransformation</code>","text":"<p>               Bases: <code>Component</code></p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/overture_data_transformation.py</code> <pre><code>class OvertureDataTransformation(Component):\n    \"\"\" \"\"\"\n\n    COMPONENT_ID = \"OvertureDataTransformation\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.transportation_reclass_map = self.config.geteval(self.COMPONENT_ID, \"transportation_reclass_map\")\n        self.landuse_reclass_map = self.config.geteval(self.COMPONENT_ID, \"landuse_landcover_reclass_map\")\n        self.bulding_reclass_map = self.config.geteval(self.COMPONENT_ID, \"buildings_reclass_map\")\n\n        self.landuse_filter_subtypes = self.config.geteval(self.COMPONENT_ID, \"landuse_filter_subtypes\")\n        self.landcover_filter_subtypes = self.config.geteval(self.COMPONENT_ID, \"landcover_filter_subtypes\")\n        self.transportation_filter_subtypes = self.config.geteval(self.COMPONENT_ID, \"transportation_filter_subtypes\")\n        self.buildings_filter_subtypes = self.config.geteval(self.COMPONENT_ID, \"buildings_filter_subtypes\")\n        self.quadkey_processing_batch = self.config.getint(self.COMPONENT_ID, \"quadkey_processing_batch\")\n        self.number_of_self_clip_iterations = 2\n\n        self.water_filter_area_threshold_m2 = self.config.getint(self.COMPONENT_ID, \"water_filter_area_threshold_m2\")\n        self.landuse_filter_area_threshold_m2 = self.config.getint(\n            self.COMPONENT_ID, \"landuse_filter_area_threshold_m2\"\n        )\n        self.buildings_filter_area_threshold_m2 = self.config.getint(\n            self.COMPONENT_ID, \"buildings_filter_area_threshold_m2\"\n        )\n        self.landcover_filter_area_threshold_m2 = self.config.getint(\n            self.COMPONENT_ID, \"landcover_filter_area_threshold_m2\"\n        )\n\n        self.extent = self.config.geteval(self.COMPONENT_ID, \"extent\")\n        self.quadkey_partition_level = self.config.getint(self.COMPONENT_ID, \"quadkey_partition_level\")\n        self.quadkeys_to_process = self.config.geteval(self.COMPONENT_ID, \"quadkeys_to_process\")\n        self.repartition_factor = self.config.getint(self.COMPONENT_ID, \"repartition_factor\")\n\n    def initalize_data_objects(self):\n\n        self.transportation = \"transportation\"\n        self.landuse = \"landuse\"\n        self.landcover = \"landcover\"\n        self.buildings = \"buildings\"\n        self.water = \"water\"\n\n        transportation_do_path = self.config.get(CONFIG_LANDING_PATHS_KEY, \"transportation_data_landing\")\n        landuse_do_path = self.config.get(CONFIG_LANDING_PATHS_KEY, \"landuse_data_landing\")\n        landcover_do_path = self.config.get(CONFIG_LANDING_PATHS_KEY, \"landcover_data_landing\")\n        buildings_do_path = self.config.get(CONFIG_LANDING_PATHS_KEY, \"buildings_data_landing\")\n        water_do_path = self.config.get(CONFIG_LANDING_PATHS_KEY, \"water_data_landing\")\n\n        self.input_data_objects = {}\n        self.input_data_objects[self.transportation] = LandingGeoParquetDataObject(self.spark, transportation_do_path)\n        self.input_data_objects[self.buildings] = LandingGeoParquetDataObject(self.spark, buildings_do_path)\n        self.input_data_objects[self.landcover] = LandingGeoParquetDataObject(self.spark, landcover_do_path)\n        self.input_data_objects[self.landuse] = LandingGeoParquetDataObject(self.spark, landuse_do_path)\n        self.input_data_objects[self.water] = LandingGeoParquetDataObject(self.spark, water_do_path)\n\n        self.clear_destination_directory = self.config.getboolean(self.COMPONENT_ID, \"clear_destination_directory\")\n\n        transportation_do_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"transportation_data_bronze\")\n        landuse_do_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"landuse_data_bronze\")\n        buildings_do_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"buildings_data_bronze\")\n\n        self.output_data_objects = {}\n\n        self.output_data_objects[BronzeTransportationDataObject.ID] = BronzeTransportationDataObject(\n            self.spark,\n            transportation_do_path,\n        )\n\n        self.output_data_objects[BronzeLanduseDataObject.ID] = BronzeLanduseDataObject(self.spark, landuse_do_path)\n        self.output_data_objects[BronzeBuildingsDataObject.ID] = BronzeBuildingsDataObject(\n            self.spark, buildings_do_path\n        )\n\n        if self.clear_destination_directory:\n            for do in self.output_data_objects.values():\n                self.logger.info(f\"Clearing {do.default_path}\")\n                delete_file_or_folder(self.spark, do.default_path)\n\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n\n        if len(self.quadkeys_to_process) == 0:\n            self.quadkeys_to_process = quadkey_utils.get_quadkeys_for_bbox(self.extent, self.quadkey_partition_level)\n\n        self.logger.info(\"quadkeys to process: \")\n        self.logger.info(f\"{self.quadkeys_to_process}\")\n\n        # generate quadkey batches\n        self.quadkey_batches = self.generate_batches(self.quadkeys_to_process, self.quadkey_processing_batch)\n        self.logger.info(f\"Will be processrd in: {len(self.quadkey_batches)} parts\")\n        processed = 0\n        for quadkey_batch in self.quadkey_batches:\n            self.logger.info(f\"Processing quadkeys {quadkey_batch}\")\n            self.current_quadkey_batch = quadkey_batch\n            self.transform()\n            self.spark.catalog.clearCache()\n            processed += 1\n            self.logger.info(f\"Processed {processed} out of {len(self.quadkey_batches)} batches\")\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def generate_batches(self, elements_list, batch_size):\n        \"\"\"\n        Generates batches of elements from list.\n        \"\"\"\n        return [elements_list[i : i + batch_size] for i in range(0, len(elements_list), batch_size)]\n\n    def write(self, data_object_id: str):\n        self.logger.info(f\"Writing {data_object_id} data object\")\n        self.output_data_objects[data_object_id].write()\n        return None\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n\n        # process transportaion\n        transportation_sdf = self.filter_data(self.transportation, self.transportation_filter_subtypes)\n        transportation_sdf = self.reclassify(\n            transportation_sdf,\n            self.transportation_reclass_map,\n            \"subtype\",\n            ColNames.category,\n            \"unknown\",\n        )\n        transportation_sdf = utils.apply_schema_casting(transportation_sdf, BronzeTransportationDataObject.SCHEMA)\n        transportation_sdf = transportation_sdf.repartition(*BronzeTransportationDataObject.PARTITION_COLUMNS)\n        self.output_data_objects[BronzeTransportationDataObject.ID].df = transportation_sdf\n        self.write(BronzeTransportationDataObject.ID)\n\n        # get higher resolution quadkeys for dissolving geometries\n        child_quadkeys = [quadkey_utils.get_children_quadkeys(quadkey, 14) for quadkey in self.current_quadkey_batch]\n        child_quadkeys = reduce(operator.add, child_quadkeys, [])\n\n        quadkeys_mask_sdf = quadkey_utils.quadkeys_to_extent_dataframe(self.spark, child_quadkeys, 3035)\n        quadkeys_mask_sdf = quadkeys_mask_sdf.withColumnRenamed(ColNames.quadkey, \"mask_quadkey\")\n\n        quadkeys_mask_sdf = quadkeys_mask_sdf.persist()\n        quadkeys_mask_sdf.count()\n        quadkeys_mask_sdf = F.broadcast(quadkeys_mask_sdf)\n\n        # process landuse\n        landuse_cols_to_select = [\"subtype\", \"geometry\", \"quadkey\"]\n\n        landuse_sdf = self.filter_data(\n            self.landuse, self.landuse_filter_subtypes, self.landuse_filter_area_threshold_m2\n        )\n\n        landuse_sdf = utils.merge_geom_within_mask_geom(\n            landuse_sdf,\n            quadkeys_mask_sdf,\n            [\"subtype\", ColNames.quadkey, \"mask_quadkey\"],\n            ColNames.geometry,\n        ).select(\"subtype\", ColNames.geometry, ColNames.quadkey)\n\n        # self clip is neded to get rid of overlapping geometries to make sure that only one landuse category is present in each area\n        # multiple iterations are needed as only one overlap can be clipped at a time\n        for i in range(self.number_of_self_clip_iterations):\n            landuse_sdf = utils.clip_polygons_with_mask_polygons(landuse_sdf, landuse_sdf, landuse_cols_to_select, True)\n            landuse_sdf = landuse_sdf.persist()\n            landuse_sdf.count()\n            self.logger.info(f\"Landuse prepared {i + 1}\")\n\n        landcover_sdf = self.filter_data(\n            self.landcover, self.landcover_filter_subtypes, self.landcover_filter_area_threshold_m2\n        )\n\n        landcover_sdf = utils.merge_geom_within_mask_geom(\n            landcover_sdf,\n            quadkeys_mask_sdf,\n            [\"subtype\", ColNames.quadkey, \"mask_quadkey\"],\n            ColNames.geometry,\n        ).select(\"subtype\", ColNames.geometry, ColNames.quadkey)\n\n        landcover_sdf = utils.clip_polygons_with_mask_polygons(landcover_sdf, landuse_sdf, landuse_cols_to_select)\n        landcover_sdf = landcover_sdf.persist()\n        landcover_sdf.count()\n        self.logger.info(\"Landcover prepared\")\n\n        landuse_sdf = utils.clip_polygons_with_mask_polygons(landuse_sdf, landcover_sdf, landuse_cols_to_select)\n\n        landuse_landcover_sdf = landcover_sdf.union(landuse_sdf)\n        landuse_landcover_sdf = landuse_landcover_sdf.persist()\n        landuse_landcover_sdf.count()\n        self.logger.info(\"Landuse and landcover merged\")\n\n        # process water\n        water_sdf = self.filter_data(self.water, [\"water\"], self.water_filter_area_threshold_m2)\n\n        water_sdf = utils.merge_geom_within_mask_geom(\n            water_sdf,\n            quadkeys_mask_sdf,\n            [\"subtype\", ColNames.quadkey, \"mask_quadkey\"],\n            ColNames.geometry,\n        ).select(\"subtype\", ColNames.geometry, ColNames.quadkey)\n\n        # combine landuse with water\n        landuse_landcover_sdf = utils.clip_polygons_with_mask_polygons(\n            landuse_landcover_sdf, water_sdf, landuse_cols_to_select\n        )\n        landuse_cover_water_sdf = landuse_landcover_sdf.union(water_sdf)\n\n        # reclassify to config categories\n        landuse_cover_water_sdf = self.reclassify(\n            landuse_cover_water_sdf,\n            self.landuse_reclass_map,\n            \"subtype\",\n            ColNames.category,\n            \"open_area\",\n        )\n\n        landuse_cover_water_sdf = landuse_cover_water_sdf.persist()\n        landuse_cover_water_sdf.count()\n        self.logger.info(\"Landuse, landcover and water merged\")\n\n        # add buildings data to full landuse\n        buildings_sdf = self.filter_data(self.buildings, self.buildings_filter_subtypes)\n        buildings_sdf = self.reclassify(\n            buildings_sdf,\n            self.bulding_reclass_map,\n            \"subtype\",\n            ColNames.category,\n            \"other_builtup\",\n        )\n\n        # slightly buffer buildings to merge small geometries together\n        # buildings_buff_sdf = buildings_sdf.withColumn(ColNames.geometry, STF.ST_Buffer(ColNames.geometry, 2))\n\n        merged_buildings_sdf = utils.merge_geom_within_mask_geom(\n            buildings_sdf,\n            quadkeys_mask_sdf,\n            [ColNames.category, ColNames.quadkey, \"mask_quadkey\"],\n            ColNames.geometry,\n        ).select(ColNames.category, ColNames.geometry, ColNames.quadkey)\n\n        # filter out very small buildings\n        merged_buildings_sdf = merged_buildings_sdf.filter(\n            STF.ST_Area(ColNames.geometry) &gt; self.buildings_filter_area_threshold_m2\n        )\n        merged_buildings_sdf = merged_buildings_sdf.persist()\n        merged_buildings_sdf.count()\n        self.logger.info(\"Buildings merged\")\n\n        # combine landuse with buildings\n        landuse_cover_water_sdf = utils.clip_polygons_with_mask_polygons(\n            landuse_cover_water_sdf,\n            merged_buildings_sdf,\n            [ColNames.category, ColNames.geometry, ColNames.quadkey],\n        )\n\n        landuse_cover_water_sdf = landuse_cover_water_sdf.union(merged_buildings_sdf)\n\n        landuse_cover_water_sdf = utils.apply_schema_casting(landuse_cover_water_sdf, BronzeLanduseDataObject.SCHEMA)\n        landuse_cover_water_sdf = landuse_cover_water_sdf.repartition(*BronzeLanduseDataObject.PARTITION_COLUMNS)\n        self.output_data_objects[BronzeLanduseDataObject.ID].df = landuse_cover_water_sdf\n        self.write(BronzeLanduseDataObject.ID)\n\n        # process buildings\n        buildings_sdf = utils.apply_schema_casting(buildings_sdf, BronzeBuildingsDataObject.SCHEMA)\n        buildings_sdf = buildings_sdf.repartition(*BronzeBuildingsDataObject.PARTITION_COLUMNS)\n        self.output_data_objects[BronzeBuildingsDataObject.ID].df = buildings_sdf\n        self.write(BronzeBuildingsDataObject.ID)\n\n    def filter_data(self, data_type, subtypes_to_filter, area_filter_threshold_m2=None) -&gt; DataFrame:\n        \"\"\"\n        Processes and returns Overture Maps transportation data.\n\n        This function filters input data objects based on the transportation class and specified subtypes,\n        reclassifies the transportation data based on a predefined map, and selects specific columns for further processing.\n        It then fixes the line geometry, assigns quadkeys, projects the data to a specific CRS,\n        orders the data by quadkey, and repartitions the data based on quadkey.\n\n        Returns:\n            DataFrame: A DataFrame containing the processed transportation data.\n            The DataFrame includes a category column, a geometry column, and a quadkey column.\n        \"\"\"\n\n        do_sdf = self.input_data_objects[data_type].df\n        do_sdf = do_sdf.repartition(self.repartition_factor)\n        # TODO move to data ingestion\n        if data_type == \"transportation\":\n            do_sdf = do_sdf.filter(F.col(\"subtype\").isin([\"road\", \"rail\"]))\n            # assign subtype railroad to rail\n            do_sdf = do_sdf.withColumn(\n                \"class\", F.when(F.col(\"subtype\") == \"rail\", \"railroad\").otherwise(F.col(\"class\"))\n            ).drop(\"subtype\")\n            do_sdf = do_sdf.withColumnRenamed(\"class\", \"subtype\")\n\n        # TODO make subtype column a parameter\n        do_sdf = do_sdf.withColumn(\"subtype\", F.coalesce(F.col(\"subtype\"), F.lit(\"unknown\")))\n        do_sdf = do_sdf.filter(F.col(\"subtype\").isin(subtypes_to_filter))\n\n        mask_sdf = quadkey_utils.quadkeys_to_extent_dataframe(self.spark, self.current_quadkey_batch, 3035)\n        do_sdf = self.cut_geoms_with_mask_polygons(do_sdf, mask_sdf, [\"subtype\", ColNames.geometry], [ColNames.quadkey])\n\n        do_sdf = do_sdf.withColumn(\n            ColNames.geometry, F.explode(STF.ST_Dump(ColNames.geometry)).alias(ColNames.geometry)\n        )\n        if area_filter_threshold_m2:\n            do_sdf = do_sdf.filter(STF.ST_Area(ColNames.geometry) &gt; area_filter_threshold_m2)\n\n        return do_sdf\n\n    def cut_geoms_with_mask_polygons(\n        self,\n        sdf: DataFrame,\n        mask_sdf: DataFrame,\n        orig_cols_to_select: List[str],\n        cut_cols_to_select: List[str],\n        cut_buffer: int = 100,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Cuts geometries in a DataFrame with mask polygons from another DataFrame.\n        \"\"\"\n        cut_sdf = sdf.select(*orig_cols_to_select).join(\n            mask_sdf.select(F.col(ColNames.geometry).alias(\"cut_geometry\"), *cut_cols_to_select),\n            on=STP.ST_Intersects(ColNames.geometry, \"cut_geometry\"),\n        )\n\n        cut_sdf = cut_sdf.withColumn(\n            ColNames.geometry,\n            STF.ST_Intersection(cut_sdf[ColNames.geometry], STF.ST_Buffer(cut_sdf[\"cut_geometry\"], cut_buffer)),\n        ).drop(\"cut_geometry\")\n\n        return cut_sdf\n\n    @staticmethod\n    def reclassify(\n        sdf: DataFrame,\n        reclass_map: Dict[str, List[str]],\n        class_column: str,\n        reclass_column: str,\n        default_reclass: str = \"unknown\",\n    ) -&gt; DataFrame:\n        \"\"\"\n        Reclassifies a column in a DataFrame based on a reclassification map.\n\n        This function takes a DataFrame, a reclassification map, and the names of a class column and a reclassification column.\n        It creates a new column in the DataFrame by reclassifying the values in the class column based on the reclassification map.\n        If a value in the class column is not in the reclassification map, it is classified as the default reclassification.\n\n        Args:\n            sdf (DataFrame): The DataFrame to reclassify.\n            reclass_map (dict): The reclassification map. The keys are the reclassified classes, and the values are lists of original classes.\n            class_column (str): The name of the column in the DataFrame to reclassify.\n            reclass_column (str): The name of the new column to create with the reclassified classes.\n            default_reclass (str, optional): The class to assign to values in the class column that are not in the map. Defaults to \"unknown\".\n\n        Returns:\n            DataFrame: A DataFrame containing the same rows as the input DataFrame, but with the class column replaced by the reclassification column.\n        \"\"\"\n        # function implementation...\n\n        keys = list(reclass_map.keys())\n        reclass_expr = F.when(F.col(class_column).isin(reclass_map[keys[0]]), keys[0])\n\n        for key in keys[1:]:\n            reclass_expr = reclass_expr.when(F.col(class_column).isin(reclass_map[key]), key)\n\n        reclass_expr = reclass_expr.otherwise(default_reclass)\n\n        sdf = sdf.withColumn(reclass_column, reclass_expr)\n\n        return sdf.select(ColNames.category, ColNames.geometry, ColNames.quadkey).drop(class_column)\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/overture_data_transformation/#components.ingestion.spatial_data_ingestion.overture_data_transformation.OvertureDataTransformation.cut_geoms_with_mask_polygons","title":"<code>cut_geoms_with_mask_polygons(sdf, mask_sdf, orig_cols_to_select, cut_cols_to_select, cut_buffer=100)</code>","text":"<p>Cuts geometries in a DataFrame with mask polygons from another DataFrame.</p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/overture_data_transformation.py</code> <pre><code>def cut_geoms_with_mask_polygons(\n    self,\n    sdf: DataFrame,\n    mask_sdf: DataFrame,\n    orig_cols_to_select: List[str],\n    cut_cols_to_select: List[str],\n    cut_buffer: int = 100,\n) -&gt; DataFrame:\n    \"\"\"\n    Cuts geometries in a DataFrame with mask polygons from another DataFrame.\n    \"\"\"\n    cut_sdf = sdf.select(*orig_cols_to_select).join(\n        mask_sdf.select(F.col(ColNames.geometry).alias(\"cut_geometry\"), *cut_cols_to_select),\n        on=STP.ST_Intersects(ColNames.geometry, \"cut_geometry\"),\n    )\n\n    cut_sdf = cut_sdf.withColumn(\n        ColNames.geometry,\n        STF.ST_Intersection(cut_sdf[ColNames.geometry], STF.ST_Buffer(cut_sdf[\"cut_geometry\"], cut_buffer)),\n    ).drop(\"cut_geometry\")\n\n    return cut_sdf\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/overture_data_transformation/#components.ingestion.spatial_data_ingestion.overture_data_transformation.OvertureDataTransformation.filter_data","title":"<code>filter_data(data_type, subtypes_to_filter, area_filter_threshold_m2=None)</code>","text":"<p>Processes and returns Overture Maps transportation data.</p> <p>This function filters input data objects based on the transportation class and specified subtypes, reclassifies the transportation data based on a predefined map, and selects specific columns for further processing. It then fixes the line geometry, assigns quadkeys, projects the data to a specific CRS, orders the data by quadkey, and repartitions the data based on quadkey.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing the processed transportation data.</p> <code>DataFrame</code> <p>The DataFrame includes a category column, a geometry column, and a quadkey column.</p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/overture_data_transformation.py</code> <pre><code>def filter_data(self, data_type, subtypes_to_filter, area_filter_threshold_m2=None) -&gt; DataFrame:\n    \"\"\"\n    Processes and returns Overture Maps transportation data.\n\n    This function filters input data objects based on the transportation class and specified subtypes,\n    reclassifies the transportation data based on a predefined map, and selects specific columns for further processing.\n    It then fixes the line geometry, assigns quadkeys, projects the data to a specific CRS,\n    orders the data by quadkey, and repartitions the data based on quadkey.\n\n    Returns:\n        DataFrame: A DataFrame containing the processed transportation data.\n        The DataFrame includes a category column, a geometry column, and a quadkey column.\n    \"\"\"\n\n    do_sdf = self.input_data_objects[data_type].df\n    do_sdf = do_sdf.repartition(self.repartition_factor)\n    # TODO move to data ingestion\n    if data_type == \"transportation\":\n        do_sdf = do_sdf.filter(F.col(\"subtype\").isin([\"road\", \"rail\"]))\n        # assign subtype railroad to rail\n        do_sdf = do_sdf.withColumn(\n            \"class\", F.when(F.col(\"subtype\") == \"rail\", \"railroad\").otherwise(F.col(\"class\"))\n        ).drop(\"subtype\")\n        do_sdf = do_sdf.withColumnRenamed(\"class\", \"subtype\")\n\n    # TODO make subtype column a parameter\n    do_sdf = do_sdf.withColumn(\"subtype\", F.coalesce(F.col(\"subtype\"), F.lit(\"unknown\")))\n    do_sdf = do_sdf.filter(F.col(\"subtype\").isin(subtypes_to_filter))\n\n    mask_sdf = quadkey_utils.quadkeys_to_extent_dataframe(self.spark, self.current_quadkey_batch, 3035)\n    do_sdf = self.cut_geoms_with_mask_polygons(do_sdf, mask_sdf, [\"subtype\", ColNames.geometry], [ColNames.quadkey])\n\n    do_sdf = do_sdf.withColumn(\n        ColNames.geometry, F.explode(STF.ST_Dump(ColNames.geometry)).alias(ColNames.geometry)\n    )\n    if area_filter_threshold_m2:\n        do_sdf = do_sdf.filter(STF.ST_Area(ColNames.geometry) &gt; area_filter_threshold_m2)\n\n    return do_sdf\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/overture_data_transformation/#components.ingestion.spatial_data_ingestion.overture_data_transformation.OvertureDataTransformation.generate_batches","title":"<code>generate_batches(elements_list, batch_size)</code>","text":"<p>Generates batches of elements from list.</p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/overture_data_transformation.py</code> <pre><code>def generate_batches(self, elements_list, batch_size):\n    \"\"\"\n    Generates batches of elements from list.\n    \"\"\"\n    return [elements_list[i : i + batch_size] for i in range(0, len(elements_list), batch_size)]\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/overture_data_transformation/#components.ingestion.spatial_data_ingestion.overture_data_transformation.OvertureDataTransformation.reclassify","title":"<code>reclassify(sdf, reclass_map, class_column, reclass_column, default_reclass='unknown')</code>  <code>staticmethod</code>","text":"<p>Reclassifies a column in a DataFrame based on a reclassification map.</p> <p>This function takes a DataFrame, a reclassification map, and the names of a class column and a reclassification column. It creates a new column in the DataFrame by reclassifying the values in the class column based on the reclassification map. If a value in the class column is not in the reclassification map, it is classified as the default reclassification.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The DataFrame to reclassify.</p> required <code>reclass_map</code> <code>dict</code> <p>The reclassification map. The keys are the reclassified classes, and the values are lists of original classes.</p> required <code>class_column</code> <code>str</code> <p>The name of the column in the DataFrame to reclassify.</p> required <code>reclass_column</code> <code>str</code> <p>The name of the new column to create with the reclassified classes.</p> required <code>default_reclass</code> <code>str</code> <p>The class to assign to values in the class column that are not in the map. Defaults to \"unknown\".</p> <code>'unknown'</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing the same rows as the input DataFrame, but with the class column replaced by the reclassification column.</p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/overture_data_transformation.py</code> <pre><code>@staticmethod\ndef reclassify(\n    sdf: DataFrame,\n    reclass_map: Dict[str, List[str]],\n    class_column: str,\n    reclass_column: str,\n    default_reclass: str = \"unknown\",\n) -&gt; DataFrame:\n    \"\"\"\n    Reclassifies a column in a DataFrame based on a reclassification map.\n\n    This function takes a DataFrame, a reclassification map, and the names of a class column and a reclassification column.\n    It creates a new column in the DataFrame by reclassifying the values in the class column based on the reclassification map.\n    If a value in the class column is not in the reclassification map, it is classified as the default reclassification.\n\n    Args:\n        sdf (DataFrame): The DataFrame to reclassify.\n        reclass_map (dict): The reclassification map. The keys are the reclassified classes, and the values are lists of original classes.\n        class_column (str): The name of the column in the DataFrame to reclassify.\n        reclass_column (str): The name of the new column to create with the reclassified classes.\n        default_reclass (str, optional): The class to assign to values in the class column that are not in the map. Defaults to \"unknown\".\n\n    Returns:\n        DataFrame: A DataFrame containing the same rows as the input DataFrame, but with the class column replaced by the reclassification column.\n    \"\"\"\n    # function implementation...\n\n    keys = list(reclass_map.keys())\n    reclass_expr = F.when(F.col(class_column).isin(reclass_map[keys[0]]), keys[0])\n\n    for key in keys[1:]:\n        reclass_expr = reclass_expr.when(F.col(class_column).isin(reclass_map[key]), key)\n\n    reclass_expr = reclass_expr.otherwise(default_reclass)\n\n    sdf = sdf.withColumn(reclass_column, reclass_expr)\n\n    return sdf.select(ColNames.category, ColNames.geometry, ColNames.quadkey).drop(class_column)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/","title":"synthetic","text":""},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/","title":"synthetic_diaries","text":"<p>Module that generates a MNO synthetic network.</p>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries","title":"<code>SyntheticDiaries</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class that generates the synthetic activity-trip diaries data. It inherits from the Component abstract class.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>class SyntheticDiaries(Component):\n    \"\"\"\n    Class that generates the synthetic activity-trip diaries data.\n    It inherits from the Component abstract class.\n    \"\"\"\n\n    COMPONENT_ID = \"SyntheticDiaries\"\n\n    def __init__(self, general_config_path: str, component_config_path: str):\n        # keep super class init method:\n        super().__init__(general_config_path=general_config_path, component_config_path=component_config_path)\n\n        # and additionally:\n        # self.n_partitions = self.config.getint(self.COMPONENT_ID, \"n_partitions\")\n\n        self.number_of_users = self.config.getint(self.COMPONENT_ID, \"number_of_users\")\n\n        self.date_format = self.config.get(self.COMPONENT_ID, \"date_format\")\n        self.initial_date = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"initial_date\"), self.date_format\n        ).date()\n        self.number_of_dates = self.config.getint(self.COMPONENT_ID, \"number_of_dates\")\n        self.date_range = [(self.initial_date + datetime.timedelta(days=d)) for d in range(self.number_of_dates)]\n\n        self.longitude_min = self.config.getfloat(self.COMPONENT_ID, \"longitude_min\")\n        self.longitude_max = self.config.getfloat(self.COMPONENT_ID, \"longitude_max\")\n        self.latitude_min = self.config.getfloat(self.COMPONENT_ID, \"latitude_min\")\n        self.latitude_max = self.config.getfloat(self.COMPONENT_ID, \"latitude_max\")\n\n        self.home_work_distance_min = self.config.getfloat(self.COMPONENT_ID, \"home_work_distance_min\")\n        self.home_work_distance_max = self.config.getfloat(self.COMPONENT_ID, \"home_work_distance_max\")\n        self.other_distance_min = self.config.getfloat(self.COMPONENT_ID, \"other_distance_min\")\n        self.other_distance_max = self.config.getfloat(self.COMPONENT_ID, \"other_distance_max\")\n\n        self.home_duration_min = self.config.getfloat(self.COMPONENT_ID, \"home_duration_min\")\n        self.home_duration_max = self.config.getfloat(self.COMPONENT_ID, \"home_duration_max\")\n        self.work_duration_min = self.config.getfloat(self.COMPONENT_ID, \"work_duration_min\")\n        self.work_duration_max = self.config.getfloat(self.COMPONENT_ID, \"work_duration_max\")\n        self.other_duration_min = self.config.getfloat(self.COMPONENT_ID, \"other_duration_min\")\n        self.other_duration_max = self.config.getfloat(self.COMPONENT_ID, \"other_duration_max\")\n\n        self.displacement_speed = self.config.getfloat(self.COMPONENT_ID, \"displacement_speed\")\n\n        self.stay_sequence_superset = self.config.get(self.COMPONENT_ID, \"stay_sequence_superset\").split(\",\")\n        self.stay_sequence_probabilities = [\n            float(w)\n            for w in self.config.get(self.COMPONENT_ID, \"stay_sequence_probabilities\").split(\n                \",\"\n            )  # TODO: cambiar por stay_sequence\n        ]\n        if len(self.stay_sequence_superset) != len(self.stay_sequence_probabilities):\n            raise ValueError(\n                \"Configuration error: stay_sequence_superset and stay_sequence_probabilities must have the same length\"\n            )\n\n    def initalize_data_objects(self):\n        output_synthetic_diaries_data_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"diaries_data_bronze\")\n        bronze_synthetic_diaries = BronzeSyntheticDiariesDataObject(self.spark, output_synthetic_diaries_data_path)\n        self.output_data_objects = {BronzeSyntheticDiariesDataObject.ID: bronze_synthetic_diaries}\n\n    def read(self):\n        pass  # No input datasets are used in this component\n\n    def transform(self):\n        spark = self.spark\n        activities_df = spark.createDataFrame(self.generate_activities())\n        activities_df = calc_hashed_user_id(activities_df)\n        columns = {\n            field.name: F.col(field.name).cast(field.dataType)\n            for field in BronzeSyntheticDiariesDataObject.SCHEMA.fields\n        }\n        activities_df = activities_df.withColumns(columns)\n        self.output_data_objects[BronzeSyntheticDiariesDataObject.ID].df = activities_df\n\n    def haversine(self, lon1: float, lat1: float, lon2: float, lat2: float) -&gt; float:\n        \"\"\"\n        Calculate the haversine distance in meters between two points.\n\n        Args:\n            lon1 (float): longitude of first point, in decimal degrees.\n            lat1 (float): latitude of first point, in decimal degrees.\n            lon2 (float): longitude of second point, in decimal degrees.\n            lat2 (float): latitude of second point, in decimal degrees.\n\n        Returns:\n            float: distance between both points, in meters.\n        \"\"\"\n        r = 6_371_000  # Radius of earth in meters.\n\n        # convert decimal degrees to radians\n        lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n\n        # haversine formula\n        dlon = lon2 - lon1\n        dlat = lat2 - lat1\n        a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n        c = 2 * asin(sqrt(a))\n        return c * r\n\n    def random_seed_number_generator(\n        self, base_seed: int, agent_id: int = None, date: datetime.date = None, i: int = None\n    ) -&gt; int:\n        \"\"\"\n        Generate random seed integer based on provided arguments.\n\n        Args:\n            base_seed (int): base integer for operations.\n            agent_id (int, optional): agent identifier. Defaults to None.\n            date (datetime.date, optional): date. Defaults to None.\n            i (int, optional): position integer. Defaults to None.\n\n        Returns:\n            int: generated random seed integer.\n        \"\"\"\n        seed = base_seed\n        if agent_id is not None:\n            seed += int(agent_id) * 100\n        if date is not None:\n            start_datetime = datetime.datetime.combine(date, datetime.time(0))\n            seed += int(start_datetime.timestamp())\n        if i is not None:\n            seed += i\n        return seed\n\n    def calculate_trip_time(self, o_location: Tuple[float, float], d_location: Tuple[float, float]) -&gt; float:\n        \"\"\"\n        Calculate trip time given an origin location and a destination\n        location, according to the specified trip speed.\n\n        Args:\n            o_location (Tuple[float,float]): lon, lat of 1st point,\n                in decimal degrees.\n            d_location (Tuple[float,float]): lon, lat of 2nd point,\n                in decimal degrees.\n\n        Returns:\n            float: trip time, in seconds.\n        \"\"\"\n        trip_distance = self.haversine(o_location[0], o_location[1], d_location[0], d_location[1])  # m\n        trip_speed = self.displacement_speed  # m/s\n        trip_time = trip_distance / trip_speed  # s\n        return trip_time\n\n    def calculate_trip_final_time(\n        self,\n        origin_location: Tuple[float, float],\n        destin_location: Tuple[float, float],\n        origin_timestamp: datetime.datetime,\n    ) -&gt; datetime.datetime:\n        \"\"\"\n        Calculate end time of a trip given an origin time, an origin location,\n        a destination location and a speed.\n\n        Args:\n            origin_location (Tuple[float,float]): lon, lat of 1st point,\n                in decimal degrees.\n            destin_location (Tuple[float,float]): lon, lat of 2nd point,\n                in decimal degrees.\n            origin_timestamp (datetime.datetime): start time of trip.\n\n        Returns:\n            datetime.datetime: end time of trip.\n        \"\"\"\n\n        trip_time = self.calculate_trip_time(origin_location, destin_location)  # s\n        return origin_timestamp + datetime.timedelta(seconds=trip_time)\n\n    def generate_stay_location(\n        self,\n        stay_type: str,\n        home_location: Tuple[float, float],\n        work_location: Tuple[float, float],\n        previous_location: Tuple[float, float],\n        user_id: int,\n        date: datetime.date,\n        i: int,\n    ) -&gt; Tuple[float, float]:\n        \"\"\"\n        Generate a random activity location within the bounding box limits based\n        on the activity type and previous activity locations.\n\n        Args:\n            stay_type (str): type of stay (\"home\", \"work\" or \"other\").\n            home_location (Tuple[float,float]): coordinates of home location.\n            work_location (Tuple[float,float]): coordinates of work location.\n            previous_location (Tuple[float,float]): coordinates of previous\n                activity location.\n            user_id (int): agent identifier, used for random seed generation.\n            date (datetime.date): date, used for random seed generation.\n            i (int): activity position, used for random seed generation.\n\n        Returns:\n            Tuple[float,float]: randomly generated activity location coordinates.\n        \"\"\"\n        if stay_type == \"home\":\n            location = home_location\n        elif stay_type == \"work\":\n            location = work_location\n        else:\n            location = self.generate_other_location(user_id, date, i, home_location, previous_location)\n        return location\n\n    def create_agent_activities_min_duration(\n        self,\n        user_id: int,\n        agent_stay_type_sequence: List[str],\n        home_location: Tuple[float, float],\n        work_location: Tuple[float, float],\n        date: datetime.date,\n        start_of_date: datetime.datetime,\n        end_of_date: datetime.datetime,\n    ) -&gt; List[Row]:\n        \"\"\"\n        Generate activities of the minimum duration following the specified agent\n        activity sequence for this agent and date.\n\n        Args:\n            user_id (int): agent identifier.\n            agent_stay_type_sequence (List[str]): list of generated stay types,\n                each represented by a string indicating the stay type.\n            home_location (Tuple[float,float]): coordinates of home location.\n            work_location (Tuple[float,float]): coordinates of work location.\n            date (datetime.date): date for activity sequence generation, used for\n                timestamps and random seed generation.\n            start_of_date (datetime.datetime): timestamp of current date at 00:00:00.\n            end_of_date (datetime.datetime): timestamp of current date at 23:59:59.\n\n        Returns:\n            List[Row]: list of generated activities and trips, each represented by a\n                spark row object with all its information.\n        \"\"\"\n        date_activities = []\n        previous_location = None\n        for i, stay_type in enumerate(agent_stay_type_sequence):\n            # activity location:\n            location = self.generate_stay_location(\n                stay_type, home_location, work_location, previous_location, user_id, date, i\n            )\n            # previous move (unless first stay)\n            if i != 0:\n                # move timestamps:\n                trip_initial_timestamp = stay_final_timestamp\n                trip_final_timestamp = self.calculate_trip_final_time(\n                    previous_location, location, trip_initial_timestamp\n                )\n                # add move:\n                date_activities.append(\n                    Row(\n                        user_id=user_id,\n                        activity_type=\"move\",\n                        stay_type=\"move\",\n                        longitude=float(\"nan\"),\n                        latitude=float(\"nan\"),\n                        initial_timestamp=trip_initial_timestamp,\n                        final_timestamp=trip_final_timestamp,\n                        year=date.year,\n                        month=date.month,\n                        day=date.day,\n                    )\n                )\n            # stay timestamps:\n            stay_initial_timestamp = start_of_date if i == 0 else trip_final_timestamp\n            stay_duration = self.generate_min_stay_duration(stay_type)\n            stay_final_timestamp = stay_initial_timestamp + datetime.timedelta(hours=stay_duration)\n\n            # add stay:\n            date_activities.append(\n                Row(\n                    user_id=user_id,\n                    activity_type=\"stay\",\n                    stay_type=stay_type,\n                    longitude=location[0],\n                    latitude=location[1],\n                    initial_timestamp=stay_initial_timestamp,\n                    final_timestamp=stay_final_timestamp,\n                    year=date.year,\n                    month=date.month,\n                    day=date.day,\n                )\n            )\n\n            previous_location = location\n\n        # after the iterations:\n        if not date_activities:  # 0 stays\n            condition_for_full_home = True\n        elif stay_final_timestamp &gt; end_of_date:  # too many stays\n            condition_for_full_home = True\n        else:\n            condition_for_full_home = False\n\n        if condition_for_full_home:  # simple \"only home\" diary\n            return [\n                Row(\n                    user_id=user_id,\n                    activity_type=\"stay\",\n                    stay_type=\"home\",\n                    longitude=home_location[0],\n                    latitude=home_location[1],\n                    initial_timestamp=start_of_date,\n                    final_timestamp=end_of_date,\n                    year=date.year,\n                    month=date.month,\n                    day=date.day,\n                )\n            ]\n        else:\n            return date_activities  # actual generated diary\n\n    def update_spark_row(self, row: Row, column_name: str, new_value: Any) -&gt; Row:\n        \"\"\"\n        Return an updated spark row object, changing the value of a column.\n\n        Args:\n            row (Row): input spark row.\n            column_name (str): name of column to modify.\n            new_value (Any): new value to assign.\n\n        Returns:\n            Row: modified spark row\n        \"\"\"\n        return Row(**{**row.asDict(), **{column_name: new_value}})\n\n    def adjust_activity_times(\n        self,\n        date_activities: List[Row],\n        remaining_time: float,\n        user_id: int,\n        date: datetime.date,\n        start_of_date: datetime.datetime,\n        end_of_date: datetime.datetime,\n    ):\n        \"\"\"\n        Modifies the \"date_activities\" list, changing the initial and\n        final timestamps of both stays and moves probablilistically in order to\n        generate stay durations different from the minimum and adjust the\n        durations of the activities to the 24h of the day.\n\n        Args:\n            date_activities (List[Row]): list of generated activities (stays and\n                moves) of the agent for the specified date. Each activity/trip is a\n                spark row object.\n            user_id (int): agent identifier.\n            date (datetime.date): date for activity sequence generation, used for\n                timestamps and random seed generation.\n            start_of_date (datetime.datetime): timestamp of current date at 00:00:00.\n            end_of_date (datetime.datetime): timestamp of current date at 23:59:59.\n        \"\"\"\n        current_timestamp = start_of_date\n        for i, activity_row in enumerate(date_activities):\n            if activity_row.activity_type == \"stay\":  # stay:\n                stay_type = activity_row.stay_type\n                old_stay_duration = (\n                    activity_row.final_timestamp - activity_row.initial_timestamp\n                ).total_seconds() / 3600.0\n                new_initial_timestamp = current_timestamp\n                if i == len(date_activities) - 1:\n                    new_final_timestamp = end_of_date\n                    remaining_time = 0.0\n                else:\n                    new_stay_duration = self.generate_stay_duration(user_id, date, i, stay_type, remaining_time)\n                    new_duration_td = datetime.timedelta(seconds=new_stay_duration * 3600.0)\n                    new_final_timestamp = new_initial_timestamp + new_duration_td\n                    remaining_time -= new_stay_duration - old_stay_duration\n            else:  # move:\n                old_move_duration = activity_row.final_timestamp - activity_row.initial_timestamp\n                new_initial_timestamp = current_timestamp\n                new_final_timestamp = new_initial_timestamp + old_move_duration\n\n            # common for all activities (stays and moves):\n            activity_row = self.update_spark_row(activity_row, \"initial_timestamp\", new_initial_timestamp)\n            activity_row = self.update_spark_row(activity_row, \"final_timestamp\", new_final_timestamp)\n            date_activities[i] = activity_row\n            current_timestamp = new_final_timestamp\n\n    def add_agent_date_activities(\n        self,\n        activities: List[Row],\n        user_id: int,\n        agent_stay_type_sequence: List[str],\n        home_location: Tuple[float, float],\n        work_location: Tuple[float, float],\n        date: datetime.date,\n        start_of_date: datetime.datetime,\n        end_of_date: datetime.datetime,\n    ):\n        \"\"\"\n        For a specific date and user, generate a sequence of activities probabilistically\n        according to the specified activity superset and the activity probabilities.\n        Firstly, assign to each of these activities the minimum duration considered for\n        that activity type. Trip times are based on Pythagorean distance and a specified\n        average speed.\n        If the sum of all minimum duration of the activities and the duration of the trips\n        is higher than the 24h of the day, then assign just one \"home\" activity to the\n        agent from 00:00:00 to 23:59:59.\n        Else, there will be a remaining time. E.g., the diary of an agent, after adding\n        up all trip durations and minimum activity durations may end at 20:34:57. There is\n        a remaining time to complete the full diary (23:59:59 - 20:34:57).\n        Adjust activity times probabilistically according to the maximum activity duration\n        and this remaining time, making the diary end at exactly 23:59:59.\n\n        Args:\n            activities (List[Row]): list of generated activities (stays and moves) for\n                the agent for all of the specified dates. Each activity is a spark\n                row object.\n            user_id (int): agent identifier.\n            agent_stay_type_sequence (List[str]): list of generated stay types,\n                each represented by a string indicating the stay type.\n            home_location (Tuple[float,float]): coordinates of home location.\n            work_location (Tuple[float,float]): coordinates of work location.\n            date (datetime.date): date for activity sequence generation, used for\n                timestamps and random seed generation.\n            start_of_date (datetime.datetime): timestamp of current date at 00:00:00.\n            end_of_date (datetime.datetime): timestamp of current date at 23:59:59.\n        \"\"\"\n        date_activities = self.create_agent_activities_min_duration(\n            user_id, agent_stay_type_sequence, home_location, work_location, date, start_of_date, end_of_date\n        )\n        remaining_time = (end_of_date - date_activities[-1].final_timestamp).total_seconds() / 3600.0\n\n        if remaining_time != 0:\n            self.adjust_activity_times(\n                date_activities,\n                remaining_time,\n                user_id,\n                date,\n                start_of_date,\n                end_of_date,\n            )\n        activities += date_activities\n\n    def add_date_activities(self, date: datetime.date, activities: List[Row]):\n        \"\"\"\n        Generate activity (stays and moves) rows for a specific date according to\n        parameters.\n\n        Args:\n            date (datetime.date): date for activity sequence generation, used for\n                timestamps and random seed generation.\n            activities (List[Row]): list of generated activities (stays and moves) for\n                the agent for all of the specified dates. Each activity is a spark\n                row object.\n        \"\"\"\n        # Start of date, end of date: datetime object generation\n        start_of_date = datetime.datetime.combine(date, datetime.time(0, 0, 0))\n        end_of_date = datetime.datetime.combine(date, datetime.time(23, 59, 59))\n        for user_id in range(self.number_of_users):\n            # generate user information:\n            agent_stay_type_sequence = self.generate_stay_type_sequence(user_id, date)\n            home_location = self.generate_home_location(user_id)\n            work_location = self.generate_work_location(user_id, home_location)\n            self.add_agent_date_activities(\n                activities,\n                user_id,\n                agent_stay_type_sequence,\n                home_location,\n                work_location,\n                date,\n                start_of_date,\n                end_of_date,\n            )\n\n    def generate_activities(self) -&gt; List[Row]:\n        \"\"\"\n        Generate activity and trip rows according to parameters.\n\n        Returns:\n            List[Row]: list of generated activities and trips for the agent for all\n                of the specified dates. Each activity/trip is a spark row object.\n        \"\"\"\n        activities = []\n        for date in self.date_range:\n            self.add_date_activities(date, activities)\n        return activities\n\n    def generate_lonlat_at_distance(self, lon1: float, lat1: float, d: float, seed: int) -&gt; Tuple[float, float]:\n        \"\"\"\n        Given a point (lon, lat) and a distance, in meters, calculate a new random\n        point that is exactly at the specified distance of the provided lon, lat.\n\n        Args:\n            lon1 (float): longitude of point, specified in decimal degrees.\n            lat1 (float): latitude of point, specified in decimal degrees.\n            d (float): distance, in meters.\n            seed (int): random seed integer.\n\n        Returns:\n            Tuple[float, float]: coordinates of randomly generated point.\n        \"\"\"\n        r = 6_371_000  # Radius of earth in meters.\n\n        d_x = Random(seed).uniform(0, d)\n        d_y = sqrt(d**2 - d_x**2)\n\n        # firstly, convert lat to radians for later\n        lat1_radians = lat1 * pi / 180.0\n\n        # how many meters correspond to one degree of latitude?\n        deg_to_meters = r * pi / 180  # aprox. 111111 meters\n        # thus, the northwards displacement, in degrees of latitude is:\n        north_delta = d_y / deg_to_meters\n\n        # but one degree of longitude does not always correspond to the\n        # same distance... depends on the latitude at where you are!\n        parallel_radius = abs(r * cos(lat1_radians))\n        deg_to_meters = parallel_radius * pi / 180  # variable\n        # thus, the eastwards displacement, in degrees of longitude is:\n        east_delta = d_x / deg_to_meters\n\n        final_lon = lon1 + east_delta * Random(seed).choice([-1, 1])\n        final_lat = lat1 + north_delta * Random(seed).choice([-1, 1])\n\n        return (final_lon, final_lat)\n\n    def generate_home_location(self, agent_id: int) -&gt; Tuple[float, float]:\n        \"\"\"\n        Generate random home location based on bounding box limits.\n\n        Args:\n            agent_id (int): identifier of agent, used for random seed generation.\n\n        Returns:\n            Tuple[float,float]: coordinates of generated home location.\n        \"\"\"\n        seed_lon = self.random_seed_number_generator(1, agent_id)\n        seed_lat = self.random_seed_number_generator(2, agent_id)\n        hlon = Random(seed_lon).uniform(self.longitude_min, self.longitude_max)\n        hlat = Random(seed_lat).uniform(self.latitude_min, self.latitude_max)\n        return (hlon, hlat)\n\n    def generate_work_location(\n        self, agent_id: int, home_location: Tuple[float, float], seed: int = 4\n    ) -&gt; Tuple[float, float]:\n        \"\"\"\n        Generate random work location based on home location and maximum distance to\n        home. If the work location falls outside of bounding box limits, try again.\n\n        Args:\n            agent_id (int): identifier of agent, used for random seed generation.\n            home_location (Tuple[float,float]): coordinates of home location.\n            seed (int, optional): random seed integer. Defaults to 4.\n\n        Returns:\n            Tuple[float,float]: coordinates of generated work location.\n        \"\"\"\n        seed_distance = self.random_seed_number_generator(seed - 1, agent_id)\n        random_distance = Random(seed_distance).uniform(self.home_work_distance_min, self.home_work_distance_max)\n        hlon, hlat = home_location\n        seed_coords = self.random_seed_number_generator(seed, agent_id)\n        wlon, wlat = self.generate_lonlat_at_distance(hlon, hlat, random_distance, seed_coords)\n\n        if not (self.longitude_min &lt; wlon &lt; self.longitude_max) or not (\n            self.latitude_min &lt; wlat &lt; self.latitude_max\n        ):  # outside limits\n            seed += 1\n            wlon, wlat = self.generate_work_location(agent_id, home_location, seed=seed)\n\n        return (wlon, wlat)\n\n    def generate_other_location(\n        self,\n        agent_id: int,\n        date: datetime.date,\n        activity_number: int,\n        home_location: Tuple[float, float],\n        previous_location: Tuple[float, float],\n        seed: int = 6,\n    ) -&gt; Tuple[float, float]:\n        \"\"\"\n        Generate other activity location based on previous location and maximum distance\n        to previous location. If there is no previous location (this is the first\n        activity of the day), then the home location is considered as previous location.\n        If the location falls outside of bounding box limits, try again.\n\n        Args:\n            agent_id (int): identifier of agent, used for random seed generation.\n            date (datetime.date): date, used for random seed generation.\n            activity_number (int): act position, used for random seed generation.\n            home_location (Tuple[float,float]): coordinates of home location.\n            previous_location (Tuple[float,float]): coordinates of previous location.\n            seed (int, optional): random seed integer. Defaults to 6.\n\n        Returns:\n            Tuple[float,float]: coordinates of generated location.\n        \"\"\"\n        seed_distance = self.random_seed_number_generator(seed - 1, agent_id)\n        random_distance = Random(seed_distance).uniform(self.other_distance_min, self.other_distance_max)\n        if previous_location is None:\n            plon, plat = home_location\n        else:\n            plon, plat = previous_location\n\n        seed_coords = self.random_seed_number_generator(seed, agent_id)\n        olon, olat = self.generate_lonlat_at_distance(plon, plat, random_distance, seed_coords)\n        if not (self.longitude_min &lt; olon &lt; self.longitude_max) or not (\n            self.latitude_min &lt; olat &lt; self.latitude_max\n        ):  # outside limits\n            seed += 1\n            olon, olat = self.generate_other_location(\n                agent_id, date, activity_number, home_location, previous_location, seed=seed\n            )\n\n        return (olon, olat)\n\n    def generate_stay_duration(\n        self, agent_id: int, date: datetime.date, i: int, stay_type: str, remaining_time: float\n    ) -&gt; float:\n        \"\"\"\n        Generate stay duration probabilistically based on activity type\n        abd remaining time.\n\n        Args:\n            agent_id (int): identifier of agent, used for random seed generation.\n            date (datetime.date): date, used for random seed generation.\n            i (int): activity position, used for random seed generation.\n            stay_type (str): type of stay. Shall be \"home\", \"work\" or \"other\".\n            remaining_time (float): same units as durations.\n\n        Returns:\n            float: generated activity duration.\n        \"\"\"\n        if stay_type == \"home\":\n            min_duration = self.home_duration_min\n            max_duration = self.home_duration_max\n        elif stay_type == \"work\":\n            min_duration = self.work_duration_min\n            max_duration = self.work_duration_max\n        elif stay_type == \"other\":\n            min_duration = self.other_duration_min\n            max_duration = self.other_duration_max\n        else:\n            raise ValueError\n        seed = self.random_seed_number_generator(7, agent_id, date, i)\n        max_value = min(max_duration, min_duration + remaining_time)\n        return Random(seed).uniform(min_duration, max_value)\n\n    def generate_min_stay_duration(self, stay_type: str) -&gt; float:\n        \"\"\"\n        Generate minimum stay duration based on stay type specifications.\n\n        Args:\n            stay_type (str): type of stay. Shall be \"home\", \"work\" or \"other\".\n\n        Returns:\n            float: minimum stay duration.\n        \"\"\"\n        if stay_type == \"home\":\n            return self.home_duration_min\n        elif stay_type == \"work\":\n            return self.work_duration_min\n        elif stay_type == \"other\":\n            return self.other_duration_min\n        else:\n            raise ValueError\n\n    def remove_consecutive_stay_types(self, stay_sequence_list: List[str], stay_types_to_group: Set[str]) -&gt; List[str]:\n        \"\"\"\n        Generate new list replacing consecutive stays of the same type by\n        a unique stay as long as the stay type is contained in the\n        \"stay_types_to_group\" list.\n\n        Args:\n            stay_sequence_list (List[str]): input stay type list.\n            stay_types_to_group (Set[str]): stay types to group.\n\n        Returns:\n            List[str]: output stay sequence list.\n        \"\"\"\n        new_stay_sequence_list = []\n        previous_stay = None\n        for stay in stay_sequence_list:\n            if stay == previous_stay and stay in stay_types_to_group:\n                pass\n            else:\n                new_stay_sequence_list.append(stay)\n            previous_stay = stay\n        return new_stay_sequence_list\n\n    def generate_stay_type_sequence(self, agent_id: int, date: datetime.date) -&gt; List[str]:\n        \"\"\"\n        Generate the sequence of stay types for an agent for a specific date\n        probabilistically based on the superset sequence and specified\n        probabilities.\n        Replace 'home'-'home' and 'work'-'work' sequences by just 'home' or\n        'work'.\n\n        Args:\n            agent_id (int): identifier of agent, used for random seed generation.\n            date (datetime.date): date for activity sequence generation, used for\n                random seed generation.\n\n        Returns:\n            List[str]: list of generated stay types, each represented by a string\n                indicating the stay type (e.g. \"home\", \"work\", \"other\").\n        \"\"\"\n        stay_type_sequence = []\n        for i, stay_type in enumerate(self.stay_sequence_superset):\n            stay_weight = self.stay_sequence_probabilities[i]\n            seed = self.random_seed_number_generator(0, agent_id, date, i)\n            if Random(seed).random() &lt; stay_weight:\n                stay_type_sequence.append(stay_type)\n        stay_type_sequence = self.remove_consecutive_stay_types(stay_type_sequence, {\"home\", \"work\"})\n        return stay_type_sequence\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.add_agent_date_activities","title":"<code>add_agent_date_activities(activities, user_id, agent_stay_type_sequence, home_location, work_location, date, start_of_date, end_of_date)</code>","text":"<p>For a specific date and user, generate a sequence of activities probabilistically according to the specified activity superset and the activity probabilities. Firstly, assign to each of these activities the minimum duration considered for that activity type. Trip times are based on Pythagorean distance and a specified average speed. If the sum of all minimum duration of the activities and the duration of the trips is higher than the 24h of the day, then assign just one \"home\" activity to the agent from 00:00:00 to 23:59:59. Else, there will be a remaining time. E.g., the diary of an agent, after adding up all trip durations and minimum activity durations may end at 20:34:57. There is a remaining time to complete the full diary (23:59:59 - 20:34:57). Adjust activity times probabilistically according to the maximum activity duration and this remaining time, making the diary end at exactly 23:59:59.</p> <p>Parameters:</p> Name Type Description Default <code>activities</code> <code>List[Row]</code> <p>list of generated activities (stays and moves) for the agent for all of the specified dates. Each activity is a spark row object.</p> required <code>user_id</code> <code>int</code> <p>agent identifier.</p> required <code>agent_stay_type_sequence</code> <code>List[str]</code> <p>list of generated stay types, each represented by a string indicating the stay type.</p> required <code>home_location</code> <code>Tuple[float, float]</code> <p>coordinates of home location.</p> required <code>work_location</code> <code>Tuple[float, float]</code> <p>coordinates of work location.</p> required <code>date</code> <code>date</code> <p>date for activity sequence generation, used for timestamps and random seed generation.</p> required <code>start_of_date</code> <code>datetime</code> <p>timestamp of current date at 00:00:00.</p> required <code>end_of_date</code> <code>datetime</code> <p>timestamp of current date at 23:59:59.</p> required Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def add_agent_date_activities(\n    self,\n    activities: List[Row],\n    user_id: int,\n    agent_stay_type_sequence: List[str],\n    home_location: Tuple[float, float],\n    work_location: Tuple[float, float],\n    date: datetime.date,\n    start_of_date: datetime.datetime,\n    end_of_date: datetime.datetime,\n):\n    \"\"\"\n    For a specific date and user, generate a sequence of activities probabilistically\n    according to the specified activity superset and the activity probabilities.\n    Firstly, assign to each of these activities the minimum duration considered for\n    that activity type. Trip times are based on Pythagorean distance and a specified\n    average speed.\n    If the sum of all minimum duration of the activities and the duration of the trips\n    is higher than the 24h of the day, then assign just one \"home\" activity to the\n    agent from 00:00:00 to 23:59:59.\n    Else, there will be a remaining time. E.g., the diary of an agent, after adding\n    up all trip durations and minimum activity durations may end at 20:34:57. There is\n    a remaining time to complete the full diary (23:59:59 - 20:34:57).\n    Adjust activity times probabilistically according to the maximum activity duration\n    and this remaining time, making the diary end at exactly 23:59:59.\n\n    Args:\n        activities (List[Row]): list of generated activities (stays and moves) for\n            the agent for all of the specified dates. Each activity is a spark\n            row object.\n        user_id (int): agent identifier.\n        agent_stay_type_sequence (List[str]): list of generated stay types,\n            each represented by a string indicating the stay type.\n        home_location (Tuple[float,float]): coordinates of home location.\n        work_location (Tuple[float,float]): coordinates of work location.\n        date (datetime.date): date for activity sequence generation, used for\n            timestamps and random seed generation.\n        start_of_date (datetime.datetime): timestamp of current date at 00:00:00.\n        end_of_date (datetime.datetime): timestamp of current date at 23:59:59.\n    \"\"\"\n    date_activities = self.create_agent_activities_min_duration(\n        user_id, agent_stay_type_sequence, home_location, work_location, date, start_of_date, end_of_date\n    )\n    remaining_time = (end_of_date - date_activities[-1].final_timestamp).total_seconds() / 3600.0\n\n    if remaining_time != 0:\n        self.adjust_activity_times(\n            date_activities,\n            remaining_time,\n            user_id,\n            date,\n            start_of_date,\n            end_of_date,\n        )\n    activities += date_activities\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.add_date_activities","title":"<code>add_date_activities(date, activities)</code>","text":"<p>Generate activity (stays and moves) rows for a specific date according to parameters.</p> <p>Parameters:</p> Name Type Description Default <code>date</code> <code>date</code> <p>date for activity sequence generation, used for timestamps and random seed generation.</p> required <code>activities</code> <code>List[Row]</code> <p>list of generated activities (stays and moves) for the agent for all of the specified dates. Each activity is a spark row object.</p> required Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def add_date_activities(self, date: datetime.date, activities: List[Row]):\n    \"\"\"\n    Generate activity (stays and moves) rows for a specific date according to\n    parameters.\n\n    Args:\n        date (datetime.date): date for activity sequence generation, used for\n            timestamps and random seed generation.\n        activities (List[Row]): list of generated activities (stays and moves) for\n            the agent for all of the specified dates. Each activity is a spark\n            row object.\n    \"\"\"\n    # Start of date, end of date: datetime object generation\n    start_of_date = datetime.datetime.combine(date, datetime.time(0, 0, 0))\n    end_of_date = datetime.datetime.combine(date, datetime.time(23, 59, 59))\n    for user_id in range(self.number_of_users):\n        # generate user information:\n        agent_stay_type_sequence = self.generate_stay_type_sequence(user_id, date)\n        home_location = self.generate_home_location(user_id)\n        work_location = self.generate_work_location(user_id, home_location)\n        self.add_agent_date_activities(\n            activities,\n            user_id,\n            agent_stay_type_sequence,\n            home_location,\n            work_location,\n            date,\n            start_of_date,\n            end_of_date,\n        )\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.adjust_activity_times","title":"<code>adjust_activity_times(date_activities, remaining_time, user_id, date, start_of_date, end_of_date)</code>","text":"<p>Modifies the \"date_activities\" list, changing the initial and final timestamps of both stays and moves probablilistically in order to generate stay durations different from the minimum and adjust the durations of the activities to the 24h of the day.</p> <p>Parameters:</p> Name Type Description Default <code>date_activities</code> <code>List[Row]</code> <p>list of generated activities (stays and moves) of the agent for the specified date. Each activity/trip is a spark row object.</p> required <code>user_id</code> <code>int</code> <p>agent identifier.</p> required <code>date</code> <code>date</code> <p>date for activity sequence generation, used for timestamps and random seed generation.</p> required <code>start_of_date</code> <code>datetime</code> <p>timestamp of current date at 00:00:00.</p> required <code>end_of_date</code> <code>datetime</code> <p>timestamp of current date at 23:59:59.</p> required Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def adjust_activity_times(\n    self,\n    date_activities: List[Row],\n    remaining_time: float,\n    user_id: int,\n    date: datetime.date,\n    start_of_date: datetime.datetime,\n    end_of_date: datetime.datetime,\n):\n    \"\"\"\n    Modifies the \"date_activities\" list, changing the initial and\n    final timestamps of both stays and moves probablilistically in order to\n    generate stay durations different from the minimum and adjust the\n    durations of the activities to the 24h of the day.\n\n    Args:\n        date_activities (List[Row]): list of generated activities (stays and\n            moves) of the agent for the specified date. Each activity/trip is a\n            spark row object.\n        user_id (int): agent identifier.\n        date (datetime.date): date for activity sequence generation, used for\n            timestamps and random seed generation.\n        start_of_date (datetime.datetime): timestamp of current date at 00:00:00.\n        end_of_date (datetime.datetime): timestamp of current date at 23:59:59.\n    \"\"\"\n    current_timestamp = start_of_date\n    for i, activity_row in enumerate(date_activities):\n        if activity_row.activity_type == \"stay\":  # stay:\n            stay_type = activity_row.stay_type\n            old_stay_duration = (\n                activity_row.final_timestamp - activity_row.initial_timestamp\n            ).total_seconds() / 3600.0\n            new_initial_timestamp = current_timestamp\n            if i == len(date_activities) - 1:\n                new_final_timestamp = end_of_date\n                remaining_time = 0.0\n            else:\n                new_stay_duration = self.generate_stay_duration(user_id, date, i, stay_type, remaining_time)\n                new_duration_td = datetime.timedelta(seconds=new_stay_duration * 3600.0)\n                new_final_timestamp = new_initial_timestamp + new_duration_td\n                remaining_time -= new_stay_duration - old_stay_duration\n        else:  # move:\n            old_move_duration = activity_row.final_timestamp - activity_row.initial_timestamp\n            new_initial_timestamp = current_timestamp\n            new_final_timestamp = new_initial_timestamp + old_move_duration\n\n        # common for all activities (stays and moves):\n        activity_row = self.update_spark_row(activity_row, \"initial_timestamp\", new_initial_timestamp)\n        activity_row = self.update_spark_row(activity_row, \"final_timestamp\", new_final_timestamp)\n        date_activities[i] = activity_row\n        current_timestamp = new_final_timestamp\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.calculate_trip_final_time","title":"<code>calculate_trip_final_time(origin_location, destin_location, origin_timestamp)</code>","text":"<p>Calculate end time of a trip given an origin time, an origin location, a destination location and a speed.</p> <p>Parameters:</p> Name Type Description Default <code>origin_location</code> <code>Tuple[float, float]</code> <p>lon, lat of 1st point, in decimal degrees.</p> required <code>destin_location</code> <code>Tuple[float, float]</code> <p>lon, lat of 2nd point, in decimal degrees.</p> required <code>origin_timestamp</code> <code>datetime</code> <p>start time of trip.</p> required <p>Returns:</p> Type Description <code>datetime</code> <p>datetime.datetime: end time of trip.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def calculate_trip_final_time(\n    self,\n    origin_location: Tuple[float, float],\n    destin_location: Tuple[float, float],\n    origin_timestamp: datetime.datetime,\n) -&gt; datetime.datetime:\n    \"\"\"\n    Calculate end time of a trip given an origin time, an origin location,\n    a destination location and a speed.\n\n    Args:\n        origin_location (Tuple[float,float]): lon, lat of 1st point,\n            in decimal degrees.\n        destin_location (Tuple[float,float]): lon, lat of 2nd point,\n            in decimal degrees.\n        origin_timestamp (datetime.datetime): start time of trip.\n\n    Returns:\n        datetime.datetime: end time of trip.\n    \"\"\"\n\n    trip_time = self.calculate_trip_time(origin_location, destin_location)  # s\n    return origin_timestamp + datetime.timedelta(seconds=trip_time)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.calculate_trip_time","title":"<code>calculate_trip_time(o_location, d_location)</code>","text":"<p>Calculate trip time given an origin location and a destination location, according to the specified trip speed.</p> <p>Parameters:</p> Name Type Description Default <code>o_location</code> <code>Tuple[float, float]</code> <p>lon, lat of 1st point, in decimal degrees.</p> required <code>d_location</code> <code>Tuple[float, float]</code> <p>lon, lat of 2nd point, in decimal degrees.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>trip time, in seconds.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def calculate_trip_time(self, o_location: Tuple[float, float], d_location: Tuple[float, float]) -&gt; float:\n    \"\"\"\n    Calculate trip time given an origin location and a destination\n    location, according to the specified trip speed.\n\n    Args:\n        o_location (Tuple[float,float]): lon, lat of 1st point,\n            in decimal degrees.\n        d_location (Tuple[float,float]): lon, lat of 2nd point,\n            in decimal degrees.\n\n    Returns:\n        float: trip time, in seconds.\n    \"\"\"\n    trip_distance = self.haversine(o_location[0], o_location[1], d_location[0], d_location[1])  # m\n    trip_speed = self.displacement_speed  # m/s\n    trip_time = trip_distance / trip_speed  # s\n    return trip_time\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.create_agent_activities_min_duration","title":"<code>create_agent_activities_min_duration(user_id, agent_stay_type_sequence, home_location, work_location, date, start_of_date, end_of_date)</code>","text":"<p>Generate activities of the minimum duration following the specified agent activity sequence for this agent and date.</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>int</code> <p>agent identifier.</p> required <code>agent_stay_type_sequence</code> <code>List[str]</code> <p>list of generated stay types, each represented by a string indicating the stay type.</p> required <code>home_location</code> <code>Tuple[float, float]</code> <p>coordinates of home location.</p> required <code>work_location</code> <code>Tuple[float, float]</code> <p>coordinates of work location.</p> required <code>date</code> <code>date</code> <p>date for activity sequence generation, used for timestamps and random seed generation.</p> required <code>start_of_date</code> <code>datetime</code> <p>timestamp of current date at 00:00:00.</p> required <code>end_of_date</code> <code>datetime</code> <p>timestamp of current date at 23:59:59.</p> required <p>Returns:</p> Type Description <code>List[Row]</code> <p>List[Row]: list of generated activities and trips, each represented by a spark row object with all its information.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def create_agent_activities_min_duration(\n    self,\n    user_id: int,\n    agent_stay_type_sequence: List[str],\n    home_location: Tuple[float, float],\n    work_location: Tuple[float, float],\n    date: datetime.date,\n    start_of_date: datetime.datetime,\n    end_of_date: datetime.datetime,\n) -&gt; List[Row]:\n    \"\"\"\n    Generate activities of the minimum duration following the specified agent\n    activity sequence for this agent and date.\n\n    Args:\n        user_id (int): agent identifier.\n        agent_stay_type_sequence (List[str]): list of generated stay types,\n            each represented by a string indicating the stay type.\n        home_location (Tuple[float,float]): coordinates of home location.\n        work_location (Tuple[float,float]): coordinates of work location.\n        date (datetime.date): date for activity sequence generation, used for\n            timestamps and random seed generation.\n        start_of_date (datetime.datetime): timestamp of current date at 00:00:00.\n        end_of_date (datetime.datetime): timestamp of current date at 23:59:59.\n\n    Returns:\n        List[Row]: list of generated activities and trips, each represented by a\n            spark row object with all its information.\n    \"\"\"\n    date_activities = []\n    previous_location = None\n    for i, stay_type in enumerate(agent_stay_type_sequence):\n        # activity location:\n        location = self.generate_stay_location(\n            stay_type, home_location, work_location, previous_location, user_id, date, i\n        )\n        # previous move (unless first stay)\n        if i != 0:\n            # move timestamps:\n            trip_initial_timestamp = stay_final_timestamp\n            trip_final_timestamp = self.calculate_trip_final_time(\n                previous_location, location, trip_initial_timestamp\n            )\n            # add move:\n            date_activities.append(\n                Row(\n                    user_id=user_id,\n                    activity_type=\"move\",\n                    stay_type=\"move\",\n                    longitude=float(\"nan\"),\n                    latitude=float(\"nan\"),\n                    initial_timestamp=trip_initial_timestamp,\n                    final_timestamp=trip_final_timestamp,\n                    year=date.year,\n                    month=date.month,\n                    day=date.day,\n                )\n            )\n        # stay timestamps:\n        stay_initial_timestamp = start_of_date if i == 0 else trip_final_timestamp\n        stay_duration = self.generate_min_stay_duration(stay_type)\n        stay_final_timestamp = stay_initial_timestamp + datetime.timedelta(hours=stay_duration)\n\n        # add stay:\n        date_activities.append(\n            Row(\n                user_id=user_id,\n                activity_type=\"stay\",\n                stay_type=stay_type,\n                longitude=location[0],\n                latitude=location[1],\n                initial_timestamp=stay_initial_timestamp,\n                final_timestamp=stay_final_timestamp,\n                year=date.year,\n                month=date.month,\n                day=date.day,\n            )\n        )\n\n        previous_location = location\n\n    # after the iterations:\n    if not date_activities:  # 0 stays\n        condition_for_full_home = True\n    elif stay_final_timestamp &gt; end_of_date:  # too many stays\n        condition_for_full_home = True\n    else:\n        condition_for_full_home = False\n\n    if condition_for_full_home:  # simple \"only home\" diary\n        return [\n            Row(\n                user_id=user_id,\n                activity_type=\"stay\",\n                stay_type=\"home\",\n                longitude=home_location[0],\n                latitude=home_location[1],\n                initial_timestamp=start_of_date,\n                final_timestamp=end_of_date,\n                year=date.year,\n                month=date.month,\n                day=date.day,\n            )\n        ]\n    else:\n        return date_activities  # actual generated diary\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_activities","title":"<code>generate_activities()</code>","text":"<p>Generate activity and trip rows according to parameters.</p> <p>Returns:</p> Type Description <code>List[Row]</code> <p>List[Row]: list of generated activities and trips for the agent for all of the specified dates. Each activity/trip is a spark row object.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_activities(self) -&gt; List[Row]:\n    \"\"\"\n    Generate activity and trip rows according to parameters.\n\n    Returns:\n        List[Row]: list of generated activities and trips for the agent for all\n            of the specified dates. Each activity/trip is a spark row object.\n    \"\"\"\n    activities = []\n    for date in self.date_range:\n        self.add_date_activities(date, activities)\n    return activities\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_home_location","title":"<code>generate_home_location(agent_id)</code>","text":"<p>Generate random home location based on bounding box limits.</p> <p>Parameters:</p> Name Type Description Default <code>agent_id</code> <code>int</code> <p>identifier of agent, used for random seed generation.</p> required <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>Tuple[float,float]: coordinates of generated home location.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_home_location(self, agent_id: int) -&gt; Tuple[float, float]:\n    \"\"\"\n    Generate random home location based on bounding box limits.\n\n    Args:\n        agent_id (int): identifier of agent, used for random seed generation.\n\n    Returns:\n        Tuple[float,float]: coordinates of generated home location.\n    \"\"\"\n    seed_lon = self.random_seed_number_generator(1, agent_id)\n    seed_lat = self.random_seed_number_generator(2, agent_id)\n    hlon = Random(seed_lon).uniform(self.longitude_min, self.longitude_max)\n    hlat = Random(seed_lat).uniform(self.latitude_min, self.latitude_max)\n    return (hlon, hlat)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_lonlat_at_distance","title":"<code>generate_lonlat_at_distance(lon1, lat1, d, seed)</code>","text":"<p>Given a point (lon, lat) and a distance, in meters, calculate a new random point that is exactly at the specified distance of the provided lon, lat.</p> <p>Parameters:</p> Name Type Description Default <code>lon1</code> <code>float</code> <p>longitude of point, specified in decimal degrees.</p> required <code>lat1</code> <code>float</code> <p>latitude of point, specified in decimal degrees.</p> required <code>d</code> <code>float</code> <p>distance, in meters.</p> required <code>seed</code> <code>int</code> <p>random seed integer.</p> required <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>Tuple[float, float]: coordinates of randomly generated point.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_lonlat_at_distance(self, lon1: float, lat1: float, d: float, seed: int) -&gt; Tuple[float, float]:\n    \"\"\"\n    Given a point (lon, lat) and a distance, in meters, calculate a new random\n    point that is exactly at the specified distance of the provided lon, lat.\n\n    Args:\n        lon1 (float): longitude of point, specified in decimal degrees.\n        lat1 (float): latitude of point, specified in decimal degrees.\n        d (float): distance, in meters.\n        seed (int): random seed integer.\n\n    Returns:\n        Tuple[float, float]: coordinates of randomly generated point.\n    \"\"\"\n    r = 6_371_000  # Radius of earth in meters.\n\n    d_x = Random(seed).uniform(0, d)\n    d_y = sqrt(d**2 - d_x**2)\n\n    # firstly, convert lat to radians for later\n    lat1_radians = lat1 * pi / 180.0\n\n    # how many meters correspond to one degree of latitude?\n    deg_to_meters = r * pi / 180  # aprox. 111111 meters\n    # thus, the northwards displacement, in degrees of latitude is:\n    north_delta = d_y / deg_to_meters\n\n    # but one degree of longitude does not always correspond to the\n    # same distance... depends on the latitude at where you are!\n    parallel_radius = abs(r * cos(lat1_radians))\n    deg_to_meters = parallel_radius * pi / 180  # variable\n    # thus, the eastwards displacement, in degrees of longitude is:\n    east_delta = d_x / deg_to_meters\n\n    final_lon = lon1 + east_delta * Random(seed).choice([-1, 1])\n    final_lat = lat1 + north_delta * Random(seed).choice([-1, 1])\n\n    return (final_lon, final_lat)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_min_stay_duration","title":"<code>generate_min_stay_duration(stay_type)</code>","text":"<p>Generate minimum stay duration based on stay type specifications.</p> <p>Parameters:</p> Name Type Description Default <code>stay_type</code> <code>str</code> <p>type of stay. Shall be \"home\", \"work\" or \"other\".</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>minimum stay duration.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_min_stay_duration(self, stay_type: str) -&gt; float:\n    \"\"\"\n    Generate minimum stay duration based on stay type specifications.\n\n    Args:\n        stay_type (str): type of stay. Shall be \"home\", \"work\" or \"other\".\n\n    Returns:\n        float: minimum stay duration.\n    \"\"\"\n    if stay_type == \"home\":\n        return self.home_duration_min\n    elif stay_type == \"work\":\n        return self.work_duration_min\n    elif stay_type == \"other\":\n        return self.other_duration_min\n    else:\n        raise ValueError\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_other_location","title":"<code>generate_other_location(agent_id, date, activity_number, home_location, previous_location, seed=6)</code>","text":"<p>Generate other activity location based on previous location and maximum distance to previous location. If there is no previous location (this is the first activity of the day), then the home location is considered as previous location. If the location falls outside of bounding box limits, try again.</p> <p>Parameters:</p> Name Type Description Default <code>agent_id</code> <code>int</code> <p>identifier of agent, used for random seed generation.</p> required <code>date</code> <code>date</code> <p>date, used for random seed generation.</p> required <code>activity_number</code> <code>int</code> <p>act position, used for random seed generation.</p> required <code>home_location</code> <code>Tuple[float, float]</code> <p>coordinates of home location.</p> required <code>previous_location</code> <code>Tuple[float, float]</code> <p>coordinates of previous location.</p> required <code>seed</code> <code>int</code> <p>random seed integer. Defaults to 6.</p> <code>6</code> <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>Tuple[float,float]: coordinates of generated location.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_other_location(\n    self,\n    agent_id: int,\n    date: datetime.date,\n    activity_number: int,\n    home_location: Tuple[float, float],\n    previous_location: Tuple[float, float],\n    seed: int = 6,\n) -&gt; Tuple[float, float]:\n    \"\"\"\n    Generate other activity location based on previous location and maximum distance\n    to previous location. If there is no previous location (this is the first\n    activity of the day), then the home location is considered as previous location.\n    If the location falls outside of bounding box limits, try again.\n\n    Args:\n        agent_id (int): identifier of agent, used for random seed generation.\n        date (datetime.date): date, used for random seed generation.\n        activity_number (int): act position, used for random seed generation.\n        home_location (Tuple[float,float]): coordinates of home location.\n        previous_location (Tuple[float,float]): coordinates of previous location.\n        seed (int, optional): random seed integer. Defaults to 6.\n\n    Returns:\n        Tuple[float,float]: coordinates of generated location.\n    \"\"\"\n    seed_distance = self.random_seed_number_generator(seed - 1, agent_id)\n    random_distance = Random(seed_distance).uniform(self.other_distance_min, self.other_distance_max)\n    if previous_location is None:\n        plon, plat = home_location\n    else:\n        plon, plat = previous_location\n\n    seed_coords = self.random_seed_number_generator(seed, agent_id)\n    olon, olat = self.generate_lonlat_at_distance(plon, plat, random_distance, seed_coords)\n    if not (self.longitude_min &lt; olon &lt; self.longitude_max) or not (\n        self.latitude_min &lt; olat &lt; self.latitude_max\n    ):  # outside limits\n        seed += 1\n        olon, olat = self.generate_other_location(\n            agent_id, date, activity_number, home_location, previous_location, seed=seed\n        )\n\n    return (olon, olat)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_stay_duration","title":"<code>generate_stay_duration(agent_id, date, i, stay_type, remaining_time)</code>","text":"<p>Generate stay duration probabilistically based on activity type abd remaining time.</p> <p>Parameters:</p> Name Type Description Default <code>agent_id</code> <code>int</code> <p>identifier of agent, used for random seed generation.</p> required <code>date</code> <code>date</code> <p>date, used for random seed generation.</p> required <code>i</code> <code>int</code> <p>activity position, used for random seed generation.</p> required <code>stay_type</code> <code>str</code> <p>type of stay. Shall be \"home\", \"work\" or \"other\".</p> required <code>remaining_time</code> <code>float</code> <p>same units as durations.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>generated activity duration.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_stay_duration(\n    self, agent_id: int, date: datetime.date, i: int, stay_type: str, remaining_time: float\n) -&gt; float:\n    \"\"\"\n    Generate stay duration probabilistically based on activity type\n    abd remaining time.\n\n    Args:\n        agent_id (int): identifier of agent, used for random seed generation.\n        date (datetime.date): date, used for random seed generation.\n        i (int): activity position, used for random seed generation.\n        stay_type (str): type of stay. Shall be \"home\", \"work\" or \"other\".\n        remaining_time (float): same units as durations.\n\n    Returns:\n        float: generated activity duration.\n    \"\"\"\n    if stay_type == \"home\":\n        min_duration = self.home_duration_min\n        max_duration = self.home_duration_max\n    elif stay_type == \"work\":\n        min_duration = self.work_duration_min\n        max_duration = self.work_duration_max\n    elif stay_type == \"other\":\n        min_duration = self.other_duration_min\n        max_duration = self.other_duration_max\n    else:\n        raise ValueError\n    seed = self.random_seed_number_generator(7, agent_id, date, i)\n    max_value = min(max_duration, min_duration + remaining_time)\n    return Random(seed).uniform(min_duration, max_value)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_stay_location","title":"<code>generate_stay_location(stay_type, home_location, work_location, previous_location, user_id, date, i)</code>","text":"<p>Generate a random activity location within the bounding box limits based on the activity type and previous activity locations.</p> <p>Parameters:</p> Name Type Description Default <code>stay_type</code> <code>str</code> <p>type of stay (\"home\", \"work\" or \"other\").</p> required <code>home_location</code> <code>Tuple[float, float]</code> <p>coordinates of home location.</p> required <code>work_location</code> <code>Tuple[float, float]</code> <p>coordinates of work location.</p> required <code>previous_location</code> <code>Tuple[float, float]</code> <p>coordinates of previous activity location.</p> required <code>user_id</code> <code>int</code> <p>agent identifier, used for random seed generation.</p> required <code>date</code> <code>date</code> <p>date, used for random seed generation.</p> required <code>i</code> <code>int</code> <p>activity position, used for random seed generation.</p> required <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>Tuple[float,float]: randomly generated activity location coordinates.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_stay_location(\n    self,\n    stay_type: str,\n    home_location: Tuple[float, float],\n    work_location: Tuple[float, float],\n    previous_location: Tuple[float, float],\n    user_id: int,\n    date: datetime.date,\n    i: int,\n) -&gt; Tuple[float, float]:\n    \"\"\"\n    Generate a random activity location within the bounding box limits based\n    on the activity type and previous activity locations.\n\n    Args:\n        stay_type (str): type of stay (\"home\", \"work\" or \"other\").\n        home_location (Tuple[float,float]): coordinates of home location.\n        work_location (Tuple[float,float]): coordinates of work location.\n        previous_location (Tuple[float,float]): coordinates of previous\n            activity location.\n        user_id (int): agent identifier, used for random seed generation.\n        date (datetime.date): date, used for random seed generation.\n        i (int): activity position, used for random seed generation.\n\n    Returns:\n        Tuple[float,float]: randomly generated activity location coordinates.\n    \"\"\"\n    if stay_type == \"home\":\n        location = home_location\n    elif stay_type == \"work\":\n        location = work_location\n    else:\n        location = self.generate_other_location(user_id, date, i, home_location, previous_location)\n    return location\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_stay_type_sequence","title":"<code>generate_stay_type_sequence(agent_id, date)</code>","text":"<p>Generate the sequence of stay types for an agent for a specific date probabilistically based on the superset sequence and specified probabilities. Replace 'home'-'home' and 'work'-'work' sequences by just 'home' or 'work'.</p> <p>Parameters:</p> Name Type Description Default <code>agent_id</code> <code>int</code> <p>identifier of agent, used for random seed generation.</p> required <code>date</code> <code>date</code> <p>date for activity sequence generation, used for random seed generation.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: list of generated stay types, each represented by a string indicating the stay type (e.g. \"home\", \"work\", \"other\").</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_stay_type_sequence(self, agent_id: int, date: datetime.date) -&gt; List[str]:\n    \"\"\"\n    Generate the sequence of stay types for an agent for a specific date\n    probabilistically based on the superset sequence and specified\n    probabilities.\n    Replace 'home'-'home' and 'work'-'work' sequences by just 'home' or\n    'work'.\n\n    Args:\n        agent_id (int): identifier of agent, used for random seed generation.\n        date (datetime.date): date for activity sequence generation, used for\n            random seed generation.\n\n    Returns:\n        List[str]: list of generated stay types, each represented by a string\n            indicating the stay type (e.g. \"home\", \"work\", \"other\").\n    \"\"\"\n    stay_type_sequence = []\n    for i, stay_type in enumerate(self.stay_sequence_superset):\n        stay_weight = self.stay_sequence_probabilities[i]\n        seed = self.random_seed_number_generator(0, agent_id, date, i)\n        if Random(seed).random() &lt; stay_weight:\n            stay_type_sequence.append(stay_type)\n    stay_type_sequence = self.remove_consecutive_stay_types(stay_type_sequence, {\"home\", \"work\"})\n    return stay_type_sequence\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_work_location","title":"<code>generate_work_location(agent_id, home_location, seed=4)</code>","text":"<p>Generate random work location based on home location and maximum distance to home. If the work location falls outside of bounding box limits, try again.</p> <p>Parameters:</p> Name Type Description Default <code>agent_id</code> <code>int</code> <p>identifier of agent, used for random seed generation.</p> required <code>home_location</code> <code>Tuple[float, float]</code> <p>coordinates of home location.</p> required <code>seed</code> <code>int</code> <p>random seed integer. Defaults to 4.</p> <code>4</code> <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>Tuple[float,float]: coordinates of generated work location.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_work_location(\n    self, agent_id: int, home_location: Tuple[float, float], seed: int = 4\n) -&gt; Tuple[float, float]:\n    \"\"\"\n    Generate random work location based on home location and maximum distance to\n    home. If the work location falls outside of bounding box limits, try again.\n\n    Args:\n        agent_id (int): identifier of agent, used for random seed generation.\n        home_location (Tuple[float,float]): coordinates of home location.\n        seed (int, optional): random seed integer. Defaults to 4.\n\n    Returns:\n        Tuple[float,float]: coordinates of generated work location.\n    \"\"\"\n    seed_distance = self.random_seed_number_generator(seed - 1, agent_id)\n    random_distance = Random(seed_distance).uniform(self.home_work_distance_min, self.home_work_distance_max)\n    hlon, hlat = home_location\n    seed_coords = self.random_seed_number_generator(seed, agent_id)\n    wlon, wlat = self.generate_lonlat_at_distance(hlon, hlat, random_distance, seed_coords)\n\n    if not (self.longitude_min &lt; wlon &lt; self.longitude_max) or not (\n        self.latitude_min &lt; wlat &lt; self.latitude_max\n    ):  # outside limits\n        seed += 1\n        wlon, wlat = self.generate_work_location(agent_id, home_location, seed=seed)\n\n    return (wlon, wlat)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.haversine","title":"<code>haversine(lon1, lat1, lon2, lat2)</code>","text":"<p>Calculate the haversine distance in meters between two points.</p> <p>Parameters:</p> Name Type Description Default <code>lon1</code> <code>float</code> <p>longitude of first point, in decimal degrees.</p> required <code>lat1</code> <code>float</code> <p>latitude of first point, in decimal degrees.</p> required <code>lon2</code> <code>float</code> <p>longitude of second point, in decimal degrees.</p> required <code>lat2</code> <code>float</code> <p>latitude of second point, in decimal degrees.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>distance between both points, in meters.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def haversine(self, lon1: float, lat1: float, lon2: float, lat2: float) -&gt; float:\n    \"\"\"\n    Calculate the haversine distance in meters between two points.\n\n    Args:\n        lon1 (float): longitude of first point, in decimal degrees.\n        lat1 (float): latitude of first point, in decimal degrees.\n        lon2 (float): longitude of second point, in decimal degrees.\n        lat2 (float): latitude of second point, in decimal degrees.\n\n    Returns:\n        float: distance between both points, in meters.\n    \"\"\"\n    r = 6_371_000  # Radius of earth in meters.\n\n    # convert decimal degrees to radians\n    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n\n    # haversine formula\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n    a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n    c = 2 * asin(sqrt(a))\n    return c * r\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.random_seed_number_generator","title":"<code>random_seed_number_generator(base_seed, agent_id=None, date=None, i=None)</code>","text":"<p>Generate random seed integer based on provided arguments.</p> <p>Parameters:</p> Name Type Description Default <code>base_seed</code> <code>int</code> <p>base integer for operations.</p> required <code>agent_id</code> <code>int</code> <p>agent identifier. Defaults to None.</p> <code>None</code> <code>date</code> <code>date</code> <p>date. Defaults to None.</p> <code>None</code> <code>i</code> <code>int</code> <p>position integer. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>generated random seed integer.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def random_seed_number_generator(\n    self, base_seed: int, agent_id: int = None, date: datetime.date = None, i: int = None\n) -&gt; int:\n    \"\"\"\n    Generate random seed integer based on provided arguments.\n\n    Args:\n        base_seed (int): base integer for operations.\n        agent_id (int, optional): agent identifier. Defaults to None.\n        date (datetime.date, optional): date. Defaults to None.\n        i (int, optional): position integer. Defaults to None.\n\n    Returns:\n        int: generated random seed integer.\n    \"\"\"\n    seed = base_seed\n    if agent_id is not None:\n        seed += int(agent_id) * 100\n    if date is not None:\n        start_datetime = datetime.datetime.combine(date, datetime.time(0))\n        seed += int(start_datetime.timestamp())\n    if i is not None:\n        seed += i\n    return seed\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.remove_consecutive_stay_types","title":"<code>remove_consecutive_stay_types(stay_sequence_list, stay_types_to_group)</code>","text":"<p>Generate new list replacing consecutive stays of the same type by a unique stay as long as the stay type is contained in the \"stay_types_to_group\" list.</p> <p>Parameters:</p> Name Type Description Default <code>stay_sequence_list</code> <code>List[str]</code> <p>input stay type list.</p> required <code>stay_types_to_group</code> <code>Set[str]</code> <p>stay types to group.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: output stay sequence list.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def remove_consecutive_stay_types(self, stay_sequence_list: List[str], stay_types_to_group: Set[str]) -&gt; List[str]:\n    \"\"\"\n    Generate new list replacing consecutive stays of the same type by\n    a unique stay as long as the stay type is contained in the\n    \"stay_types_to_group\" list.\n\n    Args:\n        stay_sequence_list (List[str]): input stay type list.\n        stay_types_to_group (Set[str]): stay types to group.\n\n    Returns:\n        List[str]: output stay sequence list.\n    \"\"\"\n    new_stay_sequence_list = []\n    previous_stay = None\n    for stay in stay_sequence_list:\n        if stay == previous_stay and stay in stay_types_to_group:\n            pass\n        else:\n            new_stay_sequence_list.append(stay)\n        previous_stay = stay\n    return new_stay_sequence_list\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.update_spark_row","title":"<code>update_spark_row(row, column_name, new_value)</code>","text":"<p>Return an updated spark row object, changing the value of a column.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Row</code> <p>input spark row.</p> required <code>column_name</code> <code>str</code> <p>name of column to modify.</p> required <code>new_value</code> <code>Any</code> <p>new value to assign.</p> required <p>Returns:</p> Name Type Description <code>Row</code> <code>Row</code> <p>modified spark row</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def update_spark_row(self, row: Row, column_name: str, new_value: Any) -&gt; Row:\n    \"\"\"\n    Return an updated spark row object, changing the value of a column.\n\n    Args:\n        row (Row): input spark row.\n        column_name (str): name of column to modify.\n        new_value (Any): new value to assign.\n\n    Returns:\n        Row: modified spark row\n    \"\"\"\n    return Row(**{**row.asDict(), **{column_name: new_value}})\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/","title":"synthetic_events","text":"<p>This module contains the SyntheticEvents class, which is responsible for generating the synthetic event data.</p>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents","title":"<code>SyntheticEvents</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class that generates the event synthetic data. It inherits from the Component abstract class.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>class SyntheticEvents(Component):\n    \"\"\"\n    Class that generates the event synthetic data. It inherits from the Component abstract class.\n    \"\"\"\n\n    COMPONENT_ID = \"SyntheticEvents\"\n\n    def __init__(self, general_config_path: str, component_config_path: str):\n        super().__init__(\n            general_config_path=general_config_path,\n            component_config_path=component_config_path,\n        )\n\n        self.seed = self.config.getint(self.COMPONENT_ID, \"seed\")\n        self.event_freq_stays = self.config.getint(self.COMPONENT_ID, \"event_freq_stays\")\n        self.event_freq_moves = self.config.getint(self.COMPONENT_ID, \"event_freq_moves\")\n        self.closest_cell_distance_max = self.config.getint(self.COMPONENT_ID, \"closest_cell_distance_max\")\n        self.closest_cell_distance_max_for_errors = self.config.getint(\n            self.COMPONENT_ID, \"closest_cell_distance_max_for_errors\"\n        )\n        self.error_location_probability = self.config.getfloat(self.COMPONENT_ID, \"error_location_probability\")\n        self.error_location_distance_min = self.config.getint(self.COMPONENT_ID, \"error_location_distance_min\")\n        self.error_location_distance_max = self.config.getint(self.COMPONENT_ID, \"error_location_distance_max\")\n        self.cartesian_crs = self.config.getint(self.COMPONENT_ID, \"cartesian_crs\")\n        self.error_cell_id_probability = self.config.getfloat(self.COMPONENT_ID, \"error_cell_id_probability\")\n        self.maximum_number_of_cells_for_event = self.config.getfloat(\n            self.COMPONENT_ID, \"maximum_number_of_cells_for_event\"\n        )\n\n        self.mcc = self.config.getint(self.COMPONENT_ID, \"mcc\")\n        self.mnc = self.config.get(self.COMPONENT_ID, \"mnc\")\n\n        # Parameters for synthetic event errors generation (these are not locational errors)\n\n        self.do_event_error_generation = self.config.getboolean(self.COMPONENT_ID, \"do_event_error_generation\")\n        self.column_is_null_probability = self.config.getfloat(self.COMPONENT_ID, \"column_is_null_probability\")\n        self.null_row_prob = self.config.getfloat(self.COMPONENT_ID, \"null_row_probability\")\n        self.data_type_error_prob = self.config.getfloat(self.COMPONENT_ID, \"data_type_error_probability\")\n        self.out_of_bounds_prob = self.config.getfloat(self.COMPONENT_ID, \"out_of_bounds_probability\")\n        self.same_location_duplicate_prob = self.config.getfloat(\n            self.COMPONENT_ID, \"same_location_duplicates_probability\"\n        )\n        self.different_location_duplicate_prob = self.config.getfloat(\n            self.COMPONENT_ID, \"different_location_duplicates_probability\"\n        )\n\n        self.mandatory_columns = [i.name for i in BronzeEventDataObject.SCHEMA]\n        self.error_generation_allowed_columns = set(self.mandatory_columns) - set(\n            [ColNames.year, ColNames.month, ColNames.day]\n        )\n\n        self.order_output_by_timestamp = self.config.getboolean(self.COMPONENT_ID, \"order_output_by_timestamp\")\n\n        self.data_period_start = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n        self.data_period_end = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n\n        self.data_period_dates = [\n            self.data_period_start + timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n    def initalize_data_objects(self):\n\n        pop_diares_input_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"diaries_data_bronze\")\n        pop_diaries_bronze_event = BronzeSyntheticDiariesDataObject(self.spark, pop_diares_input_path)\n\n        # Input for cell attributes\n        network_data_input_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"network_data_bronze\")\n\n        cell_locations_bronze = BronzeNetworkDataObject(self.spark, network_data_input_path)\n\n        self.input_data_objects = {\n            BronzeSyntheticDiariesDataObject.ID: pop_diaries_bronze_event,\n            BronzeNetworkDataObject.ID: cell_locations_bronze,\n        }\n\n        output_records_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"event_data_bronze\")\n        bronze_event = BronzeEventDataObject(self.spark, output_records_path)\n\n        self.output_data_objects = {BronzeEventDataObject.ID: bronze_event}\n\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n\n        for current_date in self.data_period_dates:\n\n            self.logger.info(f\"Processing diaries for {current_date.strftime('%Y-%m-%d')}\")\n\n            self.current_diaries = self.input_data_objects[BronzeSyntheticDiariesDataObject.ID].df.filter(\n                (F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(current_date))\n            )\n\n            self.current_cells = (\n                self.input_data_objects[BronzeNetworkDataObject.ID]\n                .df.filter(\n                    (\n                        F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n                        == F.lit(current_date)\n                    )\n                )\n                .select(\n                    ColNames.cell_id,\n                    ColNames.latitude,\n                    ColNames.longitude,\n                    ColNames.year,\n                    ColNames.month,\n                    ColNames.day,\n                )\n            )\n\n            self.transform()\n            self.write()\n            self.spark.catalog.clearCache()\n\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n\n        pop_diaries_df = self.current_diaries\n        cells_df = self.current_cells\n\n        # Filtering stays to get the lat and lon of movement starting and end point\n        stays_df = pop_diaries_df.filter(F.col(ColNames.activity_type) == \"stay\")\n\n        move_events_df = self.generate_event_timestamps_for_moves(\n            stays_df, self.event_freq_moves, self.cartesian_crs, self.seed\n        )\n        move_events_with_locations_df = self.generate_locations_for_moves(move_events_df, self.cartesian_crs)\n\n        stay_events_df = self.generate_event_timestamps_for_stays(\n            stays_df, self.event_freq_stays, self.cartesian_crs, self.seed\n        )\n\n        generated_stays_and_moves = stay_events_df.union(move_events_with_locations_df)\n\n        # Add geometry column to cells\n        cells_df = cells_df.withColumn(\n            \"cell_geometry\",\n            STF.ST_Transform(\n                STC.ST_Point(F.col(ColNames.longitude), F.col(ColNames.latitude)),\n                F.lit(\"EPSG:4326\"),\n                F.lit(f\"EPSG:{self.cartesian_crs}\"),\n            ),\n        ).select(ColNames.cell_id, \"cell_geometry\")\n\n        # 1) From the clean records, sample records for location errors\n        sampled_records = generated_stays_and_moves.sample(self.error_location_probability, self.seed)\n        generated_stays_and_moves = generated_stays_and_moves.subtract(sampled_records)\n        generated_stays_and_moves = generated_stays_and_moves.withColumn(\n            ColNames.loc_error, F.lit(None).cast(FloatType())\n        )\n\n        records_with_location_errors = self.generate_location_errors(\n            sampled_records,\n            self.error_location_distance_max,\n            self.error_location_distance_min,\n            self.closest_cell_distance_max_for_errors,\n            self.cartesian_crs,\n            self.seed,\n        )\n\n        # 2) From the clean records, sample records for erroneous cell id creation\n        sampled_records = generated_stays_and_moves.sample(self.error_cell_id_probability, self.seed)\n        records_with_cell_id_errors = self.generate_records_with_non_existant_cell_ids(\n            sampled_records, cells_df, self.seed\n        )\n\n        generated_stays_and_moves = generated_stays_and_moves.subtract(sampled_records)\n        generated_stays_and_moves = generated_stays_and_moves.withColumn(\n            \"closest_cell_distance_max\", F.lit(self.closest_cell_distance_max)\n        )\n\n        # Label error rows so that these would be ignored in syntactic error generation\n        records_with_location_errors = records_with_location_errors.withColumn(\"is_modified\", F.lit(True))\n        records_with_cell_id_errors = records_with_cell_id_errors.withColumn(\"is_modified\", F.lit(True))\n        generated_stays_and_moves = generated_stays_and_moves.withColumn(\"is_modified\", F.lit(False))\n\n        # 3) Link a cell id to each location\n\n        records_sdf = generated_stays_and_moves.union(records_with_location_errors)\n\n        records_sdf = self.add_cell_ids_to_locations(\n            records_sdf,\n            cells_df,\n            self.maximum_number_of_cells_for_event,\n            self.seed,\n        )\n\n        # 4) Continuing with the combined dataframe\n\n        records_sdf = records_sdf.union(records_with_cell_id_errors)\n\n        records_sdf = records_sdf.dropDuplicates([ColNames.user_id, ColNames.timestamp])\n\n        records_sdf = records_sdf.withColumn(\n            \"generated_geometry\",\n            STF.ST_Transform(\n                \"generated_geometry\",\n                F.lit(f\"EPSG:{self.cartesian_crs}\"),\n                F.lit(\"EPSG:4326\"),\n            ),\n        )\n\n        records_sdf = (\n            records_sdf.withColumn(ColNames.longitude, STF.ST_X(F.col(\"generated_geometry\")))\n            .withColumn(ColNames.latitude, STF.ST_Y(F.col(\"generated_geometry\")))\n            .drop(\"generated_geometry\")\n        )\n\n        # TODO: add rows with PLMN\n        # MCC and loc_error are added to the records\n        records_sdf = records_sdf.withColumn(ColNames.mcc, F.lit(self.mcc).cast(IntegerType()))\n\n        records_sdf = records_sdf.withColumn(ColNames.mnc, F.lit(self.mnc))\n\n        records_sdf = records_sdf.withColumn(ColNames.plmn, F.lit(None).cast(IntegerType()))\n\n        records_sdf = records_sdf.withColumn(ColNames.year, F.year(F.col(ColNames.timestamp)))\n        records_sdf = records_sdf.withColumn(ColNames.month, F.month(F.col(ColNames.timestamp)))\n        records_sdf = records_sdf.withColumn(ColNames.day, F.dayofmonth(F.col(ColNames.timestamp)))\n\n        # Generate errors\n\n        if self.do_event_error_generation:\n            records_sdf = self.generate_errors(synth_df_raw=records_sdf)\n\n        # Select bronze schema columns\n        columns = {field.name: F.col(field.name).cast(field.dataType) for field in BronzeEventDataObject.SCHEMA.fields}\n        records_sdf = records_sdf.withColumns(columns)\n\n        if self.order_output_by_timestamp:\n            records_sdf = records_sdf.orderBy(ColNames.timestamp)\n\n        self.output_data_objects[BronzeEventDataObject.ID].df = records_sdf\n\n    @staticmethod\n    def generate_event_timestamps_for_moves(\n        stays_sdf: DataFrame, event_freq_moves: int, cartesian_crs: int, seed: int\n    ) -&gt; DataFrame:\n        \"\"\"\n        Generates a DataFrame of event timestamps for movements between stays.\n\n        For each stay in the input DataFrame, this method generates a random number of timestamps equal to the time\n            difference between the end of the current stay and the start of the next stay, divided by the event\n            frequency for moves.\n\n        Args:\n            stays_sdf (pyspark.sql.DataFrame): A DataFrame of stays.\n            event_freq_moves (int): The frequency of events for movements.\n            cartesian_crs (int): The EPSG code of the Cartesian coordinate reference system to use for the geometries.\n            seed (int): The seed that determines the randomness of timestamp generation, and subsequent lat/lon generation.\n\n        Returns:\n            pyspark.sql.DataFrame: A DataFrame of event timestamps for movements.\n        \"\"\"\n\n        # Since the rows with activity_type = movement don't have any locations in the population diaries,\n        # we select the stay points and start generating timestamps in between the start and end of the stay\n\n        stays_sdf = stays_sdf.withColumn(\n            ColNames.geometry,\n            STF.ST_Transform(\n                STC.ST_Point(F.col(ColNames.longitude), F.col(ColNames.latitude)),\n                F.lit(\"EPSG:4326\"),\n                F.lit(f\"EPSG:{cartesian_crs}\"),\n            ),\n        )\n\n        # Define the window specification\u00a0\u2022\n        window_spec = Window.partitionBy(ColNames.year, ColNames.month, ColNames.day, ColNames.user_id).orderBy(\n            ColNames.initial_timestamp\n        )\n\n        # Add columns for next stay's geometry and start timestamp using the lead function\n        stays_sdf = stays_sdf.withColumn(\n            \"next_stay_initial_timestamp\",\n            F.lead(ColNames.initial_timestamp, 1).over(window_spec),\n        )\n\n        stays_sdf = stays_sdf.withColumn(\"next_stay_geometry\", F.lead(ColNames.geometry, 1).over(window_spec))\n\n        stays_sdf = stays_sdf.withColumn(\n            \"time_diff_seconds\",\n            F.unix_timestamp(\"next_stay_initial_timestamp\") - F.unix_timestamp(ColNames.final_timestamp),\n        )\n\n        # Calculate how many timestamps fit in the interval for the given frequency\n        stays_sdf = stays_sdf.withColumn(\n            \"timestamps_count\",\n            (F.col(\"time_diff_seconds\") / event_freq_moves).cast(\"integer\"),\n        )\n\n        expr_str = f\"transform(sequence(1, timestamps_count), x -&gt; rand({seed}))\"\n\n        stays_sdf = stays_sdf.withColumn(\n            \"random_fraction_on_line\",\n            F.expr(expr_str),\n        ).withColumn(\"random_fraction_on_line\", F.explode(F.col(\"random_fraction_on_line\")))\n\n        # Generate timestamps\n        stays_sdf = stays_sdf.withColumn(\n            \"offset_seconds\",\n            F.col(\"random_fraction_on_line\") * F.col(\"time_diff_seconds\"),\n        )\n        stays_sdf = stays_sdf.withColumn(\n            ColNames.timestamp,\n            F.from_unixtime(F.unix_timestamp(ColNames.final_timestamp) + F.col(\"offset_seconds\")),\n        )\n\n        moves_sdf = stays_sdf.withColumn(ColNames.activity_type, F.lit(\"move\"))\n\n        # Keep only necessary columns\n        moves_sdf = moves_sdf.select(\n            ColNames.user_id,\n            ColNames.timestamp,\n            ColNames.geometry,\n            \"next_stay_geometry\",\n            \"random_fraction_on_line\",\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n        )\n\n        return moves_sdf\n\n    @staticmethod\n    def generate_event_timestamps_for_stays(\n        stays_df: DataFrame, event_freq_stays: int, cartesian_crs: int, seed: int\n    ) -&gt; DataFrame:\n        \"\"\"\n        Generates a DataFrame of event timestamps for stays based on the event frequency for stays.\n\n        For each stay in the input DataFrame, this method calculates\n        the time difference between the initial and final timestamps of the stay.\n        It then generates a number of timestamps equal to this time difference divided by the event\n        frequency for stays. Each timestamp is associated with the location of the stay.\n\n        Args:\n            stays_df (pyspark.sql.DataFrame): A DataFrame of stays.\n            event_freq_stays (int): The frequency of events for stays.\n            cartesian_crs (int): The EPSG code of the Cartesian coordinate reference system to use for the geometries.\n            seed (int): The seed for generating timestamps randomly.\n        Returns:\n            pyspark.sql.DataFrame: A DataFrame of event timestamps for stays.\n        \"\"\"\n\n        stays_df = stays_df.withColumn(\n            \"time_diff_seconds\",\n            F.unix_timestamp(F.col(ColNames.final_timestamp)) - F.unix_timestamp(F.col(ColNames.initial_timestamp)),\n        )\n\n        stays_df = stays_df.withColumn(\n            \"timestamps_count\",\n            (F.col(\"time_diff_seconds\") / event_freq_stays).cast(\"integer\"),\n        )\n\n        expr_str = f\"transform(sequence(1, timestamps_count), x -&gt; rand({seed}))\"\n        stays_df = stays_df.withColumn(\n            \"random_fraction_between_timestamps\",\n            F.expr(expr_str),\n        )\n\n        stays_df = stays_df.withColumn(\n            \"random_fraction_between_timestamps\",\n            F.explode(F.col(\"random_fraction_between_timestamps\")),\n        )\n        stays_df = stays_df.withColumn(\n            \"offset_seconds\",\n            F.col(\"random_fraction_between_timestamps\") * F.col(\"time_diff_seconds\"),\n        )\n        stays_df = stays_df.withColumn(\n            ColNames.timestamp,\n            F.from_unixtime(F.unix_timestamp(ColNames.initial_timestamp) + F.col(\"offset_seconds\")),\n        )\n\n        stays_df = stays_df.withColumn(\n            \"generated_geometry\",\n            STF.ST_Transform(\n                STC.ST_Point(F.col(ColNames.longitude), F.col(ColNames.latitude)),\n                F.lit(\"EPSG:4326\"),\n                F.lit(f\"EPSG:{cartesian_crs}\"),\n            ),\n        )\n\n        stays_df = stays_df.select(\n            ColNames.user_id,\n            ColNames.timestamp,\n            \"generated_geometry\",\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n        )\n\n        return stays_df\n\n    @staticmethod\n    def generate_locations_for_moves(event_timestamps_df: DataFrame, cartesian_crs: int) -&gt; DataFrame:\n        \"\"\"\n        Generates locations for moves based on the event timestamps dataframe.\n        Returns a dataframe, where for each move in the event timestamps dataframe\n        a geometry column is added, representing the location of the move.\n\n        Performs interpolation along the line between the starting move point (previous stay point)\n        and next move point (next stay point) using the randomly generated values in the column random_fraction_on_line.\n\n        Args:\n            event_timestamps_df (pyspark.sql.DataFrame): The event timestamps dataframe.\n\n        Returns:\n            pyspark.sql.DataFrame: The event timestamps dataframe with the added geometry column.\n        \"\"\"\n\n        moves_with_geometry = event_timestamps_df.withColumn(\n            \"line\",\n            STF.ST_MakeLine(F.col(ColNames.geometry), F.col(\"next_stay_geometry\")),\n        )\n        moves_with_geometry = moves_with_geometry.withColumn(\n            \"generated_geometry\",\n            STF.ST_SetSRID(\n                STF.ST_LineInterpolatePoint(F.col(\"line\"), F.col(\"random_fraction_on_line\")),\n                cartesian_crs,\n            ),\n        )\n\n        moves_with_geometry = moves_with_geometry.select(\n            ColNames.user_id,\n            ColNames.timestamp,\n            \"generated_geometry\",\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n        )\n\n        return moves_with_geometry\n\n    @staticmethod\n    def add_cell_ids_to_locations(\n        events_with_locations_df: DataFrame,\n        cells_df: DataFrame,\n        max_n_of_cells: int,\n        seed: int,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Links cell IDs to locations in the events DataFrame.\n\n        This method performs a spatial join between the events and cells DataFrames to add cell IDs to each event.\n        It first creates a buffer around each event location and finds cells that intersect with this buffer.\n        It then calculates the distance from each event location to the cell and ranks the cells based on this distance.\n        It keeps only the top 'max_n_of_cells' closest cells for each event.\n\n        The method also adds a random index to each event-cell pair and filters to keep only one pair per event,\n        randomly selecting one of the closest cells for each event.\n\n        Args:\n            events_with_locations_df (pyspark.sql.DataFrame): A DataFrame of events\n            cells_df (pyspark.sql.DataFrame): A DataFrame of cells\n            max_n_of_cells (int): The maximum number of closest cells to consider for each event.\n\n        Returns:\n            pyspark.sql.DataFrame: A DataFrame of events with cell IDs added.\n        \"\"\"\n        events_with_cells_sdf = events_with_locations_df.join(\n            cells_df[[ColNames.cell_id, \"cell_geometry\"]],\n            (\n                STP.ST_Intersects(\n                    STF.ST_Buffer(\n                        events_with_locations_df[\"generated_geometry\"],\n                        F.col(\"closest_cell_distance_max\"),\n                    ),\n                    cells_df[\"cell_geometry\"],\n                )\n            ),\n        ).withColumn(\n            \"distance_to_cell\",\n            STF.ST_Distance(\n                events_with_locations_df[\"generated_geometry\"],\n                cells_df[\"cell_geometry\"],\n            ),\n        )\n\n        # Selection of different cell ids for a given timestamp is random\n        window_spec = Window.partitionBy(\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            ColNames.user_id,\n            ColNames.timestamp,\n        ).orderBy(F.col(\"distance_to_cell\"))\n\n        events_with_cells_sdf = events_with_cells_sdf.withColumn(\n            \"closest_cells_index\", F.row_number().over(window_spec)\n        ).filter(F.col(\"closest_cells_index\") &lt;= max_n_of_cells)\n\n        window_spec_random = Window.partitionBy(\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            ColNames.user_id,\n            ColNames.timestamp,\n        ).orderBy(F.rand(seed=seed))\n        events_with_cells_sdf = events_with_cells_sdf.withColumn(\n            \"random_cell_index\", F.row_number().over(window_spec_random)\n        ).filter(F.col(\"random_cell_index\") == 1)\n\n        records_sdf = events_with_cells_sdf.select(\n            ColNames.user_id,\n            ColNames.timestamp,\n            \"generated_geometry\",\n            ColNames.cell_id,\n            ColNames.loc_error,\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            \"is_modified\",\n        )\n\n        return records_sdf\n\n    @staticmethod\n    def generate_location_errors(\n        records_sdf: DataFrame,\n        error_location_distance_max: float,\n        error_location_distance_min: float,\n        closest_cell_distance_max: float,\n        cartesian_crs: int,\n        seed: int,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Generates location errors for x and y coordinates of each record in the DataFrame.\n\n        This method adds a random location error to the x and y coordinates of each record in the input DataFrame.\n        The location error is a random value between error_location_distance_min and error_location_distance_max,\n        and is added or subtracted from the x and y coordinates based on a random sign.\n\n        Args:\n            records_sdf (DataFrame): A DataFrame of records\n            error_location_distance_max (float): The maximum location error distance.\n            error_location_distance_min (float): The minimum location error distance.\n            closest_cell_distance_max (float): The maximum distance to the closest cell.\n            cartesian_crs (int): The EPSG code of the Cartesian coordinate reference system to use for the geometries.\n            seed (int): The seed for the random number generator.\n\n        Returns:\n            DataFrame: A DataFrame of records with location errors added to the x and y coordinates.\n        \"\"\"\n\n        errors_df = (\n            records_sdf.withColumn(\"y\", STF.ST_Y(F.col(\"generated_geometry\")))\n            .withColumn(\"x\", STF.ST_X(F.col(\"generated_geometry\")))\n            .drop(\"generated_geometry\")\n        )\n\n        errors_df = errors_df.withColumn(\n            ColNames.loc_error,\n            (F.rand(seed=seed) * (error_location_distance_max - error_location_distance_min))\n            + error_location_distance_min,\n        )\n\n        errors_df = (\n            errors_df.withColumn(\"sign\", F.when(F.rand(seed=seed) &gt; 0.5, 1).otherwise(-1))\n            .withColumn(\"new_x\", F.col(\"x\") + (F.col(ColNames.loc_error) * F.col(\"sign\")))\n            .withColumn(\"new_y\", F.col(\"y\") + (F.col(ColNames.loc_error) * F.col(\"sign\")))\n        )\n\n        errors_df = errors_df.withColumn(\n            \"generated_geometry\",\n            STF.ST_SetSRID(STC.ST_Point(F.col(\"new_x\"), F.col(\"new_y\")), cartesian_crs),\n        ).drop(\"new_x\", \"new_y\")\n\n        errors_df = errors_df.withColumn(\"closest_cell_distance_max\", F.lit(closest_cell_distance_max))\n\n        errors_df = errors_df.select(\n            ColNames.user_id,\n            ColNames.timestamp,\n            \"generated_geometry\",\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            ColNames.loc_error,\n            \"closest_cell_distance_max\",\n        )\n\n        return errors_df\n\n    @staticmethod\n    def generate_records_with_non_existant_cell_ids(records_sdf: DataFrame, cells_sdf: DataFrame, seed) -&gt; DataFrame:\n        \"\"\"\n        Adds the cell_id column so that it will contain cell_ids that\n        are not present in the cells_df dataframe, yet follow the format of a cell id.\n\n        Args:\n            records_sdf (DataFrame): generated records\n            cells_sdf (DataFrame): cells dataframe\n\n            DataFrame: records with cell ids that are not present in the cells_df dataframe\n        \"\"\"\n\n        # Generates random cell ids for cells_df, and selects those\n        # Join to records is implemented with a monotonically increasing id\n        # So to limit that, this number of all unique cells is used\n        # TODO check how to make this more optimal\n\n        n_of_actual_cell_ids_as_base = cells_sdf.count()\n\n        cells_df_with_random_cell_ids = cells_sdf[[ColNames.cell_id]].withColumn(\n            \"random_cell_id\",\n            (F.rand(seed=seed) * (999_999_999_999_999 - 10_000_000_000_000) + 10_000_000_000_000).cast(\"bigint\"),\n        )\n\n        # Do an  left anti join to ensure that now generated cell ids are not among actual cell ids\n\n        cells_df_inner_joined = cells_df_with_random_cell_ids[[\"random_cell_id\"]].join(\n            cells_sdf[[ColNames.cell_id]],\n            on=F.col(\"random_cell_id\") == F.col(ColNames.cell_id),\n            how=\"leftanti\",\n        )\n\n        cells_df_inner_joined = cells_df_inner_joined.withColumn(\"row_number\", (F.monotonically_increasing_id()))\n        records_sdf = records_sdf.withColumn(\n            \"row_number\",\n            (F.monotonically_increasing_id() % n_of_actual_cell_ids_as_base) + 1,\n        )\n\n        cells_df_inner_joined = cells_df_inner_joined.select(\n            \"row_number\", F.col(\"random_cell_id\").alias(ColNames.cell_id)\n        )\n\n        records_with_random_cell_id = records_sdf.join(cells_df_inner_joined, on=\"row_number\", how=\"left\").drop(\n            \"row_number\"\n        )\n\n        records_with_random_cell_id = records_with_random_cell_id.select(\n            ColNames.user_id,\n            ColNames.timestamp,\n            \"generated_geometry\",\n            ColNames.cell_id,\n            ColNames.loc_error,\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n        )\n\n        return records_with_random_cell_id\n\n    # @staticmethod\n    def generate_nulls_in_mandatory_fields(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Generates null values in some fields of some rows based on configuration parameters.\n\n        Args:\n            df (pyspark.sql.DataFrame): clean synthetic data\n\n        Returns:\n            pyspark.sql.DataFrame: synthetic records dataframe with nulls in some columns of some rows\n        \"\"\"\n\n        # Two probability parameters from config apply:\n        # First one sets how many rows (as fraction of all rows) are selected for possible value nulling.\n        # Second one sets the likelyhood for each column to be set to null.\n        # If 1.0, then all mandatory columns of each selected row will be nulled.\n\n        if self.null_row_prob == 0.0:\n            # TODO logging\n            return df\n\n        # Split input dataframe to unchanged and changed portions\n        df = df.cache()\n        error_row_prob = self.null_row_prob\n        unchanged_row_prob = 1.0 - error_row_prob\n        unchanged_rows_df, error_rows_df = df.randomSplit([unchanged_row_prob, error_row_prob], seed=self.seed + 1)\n\n        columns_for_null_selection = list(self.error_generation_allowed_columns)\n        columns_for_null_selection.sort()\n\n        random.seed(self.seed)\n        columns_to_set_as_null = random.sample(\n            columns_for_null_selection,\n            int(round(self.column_is_null_probability * len(columns_for_null_selection), 0)),\n        )\n\n        for column in columns_to_set_as_null:\n            error_rows_df = error_rows_df.withColumn(column, F.lit(None))\n\n        error_rows_df = error_rows_df.withColumn(\"is_modified\", F.lit(True))\n\n        # Re-combine unchanged and changed rows of the dataframe.\n        return unchanged_rows_df.union(error_rows_df)\n\n    # @staticmethod\n    def generate_out_of_bounds_dates(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Transforms the timestamp column values to be out of bound of the selected period,\n        based on probabilities from configuration.\n        Only rows with non-null timestamp values can become altered here.\n\n        Args:\n            df (pyspark.sql.DataFrame): Dataframe of clean synthetic events.\n\n        Returns:\n            pyspark.sql.DataFrame: Data where some timestamp column values are out of bounds as per config.\n        \"\"\"\n\n        if self.out_of_bounds_prob == 0.0:\n            # TODO logging\n            return df\n\n        # Calculate approximate span in months from config parameters.\n        # The parameters should approximately cover the range of \"clean\" dates, so that the erroneous values can be generated outside the range.\n\n        # TODO\n        # This now uses the whole input data to set the bounds\n        ending_timestamp = df.agg(F.max(ColNames.timestamp)).collect()[0][0]\n        starting_timestamp = df.agg(F.min(ColNames.timestamp)).collect()[0][0]\n        events_span_in_months = max(1, (pd.Timestamp(ending_timestamp) - pd.Timestamp(starting_timestamp)).days / 30)\n\n        # Split rows by null/non-null timestamp.\n        df = df.cache()\n        null_timestamp_df = df.where(F.col(ColNames.timestamp).isNull())\n        nonnull_timestamp_df = df.where(F.col(ColNames.timestamp).isNotNull())\n        df.unpersist()\n\n        # From non-null timestamp rows, select a subset for adding errors to, depending on config parameter.\n        nonnull_timestamp_df = nonnull_timestamp_df.cache()\n        error_row_prob = self.out_of_bounds_prob\n        unchanged_row_prob = 1.0 - error_row_prob\n        unchanged_rows_df, error_rows_df = nonnull_timestamp_df.randomSplit(\n            [unchanged_row_prob, error_row_prob], seed=self.seed + 3\n        )\n        # Combine null timestamp rows and not-modified non-null timestamp rows.\n        unchanged_rows_df = unchanged_rows_df.union(null_timestamp_df)\n\n        # Add months offset to error rows to make their timestamp values become outside expected range.\n        months_to_add_col = (F.lit(2) + F.rand(seed=self.seed)) * F.lit(events_span_in_months)\n        modified_date_col = F.add_months(F.col(ColNames.timestamp), months_to_add_col)\n        time_col = F.date_format(F.col(ColNames.timestamp), \" HH:mm:ss\")\n        error_rows_df = error_rows_df.withColumn(ColNames.timestamp, F.concat(modified_date_col, time_col))\n        error_rows_df = error_rows_df.withColumn(\"is_modified\", F.lit(True))\n\n        # Combine changed and unchanged rows dataframes.\n        return unchanged_rows_df.union(error_rows_df)\n\n    # @staticmethod\n    def generate_erroneous_type_values(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Generates errors for sampled rows. Errors are custom defined, for instance a random string, or corrupt timestamp.\n        Does not cast the columns to a different type.\n\n        Args:\n            df (pyspark.sql.DataFrame): dataframe that may have out of bound and null records.\n\n        Returns:\n            pyspark.sql.DataFrame: dataframe with erroneous rows, and possibly, with nulls and out of bound records.\n        \"\"\"\n\n        if self.data_type_error_prob == 0:\n            # TODO logging\n            return df\n\n        # Split dataframe by whether the row has been modified during the error-adding process already.\n        # Already errored rows do not get further changes.\n        df = df.cache()\n        previously_modified_rows_df = df.where(F.col(\"is_modified\"))\n        unmodified_rows_df = df.where(~(F.col(\"is_modified\")))\n        df.unpersist()\n\n        # From unmodified rows, select a subset for adding errors to, depending on config parameter.\n        unmodified_rows_df = unmodified_rows_df.cache()\n        error_row_prob = self.data_type_error_prob\n        unchanged_row_prob = 1.0 - error_row_prob\n        unchanged_rows_df, error_rows_df = unmodified_rows_df.randomSplit(\n            [unchanged_row_prob, error_row_prob], seed=self.seed + 5\n        )\n        # Gather rows that are not modified in this step (combine previously-modified rows and not-selected unmodified rows).\n        unchanged_rows_df = unchanged_rows_df.union(previously_modified_rows_df)\n\n        # Iterate over mandatory columns to mutate the value, depending on column data type.\n        for struct_schema in BronzeEventDataObject.SCHEMA:\n            if struct_schema.name not in self.error_generation_allowed_columns:\n                continue\n\n            column = struct_schema.name\n            col_dtype = struct_schema.dataType\n\n            if col_dtype in [BinaryType()]:\n                # md5 is a smaller hash,\n                to_value = F.unhex(F.md5(F.base64(F.col(column)).cast(StringType())))\n\n            if col_dtype in [FloatType(), IntegerType()]:\n                # changes mcc, lat, lon\n                to_value = (F.col(column) + ((F.rand(seed=self.seed) + F.lit(180)) * 10000)).cast(\"int\")\n\n            if column == ColNames.timestamp and col_dtype == StringType():\n                # Timezone difference manipulation may be performed here, if cleaning module were to support it.\n                # statically one timezone difference\n                # timezone_to = random.randint(0, 12)\n                to_value = F.concat(\n                    F.substring(F.col(column), 1, 10),\n                    F.lit(\"T\"),\n                    F.substring(F.col(column), 12, 9),\n                    # TODO: Temporary remove of timezone addition as cleaning\n                    # module does not support it\n                    # F.lit(f\"+0{timezone_to}:00\")\n                )\n\n            if column == ColNames.cell_id and col_dtype == StringType():\n                random.seed(self.seed)\n                random_string = \"\".join(random.choice(string.ascii_uppercase + string.digits) for _ in range(6)) + \"_\"\n                to_value = F.concat(F.lit(random_string), (F.rand(seed=self.seed) * 100).cast(\"int\"))\n\n            error_rows_df = error_rows_df.withColumn(column, to_value)\n        error_rows_df = error_rows_df.withColumn(\"is_modified\", F.lit(True))\n        return unchanged_rows_df.union(error_rows_df)\n\n    def generate_same_location_duplicates(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Selects a subset of the previously generated data (syntactically clean data) and creates same location duplicates.\n        If input has odd number of rows, one row is discarded.\n\n        Args:\n            df (pyspark.sql.DataFrame): Dataframe of clean synthetic events.\n\n        Returns:\n            pyspark.sql.DataFrame: Dataframe with same location duplicates.\n\n        \"\"\"\n\n        if self.same_location_duplicate_prob == 0:\n            return df\n\n        # Split dataframe by whether the row has been modified during the error-adding process already.\n        # Already errored rows do not get further changes.\n        df = df.cache()\n        previously_modified_rows_df = df.where(F.col(\"is_modified\"))\n        unmodified_rows_df = df.where(~(F.col(\"is_modified\")))\n        df.unpersist()\n\n        # From unmodified rows, select a subset for adding errors to, depending on config parameter.\n        unmodified_rows_df = unmodified_rows_df.cache()\n        error_row_prob = self.same_location_duplicate_prob\n        unchanged_row_prob = 1.0 - error_row_prob\n\n        unchanged_rows_df, error_rows_df = unmodified_rows_df.randomSplit(\n            [unchanged_row_prob, error_row_prob], seed=self.seed + 7\n        )\n\n        # Gather rows that are not modified in this step (combine previously-modified rows and not-selected unmodified rows).\n        unchanged_rows_df = unchanged_rows_df.union(previously_modified_rows_df)\n\n        # Select all even rows and duplicate these\n        error_rows_df = error_rows_df.withColumn(\n            \"user_row_num\",\n            F.row_number().over(Window.partitionBy(ColNames.user_id).orderBy(ColNames.timestamp)),\n        )\n\n        even_rows = error_rows_df.where(F.col(\"user_row_num\") % 2 == 0).drop(\"user_row_num\")\n\n        error_rows_df = even_rows.union(even_rows)\n        error_rows_df = error_rows_df.withColumn(\"is_modified\", F.lit(True))\n\n        return unchanged_rows_df.union(error_rows_df)\n\n    def generate_different_location_duplicates(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Selects a subset of the previously generated data (syntactically clean data) and creates different location duplicates.\n        If input has odd number of rows, one row is discarded.\n\n        Args:\n            df (pyspark.sql.DataFrame): Dataframe of clean synthetic events.\n\n        Returns:\n            pyspark.sql.DataFrame: Dataframe with same different location duplicates.\n\n        \"\"\"\n\n        if self.different_location_duplicate_prob == 0:\n            return df\n\n        # Split dataframe by whether the row has been modified during the error-adding process already.\n        # Already errored rows do not get further changes.\n        df = df.cache()\n        previously_modified_rows_df = df.where(F.col(\"is_modified\"))\n        unmodified_rows_df = df.where(~(F.col(\"is_modified\")))\n        df.unpersist()\n\n        # From unmodified rows, select a subset for adding errors to, depending on config parameter.\n        unmodified_rows_df = unmodified_rows_df.cache()\n        error_row_prob = self.different_location_duplicate_prob\n        unchanged_row_prob = 1.0 - error_row_prob\n        unchanged_rows_df, error_rows_df = unmodified_rows_df.randomSplit(\n            [unchanged_row_prob, error_row_prob], seed=self.seed + 9\n        )\n\n        # Gather rows that are not modified in this step (combine previously-modified rows and not-selected unmodified rows).\n        unchanged_rows_df = unchanged_rows_df.union(previously_modified_rows_df)\n\n        # Select all even rows and modify one set of these to offset the location\n\n        error_rows_df = error_rows_df.withColumn(\n            \"user_row_num\",\n            F.row_number().over(Window.partitionBy(ColNames.user_id).orderBy(ColNames.timestamp)),\n        )\n\n        even_rows = error_rows_df.where(F.col(\"user_row_num\") % 2 == 0).drop(\"user_row_num\")\n\n        # No change to cell id here, as that would additionnally need to check that the modified cell id is actual in the diaries\n        modified_rows = even_rows.withColumn(ColNames.latitude, F.col(ColNames.latitude) * F.lit(0.95)).withColumn(\n            ColNames.longitude, F.col(ColNames.longitude) * F.lit(0.95)\n        )\n\n        error_rows_df = even_rows.union(modified_rows)\n        error_rows_df = error_rows_df.withColumn(\"is_modified\", F.lit(True))\n\n        return unchanged_rows_df.union(error_rows_df)\n\n    def generate_errors(self, synth_df_raw: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Inputs a dataframe that contains synthetic records based on diaries.\n        These records include locational errors, etc. This function only selects the clean generated records from previous steps.\n        Generates errors for those clean records.\n        Calls all error generation functions.\n\n        Args:\n            synth_df_raw (pyspark.sql.DataFrame): Data of raw and clean synthetic events.\n\n        Returns:\n            pyspark.sql.DataFrame: Dataframe, with erroneous records, according to probabilities defined in the configuration.\n        \"\"\"\n\n        synth_df_raw = synth_df_raw.where(~F.col(\"is_modified\"))\n\n        synth_df = synth_df_raw.cache()\n\n        synth_df = self.generate_nulls_in_mandatory_fields(synth_df)\n        synth_df = self.generate_out_of_bounds_dates(synth_df)\n        synth_df = self.generate_erroneous_type_values(synth_df)\n        synth_df = self.generate_same_location_duplicates(synth_df)\n        synth_df = self.generate_different_location_duplicates(synth_df)\n\n        return synth_df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.add_cell_ids_to_locations","title":"<code>add_cell_ids_to_locations(events_with_locations_df, cells_df, max_n_of_cells, seed)</code>  <code>staticmethod</code>","text":"<p>Links cell IDs to locations in the events DataFrame.</p> <p>This method performs a spatial join between the events and cells DataFrames to add cell IDs to each event. It first creates a buffer around each event location and finds cells that intersect with this buffer. It then calculates the distance from each event location to the cell and ranks the cells based on this distance. It keeps only the top 'max_n_of_cells' closest cells for each event.</p> <p>The method also adds a random index to each event-cell pair and filters to keep only one pair per event, randomly selecting one of the closest cells for each event.</p> <p>Parameters:</p> Name Type Description Default <code>events_with_locations_df</code> <code>DataFrame</code> <p>A DataFrame of events</p> required <code>cells_df</code> <code>DataFrame</code> <p>A DataFrame of cells</p> required <code>max_n_of_cells</code> <code>int</code> <p>The maximum number of closest cells to consider for each event.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: A DataFrame of events with cell IDs added.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>@staticmethod\ndef add_cell_ids_to_locations(\n    events_with_locations_df: DataFrame,\n    cells_df: DataFrame,\n    max_n_of_cells: int,\n    seed: int,\n) -&gt; DataFrame:\n    \"\"\"\n    Links cell IDs to locations in the events DataFrame.\n\n    This method performs a spatial join between the events and cells DataFrames to add cell IDs to each event.\n    It first creates a buffer around each event location and finds cells that intersect with this buffer.\n    It then calculates the distance from each event location to the cell and ranks the cells based on this distance.\n    It keeps only the top 'max_n_of_cells' closest cells for each event.\n\n    The method also adds a random index to each event-cell pair and filters to keep only one pair per event,\n    randomly selecting one of the closest cells for each event.\n\n    Args:\n        events_with_locations_df (pyspark.sql.DataFrame): A DataFrame of events\n        cells_df (pyspark.sql.DataFrame): A DataFrame of cells\n        max_n_of_cells (int): The maximum number of closest cells to consider for each event.\n\n    Returns:\n        pyspark.sql.DataFrame: A DataFrame of events with cell IDs added.\n    \"\"\"\n    events_with_cells_sdf = events_with_locations_df.join(\n        cells_df[[ColNames.cell_id, \"cell_geometry\"]],\n        (\n            STP.ST_Intersects(\n                STF.ST_Buffer(\n                    events_with_locations_df[\"generated_geometry\"],\n                    F.col(\"closest_cell_distance_max\"),\n                ),\n                cells_df[\"cell_geometry\"],\n            )\n        ),\n    ).withColumn(\n        \"distance_to_cell\",\n        STF.ST_Distance(\n            events_with_locations_df[\"generated_geometry\"],\n            cells_df[\"cell_geometry\"],\n        ),\n    )\n\n    # Selection of different cell ids for a given timestamp is random\n    window_spec = Window.partitionBy(\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n        ColNames.user_id,\n        ColNames.timestamp,\n    ).orderBy(F.col(\"distance_to_cell\"))\n\n    events_with_cells_sdf = events_with_cells_sdf.withColumn(\n        \"closest_cells_index\", F.row_number().over(window_spec)\n    ).filter(F.col(\"closest_cells_index\") &lt;= max_n_of_cells)\n\n    window_spec_random = Window.partitionBy(\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n        ColNames.user_id,\n        ColNames.timestamp,\n    ).orderBy(F.rand(seed=seed))\n    events_with_cells_sdf = events_with_cells_sdf.withColumn(\n        \"random_cell_index\", F.row_number().over(window_spec_random)\n    ).filter(F.col(\"random_cell_index\") == 1)\n\n    records_sdf = events_with_cells_sdf.select(\n        ColNames.user_id,\n        ColNames.timestamp,\n        \"generated_geometry\",\n        ColNames.cell_id,\n        ColNames.loc_error,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n        \"is_modified\",\n    )\n\n    return records_sdf\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_different_location_duplicates","title":"<code>generate_different_location_duplicates(df)</code>","text":"<p>Selects a subset of the previously generated data (syntactically clean data) and creates different location duplicates. If input has odd number of rows, one row is discarded.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe of clean synthetic events.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: Dataframe with same different location duplicates.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>def generate_different_location_duplicates(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Selects a subset of the previously generated data (syntactically clean data) and creates different location duplicates.\n    If input has odd number of rows, one row is discarded.\n\n    Args:\n        df (pyspark.sql.DataFrame): Dataframe of clean synthetic events.\n\n    Returns:\n        pyspark.sql.DataFrame: Dataframe with same different location duplicates.\n\n    \"\"\"\n\n    if self.different_location_duplicate_prob == 0:\n        return df\n\n    # Split dataframe by whether the row has been modified during the error-adding process already.\n    # Already errored rows do not get further changes.\n    df = df.cache()\n    previously_modified_rows_df = df.where(F.col(\"is_modified\"))\n    unmodified_rows_df = df.where(~(F.col(\"is_modified\")))\n    df.unpersist()\n\n    # From unmodified rows, select a subset for adding errors to, depending on config parameter.\n    unmodified_rows_df = unmodified_rows_df.cache()\n    error_row_prob = self.different_location_duplicate_prob\n    unchanged_row_prob = 1.0 - error_row_prob\n    unchanged_rows_df, error_rows_df = unmodified_rows_df.randomSplit(\n        [unchanged_row_prob, error_row_prob], seed=self.seed + 9\n    )\n\n    # Gather rows that are not modified in this step (combine previously-modified rows and not-selected unmodified rows).\n    unchanged_rows_df = unchanged_rows_df.union(previously_modified_rows_df)\n\n    # Select all even rows and modify one set of these to offset the location\n\n    error_rows_df = error_rows_df.withColumn(\n        \"user_row_num\",\n        F.row_number().over(Window.partitionBy(ColNames.user_id).orderBy(ColNames.timestamp)),\n    )\n\n    even_rows = error_rows_df.where(F.col(\"user_row_num\") % 2 == 0).drop(\"user_row_num\")\n\n    # No change to cell id here, as that would additionnally need to check that the modified cell id is actual in the diaries\n    modified_rows = even_rows.withColumn(ColNames.latitude, F.col(ColNames.latitude) * F.lit(0.95)).withColumn(\n        ColNames.longitude, F.col(ColNames.longitude) * F.lit(0.95)\n    )\n\n    error_rows_df = even_rows.union(modified_rows)\n    error_rows_df = error_rows_df.withColumn(\"is_modified\", F.lit(True))\n\n    return unchanged_rows_df.union(error_rows_df)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_erroneous_type_values","title":"<code>generate_erroneous_type_values(df)</code>","text":"<p>Generates errors for sampled rows. Errors are custom defined, for instance a random string, or corrupt timestamp. Does not cast the columns to a different type.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe that may have out of bound and null records.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: dataframe with erroneous rows, and possibly, with nulls and out of bound records.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>def generate_erroneous_type_values(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Generates errors for sampled rows. Errors are custom defined, for instance a random string, or corrupt timestamp.\n    Does not cast the columns to a different type.\n\n    Args:\n        df (pyspark.sql.DataFrame): dataframe that may have out of bound and null records.\n\n    Returns:\n        pyspark.sql.DataFrame: dataframe with erroneous rows, and possibly, with nulls and out of bound records.\n    \"\"\"\n\n    if self.data_type_error_prob == 0:\n        # TODO logging\n        return df\n\n    # Split dataframe by whether the row has been modified during the error-adding process already.\n    # Already errored rows do not get further changes.\n    df = df.cache()\n    previously_modified_rows_df = df.where(F.col(\"is_modified\"))\n    unmodified_rows_df = df.where(~(F.col(\"is_modified\")))\n    df.unpersist()\n\n    # From unmodified rows, select a subset for adding errors to, depending on config parameter.\n    unmodified_rows_df = unmodified_rows_df.cache()\n    error_row_prob = self.data_type_error_prob\n    unchanged_row_prob = 1.0 - error_row_prob\n    unchanged_rows_df, error_rows_df = unmodified_rows_df.randomSplit(\n        [unchanged_row_prob, error_row_prob], seed=self.seed + 5\n    )\n    # Gather rows that are not modified in this step (combine previously-modified rows and not-selected unmodified rows).\n    unchanged_rows_df = unchanged_rows_df.union(previously_modified_rows_df)\n\n    # Iterate over mandatory columns to mutate the value, depending on column data type.\n    for struct_schema in BronzeEventDataObject.SCHEMA:\n        if struct_schema.name not in self.error_generation_allowed_columns:\n            continue\n\n        column = struct_schema.name\n        col_dtype = struct_schema.dataType\n\n        if col_dtype in [BinaryType()]:\n            # md5 is a smaller hash,\n            to_value = F.unhex(F.md5(F.base64(F.col(column)).cast(StringType())))\n\n        if col_dtype in [FloatType(), IntegerType()]:\n            # changes mcc, lat, lon\n            to_value = (F.col(column) + ((F.rand(seed=self.seed) + F.lit(180)) * 10000)).cast(\"int\")\n\n        if column == ColNames.timestamp and col_dtype == StringType():\n            # Timezone difference manipulation may be performed here, if cleaning module were to support it.\n            # statically one timezone difference\n            # timezone_to = random.randint(0, 12)\n            to_value = F.concat(\n                F.substring(F.col(column), 1, 10),\n                F.lit(\"T\"),\n                F.substring(F.col(column), 12, 9),\n                # TODO: Temporary remove of timezone addition as cleaning\n                # module does not support it\n                # F.lit(f\"+0{timezone_to}:00\")\n            )\n\n        if column == ColNames.cell_id and col_dtype == StringType():\n            random.seed(self.seed)\n            random_string = \"\".join(random.choice(string.ascii_uppercase + string.digits) for _ in range(6)) + \"_\"\n            to_value = F.concat(F.lit(random_string), (F.rand(seed=self.seed) * 100).cast(\"int\"))\n\n        error_rows_df = error_rows_df.withColumn(column, to_value)\n    error_rows_df = error_rows_df.withColumn(\"is_modified\", F.lit(True))\n    return unchanged_rows_df.union(error_rows_df)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_errors","title":"<code>generate_errors(synth_df_raw)</code>","text":"<p>Inputs a dataframe that contains synthetic records based on diaries. These records include locational errors, etc. This function only selects the clean generated records from previous steps. Generates errors for those clean records. Calls all error generation functions.</p> <p>Parameters:</p> Name Type Description Default <code>synth_df_raw</code> <code>DataFrame</code> <p>Data of raw and clean synthetic events.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: Dataframe, with erroneous records, according to probabilities defined in the configuration.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>def generate_errors(self, synth_df_raw: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Inputs a dataframe that contains synthetic records based on diaries.\n    These records include locational errors, etc. This function only selects the clean generated records from previous steps.\n    Generates errors for those clean records.\n    Calls all error generation functions.\n\n    Args:\n        synth_df_raw (pyspark.sql.DataFrame): Data of raw and clean synthetic events.\n\n    Returns:\n        pyspark.sql.DataFrame: Dataframe, with erroneous records, according to probabilities defined in the configuration.\n    \"\"\"\n\n    synth_df_raw = synth_df_raw.where(~F.col(\"is_modified\"))\n\n    synth_df = synth_df_raw.cache()\n\n    synth_df = self.generate_nulls_in_mandatory_fields(synth_df)\n    synth_df = self.generate_out_of_bounds_dates(synth_df)\n    synth_df = self.generate_erroneous_type_values(synth_df)\n    synth_df = self.generate_same_location_duplicates(synth_df)\n    synth_df = self.generate_different_location_duplicates(synth_df)\n\n    return synth_df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_event_timestamps_for_moves","title":"<code>generate_event_timestamps_for_moves(stays_sdf, event_freq_moves, cartesian_crs, seed)</code>  <code>staticmethod</code>","text":"<p>Generates a DataFrame of event timestamps for movements between stays.</p> <p>For each stay in the input DataFrame, this method generates a random number of timestamps equal to the time     difference between the end of the current stay and the start of the next stay, divided by the event     frequency for moves.</p> <p>Parameters:</p> Name Type Description Default <code>stays_sdf</code> <code>DataFrame</code> <p>A DataFrame of stays.</p> required <code>event_freq_moves</code> <code>int</code> <p>The frequency of events for movements.</p> required <code>cartesian_crs</code> <code>int</code> <p>The EPSG code of the Cartesian coordinate reference system to use for the geometries.</p> required <code>seed</code> <code>int</code> <p>The seed that determines the randomness of timestamp generation, and subsequent lat/lon generation.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: A DataFrame of event timestamps for movements.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>@staticmethod\ndef generate_event_timestamps_for_moves(\n    stays_sdf: DataFrame, event_freq_moves: int, cartesian_crs: int, seed: int\n) -&gt; DataFrame:\n    \"\"\"\n    Generates a DataFrame of event timestamps for movements between stays.\n\n    For each stay in the input DataFrame, this method generates a random number of timestamps equal to the time\n        difference between the end of the current stay and the start of the next stay, divided by the event\n        frequency for moves.\n\n    Args:\n        stays_sdf (pyspark.sql.DataFrame): A DataFrame of stays.\n        event_freq_moves (int): The frequency of events for movements.\n        cartesian_crs (int): The EPSG code of the Cartesian coordinate reference system to use for the geometries.\n        seed (int): The seed that determines the randomness of timestamp generation, and subsequent lat/lon generation.\n\n    Returns:\n        pyspark.sql.DataFrame: A DataFrame of event timestamps for movements.\n    \"\"\"\n\n    # Since the rows with activity_type = movement don't have any locations in the population diaries,\n    # we select the stay points and start generating timestamps in between the start and end of the stay\n\n    stays_sdf = stays_sdf.withColumn(\n        ColNames.geometry,\n        STF.ST_Transform(\n            STC.ST_Point(F.col(ColNames.longitude), F.col(ColNames.latitude)),\n            F.lit(\"EPSG:4326\"),\n            F.lit(f\"EPSG:{cartesian_crs}\"),\n        ),\n    )\n\n    # Define the window specification\u00a0\u2022\n    window_spec = Window.partitionBy(ColNames.year, ColNames.month, ColNames.day, ColNames.user_id).orderBy(\n        ColNames.initial_timestamp\n    )\n\n    # Add columns for next stay's geometry and start timestamp using the lead function\n    stays_sdf = stays_sdf.withColumn(\n        \"next_stay_initial_timestamp\",\n        F.lead(ColNames.initial_timestamp, 1).over(window_spec),\n    )\n\n    stays_sdf = stays_sdf.withColumn(\"next_stay_geometry\", F.lead(ColNames.geometry, 1).over(window_spec))\n\n    stays_sdf = stays_sdf.withColumn(\n        \"time_diff_seconds\",\n        F.unix_timestamp(\"next_stay_initial_timestamp\") - F.unix_timestamp(ColNames.final_timestamp),\n    )\n\n    # Calculate how many timestamps fit in the interval for the given frequency\n    stays_sdf = stays_sdf.withColumn(\n        \"timestamps_count\",\n        (F.col(\"time_diff_seconds\") / event_freq_moves).cast(\"integer\"),\n    )\n\n    expr_str = f\"transform(sequence(1, timestamps_count), x -&gt; rand({seed}))\"\n\n    stays_sdf = stays_sdf.withColumn(\n        \"random_fraction_on_line\",\n        F.expr(expr_str),\n    ).withColumn(\"random_fraction_on_line\", F.explode(F.col(\"random_fraction_on_line\")))\n\n    # Generate timestamps\n    stays_sdf = stays_sdf.withColumn(\n        \"offset_seconds\",\n        F.col(\"random_fraction_on_line\") * F.col(\"time_diff_seconds\"),\n    )\n    stays_sdf = stays_sdf.withColumn(\n        ColNames.timestamp,\n        F.from_unixtime(F.unix_timestamp(ColNames.final_timestamp) + F.col(\"offset_seconds\")),\n    )\n\n    moves_sdf = stays_sdf.withColumn(ColNames.activity_type, F.lit(\"move\"))\n\n    # Keep only necessary columns\n    moves_sdf = moves_sdf.select(\n        ColNames.user_id,\n        ColNames.timestamp,\n        ColNames.geometry,\n        \"next_stay_geometry\",\n        \"random_fraction_on_line\",\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    )\n\n    return moves_sdf\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_event_timestamps_for_stays","title":"<code>generate_event_timestamps_for_stays(stays_df, event_freq_stays, cartesian_crs, seed)</code>  <code>staticmethod</code>","text":"<p>Generates a DataFrame of event timestamps for stays based on the event frequency for stays.</p> <p>For each stay in the input DataFrame, this method calculates the time difference between the initial and final timestamps of the stay. It then generates a number of timestamps equal to this time difference divided by the event frequency for stays. Each timestamp is associated with the location of the stay.</p> <p>Parameters:</p> Name Type Description Default <code>stays_df</code> <code>DataFrame</code> <p>A DataFrame of stays.</p> required <code>event_freq_stays</code> <code>int</code> <p>The frequency of events for stays.</p> required <code>cartesian_crs</code> <code>int</code> <p>The EPSG code of the Cartesian coordinate reference system to use for the geometries.</p> required <code>seed</code> <code>int</code> <p>The seed for generating timestamps randomly.</p> required <p>Returns:     pyspark.sql.DataFrame: A DataFrame of event timestamps for stays.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>@staticmethod\ndef generate_event_timestamps_for_stays(\n    stays_df: DataFrame, event_freq_stays: int, cartesian_crs: int, seed: int\n) -&gt; DataFrame:\n    \"\"\"\n    Generates a DataFrame of event timestamps for stays based on the event frequency for stays.\n\n    For each stay in the input DataFrame, this method calculates\n    the time difference between the initial and final timestamps of the stay.\n    It then generates a number of timestamps equal to this time difference divided by the event\n    frequency for stays. Each timestamp is associated with the location of the stay.\n\n    Args:\n        stays_df (pyspark.sql.DataFrame): A DataFrame of stays.\n        event_freq_stays (int): The frequency of events for stays.\n        cartesian_crs (int): The EPSG code of the Cartesian coordinate reference system to use for the geometries.\n        seed (int): The seed for generating timestamps randomly.\n    Returns:\n        pyspark.sql.DataFrame: A DataFrame of event timestamps for stays.\n    \"\"\"\n\n    stays_df = stays_df.withColumn(\n        \"time_diff_seconds\",\n        F.unix_timestamp(F.col(ColNames.final_timestamp)) - F.unix_timestamp(F.col(ColNames.initial_timestamp)),\n    )\n\n    stays_df = stays_df.withColumn(\n        \"timestamps_count\",\n        (F.col(\"time_diff_seconds\") / event_freq_stays).cast(\"integer\"),\n    )\n\n    expr_str = f\"transform(sequence(1, timestamps_count), x -&gt; rand({seed}))\"\n    stays_df = stays_df.withColumn(\n        \"random_fraction_between_timestamps\",\n        F.expr(expr_str),\n    )\n\n    stays_df = stays_df.withColumn(\n        \"random_fraction_between_timestamps\",\n        F.explode(F.col(\"random_fraction_between_timestamps\")),\n    )\n    stays_df = stays_df.withColumn(\n        \"offset_seconds\",\n        F.col(\"random_fraction_between_timestamps\") * F.col(\"time_diff_seconds\"),\n    )\n    stays_df = stays_df.withColumn(\n        ColNames.timestamp,\n        F.from_unixtime(F.unix_timestamp(ColNames.initial_timestamp) + F.col(\"offset_seconds\")),\n    )\n\n    stays_df = stays_df.withColumn(\n        \"generated_geometry\",\n        STF.ST_Transform(\n            STC.ST_Point(F.col(ColNames.longitude), F.col(ColNames.latitude)),\n            F.lit(\"EPSG:4326\"),\n            F.lit(f\"EPSG:{cartesian_crs}\"),\n        ),\n    )\n\n    stays_df = stays_df.select(\n        ColNames.user_id,\n        ColNames.timestamp,\n        \"generated_geometry\",\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    )\n\n    return stays_df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_location_errors","title":"<code>generate_location_errors(records_sdf, error_location_distance_max, error_location_distance_min, closest_cell_distance_max, cartesian_crs, seed)</code>  <code>staticmethod</code>","text":"<p>Generates location errors for x and y coordinates of each record in the DataFrame.</p> <p>This method adds a random location error to the x and y coordinates of each record in the input DataFrame. The location error is a random value between error_location_distance_min and error_location_distance_max, and is added or subtracted from the x and y coordinates based on a random sign.</p> <p>Parameters:</p> Name Type Description Default <code>records_sdf</code> <code>DataFrame</code> <p>A DataFrame of records</p> required <code>error_location_distance_max</code> <code>float</code> <p>The maximum location error distance.</p> required <code>error_location_distance_min</code> <code>float</code> <p>The minimum location error distance.</p> required <code>closest_cell_distance_max</code> <code>float</code> <p>The maximum distance to the closest cell.</p> required <code>cartesian_crs</code> <code>int</code> <p>The EPSG code of the Cartesian coordinate reference system to use for the geometries.</p> required <code>seed</code> <code>int</code> <p>The seed for the random number generator.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame of records with location errors added to the x and y coordinates.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>@staticmethod\ndef generate_location_errors(\n    records_sdf: DataFrame,\n    error_location_distance_max: float,\n    error_location_distance_min: float,\n    closest_cell_distance_max: float,\n    cartesian_crs: int,\n    seed: int,\n) -&gt; DataFrame:\n    \"\"\"\n    Generates location errors for x and y coordinates of each record in the DataFrame.\n\n    This method adds a random location error to the x and y coordinates of each record in the input DataFrame.\n    The location error is a random value between error_location_distance_min and error_location_distance_max,\n    and is added or subtracted from the x and y coordinates based on a random sign.\n\n    Args:\n        records_sdf (DataFrame): A DataFrame of records\n        error_location_distance_max (float): The maximum location error distance.\n        error_location_distance_min (float): The minimum location error distance.\n        closest_cell_distance_max (float): The maximum distance to the closest cell.\n        cartesian_crs (int): The EPSG code of the Cartesian coordinate reference system to use for the geometries.\n        seed (int): The seed for the random number generator.\n\n    Returns:\n        DataFrame: A DataFrame of records with location errors added to the x and y coordinates.\n    \"\"\"\n\n    errors_df = (\n        records_sdf.withColumn(\"y\", STF.ST_Y(F.col(\"generated_geometry\")))\n        .withColumn(\"x\", STF.ST_X(F.col(\"generated_geometry\")))\n        .drop(\"generated_geometry\")\n    )\n\n    errors_df = errors_df.withColumn(\n        ColNames.loc_error,\n        (F.rand(seed=seed) * (error_location_distance_max - error_location_distance_min))\n        + error_location_distance_min,\n    )\n\n    errors_df = (\n        errors_df.withColumn(\"sign\", F.when(F.rand(seed=seed) &gt; 0.5, 1).otherwise(-1))\n        .withColumn(\"new_x\", F.col(\"x\") + (F.col(ColNames.loc_error) * F.col(\"sign\")))\n        .withColumn(\"new_y\", F.col(\"y\") + (F.col(ColNames.loc_error) * F.col(\"sign\")))\n    )\n\n    errors_df = errors_df.withColumn(\n        \"generated_geometry\",\n        STF.ST_SetSRID(STC.ST_Point(F.col(\"new_x\"), F.col(\"new_y\")), cartesian_crs),\n    ).drop(\"new_x\", \"new_y\")\n\n    errors_df = errors_df.withColumn(\"closest_cell_distance_max\", F.lit(closest_cell_distance_max))\n\n    errors_df = errors_df.select(\n        ColNames.user_id,\n        ColNames.timestamp,\n        \"generated_geometry\",\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n        ColNames.loc_error,\n        \"closest_cell_distance_max\",\n    )\n\n    return errors_df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_locations_for_moves","title":"<code>generate_locations_for_moves(event_timestamps_df, cartesian_crs)</code>  <code>staticmethod</code>","text":"<p>Generates locations for moves based on the event timestamps dataframe. Returns a dataframe, where for each move in the event timestamps dataframe a geometry column is added, representing the location of the move.</p> <p>Performs interpolation along the line between the starting move point (previous stay point) and next move point (next stay point) using the randomly generated values in the column random_fraction_on_line.</p> <p>Parameters:</p> Name Type Description Default <code>event_timestamps_df</code> <code>DataFrame</code> <p>The event timestamps dataframe.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: The event timestamps dataframe with the added geometry column.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>@staticmethod\ndef generate_locations_for_moves(event_timestamps_df: DataFrame, cartesian_crs: int) -&gt; DataFrame:\n    \"\"\"\n    Generates locations for moves based on the event timestamps dataframe.\n    Returns a dataframe, where for each move in the event timestamps dataframe\n    a geometry column is added, representing the location of the move.\n\n    Performs interpolation along the line between the starting move point (previous stay point)\n    and next move point (next stay point) using the randomly generated values in the column random_fraction_on_line.\n\n    Args:\n        event_timestamps_df (pyspark.sql.DataFrame): The event timestamps dataframe.\n\n    Returns:\n        pyspark.sql.DataFrame: The event timestamps dataframe with the added geometry column.\n    \"\"\"\n\n    moves_with_geometry = event_timestamps_df.withColumn(\n        \"line\",\n        STF.ST_MakeLine(F.col(ColNames.geometry), F.col(\"next_stay_geometry\")),\n    )\n    moves_with_geometry = moves_with_geometry.withColumn(\n        \"generated_geometry\",\n        STF.ST_SetSRID(\n            STF.ST_LineInterpolatePoint(F.col(\"line\"), F.col(\"random_fraction_on_line\")),\n            cartesian_crs,\n        ),\n    )\n\n    moves_with_geometry = moves_with_geometry.select(\n        ColNames.user_id,\n        ColNames.timestamp,\n        \"generated_geometry\",\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    )\n\n    return moves_with_geometry\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_nulls_in_mandatory_fields","title":"<code>generate_nulls_in_mandatory_fields(df)</code>","text":"<p>Generates null values in some fields of some rows based on configuration parameters.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>clean synthetic data</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: synthetic records dataframe with nulls in some columns of some rows</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>def generate_nulls_in_mandatory_fields(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Generates null values in some fields of some rows based on configuration parameters.\n\n    Args:\n        df (pyspark.sql.DataFrame): clean synthetic data\n\n    Returns:\n        pyspark.sql.DataFrame: synthetic records dataframe with nulls in some columns of some rows\n    \"\"\"\n\n    # Two probability parameters from config apply:\n    # First one sets how many rows (as fraction of all rows) are selected for possible value nulling.\n    # Second one sets the likelyhood for each column to be set to null.\n    # If 1.0, then all mandatory columns of each selected row will be nulled.\n\n    if self.null_row_prob == 0.0:\n        # TODO logging\n        return df\n\n    # Split input dataframe to unchanged and changed portions\n    df = df.cache()\n    error_row_prob = self.null_row_prob\n    unchanged_row_prob = 1.0 - error_row_prob\n    unchanged_rows_df, error_rows_df = df.randomSplit([unchanged_row_prob, error_row_prob], seed=self.seed + 1)\n\n    columns_for_null_selection = list(self.error_generation_allowed_columns)\n    columns_for_null_selection.sort()\n\n    random.seed(self.seed)\n    columns_to_set_as_null = random.sample(\n        columns_for_null_selection,\n        int(round(self.column_is_null_probability * len(columns_for_null_selection), 0)),\n    )\n\n    for column in columns_to_set_as_null:\n        error_rows_df = error_rows_df.withColumn(column, F.lit(None))\n\n    error_rows_df = error_rows_df.withColumn(\"is_modified\", F.lit(True))\n\n    # Re-combine unchanged and changed rows of the dataframe.\n    return unchanged_rows_df.union(error_rows_df)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_out_of_bounds_dates","title":"<code>generate_out_of_bounds_dates(df)</code>","text":"<p>Transforms the timestamp column values to be out of bound of the selected period, based on probabilities from configuration. Only rows with non-null timestamp values can become altered here.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe of clean synthetic events.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: Data where some timestamp column values are out of bounds as per config.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>def generate_out_of_bounds_dates(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the timestamp column values to be out of bound of the selected period,\n    based on probabilities from configuration.\n    Only rows with non-null timestamp values can become altered here.\n\n    Args:\n        df (pyspark.sql.DataFrame): Dataframe of clean synthetic events.\n\n    Returns:\n        pyspark.sql.DataFrame: Data where some timestamp column values are out of bounds as per config.\n    \"\"\"\n\n    if self.out_of_bounds_prob == 0.0:\n        # TODO logging\n        return df\n\n    # Calculate approximate span in months from config parameters.\n    # The parameters should approximately cover the range of \"clean\" dates, so that the erroneous values can be generated outside the range.\n\n    # TODO\n    # This now uses the whole input data to set the bounds\n    ending_timestamp = df.agg(F.max(ColNames.timestamp)).collect()[0][0]\n    starting_timestamp = df.agg(F.min(ColNames.timestamp)).collect()[0][0]\n    events_span_in_months = max(1, (pd.Timestamp(ending_timestamp) - pd.Timestamp(starting_timestamp)).days / 30)\n\n    # Split rows by null/non-null timestamp.\n    df = df.cache()\n    null_timestamp_df = df.where(F.col(ColNames.timestamp).isNull())\n    nonnull_timestamp_df = df.where(F.col(ColNames.timestamp).isNotNull())\n    df.unpersist()\n\n    # From non-null timestamp rows, select a subset for adding errors to, depending on config parameter.\n    nonnull_timestamp_df = nonnull_timestamp_df.cache()\n    error_row_prob = self.out_of_bounds_prob\n    unchanged_row_prob = 1.0 - error_row_prob\n    unchanged_rows_df, error_rows_df = nonnull_timestamp_df.randomSplit(\n        [unchanged_row_prob, error_row_prob], seed=self.seed + 3\n    )\n    # Combine null timestamp rows and not-modified non-null timestamp rows.\n    unchanged_rows_df = unchanged_rows_df.union(null_timestamp_df)\n\n    # Add months offset to error rows to make their timestamp values become outside expected range.\n    months_to_add_col = (F.lit(2) + F.rand(seed=self.seed)) * F.lit(events_span_in_months)\n    modified_date_col = F.add_months(F.col(ColNames.timestamp), months_to_add_col)\n    time_col = F.date_format(F.col(ColNames.timestamp), \" HH:mm:ss\")\n    error_rows_df = error_rows_df.withColumn(ColNames.timestamp, F.concat(modified_date_col, time_col))\n    error_rows_df = error_rows_df.withColumn(\"is_modified\", F.lit(True))\n\n    # Combine changed and unchanged rows dataframes.\n    return unchanged_rows_df.union(error_rows_df)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_records_with_non_existant_cell_ids","title":"<code>generate_records_with_non_existant_cell_ids(records_sdf, cells_sdf, seed)</code>  <code>staticmethod</code>","text":"<p>Adds the cell_id column so that it will contain cell_ids that are not present in the cells_df dataframe, yet follow the format of a cell id.</p> <p>Parameters:</p> Name Type Description Default <code>records_sdf</code> <code>DataFrame</code> <p>generated records</p> required <code>cells_sdf</code> <code>DataFrame</code> <p>cells dataframe</p> required <code>DataFrame</code> <p>records with cell ids that are not present in the cells_df dataframe</p> required Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>@staticmethod\ndef generate_records_with_non_existant_cell_ids(records_sdf: DataFrame, cells_sdf: DataFrame, seed) -&gt; DataFrame:\n    \"\"\"\n    Adds the cell_id column so that it will contain cell_ids that\n    are not present in the cells_df dataframe, yet follow the format of a cell id.\n\n    Args:\n        records_sdf (DataFrame): generated records\n        cells_sdf (DataFrame): cells dataframe\n\n        DataFrame: records with cell ids that are not present in the cells_df dataframe\n    \"\"\"\n\n    # Generates random cell ids for cells_df, and selects those\n    # Join to records is implemented with a monotonically increasing id\n    # So to limit that, this number of all unique cells is used\n    # TODO check how to make this more optimal\n\n    n_of_actual_cell_ids_as_base = cells_sdf.count()\n\n    cells_df_with_random_cell_ids = cells_sdf[[ColNames.cell_id]].withColumn(\n        \"random_cell_id\",\n        (F.rand(seed=seed) * (999_999_999_999_999 - 10_000_000_000_000) + 10_000_000_000_000).cast(\"bigint\"),\n    )\n\n    # Do an  left anti join to ensure that now generated cell ids are not among actual cell ids\n\n    cells_df_inner_joined = cells_df_with_random_cell_ids[[\"random_cell_id\"]].join(\n        cells_sdf[[ColNames.cell_id]],\n        on=F.col(\"random_cell_id\") == F.col(ColNames.cell_id),\n        how=\"leftanti\",\n    )\n\n    cells_df_inner_joined = cells_df_inner_joined.withColumn(\"row_number\", (F.monotonically_increasing_id()))\n    records_sdf = records_sdf.withColumn(\n        \"row_number\",\n        (F.monotonically_increasing_id() % n_of_actual_cell_ids_as_base) + 1,\n    )\n\n    cells_df_inner_joined = cells_df_inner_joined.select(\n        \"row_number\", F.col(\"random_cell_id\").alias(ColNames.cell_id)\n    )\n\n    records_with_random_cell_id = records_sdf.join(cells_df_inner_joined, on=\"row_number\", how=\"left\").drop(\n        \"row_number\"\n    )\n\n    records_with_random_cell_id = records_with_random_cell_id.select(\n        ColNames.user_id,\n        ColNames.timestamp,\n        \"generated_geometry\",\n        ColNames.cell_id,\n        ColNames.loc_error,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    )\n\n    return records_with_random_cell_id\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_same_location_duplicates","title":"<code>generate_same_location_duplicates(df)</code>","text":"<p>Selects a subset of the previously generated data (syntactically clean data) and creates same location duplicates. If input has odd number of rows, one row is discarded.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe of clean synthetic events.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: Dataframe with same location duplicates.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>def generate_same_location_duplicates(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Selects a subset of the previously generated data (syntactically clean data) and creates same location duplicates.\n    If input has odd number of rows, one row is discarded.\n\n    Args:\n        df (pyspark.sql.DataFrame): Dataframe of clean synthetic events.\n\n    Returns:\n        pyspark.sql.DataFrame: Dataframe with same location duplicates.\n\n    \"\"\"\n\n    if self.same_location_duplicate_prob == 0:\n        return df\n\n    # Split dataframe by whether the row has been modified during the error-adding process already.\n    # Already errored rows do not get further changes.\n    df = df.cache()\n    previously_modified_rows_df = df.where(F.col(\"is_modified\"))\n    unmodified_rows_df = df.where(~(F.col(\"is_modified\")))\n    df.unpersist()\n\n    # From unmodified rows, select a subset for adding errors to, depending on config parameter.\n    unmodified_rows_df = unmodified_rows_df.cache()\n    error_row_prob = self.same_location_duplicate_prob\n    unchanged_row_prob = 1.0 - error_row_prob\n\n    unchanged_rows_df, error_rows_df = unmodified_rows_df.randomSplit(\n        [unchanged_row_prob, error_row_prob], seed=self.seed + 7\n    )\n\n    # Gather rows that are not modified in this step (combine previously-modified rows and not-selected unmodified rows).\n    unchanged_rows_df = unchanged_rows_df.union(previously_modified_rows_df)\n\n    # Select all even rows and duplicate these\n    error_rows_df = error_rows_df.withColumn(\n        \"user_row_num\",\n        F.row_number().over(Window.partitionBy(ColNames.user_id).orderBy(ColNames.timestamp)),\n    )\n\n    even_rows = error_rows_df.where(F.col(\"user_row_num\") % 2 == 0).drop(\"user_row_num\")\n\n    error_rows_df = even_rows.union(even_rows)\n    error_rows_df = error_rows_df.withColumn(\"is_modified\", F.lit(True))\n\n    return unchanged_rows_df.union(error_rows_df)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/","title":"synthetic_network","text":"<p>Module that generates a MNO synthetic network.</p>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.CellIDGenerator","title":"<code>CellIDGenerator</code>","text":"<p>Abstract class for cell ID generation.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>class CellIDGenerator(metaclass=ABCMeta):\n    \"\"\"\n    Abstract class for cell ID generation.\n    \"\"\"\n\n    def __init__(self, rng: Union[int, Random]) -&gt; None:\n        \"\"\"Cell ID Generator constructor\n\n        Args:\n            rng (Union[int, Random]): either an integer to act as a seed for RNG, or an instantiated Random object.\n        \"\"\"\n        if isinstance(rng, int):\n            self.rng = Random(rng)\n        elif isinstance(rng, Random):\n            self.rng = rng\n\n    @abstractmethod\n    def generate_cell_ids(self, n_cells: int) -&gt; List[str]:\n        \"\"\"Method that generates random cell IDs.\n\n        Args:\n            n_cells (int): number of cell IDs to generate.\n\n        Returns:\n            List[str]: list of cell IDs.\n        \"\"\"\n        return None\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.CellIDGenerator.__init__","title":"<code>__init__(rng)</code>","text":"<p>Cell ID Generator constructor</p> <p>Parameters:</p> Name Type Description Default <code>rng</code> <code>Union[int, Random]</code> <p>either an integer to act as a seed for RNG, or an instantiated Random object.</p> required Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>def __init__(self, rng: Union[int, Random]) -&gt; None:\n    \"\"\"Cell ID Generator constructor\n\n    Args:\n        rng (Union[int, Random]): either an integer to act as a seed for RNG, or an instantiated Random object.\n    \"\"\"\n    if isinstance(rng, int):\n        self.rng = Random(rng)\n    elif isinstance(rng, Random):\n        self.rng = rng\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.CellIDGenerator.generate_cell_ids","title":"<code>generate_cell_ids(n_cells)</code>  <code>abstractmethod</code>","text":"<p>Method that generates random cell IDs.</p> <p>Parameters:</p> Name Type Description Default <code>n_cells</code> <code>int</code> <p>number of cell IDs to generate.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: list of cell IDs.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>@abstractmethod\ndef generate_cell_ids(self, n_cells: int) -&gt; List[str]:\n    \"\"\"Method that generates random cell IDs.\n\n    Args:\n        n_cells (int): number of cell IDs to generate.\n\n    Returns:\n        List[str]: list of cell IDs.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.CellIDGeneratorBuilder","title":"<code>CellIDGeneratorBuilder</code>","text":"<p>Type/method of cell ID generation enumeration class.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>class CellIDGeneratorBuilder:\n    \"\"\"\n    Type/method of cell ID generation enumeration class.\n    \"\"\"\n\n    RANDOM_CELL_ID = \"random_cell_id\"\n\n    CONSTRUCTORS = {RANDOM_CELL_ID: RandomCellIDGenerator}\n\n    @staticmethod\n    def build(constructor_key: str, rng: Union[int, Random]) -&gt; CellIDGenerator:\n        \"\"\"\n        Method that builds a CellIDGenerator.\n\n        Args:\n            constructor_key (str): Key of the constructor\n            rng (Union[int, Random]): either an integer to act as a seed for RNG, or an instantiated Random object.\n\n        Raises:\n            ValueError: If the given constructor_key is not supported\n\n        Returns:\n            CellIDGenerator: Class that generates random cell_id's\n        \"\"\"\n        try:\n            constructor = CellIDGeneratorBuilder.CONSTRUCTORS[constructor_key]\n        except KeyError as e:\n            raise ValueError(f\"Random cell ID generator: {constructor_key} is not supported\") from e\n\n        return constructor(rng)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.CellIDGeneratorBuilder.build","title":"<code>build(constructor_key, rng)</code>  <code>staticmethod</code>","text":"<p>Method that builds a CellIDGenerator.</p> <p>Parameters:</p> Name Type Description Default <code>constructor_key</code> <code>str</code> <p>Key of the constructor</p> required <code>rng</code> <code>Union[int, Random]</code> <p>either an integer to act as a seed for RNG, or an instantiated Random object.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the given constructor_key is not supported</p> <p>Returns:</p> Name Type Description <code>CellIDGenerator</code> <code>CellIDGenerator</code> <p>Class that generates random cell_id's</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>@staticmethod\ndef build(constructor_key: str, rng: Union[int, Random]) -&gt; CellIDGenerator:\n    \"\"\"\n    Method that builds a CellIDGenerator.\n\n    Args:\n        constructor_key (str): Key of the constructor\n        rng (Union[int, Random]): either an integer to act as a seed for RNG, or an instantiated Random object.\n\n    Raises:\n        ValueError: If the given constructor_key is not supported\n\n    Returns:\n        CellIDGenerator: Class that generates random cell_id's\n    \"\"\"\n    try:\n        constructor = CellIDGeneratorBuilder.CONSTRUCTORS[constructor_key]\n    except KeyError as e:\n        raise ValueError(f\"Random cell ID generator: {constructor_key} is not supported\") from e\n\n    return constructor(rng)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.RandomCellIDGenerator","title":"<code>RandomCellIDGenerator</code>","text":"<p>               Bases: <code>CellIDGenerator</code></p> <p>Class that generates completely random cell IDs. Inherits from the AbstractCellIDGenerator class.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>class RandomCellIDGenerator(CellIDGenerator):\n    \"\"\"\n    Class that generates completely random cell IDs. Inherits from the AbstractCellIDGenerator class.\n    \"\"\"\n\n    def generate_cell_ids(self, n_cells: int) -&gt; List[str]:\n        \"\"\"Generate UNIQUE random cell IDs with no logic behind it, i.e. not following CGI/eCGI standards.\n        The resuling cell IDs are 14- or 15-digit strings.\n\n        Args:\n            n_cells (int): number of cell IDs to generate.\n\n        Returns:\n            List[str]: list of cell IDs.\n        \"\"\"\n        return list(map(str, self.rng.sample(range(10_000_000_000_000, 999_999_999_999_999), n_cells)))\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.RandomCellIDGenerator.generate_cell_ids","title":"<code>generate_cell_ids(n_cells)</code>","text":"<p>Generate UNIQUE random cell IDs with no logic behind it, i.e. not following CGI/eCGI standards. The resuling cell IDs are 14- or 15-digit strings.</p> <p>Parameters:</p> Name Type Description Default <code>n_cells</code> <code>int</code> <p>number of cell IDs to generate.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: list of cell IDs.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>def generate_cell_ids(self, n_cells: int) -&gt; List[str]:\n    \"\"\"Generate UNIQUE random cell IDs with no logic behind it, i.e. not following CGI/eCGI standards.\n    The resuling cell IDs are 14- or 15-digit strings.\n\n    Args:\n        n_cells (int): number of cell IDs to generate.\n\n    Returns:\n        List[str]: list of cell IDs.\n    \"\"\"\n    return list(map(str, self.rng.sample(range(10_000_000_000_000, 999_999_999_999_999), n_cells)))\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.SyntheticNetwork","title":"<code>SyntheticNetwork</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class that generates the synthetic network topology data. It inherits from the Component abstract class.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>class SyntheticNetwork(Component):\n    \"\"\"\n    Class that generates the synthetic network topology data. It inherits from the Component abstract class.\n    \"\"\"\n\n    COMPONENT_ID = \"SyntheticNetwork\"\n\n    def __init__(self, general_config_path: str, component_config_path: str):\n        super().__init__(general_config_path=general_config_path, component_config_path=component_config_path)\n        self.seed = self.config.getint(self.COMPONENT_ID, \"seed\")\n        self.rng = Random(self.seed)\n        self.n_cells = self.config.getint(self.COMPONENT_ID, \"n_cells\")\n        self.cell_type_options = (\n            self.config.get(self.COMPONENT_ID, \"cell_type_options\").strip().replace(\" \", \"\").split(\",\")\n        )\n        self.tech = [\"5G\", \"LTE\", \"UMTS\", \"GSM\"]\n        self.latitude_min = self.config.getfloat(self.COMPONENT_ID, \"latitude_min\")\n        self.latitude_max = self.config.getfloat(self.COMPONENT_ID, \"latitude_max\")\n        self.longitude_min = self.config.getfloat(self.COMPONENT_ID, \"longitude_min\")\n        self.longitude_max = self.config.getfloat(self.COMPONENT_ID, \"longitude_max\")\n        self.altitude_min = self.config.getfloat(self.COMPONENT_ID, \"altitude_min\")\n        self.altitude_max = self.config.getfloat(self.COMPONENT_ID, \"altitude_max\")\n        self.antenna_height_max = self.config.getfloat(self.COMPONENT_ID, \"antenna_height_max\")\n        self.power_min = self.config.getfloat(self.COMPONENT_ID, \"power_min\")\n        self.power_max = self.config.getfloat(self.COMPONENT_ID, \"power_max\")\n        self.range_min = self.config.getfloat(self.COMPONENT_ID, \"range_min\")\n        self.range_max = self.config.getfloat(self.COMPONENT_ID, \"range_max\")\n        self.frequency_min = self.config.getfloat(self.COMPONENT_ID, \"frequency_min\")\n        self.frequency_max = self.config.getfloat(self.COMPONENT_ID, \"frequency_max\")\n\n        self.timestamp_format = self.config.get(self.COMPONENT_ID, \"timestamp_format\")\n        self.earliest_valid_date_start = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"earliest_valid_date_start\"), self.timestamp_format\n        )\n        self.latest_valid_date_end = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"latest_valid_date_end\"), self.timestamp_format\n        )\n\n        self.date_format = self.config.get(self.COMPONENT_ID, \"date_format\")\n\n        self.starting_date = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"starting_date\"), self.date_format\n        ).date()\n        self.ending_date = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"ending_date\"), self.date_format\n        ).date()\n\n        self.date_range = [\n            (self.starting_date + datetime.timedelta(days=dd))\n            for dd in range((self.ending_date - self.starting_date).days + 1)\n        ]\n\n        # Cell generation object\n        cell_id_generation_type = self.config.get(self.COMPONENT_ID, \"cell_id_generation_type\")\n\n        self.cell_id_generator = CellIDGeneratorBuilder.build(cell_id_generation_type, self.seed)\n\n        self.no_optional_fields_probability = self.config.getfloat(self.COMPONENT_ID, \"no_optional_fields_probability\")\n        self.mandatory_null_probability = self.config.getfloat(self.COMPONENT_ID, \"mandatory_null_probability\")\n        self.out_of_bounds_values_probability = self.config.getfloat(\n            self.COMPONENT_ID, \"out_of_bounds_values_probability\"\n        )\n        self.erroneous_values_probability = self.config.getfloat(self.COMPONENT_ID, \"erroneous_values_probability\")\n\n    def initalize_data_objects(self):\n        output_network_data_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"network_data_bronze\")\n        bronze_network = BronzeNetworkDataObject(self.spark, output_network_data_path)\n        self.output_data_objects = {bronze_network.ID: bronze_network}\n\n    def read(self):\n        pass  # No input datasets are used in this component\n\n    def transform(self):\n        spark = self.spark\n\n        # Create Spark DataFrame with all valid cells and all optional fields\n\n        cells_df = spark.createDataFrame(self.clean_cells_generator(), schema=BronzeNetworkDataObject.SCHEMA)\n\n        # With certain probability, set ALL optional fields of a row to null.\n        # Fixing F.rand(seed=self.seed) will generate the same random column for every column, so\n        # it could be optimized to be generated only once\n        # If random optional fields should be set to zero (not all at the same time), use seed = self.seed + i(col_name)\n\n        for col_name in BronzeNetworkDataObject.OPTIONAL_COLUMNS:\n            cells_df = cells_df.withColumn(\n                col_name,\n                F.when(F.rand(seed=self.seed) &lt; self.no_optional_fields_probability, None).otherwise(F.col(col_name)),\n            )\n\n        cells_df = self.generate_errors(cells_df)\n\n        self.output_data_objects[BronzeNetworkDataObject.ID].df = cells_df\n\n    def clean_cells_generator(self):\n        \"\"\"Method that generates valid and fully complete (mandatory and optional) cell physical attibutes.\n\n        An underlying set of cells are created, covering the config-specified date interval.\n        Then, for each cell and date,\n        the valid_date_start and valid_date_end fields, marking the time interval in which the cell is operational, are\n        comparted with the date:\n            a) If  date &lt; valid_date_start, the cell-date row will not appear.\n            b) If valid_date_start &lt;= date &lt; valid_date_end, the cell-date row will appear and\n                the valid_date_end will be null, as the cell was currently operational\n            c) If valid_date_end &lt;= date, the cell-date row will appear and\n                the valid_date_end will NOT be null, marking the past, now known, time interval of operation.\n\n        Yields:\n            (\n                cell_id (str),\n                latitude (float),\n                longitude (float),\n                altitudes (float),\n                antenna_height (float),\n                directionality (int),\n                azimuth_angle (float | None),\n                elevation_angle (float),\n                hor_beam_width (float),\n                ver_beam_width (float),\n                power (float),\n                range (float),\n                frequency (int),\n                technology (str),\n                valid_date_start (str),\n                valid_date_end (str | None)\n                cell_type (str),\n                year (int),\n                month (int),\n                day (int)\n            )\n        \"\"\"\n        # MANDATORY FIELDS\n        cell_ids = self.cell_id_generator.generate_cell_ids(self.n_cells)\n\n        latitudes = [self.rng.uniform(self.latitude_min, self.latitude_max) for _ in range(self.n_cells)]\n        longitudes = [self.rng.uniform(self.longitude_min, self.longitude_max) for _ in range(self.n_cells)]\n\n        # OPTIONAL FIELDS\n        altitudes = [self.rng.uniform(self.altitude_min, self.altitude_max) for _ in range(self.n_cells)]\n\n        # antenna height always positive\n        antenna_heights = [self.rng.uniform(0, self.antenna_height_max) for _ in range(self.n_cells)]\n\n        # Directionality: 0 or 1\n        directionalities = [self.rng.choice([0, 1]) for _ in range(self.n_cells)]\n\n        def random_azimuth_angle(directionality):\n            if directionality == 0:\n                return None\n            else:\n                return self.rng.uniform(0, 360)\n\n        # Azimuth angle: None if directionality is 0, float in [0, 360] if directionality is 1\n        azimuth_angles = [random_azimuth_angle(direc) for direc in directionalities]\n\n        # Eleveation angle: in [-90, 90]\n        elevation_angles = [self.rng.uniform(-90, 90) for _ in range(self.n_cells)]\n\n        # Horizontal/Vertical beam width: float in [0, 360]\n        hor_beam_widths = [self.rng.uniform(0, 360) for _ in range(self.n_cells)]\n        ver_beam_widths = [self.rng.uniform(0, 360) for _ in range(self.n_cells)]\n\n        # Power, float in specified range (unit: watts, W)\n        powers = [self.rng.uniform(self.power_min, self.power_max) for _ in range(self.n_cells)]\n\n        # Range, float in specified range (unit: metres, m)\n        ranges = [self.rng.uniform(self.range_min, self.range_max) for _ in range(self.n_cells)]\n\n        # Frequency: int in specifed range (unit: MHz)\n        frequencies = [self.rng.randint(self.frequency_min, self.frequency_max) for _ in range(self.n_cells)]\n\n        # Technology: str\n        technologies = [self.rng.choice(self.tech) for _ in range(self.n_cells)]\n\n        # # Valid start date, should be in the timestamp interval provided via config file\n        # span_seconds = (self.latest_valid_date_end - self.earliest_valid_date_start).total_seconds()\n\n        # # Start date will be some random nb of seconds after the earliest valid date start\n        # valid_date_start_dts = [\n        #     self.earliest_valid_date_start + datetime.timedelta(seconds=self.rng.uniform(0, span_seconds))\n        #     for _ in range(self.n_cells)\n        # ]\n\n        # # Remaining seconds from the valid date starts to the ending date\n        # remaining_seconds = [\n        #     (self.latest_valid_date_end - start_dt).total_seconds() for start_dt in valid_date_start_dts\n        # ]\n        # # Choose valid date ends ALWAYS after the valid date start\n        # # Minimum of two seconds, as valid date end is excluded from the time window,\n        # # so cell will be valid for at least 1 second\n        # valid_date_end_dts = [\n        #     (start_dt + datetime.timedelta(seconds=self.rng.uniform(2, rem_secs)))\n        #     for (start_dt, rem_secs) in zip(valid_date_start_dts, remaining_seconds)\n        # ]\n\n        valid_date_start_dts = [self.earliest_valid_date_start for _ in range(self.n_cells)]\n        valid_date_end_dts = [self.latest_valid_date_end for _ in range(self.n_cells)]\n\n        def check_operational(_curr_date: datetime.date, _end_datetime: datetime.datetime):\n            if _curr_date &lt; _end_datetime.date():  # still operational, return None\n                return None\n            else:\n                return _end_datetime.strftime(self.timestamp_format)\n\n        # Cell type: str in option list\n        cell_types = [self.rng.choice(self.cell_type_options) for _ in range(self.n_cells)]\n\n        for date in self.date_range:\n            for i in range(self.n_cells):\n                # Assume we do not have info about future cells\n                if valid_date_start_dts[i].date() &lt;= date:\n                    yield (\n                        cell_ids[i],\n                        latitudes[i],\n                        longitudes[i],\n                        altitudes[i],\n                        antenna_heights[i],\n                        directionalities[i],\n                        azimuth_angles[i],\n                        elevation_angles[i],\n                        hor_beam_widths[i],\n                        ver_beam_widths[i],\n                        powers[i],\n                        ranges[i],\n                        frequencies[i],\n                        technologies[i],\n                        valid_date_start_dts[i].strftime(self.timestamp_format),\n                        check_operational(date, valid_date_end_dts[i]),  # null if still operational today\n                        cell_types[i],\n                        date.year,\n                        date.month,\n                        date.day,\n                    )\n\n    def generate_errors(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Function handling the generation of out-of-bounds, null and erroneous values in different columns\n        according to the config-specified probabilities\n\n        Args:\n            df (DataFrame): clean DataFrame\n\n        Returns:\n            DataFrame: DataFrame after the generation of different invalid or null values\n        \"\"\"\n\n        if self.out_of_bounds_values_probability &gt; 0:\n            df = self.generate_out_of_bounds_values(df)\n\n        if self.mandatory_null_probability &gt; 0:\n            df = self.generate_nulls_in_mandatory_columns(df)\n\n        if self.erroneous_values_probability &gt; 0:\n            df = self.generate_erroneous_values(df)\n\n        return df\n\n    def generate_nulls_in_mandatory_columns(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Generates null values in the mandatory fields based on probabilities form config\n\n        Args:\n            df (DataFrame): synthetic dataframe\n\n        Returns:\n            DataFrame: synthetic dataframe with nulls in some mandatory fields\n        \"\"\"\n        # Use different seed for each column\n        for i, col_name in enumerate(BronzeNetworkDataObject.MANDATORY_COLUMNS):\n            df = df.withColumn(\n                col_name,\n                F.when(F.rand(seed=self.seed + i + 1) &lt; self.mandatory_null_probability, None).otherwise(\n                    F.col(col_name)\n                ),\n            )\n\n        return df\n\n    def generate_out_of_bounds_values(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Function that generates out-of-bounds values for the appropriate columns of the data object\n\n        Args:\n            df (DataFrame): cell dataframe with in-bound values\n\n        Returns:\n            DataFrame: cell dataframe with some out-of-bounds values\n        \"\"\"\n        # (2 * F.round(F.rand(seed=self.seed-1)) - 1) -&gt; random column of 1s and -1s\n        df = df.withColumn(\n            ColNames.latitude,\n            F.when(\n                F.rand(seed=self.seed - 1) &lt; self.out_of_bounds_values_probability,\n                F.col(ColNames.latitude) + (2 * F.round(F.rand(seed=self.seed - 1)) - 1) * 90,\n            )\n            .otherwise(F.col(ColNames.latitude))\n            .cast(FloatType()),  # the operation turns it into DoubleType, so cast it to Float\n        )\n\n        df = df.withColumn(\n            ColNames.longitude,\n            F.when(\n                F.rand(seed=self.seed - 2) &lt; self.out_of_bounds_values_probability,\n                F.col(ColNames.longitude) + (2 * F.round(F.rand(seed=self.seed - 2)) - 1) * 180,\n            )\n            .otherwise(F.col(ColNames.longitude))\n            .cast(FloatType()),\n        )\n\n        # antenna height, non positive\n        df = df.withColumn(\n            ColNames.antenna_height,\n            F.when(F.rand(seed=self.seed - 3) &lt; self.out_of_bounds_values_probability, -F.col(ColNames.antenna_height))\n            .otherwise(F.col(ColNames.antenna_height))\n            .cast(FloatType()),\n        )\n\n        # directionality: int different from 0 or 1. Just add a static 5 to the value\n        df = df.withColumn(\n            ColNames.directionality,\n            F.when(\n                F.rand(seed=self.seed - 4) &lt; self.out_of_bounds_values_probability, F.col(ColNames.directionality) + 5\n            )\n            .otherwise(F.col(ColNames.directionality))\n            .cast(IntegerType()),\n        )\n\n        # azimuth angle: outside of [0, 360]\n        df = df.withColumn(\n            ColNames.azimuth_angle,\n            F.when(\n                F.rand(seed=self.seed - 5) &lt; self.out_of_bounds_values_probability,\n                F.col(ColNames.azimuth_angle) + (2 * F.round(F.rand(seed=self.seed - 5)) - 1) * 360,\n            )\n            .otherwise(F.col(ColNames.azimuth_angle))\n            .cast(FloatType()),\n        )\n\n        # elevation_angle: outside of [-90, 90]\n        df = df.withColumn(\n            ColNames.elevation_angle,\n            F.when(\n                F.rand(seed=self.seed - 6) &lt; self.out_of_bounds_values_probability,\n                F.col(ColNames.elevation_angle) + (2 * F.round(F.rand(seed=self.seed - 6)) - 1) * 180,\n            )\n            .otherwise(F.col(ColNames.elevation_angle))\n            .cast(FloatType()),\n        )\n\n        # horizontal_beam_width: outside of [0, 360]\n        df = df.withColumn(\n            ColNames.horizontal_beam_width,\n            F.when(\n                F.rand(seed=self.seed - 7) &lt; self.out_of_bounds_values_probability,\n                F.col(ColNames.horizontal_beam_width) + (2 * F.round(F.rand(seed=self.seed - 7)) - 1) * 360,\n            )\n            .otherwise(F.col(ColNames.horizontal_beam_width))\n            .cast(FloatType()),\n        )\n\n        # vertical_beam_width: outside of [0, 360]\n        df = df.withColumn(\n            ColNames.vertical_beam_width,\n            F.when(\n                F.rand(seed=self.seed - 8) &lt; self.out_of_bounds_values_probability,\n                F.col(ColNames.vertical_beam_width) + (2 * F.round(F.rand(seed=self.seed - 8)) - 1) * 360,\n            )\n            .otherwise(F.col(ColNames.vertical_beam_width))\n            .cast(FloatType()),\n        )\n\n        # power: non positive value\n        df = df.withColumn(\n            ColNames.power,\n            F.when(F.rand(seed=self.seed - 9) &lt; self.out_of_bounds_values_probability, -F.col(ColNames.power))\n            .otherwise(F.col(ColNames.power))\n            .cast(FloatType()),\n        )\n\n        # range: non positive value\n        df = df.withColumn(\n            ColNames.range,\n            F.when(F.rand(seed=self.seed - 10) &lt; self.out_of_bounds_values_probability, -F.col(ColNames.range))\n            .otherwise(F.col(ColNames.range))\n            .cast(FloatType()),\n        )\n\n        # frequency: non positive vallue\n        df = df.withColumn(\n            ColNames.frequency,\n            F.when(F.rand(seed=self.seed - 11) &lt; self.out_of_bounds_values_probability, -F.col(ColNames.frequency))\n            .otherwise(F.col(ColNames.frequency))\n            .cast(IntegerType()),\n        )\n        return df\n\n    def generate_erroneous_values(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Function that generates erroneous values in the cell_id, valid_date_start and valid_date_end columns\n\n        Args:\n            df (DataFrame): DataFrame before the generation of erroneous values\n\n        Returns:\n            DataFrame: DataFrame with erroneous values\n        \"\"\"\n        # Erroneous cells: for now, a string not of 14 or 15 digits\n        df = df.withColumn(\n            ColNames.cell_id,\n            F.when(\n                F.rand(seed=self.seed * 1000) &lt; self.erroneous_values_probability,\n                (\n                    F.when(\n                        F.rand(seed=self.seed * 2000) &gt; 0.5,\n                        F.concat(\n                            F.col(ColNames.cell_id), F.substring(F.col(ColNames.cell_id), 3, 3)\n                        ),  # 17 or 18 digits\n                    ).otherwise(\n                        F.substring(F.col(ColNames.cell_id), 3, 10)  # 10 digits\n                    )\n                ),\n            )\n            .otherwise(F.col(ColNames.cell_id))\n            .cast(LongType())\n            .cast(StringType()),\n        )\n\n        # Dates\n        df_as_is, df_swap, df_wrong = df.randomSplit(\n            weights=[\n                1 - self.erroneous_values_probability,\n                self.erroneous_values_probability / 2,\n                self.erroneous_values_probability / 2,\n            ],\n            seed=self.seed,\n        )\n        # For some columns, swap valid_date_start and valid_date_end\n        df_swap = df_swap.withColumns(\n            {ColNames.valid_date_start: ColNames.valid_date_end, ColNames.valid_date_end: ColNames.valid_date_start}\n        )\n\n        chars_to_change = [\"0\", \"T\", \":\", \"/\", \"2\"]\n        changed_chars = [\"0\", \"T\", \":\", \"/\", \"2\"]\n        # Now, for dates as well, make the timestamp format incorrect\n\n        self.rng.shuffle(changed_chars)\n        df_wrong = df_wrong.withColumn(\n            ColNames.valid_date_start,\n            F.translate(F.col(ColNames.valid_date_start), \"\".join(chars_to_change), \"\".join(changed_chars)),\n        )\n\n        self.rng.shuffle(changed_chars)\n        df_wrong = df_wrong.withColumn(\n            ColNames.valid_date_end,\n            F.translate(F.col(ColNames.valid_date_end), \"\".join(chars_to_change), \"\".join(changed_chars)),\n        )\n\n        df = df_as_is.union(df_swap).union(df_wrong)\n\n        return df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.SyntheticNetwork.clean_cells_generator","title":"<code>clean_cells_generator()</code>","text":"<p>Method that generates valid and fully complete (mandatory and optional) cell physical attibutes.</p> <p>An underlying set of cells are created, covering the config-specified date interval. Then, for each cell and date, the valid_date_start and valid_date_end fields, marking the time interval in which the cell is operational, are comparted with the date:     a) If  date &lt; valid_date_start, the cell-date row will not appear.     b) If valid_date_start &lt;= date &lt; valid_date_end, the cell-date row will appear and         the valid_date_end will be null, as the cell was currently operational     c) If valid_date_end &lt;= date, the cell-date row will appear and         the valid_date_end will NOT be null, marking the past, now known, time interval of operation.</p> <p>Yields:</p> Type Description <p>( cell_id (str), latitude (float), longitude (float), altitudes (float), antenna_height (float), directionality (int), azimuth_angle (float | None), elevation_angle (float), hor_beam_width (float), ver_beam_width (float), power (float), range (float), frequency (int), technology (str), valid_date_start (str), valid_date_end (str | None) cell_type (str), year (int), month (int), day (int)</p> <p>)</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>def clean_cells_generator(self):\n    \"\"\"Method that generates valid and fully complete (mandatory and optional) cell physical attibutes.\n\n    An underlying set of cells are created, covering the config-specified date interval.\n    Then, for each cell and date,\n    the valid_date_start and valid_date_end fields, marking the time interval in which the cell is operational, are\n    comparted with the date:\n        a) If  date &lt; valid_date_start, the cell-date row will not appear.\n        b) If valid_date_start &lt;= date &lt; valid_date_end, the cell-date row will appear and\n            the valid_date_end will be null, as the cell was currently operational\n        c) If valid_date_end &lt;= date, the cell-date row will appear and\n            the valid_date_end will NOT be null, marking the past, now known, time interval of operation.\n\n    Yields:\n        (\n            cell_id (str),\n            latitude (float),\n            longitude (float),\n            altitudes (float),\n            antenna_height (float),\n            directionality (int),\n            azimuth_angle (float | None),\n            elevation_angle (float),\n            hor_beam_width (float),\n            ver_beam_width (float),\n            power (float),\n            range (float),\n            frequency (int),\n            technology (str),\n            valid_date_start (str),\n            valid_date_end (str | None)\n            cell_type (str),\n            year (int),\n            month (int),\n            day (int)\n        )\n    \"\"\"\n    # MANDATORY FIELDS\n    cell_ids = self.cell_id_generator.generate_cell_ids(self.n_cells)\n\n    latitudes = [self.rng.uniform(self.latitude_min, self.latitude_max) for _ in range(self.n_cells)]\n    longitudes = [self.rng.uniform(self.longitude_min, self.longitude_max) for _ in range(self.n_cells)]\n\n    # OPTIONAL FIELDS\n    altitudes = [self.rng.uniform(self.altitude_min, self.altitude_max) for _ in range(self.n_cells)]\n\n    # antenna height always positive\n    antenna_heights = [self.rng.uniform(0, self.antenna_height_max) for _ in range(self.n_cells)]\n\n    # Directionality: 0 or 1\n    directionalities = [self.rng.choice([0, 1]) for _ in range(self.n_cells)]\n\n    def random_azimuth_angle(directionality):\n        if directionality == 0:\n            return None\n        else:\n            return self.rng.uniform(0, 360)\n\n    # Azimuth angle: None if directionality is 0, float in [0, 360] if directionality is 1\n    azimuth_angles = [random_azimuth_angle(direc) for direc in directionalities]\n\n    # Eleveation angle: in [-90, 90]\n    elevation_angles = [self.rng.uniform(-90, 90) for _ in range(self.n_cells)]\n\n    # Horizontal/Vertical beam width: float in [0, 360]\n    hor_beam_widths = [self.rng.uniform(0, 360) for _ in range(self.n_cells)]\n    ver_beam_widths = [self.rng.uniform(0, 360) for _ in range(self.n_cells)]\n\n    # Power, float in specified range (unit: watts, W)\n    powers = [self.rng.uniform(self.power_min, self.power_max) for _ in range(self.n_cells)]\n\n    # Range, float in specified range (unit: metres, m)\n    ranges = [self.rng.uniform(self.range_min, self.range_max) for _ in range(self.n_cells)]\n\n    # Frequency: int in specifed range (unit: MHz)\n    frequencies = [self.rng.randint(self.frequency_min, self.frequency_max) for _ in range(self.n_cells)]\n\n    # Technology: str\n    technologies = [self.rng.choice(self.tech) for _ in range(self.n_cells)]\n\n    # # Valid start date, should be in the timestamp interval provided via config file\n    # span_seconds = (self.latest_valid_date_end - self.earliest_valid_date_start).total_seconds()\n\n    # # Start date will be some random nb of seconds after the earliest valid date start\n    # valid_date_start_dts = [\n    #     self.earliest_valid_date_start + datetime.timedelta(seconds=self.rng.uniform(0, span_seconds))\n    #     for _ in range(self.n_cells)\n    # ]\n\n    # # Remaining seconds from the valid date starts to the ending date\n    # remaining_seconds = [\n    #     (self.latest_valid_date_end - start_dt).total_seconds() for start_dt in valid_date_start_dts\n    # ]\n    # # Choose valid date ends ALWAYS after the valid date start\n    # # Minimum of two seconds, as valid date end is excluded from the time window,\n    # # so cell will be valid for at least 1 second\n    # valid_date_end_dts = [\n    #     (start_dt + datetime.timedelta(seconds=self.rng.uniform(2, rem_secs)))\n    #     for (start_dt, rem_secs) in zip(valid_date_start_dts, remaining_seconds)\n    # ]\n\n    valid_date_start_dts = [self.earliest_valid_date_start for _ in range(self.n_cells)]\n    valid_date_end_dts = [self.latest_valid_date_end for _ in range(self.n_cells)]\n\n    def check_operational(_curr_date: datetime.date, _end_datetime: datetime.datetime):\n        if _curr_date &lt; _end_datetime.date():  # still operational, return None\n            return None\n        else:\n            return _end_datetime.strftime(self.timestamp_format)\n\n    # Cell type: str in option list\n    cell_types = [self.rng.choice(self.cell_type_options) for _ in range(self.n_cells)]\n\n    for date in self.date_range:\n        for i in range(self.n_cells):\n            # Assume we do not have info about future cells\n            if valid_date_start_dts[i].date() &lt;= date:\n                yield (\n                    cell_ids[i],\n                    latitudes[i],\n                    longitudes[i],\n                    altitudes[i],\n                    antenna_heights[i],\n                    directionalities[i],\n                    azimuth_angles[i],\n                    elevation_angles[i],\n                    hor_beam_widths[i],\n                    ver_beam_widths[i],\n                    powers[i],\n                    ranges[i],\n                    frequencies[i],\n                    technologies[i],\n                    valid_date_start_dts[i].strftime(self.timestamp_format),\n                    check_operational(date, valid_date_end_dts[i]),  # null if still operational today\n                    cell_types[i],\n                    date.year,\n                    date.month,\n                    date.day,\n                )\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.SyntheticNetwork.generate_erroneous_values","title":"<code>generate_erroneous_values(df)</code>","text":"<p>Function that generates erroneous values in the cell_id, valid_date_start and valid_date_end columns</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame before the generation of erroneous values</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with erroneous values</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>def generate_erroneous_values(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Function that generates erroneous values in the cell_id, valid_date_start and valid_date_end columns\n\n    Args:\n        df (DataFrame): DataFrame before the generation of erroneous values\n\n    Returns:\n        DataFrame: DataFrame with erroneous values\n    \"\"\"\n    # Erroneous cells: for now, a string not of 14 or 15 digits\n    df = df.withColumn(\n        ColNames.cell_id,\n        F.when(\n            F.rand(seed=self.seed * 1000) &lt; self.erroneous_values_probability,\n            (\n                F.when(\n                    F.rand(seed=self.seed * 2000) &gt; 0.5,\n                    F.concat(\n                        F.col(ColNames.cell_id), F.substring(F.col(ColNames.cell_id), 3, 3)\n                    ),  # 17 or 18 digits\n                ).otherwise(\n                    F.substring(F.col(ColNames.cell_id), 3, 10)  # 10 digits\n                )\n            ),\n        )\n        .otherwise(F.col(ColNames.cell_id))\n        .cast(LongType())\n        .cast(StringType()),\n    )\n\n    # Dates\n    df_as_is, df_swap, df_wrong = df.randomSplit(\n        weights=[\n            1 - self.erroneous_values_probability,\n            self.erroneous_values_probability / 2,\n            self.erroneous_values_probability / 2,\n        ],\n        seed=self.seed,\n    )\n    # For some columns, swap valid_date_start and valid_date_end\n    df_swap = df_swap.withColumns(\n        {ColNames.valid_date_start: ColNames.valid_date_end, ColNames.valid_date_end: ColNames.valid_date_start}\n    )\n\n    chars_to_change = [\"0\", \"T\", \":\", \"/\", \"2\"]\n    changed_chars = [\"0\", \"T\", \":\", \"/\", \"2\"]\n    # Now, for dates as well, make the timestamp format incorrect\n\n    self.rng.shuffle(changed_chars)\n    df_wrong = df_wrong.withColumn(\n        ColNames.valid_date_start,\n        F.translate(F.col(ColNames.valid_date_start), \"\".join(chars_to_change), \"\".join(changed_chars)),\n    )\n\n    self.rng.shuffle(changed_chars)\n    df_wrong = df_wrong.withColumn(\n        ColNames.valid_date_end,\n        F.translate(F.col(ColNames.valid_date_end), \"\".join(chars_to_change), \"\".join(changed_chars)),\n    )\n\n    df = df_as_is.union(df_swap).union(df_wrong)\n\n    return df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.SyntheticNetwork.generate_errors","title":"<code>generate_errors(df)</code>","text":"<p>Function handling the generation of out-of-bounds, null and erroneous values in different columns according to the config-specified probabilities</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>clean DataFrame</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame after the generation of different invalid or null values</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>def generate_errors(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Function handling the generation of out-of-bounds, null and erroneous values in different columns\n    according to the config-specified probabilities\n\n    Args:\n        df (DataFrame): clean DataFrame\n\n    Returns:\n        DataFrame: DataFrame after the generation of different invalid or null values\n    \"\"\"\n\n    if self.out_of_bounds_values_probability &gt; 0:\n        df = self.generate_out_of_bounds_values(df)\n\n    if self.mandatory_null_probability &gt; 0:\n        df = self.generate_nulls_in_mandatory_columns(df)\n\n    if self.erroneous_values_probability &gt; 0:\n        df = self.generate_erroneous_values(df)\n\n    return df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.SyntheticNetwork.generate_nulls_in_mandatory_columns","title":"<code>generate_nulls_in_mandatory_columns(df)</code>","text":"<p>Generates null values in the mandatory fields based on probabilities form config</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>synthetic dataframe</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>synthetic dataframe with nulls in some mandatory fields</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>def generate_nulls_in_mandatory_columns(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Generates null values in the mandatory fields based on probabilities form config\n\n    Args:\n        df (DataFrame): synthetic dataframe\n\n    Returns:\n        DataFrame: synthetic dataframe with nulls in some mandatory fields\n    \"\"\"\n    # Use different seed for each column\n    for i, col_name in enumerate(BronzeNetworkDataObject.MANDATORY_COLUMNS):\n        df = df.withColumn(\n            col_name,\n            F.when(F.rand(seed=self.seed + i + 1) &lt; self.mandatory_null_probability, None).otherwise(\n                F.col(col_name)\n            ),\n        )\n\n    return df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.SyntheticNetwork.generate_out_of_bounds_values","title":"<code>generate_out_of_bounds_values(df)</code>","text":"<p>Function that generates out-of-bounds values for the appropriate columns of the data object</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>cell dataframe with in-bound values</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>cell dataframe with some out-of-bounds values</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>def generate_out_of_bounds_values(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Function that generates out-of-bounds values for the appropriate columns of the data object\n\n    Args:\n        df (DataFrame): cell dataframe with in-bound values\n\n    Returns:\n        DataFrame: cell dataframe with some out-of-bounds values\n    \"\"\"\n    # (2 * F.round(F.rand(seed=self.seed-1)) - 1) -&gt; random column of 1s and -1s\n    df = df.withColumn(\n        ColNames.latitude,\n        F.when(\n            F.rand(seed=self.seed - 1) &lt; self.out_of_bounds_values_probability,\n            F.col(ColNames.latitude) + (2 * F.round(F.rand(seed=self.seed - 1)) - 1) * 90,\n        )\n        .otherwise(F.col(ColNames.latitude))\n        .cast(FloatType()),  # the operation turns it into DoubleType, so cast it to Float\n    )\n\n    df = df.withColumn(\n        ColNames.longitude,\n        F.when(\n            F.rand(seed=self.seed - 2) &lt; self.out_of_bounds_values_probability,\n            F.col(ColNames.longitude) + (2 * F.round(F.rand(seed=self.seed - 2)) - 1) * 180,\n        )\n        .otherwise(F.col(ColNames.longitude))\n        .cast(FloatType()),\n    )\n\n    # antenna height, non positive\n    df = df.withColumn(\n        ColNames.antenna_height,\n        F.when(F.rand(seed=self.seed - 3) &lt; self.out_of_bounds_values_probability, -F.col(ColNames.antenna_height))\n        .otherwise(F.col(ColNames.antenna_height))\n        .cast(FloatType()),\n    )\n\n    # directionality: int different from 0 or 1. Just add a static 5 to the value\n    df = df.withColumn(\n        ColNames.directionality,\n        F.when(\n            F.rand(seed=self.seed - 4) &lt; self.out_of_bounds_values_probability, F.col(ColNames.directionality) + 5\n        )\n        .otherwise(F.col(ColNames.directionality))\n        .cast(IntegerType()),\n    )\n\n    # azimuth angle: outside of [0, 360]\n    df = df.withColumn(\n        ColNames.azimuth_angle,\n        F.when(\n            F.rand(seed=self.seed - 5) &lt; self.out_of_bounds_values_probability,\n            F.col(ColNames.azimuth_angle) + (2 * F.round(F.rand(seed=self.seed - 5)) - 1) * 360,\n        )\n        .otherwise(F.col(ColNames.azimuth_angle))\n        .cast(FloatType()),\n    )\n\n    # elevation_angle: outside of [-90, 90]\n    df = df.withColumn(\n        ColNames.elevation_angle,\n        F.when(\n            F.rand(seed=self.seed - 6) &lt; self.out_of_bounds_values_probability,\n            F.col(ColNames.elevation_angle) + (2 * F.round(F.rand(seed=self.seed - 6)) - 1) * 180,\n        )\n        .otherwise(F.col(ColNames.elevation_angle))\n        .cast(FloatType()),\n    )\n\n    # horizontal_beam_width: outside of [0, 360]\n    df = df.withColumn(\n        ColNames.horizontal_beam_width,\n        F.when(\n            F.rand(seed=self.seed - 7) &lt; self.out_of_bounds_values_probability,\n            F.col(ColNames.horizontal_beam_width) + (2 * F.round(F.rand(seed=self.seed - 7)) - 1) * 360,\n        )\n        .otherwise(F.col(ColNames.horizontal_beam_width))\n        .cast(FloatType()),\n    )\n\n    # vertical_beam_width: outside of [0, 360]\n    df = df.withColumn(\n        ColNames.vertical_beam_width,\n        F.when(\n            F.rand(seed=self.seed - 8) &lt; self.out_of_bounds_values_probability,\n            F.col(ColNames.vertical_beam_width) + (2 * F.round(F.rand(seed=self.seed - 8)) - 1) * 360,\n        )\n        .otherwise(F.col(ColNames.vertical_beam_width))\n        .cast(FloatType()),\n    )\n\n    # power: non positive value\n    df = df.withColumn(\n        ColNames.power,\n        F.when(F.rand(seed=self.seed - 9) &lt; self.out_of_bounds_values_probability, -F.col(ColNames.power))\n        .otherwise(F.col(ColNames.power))\n        .cast(FloatType()),\n    )\n\n    # range: non positive value\n    df = df.withColumn(\n        ColNames.range,\n        F.when(F.rand(seed=self.seed - 10) &lt; self.out_of_bounds_values_probability, -F.col(ColNames.range))\n        .otherwise(F.col(ColNames.range))\n        .cast(FloatType()),\n    )\n\n    # frequency: non positive vallue\n    df = df.withColumn(\n        ColNames.frequency,\n        F.when(F.rand(seed=self.seed - 11) &lt; self.out_of_bounds_values_probability, -F.col(ColNames.frequency))\n        .otherwise(F.col(ColNames.frequency))\n        .cast(IntegerType()),\n    )\n    return df\n</code></pre>"},{"location":"reference/components/quality/","title":"quality","text":""},{"location":"reference/components/quality/cell_footprint_quality_metrics/","title":"cell_footprint_quality_metrics","text":""},{"location":"reference/components/quality/cell_footprint_quality_metrics/cell_footprint_quality_metrics/","title":"cell_footprint_quality_metrics","text":"<p>Module that computes quality metrics and warnings of the CellFootPrintEstimation component</p>"},{"location":"reference/components/quality/cell_footprint_quality_metrics/cell_footprint_quality_metrics/#components.quality.cell_footprint_quality_metrics.cell_footprint_quality_metrics.CellFootPrintQualityMetrics","title":"<code>CellFootPrintQualityMetrics</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class responsible for computing quality metrics on the cell footprint. In particular, it checks how many cells have no footprint assigned to them, computes the total events that make reference to each of those cells, and raises a noncritical or critical warning if these \"lost\" events represent a high percentage of the total events.</p> Source code in <code>multimno/components/quality/cell_footprint_quality_metrics/cell_footprint_quality_metrics.py</code> <pre><code>class CellFootPrintQualityMetrics(Component):\n    \"\"\"\n    Class responsible for computing quality metrics on the cell footprint. In particular, it checks how many\n    cells have no footprint assigned to them, computes the total events that make reference to each of those cells,\n    and raises a noncritical or critical warning if these \"lost\" events represent a high percentage of the total\n    events.\n    \"\"\"\n\n    COMPONENT_ID = \"CellFootPrintQualityMetrics\"\n\n    def __init__(self, general_config_path, component_config_path):\n        super().__init__(general_config_path, component_config_path)\n\n        self.timestamp = dt.datetime.now()\n\n        self.data_period_start = dt.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n        self.data_period_end = dt.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n\n        self.data_period_dates = [\n            self.data_period_start + dt.timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n        self.non_crit_event_pct_threshold = self.config.getfloat(self.COMPONENT_ID, \"non_crit_event_pct_threshold\")\n        self.crit_event_pct_threshold = self.config.getfloat(self.COMPONENT_ID, \"crit_event_pct_threshold\")\n\n        if self.non_crit_event_pct_threshold &lt; 0 or self.non_crit_event_pct_threshold &gt; 100:\n            raise ValueError(\n                f\"Threshold `non_crit_event_pct_threshold` must be a float between 0 and 100 -- found {self.non_crit_event_pct_threshold}\"\n            )\n\n        if self.crit_event_pct_threshold &lt; 0 or self.crit_event_pct_threshold &gt; 100:\n            raise ValueError(\n                f\"Threshold `crit_event_pct_threshold` must be a float between 0 and 100 -- found {self.crit_event_pct_threshold}\"\n            )\n\n        if self.non_crit_event_pct_threshold &gt; self.crit_event_pct_threshold:\n            raise ValueError(\n                f\"Non-critical warning threshold {self.non_crit_event_pct_threshold} % should be lower than critical threshold {self.crit_event_pct_threshold}\"\n            )\n\n        self.non_crit_cell_pct_threshold = self.config.getfloat(self.COMPONENT_ID, \"non_crit_cell_pct_threshold\")\n        self.crit_cell_pct_threshold = self.config.getfloat(self.COMPONENT_ID, \"crit_cell_pct_threshold\")\n\n        if self.non_crit_cell_pct_threshold &lt; 0 or self.non_crit_cell_pct_threshold &gt; 100:\n            raise ValueError(\n                f\"Threshold `non_crit_cell_pct_threshold` must be a float between 0 and 100 -- found {self.non_crit_cell_pct_threshold}\"\n            )\n\n        if self.crit_event_pct_threshold &lt; 0 or self.crit_event_pct_threshold &gt; 100:\n            raise ValueError(\n                f\"Threshold `crit_event_pct_threshold` must be a float between 0 and 100 -- found {self.crit_event_pct_threshold}\"\n            )\n\n        if self.non_crit_cell_pct_threshold &gt; self.crit_cell_pct_threshold:\n            raise ValueError(\n                f\"Non-critical warning threshold {self.non_crit_cell_pct_threshold} % should be lower than critical threshold {self.crit_cell_pct_threshold}\"\n            )\n\n        self.current_date: dt.date = None\n        self.current_network: DataFrame = None\n        self.current_cell_footprint: DataFrame = None\n        self.current_events: DataFrame = None\n        self.event_warnings: dict = {}\n        self.cell_warnings: dict = {}\n\n    def initalize_data_objects(self):\n        self.clear_destination_directory = self.config.getboolean(self.COMPONENT_ID, \"clear_destination_directory\")\n        input_network_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"network_data_silver\")\n        input_cell_footprint_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"cell_footprint_data_silver\")\n        input_events_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"event_data_silver\")\n\n        self.input_data_objects = {}\n        if check_if_data_path_exists(self.spark, input_network_path):\n            self.input_data_objects[SilverNetworkDataObject.ID] = SilverCellFootprintDataObject(\n                self.spark, input_network_path\n            )\n        else:\n            self.logger.warning(f\"Expected path {input_network_path} to exist but it does not\")\n            raise ValueError(f\"Invalid path for {SilverNetworkDataObject.ID}: {input_network_path}\")\n\n        if check_if_data_path_exists(self.spark, input_cell_footprint_path):\n            self.input_data_objects[SilverCellFootprintDataObject.ID] = SilverCellFootprintDataObject(\n                self.spark, input_cell_footprint_path\n            )\n        else:\n            self.logger.warning(f\"Expected path {input_cell_footprint_path} to exist but it does not\")\n            raise ValueError(f\"Invalid path for {SilverCellFootprintDataObject.ID}: {input_cell_footprint_path}\")\n\n        if check_if_data_path_exists(self.spark, input_events_path):\n            self.input_data_objects[SilverEventDataObject.ID] = SilverEventDataObject(self.spark, input_events_path)\n        else:\n            self.logger.warning(f\"Expected path {input_events_path} to exist but it does not\")\n            raise ValueError(f\"Invalid path for {SilverEventDataObject.ID}: {input_events_path}\")\n\n        self.output_data_objects = {}\n        output_metrics_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"cell_footprint_quality_metrics\")\n\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, output_metrics_path)\n\n        self.output_data_objects[SilverCellFootprintQualityMetrics.ID] = SilverCellFootprintQualityMetrics(\n            self.spark, output_metrics_path\n        )\n\n    def transform(self):\n        # Get cell IDs with no cell footprint\n        no_footprint_cells = self.current_network.join(\n            self.current_cell_footprint, on=ColNames.cell_id, how=\"left_anti\"\n        )\n        # Get total number of events\n        total_events = self.current_events.count()\n        total_cells = self.current_network.count()\n\n        # Get number of events related to cells with no footprint\n        no_footprint_cells = (\n            no_footprint_cells\n            # `day` column will be NULL if some cell has zero events\n            .join(self.current_events, on=ColNames.cell_id, how=\"left\")\n            .groupBy(ColNames.cell_id)\n            .agg(F.count(ColNames.day).alias(ColNames.number_of_events))\n            .withColumn(\n                ColNames.percentage_total_events, F.lit(100.0 / total_events) * F.col(ColNames.number_of_events)\n            )\n        )\n\n        # Add metadata columns\n        no_footprint_cells = no_footprint_cells.withColumns(\n            {\n                ColNames.year: F.lit(self.current_date.year),\n                ColNames.month: F.lit(self.current_date.month),\n                ColNames.day: F.lit(self.current_date.day),\n                ColNames.result_timestamp: F.lit(self.timestamp),\n            }\n        )\n\n        no_footprint_cells = apply_schema_casting(no_footprint_cells, SilverCellFootprintQualityMetrics.SCHEMA)\n        no_footprint_cells.cache()\n\n        missed_cells_pct = 100 * no_footprint_cells.count() / total_cells\n\n        missed_events_pct = no_footprint_cells.select(\n            F.sum(ColNames.percentage_total_events).alias(\"total_pct\")\n        ).collect()[0][\"total_pct\"]\n        missed_events_pct = missed_events_pct if missed_events_pct is not None else 0\n\n        self.cell_warnings[self.current_date] = {\"metric\": missed_cells_pct}\n        self.event_warnings[self.current_date] = {\"metric\": missed_events_pct}\n\n        # Event warnings\n        if missed_events_pct &gt;= self.crit_event_pct_threshold:\n            self.event_warnings[self.current_date][\"warning\"] = \"critical\"\n        elif missed_events_pct &gt; self.non_crit_event_pct_threshold:\n            self.event_warnings[self.current_date][\"warning\"] = \"non_critical\"\n        else:\n            self.event_warnings[self.current_date][\"warning\"] = \"no\"\n\n        # Cell warnings\n        if missed_cells_pct &gt;= self.crit_cell_pct_threshold:\n            self.cell_warnings[self.current_date][\"warning\"] = \"critical\"\n        elif missed_cells_pct &gt; self.non_crit_cell_pct_threshold:\n            self.cell_warnings[self.current_date][\"warning\"] = \"non_critical\"\n        else:\n            self.cell_warnings[self.current_date][\"warning\"] = \"no\"\n\n        self.output_data_objects[SilverCellFootprintQualityMetrics.ID].df = no_footprint_cells\n\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n\n        self.warnings = {}\n\n        for current_date in self.data_period_dates:\n            self.logger.info(f\"Computing Cell Footprint Quality Metrics for {current_date.strftime('%Y-%m-%d')}\")\n            self.current_date = current_date\n\n            self.current_network = (\n                self.input_data_objects[SilverNetworkDataObject.ID]\n                .df.select(ColNames.cell_id, ColNames.year, ColNames.month, ColNames.day)\n                .filter(\n                    (F.col(ColNames.year) == F.lit(current_date.year))\n                    &amp; (F.col(ColNames.month) == F.lit(current_date.month))\n                    &amp; (F.col(ColNames.day) == F.lit(current_date.day))\n                )\n                .select(ColNames.cell_id)\n            )\n\n            self.current_cell_footprint = (\n                self.input_data_objects[SilverCellFootprintDataObject.ID]\n                .df.select(ColNames.cell_id, ColNames.grid_id, ColNames.year, ColNames.month, ColNames.day)\n                .filter(\n                    (F.col(ColNames.year) == F.lit(current_date.year))\n                    &amp; (F.col(ColNames.month) == F.lit(current_date.month))\n                    &amp; (F.col(ColNames.day) == F.lit(current_date.day))\n                )\n                .select(ColNames.cell_id, ColNames.grid_id)\n            )\n\n            self.current_events = (\n                self.input_data_objects[SilverEventDataObject.ID]\n                .df.select(ColNames.cell_id, ColNames.year, ColNames.month, ColNames.day)\n                .filter(\n                    (F.col(ColNames.year) == F.lit(current_date.year))\n                    &amp; (F.col(ColNames.month) == F.lit(current_date.month))\n                    &amp; (F.col(ColNames.day) == F.lit(current_date.day))\n                )\n                # we keep `day` column for later counting of events on cells\n                .select(ColNames.cell_id, ColNames.day)\n            )\n\n            current_network_exists = len(self.current_network.take(1)) &gt; 0\n            current_footprint_exists = len(self.current_cell_footprint.take(1)) &gt; 0\n            current_event_exists = len(self.current_events.take(1)) &gt; 0\n\n            if not current_network_exists:\n                self.logger.warning(\n                    f\"No silver network data for {current_date} -- skipping quality metrics for this date\"\n                )\n                continue\n\n            if not current_footprint_exists:\n                self.logger.warning(\n                    f\"No cell footprint data for {current_date} -- skipping quality metrics for this date\"\n                )\n                continue\n\n            if not current_event_exists:\n                self.logger.warning(\n                    f\"No silver event data for {current_date} -- skipping quality metrics for this date\"\n                )\n                continue\n\n            self.transform()\n            self.write()\n\n        critical_warning = False\n        for current_date in self.data_period_dates:\n            event_metric = self.event_warnings[current_date][\"metric\"]\n            cell_metric = self.cell_warnings[current_date][\"metric\"]\n            event_warning = self.event_warnings[current_date][\"warning\"]\n            cell_warning = self.cell_warnings[current_date][\"warning\"]\n\n            if event_warning == \"critical\":\n                critical_warning = True\n                self.logger.warning(\n                    f\"Critical warning: Events of {current_date} in cells with no footprint represents {event_metric:.2f} % &gt;= {self.crit_event_pct_threshold} %\"\n                )\n            elif event_warning == \"non_critical\":\n                self.logger.warning(\n                    f\"Non-critical warning: Events of {current_date} in cells with no footprint represents {event_metric:.2f} % &gt; {self.non_crit_event_pct_threshold} %\"\n                )\n\n            if cell_warning == \"critical\":\n                critical_warning = True\n                self.logger.warning(\n                    f\"Critical warning: Cells of {current_date} with no footprint represents {cell_metric:.2f} % &gt;= {self.crit_cell_pct_threshold} %\"\n                )\n            elif cell_warning == \"non_critical\":\n                self.logger.warning(\n                    f\"Non-critical warning: Cells of {current_date} with no footprint represents {cell_metric:.2f} % &gt; {self.non_crit_cell_pct_threshold} %\"\n                )\n\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n        if critical_warning:\n            raise CriticalQualityWarningRaisedException(self.COMPONENT_ID)\n</code></pre>"},{"location":"reference/components/quality/daily_permanence_score_quality_metrics/","title":"daily_permanence_score_quality_metrics","text":""},{"location":"reference/components/quality/daily_permanence_score_quality_metrics/daily_permanence_score_quality_metrics/","title":"daily_permanence_score_quality_metrics","text":"<p>Module that computes quality metrics and warnings of the DailyPermanenceScore component</p>"},{"location":"reference/components/quality/daily_permanence_score_quality_metrics/daily_permanence_score_quality_metrics/#components.quality.daily_permanence_score_quality_metrics.daily_permanence_score_quality_metrics.DailyPermanenceScoreQualityMetrics","title":"<code>DailyPermanenceScoreQualityMetrics</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class responsible for computing quality metrics on the daily permanence score.</p> Source code in <code>multimno/components/quality/daily_permanence_score_quality_metrics/daily_permanence_score_quality_metrics.py</code> <pre><code>class DailyPermanenceScoreQualityMetrics(Component):\n    \"\"\"\n    Class responsible for computing quality metrics on the daily permanence score.\n    \"\"\"\n\n    COMPONENT_ID = \"DailyPermanenceScoreQualityMetrics\"\n\n    def __init__(self, general_config_path, component_config_path):\n        super().__init__(general_config_path, component_config_path)\n\n        self.timestamp = dt.datetime.now()\n\n        self.data_period_start = dt.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n        self.data_period_end = dt.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n\n        self.data_period_dates = [\n            self.data_period_start + dt.timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n        self.unknown_intervals_pct_threshold = self.config.getfloat(\n            self.COMPONENT_ID, \"unknown_intervals_pct_threshold\"\n        )\n        self.non_crit_unknown_devices_pct_threshold = self.config.getfloat(\n            self.COMPONENT_ID, \"non_crit_unknown_devices_pct_threshold\"\n        )\n        self.crit_unknown_devices_pct_threshold = self.config.getfloat(\n            self.COMPONENT_ID, \"crit_unknown_devices_pct_threshold\"\n        )\n\n        if self.unknown_intervals_pct_threshold &lt; 0 or self.unknown_intervals_pct_threshold &gt; 100:\n            raise ValueError(\n                f\"Threshold `unknown_intervals_pct_threshold` must be a float between 0 and 100 -- found {self.unknown_intervals_pct_threshold}\"\n            )\n\n        if self.non_crit_unknown_devices_pct_threshold &lt; 0 or self.non_crit_unknown_devices_pct_threshold &gt; 100:\n            raise ValueError(\n                f\"Threshold `non_crit_unknown_devices_pct_threshold` must be a float between 0 and 100 -- found {self.non_crit_unknown_devices_pct_threshold}\"\n            )\n\n        if self.crit_unknown_devices_pct_threshold &lt; 0 or self.crit_unknown_devices_pct_threshold &gt; 100:\n            raise ValueError(\n                f\"Threshold `crit_cell_pct_threshold` must be a float between 0 and 100 -- found {self.crit_unknown_devices_pct_threshold}\"\n            )\n\n        if self.non_crit_unknown_devices_pct_threshold &gt; self.crit_unknown_devices_pct_threshold:\n            raise ValueError(\n                f\"Non-critical warning threshold {self.non_crit_unknown_devices_pct_threshold} % should be lower than critical threshold {self.crit_unknown_devices_pct_threshold} %\"\n            )\n\n        self.current_date: dt.date = None\n        self.current_dps: DataFrame = None\n        self.warnings: dict = {}\n\n    def initalize_data_objects(self):\n        self.clear_destination_directory = self.config.getboolean(self.COMPONENT_ID, \"clear_destination_directory\")\n\n        input_dps_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"daily_permanence_score_data_silver\")\n\n        self.input_data_objects = {}\n        if check_if_data_path_exists(self.spark, input_dps_path):\n            self.input_data_objects[SilverDailyPermanenceScoreDataObject.ID] = SilverDailyPermanenceScoreDataObject(\n                self.spark, input_dps_path\n            )\n        else:\n            self.logger.warning(f\"Expected path {input_dps_path} to exist but it does not\")\n            raise ValueError(f\"Invalid path for {SilverDailyPermanenceScoreDataObject.ID}: {input_dps_path}\")\n\n        self.output_data_objects = {}\n        output_metrics_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"daily_permanence_score_quality_metrics\")\n\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, output_metrics_path)\n\n        self.output_data_objects[SilverDailyPermanenceScoreQualityMetrics.ID] = (\n            SilverDailyPermanenceScoreQualityMetrics(self.spark, output_metrics_path)\n        )\n\n    def transform(self):\n        total_devices = self.current_dps.select(ColNames.user_id).distinct().count()\n\n        # Find out the size of the time interval\n        time_row = self.current_dps.select(ColNames.time_slot_initial_time, ColNames.time_slot_end_time).take(1)[0]\n        initial_time = time_row[ColNames.time_slot_initial_time]\n        end_time = time_row[ColNames.time_slot_end_time]\n\n        day_time_slots = round(dt.timedelta(days=1).total_seconds() / (end_time - initial_time).total_seconds())\n\n        # Find out the number of devices with too many unknown time intervals\n        unknown_devices = (\n            self.current_dps.filter(F.col(ColNames.id_type) == F.lit(UeGridIdType.UKNOWN_STR))  # unknown timeslots\n            .groupBy(ColNames.user_id_modulo, ColNames.user_id)\n            .agg((F.count(\"*\") * F.lit(100 / day_time_slots)).alias(\"pct_unknown_timeslots\"))  # pct of unknown slots\n            .filter(F.col(\"pct_unknown_timeslots\") &gt;= F.lit(self.unknown_intervals_pct_threshold))  # filter\n            .count()  # count number of devices\n        )\n\n        pct_unknown_devices = 100.0 * unknown_devices / total_devices\n\n        apply_schema_casting\n        qm_data = self.spark.createDataFrame(\n            [\n                Row(\n                    **{\n                        ColNames.result_timestamp: self.timestamp,\n                        ColNames.num_unknown_devices: unknown_devices,\n                        ColNames.pct_unknown_devices: pct_unknown_devices,\n                        ColNames.month: self.current_date.month,\n                        ColNames.day: self.current_date.day,\n                        ColNames.year: self.current_date.year,\n                    }\n                )\n            ],\n            schema=SilverDailyPermanenceScoreQualityMetrics.SCHEMA,\n        )\n\n        qm_data = apply_schema_casting(qm_data, SilverDailyPermanenceScoreQualityMetrics.SCHEMA)\n\n        self.warnings[self.current_date] = {\"metric\": pct_unknown_devices}\n\n        if pct_unknown_devices &gt;= self.crit_unknown_devices_pct_threshold:\n            self.warnings[self.current_date][\"warning\"] = \"critical\"\n        elif pct_unknown_devices &gt; self.non_crit_unknown_devices_pct_threshold:\n            self.warnings[self.current_date][\"warning\"] = \"non_critical\"\n        else:\n            self.warnings[self.current_date][\"warning\"] = \"no\"\n\n        self.output_data_objects[SilverDailyPermanenceScoreQualityMetrics.ID].df = qm_data\n\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n\n        self.warnings = {}\n\n        for current_date in self.data_period_dates:\n            self.logger.info(\n                f\"Computing Daily Permanence Score Quality Metrics for {current_date.strftime('%Y-%m-%d')}\"\n            )\n            self.current_date = current_date\n\n            self.current_dps = (\n                self.input_data_objects[SilverDailyPermanenceScoreDataObject.ID]\n                .df.select(\n                    ColNames.user_id_modulo,\n                    ColNames.user_id,\n                    ColNames.id_type,\n                    ColNames.time_slot_initial_time,\n                    ColNames.time_slot_end_time,\n                    ColNames.year,\n                    ColNames.month,\n                    ColNames.day,\n                )\n                .filter(\n                    (F.col(ColNames.year) == F.lit(current_date.year))\n                    &amp; (F.col(ColNames.month) == F.lit(current_date.month))\n                    &amp; (F.col(ColNames.day) == F.lit(current_date.day))\n                )\n                .select(\n                    ColNames.user_id_modulo,\n                    ColNames.user_id,\n                    ColNames.id_type,\n                    ColNames.time_slot_initial_time,\n                    ColNames.time_slot_end_time,\n                )\n            )\n\n            current_dps_exists = len(self.current_dps.take(1)) &gt; 0\n\n            if not current_dps_exists:\n                self.logger.warning(\n                    f\"No silver network data for {current_date} -- skipping quality metrics for this date\"\n                )\n                continue\n\n            self.transform()\n            self.write()\n\n        critical_warning = False\n        for current_date in self.data_period_dates:\n            metric = self.warnings[current_date][\"metric\"]\n            warning = self.warnings[current_date][\"warning\"]\n\n            if warning == \"critical\":\n                critical_warning = True\n                self.logger.warning(\n                    f\"Critical warning: Events of {current_date} in cells with no footprint represents {metric:.2f} % &gt;= {self.crit_unknown_devices_pct_threshold} %\"\n                )\n            elif warning == \"non_critical\":\n                self.logger.warning(\n                    f\"Non-critical warning: Events of {current_date} in cells with no footprint represents {metric:.2f} % &gt; {self.non_crit_unknown_devices_pct_threshold} %\"\n                )\n\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n        if critical_warning:\n            raise CriticalQualityWarningRaisedException(self.COMPONENT_ID)\n</code></pre>"},{"location":"reference/components/quality/event_quality_warnings/","title":"event_quality_warnings","text":""},{"location":"reference/components/quality/event_quality_warnings/event_quality_warnings/","title":"event_quality_warnings","text":""},{"location":"reference/components/quality/event_quality_warnings/event_quality_warnings/#components.quality.event_quality_warnings.event_quality_warnings.EventQualityWarnings","title":"<code>EventQualityWarnings</code>","text":"<p>               Bases: <code>Component</code></p> <p>Component that calculates Quality Warnings from the Quality Metrics generated by the Event Cleaning component</p> Source code in <code>multimno/components/quality/event_quality_warnings/event_quality_warnings.py</code> <pre><code>class EventQualityWarnings(Component):\n    \"\"\"\n    Component that calculates Quality Warnings from the Quality Metrics generated by the Event Cleaning component\n    \"\"\"\n\n    COMPONENT_ID = \"EventQualityWarnings\"\n\n    dict_convert_to_num_days = {\"week\": 7, \"month\": 30}\n    # dict to store info regarding error type\n    # first element - corresponding encoding of ErrorTypes class\n    # second element - naming constants for coresponding measure definitions, conditions, and warning texts\n    dict_error_type_info = {\n        \"missing_value\": [ErrorTypes.NULL_VALUE, \"Missing value rate\"],\n        \"not_right_syntactic_format\": [\n            ErrorTypes.CANNOT_PARSE,\n            \"Wrong type/format rate\",\n        ],\n        \"out_of_admissible_values\": [\n            ErrorTypes.OUT_OF_RANGE,\n            \"Out of range rate\",\n        ],\n        \"no_location\": [ErrorTypes.NO_LOCATION_INFO, \"No location error rate\"],\n        \"no_domain\": [ErrorTypes.NO_MNO_INFO, \"No domain error rate\"],\n        \"out_of_bounding_box\": [\n            ErrorTypes.OUT_OF_RANGE,\n            \"Out of bounding box error rate\",\n        ],\n        \"same_location_duplicate\": [\n            ErrorTypes.DUPLICATED,\n            \"Deduplication same locations rate\",\n        ],\n    }\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        print(self.config)\n        self.lookback_period = self.config.get(EventQualityWarnings.COMPONENT_ID, \"lookback_period\")\n        self.lookback_period_in_days = self.dict_convert_to_num_days[self.lookback_period]\n\n        self.data_period_start = self.config.get(EventQualityWarnings.COMPONENT_ID, \"data_period_start\")\n        self.data_period_end = self.config.get(EventQualityWarnings.COMPONENT_ID, \"data_period_end\")\n\n        self.qw_dfs_log = []\n        self.qw_dfs_plots = []\n\n        # FOR SYNTACTIC QUALITY WARNINGS\n        self.do_size_raw_data_qw = self.config.getboolean(\n            EventQualityWarnings.COMPONENT_ID, \"do_size_raw_data_qw\", fallback=False\n        )\n        self.do_size_clean_data_qw = self.config.getboolean(\n            EventQualityWarnings.COMPONENT_ID, \"do_size_clean_data_qw\", fallback=False\n        )\n\n        self.data_size_tresholds = self.config.geteval(\n            EventQualityWarnings.COMPONENT_ID, \"data_size_tresholds\", fallback=None\n        )\n\n        self.do_error_rate_by_date_qw = self.config.getboolean(\n            EventQualityWarnings.COMPONENT_ID,\n            \"do_error_rate_by_date_qw\",\n            fallback=False,\n        )\n\n        self.do_error_rate_by_date_and_cell_qw = self.config.getboolean(\n            EventQualityWarnings.COMPONENT_ID,\n            \"do_error_rate_by_date_and_cell_qw\",\n            fallback=False,\n        )\n\n        self.do_error_rate_by_date_and_user_qw = self.config.getboolean(\n            EventQualityWarnings.COMPONENT_ID,\n            \"do_error_rate_by_date_and_user_qw\",\n            fallback=False,\n        )\n\n        self.do_error_rate_by_date_and_cell_user_qw = self.config.getboolean(\n            EventQualityWarnings.COMPONENT_ID,\n            \"do_error_rate_by_date_and_cell_user_qw\",\n            fallback=False,\n        )\n\n        self.error_rate_tresholds = self.config.geteval(\n            EventQualityWarnings.COMPONENT_ID, \"error_rate_tresholds\", fallback=None\n        )\n\n        self.error_type_qw_checks = self.config.geteval(\n            EventQualityWarnings.COMPONENT_ID, \"error_type_qw_checks\", fallback=None\n        )\n\n        self.missing_value_thresholds = self.config.geteval(\n            EventQualityWarnings.COMPONENT_ID, \"missing_value_thresholds\", fallback=None\n        )\n\n        self.out_of_admissible_values_thresholds = self.config.geteval(\n            EventQualityWarnings.COMPONENT_ID,\n            \"out_of_admissible_values_thresholds\",\n            fallback=None,\n        )\n\n        self.not_right_syntactic_format_thresholds = self.config.geteval(\n            EventQualityWarnings.COMPONENT_ID,\n            \"not_right_syntactic_format_thresholds\",\n            fallback=None,\n        )\n\n        self.no_location_thresholds = self.config.geteval(\n            EventQualityWarnings.COMPONENT_ID, \"no_location_thresholds\", fallback=None\n        )\n\n        self.no_domain_thresholds = self.config.geteval(\n            EventQualityWarnings.COMPONENT_ID, \"no_domain_thresholds\", fallback=None\n        )\n\n        self.out_of_bounding_box_thresholds = self.config.geteval(\n            EventQualityWarnings.COMPONENT_ID,\n            \"out_of_bounding_box_thresholds\",\n            fallback=None,\n        )\n        # FOR DEDUPLICATION QUALITY WARNINGS\n        self.deduplication_same_location_thresholds = self.config.geteval(\n            EventQualityWarnings.COMPONENT_ID,\n            \"deduplication_same_location_thresholds\",\n            fallback=None,\n        )\n\n    def initalize_data_objects(self):\n        self.input_qm_data_objects = {}\n        self.output_qw_data_objects = {}\n        self.clear_destination_directory = self.config.getboolean(\n            EventQualityWarnings.COMPONENT_ID, \"clear_destination_directory\"\n        )\n\n        self.input_qm_by_column_path_key = self.config.get(\n            EventQualityWarnings.COMPONENT_ID, \"input_qm_by_column_path_key\"\n        )\n        self.input_qm_freq_distr_path_key = self.config.get(\n            EventQualityWarnings.COMPONENT_ID, \"input_qm_freq_distr_path_key\"\n        )\n        self.output_qw_log_table_path_key = self.config.get(\n            EventQualityWarnings.COMPONENT_ID, \"output_qw_log_table_path_key\"\n        )\n        self.output_qw_for_plots_path_key = self.config.get(\n            EventQualityWarnings.COMPONENT_ID,\n            \"output_qw_for_plots_path_key\",\n            fallback=None,\n        )\n\n        self.input_qm_by_column_path = self.config.get(CONFIG_SILVER_PATHS_KEY, self.input_qm_by_column_path_key)\n        if check_if_data_path_exists(self.spark, self.input_qm_by_column_path):\n            self.input_qm_data_objects[SilverEventDataSyntacticQualityMetricsByColumn.ID] = (\n                SilverEventDataSyntacticQualityMetricsByColumn(self.spark, self.input_qm_by_column_path)\n            )\n        else:\n            self.logger.warning(\"Wrong path for Quality Metrics By Column, terminating component execution\")\n            raise ValueError(\"Invalid path for Quality Metrics By Column\")\n\n        self.input_qm_freq_distr_path = self.config.get(CONFIG_SILVER_PATHS_KEY, self.input_qm_freq_distr_path_key)\n        if check_if_data_path_exists(self.spark, self.input_qm_freq_distr_path):\n            self.input_qm_data_objects[SilverEventDataSyntacticQualityMetricsFrequencyDistribution.ID] = (\n                SilverEventDataSyntacticQualityMetricsFrequencyDistribution(self.spark, self.input_qm_freq_distr_path)\n            )\n        else:\n            self.logger.warning(\n                \"Wrong path for Quality Metrics Frequency Distribution, terminating component execution\"\n            )\n            raise ValueError(\"Invalid path for Quality Metrics Frequency Distribution\")\n\n        self.output_qw_log_table_path = self.config.get(CONFIG_SILVER_PATHS_KEY, self.output_qw_log_table_path_key)\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, self.output_qw_log_table_path)\n        check_or_create_data_path(self.spark, self.output_qw_log_table_path)\n        self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsLogTable.ID] = (\n            SilverEventDataSyntacticQualityWarningsLogTable(self.spark, self.output_qw_log_table_path)\n        )\n        # no plots information is intended for EventDeduplicationQualityWarnings\n        if self.output_qw_for_plots_path_key is not None:\n            self.output_qw_for_plots_path = self.config.get(CONFIG_SILVER_PATHS_KEY, self.output_qw_for_plots_path_key)\n            if self.clear_destination_directory:\n                delete_file_or_folder(self.spark, self.output_qw_for_plots_path)\n            check_or_create_data_path(self.spark, self.output_qw_for_plots_path)\n            self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsForPlots.ID] = (\n                SilverEventDataSyntacticQualityWarningsForPlots(self.spark, self.output_qw_for_plots_path)\n            )\n\n    def read(self):\n        self.input_qm_data_objects[SilverEventDataSyntacticQualityMetricsByColumn.ID].read()\n        self.input_qm_data_objects[SilverEventDataSyntacticQualityMetricsFrequencyDistribution.ID].read()\n\n    def write(self):\n        self.save_quality_warnings_log_table(self.qw_dfs_log)\n        if self.output_qw_for_plots_path_key is not None:\n            self.save_quality_warnings_for_plots(self.qw_dfs_plots)\n\n    def execute(self):\n        self.logger.info(f\"Starting {EventQualityWarnings.COMPONENT_ID}...\")\n        self.read()\n        self.transform()  # Transforms the input_df\n        self.write()\n        self.logger.info(f\"Finished {EventQualityWarnings.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {EventQualityWarnings.COMPONENT_ID}\")\n        # Read QA Metrics of EventCleaning Component, the period of intrest is\n        #  [data_period_start-lookback_period_in_days, data_period_end]\n        # Since QualityWarnings are calculated based on prior data\n        # TODO: deal with cases when df_qa_by_column, df_qa_freq_distribution do not have data for\n        # whole defined period\n        # TODO: dynamically define/check the possible research period of QW  based on data period\n        #  of df_qa_by_column and df_qa_freq_distribution\n        # TODO: implement min_period conf param which is minimal amount of days with previous data to\n        #  have in order to calculate QW (the case for first days in reaserch period)\n        sdate = pd.to_datetime(self.data_period_start) - pd.Timedelta(days=self.lookback_period_in_days)\n        edate = pd.to_datetime(self.data_period_end)\n\n        df_qa_by_column = self.input_qm_data_objects[SilverEventDataSyntacticQualityMetricsByColumn.ID].df.where(\n            psf.col(ColNames.date).between(sdate, edate)\n        )\n\n        df_qa_freq_distribution = self.input_qm_data_objects[\n            SilverEventDataSyntacticQualityMetricsFrequencyDistribution.ID\n        ].df.where(psf.col(ColNames.date).between(sdate, edate))\n\n        df_qa_by_column = df_qa_by_column.cache()\n        # TODO: maybe makes sense to first sum init and final freq, cache and parse this aggregation\n        # to further QW functions\n        df_qa_freq_distribution = df_qa_freq_distribution.cache()\n\n        if self.do_size_raw_data_qw:\n            # for raw data size QW compute warnings and also retrive data to plot distribution of initial frequency\n            df_raw_data_qw, df_raw_plots = self.data_size_qw(\n                df_qa_freq_distribution,\n                self.lookback_period_in_days,\n                self.data_size_tresholds[\"SIZE_RAW_DATA_BYDATE_VARIABILITY\"],\n                self.data_size_tresholds[\"SIZE_RAW_DATA_BYDATE_ABS_VALUE_LOWER_LIMIT\"],\n                self.data_size_tresholds[\"SIZE_RAW_DATA_BYDATE_ABS_VALUE_UPPER_LIMIT\"],\n                type_of_data=\"raw\",\n            )\n            self.qw_dfs_log.append(df_raw_data_qw)\n            self.qw_dfs_plots.append(df_raw_plots)\n\n        if self.do_size_clean_data_qw:\n            # for clean data size QW compute warnings and also retrive data to plot distribution of total frequency\n            df_clean_data_qw, df_clean_plots = self.data_size_qw(\n                df_qa_freq_distribution,\n                self.lookback_period_in_days,\n                self.data_size_tresholds[\"SIZE_CLEAN_DATA_BYDATE_VARIABILITY\"],\n                self.data_size_tresholds[\"SIZE_CLEAN_DATA_BYDATE_ABS_VALUE_LOWER_LIMIT\"],\n                self.data_size_tresholds[\"SIZE_CLEAN_DATA_BYDATE_ABS_VALUE_UPPER_LIMIT\"],\n                type_of_data=\"clean\",\n            )\n            self.qw_dfs_log.append(df_clean_data_qw)\n            self.qw_dfs_plots.append(df_clean_plots)\n\n        if self.do_error_rate_by_date_qw:\n            # for error rate by date QW compute warnings and also retrive data to\n            # plot distribution of error rate by date\n            df_error_rate_by_date_qw, df_error_rate_plots = self.error_rate_qw(\n                df_qa_freq_distribution,\n                self.lookback_period_in_days,\n                [ColNames.date],\n                self.error_rate_tresholds[\"TOTAL_ERROR_RATE_BYDATE_OVER_AVERAGE\"],\n                self.error_rate_tresholds[\"TOTAL_ERROR_RATE_BYDATE_VARIABILITY\"],\n                self.error_rate_tresholds[\"TOTAL_ERROR_RATE_BYDATE_ABS_VALUE_UPPER_LIMIT\"],\n                save_data_for_plots=True,\n            )\n\n            self.qw_dfs_log.append(df_error_rate_by_date_qw)\n            self.qw_dfs_plots.append(df_error_rate_plots)\n        # The current aggrement is that for next error rates (more granular ones) do not store any data for plots\n        # although it could be done with save_data_for_plots=True\n        # TODO: should we consider error rate of null user_id or/and null cell_id in QW computation\n        if self.do_error_rate_by_date_and_cell_qw:\n            df_error_rate_by_date_and_cell_qw, _ = self.error_rate_qw(\n                df_qa_freq_distribution,\n                self.lookback_period_in_days,\n                [ColNames.date, ColNames.cell_id],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYCELL_OVER_AVERAGE\"],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYCELL_VARIABILITY\"],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYCELL_ABS_VALUE_UPPER_LIMIT\"],\n            )\n\n            self.qw_dfs_log.append(df_error_rate_by_date_and_cell_qw)\n\n        if self.do_error_rate_by_date_and_user_qw:\n            df_error_rate_by_date_and_user_qw, _ = self.error_rate_qw(\n                df_qa_freq_distribution,\n                self.lookback_period_in_days,\n                [ColNames.date, ColNames.user_id],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYUSER_OVER_AVERAGE\"],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYUSER_VARIABILITY\"],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYUSER_ABS_VALUE_UPPER_LIMIT\"],\n            )\n\n            self.qw_dfs_log.append(df_error_rate_by_date_and_user_qw)\n\n        if self.do_error_rate_by_date_and_cell_user_qw:\n            df_error_rate_by_date_and_cell_user_qw, _ = self.error_rate_qw(\n                df_qa_freq_distribution,\n                self.lookback_period_in_days,\n                [ColNames.date, ColNames.cell_id, ColNames.user_id],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYCELL_USER_OVER_AVERAGE\"],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYCELL_USER_VARIABILITY\"],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYCELL_USER_ABS_VALUE_UPPER_LIMIT\"],\n            )\n\n            self.qw_dfs_log.append(df_error_rate_by_date_and_cell_user_qw)\n\n        # Two previous types of QW were using df_qa_freq_distribution only\n        # Now calculate error rate for different error types like missing_value, wrong type\n        # based on two QA metrics - df_qa_by_column and df_qa_freq_distribution\n        # error_type_qw_checks - dict('error_type':[relevant columns])\n        for error_type, field_names in self.error_type_qw_checks.items():\n            if field_names == []:\n                self.logger.info(f\"No field name(s) were specified for error type: {error_type}\")\n            else:\n                # if you have a new error_type and thus new error_type_thresholds entry in config\n                # make sure to add it to class atributes and to this block with elif statement\n                if error_type == \"missing_value\":\n                    error_type_thresholds = self.missing_value_thresholds\n                elif error_type == \"out_of_admissible_values\":\n                    error_type_thresholds = self.out_of_admissible_values_thresholds\n                elif error_type == \"not_right_syntactic_format\":\n                    error_type_thresholds = self.not_right_syntactic_format_thresholds\n                elif error_type == \"no_location\":\n                    error_type_thresholds = self.no_location_thresholds\n                elif error_type == \"no_domain\":\n                    error_type_thresholds = self.no_domain_thresholds\n                elif error_type == \"out_of_bounding_box\":\n                    error_type_thresholds = self.out_of_bounding_box_thresholds\n                elif error_type == \"same_location_duplicate\":\n                    error_type_thresholds = self.deduplication_same_location_thresholds\n                else:\n                    self.logger.warning(\n                        f\"Unexpected error type in error_type_qw_checks config param\"\n                        f\": {error_type}, skipping calculation for this qw\"\n                    )\n                    continue\n\n            for field_name in field_names:\n                if field_name in error_type_thresholds.keys():\n                    error_type_qw, _ = self.error_type_rate_qw(\n                        df_qa_by_column,\n                        df_qa_freq_distribution,\n                        field_name,\n                        error_type,\n                        self.lookback_period_in_days,\n                        *list(error_type_thresholds[field_name].values()),\n                    )\n                    self.qw_dfs_log.append(error_type_qw)\n                else:\n                    self.logger.warning(\n                        f\"No thresholds were specified for field {field_name} of {error_type} error_type\"\n                    )\n\n        self.spark.catalog.clearCache()\n\n    def data_size_qw(\n        self,\n        df_freq_distribution: DataFrame,\n        lookback_period_in_days: int,\n        variablility: Union[int, float],\n        lower_limit: Union[int, float],\n        upper_limit: Union[int, float],\n        type_of_data: str,\n        measure_definition_canva: str = f\"{MeasureDefinitions.size_data}\",\n        cond_warn_variability_canva: str = f\"{Conditions.size_data_variability}-{Warnings.size_data_variability}\",\n        cond_warn_upper_lower_canva: str = f\"{Conditions.size_data_upper_lower}-{Warnings.size_data_upper_lower}\",\n    ) -&gt; Tuple[DataFrame]:\n        \"\"\"\n        A unified function to check both raw and clean data sizes, calculates four types of QWs:\n        LOWER_VARIABILITY - for each row using calculated mean and std compute lower variability limit\n            which is mean - SD*variability, check if  daily_value is lower tan limit\n        UPPER_VARIABILITY - for each row using calculated mean and std compute upper variability limit\n             which is mean + SD*variability, check if  daily_value exceeds limit\n        ABS_LOWER_LIMIT - check if daily_value is lower than absolute number lower_limit\n        ABS_UPPER_LIMIT - check if daily_value exceeds absolute number upper_limit\n        All four QWs depend on thresholds, in case if statement condition is met -&gt; store cond-warn-condition_value\n            information in array column,\n        some rows may have several QWs. In the end array column is exploded and cond-warn-consition_value information\n            is split into three corresponding columns.\n        The function returns almost ready dfs for SilverEventDataSyntacticQualityWarningsLogTable\n             and SilverEventDataSyntacticQualityWarningsForPlots DOs\n\n        Args:\n            df_freq_distribution (DataFrame): df with frequency data\n            lookback_period_in_days (int): lenght of lookback period in days\n            variablility (Union[int, float]): config param, the number of SD to define the upper and lower varibaility\n                limits: mean_size \u00b1 SD*variability, which daily_value should not exceed/be lower\n            lower_limit (Union[int, float]): absolute number which daily_value should not be lower\n            upper_limit (Union[int, float]): absolute number which daily_value can not exceed\n            type_of_data (str): which type of data raw or clean to check for QWs\n            measure_definition_canva (str): canva text to use for measure_definition column (see measure_definition.py)\n            cond_warn_variability_canva (str): canva text to use for lower_upper_variability cases\n                of data_size QWs (see conditions.py and warnings.py)\n            cond_warn_upper_lower_canva (str): canva text to use for lower_upper_limit cases\n                of data_size QWs (see conditions.py and warnings.py)\n\n        Returns:\n            tuple(DataFrame, DataFrame): a tuple, where first df\n                is used for warning log table, and the second df - for plots\n        \"\"\"\n        # based on type_of_data calculate either total daily initial freqeuncy or total daily final frequency\n        if type_of_data == \"raw\":\n            sum_column = ColNames.initial_frequency\n        else:\n            sum_column = ColNames.final_frequency\n        # fill in string canvases\n        measure_definition = measure_definition_canva.format(type_of_data=type_of_data)\n        cond_warn_variability = cond_warn_variability_canva.format(SD=variablility, type_of_data=type_of_data)\n        cond_warn_upper_lower = cond_warn_upper_lower_canva.format(\n            X=lower_limit, Y=upper_limit, type_of_data=type_of_data\n        )\n        # define lookback period\n        window = Window.orderBy(ColNames.date).rowsBetween(-lookback_period_in_days, -1)\n        # prepare data\n        # calculate mean and std over window, and based on them UCL - for UPPER_VARIABILITY check and LCL\n        #   - for LOWER_VARIABILITY check\n        # create empty array cond_warn_condition_value column to store information about qws\n        df_prep = (\n            df_freq_distribution.groupBy(ColNames.date)\n            .agg(psf.sum(sum_column).alias(ColNames.daily_value))\n            .withColumns(\n                {\n                    ColNames.average: psf.avg(ColNames.daily_value).over(window),\n                    \"std_freq\": psf.stddev(ColNames.daily_value).over(window),\n                    ColNames.UCL: psf.col(ColNames.average) + variablility * psf.col(\"std_freq\"),\n                    ColNames.LCL: psf.col(ColNames.average) - variablility * psf.col(\"std_freq\"),\n                    \"cond_warn_condition_value\": psf.array(),\n                }\n            )\n        )\n\n        df_prep = df_prep.cache()\n        # continue with QWs checks\n        # first filter data by period from [data_period_start-lookback_period_in_days, data_period_end]\n        # to [data_period_start, data_period_end]\n        # - a specified research period of QW\n        df_qw = df_prep.filter(psf.col(ColNames.date) &gt;= self.data_period_start)\n        # perform LOWER_VARIABILITY, UPPER_VARIABILITY, ABS_LOWER_LIMIT and ABS_UPPER_LIMIT checks\n        # if condition is met append information about condition-warning_text-condition_value as a string\n        # into array column 'cond_warn_condition_value'\n        df_qw = (\n            df_qw.withColumn(\n                \"cond_warn_condition_value\",\n                psf.when(\n                    (psf.col(ColNames.daily_value) &lt; psf.col(ColNames.LCL)),\n                    psf.array_append(\n                        psf.col(\"cond_warn_condition_value\"),\n                        psf.concat(\n                            psf.lit(f\"{cond_warn_variability}-\"),\n                            psf.col(ColNames.LCL).cast(\"string\"),\n                        ),\n                    ),\n                ).otherwise(psf.col(\"cond_warn_condition_value\")),\n            )\n            .withColumn(\n                \"cond_warn_condition_value\",\n                psf.when(\n                    (psf.col(ColNames.daily_value) &gt; psf.col(ColNames.UCL)),\n                    psf.array_append(\n                        psf.col(\"cond_warn_condition_value\"),\n                        psf.concat(\n                            psf.lit(f\"{cond_warn_variability}-\"),\n                            psf.col(ColNames.UCL).cast(\"string\"),\n                        ),\n                    ),\n                ).otherwise(psf.col(\"cond_warn_condition_value\")),\n            )\n            .withColumn(\n                \"cond_warn_condition_value\",\n                psf.when(\n                    (psf.col(ColNames.daily_value) &lt; psf.lit(lower_limit)),\n                    psf.array_append(\n                        psf.col(\"cond_warn_condition_value\"),\n                        psf.lit(f\"{cond_warn_upper_lower}-{str(lower_limit)}\"),\n                    ),\n                ).otherwise(psf.col(\"cond_warn_condition_value\")),\n            )\n            .withColumn(\n                \"cond_warn_condition_value\",\n                psf.when(\n                    (psf.col(ColNames.daily_value) &gt; psf.lit(upper_limit)),\n                    psf.array_append(\n                        psf.col(\"cond_warn_condition_value\"),\n                        psf.lit(f\"{cond_warn_upper_lower}-{str(upper_limit)}\"),\n                    ),\n                ).otherwise(psf.col(\"cond_warn_condition_value\")),\n            )\n        )\n        # explode array column 'cond_warn_condition_value'\n        df_qw = df_qw.withColumn(\n            \"cond_warn_condition_value\",\n            psf.explode(psf.col(\"cond_warn_condition_value\")),\n        )\n        # add some column constants\n        # split \"cond_warn_condition_value\" column into three: condition, wanring_text, conditon_value to\n        # match SilverEventDataSyntacticQualityWarningsLogTable.SCHEMA\n        # select only needed columns\n        df_qw = df_qw.withColumns(\n            {\n                ColNames.lookback_period: psf.lit(self.lookback_period),\n                ColNames.measure_definition: psf.lit(measure_definition),\n                ColNames.condition: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(0),\n                ColNames.warning_text: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(1),\n                ColNames.condition_value: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(2).cast(\"float\"),\n            }\n        ).select(self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsLogTable.ID].SCHEMA.fieldNames())\n\n        # save data for plots\n        # no filter by date because we need previous data of first days for plots\n        df_plots = df_prep.withColumns(\n            {\n                ColNames.lookback_period: psf.lit(self.lookback_period),\n                ColNames.type_of_qw: psf.lit(f\"{type_of_data}_data_size\"),\n            }\n        ).select(self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsForPlots.ID].SCHEMA.fieldNames())\n\n        return df_qw, df_plots\n\n    def error_rate_qw(\n        self,\n        df_freq_distribution: DataFrame,\n        lookback_period_in_days: int,\n        variables: List[str],\n        error_rate_over_average: Union[int, float],\n        error_rate_upper_variability: Union[int, float],\n        error_rate_upper_limit: Union[int, float],\n        error_rate_measure_definition_canva: str = f\"{MeasureDefinitions.error_rate}\",\n        error_rate_cond_warn_over_average_canva: str = f\"{Conditions.error_rate_over_average}-{Warnings.error_rate_over_average}\",\n        error_rate_cond_warn_upper_variability_canva: str = f\"{Conditions.error_rate_upper_variability}-{Warnings.error_rate_upper_variability}\",\n        error_rate_cond_warn_upper_limit_canva: str = f\"{Conditions.error_rate_upper_limit}-{Warnings.error_rate_upper_limit}\",\n        save_data_for_plots: bool = False,\n    ) -&gt; Tuple[Union[DataFrame, None]]:\n        \"\"\"\n        Prepare data for error rate calculation. First fill in different string canvas,\n            then define window of aggregation, and calculate error_rate over the window on follwoing formula:\n            (Total initial frequency - Total final frequency) / Total initial frequency*100.\n            Parse preprocessed input to self.rate_common_qw function which calculates three types\n                of QWs: OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT\n\n        Args:\n            df_freq_distribution (DataFrame): df with frequency data.\n            lookback_period_in_days (int): number of days prior to date of interest.\n            variables (List[str]): list of column names by which error rate is calculated, kind of granularity level\n            error_rate_over_average (Union[int, float]): config param, specifies the upper limit which a daily value\n                can not exceed its corresponding mean error rate\n            error_rate_upper_variability (Union[int, float]): config param, the number of SD to define the upper\n                varibaility limit: mean_rate + SD*error_rate_upper_variability, which error rate can't exceed\n            error_rate_upper_limit (Union[int, float]): absolute number which error rate can not exceed\n            error_rate_measure_definition_canva (str): canva text to use for measure_definition\n                column (see measure_definition.py)\n            error_rate_cond_warn_over_average_canva (str): canva text to use for over_average cases of\n                error_rate QWs (see conditions.py and warnings.py)\n            error_rate_cond_warn_upper_variability_canva (str): canva text to use for upper_variability cases of\n                error_rate QWs (see conditions.py and warnings.py)\n            error_rate_cond_warn_upper_limit_canva (str): canva text to use for upper_limit cases of error_rate\n                QWs (see conditions.py and warnings.py)\n            save_data_for_plots (bool): boolean, decide whether to store error rate and its corresponding average\n                and upper variability limit for plots, default False\n        Returns:\n            tuple(Union[DataFrame, None]): a tuple, where first df is used for warning log table,\n                and the second df - for plots (could be also None)\n        \"\"\"\n        # fill in all string comnstants with relevant information\n        # based on variables, error_rate_over_average, error_rate_upper_variability, error_rate_upper_limit args\n        error_rate_measure_definition = error_rate_measure_definition_canva.format(variables=\"&amp;\".join(variables))\n\n        error_rate_cond_warn_over_average = error_rate_cond_warn_over_average_canva.format(\n            variables=\"&amp;\".join(variables), X=error_rate_over_average\n        )\n        error_rate_cond_warn_upper_variability = error_rate_cond_warn_upper_variability_canva.format(\n            variables=\"&amp;\".join(variables), SD=error_rate_upper_variability\n        )\n        error_rate_cond_warn_upper_limit = error_rate_cond_warn_upper_limit_canva.format(\n            variables=\"&amp;\".join(variables), X=error_rate_upper_limit\n        )\n        # qws of error rate by date is calculated based on previous days\n        if variables == [ColNames.date]:\n            window = Window.orderBy(ColNames.date).rowsBetween(-lookback_period_in_days, -1)\n        else:\n            # qws of error rate by date and other colum(s) is calculated over all \"data points\" of the same date\n            window = Window.partitionBy(ColNames.date)\n        # calculate error rate, a.k.a daily_value\n        df_qw = (\n            df_freq_distribution.groupBy(*variables)\n            .agg(\n                psf.sum(ColNames.initial_frequency).alias(\"sum_init_freq\"),\n                psf.sum(ColNames.final_frequency).alias(\"sum_final_freq\"),\n            )\n            .withColumn(\n                ColNames.daily_value,\n                (psf.col(\"sum_init_freq\") - psf.col(\"sum_final_freq\")) / psf.col(\"sum_init_freq\") * 100,\n            )\n        )\n        # using self.rate_common_qw funciton calculate three types of QWs\n        #   -&gt; OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT\n        # return a tuple of (df_log, df_plots | None)\n        qw_result = self.rate_common_qw(\n            df_qw,\n            window,\n            error_rate_upper_variability,\n            error_rate_over_average,\n            error_rate_upper_limit,\n            error_rate_measure_definition,\n            error_rate_cond_warn_upper_variability,\n            error_rate_cond_warn_over_average,\n            error_rate_cond_warn_upper_limit,\n            save_data_for_plots,\n        )\n\n        return qw_result\n\n    def error_type_rate_qw(\n        self,\n        df_qa_by_column: DataFrame,\n        df_freq_distribution: DataFrame,\n        field_name: Union[str, None],\n        error_type: str,\n        lookback_period_in_days: int,\n        error_type_rate_over_average: Union[int, float],\n        error_type_rate_upper_variability: Union[int, float],\n        error_type_rate_upper_limit: Union[int, float],\n        error_type_rate_measure_definition_canva: str = f\"{MeasureDefinitions.error_type_rate}\",\n        error_type_rate_cond_warn_over_average_canva: str = f\"{Conditions.error_type_rate_over_average}-{Warnings.error_type_rate_over_average}\",\n        error_type_rate_cond_warn_upper_variability_canva: str = f\"{Conditions.error_type_rate_upper_variability}-{Warnings.error_type_rate_upper_variability}\",\n        error_type_rate_cond_warn_upper_limit_canva: str = f\"{Conditions.error_type_rate_upper_limit}-{Warnings.error_type_rate_upper_limit}\",\n    ) -&gt; Tuple[Union[DataFrame, None]]:\n        \"\"\"\n        Prepare data for error type rate calculation. First fill in different string canvas, then based\n            on field name and error type calculate their corresponding error rate using formula:\n            number of errors of this error_type&amp;field_name combo / Total initial frequency *100 (BY DATE).\n            Parse preprocessed input along with window (which is a lookback period)\n            to self.rate_common_qw function which calculates three types of QWs:\n            OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT\n\n\n        Args:\n            df_qa_by_column (DataFrame): df with qa by column data.\n            df_freq_distribution (DataFrame): df with frequency data.\n            field_name (str | None): config param, the name of column of which to check error_type.\n            error_type (str): config param, the name of error type.\n            lookback_period_in_days (int): number of days prior to date of intrest.\n            error_type_rate_over_average (Union[int, float]): config param, specifies the upper limit over which daily\n                value can not exceed its corresponding mean error rate.\n            error_type_rate_upper_variability (Union[int, float]): config param, the number of SD to define the upper\n                varibaility limit: mean_rate + SD*error_type_rate_upper_variability, which daily value can not exceed\n            error_type_rate_upper_limit (Union[int, float]): absolute number which daily value can not exceed\n            error_type_rate_measure_definition_canva (str): canva text to use for measure_definition\n                column (see measure_definition.py)\n            error_type_rate_cond_warn_over_average_canva (str): canva text to use for over_average cases of\n                error_type_rate QWs (see conditions.py and warnings.py)\n            error_type_rate_cond_warn_upper_variability_canva (str): canva text to use for upper_variability\n                cases of error_type_rate QWs (see conditions.py and warnings.py)\n            error_type_rate_cond_warn_upper_limit_canva (str): canva text to use for upper_limit cases of\n                error_type_rate QWs (see conditions.py and warnings.py)\n        Returns:\n            tuple(Union[DataFrame, None]): a tuple, where first df is used for warning log table,\n                and the second df - for plots, but since save_data_for_plots always False, output=None\n        \"\"\"\n        # based on error_type, error_type_rate_over_average, error_type_rate_upper_variability,\n        #  error_type_rate_upper_limit\n        # fill in string canvases\n        colname_error_type, error_type_qw_name = self.dict_error_type_info[error_type]\n\n        error_type_rate_measure_definition = error_type_rate_measure_definition_canva.format(\n            error_type_name=error_type_qw_name, field_name=str(field_name)\n        )\n\n        error_type_rate_cond_warn_over_average = error_type_rate_cond_warn_over_average_canva.format(\n            error_type_name=error_type_qw_name,\n            field_name=str(field_name),\n            X=error_type_rate_over_average,\n        )\n        error_type_rate_cond_warn_upper_variability = error_type_rate_cond_warn_upper_variability_canva.format(\n            error_type_name=error_type_qw_name,\n            field_name=str(field_name),\n            SD=error_type_rate_upper_variability,\n        )\n        error_type_rate_cond_warn_upper_limit = error_type_rate_cond_warn_upper_limit_canva.format(\n            error_type_name=error_type_qw_name,\n            field_name=str(field_name),\n            X=error_type_rate_upper_limit,\n        )\n        # for error_type that have more then one or applicable columns\n        # filter df_qa_by_column by field_name and error_type\n        if field_name is not None:\n            df_qa_by_column = df_qa_by_column.filter(\n                (psf.col(ColNames.variable) == field_name) &amp; (psf.col(ColNames.type_of_error) == colname_error_type)\n            ).select(ColNames.date, ColNames.value)\n        else:\n            # for error_types which technically do not belong specifically to one of event\n            # columns filter only by error_type (e.g. no_location error_type)\n            df_qa_by_column = df_qa_by_column.filter(psf.col(ColNames.type_of_error) == colname_error_type).select(\n                ColNames.date, ColNames.value\n            )\n        # calculate total daily initial frequency\n        df_freq_distribution = (\n            df_freq_distribution.groupby(ColNames.date)\n            .agg(psf.sum(ColNames.initial_frequency).alias(\"sum_init_freq\"))\n            .select(ColNames.date, \"sum_init_freq\")\n        )\n        # for each date combine two type of information number of errors and total daily initial frequency\n        df_combined = df_qa_by_column.join(df_freq_distribution, on=ColNames.date, how=\"inner\")\n        # for each date calculate error_type_rate, a.k.a daily_value\n        df_temp = df_combined.withColumn(\n            ColNames.daily_value,\n            (psf.col(ColNames.value) / psf.col(\"sum_init_freq\")) * 100,\n        )\n\n        # qws will be caluclated based on previous days\n        window = Window.orderBy(ColNames.date).rowsBetween(-lookback_period_in_days, -1)\n        # using self.rate_common_qw funciton calculate three types of QWs -&gt;\n        # OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT\n        # return a tuple of (df_log, None)\n        qw_result = self.rate_common_qw(\n            df_temp,\n            window,\n            error_type_rate_upper_variability,\n            error_type_rate_over_average,\n            error_type_rate_upper_limit,\n            error_type_rate_measure_definition,\n            error_type_rate_cond_warn_upper_variability,\n            error_type_rate_cond_warn_over_average,\n            error_type_rate_cond_warn_upper_limit,\n        )\n        return qw_result\n\n    def rate_common_qw(\n        self,\n        df_temp: DataFrame,\n        window: Window,\n        rate_upper_variability: Union[int, float],\n        rate_over_average: Union[int, float],\n        rate_upper_limit: Union[int, float],\n        measure_definition: str,\n        cond_warn_upper_variability: str,\n        cond_warn_over_average: str,\n        cond_warn_upper_limit: str,\n        save_data_for_plots: bool = False,\n    ) -&gt; Tuple[Union[DataFrame, None]]:\n        \"\"\"\n        Take input df with \"daily_value\" column, and calculates three types of QWs:\n        OVER_AVERAGE - for each row first based on specified window take mean of values, and then check if\n            daily_value exceeds mean by more than rate_over_average\n        UPPER_VARIABILITY - for each row using already calculated mean compute upper variability limit which is\n            mean + SD*rate_upper_variability, check if  daily_value exceeds it\n        ABS_UPPER_LIMIT - check if daily_value exceeds absolute number rate_upper_limit\n        All three QWs depend on specified thresholds, if daily_value exceeds one of calculated values it will\n            store cond-warn-condition_value information in array column,\n        some rows may have several QWs. In the end array column is exploded and cond-warn-condition_value\n            information is split into three corresponding columns.\n        The function returns almost ready df for SilverEventDataSyntacticQualityWarningsLogTable DO, and based\n            on save_data_for_plots arg returns either almost ready data for plots or None\n\n        Args:\n            df_temp (DataFrame): temprory data that must have daily_value column to\n                be used in further QW calculations\n            window (Window): a window within which perform aggregation\n            rate_upper_variability (int|float): config param, specifies the upper limit over which daily value\n                 can not exceed its corresponding mean error rate\n            rate_over_average (int|float): config param, the number of SD to define the upper varibaility\n                 limit: mean_rate + SD*error_rate_upper_variability, which daily value can not exceed\n            rate_upper_limit (int|float): absolute number which daily value can not exceed\n            measure_definition (str): canva text to use for measure_definition column (see measure_definition.py)\n            cond_warn_over_average (str): canva text to use for over_average cases (see conditions.py and warnings.py)\n            cond_warn_upper_variability (str): canva text to use for\n                upper_variability cases (see conditions.py and warnings.py)\n            cond_warn_upper_limit (str): canva text to use for upper_limit cases (see conditions.py and warnings.py)\n            save_data_for_plots (bool): boolean, decide whether to store daily_value and its corresponding average\n                and upper variability limit for plots. Defaults to False.\n        Returns:\n             tuple(Union[DataFrame, None]): a tuple, where first df is used for\n                warning log table, and the second df - for plots\n        \"\"\"\n        # prepare data\n        # calculate mean and std over window, and based on them UCL - for UPPER_VARIABILITY check\n        # ratio_perc - for OVER_AVERAGE check\n        # create empty array cond_warn_condition_value column to store inromation about qws\n        df_prep = df_temp.withColumns(\n            {\n                ColNames.average: psf.avg(ColNames.daily_value).over(window),\n                \"std_rate\": psf.stddev(ColNames.daily_value).over(window),\n                \"ratio_perc\": (psf.col(ColNames.daily_value) / psf.col(ColNames.average)) * 100,\n                ColNames.UCL: psf.col(ColNames.average) + rate_upper_variability * psf.col(\"std_rate\"),\n                \"cond_warn_condition_value\": psf.array(),\n            }\n        )\n        # if save_data_for_plots=True, add some new columns with constant values\n        # and select only applicable to SilverEventDataSyntacticQualityWarningsForPlots.SCHEMA\n        # else - return None\n        if save_data_for_plots:\n            df_prep = df_prep.cache()\n            df_plots = df_prep.withColumns(\n                {\n                    ColNames.lookback_period: psf.lit(self.lookback_period),\n                    ColNames.type_of_qw: psf.lit(\"error_rate\"),\n                    ColNames.LCL: psf.lit(None).cast(\"float\"),\n                }\n            ).select(\n                self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsForPlots.ID].SCHEMA.fieldNames()\n            )\n        else:\n            df_plots = None\n\n        # continue with QWs checks\n        # first filter temp data by period from [data_period_start-lookback_period_in_days, data_period_end]\n        #  to [data_period_start, data_period_end]\n        # filter is aaplied after plot block because the first days of research period needs previous data to plot\n        df_qw = df_prep.filter(psf.col(ColNames.date) &gt;= self.data_period_start)\n        # perform UPPER_VARIABILITY, OVER_AVERAGE, and ABS_UPPER_LIMIT checks\n        # if condition is met store information about condition-warning_text-condition_value as a string into\n        # array column 'cond_warn_condition_value'\n        df_qw = (\n            df_qw.withColumn(\n                \"cond_warn_condition_value\",\n                psf.when(\n                    psf.col(ColNames.daily_value) &gt; psf.col(ColNames.UCL),\n                    psf.array_append(\n                        psf.col(\"cond_warn_condition_value\"),\n                        psf.concat(\n                            psf.lit(f\"{cond_warn_upper_variability}-\"),\n                            psf.col(ColNames.UCL).cast(\"string\"),\n                        ),\n                    ),\n                ).otherwise(psf.col(\"cond_warn_condition_value\")),\n            )\n            .withColumn(\n                \"cond_warn_condition_value\",\n                psf.when(\n                    psf.col(\"ratio_perc\") &gt; psf.lit(100 + rate_over_average),\n                    psf.array_append(\n                        psf.col(\"cond_warn_condition_value\"),\n                        psf.lit(f\"{cond_warn_over_average}-{str(rate_over_average)}\"),\n                    ),\n                ).otherwise(psf.col(\"cond_warn_condition_value\")),\n            )\n            .withColumn(\n                \"cond_warn_condition_value\",\n                psf.when(\n                    psf.col(ColNames.daily_value) &gt; psf.lit(rate_upper_limit),\n                    psf.array_append(\n                        psf.col(\"cond_warn_condition_value\"),\n                        psf.lit(f\"{cond_warn_upper_limit}-{str(rate_upper_limit)}\"),\n                    ),\n                ).otherwise(psf.col(\"cond_warn_condition_value\")),\n            )\n        )\n        # explode array column 'cond_warn_condition_value'\n        df_qw = df_qw.withColumn(\n            \"cond_warn_condition_value\",\n            psf.explode(psf.col(\"cond_warn_condition_value\")),\n        )\n        # add some column constants\n        # split \"cond_warn_condition_value\" column into three: condition, wanring_text, conditon_value to match\n        # SilverEventDataSyntacticQualityWarningsLogTable.SCHEMA\n        # select only needed columns\n        df_qw = df_qw.withColumns(\n            {\n                ColNames.lookback_period: psf.lit(self.lookback_period),\n                ColNames.measure_definition: psf.lit(measure_definition),\n                ColNames.condition: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(0),\n                ColNames.warning_text: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(1),\n                ColNames.condition_value: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(2).cast(\"float\"),\n            }\n        ).select(self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsLogTable.ID].SCHEMA.fieldNames())\n\n        return (df_qw, df_plots)\n\n    def save_quality_warnings_output(\n        self,\n        dfs_qw: List[Union[DataFrame, None]],\n        output_do: Union[\n            SilverEventDataSyntacticQualityWarningsLogTable, SilverEventDataSyntacticQualityWarningsForPlots\n        ],\n    ):\n        \"\"\"\n        Concatenates all elements in dfs_qw list, adjustes to schema of output_do, and using write\n            method of output_do stores the result\n\n        Args:\n            dfs_qw (list): _description_\n            output_do (SilverEventDataSyntacticQualityWarningsLogTable | \\\n                SilverEventDataSyntacticQualityWarningsForPlots): _description_\n        \"\"\"\n\n        output_do.df = reduce(lambda x, y: x.union(y), dfs_qw)\n\n        output_do.df = self.spark.createDataFrame(output_do.df.rdd, output_do.SCHEMA)\n\n        output_do.write()\n\n    def save_quality_warnings_log_table(self, dfs_qw):\n\n        self.save_quality_warnings_output(\n            dfs_qw,\n            self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsLogTable.ID],\n        )\n\n    def save_quality_warnings_for_plots(self, dfs_qw):\n\n        self.save_quality_warnings_output(\n            dfs_qw,\n            self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsForPlots.ID],\n        )\n</code></pre>"},{"location":"reference/components/quality/event_quality_warnings/event_quality_warnings/#components.quality.event_quality_warnings.event_quality_warnings.EventQualityWarnings.data_size_qw","title":"<code>data_size_qw(df_freq_distribution, lookback_period_in_days, variablility, lower_limit, upper_limit, type_of_data, measure_definition_canva=f'{MeasureDefinitions.size_data}', cond_warn_variability_canva=f'{Conditions.size_data_variability}-{Warnings.size_data_variability}', cond_warn_upper_lower_canva=f'{Conditions.size_data_upper_lower}-{Warnings.size_data_upper_lower}')</code>","text":"<p>A unified function to check both raw and clean data sizes, calculates four types of QWs: LOWER_VARIABILITY - for each row using calculated mean and std compute lower variability limit     which is mean - SDvariability, check if  daily_value is lower tan limit UPPER_VARIABILITY - for each row using calculated mean and std compute upper variability limit      which is mean + SDvariability, check if  daily_value exceeds limit ABS_LOWER_LIMIT - check if daily_value is lower than absolute number lower_limit ABS_UPPER_LIMIT - check if daily_value exceeds absolute number upper_limit All four QWs depend on thresholds, in case if statement condition is met -&gt; store cond-warn-condition_value     information in array column, some rows may have several QWs. In the end array column is exploded and cond-warn-consition_value information     is split into three corresponding columns. The function returns almost ready dfs for SilverEventDataSyntacticQualityWarningsLogTable      and SilverEventDataSyntacticQualityWarningsForPlots DOs</p> <p>Parameters:</p> Name Type Description Default <code>df_freq_distribution</code> <code>DataFrame</code> <p>df with frequency data</p> required <code>lookback_period_in_days</code> <code>int</code> <p>lenght of lookback period in days</p> required <code>variablility</code> <code>Union[int, float]</code> <p>config param, the number of SD to define the upper and lower varibaility limits: mean_size \u00b1 SD*variability, which daily_value should not exceed/be lower</p> required <code>lower_limit</code> <code>Union[int, float]</code> <p>absolute number which daily_value should not be lower</p> required <code>upper_limit</code> <code>Union[int, float]</code> <p>absolute number which daily_value can not exceed</p> required <code>type_of_data</code> <code>str</code> <p>which type of data raw or clean to check for QWs</p> required <code>measure_definition_canva</code> <code>str</code> <p>canva text to use for measure_definition column (see measure_definition.py)</p> <code>f'{size_data}'</code> <code>cond_warn_variability_canva</code> <code>str</code> <p>canva text to use for lower_upper_variability cases of data_size QWs (see conditions.py and warnings.py)</p> <code>f'{size_data_variability}-{size_data_variability}'</code> <code>cond_warn_upper_lower_canva</code> <code>str</code> <p>canva text to use for lower_upper_limit cases of data_size QWs (see conditions.py and warnings.py)</p> <code>f'{size_data_upper_lower}-{size_data_upper_lower}'</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>(DataFrame, DataFrame)</code> <p>a tuple, where first df is used for warning log table, and the second df - for plots</p> Source code in <code>multimno/components/quality/event_quality_warnings/event_quality_warnings.py</code> <pre><code>def data_size_qw(\n    self,\n    df_freq_distribution: DataFrame,\n    lookback_period_in_days: int,\n    variablility: Union[int, float],\n    lower_limit: Union[int, float],\n    upper_limit: Union[int, float],\n    type_of_data: str,\n    measure_definition_canva: str = f\"{MeasureDefinitions.size_data}\",\n    cond_warn_variability_canva: str = f\"{Conditions.size_data_variability}-{Warnings.size_data_variability}\",\n    cond_warn_upper_lower_canva: str = f\"{Conditions.size_data_upper_lower}-{Warnings.size_data_upper_lower}\",\n) -&gt; Tuple[DataFrame]:\n    \"\"\"\n    A unified function to check both raw and clean data sizes, calculates four types of QWs:\n    LOWER_VARIABILITY - for each row using calculated mean and std compute lower variability limit\n        which is mean - SD*variability, check if  daily_value is lower tan limit\n    UPPER_VARIABILITY - for each row using calculated mean and std compute upper variability limit\n         which is mean + SD*variability, check if  daily_value exceeds limit\n    ABS_LOWER_LIMIT - check if daily_value is lower than absolute number lower_limit\n    ABS_UPPER_LIMIT - check if daily_value exceeds absolute number upper_limit\n    All four QWs depend on thresholds, in case if statement condition is met -&gt; store cond-warn-condition_value\n        information in array column,\n    some rows may have several QWs. In the end array column is exploded and cond-warn-consition_value information\n        is split into three corresponding columns.\n    The function returns almost ready dfs for SilverEventDataSyntacticQualityWarningsLogTable\n         and SilverEventDataSyntacticQualityWarningsForPlots DOs\n\n    Args:\n        df_freq_distribution (DataFrame): df with frequency data\n        lookback_period_in_days (int): lenght of lookback period in days\n        variablility (Union[int, float]): config param, the number of SD to define the upper and lower varibaility\n            limits: mean_size \u00b1 SD*variability, which daily_value should not exceed/be lower\n        lower_limit (Union[int, float]): absolute number which daily_value should not be lower\n        upper_limit (Union[int, float]): absolute number which daily_value can not exceed\n        type_of_data (str): which type of data raw or clean to check for QWs\n        measure_definition_canva (str): canva text to use for measure_definition column (see measure_definition.py)\n        cond_warn_variability_canva (str): canva text to use for lower_upper_variability cases\n            of data_size QWs (see conditions.py and warnings.py)\n        cond_warn_upper_lower_canva (str): canva text to use for lower_upper_limit cases\n            of data_size QWs (see conditions.py and warnings.py)\n\n    Returns:\n        tuple(DataFrame, DataFrame): a tuple, where first df\n            is used for warning log table, and the second df - for plots\n    \"\"\"\n    # based on type_of_data calculate either total daily initial freqeuncy or total daily final frequency\n    if type_of_data == \"raw\":\n        sum_column = ColNames.initial_frequency\n    else:\n        sum_column = ColNames.final_frequency\n    # fill in string canvases\n    measure_definition = measure_definition_canva.format(type_of_data=type_of_data)\n    cond_warn_variability = cond_warn_variability_canva.format(SD=variablility, type_of_data=type_of_data)\n    cond_warn_upper_lower = cond_warn_upper_lower_canva.format(\n        X=lower_limit, Y=upper_limit, type_of_data=type_of_data\n    )\n    # define lookback period\n    window = Window.orderBy(ColNames.date).rowsBetween(-lookback_period_in_days, -1)\n    # prepare data\n    # calculate mean and std over window, and based on them UCL - for UPPER_VARIABILITY check and LCL\n    #   - for LOWER_VARIABILITY check\n    # create empty array cond_warn_condition_value column to store information about qws\n    df_prep = (\n        df_freq_distribution.groupBy(ColNames.date)\n        .agg(psf.sum(sum_column).alias(ColNames.daily_value))\n        .withColumns(\n            {\n                ColNames.average: psf.avg(ColNames.daily_value).over(window),\n                \"std_freq\": psf.stddev(ColNames.daily_value).over(window),\n                ColNames.UCL: psf.col(ColNames.average) + variablility * psf.col(\"std_freq\"),\n                ColNames.LCL: psf.col(ColNames.average) - variablility * psf.col(\"std_freq\"),\n                \"cond_warn_condition_value\": psf.array(),\n            }\n        )\n    )\n\n    df_prep = df_prep.cache()\n    # continue with QWs checks\n    # first filter data by period from [data_period_start-lookback_period_in_days, data_period_end]\n    # to [data_period_start, data_period_end]\n    # - a specified research period of QW\n    df_qw = df_prep.filter(psf.col(ColNames.date) &gt;= self.data_period_start)\n    # perform LOWER_VARIABILITY, UPPER_VARIABILITY, ABS_LOWER_LIMIT and ABS_UPPER_LIMIT checks\n    # if condition is met append information about condition-warning_text-condition_value as a string\n    # into array column 'cond_warn_condition_value'\n    df_qw = (\n        df_qw.withColumn(\n            \"cond_warn_condition_value\",\n            psf.when(\n                (psf.col(ColNames.daily_value) &lt; psf.col(ColNames.LCL)),\n                psf.array_append(\n                    psf.col(\"cond_warn_condition_value\"),\n                    psf.concat(\n                        psf.lit(f\"{cond_warn_variability}-\"),\n                        psf.col(ColNames.LCL).cast(\"string\"),\n                    ),\n                ),\n            ).otherwise(psf.col(\"cond_warn_condition_value\")),\n        )\n        .withColumn(\n            \"cond_warn_condition_value\",\n            psf.when(\n                (psf.col(ColNames.daily_value) &gt; psf.col(ColNames.UCL)),\n                psf.array_append(\n                    psf.col(\"cond_warn_condition_value\"),\n                    psf.concat(\n                        psf.lit(f\"{cond_warn_variability}-\"),\n                        psf.col(ColNames.UCL).cast(\"string\"),\n                    ),\n                ),\n            ).otherwise(psf.col(\"cond_warn_condition_value\")),\n        )\n        .withColumn(\n            \"cond_warn_condition_value\",\n            psf.when(\n                (psf.col(ColNames.daily_value) &lt; psf.lit(lower_limit)),\n                psf.array_append(\n                    psf.col(\"cond_warn_condition_value\"),\n                    psf.lit(f\"{cond_warn_upper_lower}-{str(lower_limit)}\"),\n                ),\n            ).otherwise(psf.col(\"cond_warn_condition_value\")),\n        )\n        .withColumn(\n            \"cond_warn_condition_value\",\n            psf.when(\n                (psf.col(ColNames.daily_value) &gt; psf.lit(upper_limit)),\n                psf.array_append(\n                    psf.col(\"cond_warn_condition_value\"),\n                    psf.lit(f\"{cond_warn_upper_lower}-{str(upper_limit)}\"),\n                ),\n            ).otherwise(psf.col(\"cond_warn_condition_value\")),\n        )\n    )\n    # explode array column 'cond_warn_condition_value'\n    df_qw = df_qw.withColumn(\n        \"cond_warn_condition_value\",\n        psf.explode(psf.col(\"cond_warn_condition_value\")),\n    )\n    # add some column constants\n    # split \"cond_warn_condition_value\" column into three: condition, wanring_text, conditon_value to\n    # match SilverEventDataSyntacticQualityWarningsLogTable.SCHEMA\n    # select only needed columns\n    df_qw = df_qw.withColumns(\n        {\n            ColNames.lookback_period: psf.lit(self.lookback_period),\n            ColNames.measure_definition: psf.lit(measure_definition),\n            ColNames.condition: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(0),\n            ColNames.warning_text: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(1),\n            ColNames.condition_value: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(2).cast(\"float\"),\n        }\n    ).select(self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsLogTable.ID].SCHEMA.fieldNames())\n\n    # save data for plots\n    # no filter by date because we need previous data of first days for plots\n    df_plots = df_prep.withColumns(\n        {\n            ColNames.lookback_period: psf.lit(self.lookback_period),\n            ColNames.type_of_qw: psf.lit(f\"{type_of_data}_data_size\"),\n        }\n    ).select(self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsForPlots.ID].SCHEMA.fieldNames())\n\n    return df_qw, df_plots\n</code></pre>"},{"location":"reference/components/quality/event_quality_warnings/event_quality_warnings/#components.quality.event_quality_warnings.event_quality_warnings.EventQualityWarnings.error_rate_qw","title":"<code>error_rate_qw(df_freq_distribution, lookback_period_in_days, variables, error_rate_over_average, error_rate_upper_variability, error_rate_upper_limit, error_rate_measure_definition_canva=f'{MeasureDefinitions.error_rate}', error_rate_cond_warn_over_average_canva=f'{Conditions.error_rate_over_average}-{Warnings.error_rate_over_average}', error_rate_cond_warn_upper_variability_canva=f'{Conditions.error_rate_upper_variability}-{Warnings.error_rate_upper_variability}', error_rate_cond_warn_upper_limit_canva=f'{Conditions.error_rate_upper_limit}-{Warnings.error_rate_upper_limit}', save_data_for_plots=False)</code>","text":"<p>Prepare data for error rate calculation. First fill in different string canvas,     then define window of aggregation, and calculate error_rate over the window on follwoing formula:     (Total initial frequency - Total final frequency) / Total initial frequency*100.     Parse preprocessed input to self.rate_common_qw function which calculates three types         of QWs: OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT</p> <p>Parameters:</p> Name Type Description Default <code>df_freq_distribution</code> <code>DataFrame</code> <p>df with frequency data.</p> required <code>lookback_period_in_days</code> <code>int</code> <p>number of days prior to date of interest.</p> required <code>variables</code> <code>List[str]</code> <p>list of column names by which error rate is calculated, kind of granularity level</p> required <code>error_rate_over_average</code> <code>Union[int, float]</code> <p>config param, specifies the upper limit which a daily value can not exceed its corresponding mean error rate</p> required <code>error_rate_upper_variability</code> <code>Union[int, float]</code> <p>config param, the number of SD to define the upper varibaility limit: mean_rate + SD*error_rate_upper_variability, which error rate can't exceed</p> required <code>error_rate_upper_limit</code> <code>Union[int, float]</code> <p>absolute number which error rate can not exceed</p> required <code>error_rate_measure_definition_canva</code> <code>str</code> <p>canva text to use for measure_definition column (see measure_definition.py)</p> <code>f'{error_rate}'</code> <code>error_rate_cond_warn_over_average_canva</code> <code>str</code> <p>canva text to use for over_average cases of error_rate QWs (see conditions.py and warnings.py)</p> <code>f'{error_rate_over_average}-{error_rate_over_average}'</code> <code>error_rate_cond_warn_upper_variability_canva</code> <code>str</code> <p>canva text to use for upper_variability cases of error_rate QWs (see conditions.py and warnings.py)</p> <code>f'{error_rate_upper_variability}-{error_rate_upper_variability}'</code> <code>error_rate_cond_warn_upper_limit_canva</code> <code>str</code> <p>canva text to use for upper_limit cases of error_rate QWs (see conditions.py and warnings.py)</p> <code>f'{error_rate_upper_limit}-{error_rate_upper_limit}'</code> <code>save_data_for_plots</code> <code>bool</code> <p>boolean, decide whether to store error rate and its corresponding average and upper variability limit for plots, default False</p> <code>False</code> <p>Returns:     tuple(Union[DataFrame, None]): a tuple, where first df is used for warning log table,         and the second df - for plots (could be also None)</p> Source code in <code>multimno/components/quality/event_quality_warnings/event_quality_warnings.py</code> <pre><code>def error_rate_qw(\n    self,\n    df_freq_distribution: DataFrame,\n    lookback_period_in_days: int,\n    variables: List[str],\n    error_rate_over_average: Union[int, float],\n    error_rate_upper_variability: Union[int, float],\n    error_rate_upper_limit: Union[int, float],\n    error_rate_measure_definition_canva: str = f\"{MeasureDefinitions.error_rate}\",\n    error_rate_cond_warn_over_average_canva: str = f\"{Conditions.error_rate_over_average}-{Warnings.error_rate_over_average}\",\n    error_rate_cond_warn_upper_variability_canva: str = f\"{Conditions.error_rate_upper_variability}-{Warnings.error_rate_upper_variability}\",\n    error_rate_cond_warn_upper_limit_canva: str = f\"{Conditions.error_rate_upper_limit}-{Warnings.error_rate_upper_limit}\",\n    save_data_for_plots: bool = False,\n) -&gt; Tuple[Union[DataFrame, None]]:\n    \"\"\"\n    Prepare data for error rate calculation. First fill in different string canvas,\n        then define window of aggregation, and calculate error_rate over the window on follwoing formula:\n        (Total initial frequency - Total final frequency) / Total initial frequency*100.\n        Parse preprocessed input to self.rate_common_qw function which calculates three types\n            of QWs: OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT\n\n    Args:\n        df_freq_distribution (DataFrame): df with frequency data.\n        lookback_period_in_days (int): number of days prior to date of interest.\n        variables (List[str]): list of column names by which error rate is calculated, kind of granularity level\n        error_rate_over_average (Union[int, float]): config param, specifies the upper limit which a daily value\n            can not exceed its corresponding mean error rate\n        error_rate_upper_variability (Union[int, float]): config param, the number of SD to define the upper\n            varibaility limit: mean_rate + SD*error_rate_upper_variability, which error rate can't exceed\n        error_rate_upper_limit (Union[int, float]): absolute number which error rate can not exceed\n        error_rate_measure_definition_canva (str): canva text to use for measure_definition\n            column (see measure_definition.py)\n        error_rate_cond_warn_over_average_canva (str): canva text to use for over_average cases of\n            error_rate QWs (see conditions.py and warnings.py)\n        error_rate_cond_warn_upper_variability_canva (str): canva text to use for upper_variability cases of\n            error_rate QWs (see conditions.py and warnings.py)\n        error_rate_cond_warn_upper_limit_canva (str): canva text to use for upper_limit cases of error_rate\n            QWs (see conditions.py and warnings.py)\n        save_data_for_plots (bool): boolean, decide whether to store error rate and its corresponding average\n            and upper variability limit for plots, default False\n    Returns:\n        tuple(Union[DataFrame, None]): a tuple, where first df is used for warning log table,\n            and the second df - for plots (could be also None)\n    \"\"\"\n    # fill in all string comnstants with relevant information\n    # based on variables, error_rate_over_average, error_rate_upper_variability, error_rate_upper_limit args\n    error_rate_measure_definition = error_rate_measure_definition_canva.format(variables=\"&amp;\".join(variables))\n\n    error_rate_cond_warn_over_average = error_rate_cond_warn_over_average_canva.format(\n        variables=\"&amp;\".join(variables), X=error_rate_over_average\n    )\n    error_rate_cond_warn_upper_variability = error_rate_cond_warn_upper_variability_canva.format(\n        variables=\"&amp;\".join(variables), SD=error_rate_upper_variability\n    )\n    error_rate_cond_warn_upper_limit = error_rate_cond_warn_upper_limit_canva.format(\n        variables=\"&amp;\".join(variables), X=error_rate_upper_limit\n    )\n    # qws of error rate by date is calculated based on previous days\n    if variables == [ColNames.date]:\n        window = Window.orderBy(ColNames.date).rowsBetween(-lookback_period_in_days, -1)\n    else:\n        # qws of error rate by date and other colum(s) is calculated over all \"data points\" of the same date\n        window = Window.partitionBy(ColNames.date)\n    # calculate error rate, a.k.a daily_value\n    df_qw = (\n        df_freq_distribution.groupBy(*variables)\n        .agg(\n            psf.sum(ColNames.initial_frequency).alias(\"sum_init_freq\"),\n            psf.sum(ColNames.final_frequency).alias(\"sum_final_freq\"),\n        )\n        .withColumn(\n            ColNames.daily_value,\n            (psf.col(\"sum_init_freq\") - psf.col(\"sum_final_freq\")) / psf.col(\"sum_init_freq\") * 100,\n        )\n    )\n    # using self.rate_common_qw funciton calculate three types of QWs\n    #   -&gt; OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT\n    # return a tuple of (df_log, df_plots | None)\n    qw_result = self.rate_common_qw(\n        df_qw,\n        window,\n        error_rate_upper_variability,\n        error_rate_over_average,\n        error_rate_upper_limit,\n        error_rate_measure_definition,\n        error_rate_cond_warn_upper_variability,\n        error_rate_cond_warn_over_average,\n        error_rate_cond_warn_upper_limit,\n        save_data_for_plots,\n    )\n\n    return qw_result\n</code></pre>"},{"location":"reference/components/quality/event_quality_warnings/event_quality_warnings/#components.quality.event_quality_warnings.event_quality_warnings.EventQualityWarnings.error_type_rate_qw","title":"<code>error_type_rate_qw(df_qa_by_column, df_freq_distribution, field_name, error_type, lookback_period_in_days, error_type_rate_over_average, error_type_rate_upper_variability, error_type_rate_upper_limit, error_type_rate_measure_definition_canva=f'{MeasureDefinitions.error_type_rate}', error_type_rate_cond_warn_over_average_canva=f'{Conditions.error_type_rate_over_average}-{Warnings.error_type_rate_over_average}', error_type_rate_cond_warn_upper_variability_canva=f'{Conditions.error_type_rate_upper_variability}-{Warnings.error_type_rate_upper_variability}', error_type_rate_cond_warn_upper_limit_canva=f'{Conditions.error_type_rate_upper_limit}-{Warnings.error_type_rate_upper_limit}')</code>","text":"<p>Prepare data for error type rate calculation. First fill in different string canvas, then based     on field name and error type calculate their corresponding error rate using formula:     number of errors of this error_type&amp;field_name combo / Total initial frequency *100 (BY DATE).     Parse preprocessed input along with window (which is a lookback period)     to self.rate_common_qw function which calculates three types of QWs:     OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT</p> <p>Parameters:</p> Name Type Description Default <code>df_qa_by_column</code> <code>DataFrame</code> <p>df with qa by column data.</p> required <code>df_freq_distribution</code> <code>DataFrame</code> <p>df with frequency data.</p> required <code>field_name</code> <code>str | None</code> <p>config param, the name of column of which to check error_type.</p> required <code>error_type</code> <code>str</code> <p>config param, the name of error type.</p> required <code>lookback_period_in_days</code> <code>int</code> <p>number of days prior to date of intrest.</p> required <code>error_type_rate_over_average</code> <code>Union[int, float]</code> <p>config param, specifies the upper limit over which daily value can not exceed its corresponding mean error rate.</p> required <code>error_type_rate_upper_variability</code> <code>Union[int, float]</code> <p>config param, the number of SD to define the upper varibaility limit: mean_rate + SD*error_type_rate_upper_variability, which daily value can not exceed</p> required <code>error_type_rate_upper_limit</code> <code>Union[int, float]</code> <p>absolute number which daily value can not exceed</p> required <code>error_type_rate_measure_definition_canva</code> <code>str</code> <p>canva text to use for measure_definition column (see measure_definition.py)</p> <code>f'{error_type_rate}'</code> <code>error_type_rate_cond_warn_over_average_canva</code> <code>str</code> <p>canva text to use for over_average cases of error_type_rate QWs (see conditions.py and warnings.py)</p> <code>f'{error_type_rate_over_average}-{error_type_rate_over_average}'</code> <code>error_type_rate_cond_warn_upper_variability_canva</code> <code>str</code> <p>canva text to use for upper_variability cases of error_type_rate QWs (see conditions.py and warnings.py)</p> <code>f'{error_type_rate_upper_variability}-{error_type_rate_upper_variability}'</code> <code>error_type_rate_cond_warn_upper_limit_canva</code> <code>str</code> <p>canva text to use for upper_limit cases of error_type_rate QWs (see conditions.py and warnings.py)</p> <code>f'{error_type_rate_upper_limit}-{error_type_rate_upper_limit}'</code> <p>Returns:     tuple(Union[DataFrame, None]): a tuple, where first df is used for warning log table,         and the second df - for plots, but since save_data_for_plots always False, output=None</p> Source code in <code>multimno/components/quality/event_quality_warnings/event_quality_warnings.py</code> <pre><code>def error_type_rate_qw(\n    self,\n    df_qa_by_column: DataFrame,\n    df_freq_distribution: DataFrame,\n    field_name: Union[str, None],\n    error_type: str,\n    lookback_period_in_days: int,\n    error_type_rate_over_average: Union[int, float],\n    error_type_rate_upper_variability: Union[int, float],\n    error_type_rate_upper_limit: Union[int, float],\n    error_type_rate_measure_definition_canva: str = f\"{MeasureDefinitions.error_type_rate}\",\n    error_type_rate_cond_warn_over_average_canva: str = f\"{Conditions.error_type_rate_over_average}-{Warnings.error_type_rate_over_average}\",\n    error_type_rate_cond_warn_upper_variability_canva: str = f\"{Conditions.error_type_rate_upper_variability}-{Warnings.error_type_rate_upper_variability}\",\n    error_type_rate_cond_warn_upper_limit_canva: str = f\"{Conditions.error_type_rate_upper_limit}-{Warnings.error_type_rate_upper_limit}\",\n) -&gt; Tuple[Union[DataFrame, None]]:\n    \"\"\"\n    Prepare data for error type rate calculation. First fill in different string canvas, then based\n        on field name and error type calculate their corresponding error rate using formula:\n        number of errors of this error_type&amp;field_name combo / Total initial frequency *100 (BY DATE).\n        Parse preprocessed input along with window (which is a lookback period)\n        to self.rate_common_qw function which calculates three types of QWs:\n        OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT\n\n\n    Args:\n        df_qa_by_column (DataFrame): df with qa by column data.\n        df_freq_distribution (DataFrame): df with frequency data.\n        field_name (str | None): config param, the name of column of which to check error_type.\n        error_type (str): config param, the name of error type.\n        lookback_period_in_days (int): number of days prior to date of intrest.\n        error_type_rate_over_average (Union[int, float]): config param, specifies the upper limit over which daily\n            value can not exceed its corresponding mean error rate.\n        error_type_rate_upper_variability (Union[int, float]): config param, the number of SD to define the upper\n            varibaility limit: mean_rate + SD*error_type_rate_upper_variability, which daily value can not exceed\n        error_type_rate_upper_limit (Union[int, float]): absolute number which daily value can not exceed\n        error_type_rate_measure_definition_canva (str): canva text to use for measure_definition\n            column (see measure_definition.py)\n        error_type_rate_cond_warn_over_average_canva (str): canva text to use for over_average cases of\n            error_type_rate QWs (see conditions.py and warnings.py)\n        error_type_rate_cond_warn_upper_variability_canva (str): canva text to use for upper_variability\n            cases of error_type_rate QWs (see conditions.py and warnings.py)\n        error_type_rate_cond_warn_upper_limit_canva (str): canva text to use for upper_limit cases of\n            error_type_rate QWs (see conditions.py and warnings.py)\n    Returns:\n        tuple(Union[DataFrame, None]): a tuple, where first df is used for warning log table,\n            and the second df - for plots, but since save_data_for_plots always False, output=None\n    \"\"\"\n    # based on error_type, error_type_rate_over_average, error_type_rate_upper_variability,\n    #  error_type_rate_upper_limit\n    # fill in string canvases\n    colname_error_type, error_type_qw_name = self.dict_error_type_info[error_type]\n\n    error_type_rate_measure_definition = error_type_rate_measure_definition_canva.format(\n        error_type_name=error_type_qw_name, field_name=str(field_name)\n    )\n\n    error_type_rate_cond_warn_over_average = error_type_rate_cond_warn_over_average_canva.format(\n        error_type_name=error_type_qw_name,\n        field_name=str(field_name),\n        X=error_type_rate_over_average,\n    )\n    error_type_rate_cond_warn_upper_variability = error_type_rate_cond_warn_upper_variability_canva.format(\n        error_type_name=error_type_qw_name,\n        field_name=str(field_name),\n        SD=error_type_rate_upper_variability,\n    )\n    error_type_rate_cond_warn_upper_limit = error_type_rate_cond_warn_upper_limit_canva.format(\n        error_type_name=error_type_qw_name,\n        field_name=str(field_name),\n        X=error_type_rate_upper_limit,\n    )\n    # for error_type that have more then one or applicable columns\n    # filter df_qa_by_column by field_name and error_type\n    if field_name is not None:\n        df_qa_by_column = df_qa_by_column.filter(\n            (psf.col(ColNames.variable) == field_name) &amp; (psf.col(ColNames.type_of_error) == colname_error_type)\n        ).select(ColNames.date, ColNames.value)\n    else:\n        # for error_types which technically do not belong specifically to one of event\n        # columns filter only by error_type (e.g. no_location error_type)\n        df_qa_by_column = df_qa_by_column.filter(psf.col(ColNames.type_of_error) == colname_error_type).select(\n            ColNames.date, ColNames.value\n        )\n    # calculate total daily initial frequency\n    df_freq_distribution = (\n        df_freq_distribution.groupby(ColNames.date)\n        .agg(psf.sum(ColNames.initial_frequency).alias(\"sum_init_freq\"))\n        .select(ColNames.date, \"sum_init_freq\")\n    )\n    # for each date combine two type of information number of errors and total daily initial frequency\n    df_combined = df_qa_by_column.join(df_freq_distribution, on=ColNames.date, how=\"inner\")\n    # for each date calculate error_type_rate, a.k.a daily_value\n    df_temp = df_combined.withColumn(\n        ColNames.daily_value,\n        (psf.col(ColNames.value) / psf.col(\"sum_init_freq\")) * 100,\n    )\n\n    # qws will be caluclated based on previous days\n    window = Window.orderBy(ColNames.date).rowsBetween(-lookback_period_in_days, -1)\n    # using self.rate_common_qw funciton calculate three types of QWs -&gt;\n    # OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT\n    # return a tuple of (df_log, None)\n    qw_result = self.rate_common_qw(\n        df_temp,\n        window,\n        error_type_rate_upper_variability,\n        error_type_rate_over_average,\n        error_type_rate_upper_limit,\n        error_type_rate_measure_definition,\n        error_type_rate_cond_warn_upper_variability,\n        error_type_rate_cond_warn_over_average,\n        error_type_rate_cond_warn_upper_limit,\n    )\n    return qw_result\n</code></pre>"},{"location":"reference/components/quality/event_quality_warnings/event_quality_warnings/#components.quality.event_quality_warnings.event_quality_warnings.EventQualityWarnings.rate_common_qw","title":"<code>rate_common_qw(df_temp, window, rate_upper_variability, rate_over_average, rate_upper_limit, measure_definition, cond_warn_upper_variability, cond_warn_over_average, cond_warn_upper_limit, save_data_for_plots=False)</code>","text":"<p>Take input df with \"daily_value\" column, and calculates three types of QWs: OVER_AVERAGE - for each row first based on specified window take mean of values, and then check if     daily_value exceeds mean by more than rate_over_average UPPER_VARIABILITY - for each row using already calculated mean compute upper variability limit which is     mean + SD*rate_upper_variability, check if  daily_value exceeds it ABS_UPPER_LIMIT - check if daily_value exceeds absolute number rate_upper_limit All three QWs depend on specified thresholds, if daily_value exceeds one of calculated values it will     store cond-warn-condition_value information in array column, some rows may have several QWs. In the end array column is exploded and cond-warn-condition_value     information is split into three corresponding columns. The function returns almost ready df for SilverEventDataSyntacticQualityWarningsLogTable DO, and based     on save_data_for_plots arg returns either almost ready data for plots or None</p> <p>Parameters:</p> Name Type Description Default <code>df_temp</code> <code>DataFrame</code> <p>temprory data that must have daily_value column to be used in further QW calculations</p> required <code>window</code> <code>Window</code> <p>a window within which perform aggregation</p> required <code>rate_upper_variability</code> <code>int | float</code> <p>config param, specifies the upper limit over which daily value  can not exceed its corresponding mean error rate</p> required <code>rate_over_average</code> <code>int | float</code> <p>config param, the number of SD to define the upper varibaility  limit: mean_rate + SD*error_rate_upper_variability, which daily value can not exceed</p> required <code>rate_upper_limit</code> <code>int | float</code> <p>absolute number which daily value can not exceed</p> required <code>measure_definition</code> <code>str</code> <p>canva text to use for measure_definition column (see measure_definition.py)</p> required <code>cond_warn_over_average</code> <code>str</code> <p>canva text to use for over_average cases (see conditions.py and warnings.py)</p> required <code>cond_warn_upper_variability</code> <code>str</code> <p>canva text to use for upper_variability cases (see conditions.py and warnings.py)</p> required <code>cond_warn_upper_limit</code> <code>str</code> <p>canva text to use for upper_limit cases (see conditions.py and warnings.py)</p> required <code>save_data_for_plots</code> <code>bool</code> <p>boolean, decide whether to store daily_value and its corresponding average and upper variability limit for plots. Defaults to False.</p> <code>False</code> <p>Returns:      tuple(Union[DataFrame, None]): a tuple, where first df is used for         warning log table, and the second df - for plots</p> Source code in <code>multimno/components/quality/event_quality_warnings/event_quality_warnings.py</code> <pre><code>def rate_common_qw(\n    self,\n    df_temp: DataFrame,\n    window: Window,\n    rate_upper_variability: Union[int, float],\n    rate_over_average: Union[int, float],\n    rate_upper_limit: Union[int, float],\n    measure_definition: str,\n    cond_warn_upper_variability: str,\n    cond_warn_over_average: str,\n    cond_warn_upper_limit: str,\n    save_data_for_plots: bool = False,\n) -&gt; Tuple[Union[DataFrame, None]]:\n    \"\"\"\n    Take input df with \"daily_value\" column, and calculates three types of QWs:\n    OVER_AVERAGE - for each row first based on specified window take mean of values, and then check if\n        daily_value exceeds mean by more than rate_over_average\n    UPPER_VARIABILITY - for each row using already calculated mean compute upper variability limit which is\n        mean + SD*rate_upper_variability, check if  daily_value exceeds it\n    ABS_UPPER_LIMIT - check if daily_value exceeds absolute number rate_upper_limit\n    All three QWs depend on specified thresholds, if daily_value exceeds one of calculated values it will\n        store cond-warn-condition_value information in array column,\n    some rows may have several QWs. In the end array column is exploded and cond-warn-condition_value\n        information is split into three corresponding columns.\n    The function returns almost ready df for SilverEventDataSyntacticQualityWarningsLogTable DO, and based\n        on save_data_for_plots arg returns either almost ready data for plots or None\n\n    Args:\n        df_temp (DataFrame): temprory data that must have daily_value column to\n            be used in further QW calculations\n        window (Window): a window within which perform aggregation\n        rate_upper_variability (int|float): config param, specifies the upper limit over which daily value\n             can not exceed its corresponding mean error rate\n        rate_over_average (int|float): config param, the number of SD to define the upper varibaility\n             limit: mean_rate + SD*error_rate_upper_variability, which daily value can not exceed\n        rate_upper_limit (int|float): absolute number which daily value can not exceed\n        measure_definition (str): canva text to use for measure_definition column (see measure_definition.py)\n        cond_warn_over_average (str): canva text to use for over_average cases (see conditions.py and warnings.py)\n        cond_warn_upper_variability (str): canva text to use for\n            upper_variability cases (see conditions.py and warnings.py)\n        cond_warn_upper_limit (str): canva text to use for upper_limit cases (see conditions.py and warnings.py)\n        save_data_for_plots (bool): boolean, decide whether to store daily_value and its corresponding average\n            and upper variability limit for plots. Defaults to False.\n    Returns:\n         tuple(Union[DataFrame, None]): a tuple, where first df is used for\n            warning log table, and the second df - for plots\n    \"\"\"\n    # prepare data\n    # calculate mean and std over window, and based on them UCL - for UPPER_VARIABILITY check\n    # ratio_perc - for OVER_AVERAGE check\n    # create empty array cond_warn_condition_value column to store inromation about qws\n    df_prep = df_temp.withColumns(\n        {\n            ColNames.average: psf.avg(ColNames.daily_value).over(window),\n            \"std_rate\": psf.stddev(ColNames.daily_value).over(window),\n            \"ratio_perc\": (psf.col(ColNames.daily_value) / psf.col(ColNames.average)) * 100,\n            ColNames.UCL: psf.col(ColNames.average) + rate_upper_variability * psf.col(\"std_rate\"),\n            \"cond_warn_condition_value\": psf.array(),\n        }\n    )\n    # if save_data_for_plots=True, add some new columns with constant values\n    # and select only applicable to SilverEventDataSyntacticQualityWarningsForPlots.SCHEMA\n    # else - return None\n    if save_data_for_plots:\n        df_prep = df_prep.cache()\n        df_plots = df_prep.withColumns(\n            {\n                ColNames.lookback_period: psf.lit(self.lookback_period),\n                ColNames.type_of_qw: psf.lit(\"error_rate\"),\n                ColNames.LCL: psf.lit(None).cast(\"float\"),\n            }\n        ).select(\n            self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsForPlots.ID].SCHEMA.fieldNames()\n        )\n    else:\n        df_plots = None\n\n    # continue with QWs checks\n    # first filter temp data by period from [data_period_start-lookback_period_in_days, data_period_end]\n    #  to [data_period_start, data_period_end]\n    # filter is aaplied after plot block because the first days of research period needs previous data to plot\n    df_qw = df_prep.filter(psf.col(ColNames.date) &gt;= self.data_period_start)\n    # perform UPPER_VARIABILITY, OVER_AVERAGE, and ABS_UPPER_LIMIT checks\n    # if condition is met store information about condition-warning_text-condition_value as a string into\n    # array column 'cond_warn_condition_value'\n    df_qw = (\n        df_qw.withColumn(\n            \"cond_warn_condition_value\",\n            psf.when(\n                psf.col(ColNames.daily_value) &gt; psf.col(ColNames.UCL),\n                psf.array_append(\n                    psf.col(\"cond_warn_condition_value\"),\n                    psf.concat(\n                        psf.lit(f\"{cond_warn_upper_variability}-\"),\n                        psf.col(ColNames.UCL).cast(\"string\"),\n                    ),\n                ),\n            ).otherwise(psf.col(\"cond_warn_condition_value\")),\n        )\n        .withColumn(\n            \"cond_warn_condition_value\",\n            psf.when(\n                psf.col(\"ratio_perc\") &gt; psf.lit(100 + rate_over_average),\n                psf.array_append(\n                    psf.col(\"cond_warn_condition_value\"),\n                    psf.lit(f\"{cond_warn_over_average}-{str(rate_over_average)}\"),\n                ),\n            ).otherwise(psf.col(\"cond_warn_condition_value\")),\n        )\n        .withColumn(\n            \"cond_warn_condition_value\",\n            psf.when(\n                psf.col(ColNames.daily_value) &gt; psf.lit(rate_upper_limit),\n                psf.array_append(\n                    psf.col(\"cond_warn_condition_value\"),\n                    psf.lit(f\"{cond_warn_upper_limit}-{str(rate_upper_limit)}\"),\n                ),\n            ).otherwise(psf.col(\"cond_warn_condition_value\")),\n        )\n    )\n    # explode array column 'cond_warn_condition_value'\n    df_qw = df_qw.withColumn(\n        \"cond_warn_condition_value\",\n        psf.explode(psf.col(\"cond_warn_condition_value\")),\n    )\n    # add some column constants\n    # split \"cond_warn_condition_value\" column into three: condition, wanring_text, conditon_value to match\n    # SilverEventDataSyntacticQualityWarningsLogTable.SCHEMA\n    # select only needed columns\n    df_qw = df_qw.withColumns(\n        {\n            ColNames.lookback_period: psf.lit(self.lookback_period),\n            ColNames.measure_definition: psf.lit(measure_definition),\n            ColNames.condition: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(0),\n            ColNames.warning_text: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(1),\n            ColNames.condition_value: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(2).cast(\"float\"),\n        }\n    ).select(self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsLogTable.ID].SCHEMA.fieldNames())\n\n    return (df_qw, df_plots)\n</code></pre>"},{"location":"reference/components/quality/event_quality_warnings/event_quality_warnings/#components.quality.event_quality_warnings.event_quality_warnings.EventQualityWarnings.save_quality_warnings_output","title":"<code>save_quality_warnings_output(dfs_qw, output_do)</code>","text":"<p>Concatenates all elements in dfs_qw list, adjustes to schema of output_do, and using write     method of output_do stores the result</p> <p>Parameters:</p> Name Type Description Default <code>dfs_qw</code> <code>list</code> <p>description</p> required <code>output_do</code> <code>SilverEventDataSyntacticQualityWarningsLogTable | SilverEventDataSyntacticQualityWarningsForPlots</code> <p>description</p> required Source code in <code>multimno/components/quality/event_quality_warnings/event_quality_warnings.py</code> <pre><code>def save_quality_warnings_output(\n    self,\n    dfs_qw: List[Union[DataFrame, None]],\n    output_do: Union[\n        SilverEventDataSyntacticQualityWarningsLogTable, SilverEventDataSyntacticQualityWarningsForPlots\n    ],\n):\n    \"\"\"\n    Concatenates all elements in dfs_qw list, adjustes to schema of output_do, and using write\n        method of output_do stores the result\n\n    Args:\n        dfs_qw (list): _description_\n        output_do (SilverEventDataSyntacticQualityWarningsLogTable | \\\n            SilverEventDataSyntacticQualityWarningsForPlots): _description_\n    \"\"\"\n\n    output_do.df = reduce(lambda x, y: x.union(y), dfs_qw)\n\n    output_do.df = self.spark.createDataFrame(output_do.df.rdd, output_do.SCHEMA)\n\n    output_do.write()\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/","title":"network_quality_warnings","text":""},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/","title":"network_quality_warnings","text":"<p>Module that generates the quality warnings associated to the syntactic checks/cleaning of the raw MNO Network Topology Data.</p>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings","title":"<code>NetworkQualityWarnings</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class that produces the log tables and data required for plotting associated to Network Topology Data cleaning/syntactic checks.</p> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>class NetworkQualityWarnings(Component):\n    \"\"\"\n    Class that produces the log tables and data required for plotting associated to Network Topology Data\n    cleaning/syntactic checks.\n    \"\"\"\n\n    COMPONENT_ID = \"NetworkQualityWarnings\"\n\n    PERIOD_DURATION = {\"week\": 7, \"month\": 30, \"quarter\": 90}\n\n    TITLE = \"MNO Network Topology Data Quality Warnings\"\n\n    MEASURE_DEFINITION = {\n        \"SIZE_RAW_DATA\": \"Value of the size of the raw data object\",\n        \"SIZE_CLEAN_DATA\": \"Value of the size of the clean data object\",\n        \"TOTAL_ERROR_RATE\": \"Error rate\",\n        \"Missing_value_RATE\": \"Missing rate value of {field_name}\".format,\n        \"Out_of_range_RATE\": \"Out of range rate of {field_name}\".format,\n        \"Parsing_error_RATE\": \"Parsing error rate of {field_name}\".format,\n    }\n\n    ERROR_TYPE = {\n        \"Missing_value_RATE\": NetworkErrorType.NULL_VALUE,\n        \"Out_of_range_RATE\": NetworkErrorType.OUT_OF_RANGE,\n        \"Parsing_error_RATE\": NetworkErrorType.CANNOT_PARSE,\n    }\n\n    CONDITION = {\n        \"Missing_value_RATE\": {\n            \"AVERAGE\": \"Missing value rate of {field_name} is over the previous period average by more than {value} %\".format,\n            \"VARIABILITY\": (\n                \"Missing value rate of {field_name} is over the upper control limit calculated on the basis of average and standard deviation \"\n                + \"of the distribution of the missing value rate of {field_name} in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\"\n            ).format,\n            \"ABS_VALUE_UPPER_LIMIT\": \"Missing value rate of {field_name} is over the value {value}\".format,\n        },\n        \"Out_of_range_RATE\": {\n            \"AVERAGE\": \"Out of range rate of {field_name} is over the previous period average by more than {value} %\".format,\n            \"VARIABILITY\": (\n                \"Out of range rate of {field_name} is over the upper control limit calculated on the basis of average and standard deviation \"\n                + \"of the distribution of the out of range rate of {field_name} in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\"\n            ).format,\n            \"ABS_VALUE_UPPER_LIMIT\": \"Out of range rate of {field_name} is over the value {value} %\".format,\n        },\n        \"Parsing_error_RATE\": {\n            \"AVERAGE\": \"Parsing error rate of {field_name} is over the previous period average by more than {value} %\".format,\n            \"VARIABILITY\": (\n                \"Parsing error rate of {field_name} is over the upper control limit calculated on the basis of average and standard deviation \"\n                + \"of the distribution of the parsing error rate of {field_name} in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\"\n            ).format,\n            \"ABS_VALUE_UPPER_LIMIT\": \"Parsing error rate of {field_name} is over the value {value}\".format,\n        },\n    }\n\n    WARNING_MESSAGE = {\n        \"Missing_value_RATE\": {\n            \"AVERAGE\": \"The missing value rate of {field_name} after the syntactic check procedure is unexpectedly high with respect to the previous period.\".format,\n            \"VARIABILITY\": (\n                \"The missing value rate of {field_name} after the syntactic check procedure is unexpectedly high \"\n                + \"with respect to previous period taking into account its usual variability.\"\n            ).format,\n            \"ABS_VALUE_UPPER_LIMIT\": \"The missing value rate of {field_name} is over the threshold\".format,\n        },\n        \"Out_of_range_RATE\": {\n            \"AVERAGE\": \"The out of range rate of {field_name} after the syntactic check procedure is unexpectedly high with respect to the previous period.\".format,\n            \"VARIABILITY\": (\n                \"The out of range of {field_name} after the syntactic check procedure is unexpectedly high \"\n                + \"with respect to previous period taking into account its usual variability.\"\n            ).format,\n            \"ABS_VALUE_UPPER_LIMIT\": \"The out of range rate of {field_name} is over the threshold\".format,\n        },\n        \"Parsing_error_RATE\": {\n            \"AVERAGE\": \"The parsing error rate of {field_name} after the syntactic check procedure is unexpectedly high with respect to the previous period.\".format,\n            \"VARIABILITY\": (\n                \"The parsing error of {field_name} after the syntactic check procedure is unexpectedly high \"\n                + \"with respect to previous period taking into account its usual variability.\"\n            ).format,\n            \"ABS_VALUE_UPPER_LIMIT\": \"The parsing error rate of {field_name} is over the threshold\".format,\n        },\n    }\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.timestamp = datetime.datetime.now()\n\n        # Read date of study\n        self.date_format = self.config.get(self.COMPONENT_ID, \"date_format\")\n        date_of_study = self.config.get(self.COMPONENT_ID, \"date_of_study\")\n\n        try:\n            self.date_of_study = datetime.datetime.strptime(date_of_study, self.date_format).date()\n        except ValueError as e:\n            self.logger.error(str(e), exc_info=True)\n            raise e\n\n        # Read lookback period\n        self.lookback_period = self.config.get(self.COMPONENT_ID, \"lookback_period\")\n\n        if self.lookback_period not in [\"week\", \"month\", \"quarter\"]:\n            error_msg = (\n                \"Configuration parameter `lookback_period` must be one of `week`, `month`, or `quarter`, \"\n                f\"but {self.lookback_period} was passed\"\n            )\n\n            self.logger.error(error_msg)\n            raise ValueError(error_msg)\n\n        self.lookback_dates = [\n            self.date_of_study - datetime.timedelta(days=d)\n            for d in range(1, self.PERIOD_DURATION[self.lookback_period] + 1)\n        ]\n\n        self.lookback_period_start = min(self.lookback_dates)\n        self.lookback_period_end = max(self.lookback_dates)\n\n        # Read thresholds and use read values instead of default ones when appropriate\n        self.thresholds = self.get_thresholds()\n\n        self.warnings = []\n\n        self.plots_data = dict()\n\n    def get_thresholds(self) -&gt; dict:\n        \"\"\"\n        Method that reads the threshold-related parameters, contained in the config file,\n        and saves them in memory to use them instead of the ones specified by default.\n        Raises:\n            ValueError: non-numerical value that cannot be parsed to float has been used in\n                the config file\n            ValueError: Negative value for a given parameter has been given, when only\n                non-negative values make sense and are thus allowed.\n\n        Returns:\n            thresholds: dict containing the different thresholds used for computing the\n                quality warnings, with the same structure as\n                multimno.core.constants.network_default_thresholds.NETWORK_DEFAULT_THRESHOLDS\n        \"\"\"\n        config_thresholds = self.config.geteval(self.COMPONENT_ID, \"thresholds\")\n        thresholds = NETWORK_DEFAULT_THRESHOLDS\n\n        for error_key in config_thresholds.keys():\n            if error_key not in NETWORK_DEFAULT_THRESHOLDS:\n                self.logger.info(f\"Parameter key {error_key} unknown -- ignored\")\n                continue\n\n            if error_key in [\"SIZE_RAW_DATA\", \"SIZE_CLEAN_DATA\", \"TOTAL_ERROR_RATE\"]:\n                for param_key, val in config_thresholds[error_key].items():\n                    if param_key not in NETWORK_DEFAULT_THRESHOLDS[error_key]:\n                        self.logger.info(f\"Unknown parameter {param_key} under {error_key} -- ignored\")\n                        continue\n\n                    try:\n                        val = float(val)\n                    except ValueError as e:\n                        error_msg = (\n                            f\"Expected numeric value for parameter {param_key} under {error_key} - got {val} instead.\"\n                        )\n                        self.logger.error(error_msg)\n                        raise e\n\n                    if val &lt; 0:\n                        error_msg = f\"Parameter {param_key} under {error_key} must be non-negative - got {val}\"\n                        self.logger.error(error_msg)\n                        raise ValueError(error_msg)\n\n                    thresholds[error_key][param_key] = val\n\n            else:  # then is one of 'Missing_value_RATE', 'Out_of_range_RATE', 'Parsing_error_RATE'\n                for field_name in config_thresholds[error_key]:\n                    if field_name not in NETWORK_DEFAULT_THRESHOLDS[error_key]:\n                        self.logger.info(f\"Unknown field name {field_name} under {error_key} -- ignored\")\n                        continue\n\n                    for param_key, val in config_thresholds[error_key][field_name].items():\n                        if param_key not in NETWORK_DEFAULT_THRESHOLDS[error_key][field_name]:\n                            self.logger.info(\n                                f'Unknown parameter {param_key} under {error_key}[\"{field_name}\"] -- ignored'\n                            )\n                            continue\n\n                        try:\n                            val = float(val)\n                        except ValueError as e:\n                            error_msg = f'Expected numeric value for parameter {param_key} under {error_key}[\"{field_name}\"] - got {val} instead.'\n                            self.logger.error(error_msg)\n                            raise e\n\n                        if val &lt; 0:\n                            error_msg = f'Parameter {param_key} under {error_key}[\"{field_name}\"] must be non-negative - got {val}'\n                            self.logger.error(error_msg)\n                            raise ValueError(error_msg)\n\n                        thresholds[error_key][field_name][param_key] = val\n\n        return thresholds\n\n    def initalize_data_objects(self):\n        input_silver_quality_metrics_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"network_syntactic_quality_metrics_by_column\"\n        )\n        output_silver_log_table_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"network_syntactic_quality_warnings_log_table\"\n        )\n\n        output_silver_line_plot_data_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"network_syntactic_quality_warnings_line_plot_data\"\n        )\n\n        output_silver_pie_plot_data_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"network_syntactic_quality_warnings_pie_plot_data\"\n        )\n\n        silver_quality_metrics = SilverNetworkDataQualityMetricsByColumn(self.spark, input_silver_quality_metrics_path)\n\n        silver_log_table = SilverNetworkDataSyntacticQualityWarningsLogTable(self.spark, output_silver_log_table_path)\n\n        silver_line_plot_data = SilverNetworkSyntacticQualityWarningsLinePlotData(\n            self.spark, output_silver_line_plot_data_path\n        )\n\n        silver_pie_plot_data = SilverNetworkSyntacticQualityWarningsPiePlotData(\n            self.spark, output_silver_pie_plot_data_path\n        )\n\n        self.input_data_objects = {silver_quality_metrics.ID: silver_quality_metrics}\n        self.output_data_objects = {\n            silver_log_table.ID: silver_log_table,\n            silver_line_plot_data.ID: silver_line_plot_data,\n            silver_pie_plot_data.ID: silver_pie_plot_data,\n        }\n\n    def transform(self):\n\n        # Check if both the date of study and the specified lookback period dates are in file\n        self.check_needed_dates()\n\n        lookback_stats, lookback_initial_rows, lookback_final_rows = self.get_lookback_period_statistics()\n\n        today_values = self.get_study_date_values()\n\n        raw_average, raw_UCL, raw_LCL = self.raw_size_warnings(lookback_stats, today_values)\n\n        clean_average, clean_UCL, clean_LCL = self.clean_size_warnings(lookback_stats, today_values)\n\n        error_rate, error_rate_avg, error_rate_UCL = self.error_rate_warnings(\n            lookback_initial_rows, lookback_final_rows, today_values\n        )\n\n        self.all_specific_error_warnings(lookback_stats, today_values)\n\n        self.output_data_objects[SilverNetworkDataSyntacticQualityWarningsLogTable.ID].df = self.spark.createDataFrame(\n            self.warnings, SilverNetworkDataSyntacticQualityWarningsLogTable.SCHEMA\n        )\n\n        lookback_initial_rows[self.date_of_study] = today_values[None][NetworkErrorType.INITIAL_ROWS]\n        lookback_final_rows[self.date_of_study] = today_values[None][NetworkErrorType.FINAL_ROWS]\n\n        self.create_plots_data(\n            lookback_initial_rows=lookback_initial_rows,\n            lookback_final_rows=lookback_final_rows,\n            today_values=today_values,\n            error_rate=error_rate,\n            raw_average=raw_average,\n            clean_average=clean_average,\n            error_rate_avg=error_rate_avg,\n            raw_UCL=raw_UCL,\n            clean_UCL=clean_UCL,\n            error_rate_UCL=error_rate_UCL,\n            raw_LCL=raw_LCL,\n            clean_LCL=clean_LCL,\n        )\n\n    def check_needed_dates(self) -&gt; None:\n        \"\"\"\n        Method that checks if both the date of study and the dates necessary to generate\n        the quality warnings, specified through the lookback_period parameter, are present\n        in the input data.\n        \"\"\"\n\n        # Collect all distinct dates in the input quality metrics within the needed range\n        metrics = self.input_data_objects[SilverNetworkDataQualityMetricsByColumn.ID].df\n        dates = (\n            metrics.filter(\n                F.col(\"date\")\n                # left- and right- inclusive\n                .between(\n                    self.date_of_study - datetime.timedelta(days=self.PERIOD_DURATION[self.lookback_period]),\n                    self.date_of_study,\n                )\n            )\n            .select(F.col(ColNames.date))\n            .distinct()\n            .collect()\n        )\n\n        dates = [row[ColNames.date] for row in dates]\n\n        if self.date_of_study not in dates:\n            raise ValueError(\n                f\"The date of study, {self.date_of_study}, is not present in the input Quality Metrics data\"\n            )\n\n        if set(self.lookback_dates).intersection(set(dates)) != set(self.lookback_dates):\n            error_msg = f\"\"\"\n                The following dates from the lookback period are not present in the\n                input Quality Metrics data:\n                {\n                    sorted(\n                        map(\n                            lambda x: x.strftime(self.date_format),\n                            set(self.lookback_dates).difference(set(dates))\n                        )\n                    )\n                }\"\"\"\n\n            self.logger.error(error_msg)\n            raise ValueError(error_msg)\n\n    def get_lookback_period_statistics(self) -&gt; dict:\n        \"\"\"Method that computes the necessary statistics (average and standard deviation) from the\n        Quality Metrics of the lookback period.\n\n        Returns:\n            statistics (dict): dictionary containing said necessary statistics, with the following\n                structure:\n                {\n                    field_name1: {\n                        type_error1: {\n                            'average': 12.2,\n                            'stddev': 17.5\n                        },\n                        type_error2 : {\n                            'average': 4.5,\n                            'stddev': 10.1\n                        }\n                    },\n                    ...\n                }\n            initial_rows (dict): dictionary indicating the number of rows before syntactic checks for the previous period, {date -&gt; size_raw_data}\n            final_rows (dict): dictionary indicating the number of rows after syntactic checks for the previous period, {date -&gt; size_raw_data}\n        \"\"\"\n        intermediate_df = self.input_data_objects[SilverNetworkDataQualityMetricsByColumn.ID].df.filter(\n            F.col(ColNames.date).between(self.lookback_period_start, self.lookback_period_end)\n        )\n\n        intermediate_df.cache()\n\n        lookback_stats = (\n            intermediate_df.groupBy([ColNames.field_name, ColNames.type_code])\n            .agg(F.mean(F.col(ColNames.value)).alias(\"average\"), F.stddev_samp(F.col(ColNames.value)).alias(\"stddev\"))\n            .collect()\n        )\n\n        error_rate_data = (\n            intermediate_df.select([ColNames.type_code, ColNames.value, ColNames.date])\n            .filter(F.col(ColNames.type_code).isin([NetworkErrorType.INITIAL_ROWS, NetworkErrorType.FINAL_ROWS]))\n            .collect()\n        )\n\n        intermediate_df.unpersist()\n\n        initial_rows = {}\n        final_rows = {}\n\n        for row in error_rate_data:\n            if row[ColNames.type_code] == NetworkErrorType.INITIAL_ROWS:\n                initial_rows[row[ColNames.date]] = row[\"value\"]\n            else:\n                final_rows[row[ColNames.date]] = row[\"value\"]\n\n        statistics = dict()\n        for row in lookback_stats:\n            field_name, type_code, average, stddev = (\n                row[ColNames.field_name],\n                row[ColNames.type_code],\n                row[\"average\"],\n                row[\"stddev\"],\n            )\n            if field_name not in statistics:\n                statistics[field_name] = dict()\n            statistics[field_name][type_code] = {\"average\": average, \"stddev\": stddev}\n\n        return statistics, initial_rows, final_rows\n\n    def get_study_date_values(self) -&gt; dict:\n        \"\"\"Method that reads and returns the quality metrics of the date of study.\n\n        Returns:\n            today_values (dict): dictionary containing said values, with the following\n                structure:\n                {\n                    field_name1: {\n                        type_error1: 123,\n                        type_error2: 23,\n                        type_error3: 0\n                    },\n                    field_name2: {\n                        type_error1: 0,\n                        type_error2: 0,\n                        type_error3: 300\n                    },\n                }\n        \"\"\"\n        today_metrics = (\n            self.input_data_objects[SilverNetworkDataQualityMetricsByColumn.ID]\n            .df.filter(F.col(ColNames.date) == F.lit(self.date_of_study))\n            .select([ColNames.field_name, ColNames.type_code, ColNames.value])\n        ).collect()\n\n        today_values = {}\n\n        for row in today_metrics:\n            field_name, type_code, value = row[ColNames.field_name], row[ColNames.type_code], row[ColNames.value]\n\n            if field_name not in today_values:\n                today_values[field_name] = {}\n\n            today_values[field_name][type_code] = value\n\n        return today_values\n\n    def register_warning(\n        self, measure_definition: str, daily_value: float, condition: str, condition_value: float, warning_text: str\n    ) -&gt; None:\n        \"\"\"Method that abstracts away the creation in the correct format and data type, each of the quality warnings\n        that will be recorded in the log table.\n\n        Args:\n            measure_definition (str): measure that raised the warning (e.g. Error rate)\n            daily_value (float): measure's value in the date of study that raised the warning\n            condition (str): test that was checked in order to raise the warning\n            condition_value (float): value against which the date of study's daily_value was compared\n            warning_text (str): verbose explanation of the condition being satisfied and the warning\n                being raised\n        \"\"\"\n        warning = {\n            ColNames.title: self.TITLE,\n            ColNames.date: self.date_of_study,\n            ColNames.timestamp: self.timestamp,\n            ColNames.measure_definition: measure_definition,\n            ColNames.daily_value: float(daily_value),\n            ColNames.condition: condition,\n            ColNames.lookback_period: self.lookback_period,\n            ColNames.condition_value: float(condition_value),\n            ColNames.warning_text: warning_text,\n            ColNames.year: self.date_of_study.year,\n            ColNames.month: self.date_of_study.month,\n            ColNames.day: self.date_of_study.day,\n        }\n\n        self.warnings.append(Row(**warning))\n\n    def raw_size_warnings(self, lookback_stats, today_values):\n        \"\"\"Method that generates the quality warnings, that will be recorded in the output log table,\n        for the metric regarding the initial number of rows of the raw input network topology data prior\n        to the syntactic check procedure.\n\n        A total of six warnings might be generated:\n            - The study date's number of rows is greater than the average number of rows over the previous period\n                by more than a threshold percentage.\n            - The study date's number of rows is smaller than the average number of rows over the previous period\n                by more than a threshold percentage.\n            - The study date's number of rows is greater than the average number of rows over the previous period\n                by a config-specified number of standard deviations.\n            - The study date's number of rows is smaller than the average number of rows over the previous period\n                by a config-specified number of standard deviations.\n            - The study date's number of rows is greater than a config-specified threshold.\n            - The study date's number of rows is smaller than a config-specified threshold.\n        \"\"\"\n        current_val = today_values[None][NetworkErrorType.INITIAL_ROWS]\n        previous_avg = lookback_stats[None][NetworkErrorType.INITIAL_ROWS][\"average\"]\n        previous_std = lookback_stats[None][NetworkErrorType.INITIAL_ROWS][\"stddev\"]\n\n        measure_definition = self.MEASURE_DEFINITION[\"SIZE_RAW_DATA\"]\n\n        over_average = self.thresholds[\"SIZE_RAW_DATA\"][\"OVER_AVERAGE\"]\n        under_average = self.thresholds[\"SIZE_RAW_DATA\"][\"UNDER_AVERAGE\"]\n        variability = self.thresholds[\"SIZE_RAW_DATA\"][\"VARIABILITY\"]\n        absolute_upper_control_limit = self.thresholds[\"SIZE_RAW_DATA\"][\"ABS_VALUE_UPPER_LIMIT\"]\n        absolute_lower_control_limit = self.thresholds[\"SIZE_RAW_DATA\"][\"ABS_VALUE_LOWER_LIMIT\"]\n\n        if previous_avg == 0:\n            self.logger.warning(\n                f\"\"\"\n                The average of the number of rows in the raw input network topology data is 0\n                for the lookback period especified ({self.lookback_period_start} to {self.lookback_period_end},\n                 both included)\n                \"\"\"\n            )\n        else:\n            pct_difference = 100 * current_val / previous_avg - 100\n            if pct_difference &gt; over_average:\n                self.register_warning(\n                    measure_definition,\n                    pct_difference,\n                    f\"Size is greater than the previous period's average by more than {over_average} %\",\n                    over_average,\n                    \"The number of cells is unexpectedly high with respect to the previous period, please check if there have been issues in the network\",\n                )\n\n            if pct_difference &lt; -under_average:\n                self.register_warning(\n                    measure_definition,\n                    pct_difference,\n                    f\"Size is smaller than the previous period's average by more than {under_average} %\",\n                    under_average,\n                    \"The number of cells is unexpectedly low with respect to the previous period, please check if there have been issues in the network.\",\n                )\n\n        upper_control_limit = previous_avg + variability * previous_std\n        lower_control_limit = previous_avg - variability * previous_std\n\n        if current_val &gt; upper_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                \"Size is out of the upper control limit calculated on the basis of average and standard deviation of the distribution \"\n                f\"of the size in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\",\n                upper_control_limit,\n                \"The number of cells is unexpectedly high with respect to the previous period, taking into account the usual \"\n                \"variability of the cell numbers, please check if there have been issues in the network.\",\n            )\n        if current_val &lt; lower_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                \"Size is out of the lower control limit calculated on the basis of average and standard deviation of the distribution \"\n                f\"of the size in the previous period. Lower control limit = (average - {variability}\u00b7stddev)\",\n                lower_control_limit,\n                \"The number of cells is unexpectedly low with respect to the previous period, taking into account the usual \"\n                \"variability of the cell numbers, please check if there have been issues in the network.\",\n            )\n\n        # Use of UCL and LCL as default values for the absolute limits\n        if absolute_upper_control_limit is None:\n            absolute_upper_control_limit = upper_control_limit\n        if absolute_lower_control_limit is None:\n            absolute_lower_control_limit = lower_control_limit\n\n        if current_val &gt; absolute_upper_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                f\"The size is over the threshold {absolute_upper_control_limit}\",\n                absolute_upper_control_limit,\n                \"The number of cells is over the threshold, please check if there have been changes in the network.\",\n            )\n        if current_val &lt; absolute_lower_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                f\"The size is under the threshold {absolute_lower_control_limit}\",\n                absolute_upper_control_limit,\n                \"The number of cells is under the threshold, please check if there have been changes in the network.\",\n            )\n\n        return previous_avg, upper_control_limit, lower_control_limit\n\n    def clean_size_warnings(self, lookback_stats, today_values):\n        \"\"\"Method that generates quality warnings, that will be recorded in the output log table,\n        for the metric regarding the final number of rows of the raw input network topology data prior\n        to the syntactic check procedure.\n\n        A total of six warnings might be generated:\n            - The study date's number of rows is greater than the average number of rows over the previous period\n                by more than a threshold percentage.\n            - The study date's number of rows is smaller than the average number of rows over the previous period\n                by more than a threshold percentage.\n            - The study date's number of rows is greater than the average number of rows over the previous period\n                by a config-specified number of standard deviations.\n            - The study date's number of rows is smaller than the average number of rows over the previous period\n                by a config-specified number of standard deviations.\n            - The study date's number of rows is greater than a config-specified threshold.\n            - The study date's number of rows is smaller than a config-specified threshold.\n        \"\"\"\n        current_val = today_values[None][NetworkErrorType.FINAL_ROWS]\n        previous_avg = lookback_stats[None][NetworkErrorType.FINAL_ROWS][\"average\"]\n        previous_std = lookback_stats[None][NetworkErrorType.FINAL_ROWS][\"stddev\"]\n\n        measure_definition = self.MEASURE_DEFINITION[\"SIZE_CLEAN_DATA\"]\n\n        over_average = self.thresholds[\"SIZE_CLEAN_DATA\"][\"OVER_AVERAGE\"]\n        under_average = self.thresholds[\"SIZE_CLEAN_DATA\"][\"UNDER_AVERAGE\"]\n        variability = self.thresholds[\"SIZE_CLEAN_DATA\"][\"VARIABILITY\"]\n        absolute_upper_control_limit = self.thresholds[\"SIZE_CLEAN_DATA\"][\"ABS_VALUE_UPPER_LIMIT\"]\n        absolute_lower_control_limit = self.thresholds[\"SIZE_CLEAN_DATA\"][\"ABS_VALUE_LOWER_LIMIT\"]\n\n        if previous_avg == 0:\n            self.logger.warning(\n                f\"\"\"\n                The average of the number of rows in the clean input network topology data after syntactic checks is 0\n                for the lookback period especified ({self.lookback_period_start} to {self.lookback_period_end}, \n                both included)\n                \"\"\"\n            )\n        else:\n            pct_difference = 100 * current_val / previous_avg - 100\n            if pct_difference &gt; over_average:\n                self.register_warning(\n                    measure_definition,\n                    pct_difference,\n                    f\"Size is greater than the previous period's average by {over_average} %\",\n                    over_average,\n                    \"The number of cells after the syntactic checks procedure is unexpectedly high with respect to the previous period.\",\n                )\n\n            if pct_difference &lt; -under_average:\n                self.register_warning(\n                    measure_definition,\n                    pct_difference,\n                    f\"Size is smaller than the previous period's average by {under_average} %\",\n                    under_average,\n                    \"The number of cells after the syntactic checks procedure is unexpectedly low with respect to the previous period.\",\n                )\n\n        upper_control_limit = previous_avg + variability * previous_std\n        lower_control_limit = previous_avg - variability * previous_std\n\n        if current_val &gt; upper_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                \"Size is out of the upper control limit calculated on the basis of average and standard deviation of the distribution \"\n                f\"of the size in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\",\n                upper_control_limit,\n                \"The number of cells is unexpectedly high with respect to the previous period, taking into account the usual \"\n                \"variability of the cell numbers, please check if there have been issues in the network.\",\n            )\n        if current_val &lt; lower_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                \"Size is out of the lower control limit calculated on the basis of average and standard deviation of the distribution \"\n                f\"of the size in the previous period. Lower control limit = (average - {variability}\u00b7stddev)\",\n                lower_control_limit,\n                \"The number of cells is unexpectedly low with respect to the previous period, taking into account the usual \"\n                \"variability of the cell numbers, please check if there have been issues in the network.\",\n            )\n\n        # Use of UCL and LCL as default values for the absolute limits\n        if absolute_upper_control_limit is None:\n            absolute_upper_control_limit = upper_control_limit\n        if absolute_lower_control_limit is None:\n            absolute_lower_control_limit = lower_control_limit\n\n        if current_val &gt; absolute_upper_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                f\"The size is over the threshold {absolute_upper_control_limit}\",\n                absolute_upper_control_limit,\n                \"The number of cells after the syntactic checks procedure is over the threshold.\",\n            )\n        if current_val &lt; absolute_lower_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                f\"The size is under the threshold {absolute_lower_control_limit}\",\n                absolute_upper_control_limit,\n                \"The number of cells after the syntactic checks procedure is under the threshold.\",\n            )\n\n        return previous_avg, upper_control_limit, lower_control_limit\n\n    def error_rate_warnings(self, initial_rows, final_rows, today_values):\n        \"\"\"Method that generates quality warnings, that will be recorded in the output log table,\n        for the metric regarding the error rate observed in the syntactic check procedure.\n\n        A total of three warnings might be generated:\n            - The study date's error rate is greater than the average number of rows over the previous period\n                by more than a threshold percentage.\n            - The study date's error rate is greater than the average number of rows over the previous period\n                by a config-specified number of standard deviations.\n            - The study date's error rate is greater than a config-specified threshold.\n        \"\"\"\n        if len(initial_rows) != len(final_rows):\n            raise ValueError(\n                \"Input Quality Metrics do not have information on the number of rows \"\n                \"before and after syntactic checks on all dates considered!\"\n            )\n\n        error_rate = {\n            date: 100 * (initial_rows[date] - final_rows[date]) / initial_rows[date] for date in initial_rows.keys()\n        }\n\n        current_val = (\n            100\n            * (today_values[None][NetworkErrorType.INITIAL_ROWS] - today_values[None][NetworkErrorType.FINAL_ROWS])\n            / today_values[None][NetworkErrorType.INITIAL_ROWS]\n        )\n        previous_avg = sum(error_rate[date] for date in self.lookback_dates) / len(self.lookback_dates)\n        previous_std = math.sqrt(\n            sum((error_rate[date] - previous_avg) ** 2 for date in self.lookback_dates) / (len(self.lookback_dates) - 1)\n        )\n\n        measure_definition = self.MEASURE_DEFINITION[\"TOTAL_ERROR_RATE\"]\n\n        over_average = self.thresholds[\"TOTAL_ERROR_RATE\"][\"OVER_AVERAGE\"]\n        variability = self.thresholds[\"TOTAL_ERROR_RATE\"][\"VARIABILITY\"]\n        absolute_upper_control_limit = self.thresholds[\"TOTAL_ERROR_RATE\"][\"ABS_VALUE_UPPER_LIMIT\"]\n\n        if previous_avg == 0:\n            self.logger.warning(\n                f\"\"\"\n                The average error rate in the input network topology data 0\n                for the lookback period especified ({self.lookback_period_start} to {self.lookback_period_end}, \n                both included)\n                \"\"\"\n            )\n        else:\n            pct_difference = 100 * current_val / previous_avg - 100\n            if pct_difference &gt; over_average:\n                self.register_warning(\n                    measure_definition,\n                    pct_difference,\n                    f\"Error rate is greater than the previous period's average by more than {over_average} %\",\n                    over_average,\n                    \"The error rate after the syntactic checks procedure is unexpectedly high with respect to the previous period.\",\n                )\n        upper_control_limit = previous_avg + variability * previous_std\n\n        if current_val &gt; upper_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                \"Error rate is over the upper control limit calculated on the basis of average and standard deviation of the distribution \"\n                f\"of the size in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\",\n                upper_control_limit,\n                \"The error rate after the syntactic checks procedure is unexpectedly high with respect to the previous period, taking into account the usual \"\n                \"variability.\",\n            )\n\n        if current_val &gt; absolute_upper_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                f\"The error rate is over the value {absolute_upper_control_limit}\",\n                absolute_upper_control_limit,\n                \"The error rate after the syntactic checks procedure is over the threshold.\",\n            )\n        error_rate[self.date_of_study] = current_val\n        return error_rate, previous_avg, upper_control_limit\n\n    def all_specific_error_warnings(self, lookback_stats, today_values):\n        \"\"\"Parent method for the creation of warnings for each type of error rate\n\n        lookback_stats (dict): contains error information of each date of the lookback period\n        today_values (dict): contains error information of the date of study\n        \"\"\"\n        error_rate_types = [\"Missing_value_RATE\", \"Out_of_range_RATE\", \"Parsing_error_RATE\"]\n\n        for error_rate_type in error_rate_types:\n            for field_name in self.thresholds[error_rate_type]:\n                self.specific_error_warnings(error_rate_type, field_name, lookback_stats, today_values)\n\n    def specific_error_warnings(self, error_rate_type, field_name, lookback_stats, today_values):\n        \"\"\"Method that generates the quality warnings, that will be recorded in the output log table,\n        for the metric regarding a specific error type considered in the network syntactic checks.\n\n        A total of three warnings might be generated:\n            - The study date's error rate for this error and this field is greater than the average number of rows over the previous period\n                by more than a threshold percentage.\n            - The study date's error rate for this error and this field is greater than the average number of rows over the previous period\n                by a config-specified number of standard deviations.\n            - The study date's error rate for this error and this field is greater than a config-specified threshold.\n        \"\"\"\n        if error_rate_type not in self.ERROR_TYPE:\n            raise ValueError(f\"Unknown error type for quality warning `{error_rate_type}`\")\n\n        network_error_type = self.ERROR_TYPE[error_rate_type]\n\n        over_average = self.thresholds[error_rate_type][field_name][\"AVERAGE\"]\n        variability = self.thresholds[error_rate_type][field_name][\"VARIABILITY\"]\n        absolute_upper_control_limit = self.thresholds[error_rate_type][field_name][\"ABS_VALUE_UPPER_LIMIT\"]\n\n        if network_error_type == NetworkErrorType.OUT_OF_RANGE and field_name is None:\n            field_name = \"dates\"\n        current_val = today_values[field_name][network_error_type]\n        previous_avg = lookback_stats[field_name][network_error_type][\"average\"]\n        previous_std = lookback_stats[field_name][network_error_type][\"stddev\"]\n\n        if previous_avg == 0:\n            self.logger.warning(\n                f\"\"\"\n                The average {error_rate_type} for field {field_name} in the input network topology data is 0 for the lookback period\n                especified ({self.lookback_period_start} to {self.lookback_period_end}, both included)\n                \"\"\"\n            )\n        else:\n            pct_difference = 100 * current_val / previous_avg - 100\n            if pct_difference &gt; over_average:\n                self.register_warning(\n                    self.MEASURE_DEFINITION[error_rate_type](field_name=field_name),\n                    pct_difference,\n                    self.CONDITION[error_rate_type][\"AVERAGE\"](field_name=field_name, value=over_average),\n                    over_average,\n                    self.WARNING_MESSAGE[error_rate_type][\"AVERAGE\"](field_name=field_name),\n                )\n        upper_control_limit = previous_avg + variability * previous_std\n\n        if current_val &gt; upper_control_limit:\n            self.register_warning(\n                self.MEASURE_DEFINITION[error_rate_type](field_name=field_name),\n                current_val,\n                self.CONDITION[error_rate_type][\"VARIABILITY\"](field_name=field_name, variability=variability),\n                variability,\n                self.WARNING_MESSAGE[error_rate_type][\"VARIABILITY\"](field_name=field_name),\n            )\n\n        if current_val &gt; absolute_upper_control_limit:\n            self.register_warning(\n                self.MEASURE_DEFINITION[error_rate_type](field_name=field_name),\n                current_val,\n                self.CONDITION[error_rate_type][\"ABS_VALUE_UPPER_LIMIT\"](\n                    field_name=field_name, value=absolute_upper_control_limit\n                ),\n                absolute_upper_control_limit,\n                self.WARNING_MESSAGE[error_rate_type][\"ABS_VALUE_UPPER_LIMIT\"](field_name=field_name),\n            )\n\n    def create_plots_data(\n        self,\n        lookback_initial_rows,\n        lookback_final_rows,\n        today_values,\n        error_rate,\n        raw_average,\n        clean_average,\n        error_rate_avg,\n        raw_UCL,\n        clean_UCL,\n        error_rate_UCL,\n        raw_LCL,\n        clean_LCL,\n    ):\n        \"\"\"Method that takes the data needed to generate the required plots, and formats it for\n        easier plotting and saving later on.\n\n        Args:\n            lookback_initial_rows (dict): contains data on the lookback period dates' number of rows before syntactic checks\n            lookback_final_rows (dict): contains data on the lookback period dates' number of rows after syntactic checks\n            today_values (dict): contains data on the date of study error counts and rows before and after the syntactic checks\n            error_rate (dict): cotains data on the error rates for all lookback dates and date of study.\n            raw_average (float): average of rows in the raw data before syntactic checks\n            clean_average (float): average of rows in the clean data after syntactic checks\n            error_rate_avg (float): average of the error rate observed in the syntactic checks\n            raw_UCL (float): upper control limit for the rows in the raw data before syntactic checks\n            clean_UCL (float): upper control limit for the rows in the clean data after syntactic checks\n            error_rate_UCL (float): upper control limit for the error rate\n            raw_LCL (float): lower control limit for the rows in the raw data before syntactic checks\n            clean_LCL (float): lower control limit for the rows in the raw data after syntactic checks\n        \"\"\"\n        # self.plots_data = {\"rows_before_syntactic_check\": [], \"rows_after_syntactic_check\": [], \"error_rate\": []}\n\n        plots_data = {\"line_plot\": [], \"pie_plot\": []}\n\n        for date in sorted(self.lookback_dates) + [self.date_of_study]:\n            plots_data[\"line_plot\"].extend(\n                [\n                    Row(\n                        **{\n                            ColNames.date: date,\n                            ColNames.daily_value: float(lookback_initial_rows[date]),\n                            ColNames.average: float(raw_average),\n                            ColNames.UCL: float(raw_UCL),\n                            ColNames.LCL: float(raw_LCL),\n                            ColNames.variable: \"rows_before_syntactic_check\",\n                            ColNames.year: self.date_of_study.year,\n                            ColNames.month: self.date_of_study.month,\n                            ColNames.day: self.date_of_study.day,\n                            ColNames.timestamp: self.timestamp,\n                        }\n                    )\n                ]\n            )\n\n            plots_data[\"line_plot\"].extend(\n                [\n                    Row(\n                        **{\n                            ColNames.date: date,\n                            ColNames.daily_value: float(lookback_final_rows[date]),\n                            ColNames.average: float(clean_average),\n                            ColNames.UCL: float(clean_UCL),\n                            ColNames.LCL: float(clean_LCL),\n                            ColNames.variable: \"rows_after_syntactic_check\",\n                            ColNames.year: self.date_of_study.year,\n                            ColNames.month: self.date_of_study.month,\n                            ColNames.day: self.date_of_study.day,\n                            ColNames.timestamp: self.timestamp,\n                        }\n                    )\n                ]\n            )\n\n            plots_data[\"line_plot\"].extend(\n                [\n                    Row(\n                        **{\n                            ColNames.date: date,\n                            ColNames.daily_value: float(error_rate[date]),\n                            ColNames.average: float(error_rate_avg),\n                            ColNames.UCL: float(error_rate_UCL),\n                            ColNames.LCL: None,\n                            ColNames.variable: \"error_rate\",\n                            ColNames.year: self.date_of_study.year,\n                            ColNames.month: self.date_of_study.month,\n                            ColNames.day: self.date_of_study.day,\n                            ColNames.timestamp: self.timestamp,\n                        }\n                    )\n                ]\n            )\n\n        # Now, the data for the pie charts\n        # Ugly way to get the relation error_code -&gt; error attribute name\n        error_types = {getattr(NetworkErrorType, att): att for att in dir(NetworkErrorType) if not att.startswith(\"__\")}\n\n        for field_name, error_counts in today_values.items():\n            if field_name in [None, \"dates\"]:\n                continue\n\n            # boolean check if this field had any errors or not\n            field_has_errors = False\n\n            for key in error_counts.keys():\n                if key != NetworkErrorType.NO_ERROR:\n                    if error_counts[key] &gt; 0:\n                        # self.plots_data[field_name].append(\n                        #     field_name, error_types[key], error_counts[key]\n                        # )\n                        field_has_errors = True\n                        plots_data[\"pie_plot\"].append(\n                            Row(\n                                **{\n                                    ColNames.type_code: error_types[key],\n                                    ColNames.value: error_counts[key],\n                                    ColNames.variable: field_name,\n                                    ColNames.year: self.date_of_study.year,\n                                    ColNames.month: self.date_of_study.month,\n                                    ColNames.day: self.date_of_study.day,\n                                    ColNames.timestamp: self.timestamp,\n                                }\n                            )\n                        )\n            if not field_has_errors:\n                self.logger.info(f\"Field `{field_name}` had no errors\")\n        self.output_data_objects[SilverNetworkSyntacticQualityWarningsLinePlotData.ID].df = self.spark.createDataFrame(\n            plots_data[\"line_plot\"], schema=SilverNetworkSyntacticQualityWarningsLinePlotData.SCHEMA\n        )\n\n        self.output_data_objects[SilverNetworkSyntacticQualityWarningsPiePlotData.ID].df = self.spark.createDataFrame(\n            plots_data[\"pie_plot\"], schema=SilverNetworkSyntacticQualityWarningsPiePlotData.SCHEMA\n        )\n\n    def write(self):\n        # Write regular data objects\n        self.logger.info(\"Writing data objects...\")\n        super().write()\n        self.logger.info(\"Data objects written.\")\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.all_specific_error_warnings","title":"<code>all_specific_error_warnings(lookback_stats, today_values)</code>","text":"<p>Parent method for the creation of warnings for each type of error rate</p> <p>lookback_stats (dict): contains error information of each date of the lookback period today_values (dict): contains error information of the date of study</p> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def all_specific_error_warnings(self, lookback_stats, today_values):\n    \"\"\"Parent method for the creation of warnings for each type of error rate\n\n    lookback_stats (dict): contains error information of each date of the lookback period\n    today_values (dict): contains error information of the date of study\n    \"\"\"\n    error_rate_types = [\"Missing_value_RATE\", \"Out_of_range_RATE\", \"Parsing_error_RATE\"]\n\n    for error_rate_type in error_rate_types:\n        for field_name in self.thresholds[error_rate_type]:\n            self.specific_error_warnings(error_rate_type, field_name, lookback_stats, today_values)\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.check_needed_dates","title":"<code>check_needed_dates()</code>","text":"<p>Method that checks if both the date of study and the dates necessary to generate the quality warnings, specified through the lookback_period parameter, are present in the input data.</p> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def check_needed_dates(self) -&gt; None:\n    \"\"\"\n    Method that checks if both the date of study and the dates necessary to generate\n    the quality warnings, specified through the lookback_period parameter, are present\n    in the input data.\n    \"\"\"\n\n    # Collect all distinct dates in the input quality metrics within the needed range\n    metrics = self.input_data_objects[SilverNetworkDataQualityMetricsByColumn.ID].df\n    dates = (\n        metrics.filter(\n            F.col(\"date\")\n            # left- and right- inclusive\n            .between(\n                self.date_of_study - datetime.timedelta(days=self.PERIOD_DURATION[self.lookback_period]),\n                self.date_of_study,\n            )\n        )\n        .select(F.col(ColNames.date))\n        .distinct()\n        .collect()\n    )\n\n    dates = [row[ColNames.date] for row in dates]\n\n    if self.date_of_study not in dates:\n        raise ValueError(\n            f\"The date of study, {self.date_of_study}, is not present in the input Quality Metrics data\"\n        )\n\n    if set(self.lookback_dates).intersection(set(dates)) != set(self.lookback_dates):\n        error_msg = f\"\"\"\n            The following dates from the lookback period are not present in the\n            input Quality Metrics data:\n            {\n                sorted(\n                    map(\n                        lambda x: x.strftime(self.date_format),\n                        set(self.lookback_dates).difference(set(dates))\n                    )\n                )\n            }\"\"\"\n\n        self.logger.error(error_msg)\n        raise ValueError(error_msg)\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.clean_size_warnings","title":"<code>clean_size_warnings(lookback_stats, today_values)</code>","text":"<p>Method that generates quality warnings, that will be recorded in the output log table, for the metric regarding the final number of rows of the raw input network topology data prior to the syntactic check procedure.</p> A total of six warnings might be generated <ul> <li>The study date's number of rows is greater than the average number of rows over the previous period     by more than a threshold percentage.</li> <li>The study date's number of rows is smaller than the average number of rows over the previous period     by more than a threshold percentage.</li> <li>The study date's number of rows is greater than the average number of rows over the previous period     by a config-specified number of standard deviations.</li> <li>The study date's number of rows is smaller than the average number of rows over the previous period     by a config-specified number of standard deviations.</li> <li>The study date's number of rows is greater than a config-specified threshold.</li> <li>The study date's number of rows is smaller than a config-specified threshold.</li> </ul> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def clean_size_warnings(self, lookback_stats, today_values):\n    \"\"\"Method that generates quality warnings, that will be recorded in the output log table,\n    for the metric regarding the final number of rows of the raw input network topology data prior\n    to the syntactic check procedure.\n\n    A total of six warnings might be generated:\n        - The study date's number of rows is greater than the average number of rows over the previous period\n            by more than a threshold percentage.\n        - The study date's number of rows is smaller than the average number of rows over the previous period\n            by more than a threshold percentage.\n        - The study date's number of rows is greater than the average number of rows over the previous period\n            by a config-specified number of standard deviations.\n        - The study date's number of rows is smaller than the average number of rows over the previous period\n            by a config-specified number of standard deviations.\n        - The study date's number of rows is greater than a config-specified threshold.\n        - The study date's number of rows is smaller than a config-specified threshold.\n    \"\"\"\n    current_val = today_values[None][NetworkErrorType.FINAL_ROWS]\n    previous_avg = lookback_stats[None][NetworkErrorType.FINAL_ROWS][\"average\"]\n    previous_std = lookback_stats[None][NetworkErrorType.FINAL_ROWS][\"stddev\"]\n\n    measure_definition = self.MEASURE_DEFINITION[\"SIZE_CLEAN_DATA\"]\n\n    over_average = self.thresholds[\"SIZE_CLEAN_DATA\"][\"OVER_AVERAGE\"]\n    under_average = self.thresholds[\"SIZE_CLEAN_DATA\"][\"UNDER_AVERAGE\"]\n    variability = self.thresholds[\"SIZE_CLEAN_DATA\"][\"VARIABILITY\"]\n    absolute_upper_control_limit = self.thresholds[\"SIZE_CLEAN_DATA\"][\"ABS_VALUE_UPPER_LIMIT\"]\n    absolute_lower_control_limit = self.thresholds[\"SIZE_CLEAN_DATA\"][\"ABS_VALUE_LOWER_LIMIT\"]\n\n    if previous_avg == 0:\n        self.logger.warning(\n            f\"\"\"\n            The average of the number of rows in the clean input network topology data after syntactic checks is 0\n            for the lookback period especified ({self.lookback_period_start} to {self.lookback_period_end}, \n            both included)\n            \"\"\"\n        )\n    else:\n        pct_difference = 100 * current_val / previous_avg - 100\n        if pct_difference &gt; over_average:\n            self.register_warning(\n                measure_definition,\n                pct_difference,\n                f\"Size is greater than the previous period's average by {over_average} %\",\n                over_average,\n                \"The number of cells after the syntactic checks procedure is unexpectedly high with respect to the previous period.\",\n            )\n\n        if pct_difference &lt; -under_average:\n            self.register_warning(\n                measure_definition,\n                pct_difference,\n                f\"Size is smaller than the previous period's average by {under_average} %\",\n                under_average,\n                \"The number of cells after the syntactic checks procedure is unexpectedly low with respect to the previous period.\",\n            )\n\n    upper_control_limit = previous_avg + variability * previous_std\n    lower_control_limit = previous_avg - variability * previous_std\n\n    if current_val &gt; upper_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            \"Size is out of the upper control limit calculated on the basis of average and standard deviation of the distribution \"\n            f\"of the size in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\",\n            upper_control_limit,\n            \"The number of cells is unexpectedly high with respect to the previous period, taking into account the usual \"\n            \"variability of the cell numbers, please check if there have been issues in the network.\",\n        )\n    if current_val &lt; lower_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            \"Size is out of the lower control limit calculated on the basis of average and standard deviation of the distribution \"\n            f\"of the size in the previous period. Lower control limit = (average - {variability}\u00b7stddev)\",\n            lower_control_limit,\n            \"The number of cells is unexpectedly low with respect to the previous period, taking into account the usual \"\n            \"variability of the cell numbers, please check if there have been issues in the network.\",\n        )\n\n    # Use of UCL and LCL as default values for the absolute limits\n    if absolute_upper_control_limit is None:\n        absolute_upper_control_limit = upper_control_limit\n    if absolute_lower_control_limit is None:\n        absolute_lower_control_limit = lower_control_limit\n\n    if current_val &gt; absolute_upper_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            f\"The size is over the threshold {absolute_upper_control_limit}\",\n            absolute_upper_control_limit,\n            \"The number of cells after the syntactic checks procedure is over the threshold.\",\n        )\n    if current_val &lt; absolute_lower_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            f\"The size is under the threshold {absolute_lower_control_limit}\",\n            absolute_upper_control_limit,\n            \"The number of cells after the syntactic checks procedure is under the threshold.\",\n        )\n\n    return previous_avg, upper_control_limit, lower_control_limit\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.create_plots_data","title":"<code>create_plots_data(lookback_initial_rows, lookback_final_rows, today_values, error_rate, raw_average, clean_average, error_rate_avg, raw_UCL, clean_UCL, error_rate_UCL, raw_LCL, clean_LCL)</code>","text":"<p>Method that takes the data needed to generate the required plots, and formats it for easier plotting and saving later on.</p> <p>Parameters:</p> Name Type Description Default <code>lookback_initial_rows</code> <code>dict</code> <p>contains data on the lookback period dates' number of rows before syntactic checks</p> required <code>lookback_final_rows</code> <code>dict</code> <p>contains data on the lookback period dates' number of rows after syntactic checks</p> required <code>today_values</code> <code>dict</code> <p>contains data on the date of study error counts and rows before and after the syntactic checks</p> required <code>error_rate</code> <code>dict</code> <p>cotains data on the error rates for all lookback dates and date of study.</p> required <code>raw_average</code> <code>float</code> <p>average of rows in the raw data before syntactic checks</p> required <code>clean_average</code> <code>float</code> <p>average of rows in the clean data after syntactic checks</p> required <code>error_rate_avg</code> <code>float</code> <p>average of the error rate observed in the syntactic checks</p> required <code>raw_UCL</code> <code>float</code> <p>upper control limit for the rows in the raw data before syntactic checks</p> required <code>clean_UCL</code> <code>float</code> <p>upper control limit for the rows in the clean data after syntactic checks</p> required <code>error_rate_UCL</code> <code>float</code> <p>upper control limit for the error rate</p> required <code>raw_LCL</code> <code>float</code> <p>lower control limit for the rows in the raw data before syntactic checks</p> required <code>clean_LCL</code> <code>float</code> <p>lower control limit for the rows in the raw data after syntactic checks</p> required Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def create_plots_data(\n    self,\n    lookback_initial_rows,\n    lookback_final_rows,\n    today_values,\n    error_rate,\n    raw_average,\n    clean_average,\n    error_rate_avg,\n    raw_UCL,\n    clean_UCL,\n    error_rate_UCL,\n    raw_LCL,\n    clean_LCL,\n):\n    \"\"\"Method that takes the data needed to generate the required plots, and formats it for\n    easier plotting and saving later on.\n\n    Args:\n        lookback_initial_rows (dict): contains data on the lookback period dates' number of rows before syntactic checks\n        lookback_final_rows (dict): contains data on the lookback period dates' number of rows after syntactic checks\n        today_values (dict): contains data on the date of study error counts and rows before and after the syntactic checks\n        error_rate (dict): cotains data on the error rates for all lookback dates and date of study.\n        raw_average (float): average of rows in the raw data before syntactic checks\n        clean_average (float): average of rows in the clean data after syntactic checks\n        error_rate_avg (float): average of the error rate observed in the syntactic checks\n        raw_UCL (float): upper control limit for the rows in the raw data before syntactic checks\n        clean_UCL (float): upper control limit for the rows in the clean data after syntactic checks\n        error_rate_UCL (float): upper control limit for the error rate\n        raw_LCL (float): lower control limit for the rows in the raw data before syntactic checks\n        clean_LCL (float): lower control limit for the rows in the raw data after syntactic checks\n    \"\"\"\n    # self.plots_data = {\"rows_before_syntactic_check\": [], \"rows_after_syntactic_check\": [], \"error_rate\": []}\n\n    plots_data = {\"line_plot\": [], \"pie_plot\": []}\n\n    for date in sorted(self.lookback_dates) + [self.date_of_study]:\n        plots_data[\"line_plot\"].extend(\n            [\n                Row(\n                    **{\n                        ColNames.date: date,\n                        ColNames.daily_value: float(lookback_initial_rows[date]),\n                        ColNames.average: float(raw_average),\n                        ColNames.UCL: float(raw_UCL),\n                        ColNames.LCL: float(raw_LCL),\n                        ColNames.variable: \"rows_before_syntactic_check\",\n                        ColNames.year: self.date_of_study.year,\n                        ColNames.month: self.date_of_study.month,\n                        ColNames.day: self.date_of_study.day,\n                        ColNames.timestamp: self.timestamp,\n                    }\n                )\n            ]\n        )\n\n        plots_data[\"line_plot\"].extend(\n            [\n                Row(\n                    **{\n                        ColNames.date: date,\n                        ColNames.daily_value: float(lookback_final_rows[date]),\n                        ColNames.average: float(clean_average),\n                        ColNames.UCL: float(clean_UCL),\n                        ColNames.LCL: float(clean_LCL),\n                        ColNames.variable: \"rows_after_syntactic_check\",\n                        ColNames.year: self.date_of_study.year,\n                        ColNames.month: self.date_of_study.month,\n                        ColNames.day: self.date_of_study.day,\n                        ColNames.timestamp: self.timestamp,\n                    }\n                )\n            ]\n        )\n\n        plots_data[\"line_plot\"].extend(\n            [\n                Row(\n                    **{\n                        ColNames.date: date,\n                        ColNames.daily_value: float(error_rate[date]),\n                        ColNames.average: float(error_rate_avg),\n                        ColNames.UCL: float(error_rate_UCL),\n                        ColNames.LCL: None,\n                        ColNames.variable: \"error_rate\",\n                        ColNames.year: self.date_of_study.year,\n                        ColNames.month: self.date_of_study.month,\n                        ColNames.day: self.date_of_study.day,\n                        ColNames.timestamp: self.timestamp,\n                    }\n                )\n            ]\n        )\n\n    # Now, the data for the pie charts\n    # Ugly way to get the relation error_code -&gt; error attribute name\n    error_types = {getattr(NetworkErrorType, att): att for att in dir(NetworkErrorType) if not att.startswith(\"__\")}\n\n    for field_name, error_counts in today_values.items():\n        if field_name in [None, \"dates\"]:\n            continue\n\n        # boolean check if this field had any errors or not\n        field_has_errors = False\n\n        for key in error_counts.keys():\n            if key != NetworkErrorType.NO_ERROR:\n                if error_counts[key] &gt; 0:\n                    # self.plots_data[field_name].append(\n                    #     field_name, error_types[key], error_counts[key]\n                    # )\n                    field_has_errors = True\n                    plots_data[\"pie_plot\"].append(\n                        Row(\n                            **{\n                                ColNames.type_code: error_types[key],\n                                ColNames.value: error_counts[key],\n                                ColNames.variable: field_name,\n                                ColNames.year: self.date_of_study.year,\n                                ColNames.month: self.date_of_study.month,\n                                ColNames.day: self.date_of_study.day,\n                                ColNames.timestamp: self.timestamp,\n                            }\n                        )\n                    )\n        if not field_has_errors:\n            self.logger.info(f\"Field `{field_name}` had no errors\")\n    self.output_data_objects[SilverNetworkSyntacticQualityWarningsLinePlotData.ID].df = self.spark.createDataFrame(\n        plots_data[\"line_plot\"], schema=SilverNetworkSyntacticQualityWarningsLinePlotData.SCHEMA\n    )\n\n    self.output_data_objects[SilverNetworkSyntacticQualityWarningsPiePlotData.ID].df = self.spark.createDataFrame(\n        plots_data[\"pie_plot\"], schema=SilverNetworkSyntacticQualityWarningsPiePlotData.SCHEMA\n    )\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.error_rate_warnings","title":"<code>error_rate_warnings(initial_rows, final_rows, today_values)</code>","text":"<p>Method that generates quality warnings, that will be recorded in the output log table, for the metric regarding the error rate observed in the syntactic check procedure.</p> A total of three warnings might be generated <ul> <li>The study date's error rate is greater than the average number of rows over the previous period     by more than a threshold percentage.</li> <li>The study date's error rate is greater than the average number of rows over the previous period     by a config-specified number of standard deviations.</li> <li>The study date's error rate is greater than a config-specified threshold.</li> </ul> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def error_rate_warnings(self, initial_rows, final_rows, today_values):\n    \"\"\"Method that generates quality warnings, that will be recorded in the output log table,\n    for the metric regarding the error rate observed in the syntactic check procedure.\n\n    A total of three warnings might be generated:\n        - The study date's error rate is greater than the average number of rows over the previous period\n            by more than a threshold percentage.\n        - The study date's error rate is greater than the average number of rows over the previous period\n            by a config-specified number of standard deviations.\n        - The study date's error rate is greater than a config-specified threshold.\n    \"\"\"\n    if len(initial_rows) != len(final_rows):\n        raise ValueError(\n            \"Input Quality Metrics do not have information on the number of rows \"\n            \"before and after syntactic checks on all dates considered!\"\n        )\n\n    error_rate = {\n        date: 100 * (initial_rows[date] - final_rows[date]) / initial_rows[date] for date in initial_rows.keys()\n    }\n\n    current_val = (\n        100\n        * (today_values[None][NetworkErrorType.INITIAL_ROWS] - today_values[None][NetworkErrorType.FINAL_ROWS])\n        / today_values[None][NetworkErrorType.INITIAL_ROWS]\n    )\n    previous_avg = sum(error_rate[date] for date in self.lookback_dates) / len(self.lookback_dates)\n    previous_std = math.sqrt(\n        sum((error_rate[date] - previous_avg) ** 2 for date in self.lookback_dates) / (len(self.lookback_dates) - 1)\n    )\n\n    measure_definition = self.MEASURE_DEFINITION[\"TOTAL_ERROR_RATE\"]\n\n    over_average = self.thresholds[\"TOTAL_ERROR_RATE\"][\"OVER_AVERAGE\"]\n    variability = self.thresholds[\"TOTAL_ERROR_RATE\"][\"VARIABILITY\"]\n    absolute_upper_control_limit = self.thresholds[\"TOTAL_ERROR_RATE\"][\"ABS_VALUE_UPPER_LIMIT\"]\n\n    if previous_avg == 0:\n        self.logger.warning(\n            f\"\"\"\n            The average error rate in the input network topology data 0\n            for the lookback period especified ({self.lookback_period_start} to {self.lookback_period_end}, \n            both included)\n            \"\"\"\n        )\n    else:\n        pct_difference = 100 * current_val / previous_avg - 100\n        if pct_difference &gt; over_average:\n            self.register_warning(\n                measure_definition,\n                pct_difference,\n                f\"Error rate is greater than the previous period's average by more than {over_average} %\",\n                over_average,\n                \"The error rate after the syntactic checks procedure is unexpectedly high with respect to the previous period.\",\n            )\n    upper_control_limit = previous_avg + variability * previous_std\n\n    if current_val &gt; upper_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            \"Error rate is over the upper control limit calculated on the basis of average and standard deviation of the distribution \"\n            f\"of the size in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\",\n            upper_control_limit,\n            \"The error rate after the syntactic checks procedure is unexpectedly high with respect to the previous period, taking into account the usual \"\n            \"variability.\",\n        )\n\n    if current_val &gt; absolute_upper_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            f\"The error rate is over the value {absolute_upper_control_limit}\",\n            absolute_upper_control_limit,\n            \"The error rate after the syntactic checks procedure is over the threshold.\",\n        )\n    error_rate[self.date_of_study] = current_val\n    return error_rate, previous_avg, upper_control_limit\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.get_lookback_period_statistics","title":"<code>get_lookback_period_statistics()</code>","text":"<p>Method that computes the necessary statistics (average and standard deviation) from the Quality Metrics of the lookback period.</p> <p>Returns:</p> Name Type Description <code>statistics</code> <code>dict</code> <p>dictionary containing said necessary statistics, with the following structure: {     field_name1: {         type_error1: {             'average': 12.2,             'stddev': 17.5         },         type_error2 : {             'average': 4.5,             'stddev': 10.1         }     },     ... }</p> <code>initial_rows</code> <code>dict</code> <p>dictionary indicating the number of rows before syntactic checks for the previous period, {date -&gt; size_raw_data}</p> <code>final_rows</code> <code>dict</code> <p>dictionary indicating the number of rows after syntactic checks for the previous period, {date -&gt; size_raw_data}</p> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def get_lookback_period_statistics(self) -&gt; dict:\n    \"\"\"Method that computes the necessary statistics (average and standard deviation) from the\n    Quality Metrics of the lookback period.\n\n    Returns:\n        statistics (dict): dictionary containing said necessary statistics, with the following\n            structure:\n            {\n                field_name1: {\n                    type_error1: {\n                        'average': 12.2,\n                        'stddev': 17.5\n                    },\n                    type_error2 : {\n                        'average': 4.5,\n                        'stddev': 10.1\n                    }\n                },\n                ...\n            }\n        initial_rows (dict): dictionary indicating the number of rows before syntactic checks for the previous period, {date -&gt; size_raw_data}\n        final_rows (dict): dictionary indicating the number of rows after syntactic checks for the previous period, {date -&gt; size_raw_data}\n    \"\"\"\n    intermediate_df = self.input_data_objects[SilverNetworkDataQualityMetricsByColumn.ID].df.filter(\n        F.col(ColNames.date).between(self.lookback_period_start, self.lookback_period_end)\n    )\n\n    intermediate_df.cache()\n\n    lookback_stats = (\n        intermediate_df.groupBy([ColNames.field_name, ColNames.type_code])\n        .agg(F.mean(F.col(ColNames.value)).alias(\"average\"), F.stddev_samp(F.col(ColNames.value)).alias(\"stddev\"))\n        .collect()\n    )\n\n    error_rate_data = (\n        intermediate_df.select([ColNames.type_code, ColNames.value, ColNames.date])\n        .filter(F.col(ColNames.type_code).isin([NetworkErrorType.INITIAL_ROWS, NetworkErrorType.FINAL_ROWS]))\n        .collect()\n    )\n\n    intermediate_df.unpersist()\n\n    initial_rows = {}\n    final_rows = {}\n\n    for row in error_rate_data:\n        if row[ColNames.type_code] == NetworkErrorType.INITIAL_ROWS:\n            initial_rows[row[ColNames.date]] = row[\"value\"]\n        else:\n            final_rows[row[ColNames.date]] = row[\"value\"]\n\n    statistics = dict()\n    for row in lookback_stats:\n        field_name, type_code, average, stddev = (\n            row[ColNames.field_name],\n            row[ColNames.type_code],\n            row[\"average\"],\n            row[\"stddev\"],\n        )\n        if field_name not in statistics:\n            statistics[field_name] = dict()\n        statistics[field_name][type_code] = {\"average\": average, \"stddev\": stddev}\n\n    return statistics, initial_rows, final_rows\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.get_study_date_values","title":"<code>get_study_date_values()</code>","text":"<p>Method that reads and returns the quality metrics of the date of study.</p> <p>Returns:</p> Name Type Description <code>today_values</code> <code>dict</code> <p>dictionary containing said values, with the following structure: {     field_name1: {         type_error1: 123,         type_error2: 23,         type_error3: 0     },     field_name2: {         type_error1: 0,         type_error2: 0,         type_error3: 300     }, }</p> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def get_study_date_values(self) -&gt; dict:\n    \"\"\"Method that reads and returns the quality metrics of the date of study.\n\n    Returns:\n        today_values (dict): dictionary containing said values, with the following\n            structure:\n            {\n                field_name1: {\n                    type_error1: 123,\n                    type_error2: 23,\n                    type_error3: 0\n                },\n                field_name2: {\n                    type_error1: 0,\n                    type_error2: 0,\n                    type_error3: 300\n                },\n            }\n    \"\"\"\n    today_metrics = (\n        self.input_data_objects[SilverNetworkDataQualityMetricsByColumn.ID]\n        .df.filter(F.col(ColNames.date) == F.lit(self.date_of_study))\n        .select([ColNames.field_name, ColNames.type_code, ColNames.value])\n    ).collect()\n\n    today_values = {}\n\n    for row in today_metrics:\n        field_name, type_code, value = row[ColNames.field_name], row[ColNames.type_code], row[ColNames.value]\n\n        if field_name not in today_values:\n            today_values[field_name] = {}\n\n        today_values[field_name][type_code] = value\n\n    return today_values\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.get_thresholds","title":"<code>get_thresholds()</code>","text":"<p>Method that reads the threshold-related parameters, contained in the config file, and saves them in memory to use them instead of the ones specified by default. Raises:     ValueError: non-numerical value that cannot be parsed to float has been used in         the config file     ValueError: Negative value for a given parameter has been given, when only         non-negative values make sense and are thus allowed.</p> <p>Returns:</p> Name Type Description <code>thresholds</code> <code>dict</code> <p>dict containing the different thresholds used for computing the quality warnings, with the same structure as multimno.core.constants.network_default_thresholds.NETWORK_DEFAULT_THRESHOLDS</p> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def get_thresholds(self) -&gt; dict:\n    \"\"\"\n    Method that reads the threshold-related parameters, contained in the config file,\n    and saves them in memory to use them instead of the ones specified by default.\n    Raises:\n        ValueError: non-numerical value that cannot be parsed to float has been used in\n            the config file\n        ValueError: Negative value for a given parameter has been given, when only\n            non-negative values make sense and are thus allowed.\n\n    Returns:\n        thresholds: dict containing the different thresholds used for computing the\n            quality warnings, with the same structure as\n            multimno.core.constants.network_default_thresholds.NETWORK_DEFAULT_THRESHOLDS\n    \"\"\"\n    config_thresholds = self.config.geteval(self.COMPONENT_ID, \"thresholds\")\n    thresholds = NETWORK_DEFAULT_THRESHOLDS\n\n    for error_key in config_thresholds.keys():\n        if error_key not in NETWORK_DEFAULT_THRESHOLDS:\n            self.logger.info(f\"Parameter key {error_key} unknown -- ignored\")\n            continue\n\n        if error_key in [\"SIZE_RAW_DATA\", \"SIZE_CLEAN_DATA\", \"TOTAL_ERROR_RATE\"]:\n            for param_key, val in config_thresholds[error_key].items():\n                if param_key not in NETWORK_DEFAULT_THRESHOLDS[error_key]:\n                    self.logger.info(f\"Unknown parameter {param_key} under {error_key} -- ignored\")\n                    continue\n\n                try:\n                    val = float(val)\n                except ValueError as e:\n                    error_msg = (\n                        f\"Expected numeric value for parameter {param_key} under {error_key} - got {val} instead.\"\n                    )\n                    self.logger.error(error_msg)\n                    raise e\n\n                if val &lt; 0:\n                    error_msg = f\"Parameter {param_key} under {error_key} must be non-negative - got {val}\"\n                    self.logger.error(error_msg)\n                    raise ValueError(error_msg)\n\n                thresholds[error_key][param_key] = val\n\n        else:  # then is one of 'Missing_value_RATE', 'Out_of_range_RATE', 'Parsing_error_RATE'\n            for field_name in config_thresholds[error_key]:\n                if field_name not in NETWORK_DEFAULT_THRESHOLDS[error_key]:\n                    self.logger.info(f\"Unknown field name {field_name} under {error_key} -- ignored\")\n                    continue\n\n                for param_key, val in config_thresholds[error_key][field_name].items():\n                    if param_key not in NETWORK_DEFAULT_THRESHOLDS[error_key][field_name]:\n                        self.logger.info(\n                            f'Unknown parameter {param_key} under {error_key}[\"{field_name}\"] -- ignored'\n                        )\n                        continue\n\n                    try:\n                        val = float(val)\n                    except ValueError as e:\n                        error_msg = f'Expected numeric value for parameter {param_key} under {error_key}[\"{field_name}\"] - got {val} instead.'\n                        self.logger.error(error_msg)\n                        raise e\n\n                    if val &lt; 0:\n                        error_msg = f'Parameter {param_key} under {error_key}[\"{field_name}\"] must be non-negative - got {val}'\n                        self.logger.error(error_msg)\n                        raise ValueError(error_msg)\n\n                    thresholds[error_key][field_name][param_key] = val\n\n    return thresholds\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.raw_size_warnings","title":"<code>raw_size_warnings(lookback_stats, today_values)</code>","text":"<p>Method that generates the quality warnings, that will be recorded in the output log table, for the metric regarding the initial number of rows of the raw input network topology data prior to the syntactic check procedure.</p> A total of six warnings might be generated <ul> <li>The study date's number of rows is greater than the average number of rows over the previous period     by more than a threshold percentage.</li> <li>The study date's number of rows is smaller than the average number of rows over the previous period     by more than a threshold percentage.</li> <li>The study date's number of rows is greater than the average number of rows over the previous period     by a config-specified number of standard deviations.</li> <li>The study date's number of rows is smaller than the average number of rows over the previous period     by a config-specified number of standard deviations.</li> <li>The study date's number of rows is greater than a config-specified threshold.</li> <li>The study date's number of rows is smaller than a config-specified threshold.</li> </ul> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def raw_size_warnings(self, lookback_stats, today_values):\n    \"\"\"Method that generates the quality warnings, that will be recorded in the output log table,\n    for the metric regarding the initial number of rows of the raw input network topology data prior\n    to the syntactic check procedure.\n\n    A total of six warnings might be generated:\n        - The study date's number of rows is greater than the average number of rows over the previous period\n            by more than a threshold percentage.\n        - The study date's number of rows is smaller than the average number of rows over the previous period\n            by more than a threshold percentage.\n        - The study date's number of rows is greater than the average number of rows over the previous period\n            by a config-specified number of standard deviations.\n        - The study date's number of rows is smaller than the average number of rows over the previous period\n            by a config-specified number of standard deviations.\n        - The study date's number of rows is greater than a config-specified threshold.\n        - The study date's number of rows is smaller than a config-specified threshold.\n    \"\"\"\n    current_val = today_values[None][NetworkErrorType.INITIAL_ROWS]\n    previous_avg = lookback_stats[None][NetworkErrorType.INITIAL_ROWS][\"average\"]\n    previous_std = lookback_stats[None][NetworkErrorType.INITIAL_ROWS][\"stddev\"]\n\n    measure_definition = self.MEASURE_DEFINITION[\"SIZE_RAW_DATA\"]\n\n    over_average = self.thresholds[\"SIZE_RAW_DATA\"][\"OVER_AVERAGE\"]\n    under_average = self.thresholds[\"SIZE_RAW_DATA\"][\"UNDER_AVERAGE\"]\n    variability = self.thresholds[\"SIZE_RAW_DATA\"][\"VARIABILITY\"]\n    absolute_upper_control_limit = self.thresholds[\"SIZE_RAW_DATA\"][\"ABS_VALUE_UPPER_LIMIT\"]\n    absolute_lower_control_limit = self.thresholds[\"SIZE_RAW_DATA\"][\"ABS_VALUE_LOWER_LIMIT\"]\n\n    if previous_avg == 0:\n        self.logger.warning(\n            f\"\"\"\n            The average of the number of rows in the raw input network topology data is 0\n            for the lookback period especified ({self.lookback_period_start} to {self.lookback_period_end},\n             both included)\n            \"\"\"\n        )\n    else:\n        pct_difference = 100 * current_val / previous_avg - 100\n        if pct_difference &gt; over_average:\n            self.register_warning(\n                measure_definition,\n                pct_difference,\n                f\"Size is greater than the previous period's average by more than {over_average} %\",\n                over_average,\n                \"The number of cells is unexpectedly high with respect to the previous period, please check if there have been issues in the network\",\n            )\n\n        if pct_difference &lt; -under_average:\n            self.register_warning(\n                measure_definition,\n                pct_difference,\n                f\"Size is smaller than the previous period's average by more than {under_average} %\",\n                under_average,\n                \"The number of cells is unexpectedly low with respect to the previous period, please check if there have been issues in the network.\",\n            )\n\n    upper_control_limit = previous_avg + variability * previous_std\n    lower_control_limit = previous_avg - variability * previous_std\n\n    if current_val &gt; upper_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            \"Size is out of the upper control limit calculated on the basis of average and standard deviation of the distribution \"\n            f\"of the size in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\",\n            upper_control_limit,\n            \"The number of cells is unexpectedly high with respect to the previous period, taking into account the usual \"\n            \"variability of the cell numbers, please check if there have been issues in the network.\",\n        )\n    if current_val &lt; lower_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            \"Size is out of the lower control limit calculated on the basis of average and standard deviation of the distribution \"\n            f\"of the size in the previous period. Lower control limit = (average - {variability}\u00b7stddev)\",\n            lower_control_limit,\n            \"The number of cells is unexpectedly low with respect to the previous period, taking into account the usual \"\n            \"variability of the cell numbers, please check if there have been issues in the network.\",\n        )\n\n    # Use of UCL and LCL as default values for the absolute limits\n    if absolute_upper_control_limit is None:\n        absolute_upper_control_limit = upper_control_limit\n    if absolute_lower_control_limit is None:\n        absolute_lower_control_limit = lower_control_limit\n\n    if current_val &gt; absolute_upper_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            f\"The size is over the threshold {absolute_upper_control_limit}\",\n            absolute_upper_control_limit,\n            \"The number of cells is over the threshold, please check if there have been changes in the network.\",\n        )\n    if current_val &lt; absolute_lower_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            f\"The size is under the threshold {absolute_lower_control_limit}\",\n            absolute_upper_control_limit,\n            \"The number of cells is under the threshold, please check if there have been changes in the network.\",\n        )\n\n    return previous_avg, upper_control_limit, lower_control_limit\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.register_warning","title":"<code>register_warning(measure_definition, daily_value, condition, condition_value, warning_text)</code>","text":"<p>Method that abstracts away the creation in the correct format and data type, each of the quality warnings that will be recorded in the log table.</p> <p>Parameters:</p> Name Type Description Default <code>measure_definition</code> <code>str</code> <p>measure that raised the warning (e.g. Error rate)</p> required <code>daily_value</code> <code>float</code> <p>measure's value in the date of study that raised the warning</p> required <code>condition</code> <code>str</code> <p>test that was checked in order to raise the warning</p> required <code>condition_value</code> <code>float</code> <p>value against which the date of study's daily_value was compared</p> required <code>warning_text</code> <code>str</code> <p>verbose explanation of the condition being satisfied and the warning being raised</p> required Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def register_warning(\n    self, measure_definition: str, daily_value: float, condition: str, condition_value: float, warning_text: str\n) -&gt; None:\n    \"\"\"Method that abstracts away the creation in the correct format and data type, each of the quality warnings\n    that will be recorded in the log table.\n\n    Args:\n        measure_definition (str): measure that raised the warning (e.g. Error rate)\n        daily_value (float): measure's value in the date of study that raised the warning\n        condition (str): test that was checked in order to raise the warning\n        condition_value (float): value against which the date of study's daily_value was compared\n        warning_text (str): verbose explanation of the condition being satisfied and the warning\n            being raised\n    \"\"\"\n    warning = {\n        ColNames.title: self.TITLE,\n        ColNames.date: self.date_of_study,\n        ColNames.timestamp: self.timestamp,\n        ColNames.measure_definition: measure_definition,\n        ColNames.daily_value: float(daily_value),\n        ColNames.condition: condition,\n        ColNames.lookback_period: self.lookback_period,\n        ColNames.condition_value: float(condition_value),\n        ColNames.warning_text: warning_text,\n        ColNames.year: self.date_of_study.year,\n        ColNames.month: self.date_of_study.month,\n        ColNames.day: self.date_of_study.day,\n    }\n\n    self.warnings.append(Row(**warning))\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.specific_error_warnings","title":"<code>specific_error_warnings(error_rate_type, field_name, lookback_stats, today_values)</code>","text":"<p>Method that generates the quality warnings, that will be recorded in the output log table, for the metric regarding a specific error type considered in the network syntactic checks.</p> A total of three warnings might be generated <ul> <li>The study date's error rate for this error and this field is greater than the average number of rows over the previous period     by more than a threshold percentage.</li> <li>The study date's error rate for this error and this field is greater than the average number of rows over the previous period     by a config-specified number of standard deviations.</li> <li>The study date's error rate for this error and this field is greater than a config-specified threshold.</li> </ul> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def specific_error_warnings(self, error_rate_type, field_name, lookback_stats, today_values):\n    \"\"\"Method that generates the quality warnings, that will be recorded in the output log table,\n    for the metric regarding a specific error type considered in the network syntactic checks.\n\n    A total of three warnings might be generated:\n        - The study date's error rate for this error and this field is greater than the average number of rows over the previous period\n            by more than a threshold percentage.\n        - The study date's error rate for this error and this field is greater than the average number of rows over the previous period\n            by a config-specified number of standard deviations.\n        - The study date's error rate for this error and this field is greater than a config-specified threshold.\n    \"\"\"\n    if error_rate_type not in self.ERROR_TYPE:\n        raise ValueError(f\"Unknown error type for quality warning `{error_rate_type}`\")\n\n    network_error_type = self.ERROR_TYPE[error_rate_type]\n\n    over_average = self.thresholds[error_rate_type][field_name][\"AVERAGE\"]\n    variability = self.thresholds[error_rate_type][field_name][\"VARIABILITY\"]\n    absolute_upper_control_limit = self.thresholds[error_rate_type][field_name][\"ABS_VALUE_UPPER_LIMIT\"]\n\n    if network_error_type == NetworkErrorType.OUT_OF_RANGE and field_name is None:\n        field_name = \"dates\"\n    current_val = today_values[field_name][network_error_type]\n    previous_avg = lookback_stats[field_name][network_error_type][\"average\"]\n    previous_std = lookback_stats[field_name][network_error_type][\"stddev\"]\n\n    if previous_avg == 0:\n        self.logger.warning(\n            f\"\"\"\n            The average {error_rate_type} for field {field_name} in the input network topology data is 0 for the lookback period\n            especified ({self.lookback_period_start} to {self.lookback_period_end}, both included)\n            \"\"\"\n        )\n    else:\n        pct_difference = 100 * current_val / previous_avg - 100\n        if pct_difference &gt; over_average:\n            self.register_warning(\n                self.MEASURE_DEFINITION[error_rate_type](field_name=field_name),\n                pct_difference,\n                self.CONDITION[error_rate_type][\"AVERAGE\"](field_name=field_name, value=over_average),\n                over_average,\n                self.WARNING_MESSAGE[error_rate_type][\"AVERAGE\"](field_name=field_name),\n            )\n    upper_control_limit = previous_avg + variability * previous_std\n\n    if current_val &gt; upper_control_limit:\n        self.register_warning(\n            self.MEASURE_DEFINITION[error_rate_type](field_name=field_name),\n            current_val,\n            self.CONDITION[error_rate_type][\"VARIABILITY\"](field_name=field_name, variability=variability),\n            variability,\n            self.WARNING_MESSAGE[error_rate_type][\"VARIABILITY\"](field_name=field_name),\n        )\n\n    if current_val &gt; absolute_upper_control_limit:\n        self.register_warning(\n            self.MEASURE_DEFINITION[error_rate_type](field_name=field_name),\n            current_val,\n            self.CONDITION[error_rate_type][\"ABS_VALUE_UPPER_LIMIT\"](\n                field_name=field_name, value=absolute_upper_control_limit\n            ),\n            absolute_upper_control_limit,\n            self.WARNING_MESSAGE[error_rate_type][\"ABS_VALUE_UPPER_LIMIT\"](field_name=field_name),\n        )\n</code></pre>"},{"location":"reference/components/quality/semantic_quality_warnings/","title":"semantic_quality_warnings","text":""},{"location":"reference/components/quality/semantic_quality_warnings/semantic_quality_warnings/","title":"semantic_quality_warnings","text":""},{"location":"reference/components/quality/semantic_quality_warnings/semantic_quality_warnings/#components.quality.semantic_quality_warnings.semantic_quality_warnings.SemanticQualityWarnings","title":"<code>SemanticQualityWarnings</code>","text":"<p>               Bases: <code>Component</code></p> Source code in <code>multimno/components/quality/semantic_quality_warnings/semantic_quality_warnings.py</code> <pre><code>class SemanticQualityWarnings(Component):\n    \"\"\" \"\"\"\n\n    COMPONENT_ID = \"SemanticQualityWarnings\"\n\n    MINIMUM_STD_LOOKBACK_DAYS = 3\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.timestamp = datetime.datetime.now()\n\n        # Read date of study\n        self.date_format = self.config.get(self.COMPONENT_ID, \"date_format\")\n        date_of_study = self.config.get(self.COMPONENT_ID, \"date_of_study\")\n\n        try:\n            self.date_of_study = datetime.datetime.strptime(date_of_study, self.date_format).date()\n        except ValueError as e:\n            self.logger.error(str(e), exc_info=True)\n            raise e\n\n        self.thresholds = self.get_thresholds()\n\n        self.warning_long_format = []\n\n    def get_thresholds(self) -&gt; dict:\n        \"\"\"\n        Method that reads the threshold-related parameters, contained in the config file,\n        and saves them in memory to use them instead of the ones specified by default.\n\n        Raises:\n            ValueError: non-numerical value that cannot be parsed to float (or int) has been used in\n                the config file\n            ValueError: Negative value for a given parameter has been given, when only\n                non-negative values make sense and are thus allowed.\n\n        Returns:\n            thresholds: dict containing the different thresholds used for computing the\n                quality warnings, with the same structure as\n                multimno.core.constants.network_default_thresholds.ASEMANTIC_DEFAULT_THRESHOLDS\n        \"\"\"\n        config_thresholds = self.config.geteval(self.COMPONENT_ID, \"thresholds\")\n        thresholds = SEMANTIC_DEFAULT_THRESHOLDS\n\n        for error_key in config_thresholds.keys():\n            if error_key not in SEMANTIC_DEFAULT_THRESHOLDS:\n                self.logger.info(f\"Parameter key {error_key} unknown -- ignored\")\n                continue\n\n            for param_key, val in config_thresholds[error_key].items():\n                if param_key not in SEMANTIC_DEFAULT_THRESHOLDS[error_key]:\n                    self.logger.info(f\"Unknown parameter {param_key} under {error_key} -- ignored\")\n                    continue\n\n                if param_key == \"sd_lookback_days\":\n                    try:\n                        val = int(val)\n                    except ValueError as e:\n                        error_msg = (\n                            f\"Expected integer value for parameter {param_key} under {error_key} - got {val} instead.\"\n                        )\n                        self.logger.error(error_msg)\n                        raise e\n                else:\n                    try:\n                        val = float(val)\n                    except ValueError as e:\n                        error_msg = (\n                            f\"Expected numeric value for parameter {param_key} under {error_key} - got {val} instead.\"\n                        )\n                        self.logger.error(error_msg)\n                        raise e\n\n                if val &lt; 0:\n                    error_msg = f\"Parameter {param_key} under {error_key} must be non-negative - got {val}\"\n                    self.logger.error(error_msg)\n                    raise ValueError(error_msg)\n\n                thresholds[error_key][param_key] = val\n\n        return thresholds\n\n    def initalize_data_objects(self):\n        input_silver_quality_metrics_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"event_device_semantic_quality_metrics\"\n        )\n        output_silver_log_table_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"event_device_semantic_quality_warnings_log_table\"\n        )\n        output_silver_bar_plot_data_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"event_device_semantic_quality_warnings_bar_plot_data\"\n        )\n\n        silver_quality_metrics = SilverEventSemanticQualityMetrics(\n            self.spark,\n            input_silver_quality_metrics_path,\n        )\n\n        silver_log_table = SilverEventSemanticQualityWarningsLogTable(self.spark, output_silver_log_table_path)\n\n        silver_bar_plot_data = SilverEventSemanticQualityWarningsBarPlotData(\n            self.spark, output_silver_bar_plot_data_path\n        )\n\n        self.input_data_objects = {silver_quality_metrics.ID: silver_quality_metrics}\n        self.output_data_objects = {\n            silver_log_table.ID: silver_log_table,\n            silver_bar_plot_data.ID: silver_bar_plot_data,\n        }\n\n    def transform(self):\n        # Pushup filter, select only dates needed\n        # Since currently each QW has a different lookback period, we filter up to the\n        # furthest day in the past needed\n\n        metrics_df = self.input_data_objects[SilverEventSemanticQualityMetrics.ID].df\n\n        furthest_lookback = max(self.thresholds[key][\"sd_lookback_days\"] for key in self.thresholds.keys())\n\n        metrics_df = metrics_df.withColumn(\n            \"date\", F.make_date(year=F.col(ColNames.year), month=F.col(ColNames.month), day=F.col(ColNames.day))\n        ).filter(\n            F.col(\"date\").between(self.date_of_study - datetime.timedelta(days=furthest_lookback), self.date_of_study)\n        )\n\n        # Get all necessary metrics\n        error_counts = metrics_df.select([\"date\", ColNames.type_of_error, ColNames.value]).collect()\n\n        error_counts = [row.asDict() for row in error_counts]\n\n        error_stats = dict()\n        for count in error_counts:\n            date = count[\"date\"]\n            if date not in error_stats:\n                error_stats[date] = dict()\n\n            error_stats[date][count[ColNames.type_of_error]] = count[ColNames.value]\n\n        # If study date not present in the data, throw an exception\n        if self.date_of_study not in error_stats.keys():\n            raise ValueError(\n                f\"The date of study, {self.date_of_study.strftime(self.date_format)}, has no semantic checks metrics!\"\n            )\n\n        for key in error_stats.keys():\n            error_stats[key] = {\"count\": error_stats[key]}\n            error_stats[key][\"total\"] = sum(error_stats[key][\"count\"].values())\n            error_stats[key][\"percentage\"] = {\n                type_of_error: 100 * val / error_stats[key][\"total\"]\n                for type_of_error, val in error_stats[key][\"count\"].items()\n            }\n\n        for error_name in self.thresholds.keys():\n            self.quality_warnings_by_error(error_name, error_stats)\n\n        self.set_output_log_table()\n\n        self.create_plots_data(error_stats)\n\n    def quality_warnings_by_error(self, error_name: str, error_stats: dict):\n        \"\"\"Method that generates the quality warnings that will be recorded in the output log table,\n        for each type of error.\n\n        In the case that the data needed for a specific error's lookback period is not present, only the current date's\n        error percentage is computed and no warning is raised.\n\n        Args:\n            error_name (str): name of the error, an attribute of multimno.core.constants.error_types.SemanticErrorType\n            error_stats (dict): contains different values concerning each type of error, its counts, percentages, etc.\n        \"\"\"\n        # Get the code of the error given its name\n        error_code = getattr(SemanticErrorType, error_name)\n\n        # lookback days for this error\n        lookback_span = self.thresholds[error_name][\"sd_lookback_days\"]\n        lookback_dates = [self.date_of_study - datetime.timedelta(days=dd) for dd in range(1, lookback_span + 1)]\n\n        if not all(lookback_date in error_stats.keys() for lookback_date in lookback_dates):\n            # cannot compute lookback mean and average, so only showing this date's percentages\n            self.register_warning(\n                date=self.date_of_study,\n                error_code=error_code,\n                value=error_stats[self.date_of_study][\"percentage\"][error_code],\n                upper_control_limit=None,\n                display_warning=False,\n            )\n        else:\n            if lookback_span &lt; self.MINIMUM_STD_LOOKBACK_DAYS:\n                upper_control_limit = self.thresholds[error_name][\"min_percentage\"]\n                self.logger.info(\n                    f\"Lookback days for {error_name} lower than {self.MINIMUM_STD_LOOKBACK_DAYS} - using fixed \"\n                    f\"threshold {upper_control_limit}% instead of using average and standard deviation\"\n                )\n            else:\n                previous_avg = sum(error_stats[dd][\"percentage\"][error_code] for dd in lookback_dates) / lookback_span\n                previous_std = math.sqrt(\n                    sum((error_stats[dd][\"percentage\"][error_code] - previous_avg) ** 2 for dd in lookback_dates)\n                    / (lookback_span - 1)\n                )\n\n                upper_control_limit = previous_avg + self.thresholds[error_name][\"min_sd\"] * previous_std\n\n            # Now compare with todays value\n            if error_stats[self.date_of_study][\"percentage\"][error_code] &gt; upper_control_limit:\n                display_warning = True\n            else:\n                display_warning = False\n\n            self.register_warning(\n                date=self.date_of_study,\n                error_code=error_code,\n                value=error_stats[self.date_of_study][\"percentage\"][error_code],\n                upper_control_limit=upper_control_limit,\n                display_warning=display_warning,\n            )\n\n    def register_warning(\n        self, date: datetime.date, error_code: int, value: float, upper_control_limit: float, display_warning: bool\n    ):\n        \"\"\"Method that abstracts away the creation in the correct format and data type, each of the quality\n        warnings that will be recorded in the log table.\n\n        Args:\n            date (datetime.date): study date, for which the warnings are being calculated\n            error_code (int): code of the error\n            value (float): observed percentage of this specific error for the study date\n            upper_control_limit (float): upper control limit, used as threshold for the warning\n            display_warning (bool): whether the warning should be raised or not. It is currently independent of\n                the arguments values, but in theory it should be equal to (value &gt; control_limit)\n        \"\"\"\n        self.warning_long_format.append((date, error_code, value, upper_control_limit, display_warning))\n\n    def set_output_log_table(self):\n        \"\"\"\n        Method that formats the warnings into the expected table format\n        \"\"\"\n        warning_logs = pd.DataFrame(\n            self.warning_long_format, columns=[ColNames.date, ColNames.type_of_error, \"value\", \"UCL\", \"display\"]\n        ).pivot(index=ColNames.date, columns=[ColNames.type_of_error])\n        column_names = []\n        for name, code in warning_logs.columns:\n            if name == \"value\":\n                column_names.append(f\"Error {code}\")\n            elif name == \"UCL\":\n                column_names.append(f\"Error {code} upper control limit\")\n            elif name == \"display\":\n                column_names.append(f\"Error {code} display warning\")\n        warning_logs.columns = column_names\n        warning_logs = warning_logs.assign(execution_id=self.timestamp)\n        warning_logs = warning_logs.reset_index().assign(\n            **{\n                ColNames.year: lambda df_: df_[\"date\"].apply(lambda x: x.year),\n                ColNames.month: lambda df_: df_[\"date\"].apply(lambda x: x.month),\n                ColNames.day: lambda df_: df_[\"date\"].apply(lambda x: x.day),\n            }\n        )\n\n        # Force expected order of columns\n        warning_logs = warning_logs[SilverEventSemanticQualityWarningsLogTable.SCHEMA.names]\n\n        log_table_df = self.spark.createDataFrame(\n            warning_logs, schema=SilverEventSemanticQualityWarningsLogTable.SCHEMA\n        )\n\n        self.output_data_objects[SilverEventSemanticQualityWarningsLogTable.ID].df = log_table_df\n\n    def create_plots_data(self, error_stats):\n        \"\"\"\n        Method that takes the data needed to generate the required plots, and formats it for\n        easier plotting and saving later on.\n        \"\"\"\n        plot_data = []\n\n        def format_error_code(code):\n            if code == SemanticErrorType.NO_ERROR:\n                return \"No Error\"\n\n            return f\"Error {code}\"\n\n        for date in error_stats:\n            for error_code in error_stats[date][\"count\"]:\n                plot_data.append(\n                    Row(\n                        **{\n                            ColNames.date: date,\n                            ColNames.type_of_error: format_error_code(error_code),\n                            ColNames.value: float(error_stats[date][\"count\"][error_code]),\n                            ColNames.variable: \"Number of occurrences\",\n                            ColNames.year: self.date_of_study.year,\n                            ColNames.month: self.date_of_study.month,\n                            ColNames.day: self.date_of_study.day,\n                            ColNames.timestamp: self.timestamp,\n                        }\n                    )\n                )\n            for error_code in error_stats[date][\"percentage\"]:\n                plot_data.append(\n                    Row(\n                        **{\n                            ColNames.date: date,\n                            ColNames.type_of_error: format_error_code(error_code),\n                            ColNames.value: float(error_stats[date][\"percentage\"][error_code]),\n                            ColNames.variable: \"Percentage\",\n                            ColNames.year: self.date_of_study.year,\n                            ColNames.month: self.date_of_study.month,\n                            ColNames.day: self.date_of_study.day,\n                            ColNames.timestamp: self.timestamp,\n                        }\n                    )\n                )\n        self.output_data_objects[SilverEventSemanticQualityWarningsBarPlotData.ID].df = self.spark.createDataFrame(\n            plot_data, schema=SilverEventSemanticQualityWarningsBarPlotData.SCHEMA\n        )\n\n    def write(self):\n        # Write regular data objects\n        self.logger.info(\"Writing data objects...\")\n        super().write()\n        self.logger.info(\"Data objects written.\")\n</code></pre>"},{"location":"reference/components/quality/semantic_quality_warnings/semantic_quality_warnings/#components.quality.semantic_quality_warnings.semantic_quality_warnings.SemanticQualityWarnings.create_plots_data","title":"<code>create_plots_data(error_stats)</code>","text":"<p>Method that takes the data needed to generate the required plots, and formats it for easier plotting and saving later on.</p> Source code in <code>multimno/components/quality/semantic_quality_warnings/semantic_quality_warnings.py</code> <pre><code>def create_plots_data(self, error_stats):\n    \"\"\"\n    Method that takes the data needed to generate the required plots, and formats it for\n    easier plotting and saving later on.\n    \"\"\"\n    plot_data = []\n\n    def format_error_code(code):\n        if code == SemanticErrorType.NO_ERROR:\n            return \"No Error\"\n\n        return f\"Error {code}\"\n\n    for date in error_stats:\n        for error_code in error_stats[date][\"count\"]:\n            plot_data.append(\n                Row(\n                    **{\n                        ColNames.date: date,\n                        ColNames.type_of_error: format_error_code(error_code),\n                        ColNames.value: float(error_stats[date][\"count\"][error_code]),\n                        ColNames.variable: \"Number of occurrences\",\n                        ColNames.year: self.date_of_study.year,\n                        ColNames.month: self.date_of_study.month,\n                        ColNames.day: self.date_of_study.day,\n                        ColNames.timestamp: self.timestamp,\n                    }\n                )\n            )\n        for error_code in error_stats[date][\"percentage\"]:\n            plot_data.append(\n                Row(\n                    **{\n                        ColNames.date: date,\n                        ColNames.type_of_error: format_error_code(error_code),\n                        ColNames.value: float(error_stats[date][\"percentage\"][error_code]),\n                        ColNames.variable: \"Percentage\",\n                        ColNames.year: self.date_of_study.year,\n                        ColNames.month: self.date_of_study.month,\n                        ColNames.day: self.date_of_study.day,\n                        ColNames.timestamp: self.timestamp,\n                    }\n                )\n            )\n    self.output_data_objects[SilverEventSemanticQualityWarningsBarPlotData.ID].df = self.spark.createDataFrame(\n        plot_data, schema=SilverEventSemanticQualityWarningsBarPlotData.SCHEMA\n    )\n</code></pre>"},{"location":"reference/components/quality/semantic_quality_warnings/semantic_quality_warnings/#components.quality.semantic_quality_warnings.semantic_quality_warnings.SemanticQualityWarnings.get_thresholds","title":"<code>get_thresholds()</code>","text":"<p>Method that reads the threshold-related parameters, contained in the config file, and saves them in memory to use them instead of the ones specified by default.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>non-numerical value that cannot be parsed to float (or int) has been used in the config file</p> <code>ValueError</code> <p>Negative value for a given parameter has been given, when only non-negative values make sense and are thus allowed.</p> <p>Returns:</p> Name Type Description <code>thresholds</code> <code>dict</code> <p>dict containing the different thresholds used for computing the quality warnings, with the same structure as multimno.core.constants.network_default_thresholds.ASEMANTIC_DEFAULT_THRESHOLDS</p> Source code in <code>multimno/components/quality/semantic_quality_warnings/semantic_quality_warnings.py</code> <pre><code>def get_thresholds(self) -&gt; dict:\n    \"\"\"\n    Method that reads the threshold-related parameters, contained in the config file,\n    and saves them in memory to use them instead of the ones specified by default.\n\n    Raises:\n        ValueError: non-numerical value that cannot be parsed to float (or int) has been used in\n            the config file\n        ValueError: Negative value for a given parameter has been given, when only\n            non-negative values make sense and are thus allowed.\n\n    Returns:\n        thresholds: dict containing the different thresholds used for computing the\n            quality warnings, with the same structure as\n            multimno.core.constants.network_default_thresholds.ASEMANTIC_DEFAULT_THRESHOLDS\n    \"\"\"\n    config_thresholds = self.config.geteval(self.COMPONENT_ID, \"thresholds\")\n    thresholds = SEMANTIC_DEFAULT_THRESHOLDS\n\n    for error_key in config_thresholds.keys():\n        if error_key not in SEMANTIC_DEFAULT_THRESHOLDS:\n            self.logger.info(f\"Parameter key {error_key} unknown -- ignored\")\n            continue\n\n        for param_key, val in config_thresholds[error_key].items():\n            if param_key not in SEMANTIC_DEFAULT_THRESHOLDS[error_key]:\n                self.logger.info(f\"Unknown parameter {param_key} under {error_key} -- ignored\")\n                continue\n\n            if param_key == \"sd_lookback_days\":\n                try:\n                    val = int(val)\n                except ValueError as e:\n                    error_msg = (\n                        f\"Expected integer value for parameter {param_key} under {error_key} - got {val} instead.\"\n                    )\n                    self.logger.error(error_msg)\n                    raise e\n            else:\n                try:\n                    val = float(val)\n                except ValueError as e:\n                    error_msg = (\n                        f\"Expected numeric value for parameter {param_key} under {error_key} - got {val} instead.\"\n                    )\n                    self.logger.error(error_msg)\n                    raise e\n\n            if val &lt; 0:\n                error_msg = f\"Parameter {param_key} under {error_key} must be non-negative - got {val}\"\n                self.logger.error(error_msg)\n                raise ValueError(error_msg)\n\n            thresholds[error_key][param_key] = val\n\n    return thresholds\n</code></pre>"},{"location":"reference/components/quality/semantic_quality_warnings/semantic_quality_warnings/#components.quality.semantic_quality_warnings.semantic_quality_warnings.SemanticQualityWarnings.quality_warnings_by_error","title":"<code>quality_warnings_by_error(error_name, error_stats)</code>","text":"<p>Method that generates the quality warnings that will be recorded in the output log table, for each type of error.</p> <p>In the case that the data needed for a specific error's lookback period is not present, only the current date's error percentage is computed and no warning is raised.</p> <p>Parameters:</p> Name Type Description Default <code>error_name</code> <code>str</code> <p>name of the error, an attribute of multimno.core.constants.error_types.SemanticErrorType</p> required <code>error_stats</code> <code>dict</code> <p>contains different values concerning each type of error, its counts, percentages, etc.</p> required Source code in <code>multimno/components/quality/semantic_quality_warnings/semantic_quality_warnings.py</code> <pre><code>def quality_warnings_by_error(self, error_name: str, error_stats: dict):\n    \"\"\"Method that generates the quality warnings that will be recorded in the output log table,\n    for each type of error.\n\n    In the case that the data needed for a specific error's lookback period is not present, only the current date's\n    error percentage is computed and no warning is raised.\n\n    Args:\n        error_name (str): name of the error, an attribute of multimno.core.constants.error_types.SemanticErrorType\n        error_stats (dict): contains different values concerning each type of error, its counts, percentages, etc.\n    \"\"\"\n    # Get the code of the error given its name\n    error_code = getattr(SemanticErrorType, error_name)\n\n    # lookback days for this error\n    lookback_span = self.thresholds[error_name][\"sd_lookback_days\"]\n    lookback_dates = [self.date_of_study - datetime.timedelta(days=dd) for dd in range(1, lookback_span + 1)]\n\n    if not all(lookback_date in error_stats.keys() for lookback_date in lookback_dates):\n        # cannot compute lookback mean and average, so only showing this date's percentages\n        self.register_warning(\n            date=self.date_of_study,\n            error_code=error_code,\n            value=error_stats[self.date_of_study][\"percentage\"][error_code],\n            upper_control_limit=None,\n            display_warning=False,\n        )\n    else:\n        if lookback_span &lt; self.MINIMUM_STD_LOOKBACK_DAYS:\n            upper_control_limit = self.thresholds[error_name][\"min_percentage\"]\n            self.logger.info(\n                f\"Lookback days for {error_name} lower than {self.MINIMUM_STD_LOOKBACK_DAYS} - using fixed \"\n                f\"threshold {upper_control_limit}% instead of using average and standard deviation\"\n            )\n        else:\n            previous_avg = sum(error_stats[dd][\"percentage\"][error_code] for dd in lookback_dates) / lookback_span\n            previous_std = math.sqrt(\n                sum((error_stats[dd][\"percentage\"][error_code] - previous_avg) ** 2 for dd in lookback_dates)\n                / (lookback_span - 1)\n            )\n\n            upper_control_limit = previous_avg + self.thresholds[error_name][\"min_sd\"] * previous_std\n\n        # Now compare with todays value\n        if error_stats[self.date_of_study][\"percentage\"][error_code] &gt; upper_control_limit:\n            display_warning = True\n        else:\n            display_warning = False\n\n        self.register_warning(\n            date=self.date_of_study,\n            error_code=error_code,\n            value=error_stats[self.date_of_study][\"percentage\"][error_code],\n            upper_control_limit=upper_control_limit,\n            display_warning=display_warning,\n        )\n</code></pre>"},{"location":"reference/components/quality/semantic_quality_warnings/semantic_quality_warnings/#components.quality.semantic_quality_warnings.semantic_quality_warnings.SemanticQualityWarnings.register_warning","title":"<code>register_warning(date, error_code, value, upper_control_limit, display_warning)</code>","text":"<p>Method that abstracts away the creation in the correct format and data type, each of the quality warnings that will be recorded in the log table.</p> <p>Parameters:</p> Name Type Description Default <code>date</code> <code>date</code> <p>study date, for which the warnings are being calculated</p> required <code>error_code</code> <code>int</code> <p>code of the error</p> required <code>value</code> <code>float</code> <p>observed percentage of this specific error for the study date</p> required <code>upper_control_limit</code> <code>float</code> <p>upper control limit, used as threshold for the warning</p> required <code>display_warning</code> <code>bool</code> <p>whether the warning should be raised or not. It is currently independent of the arguments values, but in theory it should be equal to (value &gt; control_limit)</p> required Source code in <code>multimno/components/quality/semantic_quality_warnings/semantic_quality_warnings.py</code> <pre><code>def register_warning(\n    self, date: datetime.date, error_code: int, value: float, upper_control_limit: float, display_warning: bool\n):\n    \"\"\"Method that abstracts away the creation in the correct format and data type, each of the quality\n    warnings that will be recorded in the log table.\n\n    Args:\n        date (datetime.date): study date, for which the warnings are being calculated\n        error_code (int): code of the error\n        value (float): observed percentage of this specific error for the study date\n        upper_control_limit (float): upper control limit, used as threshold for the warning\n        display_warning (bool): whether the warning should be raised or not. It is currently independent of\n            the arguments values, but in theory it should be equal to (value &gt; control_limit)\n    \"\"\"\n    self.warning_long_format.append((date, error_code, value, upper_control_limit, display_warning))\n</code></pre>"},{"location":"reference/components/quality/semantic_quality_warnings/semantic_quality_warnings/#components.quality.semantic_quality_warnings.semantic_quality_warnings.SemanticQualityWarnings.set_output_log_table","title":"<code>set_output_log_table()</code>","text":"<p>Method that formats the warnings into the expected table format</p> Source code in <code>multimno/components/quality/semantic_quality_warnings/semantic_quality_warnings.py</code> <pre><code>def set_output_log_table(self):\n    \"\"\"\n    Method that formats the warnings into the expected table format\n    \"\"\"\n    warning_logs = pd.DataFrame(\n        self.warning_long_format, columns=[ColNames.date, ColNames.type_of_error, \"value\", \"UCL\", \"display\"]\n    ).pivot(index=ColNames.date, columns=[ColNames.type_of_error])\n    column_names = []\n    for name, code in warning_logs.columns:\n        if name == \"value\":\n            column_names.append(f\"Error {code}\")\n        elif name == \"UCL\":\n            column_names.append(f\"Error {code} upper control limit\")\n        elif name == \"display\":\n            column_names.append(f\"Error {code} display warning\")\n    warning_logs.columns = column_names\n    warning_logs = warning_logs.assign(execution_id=self.timestamp)\n    warning_logs = warning_logs.reset_index().assign(\n        **{\n            ColNames.year: lambda df_: df_[\"date\"].apply(lambda x: x.year),\n            ColNames.month: lambda df_: df_[\"date\"].apply(lambda x: x.month),\n            ColNames.day: lambda df_: df_[\"date\"].apply(lambda x: x.day),\n        }\n    )\n\n    # Force expected order of columns\n    warning_logs = warning_logs[SilverEventSemanticQualityWarningsLogTable.SCHEMA.names]\n\n    log_table_df = self.spark.createDataFrame(\n        warning_logs, schema=SilverEventSemanticQualityWarningsLogTable.SCHEMA\n    )\n\n    self.output_data_objects[SilverEventSemanticQualityWarningsLogTable.ID].df = log_table_df\n</code></pre>"},{"location":"reference/core/","title":"core","text":""},{"location":"reference/core/component/","title":"component","text":"<p>Module that defines the abstract pipeline component class</p>"},{"location":"reference/core/component/#core.component.Component","title":"<code>Component</code>","text":"<p>Class that models a pipeline component.</p> Source code in <code>multimno/core/component.py</code> <pre><code>class Component(metaclass=ABCMeta):\n    \"\"\"\n    Class that models a pipeline component.\n    \"\"\"\n\n    COMPONENT_ID: str = None\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        self.input_data_objects: Dict[str, DataObject] = None\n        self.output_data_objects: Dict[str, DataObject] = None\n\n        # Initialize variables\n        self.config: ConfigParser = parse_configuration(general_config_path, component_config_path)\n        self.logger: Logger = generate_logger(self.config, self.COMPONENT_ID)\n        self.spark: SparkSession = generate_spark_session(self.config)\n        self.initalize_data_objects()\n\n        # Log configuration\n        self.log_config()\n\n    @abstractmethod\n    def initalize_data_objects(self):\n        \"\"\"\n        Method that initializes the data objects associated with the component.\n        \"\"\"\n\n    def read(self):\n        \"\"\"\n        Method that performs the read operation of the input data objects of the component.\n        \"\"\"\n        for data_object in self.input_data_objects.values():\n            data_object.read()\n\n    @abstractmethod\n    def transform(self):\n        \"\"\"\n        Method that performs the data transformations needed to set the dataframes of the output\n         data objects from the input data objects.\n        \"\"\"\n\n    def write(self):\n        \"\"\"\n        Method that performs the write operation of the output data objects.\n        \"\"\"\n        for data_object in self.output_data_objects.values():\n            data_object.write()\n\n    @get_execution_stats\n    def execute(self):\n        \"\"\"\n        Method that performs the read, transform and write methods of the component.\n        \"\"\"\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n        self.transform()\n        self.write()\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def log_config(self):\n        \"\"\"\n        Method that logs all sections and key-value pairs of a ConfigParser object.\n        \"\"\"\n        # Validation\n        if self.config is None or self.logger is None:\n            return\n\n        # Log each section in order\n        for section in self.config.sections():\n            self.logger.info(f\"[{section}]\")\n            for key, value in self.config.items(section):\n                self.logger.info(f\"{key}: {value}\")\n            # Break line for each section\n            self.logger.info(\"\")\n</code></pre>"},{"location":"reference/core/component/#core.component.Component.execute","title":"<code>execute()</code>","text":"<p>Method that performs the read, transform and write methods of the component.</p> Source code in <code>multimno/core/component.py</code> <pre><code>@get_execution_stats\ndef execute(self):\n    \"\"\"\n    Method that performs the read, transform and write methods of the component.\n    \"\"\"\n    self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n    self.read()\n    self.transform()\n    self.write()\n    self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n</code></pre>"},{"location":"reference/core/component/#core.component.Component.initalize_data_objects","title":"<code>initalize_data_objects()</code>  <code>abstractmethod</code>","text":"<p>Method that initializes the data objects associated with the component.</p> Source code in <code>multimno/core/component.py</code> <pre><code>@abstractmethod\ndef initalize_data_objects(self):\n    \"\"\"\n    Method that initializes the data objects associated with the component.\n    \"\"\"\n</code></pre>"},{"location":"reference/core/component/#core.component.Component.log_config","title":"<code>log_config()</code>","text":"<p>Method that logs all sections and key-value pairs of a ConfigParser object.</p> Source code in <code>multimno/core/component.py</code> <pre><code>def log_config(self):\n    \"\"\"\n    Method that logs all sections and key-value pairs of a ConfigParser object.\n    \"\"\"\n    # Validation\n    if self.config is None or self.logger is None:\n        return\n\n    # Log each section in order\n    for section in self.config.sections():\n        self.logger.info(f\"[{section}]\")\n        for key, value in self.config.items(section):\n            self.logger.info(f\"{key}: {value}\")\n        # Break line for each section\n        self.logger.info(\"\")\n</code></pre>"},{"location":"reference/core/component/#core.component.Component.read","title":"<code>read()</code>","text":"<p>Method that performs the read operation of the input data objects of the component.</p> Source code in <code>multimno/core/component.py</code> <pre><code>def read(self):\n    \"\"\"\n    Method that performs the read operation of the input data objects of the component.\n    \"\"\"\n    for data_object in self.input_data_objects.values():\n        data_object.read()\n</code></pre>"},{"location":"reference/core/component/#core.component.Component.transform","title":"<code>transform()</code>  <code>abstractmethod</code>","text":"<p>Method that performs the data transformations needed to set the dataframes of the output  data objects from the input data objects.</p> Source code in <code>multimno/core/component.py</code> <pre><code>@abstractmethod\ndef transform(self):\n    \"\"\"\n    Method that performs the data transformations needed to set the dataframes of the output\n     data objects from the input data objects.\n    \"\"\"\n</code></pre>"},{"location":"reference/core/component/#core.component.Component.write","title":"<code>write()</code>","text":"<p>Method that performs the write operation of the output data objects.</p> Source code in <code>multimno/core/component.py</code> <pre><code>def write(self):\n    \"\"\"\n    Method that performs the write operation of the output data objects.\n    \"\"\"\n    for data_object in self.output_data_objects.values():\n        data_object.write()\n</code></pre>"},{"location":"reference/core/configuration/","title":"configuration","text":"<p>Module that manages the application configuration.</p>"},{"location":"reference/core/configuration/#core.configuration.parse_configuration","title":"<code>parse_configuration(general_config_path, component_config_path='')</code>","text":"<p>Function that parses a list of configurations in a single ConfigParser object. It expects the first element of the list to be the path to general configuration path. It will override values of the general configuration file with component configuration data.</p> <p>Parameters:</p> Name Type Description Default <code>general_config_path</code> <code>list</code> <p>Path to the general configuration file.</p> required <code>component_config_path</code> <code>str</code> <p>Path to the component configuration file.</p> <code>''</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the general configuration path is doesn't exist</p> <p>Returns:</p> Name Type Description <code>config</code> <code>ConfigParser</code> <p>ConfigParser object with the configuration data.</p> Source code in <code>multimno/core/configuration.py</code> <pre><code>def parse_configuration(general_config_path: str, component_config_path: str = \"\") -&gt; ConfigParser:\n    \"\"\"Function that parses a list of configurations in a single ConfigParser object. It expects\n    the first element of the list to be the path to general configuration path. It will override\n    values of the general configuration file with component configuration data.\n\n    Args:\n        general_config_path (list): Path to the general configuration file.\n        component_config_path (str): Path to the component configuration file.\n\n    Raises:\n        FileNotFoundError: If the general configuration path is doesn't exist\n\n    Returns:\n        config: ConfigParser object with the configuration data.\n    \"\"\"\n\n    # Check general configuration file\n    if not os.path.exists(general_config_path):\n        raise FileNotFoundError(f\"General Config file Not found: {general_config_path}\")\n\n    config_paths = [general_config_path, component_config_path]\n\n    converters = {\n        \"list\": lambda val: [i.strip() for i in val.strip().split(\"\\n\")],\n        \"eval\": eval,\n    }\n\n    parser: ConfigParser = ConfigParser(\n        converters=converters, interpolation=ExtendedInterpolation(), inline_comment_prefixes=\"#\"\n    )\n    parser.optionxform = str\n    parser.read(config_paths)\n\n    return parser\n</code></pre>"},{"location":"reference/core/exceptions/","title":"exceptions","text":""},{"location":"reference/core/exceptions/#core.exceptions.CriticalQualityWarningRaisedException","title":"<code>CriticalQualityWarningRaisedException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when a component raises quality warnings.</p> Source code in <code>multimno/core/exceptions.py</code> <pre><code>class CriticalQualityWarningRaisedException(Exception):\n    \"\"\"\n    Exception raised when a component raises quality warnings.\n    \"\"\"\n\n    def __init__(self, component_id: str) -&gt; None:\n        self.component_id = component_id\n        self.error_msg = (\n            f\"Critical Quality Warnings were raised during the execution of the component: {self.component_id}.\"\n        )\n        super().__init__(self.error_msg)\n</code></pre>"},{"location":"reference/core/exceptions/#core.exceptions.PpNoDevicesException","title":"<code>PpNoDevicesException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when no devices are found at the timepoint being calculated by the PresentPopulation class.</p> Source code in <code>multimno/core/exceptions.py</code> <pre><code>class PpNoDevicesException(Exception):\n    \"\"\"\n    Exception raised when no devices are found at the timepoint being calculated by the PresentPopulation class.\n    \"\"\"\n\n    def error_msg(self, time_point: str):\n        return f\"No devices found at {time_point} timepoint. Please check the MNO data of this day.\"\n</code></pre>"},{"location":"reference/core/grid/","title":"grid","text":"<p>This module provides functionality for generating a grid based on the INSPIRE grid system specification.</p>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator","title":"<code>InspireGridGenerator</code>","text":"<p>A class used to generate a grid based on the INSPIRE 100m grid system specification.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>class InspireGridGenerator:\n    \"\"\"A class used to generate a grid based on the INSPIRE 100m grid system specification.\"\"\"\n\n    GRID_CRS_EPSG_CODE = 3035\n    ACCEPTED_RESOLUTIONS = [100, 1000]\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        geometry_col: str = ColNames.geometry,\n        grid_id_col: str = ColNames.grid_id,\n        grid_partition_size: int = 2000,\n    ):\n        self.spark = spark\n        self.resolution = 100\n        self.resolution_str = self._format_distance(self.resolution)\n        self.geometry_col = geometry_col\n        self.grid_id_col = grid_id_col\n        self.grid_partition_size = grid_partition_size\n\n    @staticmethod\n    def _format_distance(value: int) -&gt; str:\n        \"\"\"Formats the given distance value to string.\n\n        Args:\n            value (int): The distance value to format.\n\n        Returns:\n            str: The formatted distance value.\n        \"\"\"\n        if value &lt; 1000:\n            return f\"{value}m\"\n        else:\n            if value % 1000 != 0:\n                raise ValueError(f\"Distance to be formatted not multiple of 1000: {value}\")\n            return f\"{value // 1000}km\"\n\n    def _project_latlon_extent(self, extent: List[float]) -&gt; Tuple[List[float], List[float]]:\n        \"\"\"Projects the given extent from lat/lon to the grid's CRS.\n\n        Args:\n            extent (List[float]): The extent to project. Order: [lon_min, lat_min, lon_max, lat_max]\n\n        Returns:\n            List[float]: The projected extent. Order: [northing_bottomleft, easting_bottomleft, northing_topright,\n                easting_topright]\n            list[float]: Auxiliar coordinates. Order: [northing_bottomright, easting_bottomright, northing_topleft,\n                easting_topleft]\n        \"\"\"\n        transformer = Transformer.from_crs(\"EPSG:4326\", f\"EPSG:{self.GRID_CRS_EPSG_CODE}\")\n        # This transformer follows the following convention (xx, yy are the first and second coords, respetively)\n        # EPSG4326: xx -&gt; lat, yy -&gt; lon\n        # EPSG3035: xx -&gt; northing, yy -&gt; easting\n        nn_bottomleft, ee_bottomleft = transformer.transform(extent[1], extent[0])  # bottom-left corner\n        nn_topright, ee_topright = transformer.transform(extent[3], extent[2])  # top-right corner\n        nn_bottomright, ee_bottomright = transformer.transform(extent[1], extent[2])  # bottom-right corner\n        nn_topleft, ee_topleft = transformer.transform(extent[3], extent[0])\n\n        return (\n            [nn_bottomleft, ee_bottomleft, nn_topright, ee_topright],\n            [nn_bottomright, ee_bottomright, nn_topleft, ee_topleft],\n        )\n\n    @staticmethod\n    def _project_bounding_box(extent: List[float], auxiliar_coords: List[float]) -&gt; Tuple[List[float], List[float]]:\n        \"\"\"Returns the bottom-left and top-right coordinates of the rectangular bounding box in the projected CRS\n        that covers the bounding box defined from the bottom-left and top-right corners in lat/lon.\n\n        Args:\n            extent (list[float]): Coordinates in the projected CRS that are the transformation of the minimum and\n                maximum latitude and longitude, in [n_bottomleft, e_bottomleft, n_topright, e_topright] order.\n            auxiliar_coords (list[float]): Auxiliar coordinates in the projected CRS that are the transformation\n                of the other two corners of the rectangular bounding box, in\n                [n_bottomright, e_bottomright, n_topleft, e_topleft] order\n\n        Returns:\n            list[float]: The projected extent, in [n_bottomleft, e_bottomleft, n_topright, y_topright] order.\n            list[float]: Raster cover bounds, in [n_topleft, e_topleft, n_bottomright, y_bottomright] order.\n        \"\"\"\n        cover_n_bottomleft = min(extent[0], auxiliar_coords[0])  # min lat\n        cover_e_bottomleft = min(extent[1], auxiliar_coords[3])  # min lon\n\n        cover_n_topright = max(extent[2], auxiliar_coords[2])  # max lat\n        cover_e_topright = max(extent[3], auxiliar_coords[1])  # max lon\n\n        cover_n_topleft = max(extent[2], auxiliar_coords[2])  # max lat\n        cover_e_topleft = min(extent[1], auxiliar_coords[3])  # min lon\n\n        cover_n_bottomright = min(extent[0], auxiliar_coords[0])  # min lat\n        cover_e_bottomright = max(extent[3], auxiliar_coords[1])  # max lon\n\n        return (\n            [cover_e_bottomleft, cover_n_bottomleft, cover_e_topright, cover_n_topright],\n            [cover_n_topleft, cover_e_topleft, cover_n_bottomright, cover_e_bottomright],\n        )\n\n    def _snap_extent_to_grid(self, extent: List[float]) -&gt; List[float]:\n        \"\"\"Snaps the given extent to the grid.\n\n        Args:\n            extent (list[float]): The extent to snap.\n\n        Returns:\n            list[float]: The snapped extent.\n        \"\"\"\n        return [round(coord / self.resolution) * self.resolution for coord in extent]\n\n    def _extend_grid_extent(self, extent: List[float], extension_factor: int = 5) -&gt; List[float]:\n        \"\"\"Extends the given extent by the specified factor in all directions.\n\n        Args:\n            extent (list[float]): The extent to extend.\n            extension_factor (int, optional): The factor by which to extend the extent. Defaults to 5.\n\n        Returns:\n            list[float]: The extended extent.\n        \"\"\"\n        extension_size = self.resolution * extension_factor\n        return [\n            extent[0] - extension_size,\n            extent[1] - extension_size,\n            extent[2] + extension_size,\n            extent[3] + extension_size,\n        ]\n\n    def _extend_grid_raster_bounds(self, raster_bounds: List[float], extension_factor: int = 5) -&gt; List[float]:\n        \"\"\"Extends the given extent by the specified factor in all directions.\n\n        Args:.\n            extent (list[float]): Raster cover bounds, in [x_topleft, y_topleft, x_bottomright, y_bottomright] order.\n            extension_factor (int, optional): The factor by which to extend the extent. Defaults to 5.\n\n        Returns:\n            list[float]: The extended extent.\n        \"\"\"\n        extension_size = self.resolution * extension_factor\n        return [\n            raster_bounds[0] + extension_size,  # n topleft\n            raster_bounds[1] - extension_size,  # e topleft\n            raster_bounds[2] - extension_size,  # n bottomright\n            raster_bounds[3] + extension_size,  # e bottomright\n        ]\n\n    def _get_grid_height(self, raster_bounds: List[float]) -&gt; int:\n        \"\"\"Calculates the height of the grid for the given extent.\n\n        Args:\n            raster_bounds (list[float]): The raster_bounds for which to calculate the grid height.\n\n        Returns:\n            int: The grid height.\n        \"\"\"\n        return int((raster_bounds[0] - raster_bounds[2]) / self.resolution)\n\n    def _get_grid_width(self, raster_bounds: List[float]) -&gt; int:\n        \"\"\"Calculates the width of the grid for the given extent.\n\n        Args:\n            raster_bounds (list[float]): The raster_bounds for which to calculate the grid width.\n\n        Returns:\n            int: The grid width.\n        \"\"\"\n        return int((raster_bounds[3] - raster_bounds[1]) / self.resolution)\n\n    def process_latlon_extent(self, extent: List[float]) -&gt; Tuple[List[float], List[float]]:\n        \"\"\"Takes an extent expressed in latitude and longitude (EPSG 4326), projects it into EPSG 3035, creates\n        bounding box, snaps to grid, and extends it some extra tiles in each direction.\n\n        Args:\n            extent (list[float]): The extent in lat/lon to process. Ordering is [lon_min, lat_min, lon_max, lat_max].\n\n        Returns:\n            extent (list[float]): Coordinates of the rectangle/bounding box that covers the projected and extended\n                extent. Order is [n_min, e_min, n_max, e_max] (bottom-left and top-right corners)\n            raster_bounds (list[float]): Appropriate raster bounds\n        \"\"\"\n        extent, auxiliar_coords = self._project_latlon_extent(extent)\n        extent, raster_bounds = self._project_bounding_box(extent, auxiliar_coords)\n\n        extent = self._snap_extent_to_grid(extent)\n        raster_bounds = self._snap_extent_to_grid(raster_bounds)\n\n        extent = self._extend_grid_extent(extent)\n        raster_bounds = self._extend_grid_raster_bounds(raster_bounds)\n        return extent, raster_bounds\n\n    def _get_grid_blueprint(self, extent: List[float]) -&gt; Tuple[DataFrame, List[float]]:\n        \"\"\"Generates a blueprint for the grid for the given extent as a raster of grid resolution.\n        Splits initial raster into smaller rasters of size grid_partition_size x grid_partition_size.\n\n        Args:\n            extent (List[float]): The extent in lat/lon for which to generate the grid blueprint. Ordering must be\n                [lon_min, lat_min, lon_max, lat_max].\n\n        Returns:\n            DataFrame: The grid blueprint.\n            proj_extent (List[float]): Coordinates of the rectangle/bounding box that covers the projected and extended\n                extent. Order is [n_min, e_min, n_max, e_max] (bottom-left and top-right corners)\n        \"\"\"\n        proj_extent, raster_bounds = self.process_latlon_extent(extent)\n\n        grid_height = self._get_grid_height(raster_bounds)\n        grid_width = self._get_grid_width(raster_bounds)\n\n        sdf = self.spark.sql(\n            f\"\"\"SELECT RS_MakeEmptyRaster(1, \"B\", {grid_width}, \n                                {grid_height}, \n                                {raster_bounds[1]},\n                                {raster_bounds[0]}, \n                                {self.resolution}, \n                               -{self.resolution}, 0.0, 0.0, {self.GRID_CRS_EPSG_CODE}) as raster\"\"\"\n        )\n\n        sdf = sdf.selectExpr(f\"RS_TileExplode(raster,{self.grid_partition_size}, {self.grid_partition_size})\")\n        return sdf.repartition(sdf.count()), proj_extent\n\n    @staticmethod\n    def _get_polygon_sdf_extent(polygon_sdf: DataFrame) -&gt; List[float]:\n        \"\"\"Gets the extent of the given polygon DataFrame. This method is currently used with geometry in\n        EPSG 4326, following order used by Sedona where the first coordinate X contains longitude and the second\n            coordinate Y contains latitude.\n\n        Args:\n            polygon_sdf (DataFrame): The polygon DataFrame.\n\n        Returns:\n            list[float]: The extent of the polygon DataFrame. Order: [x_min, y_min, x_max, y_max] where x and y\n                refer to the first and second coordinates of the geometry, respectively.\n        \"\"\"\n        polygon_sdf = polygon_sdf.withColumn(\"bbox\", STF.ST_Envelope(polygon_sdf[\"geometry\"]))\n        polygon_sdf = (\n            polygon_sdf.withColumn(\"x_min\", STF.ST_XMin(polygon_sdf[\"bbox\"]))\n            .withColumn(\"y_min\", STF.ST_YMin(polygon_sdf[\"bbox\"]))\n            .withColumn(\"x_max\", STF.ST_XMax(polygon_sdf[\"bbox\"]))\n            .withColumn(\"y_max\", STF.ST_YMax(polygon_sdf[\"bbox\"]))\n        )\n\n        return polygon_sdf.select(\"x_min\", \"y_min\", \"x_max\", \"y_max\").collect()[0][0:]\n\n    def _get_grid_intersection_with_mask(self, sdf: DataFrame, polygon_sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"Gets the intersection of the grid with the given polygon mask.\n\n        Args:\n            sdf (DataFrame): The DataFrame representing the grid.\n            polygon_sdf (DataFrame): The DataFrame representing the mask in EPSG:4326. Sedona function expects\n                first coordinate (X) to be longitude and second coordinate (Y) to be latitude.\n\n        Returns:\n            DataFrame: The DataFrame representing the intersection of the grid with the mask.\n        \"\"\"\n        polygon_sdf = polygon_sdf.withColumn(\n            \"geometry\",\n            STF.ST_Transform(polygon_sdf[\"geometry\"], F.lit(\"EPSG:4326\"), F.lit(f\"EPSG:{self.GRID_CRS_EPSG_CODE}\")),\n        )\n\n        sdf = sdf.join(polygon_sdf, STP.ST_Intersects(sdf[self.geometry_col], polygon_sdf[\"geometry\"]), \"inner\").drop(\n            polygon_sdf[\"geometry\"]\n        )\n\n        return sdf\n\n    def _get_grid_id_from_centroids(self, sdf: DataFrame, n_origin: int, e_origin: int) -&gt; DataFrame:\n        \"\"\"Takes a DataFrame that has point geometries in [self.geometry_col] column in EPSG:3035, representing the\n        centroids of grid tiles, and creates the internal unsigned 4-byte identifier used internally by the pipeline.\n\n        The 4-byte identifier consists of two parts:\n            The two most significant bytes represent the northing coordinate (X, or first coordinate of EPSG:3035)\n            The two least significant bytes represent the easting coordinate (Y, or second coordinate of EPSG:3035)\n\n        In order to fit all the necessary tiles into 4 bytes, we do a traslation of the coordinate system to have\n        a different origin, defined by (n_origin, e_origin)\n\n        Args:\n            sdf (DataFrame): DataFrame containing grid tile centroids to which we want to add the grid ID.\n            n_origin (int): Northing origin to be used in the internal 4-byte ID. In metres.\n            e_origin (int): Easting origin to be used in the internal 4-byte ID. In metres.\n\n        Returns:\n            DataFrame: DataFrame with a grid ID column added\n        \"\"\"\n        sdf = sdf.withColumn(\n            self.grid_id_col,\n            (\n                F.shiftleft(\n                    ((STF.ST_X(F.col(self.geometry_col)) - n_origin - self.resolution / 2) / self.resolution).cast(\n                        IntegerType()\n                    ),\n                    16,\n                )\n                + (\n                    ((STF.ST_Y(F.col(self.geometry_col)) - e_origin - self.resolution / 2) / self.resolution).cast(\n                        IntegerType()\n                    )\n                )\n            ),\n        )\n\n        # Add origin column\n        sdf = sdf.withColumn(\n            ColNames.origin,\n            (\n                F.shiftleft(F.lit(n_origin / self.resolution).cast(LongType()), 32)\n                + F.lit(e_origin / self.resolution).cast(LongType())\n            ).cast(LongType()),\n        )\n        return sdf\n\n    def _get_grid_id_from_grid_tiles(self, sdf: DataFrame, n_origin: int, e_origin: int) -&gt; DataFrame:\n        \"\"\"Takes a DataFrame that has tile geometries in [self.geometry_col] column in EPSG:3035, representing the\n        grid tiles, and creates the internal unsigned 4-byte identifier used internally by the pipeline.\n\n        The 4-byte identifier consists of two parts:\n            The two most significant bytes represent the northing coordinate (X, or first coordinate of EPSG:3035)\n            The two least significant bytes represent the easting coordinate (Y, or second coordinate of EPSG:3035)\n\n        In order to fit all the necessary tiles into 4 bytes, we do a traslation of the coordinate system to have\n        a different origin, defined by (n_origin, e_origin)\n\n        Args:\n            sdf (DataFrame): DataFrame containing grid tile centroids to which we want to add the grid ID.\n            n_origin (int): Northing origin to be used in the internal 4-byte ID. In metres.\n            e_origin (int): Easting origin to be used in the internal 4-byte ID. In metres.\n\n        Returns:\n            DataFrame: DataFrame with a grid ID column added\n        \"\"\"\n        sdf = sdf.withColumn(\n            self.grid_id_col,\n            (\n                F.shiftleft(\n                    ((STF.ST_XMin(F.col(self.geometry_col)) - n_origin) / self.resolution).cast(IntegerType()), 16\n                )\n                + (((STF.ST_YMin(F.col(self.geometry_col)) - e_origin) / self.resolution).cast(IntegerType()))\n            ),\n        )\n\n        # Add origin column\n        sdf = sdf.withColumn(\n            ColNames.origin,\n            (\n                F.shiftleft(F.lit(n_origin / self.resolution).cast(LongType()), 32)\n                + F.lit(e_origin / self.resolution).cast(LongType())\n            ).cast(LongType()),\n        )\n        return sdf\n\n    def cover_extent_with_grid_centroids(\n        self, extent: List[float], n_origin: int = None, e_origin: int = None\n    ) -&gt; DataFrame:\n        \"\"\"Covers the given extent with grid centroids. It takes an extent expressed in EPSG:4326 and covers it\n        with grid centroid point geometries in EPSG:3035, returning a DataFrame with these geometries, the internal\n        4-byte grid ID and the origin used to define the 4-byte ID. If both `n_origin` and `e_origin` are provided,\n        they are used as the origin of the ID; if not, the origin is taken from the provided extent.\n\n        It is desirable to define the origin using `n_origin` and `e_origin` when one wants to cover several extents\n        sharing the same origin, i.e. using the 4-byte grid ID defined in the same way for all of them.\n\n        Args:\n            extent (list[float]): The extent in lat/lon (EPSG:4326) to cover with grid centroids. Ordering must be\n                [lon_min, lat_min, lon_max, lat_max].\n            n_origin (int, optional): northing origin to be used for the 4-byte grid ID, in EPSG:3035 (metres).\n                Defaults to None.\n            e_origin (int, optional): easting origin to be used for the 4-byte grid ID, in EPSG:3035 (metres).\n                Defaults to None.\n\n        Returns:\n            DataFrame: The DataFrame representing the grid centroids covering the extent, with their grid ID and origin\n                columns.\n        \"\"\"\n        if (n_origin is None and e_origin is not None) or (n_origin is not None and e_origin is None):\n            raise ValueError(\"Either both or none of the arguments `n_origin` and `e_origin` must be passed\")\n\n        sdf, proj_extent = self._get_grid_blueprint(extent)\n\n        sdf = sdf.selectExpr(\"explode(RS_PixelAsCentroids(tile, 1)) as exploded\").selectExpr(\n            f\"exploded.geom as {self.geometry_col}\"\n        )\n\n        if n_origin is not None:\n            sdf = self._get_grid_id_from_centroids(sdf, n_origin=n_origin, e_origin=e_origin)\n        else:\n            sdf = self._get_grid_id_from_centroids(sdf, n_origin=proj_extent[0], e_origin=proj_extent[1])\n\n        return sdf\n\n    def cover_polygon_with_grid_centroids(\n        self, polygon_sdf: DataFrame, n_origin: int = None, e_origin: int = None\n    ) -&gt; DataFrame:\n        \"\"\"Covers the given polygon with grid centroids. It takes an polygon expressed in EPSG:4326 and covers it\n        with grid centroid point geometries in EPSG:3035, returning a DataFrame with these geometries, the internal\n        4-byte grid ID and the origin used to define the 4-byte ID. If both `n_origin` and `e_origin` are provided,\n        they are used as the origin of the ID; if not, the origin is taken from the extent covering the provided\n        polygon.\n\n        It is desirable to define the origin using `n_origin` and `e_origin` when one wants to cover several polygons\n        sharing the same origin, i.e. using the 4-byte grid ID defined in the same way for all of them.\n\n        Args:\n            polygon_sdf (DataFrame): DataFrame containing a single row with a polygon in EPSG:4326 in a column named\n                `geometry`.\n            n_origin (int, optional): northing origin to be used for the 4-byte grid ID, in EPSG:3035 (metres). Defaults to None.\n            e_origin (int, optional): easting origin to be used for the 4-byte grid ID, in EPSG:3035 (metres). Defaults to None.\n\n        Returns:\n            DataFrame: The DataFrame representing the grid centroids covering the polygon, with their grid ID and origin\n                columns.\n        \"\"\"\n        extent = self._get_polygon_sdf_extent(polygon_sdf)\n\n        sdf = self.cover_extent_with_grid_centroids(extent, n_origin, e_origin)\n\n        sdf = self._get_grid_intersection_with_mask(sdf, polygon_sdf)\n\n        return sdf\n\n    def cover_extent_with_grid_tiles(\n        self, extent: List[float], n_origin: int = None, e_origin: int = None\n    ) -&gt; Tuple[DataFrame, List[float]]:\n        \"\"\"Covers the given extent with grid tiles. It takes an extent expressed in EPSG:4326 and covers it\n        with grid tile polygon geometries in EPSG:3035, returning a DataFrame with these geometries, the internal\n        4-byte grid ID and the origin used to define the 4-byte ID. If both `n_origin` and `e_origin` are provided,\n        they are used as the origin of the ID; if not, the origin is taken from the provided extent.\n\n        It is desirable to define the origin using `n_origin` and `e_origin` when one wants to cover several extents\n        sharing the same origin, i.e. using the 4-byte grid ID defined in the same way for all of them.\n\n        Args:\n            extent (list[float]): The extent in lat/lon (EPSG:4326) to cover with grid tiles. Ordering must be\n                [lon_min, lat_min, lon_max, lat_max].\n            n_origin (int, optional): northing origin to be used for the 4-byte grid ID, in EPSG:3035 (metres). Defaults to None.\n            e_origin (int, optional): easting origin to be used for the 4-byte grid ID, in EPSG:3035 (metres). Defaults to None.\n\n        Returns:\n            DataFrame: The DataFrame representing the grid tiles covering the extent, with their grid ID and origin\n                columns.\n        \"\"\"\n        if (n_origin is None and e_origin is not None) or (n_origin is not None and e_origin is None):\n            raise ValueError(\"Either both or none of the arguments `n_origin` and `e_origin` must be passed\")\n\n        sdf, proj_extent = self._get_grid_blueprint(extent)\n\n        sdf = sdf.selectExpr(\"explode(RS_PixelAsPolygons(tile, 1)) as exploded\").selectExpr(\n            f\"exploded.geom as {self.geometry_col}\"\n        )\n\n        if n_origin is not None:\n            sdf = self._get_grid_id_from_grid_tiles(sdf, n_origin=n_origin, e_origin=e_origin)\n        else:\n            sdf = self._get_grid_id_from_grid_tiles(sdf, n_origin=proj_extent[0], e_origin=proj_extent[1])\n\n        return sdf\n\n    def cover_polygon_with_grid_tiles(self, polygon_sdf: DataFrame, n_origin: int, e_origin: int) -&gt; DataFrame:\n        \"\"\"Covers the given polygon with grid tiles. It takes an polygon expressed in EPSG:4326 and covers it\n        with grid tile polygon geometries in EPSG:3035, returning a DataFrame with these geometries, the internal\n        4-byte grid ID and the origin used to define the 4-byte ID. If both `n_origin` and `e_origin` are provided,\n        they are used as the origin of the ID; if not, the origin is taken from the polygon covering the provided\n        polygon.\n\n        It is desirable to define the origin using `n_origin` and `e_origin` when one wants to cover several polygons\n        sharing the same origin, i.e. using the 4-byte grid ID defined in the same way for all of them.\n\n        Args:\n            polygon_sdf (DataFrame): DataFrame containing a single row with a polygon in EPSG:4326 in a column named\n                `geometry`.\n            n_origin (int, optional): northing origin to be used for the 4-byte grid ID, in EPSG:3035 (metres). Defaults to None.\n            e_origin (int, optional): easting origin to be used for the 4-byte grid ID, in EPSG:3035 (metres). Defaults to None.\n\n        Returns:\n            DataFrame: The DataFrame representing the grid tiles covering the polygon, with their grid ID and origin\n                columns.\n        \"\"\"\n        extent = self._get_polygon_sdf_extent(polygon_sdf)\n\n        sdf = self.cover_extent_with_grid_tiles(extent, n_origin, e_origin)\n\n        sdf = self._get_grid_intersection_with_mask(sdf, polygon_sdf)\n\n        return sdf\n\n    def grid_id_to_inspire_id(\n        self, sdf: DataFrame, inspire_resolution: int, grid_id_col: str = None, origin: int = None\n    ) -&gt; DataFrame:\n        \"\"\"Function that takes a DataFrame containing 4-byte grid IDs and returns it with a new column containing\n        the official INSPIRE grid ID string. Only accepted INSPIRE grid resolutions are 100m and 1km.\n\n        It is expected that the grid ID column contains the internal representation for 100m grid tiles, and not for\n        a coarser resolution. If the 100m INSPIRE grid ID was requested, the ID corresponding to the 100m grid tile\n        represented by the internal grid ID is constructed. If the 1km INSPIRE grid ID was requested, the ID\n        corresponding to the 1km grid tile containing the internal grid ID is constructed.\n\n        By default, the function will use a ColNames.origin column of `sdf`. Only if the `origin` parameter is passed,\n        the existence of this column will not be checked, and `origin` will be used as the origin of the 4-byte grid ID\n        definition even if the column exists. This origin will be treated as an 8-byte integer, where the first (most\n        significant) 4 bytes hold the northing origin divided by 100 and the last (least significant) 4 bytes hold the\n        easting origin divided by 100. That is, taking the first 4 bytes and multiplying by 100 gets the a northing\n        value in metres (analogous for easting).\n\n        Args:\n            sdf (DataFrame): DataFrame containing the grid ID column, and a `ColNames.origin` column, to which the\n                INSPIRE grid ID is to be added\n            inspire_resolution (int): resolution for the INSPIRE grid ID. Currently accepts two value: `100` and `1000`.\n            grid_id_col (str, optional): Name of the column containing the internal 4-byte grid ID. If None, the value\n                `self.grid_id_col` is taken by default. Defaults to None\n            origin (int, optional): If provided, it will be used as the origin of the definition of the 4-byte grid ID.\n                It will ignore the ColNames.origin column even if it exists. If not provided, it is expected that\n                `sdf` contains a ColNames.origin column, and throws an error otherwise.\n\n        Returns:\n            DataFrame: DataFrame with a new column, `ColNames.inspire_id`, containing the INSPIRE grid ID strings.\n\n        Raises:\n            ValueError: If the `inspire_resolution` is not 100 or 1000.\n            ValueError: If the `origin` is not an integer.\n            ValueError: If the `sdf` does not contain a ColNames.origin column and `origin` is not passed.\n        \"\"\"\n        if grid_id_col is None:\n            grid_id_col = self.grid_id_col\n        if inspire_resolution not in self.ACCEPTED_RESOLUTIONS:\n            raise ValueError(\n                f\"Expected INSPIRE resolutions are {self.ACCEPTED_RESOLUTIONS} -- received `{inspire_resolution}`\"\n            )\n        if origin is not None:\n            if not isinstance(origin, int):\n                raise ValueError(f\"`origin` parameter must be an integer if used -- found type {type(origin)}\")\n            origin_column = F.lit(origin).cast(LongType())\n        else:\n            if ColNames.origin not in sdf.columns:\n                raise ValueError(f\"`sdf` must contain a {ColNames.origin} column, or `origin` parameter must be passed\")\n            origin_column = F.col(ColNames.origin)\n\n        sdf = sdf.withColumn(\n            \"northing\",\n            F.shiftrightunsigned(grid_id_col, 16).cast(LongType()) + F.shiftrightunsigned(origin_column, 32),\n        ).withColumn(\n            \"easting\",\n            F.col(grid_id_col).bitwiseAND((1 &lt;&lt; 16) - 1).cast(LongType()) + origin_column.bitwiseAND((1 &lt;&lt; 32) - 1),\n        )\n\n        # Substract the units digit to get the ID for 1km\n        if inspire_resolution == 1000:\n            sdf = sdf.withColumn(\"northing\", F.expr(\"northing DIV 10\")).withColumn(\"easting\", F.expr(\"easting DIV 10\"))\n        sdf = sdf.withColumn(\n            ColNames.inspire_id,\n            F.concat(\n                F.lit(self._format_distance(inspire_resolution)),\n                F.lit(\"N\"),\n                F.col(\"northing\"),\n                F.lit(\"E\"),\n                F.col(\"easting\"),\n            ),\n        ).drop(\"northing\", \"easting\")\n        return sdf\n\n    def grid_id_to_coarser_resolution(\n        self, sdf: DataFrame, coarse_resolution: int, coarse_grid_id_col: str = None\n    ) -&gt; DataFrame:\n        \"\"\"This function takes a DataFrame that contains the grid ID representation of 100m grid tiles, and transforms\n        it into a coarser resolution. It is always expected that the provided DataFrame has a grid ID that represents\n        100m grid tiles (in the `self.grid_id_col column`), and not a different resolution.\n\n        Notice that this method does not take into account the origin of the 4-byte grid IDs. Thus, the coarser grids\n        need not be compatible with the INSPIRE definition of a resolution coarser than 100m.\n\n        Args:\n            sdf (DataFrame): DataFrame for which a coarser resolution grid ID will be computed\n            coarse_resolution (int): coarser resolution to compute. Must be a multiple of `self.resolution`, i.e., 100.\n            coarse_grid_id_col (str, optional): column that will hold the IDs of the grid tiles in the coarser\n                resolution. If None, it will replace the original grid ID column. Defaults to None.\n\n        Returns:\n            DataFrame: DataFrame with the coarser grid IDs.\n        \"\"\"\n        if coarse_resolution % self.resolution != 0:\n            raise ValueError(f\"Coarser resolution {coarse_resolution} must be a multiple of {self.resolution}\")\n        if coarse_resolution &lt;= self.resolution:\n            raise ValueError(f\"Coarser resolution {coarse_resolution} must be greater than {self.resolution}\")\n\n        factor = coarse_resolution // self.resolution\n\n        if coarse_grid_id_col is None:\n            coarse_grid_id_col = self.grid_id_col\n\n        sdf = sdf.withColumn(\"northing\", F.shiftrightunsigned(ColNames.grid_id, 16)).withColumn(\n            \"easting\", F.col(ColNames.grid_id).bitwiseAND((1 &lt;&lt; 16) - 1)\n        )\n\n        sdf = sdf.withColumn(\"northing\", F.col(\"northing\") - F.col(\"northing\") % factor).withColumn(\n            \"easting\", F.col(\"easting\") - F.col(\"easting\") % factor\n        )\n\n        sdf = sdf.withColumn(coarse_grid_id_col, F.shiftleft(F.col(\"northing\"), 16) + F.col(\"easting\"))\n\n        sdf = sdf.drop(\"northing\", \"easting\")\n\n        return sdf\n\n    def grid_id_from_coarser_resolution(\n        self, sdf: DataFrame, coarse_resolution: int, coarse_grid_id_col: str, new_grid_id_col: str = None\n    ) -&gt; DataFrame:\n        \"\"\"This function takes a DataFrame that contains the grid ID representation of grid tiles in a resolution\n        coarser than 100m, and transforms it back into 100m.\n\n        Args:\n            sdf (DataFrame): DataFrame with grid IDs in a coarser resolution.\n            coarse_resolution (int): coarser resolution of the grid IDs of the provided DataFrame. Must be a multiple\n                of `self.resolution`, i.e., 100.\n            coarse_grid_id_col (str): column that currently holds the IDs of the grid tiles in the coarser\n                resolution.\n            new_grid_id_col (str, optional): column that will hold the IDs of the grid tiles in the 100m resolution.\n                If None, it will be set (and possible replace an existing column) as `self.grid_id_col`.\n                    Defaults to None.\n\n        Returns:\n            DataFrame: DataFrame with the coarser grid IDs.\n        \"\"\"\n        if coarse_resolution % self.resolution != 0:\n            raise ValueError(f\"Coarser resolution {coarse_resolution} must be a multiple of {self.resolution}\")\n        if coarse_resolution &lt;= self.resolution:\n            raise ValueError(f\"Coarser resolution {coarse_resolution} must be greater than {self.resolution}\")\n        if new_grid_id_col is None:\n            new_grid_id_col = self.grid_id_col\n\n        factor = coarse_resolution // self.resolution\n        offsets_df = self.spark.createDataFrame(\n            [(i &lt;&lt; 16) + j for i in range(factor) for j in range(factor)],\n            schema=StructType([StructField(\"offset\", IntegerType(), False)]),\n        )\n\n        offsets_df = F.broadcast(offsets_df)\n\n        sdf = (\n            sdf.crossJoin(offsets_df)\n            .withColumn(new_grid_id_col, F.col(coarse_grid_id_col) + F.col(\"offset\"))\n            .drop(\"offset\")\n        )\n\n        return sdf\n\n    def inspire_id_to_grid_centroids(\n        self, sdf: DataFrame, inspire_id_col: str = None, geometry_col: str = None\n    ) -&gt; DataFrame:\n        \"\"\"Function that takes a DataFrame containing INSPIRE grid ID strings and returns it with point geometries\n        of the centroids of the corresponding grid tiles. It extracts the units and grid size from the first element\n        of the DataFrame and uses it to construct the necessary geometries.\n\n        Args:\n            sdf (DataFrame): DataFrame containing the INSPIRE grid ID strings.\n            inspire_id_col (str, optional): name of the column holding the INSPIRE grid IDs. If None, it is set to\n                `ColNames.inspire_id`. Defaults to None.\n            geometry_col (str, optional): column that will hold the grid centroid geometries. If None, it is set to\n                `self.geometry`. Defaults to None.\n\n        Returns:\n            DataFrame: DataFrame with the grid centroid geometries\n        \"\"\"\n        if inspire_id_col is None:\n            inspire_id_col = ColNames.inspire_id\n        if geometry_col is None:\n            geometry_col = self.geometry_col\n\n        # First, get the INSPIRE resolution\n        resolution_str = sdf.select(F.regexp_extract(F.col(inspire_id_col), r\"^(.*?)N\", 1).alias(\"prefix\")).first()[\n            \"prefix\"\n        ]\n\n        # Parse and validate the INSPIRE resolution. Get the units and the grid size/resolution\n        if resolution_str[-2:] == \"km\":\n            try:\n                grid_size = int(resolution_str[:-2])\n            except ValueError:\n                raise ValueError(f\"Unexpected INSPIRE grid resolution string `{resolution_str}`\")\n            resolution_unit = 1000\n        elif resolution_str[-1:] == \"m\":\n            try:\n                grid_size = int(resolution_str[:-1])\n            except ValueError:\n                raise ValueError(f\"Unexpected INSPIRE grid resolution string `{resolution_str}`\")\n            resolution_unit = 100\n        else:\n            raise ValueError(f\"Unexpected INSPIRE grid resolution string `{resolution_str}`\")\n\n        # Create geometries. Multiply INSPIRE ID northing and easting values by the resolution unit, and add half\n        # the grid size to get the centroid of each tile\n        sdf = sdf.withColumn(\n            geometry_col,\n            STC.ST_Point(\n                F.regexp_extract(inspire_id_col, r\"E(\\d+)\", 1).cast(LongType()) * resolution_unit + grid_size // 2,\n                F.regexp_extract(inspire_id_col, r\"N(\\d+)E\", 1).cast(LongType()) * resolution_unit + grid_size // 2,\n            ),\n        )\n\n        # Set the CRS of the geometry\n        sdf = sdf.withColumn(geometry_col, STF.ST_SetSRID(geometry_col, self.GRID_CRS_EPSG_CODE))\n\n        return sdf\n\n    def inspire_id_to_grid_tiles(\n        self, sdf: DataFrame, inspire_id_col: str = None, geometry_col: str = None\n    ) -&gt; DataFrame:\n        \"\"\"Function that takes a DataFrame containing INSPIRE grid ID strings and returns it with polygon geometries\n        of the corresponding grid tiles. It extracts the units and grid size from the first element of the DataFrame\n        and uses it to construct the necessary geometries.\n\n        Args:\n            sdf (DataFrame): DataFrame containing the INSPIRE grid ID strings.\n            inspire_id_col (str, optional): name of the column holding the INSPIRE grid IDs. If None, it is set to\n                `ColNames.inspire_id`. Defaults to None.\n            geometry_col (str, optional): column that will hold the grid tile geometries. If None, it is set to\n                `ColNames.geometry`. Defaults to None.\n\n        Returns:\n            DataFrame: DataFrame with the grid centroid geometries\n        \"\"\"\n        if inspire_id_col is None:\n            inspire_id_col = ColNames.inspire_id\n        if geometry_col is None:\n            geometry_col = self.geometry_col\n\n        # First, get the INSPIRE resolution\n        resolution_str = sdf.select(F.regexp_extract(F.col(inspire_id_col), r\"^(.*?)N\", 1).alias(\"prefix\")).first()[\n            \"prefix\"\n        ]\n\n        # Parse and validate the INSPIRE resolution. Get the units and the grid size/resolution\n        if resolution_str[-2:] == \"km\":\n            try:\n                grid_size = int(resolution_str[:-2])\n            except ValueError:\n                raise ValueError(f\"Unexpected INSPIRE grid resolution string `{resolution_str}`\")\n            resolution_unit = 1000\n        elif resolution_str[-1:] == \"m\":\n            try:\n                grid_size = int(resolution_str[:-1])\n            except ValueError:\n                raise ValueError(f\"Unexpected INSPIRE grid resolution string `{resolution_str}`\")\n            resolution_unit = 100\n        else:\n            raise ValueError(f\"Unexpected INSPIRE grid resolution string `{resolution_str}`\")\n\n        sdf = sdf.withColumn(\n            \"northing\", F.regexp_extract(inspire_id_col, r\"N(\\d+)E\", 1).cast(LongType()) * resolution_unit\n        ).withColumn(\"easting\", F.regexp_extract(inspire_id_col, r\"E(\\d+)\", 1).cast(LongType()) * resolution_unit)\n\n        # Sedona has (X, Y) = (Easting, Northing) for EPSG 3035\n        sdf = sdf.withColumn(\n            geometry_col,\n            STC.ST_PolygonFromEnvelope(\n                F.col(\"easting\"),  # min_x (min_easting)\n                F.col(\"northing\"),  # min_y, (min_northing)\n                F.col(\"easting\") + F.lit(resolution_unit * grid_size),  # max_x (max_easting)\n                F.col(\"northing\") + F.lit(resolution_unit * grid_size),  # max_y (max_northing)\n            ),\n        )\n\n        sdf = sdf.drop(\"northing\", \"easting\")\n\n        # Set the CRS of the geometry\n        sdf = sdf.withColumn(geometry_col, STF.ST_SetSRID(geometry_col, self.GRID_CRS_EPSG_CODE))\n\n        return sdf\n\n    def grid_ids_to_grid_centroids(\n        self,\n        sdf: DataFrame,\n        grid_resolution: int,\n        grid_id_col: str = None,\n        geometry_col: str = None,\n        origin: int = None,\n    ) -&gt; DataFrame:\n        \"\"\"Function that takes a DataFrame containing internal 4-byte grid IDs and returns it with point geometries\n        of the centroids of the corresponding grid tiles.\n\n        By default, the function will use a ColNames.origin column of `sdf`. Only if the `origin` parameter is passed,\n        the existence of this column will not be checked, and `origin` will be used as the origin of the 4-byte grid ID\n        definition even if the column exists. This origin will be treated as an 8-byte integer, where the first (most\n        significant) 4 bytes hold the northing origin divided by 100 and the last (least significant) 4 bytes hold the\n        easting origin divided by 100. That is, taking the first 4 bytes and multiplying by 100 gets the a northing\n        value in metres (analogous for easting).\n\n        Args:\n            sdf (DataFrame): DataFrame containing the internal 4-byte grid IDs.\n            grid_resolution (int): resolution, in metres, of the current grid as represented by the internal 4-byte\n                grid IDs. Must be a multiple of `self.resolution`. i.e. 100.\n            grid_id_col (str, optional): column that holds the internal grid IDs. If None, it is set to\n                `self.grid_id_col`. Defaults to None.\n            geometry_col (str, optional): column that will hold the grid centroid geometries. If None, it is set to\n                `self.geometry_col`. Defaults to None.\n            origin (int, optional): If provided, it will be used as the origin of the definition of the 4-byte grid ID.\n                It will ignore the ColNames.origin column even if it exists. If not provided, it is expected that\n                `sdf` contains a ColNames.origin column, and throws an error otherwise.\n\n        Returns:\n            DataFrame: DataFrame with the grid centroid geometries\n        \"\"\"\n\n        if grid_resolution % self.resolution != 0:\n            raise ValueError(f\"Grid resolution must be a multiple of {self.resolution}\")\n        if geometry_col is None:\n            geometry_col = self.geometry_col\n        if grid_id_col is None:\n            grid_id_col = self.grid_id_col\n        if origin is not None:\n            if not isinstance(origin, int):\n                raise ValueError(f\"`origin` parameter must be an integer if used -- found type {type(origin)}\")\n            origin_column = F.lit(origin).cast(LongType())\n        else:\n            if ColNames.origin not in sdf.columns:\n                raise ValueError(f\"`sdf` must contain a {ColNames.origin} column, or `origin` parameter must be passed\")\n            origin_column = F.col(ColNames.origin)\n\n        # For Sedona, (X, Y) == (Northing, Easting) in EPSG 3035\n        sdf = sdf.withColumn(\n            geometry_col,\n            STC.ST_Point(\n                (F.shiftrightunsigned(ColNames.grid_id, 16).cast(LongType()) + F.shiftrightunsigned(origin_column, 32))\n                * self.resolution\n                + grid_resolution // 2,\n                (\n                    F.col(ColNames.grid_id).bitwiseAND((1 &lt;&lt; 16) - 1).cast(LongType())\n                    + origin_column.bitwiseAND((1 &lt;&lt; 32) - 1)\n                )\n                * self.resolution\n                + grid_resolution // 2,\n            ),\n        )\n\n        sdf = sdf.withColumn(geometry_col, STF.ST_SetSRID(geometry_col, self.GRID_CRS_EPSG_CODE))\n\n        return sdf\n\n    def grid_ids_to_grid_tiles(\n        self, sdf: DataFrame, grid_resolution: int, geometry_col: str = None, origin: int = None\n    ) -&gt; DataFrame:\n        \"\"\"Function that takes a DataFrame containing internal 4-byte grid IDs and returns it with polygon geometries\n        of the corresponding grid tiles.\n\n        By default, the function will use a ColNames.origin column of `sdf`. Only if the `origin` parameter is passed,\n        the existence of this column will not be checked, and `origin` will be used as the origin of the 4-byte grid ID\n        definition even if the column exists. This origin will be treated as an 8-byte integer, where the first (most\n        significant) 4 bytes hold the northing origin divided by 100 and the last (least significant) 4 bytes hold the\n        easting origin divided by 100. That is, taking the first 4 bytes and multiplying by 100 gets the a northing\n        value in metres (analogous for easting).\n\n        Args:\n            sdf (DataFrame): DataFrame containing the internal 4-byte grid IDs.\n            grid_resolution (int): resolution, in metres, of the current grid as represented by the internal 4-byte\n                grid IDs. Must be a multiple of `self.resolution`. i.e. 100.\n            geometry_col (str, optional): column that will hold the grid tile geometries. If None, it is set to\n                `self.geometry_col`. Defaults to None.\n            origin (int, optional): If provided, it will be used as the origin of the definition of the 4-byte grid ID.\n                It will ignore the ColNames.origin column even if it exists. If not provided, it is expected that\n                `sdf` contains a ColNames.origin column, and throws an error otherwise.\n\n        Returns:\n            DataFrame: DataFrame with the grid centroid geometries\n        \"\"\"\n        if grid_resolution % self.resolution != 0:\n            raise ValueError(f\"Grid resolution must be a multiple of {self.resolution}\")\n        if geometry_col is None:\n            geometry_col = self.geometry_col\n        if origin is not None:\n            if not isinstance(origin, int):\n                raise ValueError(f\"`origin` parameter must be an integer if used -- found type {type(origin)}\")\n            origin_column = F.lit(origin).cast(LongType())\n        else:\n            if ColNames.origin not in sdf.columns:\n                raise ValueError(f\"`sdf` must contain a {ColNames.origin} column, or `origin` parameter must be passed\")\n            origin_column = F.col(ColNames.origin)\n\n        sdf = sdf.withColumn(\n            \"northing\",\n            (F.shiftrightunsigned(ColNames.grid_id, 16).cast(LongType()) + F.shiftrightunsigned(origin_column, 32))\n            * self.resolution,\n        ).withColumn(\n            \"easting\",\n            (\n                F.col(ColNames.grid_id).bitwiseAND((1 &lt;&lt; 16) - 1).cast(LongType())\n                + origin_column.bitwiseAND((1 &lt;&lt; 32) - 1)\n            )\n            * self.resolution,\n        )\n\n        # For Sedona, (X, Y) == (Northing, Easting) in EPSG 3035\n        sdf = sdf.withColumn(\n            geometry_col,\n            STC.ST_PolygonFromEnvelope(\n                F.col(\"northing\"),  # min_x (min_northing)\n                F.col(\"easting\"),  # min_y (min_easting)\n                F.col(\"northing\") + grid_resolution,  # max_x (max_northing)\n                F.col(\"easting\") + grid_resolution,  # max_y (max_easting)\n            ),\n        )\n\n        sdf = sdf.drop(\"northing\", \"easting\")\n\n        # Set the CRS of the geometries\n        sdf = sdf.withColumn(geometry_col, STF.ST_SetSRID(geometry_col, self.GRID_CRS_EPSG_CODE))\n\n        return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.cover_extent_with_grid_centroids","title":"<code>cover_extent_with_grid_centroids(extent, n_origin=None, e_origin=None)</code>","text":"<p>Covers the given extent with grid centroids. It takes an extent expressed in EPSG:4326 and covers it with grid centroid point geometries in EPSG:3035, returning a DataFrame with these geometries, the internal 4-byte grid ID and the origin used to define the 4-byte ID. If both <code>n_origin</code> and <code>e_origin</code> are provided, they are used as the origin of the ID; if not, the origin is taken from the provided extent.</p> <p>It is desirable to define the origin using <code>n_origin</code> and <code>e_origin</code> when one wants to cover several extents sharing the same origin, i.e. using the 4-byte grid ID defined in the same way for all of them.</p> <p>Parameters:</p> Name Type Description Default <code>extent</code> <code>list[float]</code> <p>The extent in lat/lon (EPSG:4326) to cover with grid centroids. Ordering must be [lon_min, lat_min, lon_max, lat_max].</p> required <code>n_origin</code> <code>int</code> <p>northing origin to be used for the 4-byte grid ID, in EPSG:3035 (metres). Defaults to None.</p> <code>None</code> <code>e_origin</code> <code>int</code> <p>easting origin to be used for the 4-byte grid ID, in EPSG:3035 (metres). Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The DataFrame representing the grid centroids covering the extent, with their grid ID and origin columns.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def cover_extent_with_grid_centroids(\n    self, extent: List[float], n_origin: int = None, e_origin: int = None\n) -&gt; DataFrame:\n    \"\"\"Covers the given extent with grid centroids. It takes an extent expressed in EPSG:4326 and covers it\n    with grid centroid point geometries in EPSG:3035, returning a DataFrame with these geometries, the internal\n    4-byte grid ID and the origin used to define the 4-byte ID. If both `n_origin` and `e_origin` are provided,\n    they are used as the origin of the ID; if not, the origin is taken from the provided extent.\n\n    It is desirable to define the origin using `n_origin` and `e_origin` when one wants to cover several extents\n    sharing the same origin, i.e. using the 4-byte grid ID defined in the same way for all of them.\n\n    Args:\n        extent (list[float]): The extent in lat/lon (EPSG:4326) to cover with grid centroids. Ordering must be\n            [lon_min, lat_min, lon_max, lat_max].\n        n_origin (int, optional): northing origin to be used for the 4-byte grid ID, in EPSG:3035 (metres).\n            Defaults to None.\n        e_origin (int, optional): easting origin to be used for the 4-byte grid ID, in EPSG:3035 (metres).\n            Defaults to None.\n\n    Returns:\n        DataFrame: The DataFrame representing the grid centroids covering the extent, with their grid ID and origin\n            columns.\n    \"\"\"\n    if (n_origin is None and e_origin is not None) or (n_origin is not None and e_origin is None):\n        raise ValueError(\"Either both or none of the arguments `n_origin` and `e_origin` must be passed\")\n\n    sdf, proj_extent = self._get_grid_blueprint(extent)\n\n    sdf = sdf.selectExpr(\"explode(RS_PixelAsCentroids(tile, 1)) as exploded\").selectExpr(\n        f\"exploded.geom as {self.geometry_col}\"\n    )\n\n    if n_origin is not None:\n        sdf = self._get_grid_id_from_centroids(sdf, n_origin=n_origin, e_origin=e_origin)\n    else:\n        sdf = self._get_grid_id_from_centroids(sdf, n_origin=proj_extent[0], e_origin=proj_extent[1])\n\n    return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.cover_extent_with_grid_tiles","title":"<code>cover_extent_with_grid_tiles(extent, n_origin=None, e_origin=None)</code>","text":"<p>Covers the given extent with grid tiles. It takes an extent expressed in EPSG:4326 and covers it with grid tile polygon geometries in EPSG:3035, returning a DataFrame with these geometries, the internal 4-byte grid ID and the origin used to define the 4-byte ID. If both <code>n_origin</code> and <code>e_origin</code> are provided, they are used as the origin of the ID; if not, the origin is taken from the provided extent.</p> <p>It is desirable to define the origin using <code>n_origin</code> and <code>e_origin</code> when one wants to cover several extents sharing the same origin, i.e. using the 4-byte grid ID defined in the same way for all of them.</p> <p>Parameters:</p> Name Type Description Default <code>extent</code> <code>list[float]</code> <p>The extent in lat/lon (EPSG:4326) to cover with grid tiles. Ordering must be [lon_min, lat_min, lon_max, lat_max].</p> required <code>n_origin</code> <code>int</code> <p>northing origin to be used for the 4-byte grid ID, in EPSG:3035 (metres). Defaults to None.</p> <code>None</code> <code>e_origin</code> <code>int</code> <p>easting origin to be used for the 4-byte grid ID, in EPSG:3035 (metres). Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>Tuple[DataFrame, List[float]]</code> <p>The DataFrame representing the grid tiles covering the extent, with their grid ID and origin columns.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def cover_extent_with_grid_tiles(\n    self, extent: List[float], n_origin: int = None, e_origin: int = None\n) -&gt; Tuple[DataFrame, List[float]]:\n    \"\"\"Covers the given extent with grid tiles. It takes an extent expressed in EPSG:4326 and covers it\n    with grid tile polygon geometries in EPSG:3035, returning a DataFrame with these geometries, the internal\n    4-byte grid ID and the origin used to define the 4-byte ID. If both `n_origin` and `e_origin` are provided,\n    they are used as the origin of the ID; if not, the origin is taken from the provided extent.\n\n    It is desirable to define the origin using `n_origin` and `e_origin` when one wants to cover several extents\n    sharing the same origin, i.e. using the 4-byte grid ID defined in the same way for all of them.\n\n    Args:\n        extent (list[float]): The extent in lat/lon (EPSG:4326) to cover with grid tiles. Ordering must be\n            [lon_min, lat_min, lon_max, lat_max].\n        n_origin (int, optional): northing origin to be used for the 4-byte grid ID, in EPSG:3035 (metres). Defaults to None.\n        e_origin (int, optional): easting origin to be used for the 4-byte grid ID, in EPSG:3035 (metres). Defaults to None.\n\n    Returns:\n        DataFrame: The DataFrame representing the grid tiles covering the extent, with their grid ID and origin\n            columns.\n    \"\"\"\n    if (n_origin is None and e_origin is not None) or (n_origin is not None and e_origin is None):\n        raise ValueError(\"Either both or none of the arguments `n_origin` and `e_origin` must be passed\")\n\n    sdf, proj_extent = self._get_grid_blueprint(extent)\n\n    sdf = sdf.selectExpr(\"explode(RS_PixelAsPolygons(tile, 1)) as exploded\").selectExpr(\n        f\"exploded.geom as {self.geometry_col}\"\n    )\n\n    if n_origin is not None:\n        sdf = self._get_grid_id_from_grid_tiles(sdf, n_origin=n_origin, e_origin=e_origin)\n    else:\n        sdf = self._get_grid_id_from_grid_tiles(sdf, n_origin=proj_extent[0], e_origin=proj_extent[1])\n\n    return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.cover_polygon_with_grid_centroids","title":"<code>cover_polygon_with_grid_centroids(polygon_sdf, n_origin=None, e_origin=None)</code>","text":"<p>Covers the given polygon with grid centroids. It takes an polygon expressed in EPSG:4326 and covers it with grid centroid point geometries in EPSG:3035, returning a DataFrame with these geometries, the internal 4-byte grid ID and the origin used to define the 4-byte ID. If both <code>n_origin</code> and <code>e_origin</code> are provided, they are used as the origin of the ID; if not, the origin is taken from the extent covering the provided polygon.</p> <p>It is desirable to define the origin using <code>n_origin</code> and <code>e_origin</code> when one wants to cover several polygons sharing the same origin, i.e. using the 4-byte grid ID defined in the same way for all of them.</p> <p>Parameters:</p> Name Type Description Default <code>polygon_sdf</code> <code>DataFrame</code> <p>DataFrame containing a single row with a polygon in EPSG:4326 in a column named <code>geometry</code>.</p> required <code>n_origin</code> <code>int</code> <p>northing origin to be used for the 4-byte grid ID, in EPSG:3035 (metres). Defaults to None.</p> <code>None</code> <code>e_origin</code> <code>int</code> <p>easting origin to be used for the 4-byte grid ID, in EPSG:3035 (metres). Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The DataFrame representing the grid centroids covering the polygon, with their grid ID and origin columns.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def cover_polygon_with_grid_centroids(\n    self, polygon_sdf: DataFrame, n_origin: int = None, e_origin: int = None\n) -&gt; DataFrame:\n    \"\"\"Covers the given polygon with grid centroids. It takes an polygon expressed in EPSG:4326 and covers it\n    with grid centroid point geometries in EPSG:3035, returning a DataFrame with these geometries, the internal\n    4-byte grid ID and the origin used to define the 4-byte ID. If both `n_origin` and `e_origin` are provided,\n    they are used as the origin of the ID; if not, the origin is taken from the extent covering the provided\n    polygon.\n\n    It is desirable to define the origin using `n_origin` and `e_origin` when one wants to cover several polygons\n    sharing the same origin, i.e. using the 4-byte grid ID defined in the same way for all of them.\n\n    Args:\n        polygon_sdf (DataFrame): DataFrame containing a single row with a polygon in EPSG:4326 in a column named\n            `geometry`.\n        n_origin (int, optional): northing origin to be used for the 4-byte grid ID, in EPSG:3035 (metres). Defaults to None.\n        e_origin (int, optional): easting origin to be used for the 4-byte grid ID, in EPSG:3035 (metres). Defaults to None.\n\n    Returns:\n        DataFrame: The DataFrame representing the grid centroids covering the polygon, with their grid ID and origin\n            columns.\n    \"\"\"\n    extent = self._get_polygon_sdf_extent(polygon_sdf)\n\n    sdf = self.cover_extent_with_grid_centroids(extent, n_origin, e_origin)\n\n    sdf = self._get_grid_intersection_with_mask(sdf, polygon_sdf)\n\n    return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.cover_polygon_with_grid_tiles","title":"<code>cover_polygon_with_grid_tiles(polygon_sdf, n_origin, e_origin)</code>","text":"<p>Covers the given polygon with grid tiles. It takes an polygon expressed in EPSG:4326 and covers it with grid tile polygon geometries in EPSG:3035, returning a DataFrame with these geometries, the internal 4-byte grid ID and the origin used to define the 4-byte ID. If both <code>n_origin</code> and <code>e_origin</code> are provided, they are used as the origin of the ID; if not, the origin is taken from the polygon covering the provided polygon.</p> <p>It is desirable to define the origin using <code>n_origin</code> and <code>e_origin</code> when one wants to cover several polygons sharing the same origin, i.e. using the 4-byte grid ID defined in the same way for all of them.</p> <p>Parameters:</p> Name Type Description Default <code>polygon_sdf</code> <code>DataFrame</code> <p>DataFrame containing a single row with a polygon in EPSG:4326 in a column named <code>geometry</code>.</p> required <code>n_origin</code> <code>int</code> <p>northing origin to be used for the 4-byte grid ID, in EPSG:3035 (metres). Defaults to None.</p> required <code>e_origin</code> <code>int</code> <p>easting origin to be used for the 4-byte grid ID, in EPSG:3035 (metres). Defaults to None.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The DataFrame representing the grid tiles covering the polygon, with their grid ID and origin columns.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def cover_polygon_with_grid_tiles(self, polygon_sdf: DataFrame, n_origin: int, e_origin: int) -&gt; DataFrame:\n    \"\"\"Covers the given polygon with grid tiles. It takes an polygon expressed in EPSG:4326 and covers it\n    with grid tile polygon geometries in EPSG:3035, returning a DataFrame with these geometries, the internal\n    4-byte grid ID and the origin used to define the 4-byte ID. If both `n_origin` and `e_origin` are provided,\n    they are used as the origin of the ID; if not, the origin is taken from the polygon covering the provided\n    polygon.\n\n    It is desirable to define the origin using `n_origin` and `e_origin` when one wants to cover several polygons\n    sharing the same origin, i.e. using the 4-byte grid ID defined in the same way for all of them.\n\n    Args:\n        polygon_sdf (DataFrame): DataFrame containing a single row with a polygon in EPSG:4326 in a column named\n            `geometry`.\n        n_origin (int, optional): northing origin to be used for the 4-byte grid ID, in EPSG:3035 (metres). Defaults to None.\n        e_origin (int, optional): easting origin to be used for the 4-byte grid ID, in EPSG:3035 (metres). Defaults to None.\n\n    Returns:\n        DataFrame: The DataFrame representing the grid tiles covering the polygon, with their grid ID and origin\n            columns.\n    \"\"\"\n    extent = self._get_polygon_sdf_extent(polygon_sdf)\n\n    sdf = self.cover_extent_with_grid_tiles(extent, n_origin, e_origin)\n\n    sdf = self._get_grid_intersection_with_mask(sdf, polygon_sdf)\n\n    return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.grid_id_from_coarser_resolution","title":"<code>grid_id_from_coarser_resolution(sdf, coarse_resolution, coarse_grid_id_col, new_grid_id_col=None)</code>","text":"<p>This function takes a DataFrame that contains the grid ID representation of grid tiles in a resolution coarser than 100m, and transforms it back into 100m.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>DataFrame with grid IDs in a coarser resolution.</p> required <code>coarse_resolution</code> <code>int</code> <p>coarser resolution of the grid IDs of the provided DataFrame. Must be a multiple of <code>self.resolution</code>, i.e., 100.</p> required <code>coarse_grid_id_col</code> <code>str</code> <p>column that currently holds the IDs of the grid tiles in the coarser resolution.</p> required <code>new_grid_id_col</code> <code>str</code> <p>column that will hold the IDs of the grid tiles in the 100m resolution. If None, it will be set (and possible replace an existing column) as <code>self.grid_id_col</code>.     Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with the coarser grid IDs.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def grid_id_from_coarser_resolution(\n    self, sdf: DataFrame, coarse_resolution: int, coarse_grid_id_col: str, new_grid_id_col: str = None\n) -&gt; DataFrame:\n    \"\"\"This function takes a DataFrame that contains the grid ID representation of grid tiles in a resolution\n    coarser than 100m, and transforms it back into 100m.\n\n    Args:\n        sdf (DataFrame): DataFrame with grid IDs in a coarser resolution.\n        coarse_resolution (int): coarser resolution of the grid IDs of the provided DataFrame. Must be a multiple\n            of `self.resolution`, i.e., 100.\n        coarse_grid_id_col (str): column that currently holds the IDs of the grid tiles in the coarser\n            resolution.\n        new_grid_id_col (str, optional): column that will hold the IDs of the grid tiles in the 100m resolution.\n            If None, it will be set (and possible replace an existing column) as `self.grid_id_col`.\n                Defaults to None.\n\n    Returns:\n        DataFrame: DataFrame with the coarser grid IDs.\n    \"\"\"\n    if coarse_resolution % self.resolution != 0:\n        raise ValueError(f\"Coarser resolution {coarse_resolution} must be a multiple of {self.resolution}\")\n    if coarse_resolution &lt;= self.resolution:\n        raise ValueError(f\"Coarser resolution {coarse_resolution} must be greater than {self.resolution}\")\n    if new_grid_id_col is None:\n        new_grid_id_col = self.grid_id_col\n\n    factor = coarse_resolution // self.resolution\n    offsets_df = self.spark.createDataFrame(\n        [(i &lt;&lt; 16) + j for i in range(factor) for j in range(factor)],\n        schema=StructType([StructField(\"offset\", IntegerType(), False)]),\n    )\n\n    offsets_df = F.broadcast(offsets_df)\n\n    sdf = (\n        sdf.crossJoin(offsets_df)\n        .withColumn(new_grid_id_col, F.col(coarse_grid_id_col) + F.col(\"offset\"))\n        .drop(\"offset\")\n    )\n\n    return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.grid_id_to_coarser_resolution","title":"<code>grid_id_to_coarser_resolution(sdf, coarse_resolution, coarse_grid_id_col=None)</code>","text":"<p>This function takes a DataFrame that contains the grid ID representation of 100m grid tiles, and transforms it into a coarser resolution. It is always expected that the provided DataFrame has a grid ID that represents 100m grid tiles (in the <code>self.grid_id_col column</code>), and not a different resolution.</p> <p>Notice that this method does not take into account the origin of the 4-byte grid IDs. Thus, the coarser grids need not be compatible with the INSPIRE definition of a resolution coarser than 100m.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>DataFrame for which a coarser resolution grid ID will be computed</p> required <code>coarse_resolution</code> <code>int</code> <p>coarser resolution to compute. Must be a multiple of <code>self.resolution</code>, i.e., 100.</p> required <code>coarse_grid_id_col</code> <code>str</code> <p>column that will hold the IDs of the grid tiles in the coarser resolution. If None, it will replace the original grid ID column. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with the coarser grid IDs.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def grid_id_to_coarser_resolution(\n    self, sdf: DataFrame, coarse_resolution: int, coarse_grid_id_col: str = None\n) -&gt; DataFrame:\n    \"\"\"This function takes a DataFrame that contains the grid ID representation of 100m grid tiles, and transforms\n    it into a coarser resolution. It is always expected that the provided DataFrame has a grid ID that represents\n    100m grid tiles (in the `self.grid_id_col column`), and not a different resolution.\n\n    Notice that this method does not take into account the origin of the 4-byte grid IDs. Thus, the coarser grids\n    need not be compatible with the INSPIRE definition of a resolution coarser than 100m.\n\n    Args:\n        sdf (DataFrame): DataFrame for which a coarser resolution grid ID will be computed\n        coarse_resolution (int): coarser resolution to compute. Must be a multiple of `self.resolution`, i.e., 100.\n        coarse_grid_id_col (str, optional): column that will hold the IDs of the grid tiles in the coarser\n            resolution. If None, it will replace the original grid ID column. Defaults to None.\n\n    Returns:\n        DataFrame: DataFrame with the coarser grid IDs.\n    \"\"\"\n    if coarse_resolution % self.resolution != 0:\n        raise ValueError(f\"Coarser resolution {coarse_resolution} must be a multiple of {self.resolution}\")\n    if coarse_resolution &lt;= self.resolution:\n        raise ValueError(f\"Coarser resolution {coarse_resolution} must be greater than {self.resolution}\")\n\n    factor = coarse_resolution // self.resolution\n\n    if coarse_grid_id_col is None:\n        coarse_grid_id_col = self.grid_id_col\n\n    sdf = sdf.withColumn(\"northing\", F.shiftrightunsigned(ColNames.grid_id, 16)).withColumn(\n        \"easting\", F.col(ColNames.grid_id).bitwiseAND((1 &lt;&lt; 16) - 1)\n    )\n\n    sdf = sdf.withColumn(\"northing\", F.col(\"northing\") - F.col(\"northing\") % factor).withColumn(\n        \"easting\", F.col(\"easting\") - F.col(\"easting\") % factor\n    )\n\n    sdf = sdf.withColumn(coarse_grid_id_col, F.shiftleft(F.col(\"northing\"), 16) + F.col(\"easting\"))\n\n    sdf = sdf.drop(\"northing\", \"easting\")\n\n    return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.grid_id_to_inspire_id","title":"<code>grid_id_to_inspire_id(sdf, inspire_resolution, grid_id_col=None, origin=None)</code>","text":"<p>Function that takes a DataFrame containing 4-byte grid IDs and returns it with a new column containing the official INSPIRE grid ID string. Only accepted INSPIRE grid resolutions are 100m and 1km.</p> <p>It is expected that the grid ID column contains the internal representation for 100m grid tiles, and not for a coarser resolution. If the 100m INSPIRE grid ID was requested, the ID corresponding to the 100m grid tile represented by the internal grid ID is constructed. If the 1km INSPIRE grid ID was requested, the ID corresponding to the 1km grid tile containing the internal grid ID is constructed.</p> <p>By default, the function will use a ColNames.origin column of <code>sdf</code>. Only if the <code>origin</code> parameter is passed, the existence of this column will not be checked, and <code>origin</code> will be used as the origin of the 4-byte grid ID definition even if the column exists. This origin will be treated as an 8-byte integer, where the first (most significant) 4 bytes hold the northing origin divided by 100 and the last (least significant) 4 bytes hold the easting origin divided by 100. That is, taking the first 4 bytes and multiplying by 100 gets the a northing value in metres (analogous for easting).</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>DataFrame containing the grid ID column, and a <code>ColNames.origin</code> column, to which the INSPIRE grid ID is to be added</p> required <code>inspire_resolution</code> <code>int</code> <p>resolution for the INSPIRE grid ID. Currently accepts two value: <code>100</code> and <code>1000</code>.</p> required <code>grid_id_col</code> <code>str</code> <p>Name of the column containing the internal 4-byte grid ID. If None, the value <code>self.grid_id_col</code> is taken by default. Defaults to None</p> <code>None</code> <code>origin</code> <code>int</code> <p>If provided, it will be used as the origin of the definition of the 4-byte grid ID. It will ignore the ColNames.origin column even if it exists. If not provided, it is expected that <code>sdf</code> contains a ColNames.origin column, and throws an error otherwise.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with a new column, <code>ColNames.inspire_id</code>, containing the INSPIRE grid ID strings.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the <code>inspire_resolution</code> is not 100 or 1000.</p> <code>ValueError</code> <p>If the <code>origin</code> is not an integer.</p> <code>ValueError</code> <p>If the <code>sdf</code> does not contain a ColNames.origin column and <code>origin</code> is not passed.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def grid_id_to_inspire_id(\n    self, sdf: DataFrame, inspire_resolution: int, grid_id_col: str = None, origin: int = None\n) -&gt; DataFrame:\n    \"\"\"Function that takes a DataFrame containing 4-byte grid IDs and returns it with a new column containing\n    the official INSPIRE grid ID string. Only accepted INSPIRE grid resolutions are 100m and 1km.\n\n    It is expected that the grid ID column contains the internal representation for 100m grid tiles, and not for\n    a coarser resolution. If the 100m INSPIRE grid ID was requested, the ID corresponding to the 100m grid tile\n    represented by the internal grid ID is constructed. If the 1km INSPIRE grid ID was requested, the ID\n    corresponding to the 1km grid tile containing the internal grid ID is constructed.\n\n    By default, the function will use a ColNames.origin column of `sdf`. Only if the `origin` parameter is passed,\n    the existence of this column will not be checked, and `origin` will be used as the origin of the 4-byte grid ID\n    definition even if the column exists. This origin will be treated as an 8-byte integer, where the first (most\n    significant) 4 bytes hold the northing origin divided by 100 and the last (least significant) 4 bytes hold the\n    easting origin divided by 100. That is, taking the first 4 bytes and multiplying by 100 gets the a northing\n    value in metres (analogous for easting).\n\n    Args:\n        sdf (DataFrame): DataFrame containing the grid ID column, and a `ColNames.origin` column, to which the\n            INSPIRE grid ID is to be added\n        inspire_resolution (int): resolution for the INSPIRE grid ID. Currently accepts two value: `100` and `1000`.\n        grid_id_col (str, optional): Name of the column containing the internal 4-byte grid ID. If None, the value\n            `self.grid_id_col` is taken by default. Defaults to None\n        origin (int, optional): If provided, it will be used as the origin of the definition of the 4-byte grid ID.\n            It will ignore the ColNames.origin column even if it exists. If not provided, it is expected that\n            `sdf` contains a ColNames.origin column, and throws an error otherwise.\n\n    Returns:\n        DataFrame: DataFrame with a new column, `ColNames.inspire_id`, containing the INSPIRE grid ID strings.\n\n    Raises:\n        ValueError: If the `inspire_resolution` is not 100 or 1000.\n        ValueError: If the `origin` is not an integer.\n        ValueError: If the `sdf` does not contain a ColNames.origin column and `origin` is not passed.\n    \"\"\"\n    if grid_id_col is None:\n        grid_id_col = self.grid_id_col\n    if inspire_resolution not in self.ACCEPTED_RESOLUTIONS:\n        raise ValueError(\n            f\"Expected INSPIRE resolutions are {self.ACCEPTED_RESOLUTIONS} -- received `{inspire_resolution}`\"\n        )\n    if origin is not None:\n        if not isinstance(origin, int):\n            raise ValueError(f\"`origin` parameter must be an integer if used -- found type {type(origin)}\")\n        origin_column = F.lit(origin).cast(LongType())\n    else:\n        if ColNames.origin not in sdf.columns:\n            raise ValueError(f\"`sdf` must contain a {ColNames.origin} column, or `origin` parameter must be passed\")\n        origin_column = F.col(ColNames.origin)\n\n    sdf = sdf.withColumn(\n        \"northing\",\n        F.shiftrightunsigned(grid_id_col, 16).cast(LongType()) + F.shiftrightunsigned(origin_column, 32),\n    ).withColumn(\n        \"easting\",\n        F.col(grid_id_col).bitwiseAND((1 &lt;&lt; 16) - 1).cast(LongType()) + origin_column.bitwiseAND((1 &lt;&lt; 32) - 1),\n    )\n\n    # Substract the units digit to get the ID for 1km\n    if inspire_resolution == 1000:\n        sdf = sdf.withColumn(\"northing\", F.expr(\"northing DIV 10\")).withColumn(\"easting\", F.expr(\"easting DIV 10\"))\n    sdf = sdf.withColumn(\n        ColNames.inspire_id,\n        F.concat(\n            F.lit(self._format_distance(inspire_resolution)),\n            F.lit(\"N\"),\n            F.col(\"northing\"),\n            F.lit(\"E\"),\n            F.col(\"easting\"),\n        ),\n    ).drop(\"northing\", \"easting\")\n    return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.grid_ids_to_grid_centroids","title":"<code>grid_ids_to_grid_centroids(sdf, grid_resolution, grid_id_col=None, geometry_col=None, origin=None)</code>","text":"<p>Function that takes a DataFrame containing internal 4-byte grid IDs and returns it with point geometries of the centroids of the corresponding grid tiles.</p> <p>By default, the function will use a ColNames.origin column of <code>sdf</code>. Only if the <code>origin</code> parameter is passed, the existence of this column will not be checked, and <code>origin</code> will be used as the origin of the 4-byte grid ID definition even if the column exists. This origin will be treated as an 8-byte integer, where the first (most significant) 4 bytes hold the northing origin divided by 100 and the last (least significant) 4 bytes hold the easting origin divided by 100. That is, taking the first 4 bytes and multiplying by 100 gets the a northing value in metres (analogous for easting).</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>DataFrame containing the internal 4-byte grid IDs.</p> required <code>grid_resolution</code> <code>int</code> <p>resolution, in metres, of the current grid as represented by the internal 4-byte grid IDs. Must be a multiple of <code>self.resolution</code>. i.e. 100.</p> required <code>grid_id_col</code> <code>str</code> <p>column that holds the internal grid IDs. If None, it is set to <code>self.grid_id_col</code>. Defaults to None.</p> <code>None</code> <code>geometry_col</code> <code>str</code> <p>column that will hold the grid centroid geometries. If None, it is set to <code>self.geometry_col</code>. Defaults to None.</p> <code>None</code> <code>origin</code> <code>int</code> <p>If provided, it will be used as the origin of the definition of the 4-byte grid ID. It will ignore the ColNames.origin column even if it exists. If not provided, it is expected that <code>sdf</code> contains a ColNames.origin column, and throws an error otherwise.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with the grid centroid geometries</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def grid_ids_to_grid_centroids(\n    self,\n    sdf: DataFrame,\n    grid_resolution: int,\n    grid_id_col: str = None,\n    geometry_col: str = None,\n    origin: int = None,\n) -&gt; DataFrame:\n    \"\"\"Function that takes a DataFrame containing internal 4-byte grid IDs and returns it with point geometries\n    of the centroids of the corresponding grid tiles.\n\n    By default, the function will use a ColNames.origin column of `sdf`. Only if the `origin` parameter is passed,\n    the existence of this column will not be checked, and `origin` will be used as the origin of the 4-byte grid ID\n    definition even if the column exists. This origin will be treated as an 8-byte integer, where the first (most\n    significant) 4 bytes hold the northing origin divided by 100 and the last (least significant) 4 bytes hold the\n    easting origin divided by 100. That is, taking the first 4 bytes and multiplying by 100 gets the a northing\n    value in metres (analogous for easting).\n\n    Args:\n        sdf (DataFrame): DataFrame containing the internal 4-byte grid IDs.\n        grid_resolution (int): resolution, in metres, of the current grid as represented by the internal 4-byte\n            grid IDs. Must be a multiple of `self.resolution`. i.e. 100.\n        grid_id_col (str, optional): column that holds the internal grid IDs. If None, it is set to\n            `self.grid_id_col`. Defaults to None.\n        geometry_col (str, optional): column that will hold the grid centroid geometries. If None, it is set to\n            `self.geometry_col`. Defaults to None.\n        origin (int, optional): If provided, it will be used as the origin of the definition of the 4-byte grid ID.\n            It will ignore the ColNames.origin column even if it exists. If not provided, it is expected that\n            `sdf` contains a ColNames.origin column, and throws an error otherwise.\n\n    Returns:\n        DataFrame: DataFrame with the grid centroid geometries\n    \"\"\"\n\n    if grid_resolution % self.resolution != 0:\n        raise ValueError(f\"Grid resolution must be a multiple of {self.resolution}\")\n    if geometry_col is None:\n        geometry_col = self.geometry_col\n    if grid_id_col is None:\n        grid_id_col = self.grid_id_col\n    if origin is not None:\n        if not isinstance(origin, int):\n            raise ValueError(f\"`origin` parameter must be an integer if used -- found type {type(origin)}\")\n        origin_column = F.lit(origin).cast(LongType())\n    else:\n        if ColNames.origin not in sdf.columns:\n            raise ValueError(f\"`sdf` must contain a {ColNames.origin} column, or `origin` parameter must be passed\")\n        origin_column = F.col(ColNames.origin)\n\n    # For Sedona, (X, Y) == (Northing, Easting) in EPSG 3035\n    sdf = sdf.withColumn(\n        geometry_col,\n        STC.ST_Point(\n            (F.shiftrightunsigned(ColNames.grid_id, 16).cast(LongType()) + F.shiftrightunsigned(origin_column, 32))\n            * self.resolution\n            + grid_resolution // 2,\n            (\n                F.col(ColNames.grid_id).bitwiseAND((1 &lt;&lt; 16) - 1).cast(LongType())\n                + origin_column.bitwiseAND((1 &lt;&lt; 32) - 1)\n            )\n            * self.resolution\n            + grid_resolution // 2,\n        ),\n    )\n\n    sdf = sdf.withColumn(geometry_col, STF.ST_SetSRID(geometry_col, self.GRID_CRS_EPSG_CODE))\n\n    return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.grid_ids_to_grid_tiles","title":"<code>grid_ids_to_grid_tiles(sdf, grid_resolution, geometry_col=None, origin=None)</code>","text":"<p>Function that takes a DataFrame containing internal 4-byte grid IDs and returns it with polygon geometries of the corresponding grid tiles.</p> <p>By default, the function will use a ColNames.origin column of <code>sdf</code>. Only if the <code>origin</code> parameter is passed, the existence of this column will not be checked, and <code>origin</code> will be used as the origin of the 4-byte grid ID definition even if the column exists. This origin will be treated as an 8-byte integer, where the first (most significant) 4 bytes hold the northing origin divided by 100 and the last (least significant) 4 bytes hold the easting origin divided by 100. That is, taking the first 4 bytes and multiplying by 100 gets the a northing value in metres (analogous for easting).</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>DataFrame containing the internal 4-byte grid IDs.</p> required <code>grid_resolution</code> <code>int</code> <p>resolution, in metres, of the current grid as represented by the internal 4-byte grid IDs. Must be a multiple of <code>self.resolution</code>. i.e. 100.</p> required <code>geometry_col</code> <code>str</code> <p>column that will hold the grid tile geometries. If None, it is set to <code>self.geometry_col</code>. Defaults to None.</p> <code>None</code> <code>origin</code> <code>int</code> <p>If provided, it will be used as the origin of the definition of the 4-byte grid ID. It will ignore the ColNames.origin column even if it exists. If not provided, it is expected that <code>sdf</code> contains a ColNames.origin column, and throws an error otherwise.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with the grid centroid geometries</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def grid_ids_to_grid_tiles(\n    self, sdf: DataFrame, grid_resolution: int, geometry_col: str = None, origin: int = None\n) -&gt; DataFrame:\n    \"\"\"Function that takes a DataFrame containing internal 4-byte grid IDs and returns it with polygon geometries\n    of the corresponding grid tiles.\n\n    By default, the function will use a ColNames.origin column of `sdf`. Only if the `origin` parameter is passed,\n    the existence of this column will not be checked, and `origin` will be used as the origin of the 4-byte grid ID\n    definition even if the column exists. This origin will be treated as an 8-byte integer, where the first (most\n    significant) 4 bytes hold the northing origin divided by 100 and the last (least significant) 4 bytes hold the\n    easting origin divided by 100. That is, taking the first 4 bytes and multiplying by 100 gets the a northing\n    value in metres (analogous for easting).\n\n    Args:\n        sdf (DataFrame): DataFrame containing the internal 4-byte grid IDs.\n        grid_resolution (int): resolution, in metres, of the current grid as represented by the internal 4-byte\n            grid IDs. Must be a multiple of `self.resolution`. i.e. 100.\n        geometry_col (str, optional): column that will hold the grid tile geometries. If None, it is set to\n            `self.geometry_col`. Defaults to None.\n        origin (int, optional): If provided, it will be used as the origin of the definition of the 4-byte grid ID.\n            It will ignore the ColNames.origin column even if it exists. If not provided, it is expected that\n            `sdf` contains a ColNames.origin column, and throws an error otherwise.\n\n    Returns:\n        DataFrame: DataFrame with the grid centroid geometries\n    \"\"\"\n    if grid_resolution % self.resolution != 0:\n        raise ValueError(f\"Grid resolution must be a multiple of {self.resolution}\")\n    if geometry_col is None:\n        geometry_col = self.geometry_col\n    if origin is not None:\n        if not isinstance(origin, int):\n            raise ValueError(f\"`origin` parameter must be an integer if used -- found type {type(origin)}\")\n        origin_column = F.lit(origin).cast(LongType())\n    else:\n        if ColNames.origin not in sdf.columns:\n            raise ValueError(f\"`sdf` must contain a {ColNames.origin} column, or `origin` parameter must be passed\")\n        origin_column = F.col(ColNames.origin)\n\n    sdf = sdf.withColumn(\n        \"northing\",\n        (F.shiftrightunsigned(ColNames.grid_id, 16).cast(LongType()) + F.shiftrightunsigned(origin_column, 32))\n        * self.resolution,\n    ).withColumn(\n        \"easting\",\n        (\n            F.col(ColNames.grid_id).bitwiseAND((1 &lt;&lt; 16) - 1).cast(LongType())\n            + origin_column.bitwiseAND((1 &lt;&lt; 32) - 1)\n        )\n        * self.resolution,\n    )\n\n    # For Sedona, (X, Y) == (Northing, Easting) in EPSG 3035\n    sdf = sdf.withColumn(\n        geometry_col,\n        STC.ST_PolygonFromEnvelope(\n            F.col(\"northing\"),  # min_x (min_northing)\n            F.col(\"easting\"),  # min_y (min_easting)\n            F.col(\"northing\") + grid_resolution,  # max_x (max_northing)\n            F.col(\"easting\") + grid_resolution,  # max_y (max_easting)\n        ),\n    )\n\n    sdf = sdf.drop(\"northing\", \"easting\")\n\n    # Set the CRS of the geometries\n    sdf = sdf.withColumn(geometry_col, STF.ST_SetSRID(geometry_col, self.GRID_CRS_EPSG_CODE))\n\n    return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.inspire_id_to_grid_centroids","title":"<code>inspire_id_to_grid_centroids(sdf, inspire_id_col=None, geometry_col=None)</code>","text":"<p>Function that takes a DataFrame containing INSPIRE grid ID strings and returns it with point geometries of the centroids of the corresponding grid tiles. It extracts the units and grid size from the first element of the DataFrame and uses it to construct the necessary geometries.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>DataFrame containing the INSPIRE grid ID strings.</p> required <code>inspire_id_col</code> <code>str</code> <p>name of the column holding the INSPIRE grid IDs. If None, it is set to <code>ColNames.inspire_id</code>. Defaults to None.</p> <code>None</code> <code>geometry_col</code> <code>str</code> <p>column that will hold the grid centroid geometries. If None, it is set to <code>self.geometry</code>. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with the grid centroid geometries</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def inspire_id_to_grid_centroids(\n    self, sdf: DataFrame, inspire_id_col: str = None, geometry_col: str = None\n) -&gt; DataFrame:\n    \"\"\"Function that takes a DataFrame containing INSPIRE grid ID strings and returns it with point geometries\n    of the centroids of the corresponding grid tiles. It extracts the units and grid size from the first element\n    of the DataFrame and uses it to construct the necessary geometries.\n\n    Args:\n        sdf (DataFrame): DataFrame containing the INSPIRE grid ID strings.\n        inspire_id_col (str, optional): name of the column holding the INSPIRE grid IDs. If None, it is set to\n            `ColNames.inspire_id`. Defaults to None.\n        geometry_col (str, optional): column that will hold the grid centroid geometries. If None, it is set to\n            `self.geometry`. Defaults to None.\n\n    Returns:\n        DataFrame: DataFrame with the grid centroid geometries\n    \"\"\"\n    if inspire_id_col is None:\n        inspire_id_col = ColNames.inspire_id\n    if geometry_col is None:\n        geometry_col = self.geometry_col\n\n    # First, get the INSPIRE resolution\n    resolution_str = sdf.select(F.regexp_extract(F.col(inspire_id_col), r\"^(.*?)N\", 1).alias(\"prefix\")).first()[\n        \"prefix\"\n    ]\n\n    # Parse and validate the INSPIRE resolution. Get the units and the grid size/resolution\n    if resolution_str[-2:] == \"km\":\n        try:\n            grid_size = int(resolution_str[:-2])\n        except ValueError:\n            raise ValueError(f\"Unexpected INSPIRE grid resolution string `{resolution_str}`\")\n        resolution_unit = 1000\n    elif resolution_str[-1:] == \"m\":\n        try:\n            grid_size = int(resolution_str[:-1])\n        except ValueError:\n            raise ValueError(f\"Unexpected INSPIRE grid resolution string `{resolution_str}`\")\n        resolution_unit = 100\n    else:\n        raise ValueError(f\"Unexpected INSPIRE grid resolution string `{resolution_str}`\")\n\n    # Create geometries. Multiply INSPIRE ID northing and easting values by the resolution unit, and add half\n    # the grid size to get the centroid of each tile\n    sdf = sdf.withColumn(\n        geometry_col,\n        STC.ST_Point(\n            F.regexp_extract(inspire_id_col, r\"E(\\d+)\", 1).cast(LongType()) * resolution_unit + grid_size // 2,\n            F.regexp_extract(inspire_id_col, r\"N(\\d+)E\", 1).cast(LongType()) * resolution_unit + grid_size // 2,\n        ),\n    )\n\n    # Set the CRS of the geometry\n    sdf = sdf.withColumn(geometry_col, STF.ST_SetSRID(geometry_col, self.GRID_CRS_EPSG_CODE))\n\n    return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.inspire_id_to_grid_tiles","title":"<code>inspire_id_to_grid_tiles(sdf, inspire_id_col=None, geometry_col=None)</code>","text":"<p>Function that takes a DataFrame containing INSPIRE grid ID strings and returns it with polygon geometries of the corresponding grid tiles. It extracts the units and grid size from the first element of the DataFrame and uses it to construct the necessary geometries.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>DataFrame containing the INSPIRE grid ID strings.</p> required <code>inspire_id_col</code> <code>str</code> <p>name of the column holding the INSPIRE grid IDs. If None, it is set to <code>ColNames.inspire_id</code>. Defaults to None.</p> <code>None</code> <code>geometry_col</code> <code>str</code> <p>column that will hold the grid tile geometries. If None, it is set to <code>ColNames.geometry</code>. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with the grid centroid geometries</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def inspire_id_to_grid_tiles(\n    self, sdf: DataFrame, inspire_id_col: str = None, geometry_col: str = None\n) -&gt; DataFrame:\n    \"\"\"Function that takes a DataFrame containing INSPIRE grid ID strings and returns it with polygon geometries\n    of the corresponding grid tiles. It extracts the units and grid size from the first element of the DataFrame\n    and uses it to construct the necessary geometries.\n\n    Args:\n        sdf (DataFrame): DataFrame containing the INSPIRE grid ID strings.\n        inspire_id_col (str, optional): name of the column holding the INSPIRE grid IDs. If None, it is set to\n            `ColNames.inspire_id`. Defaults to None.\n        geometry_col (str, optional): column that will hold the grid tile geometries. If None, it is set to\n            `ColNames.geometry`. Defaults to None.\n\n    Returns:\n        DataFrame: DataFrame with the grid centroid geometries\n    \"\"\"\n    if inspire_id_col is None:\n        inspire_id_col = ColNames.inspire_id\n    if geometry_col is None:\n        geometry_col = self.geometry_col\n\n    # First, get the INSPIRE resolution\n    resolution_str = sdf.select(F.regexp_extract(F.col(inspire_id_col), r\"^(.*?)N\", 1).alias(\"prefix\")).first()[\n        \"prefix\"\n    ]\n\n    # Parse and validate the INSPIRE resolution. Get the units and the grid size/resolution\n    if resolution_str[-2:] == \"km\":\n        try:\n            grid_size = int(resolution_str[:-2])\n        except ValueError:\n            raise ValueError(f\"Unexpected INSPIRE grid resolution string `{resolution_str}`\")\n        resolution_unit = 1000\n    elif resolution_str[-1:] == \"m\":\n        try:\n            grid_size = int(resolution_str[:-1])\n        except ValueError:\n            raise ValueError(f\"Unexpected INSPIRE grid resolution string `{resolution_str}`\")\n        resolution_unit = 100\n    else:\n        raise ValueError(f\"Unexpected INSPIRE grid resolution string `{resolution_str}`\")\n\n    sdf = sdf.withColumn(\n        \"northing\", F.regexp_extract(inspire_id_col, r\"N(\\d+)E\", 1).cast(LongType()) * resolution_unit\n    ).withColumn(\"easting\", F.regexp_extract(inspire_id_col, r\"E(\\d+)\", 1).cast(LongType()) * resolution_unit)\n\n    # Sedona has (X, Y) = (Easting, Northing) for EPSG 3035\n    sdf = sdf.withColumn(\n        geometry_col,\n        STC.ST_PolygonFromEnvelope(\n            F.col(\"easting\"),  # min_x (min_easting)\n            F.col(\"northing\"),  # min_y, (min_northing)\n            F.col(\"easting\") + F.lit(resolution_unit * grid_size),  # max_x (max_easting)\n            F.col(\"northing\") + F.lit(resolution_unit * grid_size),  # max_y (max_northing)\n        ),\n    )\n\n    sdf = sdf.drop(\"northing\", \"easting\")\n\n    # Set the CRS of the geometry\n    sdf = sdf.withColumn(geometry_col, STF.ST_SetSRID(geometry_col, self.GRID_CRS_EPSG_CODE))\n\n    return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.process_latlon_extent","title":"<code>process_latlon_extent(extent)</code>","text":"<p>Takes an extent expressed in latitude and longitude (EPSG 4326), projects it into EPSG 3035, creates bounding box, snaps to grid, and extends it some extra tiles in each direction.</p> <p>Parameters:</p> Name Type Description Default <code>extent</code> <code>list[float]</code> <p>The extent in lat/lon to process. Ordering is [lon_min, lat_min, lon_max, lat_max].</p> required <p>Returns:</p> Name Type Description <code>extent</code> <code>list[float]</code> <p>Coordinates of the rectangle/bounding box that covers the projected and extended extent. Order is [n_min, e_min, n_max, e_max] (bottom-left and top-right corners)</p> <code>raster_bounds</code> <code>list[float]</code> <p>Appropriate raster bounds</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def process_latlon_extent(self, extent: List[float]) -&gt; Tuple[List[float], List[float]]:\n    \"\"\"Takes an extent expressed in latitude and longitude (EPSG 4326), projects it into EPSG 3035, creates\n    bounding box, snaps to grid, and extends it some extra tiles in each direction.\n\n    Args:\n        extent (list[float]): The extent in lat/lon to process. Ordering is [lon_min, lat_min, lon_max, lat_max].\n\n    Returns:\n        extent (list[float]): Coordinates of the rectangle/bounding box that covers the projected and extended\n            extent. Order is [n_min, e_min, n_max, e_max] (bottom-left and top-right corners)\n        raster_bounds (list[float]): Appropriate raster bounds\n    \"\"\"\n    extent, auxiliar_coords = self._project_latlon_extent(extent)\n    extent, raster_bounds = self._project_bounding_box(extent, auxiliar_coords)\n\n    extent = self._snap_extent_to_grid(extent)\n    raster_bounds = self._snap_extent_to_grid(raster_bounds)\n\n    extent = self._extend_grid_extent(extent)\n    raster_bounds = self._extend_grid_raster_bounds(raster_bounds)\n    return extent, raster_bounds\n</code></pre>"},{"location":"reference/core/io_interface/","title":"io_interface","text":"<p>Module that implements classes for reading data from different data sources into a Spark DataFrames.</p>"},{"location":"reference/core/io_interface/#core.io_interface.CsvInterface","title":"<code>CsvInterface</code>","text":"<p>               Bases: <code>PathInterface</code></p> <p>Class that implements the PathInterface abstract class for reading/writing data from a csv data source.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class CsvInterface(PathInterface):\n    \"\"\"Class that implements the PathInterface abstract class for reading/writing data from a csv data source.\"\"\"\n\n    FILE_FORMAT = \"csv\"\n\n    def read_from_interface(\n        self,\n        spark: SparkSession,\n        path: str,\n        schema: StructType,\n        header: bool = True,\n        sep: str = \",\",\n    ) -&gt; DataFrame:\n        \"\"\"Method that reads data from a csv type data source as a Spark DataFrame.\n\n        Args:\n            spark (SparkSession): Spark session.\n            path (str): Path to the data.\n            schema (StructType, optional): Schema of the data. Defaults to None.\n\n        Returns:\n            df: Spark dataframe.\n        \"\"\"\n        return spark.read.csv(path, schema=schema, header=header, sep=sep)\n\n    def write_from_interface(\n        self,\n        df: DataFrame,\n        path: str,\n        partition_columns: List[str] = None,\n        header: bool = True,\n        sep: str = \",\",\n    ):\n        \"\"\"Method that writes data from a Spark DataFrame to a csv data source.\n\n        Args:\n            df (DataFrame): Spark DataFrame.\n            path (str): Path to the data.\n            partition_columns (List[str], optional): columns used for a partition write.\n        Raises:\n            NotImplementedError: csv files should not be written in this architecture.\n        \"\"\"\n        if partition_columns is None:\n            partition_columns = []\n        df.write.option(\"header\", header).option(\"sep\", sep).mode(\"overwrite\").format(\"csv\").save(path)\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.CsvInterface.read_from_interface","title":"<code>read_from_interface(spark, path, schema, header=True, sep=',')</code>","text":"<p>Method that reads data from a csv type data source as a Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session.</p> required <code>path</code> <code>str</code> <p>Path to the data.</p> required <code>schema</code> <code>StructType</code> <p>Schema of the data. Defaults to None.</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>Spark dataframe.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def read_from_interface(\n    self,\n    spark: SparkSession,\n    path: str,\n    schema: StructType,\n    header: bool = True,\n    sep: str = \",\",\n) -&gt; DataFrame:\n    \"\"\"Method that reads data from a csv type data source as a Spark DataFrame.\n\n    Args:\n        spark (SparkSession): Spark session.\n        path (str): Path to the data.\n        schema (StructType, optional): Schema of the data. Defaults to None.\n\n    Returns:\n        df: Spark dataframe.\n    \"\"\"\n    return spark.read.csv(path, schema=schema, header=header, sep=sep)\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.CsvInterface.write_from_interface","title":"<code>write_from_interface(df, path, partition_columns=None, header=True, sep=',')</code>","text":"<p>Method that writes data from a Spark DataFrame to a csv data source.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Spark DataFrame.</p> required <code>path</code> <code>str</code> <p>Path to the data.</p> required <code>partition_columns</code> <code>List[str]</code> <p>columns used for a partition write.</p> <code>None</code> <p>Raises:     NotImplementedError: csv files should not be written in this architecture.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def write_from_interface(\n    self,\n    df: DataFrame,\n    path: str,\n    partition_columns: List[str] = None,\n    header: bool = True,\n    sep: str = \",\",\n):\n    \"\"\"Method that writes data from a Spark DataFrame to a csv data source.\n\n    Args:\n        df (DataFrame): Spark DataFrame.\n        path (str): Path to the data.\n        partition_columns (List[str], optional): columns used for a partition write.\n    Raises:\n        NotImplementedError: csv files should not be written in this architecture.\n    \"\"\"\n    if partition_columns is None:\n        partition_columns = []\n    df.write.option(\"header\", header).option(\"sep\", sep).mode(\"overwrite\").format(\"csv\").save(path)\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.GeoParquetInterface","title":"<code>GeoParquetInterface</code>","text":"<p>               Bases: <code>PathInterface</code></p> <p>Class that implements the PathInterface abstract class for reading/writing data from a geoparquet data source.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class GeoParquetInterface(PathInterface):\n    \"\"\"Class that implements the PathInterface abstract class for reading/writing data from a geoparquet data source.\"\"\"\n\n    FILE_FORMAT = \"geoparquet\"\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.HttpGeoJsonInterface","title":"<code>HttpGeoJsonInterface</code>","text":"<p>               Bases: <code>IOInterface</code></p> <p>Class that implements the IO interface abstract class for reading GeoJSON data from an HTTP source.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class HttpGeoJsonInterface(IOInterface):\n    \"\"\"Class that implements the IO interface abstract class for reading GeoJSON data from an HTTP source.\"\"\"\n\n    def read_from_interface(self, spark: SparkSession, url: str, timeout: int = 60, max_retries: int = 5) -&gt; DataFrame:\n        \"\"\"Method that reads GeoJSON data from an HTTP source and converts it to a Spark DataFrame.\n\n        Args:\n            url (str): URL of the GeoJSON data.\n            timeout (int): Timeout for the GET request in seconds. Default is 60.\n            max_retries (int): Maximum number of retries for the GET request. Default is 5.\n\n        Returns:\n            df: Spark DataFrame.\n        \"\"\"\n        session = requests.Session()\n        retry = Retry(total=max_retries, backoff_factor=0.1, status_forcelist=[500, 502, 503, 504])\n        adapter = HTTPAdapter(max_retries=retry)\n        session.mount(\"http://\", adapter)\n        session.mount(\"https://\", adapter)\n\n        try:\n            response = session.get(url, timeout=timeout)\n        except requests.exceptions.RequestException as e:\n            print(e)\n            raise Exception(\"Maximum number of retries exceeded.\")\n\n        if response.status_code != 200:\n            raise Exception(\"GET request not successful.\")\n\n        # Read the GeoJSON data into a GeoDataFrame\n        gdf = gpd.read_file(StringIO(response.text))\n\n        # Convert the GeoDataFrame to a Spark DataFrame\n        df = spark.createDataFrame(gdf)\n\n        return df\n\n    def write_from_interface(self, df: DataFrame, url: str, timeout: int = 60, max_retries: int = 5):\n        \"\"\"Method that writes a DataFrame to an HTTP source as GeoJSON data.\n\n        Args:\n            df (DataFrame): DataFrame to write.\n            url (str): URL of the HTTP source.\n            timeout (int): Timeout for the POST request in seconds. Default is 60.\n            max_retries (int): Maximum number of retries for the POST request. Default is 5.\n        \"\"\"\n        raise NotImplementedError(\"This method is not implemented.\")\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.HttpGeoJsonInterface.read_from_interface","title":"<code>read_from_interface(spark, url, timeout=60, max_retries=5)</code>","text":"<p>Method that reads GeoJSON data from an HTTP source and converts it to a Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL of the GeoJSON data.</p> required <code>timeout</code> <code>int</code> <p>Timeout for the GET request in seconds. Default is 60.</p> <code>60</code> <code>max_retries</code> <code>int</code> <p>Maximum number of retries for the GET request. Default is 5.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>Spark DataFrame.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def read_from_interface(self, spark: SparkSession, url: str, timeout: int = 60, max_retries: int = 5) -&gt; DataFrame:\n    \"\"\"Method that reads GeoJSON data from an HTTP source and converts it to a Spark DataFrame.\n\n    Args:\n        url (str): URL of the GeoJSON data.\n        timeout (int): Timeout for the GET request in seconds. Default is 60.\n        max_retries (int): Maximum number of retries for the GET request. Default is 5.\n\n    Returns:\n        df: Spark DataFrame.\n    \"\"\"\n    session = requests.Session()\n    retry = Retry(total=max_retries, backoff_factor=0.1, status_forcelist=[500, 502, 503, 504])\n    adapter = HTTPAdapter(max_retries=retry)\n    session.mount(\"http://\", adapter)\n    session.mount(\"https://\", adapter)\n\n    try:\n        response = session.get(url, timeout=timeout)\n    except requests.exceptions.RequestException as e:\n        print(e)\n        raise Exception(\"Maximum number of retries exceeded.\")\n\n    if response.status_code != 200:\n        raise Exception(\"GET request not successful.\")\n\n    # Read the GeoJSON data into a GeoDataFrame\n    gdf = gpd.read_file(StringIO(response.text))\n\n    # Convert the GeoDataFrame to a Spark DataFrame\n    df = spark.createDataFrame(gdf)\n\n    return df\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.HttpGeoJsonInterface.write_from_interface","title":"<code>write_from_interface(df, url, timeout=60, max_retries=5)</code>","text":"<p>Method that writes a DataFrame to an HTTP source as GeoJSON data.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to write.</p> required <code>url</code> <code>str</code> <p>URL of the HTTP source.</p> required <code>timeout</code> <code>int</code> <p>Timeout for the POST request in seconds. Default is 60.</p> <code>60</code> <code>max_retries</code> <code>int</code> <p>Maximum number of retries for the POST request. Default is 5.</p> <code>5</code> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def write_from_interface(self, df: DataFrame, url: str, timeout: int = 60, max_retries: int = 5):\n    \"\"\"Method that writes a DataFrame to an HTTP source as GeoJSON data.\n\n    Args:\n        df (DataFrame): DataFrame to write.\n        url (str): URL of the HTTP source.\n        timeout (int): Timeout for the POST request in seconds. Default is 60.\n        max_retries (int): Maximum number of retries for the POST request. Default is 5.\n    \"\"\"\n    raise NotImplementedError(\"This method is not implemented.\")\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.IOInterface","title":"<code>IOInterface</code>","text":"<p>Abstract interface that provides functionality for reading and writing data</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class IOInterface(metaclass=ABCMeta):\n    \"\"\"Abstract interface that provides functionality for reading and writing data\"\"\"\n\n    @classmethod\n    def __subclasshook__(cls, subclass: type) -&gt; bool:\n        if cls is IOInterface:\n            attrs: List[str] = []\n            callables: List[str] = [\"read_from_interface\", \"write_from_interface\"]\n            ret: bool = True\n            for attr in attrs:\n                ret = ret and (hasattr(subclass, attr) and isinstance(getattr(subclass, attr), property))\n            for call in callables:\n                ret = ret and (hasattr(subclass, call) and callable(getattr(subclass, call)))\n            return ret\n        else:\n            return NotImplemented\n\n    @abstractmethod\n    def read_from_interface(self, *args, **kwargs) -&gt; DataFrame:\n        pass\n\n    @abstractmethod\n    def write_from_interface(self, df: DataFrame, *args, **kwargs):\n        pass\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.JsonInterface","title":"<code>JsonInterface</code>","text":"<p>               Bases: <code>PathInterface</code></p> <p>Class that implements the PathInterface abstract class for reading/writing data from a json data source.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class JsonInterface(PathInterface):\n    \"\"\"Class that implements the PathInterface abstract class for reading/writing data from a json data source.\"\"\"\n\n    FILE_FORMAT = \"json\"\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.ParquetInterface","title":"<code>ParquetInterface</code>","text":"<p>               Bases: <code>PathInterface</code></p> <p>Class that implements the PathInterface abstract class for reading/writing data from a parquet data source.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class ParquetInterface(PathInterface):\n    \"\"\"Class that implements the PathInterface abstract class for reading/writing data from a parquet data source.\"\"\"\n\n    FILE_FORMAT = \"parquet\"\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.PathInterface","title":"<code>PathInterface</code>","text":"<p>               Bases: <code>IOInterface</code></p> <p>Abstract interface for reading/writing data from a file type data source.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class PathInterface(IOInterface, metaclass=ABCMeta):\n    \"\"\"Abstract interface for reading/writing data from a file type data source.\"\"\"\n\n    FILE_FORMAT = \"\"\n\n    def read_from_interface(self, spark: SparkSession, path: str, schema: StructType = None) -&gt; DataFrame:\n        \"\"\"Method that reads data from a file type data source as a Spark DataFrame.\n\n        Args:\n            spark (SparkSession): Spark session.\n            path (str): Path to the data.\n            schema (StructType, optional): Schema of the data. Defaults to None.\n\n        Returns:\n            df: Spark dataframe.\n        \"\"\"\n        if schema is None:\n            return spark.read.format(self.FILE_FORMAT).load(path)\n        else:\n            return (\n                spark.read.schema(schema).format(self.FILE_FORMAT).load(path)\n            )  # Read schema  # File format  # Load path\n\n    def write_from_interface(\n        self, df: DataFrame, path: str, partition_columns: List[str] = None, mode: str = SPARK_WRITING_MODES.APPEND\n    ):\n        \"\"\"Method that writes data from a Spark DataFrame to a file type data source.\n\n        Args:\n            df (DataFrame): Spark DataFrame.\n            path (str): Path to the data.\n            partition_columns (List[str], optional): columns used for a partition write.\n        \"\"\"\n        # Args check\n        if partition_columns is None:\n            partition_columns = []\n\n        df.write.format(\n            self.FILE_FORMAT,  # File format\n        ).partitionBy(partition_columns).mode(\n            mode\n        ).save(path)\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.PathInterface.read_from_interface","title":"<code>read_from_interface(spark, path, schema=None)</code>","text":"<p>Method that reads data from a file type data source as a Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session.</p> required <code>path</code> <code>str</code> <p>Path to the data.</p> required <code>schema</code> <code>StructType</code> <p>Schema of the data. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>Spark dataframe.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def read_from_interface(self, spark: SparkSession, path: str, schema: StructType = None) -&gt; DataFrame:\n    \"\"\"Method that reads data from a file type data source as a Spark DataFrame.\n\n    Args:\n        spark (SparkSession): Spark session.\n        path (str): Path to the data.\n        schema (StructType, optional): Schema of the data. Defaults to None.\n\n    Returns:\n        df: Spark dataframe.\n    \"\"\"\n    if schema is None:\n        return spark.read.format(self.FILE_FORMAT).load(path)\n    else:\n        return (\n            spark.read.schema(schema).format(self.FILE_FORMAT).load(path)\n        )  # Read schema  # File format  # Load path\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.PathInterface.write_from_interface","title":"<code>write_from_interface(df, path, partition_columns=None, mode=SPARK_WRITING_MODES.APPEND)</code>","text":"<p>Method that writes data from a Spark DataFrame to a file type data source.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Spark DataFrame.</p> required <code>path</code> <code>str</code> <p>Path to the data.</p> required <code>partition_columns</code> <code>List[str]</code> <p>columns used for a partition write.</p> <code>None</code> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def write_from_interface(\n    self, df: DataFrame, path: str, partition_columns: List[str] = None, mode: str = SPARK_WRITING_MODES.APPEND\n):\n    \"\"\"Method that writes data from a Spark DataFrame to a file type data source.\n\n    Args:\n        df (DataFrame): Spark DataFrame.\n        path (str): Path to the data.\n        partition_columns (List[str], optional): columns used for a partition write.\n    \"\"\"\n    # Args check\n    if partition_columns is None:\n        partition_columns = []\n\n    df.write.format(\n        self.FILE_FORMAT,  # File format\n    ).partitionBy(partition_columns).mode(\n        mode\n    ).save(path)\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.ShapefileInterface","title":"<code>ShapefileInterface</code>","text":"<p>               Bases: <code>PathInterface</code></p> <p>Class that implements the PathInterface abstract class for reading/writing data from a ShapeFile data source.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class ShapefileInterface(PathInterface):\n    \"\"\"Class that implements the PathInterface abstract class for reading/writing data from a ShapeFile data source.\"\"\"\n\n    def read_from_interface(self, spark: SparkSession, path: str, schema: StructType = None) -&gt; DataFrame:\n        \"\"\"Method that reads data from a ShapeFile type data source as a Spark DataFrame.\n\n        Args:\n            spark (SparkSession): Spark session.\n            path (str): Path to the data.\n            schema (StructType, optional): Schema of the data. Defaults to None.\n\n        Returns:\n            df: Spark dataframe.\n        \"\"\"\n        df = ShapefileReader.readToGeometryRDD(spark.sparkContext, path)\n        return Adapter.toDf(df, spark)\n\n    def write_from_interface(self, df: DataFrame, path: str, partition_columns: list = None):\n        \"\"\"Method that writes data from a Spark DataFrame to a ShapeFile data source.\n\n        Args:\n            df (DataFrame): Spark DataFrame.\n            path (str): Path to the data.\n            partition_columns (List[str], optional): columns used for a partition write.\n        Raises:\n            NotImplementedError: ShapeFile files should not be written in this architecture.\n        \"\"\"\n        raise NotImplementedError(\"Not implemented as Shapefiles shouldn't be written\")\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.ShapefileInterface.read_from_interface","title":"<code>read_from_interface(spark, path, schema=None)</code>","text":"<p>Method that reads data from a ShapeFile type data source as a Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session.</p> required <code>path</code> <code>str</code> <p>Path to the data.</p> required <code>schema</code> <code>StructType</code> <p>Schema of the data. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>Spark dataframe.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def read_from_interface(self, spark: SparkSession, path: str, schema: StructType = None) -&gt; DataFrame:\n    \"\"\"Method that reads data from a ShapeFile type data source as a Spark DataFrame.\n\n    Args:\n        spark (SparkSession): Spark session.\n        path (str): Path to the data.\n        schema (StructType, optional): Schema of the data. Defaults to None.\n\n    Returns:\n        df: Spark dataframe.\n    \"\"\"\n    df = ShapefileReader.readToGeometryRDD(spark.sparkContext, path)\n    return Adapter.toDf(df, spark)\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.ShapefileInterface.write_from_interface","title":"<code>write_from_interface(df, path, partition_columns=None)</code>","text":"<p>Method that writes data from a Spark DataFrame to a ShapeFile data source.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Spark DataFrame.</p> required <code>path</code> <code>str</code> <p>Path to the data.</p> required <code>partition_columns</code> <code>List[str]</code> <p>columns used for a partition write.</p> <code>None</code> <p>Raises:     NotImplementedError: ShapeFile files should not be written in this architecture.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def write_from_interface(self, df: DataFrame, path: str, partition_columns: list = None):\n    \"\"\"Method that writes data from a Spark DataFrame to a ShapeFile data source.\n\n    Args:\n        df (DataFrame): Spark DataFrame.\n        path (str): Path to the data.\n        partition_columns (List[str], optional): columns used for a partition write.\n    Raises:\n        NotImplementedError: ShapeFile files should not be written in this architecture.\n    \"\"\"\n    raise NotImplementedError(\"Not implemented as Shapefiles shouldn't be written\")\n</code></pre>"},{"location":"reference/core/log/","title":"log","text":"<p>Module that manages the logging functionality.</p>"},{"location":"reference/core/log/#core.log.generate_logger","title":"<code>generate_logger(config, component_id)</code>","text":"<p>Function that initializes a logger.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ConfigParser</code> <p>Object with the final configuration.</p> required <p>Returns:</p> Type Description <code>Logger</code> <p>Python logging object.</p> Source code in <code>multimno/core/log.py</code> <pre><code>def generate_logger(config: ConfigParser, component_id: str):\n    \"\"\"Function that initializes a logger.\n\n    Args:\n        config (ConfigParser): Object with the final configuration.\n\n    Returns:\n        (logging.Logger): Python logging object.\n    \"\"\"\n\n    notset_level = logging.getLevelName(logging.NOTSET)\n\n    # Parse config\n    console_log_level = config.get(LoggerKeys.LOG_CONFIG_KEY, LoggerKeys.CONSOLE_LOG_LEVEL, fallback=None)\n    file_log_level = config.get(LoggerKeys.LOG_CONFIG_KEY, LoggerKeys.FILE_LOG_LEVEL, fallback=None)\n    console_format = config.get(LoggerKeys.LOG_CONFIG_KEY, LoggerKeys.CONSOLE_FORMAT, fallback=None)\n    file_format = config.get(LoggerKeys.LOG_CONFIG_KEY, LoggerKeys.FILE_FORMAT, fallback=None)\n    datefmt = config.get(LoggerKeys.LOG_CONFIG_KEY, LoggerKeys.DATEFMT, fallback=None)\n    report_path = config.get(LoggerKeys.LOG_CONFIG_KEY, LoggerKeys.REPORT_PATH, fallback=None)\n\n    # Check if logger already exists\n    logger = logging.getLogger(component_id)\n    if len(logger.handlers) &gt; 0:\n        logger.warning(f\"Logger {component_id} already exists.\")\n        return logger\n\n    # Define a console logger\n    if console_log_level is not None and console_log_level != str(notset_level):\n        # Set console handler\n        console_h = logging.StreamHandler()\n        console_h.setLevel(console_log_level)\n        # Set console formatter\n        console_formatter = logging.Formatter(fmt=console_format, datefmt=datefmt)\n        console_h.setFormatter(console_formatter)\n        # Add console handler to logger\n        logger.addHandler(console_h)\n\n    # Define a file logger\n    if file_log_level is not None and file_log_level != str(notset_level):\n        # Verify required fields for file logger\n        if report_path is None:\n            raise ValueError(\"report_path is required to build a file logger.\")\n\n        # Get log path\n        today = datetime.now().strftime(\"%y%m%d_%H%M%S\")\n        log_path = f\"{report_path}/{component_id}/{component_id}_{today}.log\"\n        # Make report path + log dir\n        os.makedirs(os.path.dirname(log_path), exist_ok=True)\n\n        # Set File handler\n        file_h = logging.FileHandler(log_path)\n        file_h.setLevel(file_log_level)\n        # Set file formatter\n        file_formatter = logging.Formatter(fmt=file_format, datefmt=datefmt)\n        file_h.setFormatter(file_formatter)\n        # Add file handler to logger\n        logger.addHandler(file_h)\n\n    # Set logger level\n    logger.setLevel(logging.DEBUG)\n    # Return logger\n    return logger\n</code></pre>"},{"location":"reference/core/quadkey_utils/","title":"quadkey_utils","text":"<p>This module contains utility functions to work with quadkey geo indexing for the multimno package.</p>"},{"location":"reference/core/quadkey_utils/#core.quadkey_utils.assign_quadkey","title":"<code>assign_quadkey(sdf, crs_in, zoom_level)</code>","text":"<p>Assigns a quadkey to each row in a DataFrame based on the centroid of its geometry.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The DataFrame to assign quadkeys to. The DataFrame must contain a geometry column.</p> required <code>crs_in</code> <code>int</code> <p>The CRS of the dataframe to project to 4326 before assigning quadkeys.</p> required <code>zoom_level</code> <code>int</code> <p>The zoom level to use when assigning quadkeys.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing the same rows as the input DataFrame, but with an additional quadkey column.</p> Source code in <code>multimno/core/quadkey_utils.py</code> <pre><code>def assign_quadkey(sdf: DataFrame, crs_in: int, zoom_level: int) -&gt; DataFrame:\n    \"\"\"\n    Assigns a quadkey to each row in a DataFrame based on the centroid of its geometry.\n\n    Args:\n        sdf (DataFrame): The DataFrame to assign quadkeys to. The DataFrame must contain a geometry column.\n        crs_in (int): The CRS of the dataframe to project to 4326 before assigning quadkeys.\n        zoom_level (int): The zoom level to use when assigning quadkeys.\n\n    Returns:\n        DataFrame: A DataFrame containing the same rows as the input DataFrame, but with an additional quadkey column.\n    \"\"\"\n\n    quadkey_udf = F.udf(latlon_to_quadkey, StringType())\n    sdf = sdf.withColumn(\"centroid\", STF.ST_Centroid(ColNames.geometry))\n\n    if crs_in != 4326:\n        sdf = utils.project_to_crs(sdf, crs_in, 4326, \"centroid\")\n\n    sdf = sdf.withColumn(\n        \"quadkey\",\n        quadkey_udf(\n            STF.ST_Y(F.col(\"centroid\")),\n            STF.ST_X(F.col(\"centroid\")),\n            F.lit(zoom_level),\n        ),\n    ).drop(\"centroid\")\n\n    return sdf\n</code></pre>"},{"location":"reference/core/quadkey_utils/#core.quadkey_utils.get_children_quadkeys","title":"<code>get_children_quadkeys(quadkey, target_level)</code>","text":"<p>Generates all child quadkeys at a specified resolution level for a given quadkey.</p> <p>This function takes a parent quadkey and a target level of detail, then returns all child quadkeys that are contained within the parent quadkey's area at the specified target level.</p> <p>Parameters:</p> Name Type Description Default <code>quadkey</code> <code>str</code> <p>The parent quadkey.</p> required <code>target_level</code> <code>int</code> <p>The target level of detail (zoom level) for the child quadkeys.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of all child quadkeys at the specified target level.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If target_level is less than the level of the input quadkey.</p> Source code in <code>multimno/core/quadkey_utils.py</code> <pre><code>def get_children_quadkeys(quadkey: str, target_level: int) -&gt; List[str]:\n    \"\"\"\n    Generates all child quadkeys at a specified resolution level for a given quadkey.\n\n    This function takes a parent quadkey and a target level of detail, then returns\n    all child quadkeys that are contained within the parent quadkey's area at the\n    specified target level.\n\n    Args:\n        quadkey (str): The parent quadkey.\n        target_level (int): The target level of detail (zoom level) for the child quadkeys.\n\n    Returns:\n        List[str]: A list of all child quadkeys at the specified target level.\n\n    Raises:\n        ValueError: If target_level is less than the level of the input quadkey.\n    \"\"\"\n    # Get the level of the input quadkey\n    current_level = len(quadkey)\n\n    # Check that target_level is greater than or equal to current_level\n    if target_level &lt; current_level:\n        raise ValueError(\n            f\"Target level ({target_level}) must be greater than or equal to the current level ({current_level})\"\n        )\n\n    # If target_level is the same as current_level, return the input quadkey\n    if target_level == current_level:\n        return [quadkey]\n\n    # Initialize the list of child quadkeys with the input quadkey\n    child_quadkeys = [quadkey]\n\n    # For each level between current_level and target_level\n    for _ in range(target_level - current_level):\n        # Initialize a new list to hold the next level of child quadkeys\n        next_level_quadkeys = []\n\n        # For each quadkey in the current list\n        for qk in child_quadkeys:\n            # Generate the four children of this quadkey\n            next_level_quadkeys.extend([qk + \"0\", qk + \"1\", qk + \"2\", qk + \"3\"])\n\n        # Update the list of child quadkeys\n        child_quadkeys = next_level_quadkeys\n\n    return child_quadkeys\n</code></pre>"},{"location":"reference/core/quadkey_utils/#core.quadkey_utils.get_quadkeys_for_bbox","title":"<code>get_quadkeys_for_bbox(extent, level_of_detail)</code>","text":"<p>Generates a list of quadkeys for a bounding box at a specific zoom level.</p> <p>This function takes a bounding box defined by its lon min, lat min, lon max, and lat max extents, and a zoom level, and generates a list of quadkeys that cover the bounding box at the specified zoom level. The quadkeys are strings of digits that represent specific tiles in a quadtree-based spatial index.</p> <p>Parameters:</p> Name Type Description Default <code>extent</code> <code>tuple</code> <p>A tuple representing the bounding box. The tuple contains four elements: (west, south, east, north), which are the western, southern, eastern, and northern extents of the bounding box, respectively. Each extent is a float representing a geographic coordinate in degrees.</p> required <code>level_of_detail</code> <code>int</code> <p>The zoom level.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>List[str]</code> <p>A list of quadkeys that cover the bounding box at the specified zoom level. Each quadkey is a string.</p> Source code in <code>multimno/core/quadkey_utils.py</code> <pre><code>def get_quadkeys_for_bbox(extent: Tuple[float, float, float, float], level_of_detail: int) -&gt; List[str]:\n    \"\"\"\n    Generates a list of quadkeys for a bounding box at a specific zoom level.\n\n    This function takes a bounding box defined by its lon min, lat min, lon max, and lat max extents,\n    and a zoom level, and generates a list of quadkeys that cover the bounding box at the specified zoom level.\n    The quadkeys are strings of digits that represent specific tiles in a quadtree-based spatial index.\n\n    Args:\n        extent (tuple): A tuple representing the bounding box. The tuple contains four elements:\n            (west, south, east, north), which are the western, southern, eastern, and northern extents\n            of the bounding box, respectively. Each extent is a float representing a geographic coordinate in degrees.\n        level_of_detail (int): The zoom level.\n\n    Returns:\n        list: A list of quadkeys that cover the bounding box at the specified zoom level. Each quadkey is a string.\n    \"\"\"\n    west, south, east, north = extent\n    min_tile_x, min_tile_y = latlon_to_tilexy(north, west, level_of_detail)\n    max_tile_x, max_tile_y = latlon_to_tilexy(south, east, level_of_detail)\n    quadkeys = []\n    for x in range(min_tile_x, max_tile_x + 1):\n        for y in range(min_tile_y, max_tile_y + 1):\n            quadkeys.append(tilexy_to_quadkey(x, y, level_of_detail))\n    return quadkeys\n</code></pre>"},{"location":"reference/core/quadkey_utils/#core.quadkey_utils.latlon_to_quadkey","title":"<code>latlon_to_quadkey(latitude, longitude, level_of_detail)</code>","text":"<p>Converts a geographic coordinate to a quadkey at a specific zoom level.</p> <p>This function takes a latitude and longitude in degrees, and a zoom level, and converts them to a quadkey. The quadkey is a string of digits that represents a specific tile in a quadtree-based spatial index. The conversion process involves first converting the geographic coordinate to tile coordinates, and then converting the tile coordinates to a quadkey.</p> <p>Parameters:</p> Name Type Description Default <code>latitude</code> <code>float</code> <p>The latitude of the geographic coordinate, in degrees.</p> required <code>longitude</code> <code>float</code> <p>The longitude of the geographic coordinate, in degrees.</p> required <code>level_of_detail</code> <code>int</code> <p>The zoom level.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The quadkey representing the geographic coordinate at the specified zoom level.</p> Source code in <code>multimno/core/quadkey_utils.py</code> <pre><code>def latlon_to_quadkey(latitude: float, longitude: float, level_of_detail: int) -&gt; str:\n    \"\"\"\n    Converts a geographic coordinate to a quadkey at a specific zoom level.\n\n    This function takes a latitude and longitude in degrees, and a zoom level, and converts them to a quadkey.\n    The quadkey is a string of digits that represents a specific tile in a quadtree-based spatial index.\n    The conversion process involves first converting the geographic coordinate to tile coordinates,\n    and then converting the tile coordinates to a quadkey.\n\n    Args:\n        latitude (float): The latitude of the geographic coordinate, in degrees.\n        longitude (float): The longitude of the geographic coordinate, in degrees.\n        level_of_detail (int): The zoom level.\n\n    Returns:\n        str: The quadkey representing the geographic coordinate at the specified zoom level.\n    \"\"\"\n    x, y = latlon_to_tilexy(latitude, longitude, level_of_detail)\n    return tilexy_to_quadkey(x, y, level_of_detail)\n</code></pre>"},{"location":"reference/core/quadkey_utils/#core.quadkey_utils.latlon_to_tilexy","title":"<code>latlon_to_tilexy(latitude, longitude, level_of_detail)</code>","text":"<p>Converts a geographic coordinate to tile coordinates at a specific zoom level.</p> <p>This function takes a latitude and longitude in degrees, and a zoom level, and converts them to tile coordinates (tile_x, tile_y) at the specified zoom level. The tile coordinates are in the tile system used by Bing Maps, OpenStreetMap, MapBox and other map providers.</p> <p>Parameters:</p> Name Type Description Default <code>latitude</code> <code>float</code> <p>The latitude of the geographic coordinate, in degrees.</p> required <code>longitude</code> <code>float</code> <p>The longitude of the geographic coordinate, in degrees.</p> required <code>level_of_detail</code> <code>int</code> <p>The zoom level.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>int</code> <p>A tuple representing the tile coordinates of the geographic coordinate at the specified</p> <code>int</code> <p>zoom level. The tuple contains two elements: (tile_x, tile_y).</p> Source code in <code>multimno/core/quadkey_utils.py</code> <pre><code>def latlon_to_tilexy(latitude: float, longitude: float, level_of_detail: int) -&gt; Tuple[int, int]:\n    \"\"\"\n    Converts a geographic coordinate to tile coordinates at a specific zoom level.\n\n    This function takes a latitude and longitude in degrees, and a zoom level, and converts them to\n    tile coordinates (tile_x, tile_y) at the specified zoom level. The tile coordinates are in the\n    tile system used by Bing Maps, OpenStreetMap, MapBox and other map providers.\n\n    Args:\n        latitude (float): The latitude of the geographic coordinate, in degrees.\n        longitude (float): The longitude of the geographic coordinate, in degrees.\n        level_of_detail (int): The zoom level.\n\n    Returns:\n        tuple: A tuple representing the tile coordinates of the geographic coordinate at the specified\n        zoom level. The tuple contains two elements: (tile_x, tile_y).\n    \"\"\"\n    if not -90 &lt;= latitude &lt;= 90:\n        raise ValueError(f\"Latitude must be in the range [-90, 90], got {latitude}\")\n    if not -180 &lt;= longitude &lt;= 180:\n        raise ValueError(f\"Longitude must be in the range [-180, 180], got {longitude}\")\n    latitude = math.radians(latitude)\n    longitude = math.radians(longitude)\n\n    sinLatitude = math.sin(latitude)\n    pixelX = ((longitude + math.pi) / (2 * math.pi)) * 256 * 2**level_of_detail\n    pixelY = (0.5 - math.log((1 + sinLatitude) / (1 - sinLatitude)) / (4 * math.pi)) * 256 * 2**level_of_detail\n    tileX = int(math.floor(pixelX / 256))\n    tileY = int(math.floor(pixelY / 256))\n    return tileX, tileY\n</code></pre>"},{"location":"reference/core/quadkey_utils/#core.quadkey_utils.quadkey_to_extent","title":"<code>quadkey_to_extent(quadkey)</code>","text":"<p>Converts a quadkey to a geographic extent (bounding box).</p> <p>This function takes a quadkey and converts it to a geographic extent represented as a tuple of (longitude_min, latitude_min, longitude_max, latitude_max).</p> <p>Parameters:</p> Name Type Description Default <code>quadkey</code> <code>str</code> <p>The quadkey to convert. A quadkey is a string of digits that represents a</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[float, float, float, float]</code> <p>A tuple representing the geographic extent of the quadkey. The tuple contains four elements: (longitude_min, latitude_min, longitude_max, latitude_max).</p> Source code in <code>multimno/core/quadkey_utils.py</code> <pre><code>def quadkey_to_extent(quadkey: str) -&gt; Tuple[float, float, float, float]:\n    \"\"\"\n    Converts a quadkey to a geographic extent (bounding box).\n\n    This function takes a quadkey and converts it to a geographic extent represented as a tuple of\n    (longitude_min, latitude_min, longitude_max, latitude_max).\n\n    Args:\n        quadkey (str): The quadkey to convert. A quadkey is a string of digits that represents a\n        specific tile in a quadtree-based spatial index.\n\n    Returns:\n        tuple: A tuple representing the geographic extent of the quadkey. The tuple contains four\n            elements: (longitude_min, latitude_min, longitude_max, latitude_max).\n    \"\"\"\n    tile_x, tile_y, zoom_level = quadkey_to_tile(quadkey)\n    n = 2.0**zoom_level\n    lon_min = tile_x / n * 360.0 - 180.0\n    lat_min = math.degrees(math.atan(math.sinh(math.pi * (1 - 2 * (tile_y + 1) / n))))\n    lon_max = (tile_x + 1) / n * 360.0 - 180.0\n    lat_max = math.degrees(math.atan(math.sinh(math.pi * (1 - 2 * tile_y / n))))\n\n    return (lon_min, lat_min, lon_max, lat_max)\n</code></pre>"},{"location":"reference/core/quadkey_utils/#core.quadkey_utils.quadkey_to_tile","title":"<code>quadkey_to_tile(quadkey)</code>","text":"<p>Converts a quadkey to tile coordinates and zoom level.</p> <p>This function takes a quadkey and converts it to tile coordinates (tile_x, tile_y) and zoom level. A quadkey is a string of digits that represents a specific tile in a quadtree-based spatial index.</p> <p>Parameters:</p> Name Type Description Default <code>quadkey</code> <code>str</code> <p>The quadkey to convert.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>int</code> <p>A tuple representing the tile coordinates and zoom level of the quadkey. The tuple contains three</p> <code>elements</code> <code>int</code> <p>(tile_x, tile_y, zoom_level).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the quadkey contains an invalid character.</p> Source code in <code>multimno/core/quadkey_utils.py</code> <pre><code>def quadkey_to_tile(quadkey: str) -&gt; Tuple[int, int, int]:\n    \"\"\"\n    Converts a quadkey to tile coordinates and zoom level.\n\n    This function takes a quadkey and converts it to tile coordinates (tile_x, tile_y) and zoom level.\n    A quadkey is a string of digits that represents a specific tile in a quadtree-based spatial index.\n\n    Args:\n        quadkey (str): The quadkey to convert.\n\n    Returns:\n        tuple: A tuple representing the tile coordinates and zoom level of the quadkey. The tuple contains three\n        elements: (tile_x, tile_y, zoom_level).\n\n    Raises:\n        ValueError: If the quadkey contains an invalid character.\n    \"\"\"\n    tile_x = tile_y = 0\n    zoom_level = len(quadkey)\n    for i in range(zoom_level):\n        bit = zoom_level - i - 1\n        mask = 1 &lt;&lt; bit\n        if quadkey[i] == \"0\":\n            pass\n        elif quadkey[i] == \"1\":\n            tile_x |= mask\n        elif quadkey[i] == \"2\":\n            tile_y |= mask\n        elif quadkey[i] == \"3\":\n            tile_x |= mask\n            tile_y |= mask\n        else:\n            raise ValueError(\"Invalid quadkey character.\")\n    return tile_x, tile_y, zoom_level\n</code></pre>"},{"location":"reference/core/quadkey_utils/#core.quadkey_utils.quadkeys_to_extent_dataframe","title":"<code>quadkeys_to_extent_dataframe(spark, quadkeys, crs=4326)</code>","text":"<p>Converts a list of quadkeys to a Spark DataFrame with geometry polygons representing their extents.</p> <p>This function takes a list of quadkeys and creates a DataFrame where each row contains a quadkey and its corresponding geometry (polygon) based on the geographic extent. Uses ST_PolygonFromEnvelope for efficient polygon creation.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <p>The Spark session</p> required <code>quadkeys</code> <code>List[str]</code> <p>List of quadkeys to convert to geometries</p> required <code>crs</code> <code>str</code> <p>Coordinate reference system for the output geometries, defaults to WGS84</p> <code>4326</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A Spark DataFrame with columns 'quadkey' and 'geometry'</p> Source code in <code>multimno/core/quadkey_utils.py</code> <pre><code>def quadkeys_to_extent_dataframe(spark, quadkeys: List[str], crs: str = 4326) -&gt; DataFrame:\n    \"\"\"\n    Converts a list of quadkeys to a Spark DataFrame with geometry polygons representing their extents.\n\n    This function takes a list of quadkeys and creates a DataFrame where each row contains\n    a quadkey and its corresponding geometry (polygon) based on the geographic extent.\n    Uses ST_PolygonFromEnvelope for efficient polygon creation.\n\n    Args:\n        spark: The Spark session\n        quadkeys (List[str]): List of quadkeys to convert to geometries\n        crs (str): Coordinate reference system for the output geometries, defaults to WGS84\n\n    Returns:\n        DataFrame: A Spark DataFrame with columns 'quadkey' and 'geometry'\n    \"\"\"\n\n    # Create rows with quadkey and extent coordinates\n    data = []\n    for quadkey in quadkeys:\n        lon_min, lat_min, lon_max, lat_max = quadkey_to_extent(quadkey)\n        data.append((quadkey, [lon_min, lat_min, lon_max, lat_max]))\n\n    # Create DataFrame schema\n    schema = StructType(\n        [StructField(\"quadkey\", StringType(), False), StructField(\"extent\", ArrayType(DoubleType()), False)]\n    )\n\n    # Create DataFrame from data and schema\n    df = spark.createDataFrame(data, schema)\n\n    # Convert extent arrays to Sedona geometries using ST_PolygonFromEnvelope\n    df = df.withColumn(\n        \"geometry\",\n        STC.ST_PolygonFromEnvelope(\n            F.col(\"extent\")[0],  # xmin\n            F.col(\"extent\")[1],  # ymin\n            F.col(\"extent\")[2],  # xmax\n            F.col(\"extent\")[3],  # ymax\n        ),\n    )\n\n    # Transform to the desired CRS if needed\n    if crs != 4326:\n        df = df.withColumn(\"geometry\", STF.ST_Transform(\"geometry\", F.lit(\"epsg:4326\"), F.lit(f\"epsg:{crs}\")))\n\n    # Return the DataFrame with quadkey and geometry columns, dropping the intermediate array\n    return df.select(\"quadkey\", \"geometry\")\n</code></pre>"},{"location":"reference/core/quadkey_utils/#core.quadkey_utils.tilexy_to_quadkey","title":"<code>tilexy_to_quadkey(x, y, level_of_detail)</code>","text":"<p>Converts tile coordinates to a quadkey at a specific zoom level.</p> <p>This function takes tile coordinates (x, y) and a zoom level, and converts them to a quadkey. The quadkey is a string of digits that represents a specific tile in a quadtree-based spatial index. The conversion process involves bitwise operations on the tile coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>int</code> <p>The x-coordinate of the tile.</p> required <code>y</code> <code>int</code> <p>The y-coordinate of the tile.</p> required <code>level_of_detail</code> <code>int</code> <p>The zoom level.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The quadkey representing the tile at the specified zoom level.</p> Source code in <code>multimno/core/quadkey_utils.py</code> <pre><code>def tilexy_to_quadkey(x: int, y: int, level_of_detail: int) -&gt; str:\n    \"\"\"\n    Converts tile coordinates to a quadkey at a specific zoom level.\n\n    This function takes tile coordinates (x, y) and a zoom level, and converts them to a quadkey.\n    The quadkey is a string of digits that represents a specific tile in a quadtree-based spatial index.\n    The conversion process involves bitwise operations on the tile coordinates.\n\n    Args:\n        x (int): The x-coordinate of the tile.\n        y (int): The y-coordinate of the tile.\n        level_of_detail (int): The zoom level.\n\n    Returns:\n        str: The quadkey representing the tile at the specified zoom level.\n    \"\"\"\n    quadkey = \"\"\n    for i in range(level_of_detail, 0, -1):\n        digit = 0\n        mask = 1 &lt;&lt; (i - 1)\n        if (x &amp; mask) != 0:\n            digit += 1\n        if (y &amp; mask) != 0:\n            digit += 2\n        quadkey += str(digit)\n    return quadkey\n</code></pre>"},{"location":"reference/core/settings/","title":"settings","text":"<p>Settings module</p>"},{"location":"reference/core/spark_session/","title":"spark_session","text":"<p>Module that manages the spark session.</p>"},{"location":"reference/core/spark_session/#core.spark_session.SPARK_WRITING_MODES","title":"<code>SPARK_WRITING_MODES</code>","text":"<p>Enum class to define writing modes for spark</p> Source code in <code>multimno/core/spark_session.py</code> <pre><code>class SPARK_WRITING_MODES:\n    \"\"\"Enum class to define writing modes for spark\"\"\"\n\n    OVERWRITE = \"overwrite\"\n    APPEND = \"append\"\n    IGNORE = \"ignore\"\n    ERROR = \"error\"\n</code></pre>"},{"location":"reference/core/spark_session/#core.spark_session.check_if_data_path_exists","title":"<code>check_if_data_path_exists(spark, data_path)</code>","text":"<p>Checks whether data path exists, returns True if it does, False if not</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>active SparkSession</p> required <code>data_path</code> <code>str</code> <p>path to check</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether the passed path exists</p> Source code in <code>multimno/core/spark_session.py</code> <pre><code>def check_if_data_path_exists(spark: SparkSession, data_path: str) -&gt; bool:\n    \"\"\"\n    Checks whether data path exists, returns True if it does, False if not\n\n    Args:\n        spark (SparkSession): active SparkSession\n        data_path (str): path to check\n\n    Returns:\n        bool: Whether the passed path exists\n    \"\"\"\n    conf = spark._jsc.hadoopConfiguration()\n    uri = spark._jvm.java.net.URI.create(data_path)\n    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(uri, conf)\n    return fs.exists(spark._jvm.org.apache.hadoop.fs.Path(data_path))\n</code></pre>"},{"location":"reference/core/spark_session/#core.spark_session.check_or_create_data_path","title":"<code>check_or_create_data_path(spark, data_path)</code>","text":"<p>Create the provided path on a file system. If path already exists, do nothing.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>active SparkSession</p> required <code>data_path</code> <code>str</code> <p>path to check</p> required Source code in <code>multimno/core/spark_session.py</code> <pre><code>def check_or_create_data_path(spark: SparkSession, data_path: str):\n    \"\"\"\n    Create the provided path on a file system. If path already exists, do nothing.\n\n    Args:\n        spark (SparkSession): active SparkSession\n        data_path (str): path to check\n    \"\"\"\n    conf = spark._jsc.hadoopConfiguration()\n    uri = spark._jvm.java.net.URI.create(data_path)\n    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(uri, conf)\n    path = spark._jvm.org.apache.hadoop.fs.Path(data_path)\n    if not fs.exists(path):\n        fs.mkdirs(path)\n</code></pre>"},{"location":"reference/core/spark_session/#core.spark_session.delete_file_or_folder","title":"<code>delete_file_or_folder(spark, data_path)</code>","text":"<p>Deletes file or folder with given path</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Currently active spark session</p> required <code>data_path</code> <code>str</code> <p>Path to remove</p> required Source code in <code>multimno/core/spark_session.py</code> <pre><code>def delete_file_or_folder(spark: SparkSession, data_path: str):\n    \"\"\"\n    Deletes file or folder with given path\n\n    Args:\n        spark (SparkSession): Currently active spark session\n        data_path (str): Path to remove\n    \"\"\"\n    conf = spark._jsc.hadoopConfiguration()\n    uri = spark._jvm.java.net.URI.create(data_path)\n    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(uri, conf)\n    path = spark._jvm.org.apache.hadoop.fs.Path(data_path)\n    fs.delete(path, True)\n</code></pre>"},{"location":"reference/core/spark_session/#core.spark_session.generate_spark_session","title":"<code>generate_spark_session(config)</code>","text":"<p>Function that generates a Spark Sedona session.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ConfigParser</code> <p>Object with the final configuration.</p> required <p>Returns:</p> Name Type Description <code>SparkSession</code> <code>SparkSession</code> <p>Session of spark.</p> Source code in <code>multimno/core/spark_session.py</code> <pre><code>def generate_spark_session(config: ConfigParser) -&gt; SparkSession:\n    \"\"\"Function that generates a Spark Sedona session.\n\n    Args:\n        config (ConfigParser): Object with the final configuration.\n\n    Returns:\n        SparkSession: Session of spark.\n    \"\"\"\n    conf_dict = dict(config[SPARK_CONFIG_KEY])\n    master = conf_dict.pop(\"spark.master\")\n    session_name = conf_dict.pop(\"session_name\")\n\n    builder = SedonaContext.builder().appName(f\"{session_name}\").master(master)\n\n    # Configuration file spark configs\n    for k, v in conf_dict.items():\n        builder = builder.config(k, v)\n\n    ##################\n    # SEDONA\n    ##################\n\n    # Set sedona session\n    spark = SedonaContext.create(builder.getOrCreate())\n    sc = spark.sparkContext\n    sc.setSystemProperty(\"sedona.global.charset\", \"utf8\")\n\n    # Set log\n    sc.setLogLevel(\"ERROR\")\n    log4j = sc._jvm.org.apache.log4j\n    log4j.LogManager.getRootLogger().setLevel(log4j.Level.ERROR)\n\n    return spark\n</code></pre>"},{"location":"reference/core/spark_session/#core.spark_session.list_all_files_helper","title":"<code>list_all_files_helper(path, fs, conf)</code>","text":"<p>This function is used by list_all_files_recursively. This should not be called elsewhere Recursively traverses the file tree from given spot saving all files to a list and returns it.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>py4j.java_gateway.JavaObject: Object from parent function</p> required <code>fs</code> <code>JavaClass</code> <p>Object from parent function</p> required <code>conf</code> <code>JavaObject</code> <p>Object from parent function</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>List[str]</code> <p>List of all files this folder and subdirectories of this folder.</p> Source code in <code>multimno/core/spark_session.py</code> <pre><code>def list_all_files_helper(\n    path: py4j.java_gateway.JavaObject, fs: py4j.java_gateway.JavaClass, conf: py4j.java_gateway.JavaObject\n) -&gt; List[str]:\n    \"\"\"\n    This function is used by list_all_files_recursively. This should not be called elsewhere\n    Recursively traverses the file tree from given spot saving all files to a list and returns it.\n\n    Args:\n        path (str): py4j.java_gateway.JavaObject: Object from parent function\n        fs (py4j.java_gateway.JavaClass): Object from parent function\n        conf (py4j.java_gateway.JavaObject): Object from parent function\n\n    Returns:\n        list: List of all files this folder and subdirectories of this folder.\n    \"\"\"\n    files_list = []\n\n    for f in fs.listStatus(path):\n        if f.isDirectory():\n            files_list.extend(list_all_files_helper(f.getPath(), fs, conf))\n        else:\n            files_list.append(str(f.getPath()))\n\n    return files_list\n</code></pre>"},{"location":"reference/core/spark_session/#core.spark_session.list_all_files_recursively","title":"<code>list_all_files_recursively(spark, data_path)</code>","text":"<p>If path is a file, returns a singleton list with this path. If path is a folder, return a list of all files in this folder and any of its subfolders</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Currently active spark session</p> required <code>data_path</code> <code>str</code> <p>Path to list the files of</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of all files in that folder and its subfolders</p> Source code in <code>multimno/core/spark_session.py</code> <pre><code>def list_all_files_recursively(spark: SparkSession, data_path: str) -&gt; List[str]:\n    \"\"\"\n    If path is a file, returns a singleton list with this path.\n    If path is a folder, return a list of all files in this folder and any of its subfolders\n\n    Args:\n        spark (SparkSession): Currently active spark session\n        data_path (str): Path to list the files of\n\n    Returns:\n        List[str]: A list of all files in that folder and its subfolders\n    \"\"\"\n    conf = spark._jsc.hadoopConfiguration()\n    uri = spark._jvm.java.net.URI.create(data_path)\n    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(uri, conf)\n    path = spark._jvm.org.apache.hadoop.fs.Path(data_path)\n    return list_all_files_helper(path, fs, conf)\n</code></pre>"},{"location":"reference/core/spark_session/#core.spark_session.list_parquet_partition_col_values","title":"<code>list_parquet_partition_col_values(spark, data_path)</code>","text":"<p>Lists all partition column values given a partition parquet folder</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Currently active spark session</p> required <code>data_path</code> <code>str</code> <p>Path of parquet</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>str, List[str]: Name of partition column, List of partition col values</p> Source code in <code>multimno/core/spark_session.py</code> <pre><code>def list_parquet_partition_col_values(spark: SparkSession, data_path: str) -&gt; List[str]:\n    \"\"\"\n    Lists all partition column values given a partition parquet folder\n\n    Args:\n        spark (SparkSession): Currently active spark session\n        data_path (str): Path of parquet\n\n    Returns:\n        str, List[str]: Name of partition column, List of partition col values\n    \"\"\"\n\n    hadoop = spark._jvm.org.apache.hadoop\n    fs = hadoop.fs.FileSystem\n    conf = hadoop.conf.Configuration()\n    path = hadoop.fs.Path(data_path)\n\n    partitions = []\n    for f in fs.get(conf).listStatus(path):\n        if f.isDirectory():\n            partitions.append(str(f.getPath().getName()))\n\n    if len(partitions) == 0:\n        return None, None\n\n    partition_col = partitions[0].split(\"=\")[0]\n\n    partitions = [p.split(\"=\")[1] for p in partitions]\n    return partition_col, sorted(partitions)\n</code></pre>"},{"location":"reference/core/utils/","title":"utils","text":"<p>This module contains utility functions for the multimno package.</p>"},{"location":"reference/core/utils/#core.utils.apply_schema_casting","title":"<code>apply_schema_casting(sdf, schema)</code>","text":"<p>This function takes a DataFrame and a schema, and applies the schema to the DataFrame. It selects the columns in the DataFrame that are in the schema, and casts each column to the type specified in the schema.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The DataFrame to apply the schema to.</p> required <code>schema</code> <code>StructType</code> <p>The schema to apply to the DataFrame.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame that includes the same rows as the input DataFrame,</p> <code>DataFrame</code> <p>but with the columns cast to the types specified in the schema.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def apply_schema_casting(sdf: DataFrame, schema: StructType) -&gt; DataFrame:\n    \"\"\"\n    This function takes a DataFrame and a schema, and applies the schema to the DataFrame.\n    It selects the columns in the DataFrame that are in the schema, and casts each column to the type specified in the schema.\n\n    Args:\n        sdf (DataFrame): The DataFrame to apply the schema to.\n        schema (StructType): The schema to apply to the DataFrame.\n\n    Returns:\n        DataFrame: A new DataFrame that includes the same rows as the input DataFrame,\n        but with the columns cast to the types specified in the schema.\n    \"\"\"\n\n    sdf = sdf.select(*[F.col(field.name) for field in schema.fields])\n    for field in schema.fields:\n        sdf = sdf.withColumn(field.name, F.col(field.name).cast(field.dataType))\n\n    return sdf\n</code></pre>"},{"location":"reference/core/utils/#core.utils.calc_hashed_user_id","title":"<code>calc_hashed_user_id(df, user_column=ColNames.user_id)</code>","text":"<p>Calculates SHA2 hash of user id, takes the first 31 bits and converts them to a non-negative 32-bit integer.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Data of clean synthetic events with a user id column.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: Dataframe, where user_id column is transformered to a hashed value.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def calc_hashed_user_id(df: DataFrame, user_column: str = ColNames.user_id) -&gt; DataFrame:\n    \"\"\"\n    Calculates SHA2 hash of user id, takes the first 31 bits and converts them to a non-negative 32-bit integer.\n\n    Args:\n        df (pyspark.sql.DataFrame): Data of clean synthetic events with a user id column.\n\n    Returns:\n        pyspark.sql.DataFrame: Dataframe, where user_id column is transformered to a hashed value.\n\n    \"\"\"\n\n    df = df.withColumn(user_column, F.unhex(F.sha2(F.col(user_column).cast(\"string\"), 256)))\n    return df\n</code></pre>"},{"location":"reference/core/utils/#core.utils.clip_polygons_with_mask_polygons","title":"<code>clip_polygons_with_mask_polygons(input_sdf, mask_sdf, cols_to_keep, self_intersection=False, geometry_column='geometry')</code>","text":"<p>Cuts polygons in the input DataFrame with mask polygons from another DataFrame. This function takes two DataFrames: one with input polygons and another with mask polygons. It cuts the input polygons with the mask polygons, and returns a new DataFrame with the resulting polygons. Both dataframes have to have same coordinate system. Args:     input_sdf (DataFrame): A DataFrame containing the input polygons.     mask_sdf (DataFrame): A DataFrame containing the mask polygons.     cols_to_keep (list): A list of column names to keep from the input DataFrame.     geometry_column (str, optional): The name of the geometry column in the DataFrames.         Defaults to \"geometry\". Returns:     DataFrame: A DataFrame containing the resulting polygons after cutting the input polygons with the mask polygons.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def clip_polygons_with_mask_polygons(\n    input_sdf: DataFrame,\n    mask_sdf: DataFrame,\n    cols_to_keep: List[str],\n    self_intersection=False,\n    geometry_column: str = \"geometry\",\n) -&gt; DataFrame:\n    \"\"\"\n    Cuts polygons in the input DataFrame with mask polygons from another DataFrame.\n    This function takes two DataFrames: one with input polygons and another with mask polygons.\n    It cuts the input polygons with the mask polygons, and returns a new DataFrame with the resulting polygons.\n    Both dataframes have to have same coordinate system.\n    Args:\n        input_sdf (DataFrame): A DataFrame containing the input polygons.\n        mask_sdf (DataFrame): A DataFrame containing the mask polygons.\n        cols_to_keep (list): A list of column names to keep from the input DataFrame.\n        geometry_column (str, optional): The name of the geometry column in the DataFrames.\n            Defaults to \"geometry\".\n    Returns:\n        DataFrame: A DataFrame containing the resulting polygons after cutting the input polygons with the mask polygons.\n    \"\"\"\n    input_sdf = input_sdf.withColumn(\"id\", F.monotonically_increasing_id())\n    cols_to_keep = [f\"a.{col}\" for col in cols_to_keep]\n    if self_intersection:\n        # Join smaller polygons to larger polygons\n        input_sdf = input_sdf.withColumn(\"area\", STF.ST_Area(geometry_column))\n        intersection = input_sdf.alias(\"a\").join(\n            input_sdf.alias(\"b\"),\n            STP.ST_Intersects(\"a.geometry\", \"b.geometry\") &amp; (F.col(\"a.area\") &gt; F.col(\"b.area\")),\n        )\n        input_sdf = input_sdf.drop(\"area\")\n    else:\n        intersection = input_sdf.alias(\"a\").join(\n            mask_sdf.alias(\"b\"),\n            STP.ST_Intersects(\"a.geometry\", \"b.geometry\"),\n        )\n    intersection_cut = (\n        intersection.groupby(\"a.id\", *cols_to_keep)\n        .agg(F.array_agg(f\"b.{geometry_column}\").alias(\"cut_geometry\"))\n        .withColumn(\"cut_geometry\", STF.ST_Union(\"cut_geometry\"))\n    )\n    intersection_cut = fix_geometry(intersection_cut, 3, \"cut_geometry\")\n    intersection_cut = intersection_cut.withColumn(\n        geometry_column, STF.ST_Difference(f\"a.{geometry_column}\", \"cut_geometry\")\n    ).drop(\"cut_geometry\")\n\n    non_intersection = input_sdf.join(intersection_cut, [\"id\"], \"left_anti\")\n    full_sdf = non_intersection.union(intersection_cut).drop(\"id\")\n\n    full_sdf = fix_geometry(full_sdf, 3, geometry_column)\n\n    return full_sdf\n</code></pre>"},{"location":"reference/core/utils/#core.utils.cut_geodata_to_extent","title":"<code>cut_geodata_to_extent(sdf, extent, target_crs, geometry_column='geometry')</code>","text":"<p>Cuts geometries in a DataFrame to a specified extent.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The DataFrame to filter. The DataFrame must contain a geometry column.</p> required <code>extent</code> <code>tuple</code> <p>A tuple representing the extent. The tuple contains four elements: (west, south, east, north), which are the western, southern, eastern, and northern bounds of the WGS84 extent.</p> required <code>target_crs</code> <code>int</code> <p>The CRS of DataFrame to transform the extent to.</p> required <code>geometry_column</code> <code>str</code> <p>The name of the geometry column. Defaults to \"geometry\".</p> <code>'geometry'</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing the same rows as the input DataFrame, but with the geometries cut to the extent.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def cut_geodata_to_extent(\n    sdf: DataFrame,\n    extent: Tuple[float, float, float, float],\n    target_crs: int,\n    geometry_column: str = \"geometry\",\n) -&gt; DataFrame:\n    \"\"\"\n    Cuts geometries in a DataFrame to a specified extent.\n\n    Args:\n        sdf (DataFrame): The DataFrame to filter. The DataFrame must contain a geometry column.\n        extent (tuple): A tuple representing the extent. The tuple contains four elements:\n            (west, south, east, north), which are the western, southern, eastern, and northern bounds of the WGS84 extent.\n        target_crs (int): The CRS of DataFrame to transform the extent to.\n        geometry_column (str, optional): The name of the geometry column. Defaults to \"geometry\".\n\n    Returns:\n        DataFrame: A DataFrame containing the same rows as the input DataFrame, but with the geometries cut to the extent.\n    \"\"\"\n\n    sdf = filter_geodata_to_extent(sdf, extent, target_crs, geometry_column)\n\n    extent = STC.ST_PolygonFromEnvelope(*extent)\n    if target_crs != 4326:\n        extent = STF.ST_Transform(extent, F.lit(\"EPSG:4326\"), F.lit(f\"EPSG:{target_crs}\"))\n    sdf = sdf.withColumn(geometry_column, STF.ST_Intersection(F.col(geometry_column), extent))\n\n    return sdf\n</code></pre>"},{"location":"reference/core/utils/#core.utils.filter_geodata_to_extent","title":"<code>filter_geodata_to_extent(sdf, extent, target_crs, geometry_column='geometry')</code>","text":"<p>Filters a DataFrame to include only rows with geometries that intersect a specified extent.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The DataFrame to filter. The DataFrame must contain a geometry column.</p> required <code>extent</code> <code>tuple</code> <p>A tuple representing the extent. The tuple contains four elements: (west, south, east, north), which are the western, southern, eastern, and northern bounds of the WGS84 extent.</p> required <code>target_crs</code> <code>int</code> <p>The CRS of DataFrame to transform the extent to.</p> required <code>geometry_column</code> <code>str</code> <p>The name of the geometry column. Defaults to \"geometry\".</p> <code>'geometry'</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing only the rows from the input DataFrame where the geometry intersects the extent.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def filter_geodata_to_extent(\n    sdf: DataFrame,\n    extent: Tuple[float, float, float, float],\n    target_crs: int,\n    geometry_column: str = \"geometry\",\n) -&gt; DataFrame:\n    \"\"\"\n    Filters a DataFrame to include only rows with geometries that intersect a specified extent.\n\n    Args:\n        sdf (DataFrame): The DataFrame to filter. The DataFrame must contain a geometry column.\n        extent (tuple): A tuple representing the extent. The tuple contains four elements:\n            (west, south, east, north), which are the western, southern, eastern, and northern bounds of the WGS84 extent.\n        target_crs (int): The CRS of DataFrame to transform the extent to.\n        geometry_column (str, optional): The name of the geometry column. Defaults to \"geometry\".\n\n    Returns:\n        DataFrame: A DataFrame containing only the rows from the input DataFrame where the geometry intersects the extent.\n    \"\"\"\n\n    extent = STC.ST_PolygonFromEnvelope(*extent)\n    if target_crs != 4326:\n        extent = STF.ST_Transform(extent, F.lit(\"EPSG:4326\"), F.lit(f\"EPSG:{target_crs}\"))\n\n    sdf = sdf.filter(STP.ST_Intersects(extent, F.col(geometry_column)))\n\n    return sdf\n</code></pre>"},{"location":"reference/core/utils/#core.utils.fix_geometry","title":"<code>fix_geometry(sdf, geometry_type, geometry_column='geometry')</code>","text":"<p>Fixes the geometry of a given type in a DataFrame. This function applies several operations to the geometries in the specified geometry column of the DataFrame: 1. If a geometry is a collection of geometries, extracts only the geometries of the given type. 2. Filters out any geometries of type other than given. 3. Removes any invalid geometries. 4. Removes any empty geometries. Args:     sdf (DataFrame): The DataFrame containing the geometries to check.     geometry_column (str, optional): The name of the column containing the geometries. Defaults to \"geometry\". Returns:     DataFrame: The DataFrame with the fixed polygon geometries.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def fix_geometry(sdf: DataFrame, geometry_type: int, geometry_column: str = \"geometry\") -&gt; DataFrame:\n    \"\"\"\n    Fixes the geometry of a given type in a DataFrame.\n    This function applies several operations to the geometries in the specified geometry column of the DataFrame:\n    1. If a geometry is a collection of geometries, extracts only the geometries of the given type.\n    2. Filters out any geometries of type other than given.\n    3. Removes any invalid geometries.\n    4. Removes any empty geometries.\n    Args:\n        sdf (DataFrame): The DataFrame containing the geometries to check.\n        geometry_column (str, optional): The name of the column containing the geometries. Defaults to \"geometry\".\n    Returns:\n        DataFrame: The DataFrame with the fixed polygon geometries.\n    \"\"\"\n    geometry_name = \"Polygon\" if geometry_type == 3 else (\"Line\" if geometry_type == 2 else \"Point\")\n    sdf = (\n        sdf.withColumn(\n            geometry_column,\n            F.when(\n                STF.ST_IsCollection(F.col(geometry_column)),\n                STF.ST_CollectionExtract(geometry_column, F.lit(geometry_type)),\n            ).otherwise(F.col(geometry_column)),\n        )\n        .filter(~STF.ST_IsEmpty(F.col(geometry_column)))\n        .filter(STF.ST_GeometryType(F.col(geometry_column)).like(f\"%{geometry_name}%\"))\n        .filter(STF.ST_IsValid(geometry_column))\n        .withColumn(geometry_column, STF.ST_ReducePrecision(F.col(geometry_column), F.lit(5)))\n    )\n    return sdf\n</code></pre>"},{"location":"reference/core/utils/#core.utils.get_epsg_from_geometry_column","title":"<code>get_epsg_from_geometry_column(df)</code>","text":"<p>Get the EPSG code from the geometry column of a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with a geometry column.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the DataFrame contains multiple EPSG codes.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>EPSG code of the geometry column.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def get_epsg_from_geometry_column(df: DataFrame) -&gt; int:\n    \"\"\"\n    Get the EPSG code from the geometry column of a DataFrame.\n\n    Args:\n        df (DataFrame): DataFrame with a geometry column.\n\n    Raises:\n        ValueError: If the DataFrame contains multiple EPSG codes.\n\n    Returns:\n        int: EPSG code of the geometry column.\n    \"\"\"\n    # Get the EPSG code from the geometry column\n    temp = df.select(STF.ST_SRID(\"geometry\")).distinct().persist()\n    if temp.count() &gt; 1:\n        raise ValueError(\"Dataframe contains multiple EPSG codes\")\n\n    epsg = temp.collect()[0][0]\n    return epsg\n</code></pre>"},{"location":"reference/core/utils/#core.utils.merge_geom_within_mask_geom","title":"<code>merge_geom_within_mask_geom(input_sdf, mask_sdf, cols_to_keep, geometry_col)</code>","text":"<p>Merges geometries from an input DataFrame that intersect with geometries from a mask DataFrame.</p> <p>This function performs a spatial join between input and mask DataFrames using ST_Intersects, calculates the geometric intersection between each matching pair of geometries, then groups by specified columns and unions the resulting intersection geometries.</p> <p>Parameters:</p> Name Type Description Default <code>input_sdf</code> <code>DataFrame</code> <p>Input DataFrame containing geometries to be processed.                   Must contain a 'geometry' column.</p> required <code>mask_sdf</code> <code>DataFrame</code> <p>Mask DataFrame containing geometries that define the areas of interest.                   Must contain a 'geometry' column.</p> required <code>cols_to_keep</code> <code>List</code> <p>List of column names from the input DataFrame to preserve in the output.                 These columns will be used as grouping keys.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing merged geometries that result from intersecting the input       geometries with the mask geometries.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def merge_geom_within_mask_geom(\n    input_sdf: DataFrame, mask_sdf: DataFrame, cols_to_keep: List, geometry_col: str\n) -&gt; DataFrame:\n    \"\"\"\n    Merges geometries from an input DataFrame that intersect with geometries from a mask DataFrame.\n\n    This function performs a spatial join between input and mask DataFrames using ST_Intersects,\n    calculates the geometric intersection between each matching pair of geometries,\n    then groups by specified columns and unions the resulting intersection geometries.\n\n    Args:\n        input_sdf (DataFrame): Input DataFrame containing geometries to be processed.\n                              Must contain a 'geometry' column.\n        mask_sdf (DataFrame): Mask DataFrame containing geometries that define the areas of interest.\n                              Must contain a 'geometry' column.\n        cols_to_keep (List): List of column names from the input DataFrame to preserve in the output.\n                            These columns will be used as grouping keys.\n\n    Returns:\n        DataFrame: A DataFrame containing merged geometries that result from intersecting the input\n                  geometries with the mask geometries.\n    \"\"\"\n\n    merge_sdf = (\n        input_sdf.alias(\"a\")\n        .join(\n            mask_sdf.alias(\"b\"),\n            STP.ST_Intersects(f\"a.{geometry_col}\", f\"b.{geometry_col}\"),\n        )\n        .withColumn(\"merge_geometry\", STF.ST_Intersection(f\"a.{geometry_col}\", f\"b.{geometry_col}\"))\n        .groupBy(*cols_to_keep)\n        .agg(F.array_agg(\"merge_geometry\").alias(geometry_col))\n        .withColumn(geometry_col, F.explode(STF.ST_Dump(STF.ST_Union(geometry_col))))\n    )\n\n    return merge_sdf\n</code></pre>"},{"location":"reference/core/utils/#core.utils.project_to_crs","title":"<code>project_to_crs(sdf, crs_in, crs_out, geometry_column='geometry')</code>","text":"<p>Projects geometry to CRS.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>crs_in</code> <code>int</code> <p>Input CRS.</p> required <code>crs_out</code> <code>int</code> <p>Output CRS.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with geometry projected to cartesian CRS.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def project_to_crs(sdf: DataFrame, crs_in: int, crs_out: int, geometry_column=\"geometry\") -&gt; DataFrame:\n    \"\"\"\n    Projects geometry to CRS.\n\n    Args:\n        sdf (DataFrame): Input DataFrame.\n        crs_in (int): Input CRS.\n        crs_out (int): Output CRS.\n\n    Returns:\n        DataFrame: DataFrame with geometry projected to cartesian CRS.\n    \"\"\"\n    crs_in = f\"EPSG:{crs_in}\"\n    crs_out = f\"EPSG:{crs_out}\"\n\n    sdf = sdf.withColumn(\n        geometry_column,\n        STF.ST_Transform(sdf[geometry_column], F.lit(crs_in), F.lit(crs_out)),\n    )\n    return sdf\n</code></pre>"},{"location":"reference/core/utils/#core.utils.spark_to_geopandas","title":"<code>spark_to_geopandas(df, epsg=None)</code>","text":"<p>Convert a Spark DataFrame to a geopandas GeoDataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Spark DataFrame to convert.</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: GeoDataFrame with the same data as the input DataFrame.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def spark_to_geopandas(df: DataFrame, epsg: int = None) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Convert a Spark DataFrame to a geopandas GeoDataFrame.\n\n    Args:\n        df (DataFrame): Spark DataFrame to convert.\n\n    Returns:\n        gpd.GeoDataFrame: GeoDataFrame with the same data as the input DataFrame.\n    \"\"\"\n    # Convert the DataFrame to a GeoDataFrame\n    if epsg is None:\n        epsg = get_epsg_from_geometry_column(df)\n    gdf = gpd.GeoDataFrame(df.toPandas(), crs=f\"EPSG:{epsg}\")\n\n    return gdf\n</code></pre>"},{"location":"reference/core/constants/","title":"constants","text":""},{"location":"reference/core/constants/columns/","title":"columns","text":"<p>Reusable internal column names. Useful for referring to the the same column across multiple components.</p>"},{"location":"reference/core/constants/columns/#core.constants.columns.ColNames","title":"<code>ColNames</code>","text":"<p>Class that enumerates all the column names.</p> Source code in <code>multimno/core/constants/columns.py</code> <pre><code>class ColNames:\n    \"\"\"\n    Class that enumerates all the column names.\n    \"\"\"\n\n    user_id = \"user_id\"\n    partition_id = \"partition_id\"\n    timestamp = \"timestamp\"\n    mcc = \"mcc\"\n    mnc = \"mnc\"\n    plmn = \"plmn\"\n    cell_id = \"cell_id\"\n    latitude = \"latitude\"\n    longitude = \"longitude\"\n    error_flag = \"error_flag\"\n    transformation_flag = \"transformation_flag\"\n    affected_field = \"affected_field\"\n    domain = \"domain\"\n    is_last_event = \"is_last_event\"  # 0: initial, 1: final\n\n    altitude = \"altitude\"\n    antenna_height = \"antenna_height\"\n    directionality = \"directionality\"\n    azimuth_angle = \"azimuth_angle\"\n    elevation_angle = \"elevation_angle\"\n    horizontal_beam_width = \"horizontal_beam_width\"\n    vertical_beam_width = \"vertical_beam_width\"\n    power = \"power\"\n    range = \"range\"\n    frequency = \"frequency\"\n    technology = \"technology\"\n    valid_date_start = \"valid_date_start\"\n    valid_date_end = \"valid_date_end\"\n    cell_type = \"cell_type\"\n\n    loc_error = \"loc_error\"\n    event_id = \"event_id\"\n\n    year = \"year\"\n    month = \"month\"\n    day = \"day\"\n    user_id_modulo = \"user_id_modulo\"\n\n    # for QA by column\n    variable = \"variable\"\n    type_of_error = \"type_of_error\"\n    type_of_transformation = \"type_of_transformation\"\n    value = \"value\"\n    result_timestamp = \"result_timestamp\"\n    data_period_start = \"data_period_start\"\n    data_period_end = \"data_period_end\"\n    field_name = \"field_name\"\n    initial_frequency = \"initial_frequency\"\n    final_frequency = \"final_frequency\"\n    date = \"date\"\n\n    # warnings\n    # log table\n    measure_definition = \"measure_definition\"\n    lookback_period = \"lookback_period\"\n    daily_value = \"daily_value\"\n    condition_value = \"condition_value\"\n    condition = \"condition\"\n    warning_text = \"warning_text\"\n    # for plots\n    type_of_qw = \"type_of_qw\"\n    average = \"average\"\n    UCL = \"UCL\"\n    LCL = \"LCL\"\n    title = \"title\"\n\n    # top frequent errors\n    error_value = \"error_value\"\n    error_count = \"error_count\"\n    accumulated_percentage = \"accumulated_percentage\"\n\n    # for grid generation\n    geometry = \"geometry\"\n    grid_id = \"grid_id\"\n    inspire_id = \"INSPIRE_id\"\n    origin = \"origin\"\n    elevation = \"elevation\"\n    land_use = \"land_use\"\n    type_code = \"type_code\"\n    main_landuse_category = \"main_landuse_category\"\n    landuse_area_ratios = \"landuse_areas\"\n    prior_probability = \"prior_probability\"\n    ple_coefficient = \"environment_ple_coefficient\"\n    quadkey = \"quadkey\"\n\n    # device activity statistics\n    event_cnt = \"event_cnt\"\n    unique_cell_cnt = \"unique_cell_cnt\"\n    unique_location_cnt = \"unique_location_cnt\"\n    sum_distance_m = \"sum_distance_m\"\n    unique_hour_cnt = \"unique_hour_cnt\"\n    mean_time_gap = \"mean_time_gap\"\n    stdev_time_gap = \"stdev_time_gap\"\n\n    # signal\n    signal_strength = \"signal_strength\"\n    distance_to_cell = \"distance_to_cell\"\n    distance_to_cell_3D = \"distance_to_cell_3D\"\n    joined_geometry = \"joined_geometry\"\n    path_loss_exponent = \"path_loss_exponent\"\n    azimuth_signal_strength_back_loss = \"azimuth_signal_strength_back_loss\"\n    elevation_signal_strength_back_loss = \"elevation_signal_strength_back_loss\"\n\n    # for cell footprint\n    signal_dominance = \"signal_dominance\"\n    cells = \"cells\"\n\n    # cell footprint quality metrics\n    number_of_events = \"number_of_events\"\n    percentage_total_events = \"percentage_total_events\"\n\n    # Cell footprint intersections\n    group_id = \"group_id\"\n\n    # Nearby cells and cell overlap\n    overlapping_cell_ids = \"overlapping_cell_ids\"\n    cell_id_a = \"cell_id_a\"\n    cell_id_b = \"cell_id_b\"\n    distance = \"distance\"\n\n    # time segments\n    time_segment_id = \"time_segment_id\"\n    start_timestamp = \"start_timestamp\"\n    end_timestamp = \"end_timestamp\"\n    last_event_timestamp = \"last_event_timestamp\"\n    state = \"state\"\n    is_last = \"is_last\"\n\n    # for cell connection probability\n    cell_connection_probability = \"cell_connection_probability\"\n    posterior_probability = \"posterior_probability\"\n\n    # dps (daily permanence score)\n    dps = \"dps\"\n    stay_duration = \"stay_duration\"\n    time_slot_initial_time = \"time_slot_initial_time\"\n    time_slot_end_time = \"time_slot_end_time\"\n    id_type = \"id_type\"\n\n    num_unknown_devices = \"number_unknown_devices\"\n    pct_unknown_devices = \"percentage_unknown_devices\"\n\n    # midterm permanence score\n    mps = \"mps\"\n    day_type = \"day_type\"\n    time_interval = \"time_interval\"\n    regularity_mean = \"regularity_mean\"\n    regularity_std = \"regularity_std\"\n\n    # longterm permanence score\n    lps = \"lps\"\n    total_frequency = \"total_frequency\"\n    frequency_mean = \"frequency_mean\"\n    frequency_std = \"frequency_std\"\n    start_date = \"start_date\"\n    end_date = \"end_date\"\n    season = \"season\"\n\n    # diaries\n    stay_type = \"stay_type\"\n    activity_type = \"activity_type\"\n    initial_timestamp = \"initial_timestamp\"\n    final_timestamp = \"final_timestamp\"\n\n    # present population\n    device_count = \"device_count\"\n    population = \"population\"\n\n    # zone to grid mapping\n    zone_id = \"zone_id\"\n    hierarchical_id = \"hierarchical_id\"\n    dataset_id = \"dataset_id\"\n\n    # for spatial data\n    category = \"category\"\n    zone_id = \"zone_id\"\n    level = \"level\"\n    parent_id = \"parent_id\"\n    iso2 = \"iso2\"\n    iso3 = \"iso3\"\n    name = \"name\"\n    dataset_id = \"dataset_id\"\n    hierarchical_id = \"hierarchical_id\"\n    eurostat_code = \"eurostat_code\"\n    timezone = \"timezone\"\n\n    # for usual environment labels\n    label = \"label\"\n    label_rule = \"label_rule\"\n\n    # for usual environment labeling quality metrics\n    labeling_quality_metric = \"metric\"\n    labeling_quality_count = \"count\"\n    labeling_quality_min = \"min\"\n    labeling_quality_max = \"max\"\n    labeling_quality_avg = \"avg\"\n\n    # for usual environment aggregation\n    weighted_device_count = \"weighted_device_count\"\n    tile_weight = \"tile_weight\"\n    device_tile_weight = \"device_tile_weight\"\n\n    # for tourism stays\n    zone_weight = \"zone_weight\"\n    is_overnight = \"is_overnight\"\n    zone_weights_list = \"zone_weights_list\"\n    zone_ids_list = \"zone_ids_list\"\n\n    # for tourism trips\n    trip_id = \"trip_id\"\n    visit_id = \"visit_id\"\n    trip_start_timestamp = \"trip_start_timestamp\"\n    time_segment_ids_list = \"time_segment_ids_list\"\n    is_trip_finished = \"is_trip_finished\"\n    time_period = \"time_period\"\n    country_of_origin = \"country_of_origin\"\n    avg_destinations = \"avg_destinations\"\n    avg_nights_spent_per_destination = \"avg_nights_spent_per_destination\"\n    geography_id = \"geography_id\"\n    nights_spent = \"nights_spent\"\n    num_of_departures = \"num_of_departures\"\n    country_of_destination = \"country_of_destination\"\n\n    # internal migration\n    previous_zone = \"previous_zone\"\n    new_zone = \"new_zone\"\n    migration = \"migration\"\n    start_date_previous = \"start_date_previous\"\n    end_date_previous = \"end_date_previous\"\n    season_previous = \"season_previous\"\n    start_date_new = \"start_date_new\"\n    end_date_new = \"end_date_new\"\n    season_new = \"season_new\"\n\n    # internal migration quality metrics\n    previous_home_users = \"previous_home_users\"\n    new_home_users = \"new_home_users\"\n    common_home_users = \"common_home_users\"\n\n    # estimation factors\n    deduplication_factor = \"deduplication_factor\"\n    mno_to_target_population_factor = \"mno_to_target_population_factor\"\n</code></pre>"},{"location":"reference/core/constants/conditions/","title":"conditions","text":""},{"location":"reference/core/constants/domain_names/","title":"domain_names","text":""},{"location":"reference/core/constants/error_types/","title":"error_types","text":"<p>Transformations Error types module.</p>"},{"location":"reference/core/constants/error_types/#core.constants.error_types.ErrorTypes","title":"<code>ErrorTypes</code>","text":"<p>Class that enumerates the multiple error types of data transformations.</p> Source code in <code>multimno/core/constants/error_types.py</code> <pre><code>class ErrorTypes:\n    \"\"\"\n    Class that enumerates the multiple error types of data transformations.\n    \"\"\"\n\n    NO_ERROR = 0\n    NULL_VALUE = 1\n    OUT_OF_RANGE = 2\n    UNSUPPORTED_TYPE = 3\n    CANNOT_PARSE = 4\n    NO_MNO_INFO = 5\n    NO_LOCATION_INFO = 6\n    DUPLICATED = 7\n\n    # This shows the possible error types that can happen in syntactic event cleaning\n    # This is used for creating the quality metrics data object\n    event_syntactic_cleaning_possible_errors = [0, 1, 2, 3, 4, 5, 6, 7]\n</code></pre>"},{"location":"reference/core/constants/error_types/#core.constants.error_types.NetworkErrorType","title":"<code>NetworkErrorType</code>","text":"<p>Class that enumerates the multiple error types present in network topology data.</p> Source code in <code>multimno/core/constants/error_types.py</code> <pre><code>class NetworkErrorType:\n    \"\"\"\n    Class that enumerates the multiple error types present in network topology data.\n    \"\"\"\n\n    NO_ERROR = 0\n    NULL_VALUE = 1\n    OUT_OF_RANGE = 2\n    UNSUPPORTED_TYPE = 3\n    CANNOT_PARSE = 4\n    INITIAL_ROWS = 100\n    FINAL_ROWS = 101\n</code></pre>"},{"location":"reference/core/constants/error_types/#core.constants.error_types.SemanticErrorType","title":"<code>SemanticErrorType</code>","text":"<p>Class that enumerates the multiple error types associated to event semantic checks.</p> Source code in <code>multimno/core/constants/error_types.py</code> <pre><code>class SemanticErrorType:\n    \"\"\"\n    Class that enumerates the multiple error types associated to event semantic checks.\n    \"\"\"\n\n    NO_ERROR = 0\n    CELL_ID_NON_EXISTENT = 1\n    CELL_ID_NOT_VALID = 2\n    INCORRECT_EVENT_LOCATION = 3\n    SUSPICIOUS_EVENT_LOCATION = 4\n    DIFFERENT_LOCATION_DUPLICATE = 5\n</code></pre>"},{"location":"reference/core/constants/measure_definitions/","title":"measure_definitions","text":""},{"location":"reference/core/constants/network_default_thresholds/","title":"network_default_thresholds","text":"<p>Contains the default threshold values used by the Network Syntactic Quality Warnings</p>"},{"location":"reference/core/constants/period_names/","title":"period_names","text":"<p>List of names of the sub-daily periods/time intervals, sub-monthly periods/day types, and sub-yearly/seasons used in the Permanence Score components.</p>"},{"location":"reference/core/constants/period_names/#core.constants.period_names.PeriodBase","title":"<code>PeriodBase</code>","text":"Source code in <code>multimno/core/constants/period_names.py</code> <pre><code>class PeriodBase:\n    @classmethod\n    def is_valid_type(cls, value: str) -&gt; bool:\n        \"\"\"\n        Check if the given value is a valid type.\n\n        Args:\n            value: String to check\n\n        Returns:\n            bool: True if value is a valid type, False otherwise\n        \"\"\"\n        return value in {\n            getattr(cls, attr) for attr in dir(cls) if not attr.startswith(\"_\") and isinstance(getattr(cls, attr), str)\n        }\n\n    @classmethod\n    def values(cls) -&gt; set[str]:\n        \"\"\"\n        Get all string values defined in the class.\n\n        Returns:\n            set[str]: Set of all string values\n        \"\"\"\n        return {\n            getattr(cls, attr) for attr in dir(cls) if not attr.startswith(\"_\") and isinstance(getattr(cls, attr), str)\n        }\n</code></pre>"},{"location":"reference/core/constants/period_names/#core.constants.period_names.PeriodBase.is_valid_type","title":"<code>is_valid_type(value)</code>  <code>classmethod</code>","text":"<p>Check if the given value is a valid type.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>String to check</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if value is a valid type, False otherwise</p> Source code in <code>multimno/core/constants/period_names.py</code> <pre><code>@classmethod\ndef is_valid_type(cls, value: str) -&gt; bool:\n    \"\"\"\n    Check if the given value is a valid type.\n\n    Args:\n        value: String to check\n\n    Returns:\n        bool: True if value is a valid type, False otherwise\n    \"\"\"\n    return value in {\n        getattr(cls, attr) for attr in dir(cls) if not attr.startswith(\"_\") and isinstance(getattr(cls, attr), str)\n    }\n</code></pre>"},{"location":"reference/core/constants/period_names/#core.constants.period_names.PeriodBase.values","title":"<code>values()</code>  <code>classmethod</code>","text":"<p>Get all string values defined in the class.</p> <p>Returns:</p> Type Description <code>set[str]</code> <p>set[str]: Set of all string values</p> Source code in <code>multimno/core/constants/period_names.py</code> <pre><code>@classmethod\ndef values(cls) -&gt; set[str]:\n    \"\"\"\n    Get all string values defined in the class.\n\n    Returns:\n        set[str]: Set of all string values\n    \"\"\"\n    return {\n        getattr(cls, attr) for attr in dir(cls) if not attr.startswith(\"_\") and isinstance(getattr(cls, attr), str)\n    }\n</code></pre>"},{"location":"reference/core/constants/period_names/#core.constants.period_names.PeriodCombinations","title":"<code>PeriodCombinations</code>","text":"<p>All possible period combinations as flat lists.</p> Source code in <code>multimno/core/constants/period_names.py</code> <pre><code>class PeriodCombinations:\n    \"\"\"All possible period combinations as flat lists.\"\"\"\n\n    # Basic combinations\n    ALL_PERIODS = (Seasons.ALL, DayTypes.ALL, TimeIntervals.ALL)\n    NIGHT_TIME_ALL = (Seasons.ALL, DayTypes.ALL, TimeIntervals.NIGHT_TIME)\n    WORKING_HOURS_WORKDAYS = (Seasons.ALL, DayTypes.WORKDAYS, TimeIntervals.WORKING_HOURS)\n</code></pre>"},{"location":"reference/core/constants/reserved_dataset_ids/","title":"reserved_dataset_ids","text":"<p>Reserved zoning dataset IDs. At this moment they refer to the INSPIRE 100m and 1km grids, which receive a special treatment as their geometries are not needed in order to map the reference grid tiles (the INSPIRE 100m grid) to them. Also contains the ABROAD tag for marking outbound data.</p>"},{"location":"reference/core/constants/reserved_dataset_ids/#core.constants.reserved_dataset_ids.ReservedDatasetIDs","title":"<code>ReservedDatasetIDs</code>","text":"<p>Class that enumerates reserved dataset IDs.</p> Source code in <code>multimno/core/constants/reserved_dataset_ids.py</code> <pre><code>class ReservedDatasetIDs:\n    \"\"\"\n    Class that enumerates reserved dataset IDs.\n    \"\"\"\n\n    INSPIRE_1km = \"INSPIRE_1km\"\n    INSPIRE_100m = \"INSPIRE_100m\"\n    ABROAD = \"ABROAD\"  # Currently only used to mark outbound data (which also does not have a grid).\n\n    def __contains__(self, value):\n        if self.INSPIRE_100m == value:\n            return True\n        if self.INSPIRE_1km == value:\n            return True\n        return False\n\n    @classmethod\n    def get_resolution_m(cls, value):\n        if cls.INSPIRE_100m == value:\n            return 100\n        if cls.INSPIRE_1km == value:\n            return 1000\n        return None\n</code></pre>"},{"location":"reference/core/constants/semantic_qw_default_thresholds/","title":"semantic_qw_default_thresholds","text":"<p>Contains the default threshold values used by the Event Device Semantic Quality Warnings</p>"},{"location":"reference/core/constants/spatial/","title":"spatial","text":""},{"location":"reference/core/constants/transformations/","title":"transformations","text":"<p>Data transformations types modukle</p>"},{"location":"reference/core/constants/transformations/#core.constants.transformations.Transformations","title":"<code>Transformations</code>","text":"<p>Class that enumerates the multiple data transformations types.</p> Source code in <code>multimno/core/constants/transformations.py</code> <pre><code>class Transformations:\n    \"\"\"\n    Class that enumerates the multiple data transformations types.\n    \"\"\"\n\n    converted_timestamp = 1\n    other_conversion = 2\n    no_transformation = 9\n\n    event_syntactic_cleaning_possible_transformations = [1, 2, 9]\n</code></pre>"},{"location":"reference/core/constants/warnings/","title":"warnings","text":""},{"location":"reference/core/data_objects/","title":"data_objects","text":""},{"location":"reference/core/data_objects/data_object/","title":"data_object","text":"<p>Module that defines the data object abstract classes</p>"},{"location":"reference/core/data_objects/data_object/#core.data_objects.data_object.DataObject","title":"<code>DataObject</code>","text":"<p>Abstract class that models a DataObject. It defines its data schema including the attributes that compose it.</p> Source code in <code>multimno/core/data_objects/data_object.py</code> <pre><code>class DataObject(metaclass=ABCMeta):\n    \"\"\"\n    Abstract class that models a DataObject. It defines its data schema including the attributes that compose it.\n    \"\"\"\n\n    ID: str = None\n    SCHEMA: StructType = None\n\n    def __init__(self, spark: SparkSession) -&gt; None:\n        self.df: DataFrame = None\n        self.spark: SparkSession = spark\n        self.interface: IOInterface = None\n\n    def read(self, *args, **kwargs):\n        \"\"\"\n        Method that performs the read operation of the data object dataframe through an IOInterface.\n        \"\"\"\n        self.df = self.interface.read_from_interface(*args, **kwargs)\n        return self\n\n    def write(self, *args, **kwargs):\n        \"\"\"\n        Method that performs the write operation of the data object dataframe through an IOInterface.\n        \"\"\"\n        self.interface.write_from_interface(self.df, *args, **kwargs)\n\n    def cast_to_schema(self):\n        columns = {field.name: F.col(field.name).cast(field.dataType) for field in self.SCHEMA.fields}\n        self.df = self.df.withColumns(columns)\n</code></pre>"},{"location":"reference/core/data_objects/data_object/#core.data_objects.data_object.DataObject.read","title":"<code>read(*args, **kwargs)</code>","text":"<p>Method that performs the read operation of the data object dataframe through an IOInterface.</p> Source code in <code>multimno/core/data_objects/data_object.py</code> <pre><code>def read(self, *args, **kwargs):\n    \"\"\"\n    Method that performs the read operation of the data object dataframe through an IOInterface.\n    \"\"\"\n    self.df = self.interface.read_from_interface(*args, **kwargs)\n    return self\n</code></pre>"},{"location":"reference/core/data_objects/data_object/#core.data_objects.data_object.DataObject.write","title":"<code>write(*args, **kwargs)</code>","text":"<p>Method that performs the write operation of the data object dataframe through an IOInterface.</p> Source code in <code>multimno/core/data_objects/data_object.py</code> <pre><code>def write(self, *args, **kwargs):\n    \"\"\"\n    Method that performs the write operation of the data object dataframe through an IOInterface.\n    \"\"\"\n    self.interface.write_from_interface(self.df, *args, **kwargs)\n</code></pre>"},{"location":"reference/core/data_objects/data_object/#core.data_objects.data_object.GeoParquetDataObject","title":"<code>GeoParquetDataObject</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models a DataObject that will use a ParquetInterface for IO operations. It inherits the PathDataObject abstract class.</p> Source code in <code>multimno/core/data_objects/data_object.py</code> <pre><code>class GeoParquetDataObject(PathDataObject):\n    \"\"\"\n    Class that models a DataObject that will use a ParquetInterface for IO operations.\n    It inherits the PathDataObject abstract class.\n    \"\"\"\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        default_path: str,\n        default_partition_columns: List[str] = None,\n        default_mode: str = SPARK_WRITING_MODES.APPEND,\n        default_crs: int = INSPIRE_GRID_EPSG,\n        set_crs: bool = True,\n    ) -&gt; None:\n        super().__init__(spark, default_path, default_partition_columns, default_mode)\n        self.interface: PathInterface = GeoParquetInterface()\n        self.default_crs = default_crs\n        self.set_crs = set_crs\n\n    def read(self):\n        self.df = self.interface.read_from_interface(self.spark, self.default_path, self.SCHEMA)\n\n        if self.set_crs:\n            self.df = self.df.withColumn(\n                ColNames.geometry, STF.ST_SetSRID((ColNames.geometry), F.lit(self.default_crs))\n            )\n\n        return self\n</code></pre>"},{"location":"reference/core/data_objects/data_object/#core.data_objects.data_object.ParquetDataObject","title":"<code>ParquetDataObject</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models a DataObject that will use a ParquetInterface for IO operations. It inherits the PathDataObject abstract class.</p> Source code in <code>multimno/core/data_objects/data_object.py</code> <pre><code>class ParquetDataObject(PathDataObject):\n    \"\"\"\n    Class that models a DataObject that will use a ParquetInterface for IO operations.\n    It inherits the PathDataObject abstract class.\n    \"\"\"\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        default_path: str,\n        default_partition_columns: List[str] = None,\n        default_mode: str = SPARK_WRITING_MODES.APPEND,\n    ) -&gt; None:\n        super().__init__(spark, default_path, default_partition_columns, default_mode)\n        self.interface: PathInterface = ParquetInterface()\n</code></pre>"},{"location":"reference/core/data_objects/data_object/#core.data_objects.data_object.PathDataObject","title":"<code>PathDataObject</code>","text":"<p>               Bases: <code>DataObject</code></p> <p>Abstract Class that models DataObjects that will use a PathInterface for IO operations. It inherits the DataObject abstract class.</p> Source code in <code>multimno/core/data_objects/data_object.py</code> <pre><code>class PathDataObject(DataObject, metaclass=ABCMeta):\n    \"\"\"Abstract Class that models DataObjects that will use a PathInterface for IO operations.\n    It inherits the DataObject abstract class.\n    \"\"\"\n\n    ID = ...\n    SCHEMA = ...\n    PARTITION_COLUMNS = ...\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        default_path: str,\n        default_partition_columns: List[str] = None,\n        default_mode: str = SPARK_WRITING_MODES.APPEND,\n    ) -&gt; None:\n        super().__init__(spark)\n        self.interface: PathInterface = None\n        self.default_path: str = default_path\n        if default_partition_columns is None:\n            default_partition_columns = self.PARTITION_COLUMNS\n        self.default_partition_columns: List[str] = default_partition_columns\n        self.default_mode: str = default_mode\n\n    def read(self, *args, path: str = None, **kwargs):\n        if path is None:\n            path = self.default_path\n        self.df = self.interface.read_from_interface(self.spark, path, self.SCHEMA)\n\n        return self\n\n    def write(self, *args, path: str = None, partition_columns: list[str] = None, mode: str = None, **kwargs):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.default_partition_columns\n        if mode is None:\n            mode = self.default_mode\n\n        self.interface.write_from_interface(self.df, path=path, partition_columns=partition_columns, mode=mode)\n\n    def get_size(self) -&gt; int:\n        \"\"\"\n        Returns the size of the data object in bytes.\n        \"\"\"\n        files = self.df.inputFiles()\n\n        if len(files) == 0:\n            return 0\n\n        conf = self.spark._jsc.hadoopConfiguration()\n        # need to get proper URI prefix for the file system\n        uri = self.spark._jvm.java.net.URI.create(files[0])\n        fs = self.spark._jvm.org.apache.hadoop.fs.FileSystem.get(uri, conf)\n        total_size = 0\n\n        for file in files:\n            total_size += fs.getFileStatus(self.spark._jvm.org.apache.hadoop.fs.Path(file)).getLen()\n\n        return total_size\n\n    def get_num_files(self) -&gt; int:\n        \"\"\"\n        Returns the number of files of the data object.\n        \"\"\"\n        return len(self.df.inputFiles())\n\n    def get_top_rows(self, n: int, truncate: int = 20) -&gt; str:\n        \"\"\"\n        Returns string with top n rows. Same as df.show.\n        \"\"\"\n        return self.df._jdf.showString(n, truncate, False)\n</code></pre>"},{"location":"reference/core/data_objects/data_object/#core.data_objects.data_object.PathDataObject.get_num_files","title":"<code>get_num_files()</code>","text":"<p>Returns the number of files of the data object.</p> Source code in <code>multimno/core/data_objects/data_object.py</code> <pre><code>def get_num_files(self) -&gt; int:\n    \"\"\"\n    Returns the number of files of the data object.\n    \"\"\"\n    return len(self.df.inputFiles())\n</code></pre>"},{"location":"reference/core/data_objects/data_object/#core.data_objects.data_object.PathDataObject.get_size","title":"<code>get_size()</code>","text":"<p>Returns the size of the data object in bytes.</p> Source code in <code>multimno/core/data_objects/data_object.py</code> <pre><code>def get_size(self) -&gt; int:\n    \"\"\"\n    Returns the size of the data object in bytes.\n    \"\"\"\n    files = self.df.inputFiles()\n\n    if len(files) == 0:\n        return 0\n\n    conf = self.spark._jsc.hadoopConfiguration()\n    # need to get proper URI prefix for the file system\n    uri = self.spark._jvm.java.net.URI.create(files[0])\n    fs = self.spark._jvm.org.apache.hadoop.fs.FileSystem.get(uri, conf)\n    total_size = 0\n\n    for file in files:\n        total_size += fs.getFileStatus(self.spark._jvm.org.apache.hadoop.fs.Path(file)).getLen()\n\n    return total_size\n</code></pre>"},{"location":"reference/core/data_objects/data_object/#core.data_objects.data_object.PathDataObject.get_top_rows","title":"<code>get_top_rows(n, truncate=20)</code>","text":"<p>Returns string with top n rows. Same as df.show.</p> Source code in <code>multimno/core/data_objects/data_object.py</code> <pre><code>def get_top_rows(self, n: int, truncate: int = 20) -&gt; str:\n    \"\"\"\n    Returns string with top n rows. Same as df.show.\n    \"\"\"\n    return self.df._jdf.showString(n, truncate, False)\n</code></pre>"},{"location":"reference/core/data_objects/bronze/","title":"bronze","text":""},{"location":"reference/core/data_objects/bronze/bronze_admin_units_data_object/","title":"bronze_admin_units_data_object","text":""},{"location":"reference/core/data_objects/bronze/bronze_admin_units_data_object/#core.data_objects.bronze.bronze_admin_units_data_object.BronzeAdminUnitsDataObject","title":"<code>BronzeAdminUnitsDataObject</code>","text":"<p>               Bases: <code>GeoParquetDataObject</code></p> <p>Class that models country polygons spatial data.</p> Source code in <code>multimno/core/data_objects/bronze/bronze_admin_units_data_object.py</code> <pre><code>class BronzeAdminUnitsDataObject(GeoParquetDataObject):\n    \"\"\"\n    Class that models country polygons spatial data.\n    \"\"\"\n\n    ID = \"AdminUnitsDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.zone_id, StringType(), nullable=False),\n            StructField(ColNames.name, StringType(), nullable=True),\n            StructField(ColNames.level, ShortType(), nullable=True),\n            StructField(ColNames.parent_id, StringType(), nullable=True),\n            StructField(ColNames.iso2, StringType(), nullable=False),\n            StructField(ColNames.geometry, GeometryType(), nullable=False),\n            # partition columns\n            StructField(ColNames.dataset_id, StringType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.dataset_id, ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/bronze/bronze_buildings_data_object/","title":"bronze_buildings_data_object","text":""},{"location":"reference/core/data_objects/bronze/bronze_buildings_data_object/#core.data_objects.bronze.bronze_buildings_data_object.BronzeBuildingsDataObject","title":"<code>BronzeBuildingsDataObject</code>","text":"<p>               Bases: <code>GeoParquetDataObject</code></p> <p>Class that models the transportation network spatial data.</p> Source code in <code>multimno/core/data_objects/bronze/bronze_buildings_data_object.py</code> <pre><code>class BronzeBuildingsDataObject(GeoParquetDataObject):\n    \"\"\"\n    Class that models the transportation network spatial data.\n    \"\"\"\n\n    ID = \"BronzeBuildingsDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.category, StringType(), nullable=False),\n            StructField(ColNames.geometry, GeometryType(), nullable=False),\n            # partition columns\n            StructField(ColNames.quadkey, StringType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.quadkey]\n</code></pre>"},{"location":"reference/core/data_objects/bronze/bronze_countries_data_object/","title":"bronze_countries_data_object","text":""},{"location":"reference/core/data_objects/bronze/bronze_countries_data_object/#core.data_objects.bronze.bronze_countries_data_object.BronzeCountriesDataObject","title":"<code>BronzeCountriesDataObject</code>","text":"<p>               Bases: <code>GeoParquetDataObject</code></p> <p>Class that models country polygons spatial data.</p> Source code in <code>multimno/core/data_objects/bronze/bronze_countries_data_object.py</code> <pre><code>class BronzeCountriesDataObject(GeoParquetDataObject):\n    \"\"\"\n    Class that models country polygons spatial data.\n    \"\"\"\n\n    ID = \"BronzeCountriesDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.iso2, StringType(), nullable=False),\n            StructField(ColNames.name, StringType(), nullable=False),\n            StructField(ColNames.geometry, GeometryType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = []\n</code></pre>"},{"location":"reference/core/data_objects/bronze/bronze_event_data_object/","title":"bronze_event_data_object","text":"<p>Bronze MNO Event data module</p>"},{"location":"reference/core/data_objects/bronze/bronze_event_data_object/#core.data_objects.bronze.bronze_event_data_object.BronzeEventDataObject","title":"<code>BronzeEventDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the RAW MNO Event data.</p> Source code in <code>multimno/core/data_objects/bronze/bronze_event_data_object.py</code> <pre><code>class BronzeEventDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models the RAW MNO Event data.\n    \"\"\"\n\n    ID = \"BronzeEventDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=True),\n            StructField(ColNames.timestamp, StringType(), nullable=True),\n            StructField(ColNames.mcc, IntegerType(), nullable=True),\n            StructField(ColNames.mnc, StringType(), nullable=True),\n            StructField(ColNames.plmn, IntegerType(), nullable=True),\n            StructField(ColNames.cell_id, StringType(), nullable=True),\n            StructField(ColNames.latitude, FloatType(), nullable=True),\n            StructField(ColNames.longitude, FloatType(), nullable=True),\n            StructField(ColNames.loc_error, FloatType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=True),\n            StructField(ColNames.month, ByteType(), nullable=True),\n            StructField(ColNames.day, ByteType(), nullable=True),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/bronze/bronze_geographic_zones_data_object/","title":"bronze_geographic_zones_data_object","text":""},{"location":"reference/core/data_objects/bronze/bronze_geographic_zones_data_object/#core.data_objects.bronze.bronze_geographic_zones_data_object.BronzeGeographicZonesDataObject","title":"<code>BronzeGeographicZonesDataObject</code>","text":"<p>               Bases: <code>GeoParquetDataObject</code></p> <p>Class that models country polygons spatial data.</p> Source code in <code>multimno/core/data_objects/bronze/bronze_geographic_zones_data_object.py</code> <pre><code>class BronzeGeographicZonesDataObject(GeoParquetDataObject):\n    \"\"\"\n    Class that models country polygons spatial data.\n    \"\"\"\n\n    ID = \"GeographicZonesDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.zone_id, StringType(), nullable=False),\n            StructField(ColNames.name, StringType(), nullable=True),\n            StructField(ColNames.level, ShortType(), nullable=True),\n            StructField(ColNames.parent_id, StringType(), nullable=True),\n            StructField(ColNames.iso2, StringType(), nullable=False),\n            StructField(ColNames.geometry, GeometryType(), nullable=False),\n            # partition columns\n            StructField(ColNames.dataset_id, StringType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.dataset_id, ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/bronze/bronze_holiday_calendar_data_object/","title":"bronze_holiday_calendar_data_object","text":"<p>Bronze Calendar Information Data Object Contains the national holidays of each country</p>"},{"location":"reference/core/data_objects/bronze/bronze_holiday_calendar_data_object/#core.data_objects.bronze.bronze_holiday_calendar_data_object.BronzeHolidayCalendarDataObject","title":"<code>BronzeHolidayCalendarDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the Calendar information regarding national holidays and regular days.</p> Source code in <code>multimno/core/data_objects/bronze/bronze_holiday_calendar_data_object.py</code> <pre><code>class BronzeHolidayCalendarDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models the Calendar information regarding national holidays\n    and regular days.\n    \"\"\"\n\n    ID = \"BronzeHolidayCalendarInfoDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.iso2, StringType(), nullable=False),\n            StructField(ColNames.date, DateType(), nullable=False),\n            StructField(ColNames.name, StringType(), nullable=False),\n        ]\n    )\n    PARTITION_COLUMNS = []\n</code></pre>"},{"location":"reference/core/data_objects/bronze/bronze_inbound_estimation_factors_data_object/","title":"bronze_inbound_estimation_factors_data_object","text":""},{"location":"reference/core/data_objects/bronze/bronze_inbound_estimation_factors_data_object/#core.data_objects.bronze.bronze_inbound_estimation_factors_data_object.BronzeInboundEstimationFactorsDataObject","title":"<code>BronzeInboundEstimationFactorsDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that contains the deduplication and mno-to-target-population factors for usage on the estimation of inbound tourism data.</p> Source code in <code>multimno/core/data_objects/bronze/bronze_inbound_estimation_factors_data_object.py</code> <pre><code>class BronzeInboundEstimationFactorsDataObject(ParquetDataObject):\n    \"\"\"\n    Class that contains the deduplication and mno-to-target-population factors for usage on the estimation\n    of inbound tourism data.\n    \"\"\"\n\n    ID = \"BronzeInboundEstimationFactorDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.iso2, StringType(), nullable=False),\n            StructField(ColNames.deduplication_factor, FloatType(), nullable=True),\n            StructField(ColNames.mno_to_target_population_factor, FloatType(), nullable=True),\n        ]\n    )\n\n    PARTITION_COLUMNS = []\n</code></pre>"},{"location":"reference/core/data_objects/bronze/bronze_landuse_data_object/","title":"bronze_landuse_data_object","text":""},{"location":"reference/core/data_objects/bronze/bronze_landuse_data_object/#core.data_objects.bronze.bronze_landuse_data_object.BronzeLanduseDataObject","title":"<code>BronzeLanduseDataObject</code>","text":"<p>               Bases: <code>GeoParquetDataObject</code></p> <p>Class that models landuse spatial data.</p> Source code in <code>multimno/core/data_objects/bronze/bronze_landuse_data_object.py</code> <pre><code>class BronzeLanduseDataObject(GeoParquetDataObject):\n    \"\"\"\n    Class that models landuse spatial data.\n    \"\"\"\n\n    ID = \"BronzeLanduseDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.category, StringType(), nullable=False),\n            StructField(ColNames.geometry, GeometryType(), nullable=False),\n            # partition columns\n            StructField(ColNames.quadkey, StringType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.quadkey]\n</code></pre>"},{"location":"reference/core/data_objects/bronze/bronze_mcc_iso_tz_map/","title":"bronze_mcc_iso_tz_map","text":""},{"location":"reference/core/data_objects/bronze/bronze_mcc_iso_tz_map/#core.data_objects.bronze.bronze_mcc_iso_tz_map.BronzeMccIsoTzMap","title":"<code>BronzeMccIsoTzMap</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models country polygons spatial data.</p> Source code in <code>multimno/core/data_objects/bronze/bronze_mcc_iso_tz_map.py</code> <pre><code>class BronzeMccIsoTzMap(ParquetDataObject):\n    \"\"\"\n    Class that models country polygons spatial data.\n    \"\"\"\n\n    ID = \"BronzeMccIsoTzMapDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.mcc, IntegerType(), nullable=False),\n            StructField(ColNames.name, StringType(), nullable=False),\n            StructField(ColNames.iso2, StringType(), nullable=False),\n            StructField(ColNames.iso3, StringType(), nullable=True),\n            StructField(ColNames.eurostat_code, StringType(), nullable=True),\n            StructField(ColNames.latitude, FloatType(), nullable=True),\n            StructField(ColNames.longitude, FloatType(), nullable=True),\n            StructField(ColNames.timezone, StringType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = []\n</code></pre>"},{"location":"reference/core/data_objects/bronze/bronze_network_physical_data_object/","title":"bronze_network_physical_data_object","text":"<p>Bronze MNO Network Topology Data module</p> <p>Currently, only considers the \"Cell Locations with Physical Properties\" type</p>"},{"location":"reference/core/data_objects/bronze/bronze_network_physical_data_object/#core.data_objects.bronze.bronze_network_physical_data_object.BronzeNetworkDataObject","title":"<code>BronzeNetworkDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the RAW MNO Network Topology Data, based on the physical properties of the cells.</p> Source code in <code>multimno/core/data_objects/bronze/bronze_network_physical_data_object.py</code> <pre><code>class BronzeNetworkDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models the RAW MNO Network Topology Data, based on the physical\n    properties of the cells.\n    \"\"\"\n\n    ID = \"BronzeNetworkDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.cell_id, StringType(), nullable=True),\n            StructField(ColNames.latitude, FloatType(), nullable=True),\n            StructField(ColNames.longitude, FloatType(), nullable=True),\n            StructField(ColNames.altitude, FloatType(), nullable=True),\n            StructField(ColNames.antenna_height, FloatType(), nullable=True),\n            StructField(ColNames.directionality, IntegerType(), nullable=True),\n            StructField(ColNames.azimuth_angle, FloatType(), nullable=True),\n            StructField(ColNames.elevation_angle, FloatType(), nullable=True),\n            StructField(ColNames.horizontal_beam_width, FloatType(), nullable=True),\n            StructField(ColNames.vertical_beam_width, FloatType(), nullable=True),\n            StructField(ColNames.power, FloatType(), nullable=True),\n            StructField(ColNames.range, FloatType(), nullable=True),\n            StructField(ColNames.frequency, IntegerType(), nullable=True),\n            StructField(ColNames.technology, StringType(), nullable=True),\n            StructField(ColNames.valid_date_start, StringType(), nullable=True),\n            StructField(ColNames.valid_date_end, StringType(), nullable=True),\n            StructField(ColNames.cell_type, StringType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=True),\n            StructField(ColNames.month, ByteType(), nullable=True),\n            StructField(ColNames.day, ByteType(), nullable=True),\n        ]\n    )\n    MANDATORY_COLUMNS = [ColNames.cell_id, ColNames.latitude, ColNames.longitude]\n\n    OPTIONAL_COLUMNS = [\n        ColNames.altitude,\n        ColNames.antenna_height,\n        ColNames.directionality,\n        ColNames.azimuth_angle,\n        ColNames.elevation_angle,\n        ColNames.horizontal_beam_width,\n        ColNames.vertical_beam_width,\n        ColNames.power,\n        ColNames.range,\n        ColNames.frequency,\n        ColNames.technology,\n        ColNames.valid_date_start,\n        ColNames.valid_date_end,\n        ColNames.cell_type,\n    ]\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/bronze/bronze_synthetic_diaries_data_object/","title":"bronze_synthetic_diaries_data_object","text":"<p>Bronze Synthetic Diaries Data module</p>"},{"location":"reference/core/data_objects/bronze/bronze_synthetic_diaries_data_object/#core.data_objects.bronze.bronze_synthetic_diaries_data_object.BronzeSyntheticDiariesDataObject","title":"<code>BronzeSyntheticDiariesDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models synthetically-generated agents activity-trip diaries.</p> <pre><code>    ''''''\n</code></pre> Source code in <code>multimno/core/data_objects/bronze/bronze_synthetic_diaries_data_object.py</code> <pre><code>class BronzeSyntheticDiariesDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models synthetically-generated agents activity-trip diaries.\n\n            ''''''\n\n    \"\"\"\n\n    ID = \"BronzeSyntheticDiariesDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.activity_type, StringType(), nullable=True),\n            StructField(ColNames.stay_type, StringType(), nullable=True),\n            StructField(ColNames.longitude, FloatType(), nullable=True),\n            StructField(ColNames.latitude, FloatType(), nullable=True),\n            StructField(ColNames.initial_timestamp, TimestampType(), nullable=True),\n            StructField(ColNames.final_timestamp, TimestampType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/bronze/bronze_transportation_data_object/","title":"bronze_transportation_data_object","text":""},{"location":"reference/core/data_objects/bronze/bronze_transportation_data_object/#core.data_objects.bronze.bronze_transportation_data_object.BronzeTransportationDataObject","title":"<code>BronzeTransportationDataObject</code>","text":"<p>               Bases: <code>GeoParquetDataObject</code></p> <p>Class that models the transportation network spatial data.</p> Source code in <code>multimno/core/data_objects/bronze/bronze_transportation_data_object.py</code> <pre><code>class BronzeTransportationDataObject(GeoParquetDataObject):\n    \"\"\"\n    Class that models the transportation network spatial data.\n    \"\"\"\n\n    ID = \"BronzeTransportationDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.category, StringType(), nullable=False),\n            StructField(ColNames.geometry, GeometryType(), nullable=False),\n            # partition columns\n            StructField(ColNames.quadkey, StringType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.quadkey]\n</code></pre>"},{"location":"reference/core/data_objects/landing/","title":"landing","text":""},{"location":"reference/core/data_objects/landing/landing_geoparquet_data_object/","title":"landing_geoparquet_data_object","text":""},{"location":"reference/core/data_objects/landing/landing_geoparquet_data_object/#core.data_objects.landing.landing_geoparquet_data_object.LandingGeoParquetDataObject","title":"<code>LandingGeoParquetDataObject</code>","text":"<p>               Bases: <code>GeoParquetDataObject</code></p> <p>Class that models input geospatial data.</p> Source code in <code>multimno/core/data_objects/landing/landing_geoparquet_data_object.py</code> <pre><code>class LandingGeoParquetDataObject(GeoParquetDataObject):\n    \"\"\"\n    Class that models input geospatial data.\n    \"\"\"\n\n    ID = \"LandingGeoParquetDO\"\n    SCHEMA = None\n</code></pre>"},{"location":"reference/core/data_objects/landing/landing_http_geojson_data_object/","title":"landing_http_geojson_data_object","text":""},{"location":"reference/core/data_objects/landing/landing_http_geojson_data_object/#core.data_objects.landing.landing_http_geojson_data_object.LandingHttpGeoJsonDataObject","title":"<code>LandingHttpGeoJsonDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models input geospatial data in geojson format.</p> Source code in <code>multimno/core/data_objects/landing/landing_http_geojson_data_object.py</code> <pre><code>class LandingHttpGeoJsonDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models input geospatial data in geojson format.\n    \"\"\"\n\n    ID = \"LandingGeoJsonDO\"\n\n    def __init__(self, spark: SparkSession, url: str, timeout: int, max_retries: int) -&gt; None:\n\n        super().__init__(spark, url)\n        self.interface: HttpGeoJsonInterface = HttpGeoJsonInterface()\n        self.default_path = url\n        self.timeout = timeout\n        self.max_retries = max_retries\n\n    def read(self):\n\n        self.df = self.interface.read_from_interface(self.spark, self.default_path, self.timeout, self.max_retries)\n</code></pre>"},{"location":"reference/core/data_objects/silver/","title":"silver","text":""},{"location":"reference/core/data_objects/silver/event_cache_data_object/","title":"event_cache_data_object","text":"<p>Silver MNO Event data module with flags computed in Semantic Checks</p>"},{"location":"reference/core/data_objects/silver/event_cache_data_object/#core.data_objects.silver.event_cache_data_object.EventCacheDataObject","title":"<code>EventCacheDataObject</code>","text":"<p>               Bases: <code>SilverEventFlaggedDataObject</code></p> <p>Class that models the cleaned MNO Event data, with flags computed in the semantic checks module.</p> Source code in <code>multimno/core/data_objects/silver/event_cache_data_object.py</code> <pre><code>class EventCacheDataObject(SilverEventFlaggedDataObject):\n    \"\"\"\n    Class that models the cleaned MNO Event data, with flags computed\n    in the semantic checks module.\n    \"\"\"\n\n    ID = \"EventCacheDO\"\n\n    # SCHEMA and partition columns depend on the semantic event data\n    SCHEMA = StructType(\n        SilverEventFlaggedDataObject.SCHEMA.fields\n        + [StructField(ColNames.is_last_event, BooleanType(), nullable=False)]\n    )\n\n    PARTITION_COLUMNS = SilverEventFlaggedDataObject.PARTITION_COLUMNS + [ColNames.is_last_event]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_aggregated_usual_environments_data_object/","title":"silver_aggregated_usual_environments_data_object","text":"<p>Silver Aggregated Usual Environments data object module</p>"},{"location":"reference/core/data_objects/silver/silver_aggregated_usual_environments_data_object/#core.data_objects.silver.silver_aggregated_usual_environments_data_object.SilverAggregatedUsualEnvironmentsDataObject","title":"<code>SilverAggregatedUsualEnvironmentsDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the Aggregated Usual Environment data object.</p> Source code in <code>multimno/core/data_objects/silver/silver_aggregated_usual_environments_data_object.py</code> <pre><code>class SilverAggregatedUsualEnvironmentsDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models the Aggregated Usual Environment data object.\n    \"\"\"\n\n    ID = \"SilverAggregatedUsualEnvironmentsDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.grid_id, IntegerType(), nullable=False),\n            StructField(ColNames.weighted_device_count, FloatType(), nullable=False),\n            # partition columns\n            StructField(ColNames.label, StringType(), nullable=False),\n            StructField(ColNames.start_date, DateType(), nullable=False),\n            StructField(ColNames.end_date, DateType(), nullable=False),\n            StructField(ColNames.season, StringType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.label, ColNames.start_date, ColNames.end_date, ColNames.season]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_aggregated_usual_environments_zones_data_object/","title":"silver_aggregated_usual_environments_zones_data_object","text":"<p>Silver usual environments estimatation per zone data object</p>"},{"location":"reference/core/data_objects/silver/silver_aggregated_usual_environments_zones_data_object/#core.data_objects.silver.silver_aggregated_usual_environments_zones_data_object.SilverAggregatedUsualEnvironmentsZonesDataObject","title":"<code>SilverAggregatedUsualEnvironmentsZonesDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Estimation of the population present at a given time at the level of some zoning system.</p> Source code in <code>multimno/core/data_objects/silver/silver_aggregated_usual_environments_zones_data_object.py</code> <pre><code>class SilverAggregatedUsualEnvironmentsZonesDataObject(ParquetDataObject):\n    \"\"\"\n    Estimation of the population present at a given time at the level of some zoning system.\n    \"\"\"\n\n    ID = \"SilverAggregatedUsualEnvironmentsZonesDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.zone_id, StringType(), nullable=False),\n            StructField(ColNames.weighted_device_count, FloatType(), nullable=False),\n            # partition columns\n            StructField(ColNames.dataset_id, StringType(), nullable=False),\n            StructField(ColNames.level, ByteType(), nullable=False),\n            StructField(ColNames.label, StringType(), nullable=False),\n            StructField(ColNames.start_date, DateType(), nullable=False),\n            StructField(ColNames.end_date, DateType(), nullable=False),\n            StructField(ColNames.season, StringType(), nullable=False),\n        ]\n    )\n\n    VALUE_COLUMNS = [ColNames.weighted_device_count]\n\n    AGGREGATION_COLUMNS = [\n        ColNames.zone_id,\n        ColNames.dataset_id,\n        ColNames.label,\n        ColNames.level,\n        ColNames.start_date,\n        ColNames.end_date,\n        ColNames.season,\n    ]\n\n    PARTITION_COLUMNS = [\n        ColNames.dataset_id,\n        ColNames.level,\n        ColNames.label,\n        ColNames.start_date,\n        ColNames.end_date,\n        ColNames.season,\n    ]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_cell_connection_probabilities_data_object/","title":"silver_cell_connection_probabilities_data_object","text":"<p>Cell connection probabilities.</p>"},{"location":"reference/core/data_objects/silver/silver_cell_connection_probabilities_data_object/#core.data_objects.silver.silver_cell_connection_probabilities_data_object.SilverCellConnectionProbabilitiesDataObject","title":"<code>SilverCellConnectionProbabilitiesDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> Source code in <code>multimno/core/data_objects/silver/silver_cell_connection_probabilities_data_object.py</code> <pre><code>class SilverCellConnectionProbabilitiesDataObject(ParquetDataObject):\n    \"\"\" \"\"\"\n\n    ID = \"SilverCellConnectionProbabilitiesDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.cell_id, StringType(), nullable=True),\n            StructField(ColNames.grid_id, IntegerType(), nullable=True),\n            StructField(ColNames.cell_connection_probability, FloatType(), nullable=True),\n            StructField(ColNames.posterior_probability, FloatType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=True),\n            StructField(ColNames.month, ByteType(), nullable=True),\n            StructField(ColNames.day, ByteType(), nullable=True),\n        ]\n    )\n\n    MANDATORY_COLUMNS = [\n        ColNames.cell_id,\n        ColNames.grid_id,\n        ColNames.valid_date_start,\n        ColNames.valid_date_end,\n        ColNames.cell_connection_probability,\n        ColNames.posterior_probability,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    ]\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_cell_distance_data_object/","title":"silver_cell_distance_data_object","text":""},{"location":"reference/core/data_objects/silver/silver_cell_distance_data_object/#core.data_objects.silver.silver_cell_distance_data_object.SilverCellDistanceDataObject","title":"<code>SilverCellDistanceDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Data Object for distances between cells. Identifies for each (cell, date) the distance to another cell.</p> Source code in <code>multimno/core/data_objects/silver/silver_cell_distance_data_object.py</code> <pre><code>class SilverCellDistanceDataObject(ParquetDataObject):\n    \"\"\"\n    Data Object for distances between cells.\n    Identifies for each (cell, date) the distance to another cell.\n    \"\"\"\n\n    ID = \"SilverCellDistanceDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.cell_id_a, StringType(), nullable=True),\n            StructField(ColNames.cell_id_b, StringType(), nullable=True),\n            StructField(ColNames.distance, FloatType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=True),\n            StructField(ColNames.month, ByteType(), nullable=True),\n            StructField(ColNames.day, ByteType(), nullable=True),\n        ]\n    )\n\n    MANDATORY_COLUMNS = [\n        ColNames.cell_id_a,\n        ColNames.cell_id_b,\n        ColNames.distance,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    ]\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_cell_footprint_data_object/","title":"silver_cell_footprint_data_object","text":""},{"location":"reference/core/data_objects/silver/silver_cell_footprint_data_object/#core.data_objects.silver.silver_cell_footprint_data_object.SilverCellFootprintDataObject","title":"<code>SilverCellFootprintDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> Source code in <code>multimno/core/data_objects/silver/silver_cell_footprint_data_object.py</code> <pre><code>class SilverCellFootprintDataObject(ParquetDataObject):\n    \"\"\" \"\"\"\n\n    ID = \"SilverCellFootprintDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.cell_id, StringType(), nullable=True),\n            StructField(ColNames.grid_id, IntegerType(), nullable=True),\n            # StructField(ColNames.valid_date_start, DateType(), nullable=True),\n            # StructField(ColNames.valid_date_end, DateType(), nullable=True),\n            StructField(ColNames.signal_dominance, FloatType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=True),\n            StructField(ColNames.month, ByteType(), nullable=True),\n            StructField(ColNames.day, ByteType(), nullable=True),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_cell_footprint_quality_metrics_data_object/","title":"silver_cell_footprint_quality_metrics_data_object","text":""},{"location":"reference/core/data_objects/silver/silver_cell_footprint_quality_metrics_data_object/#core.data_objects.silver.silver_cell_footprint_quality_metrics_data_object.SilverCellFootprintQualityMetrics","title":"<code>SilverCellFootprintQualityMetrics</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> Source code in <code>multimno/core/data_objects/silver/silver_cell_footprint_quality_metrics_data_object.py</code> <pre><code>class SilverCellFootprintQualityMetrics(ParquetDataObject):\n    \"\"\" \"\"\"\n\n    ID = \"SilverCellFootprintQualityMetricsDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.result_timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.cell_id, StringType(), nullable=False),\n            StructField(ColNames.number_of_events, LongType(), nullable=False),\n            StructField(ColNames.percentage_total_events, FloatType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_cell_intersection_groups_data_object/","title":"silver_cell_intersection_groups_data_object","text":""},{"location":"reference/core/data_objects/silver/silver_cell_intersection_groups_data_object/#core.data_objects.silver.silver_cell_intersection_groups_data_object.SilverCellIntersectionGroupsDataObject","title":"<code>SilverCellIntersectionGroupsDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Data Object for cell intersection groups. Identifies for each (cell, date) the list of other cells that are considered as intersecting (having sufficiently overlapping coverage area).</p> Source code in <code>multimno/core/data_objects/silver/silver_cell_intersection_groups_data_object.py</code> <pre><code>class SilverCellIntersectionGroupsDataObject(ParquetDataObject):\n    \"\"\"\n    Data Object for cell intersection groups.\n    Identifies for each (cell, date) the list of other cells\n    that are considered as intersecting (having sufficiently overlapping coverage area).\n    \"\"\"\n\n    ID = \"SilverCellIntersectionGroupsDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.cell_id, StringType(), nullable=True),\n            StructField(ColNames.overlapping_cell_ids, ArrayType(StringType()), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=True),\n            StructField(ColNames.month, ByteType(), nullable=True),\n            StructField(ColNames.day, ByteType(), nullable=True),\n        ]\n    )\n\n    MANDATORY_COLUMNS = [\n        ColNames.cell_id,\n        ColNames.overlapping_cell_ids,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    ]\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_cell_to_group_data_object/","title":"silver_cell_to_group_data_object","text":""},{"location":"reference/core/data_objects/silver/silver_cell_to_group_data_object/#core.data_objects.silver.silver_cell_to_group_data_object.SilverCellToGroupDataObject","title":"<code>SilverCellToGroupDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> Source code in <code>multimno/core/data_objects/silver/silver_cell_to_group_data_object.py</code> <pre><code>class SilverCellToGroupDataObject(ParquetDataObject):\n    \"\"\" \"\"\"\n\n    ID = \"SilverCellToGroupDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.cell_id, StringType(), nullable=True),\n            StructField(ColNames.group_id, StringType(), nullable=True),\n            StructField(ColNames.year, ShortType(), nullable=True),\n            StructField(ColNames.month, ByteType(), nullable=True),\n            StructField(ColNames.day, ByteType(), nullable=True),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_daily_permanence_score_data_object/","title":"silver_daily_permanence_score_data_object","text":"<p>Silver Daily Permanence Score data module</p>"},{"location":"reference/core/data_objects/silver/silver_daily_permanence_score_data_object/#core.data_objects.silver.silver_daily_permanence_score_data_object.SilverDailyPermanenceScoreDataObject","title":"<code>SilverDailyPermanenceScoreDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the Daily Permanence Score data.</p> Source code in <code>multimno/core/data_objects/silver/silver_daily_permanence_score_data_object.py</code> <pre><code>class SilverDailyPermanenceScoreDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models the Daily Permanence Score data.\n    \"\"\"\n\n    ID = \"SilverDailyPermanenceScoreDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.time_slot_initial_time, TimestampType(), nullable=False),\n            StructField(ColNames.time_slot_end_time, TimestampType(), nullable=False),\n            StructField(ColNames.dps, ArrayType(IntegerType()), nullable=False),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.user_id_modulo, IntegerType(), nullable=False),\n            StructField(ColNames.id_type, StringType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n        ColNames.user_id_modulo,\n        ColNames.id_type,\n    ]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_daily_permanence_score_quality_metrics_data_object/","title":"silver_daily_permanence_score_quality_metrics_data_object","text":"<p>Silver Daily Permanence Score Quality Metric data module</p>"},{"location":"reference/core/data_objects/silver/silver_daily_permanence_score_quality_metrics_data_object/#core.data_objects.silver.silver_daily_permanence_score_quality_metrics_data_object.SilverDailyPermanenceScoreQualityMetrics","title":"<code>SilverDailyPermanenceScoreQualityMetrics</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the Daily Permanence Score quality metrics data.</p> Source code in <code>multimno/core/data_objects/silver/silver_daily_permanence_score_quality_metrics_data_object.py</code> <pre><code>class SilverDailyPermanenceScoreQualityMetrics(ParquetDataObject):\n    \"\"\"\n    Class that models the Daily Permanence Score quality metrics data.\n    \"\"\"\n\n    ID = \"SilverDailyPermanenceScoreQualityMetricsDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.result_timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.num_unknown_devices, LongType(), nullable=False),\n            StructField(ColNames.pct_unknown_devices, FloatType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            # partition column\n            StructField(ColNames.year, ShortType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_device_activity_statistics/","title":"silver_device_activity_statistics","text":""},{"location":"reference/core/data_objects/silver/silver_device_activity_statistics/#core.data_objects.silver.silver_device_activity_statistics.SilverDeviceActivityStatistics","title":"<code>SilverDeviceActivityStatistics</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the cleaned MNO Event data.</p> Source code in <code>multimno/core/data_objects/silver/silver_device_activity_statistics.py</code> <pre><code>class SilverDeviceActivityStatistics(ParquetDataObject):\n    \"\"\"\n    Class that models the cleaned MNO Event data.\n    \"\"\"\n\n    ID = \"SilverDeviceActivityStatisticsDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.event_cnt, IntegerType(), nullable=False),\n            StructField(ColNames.unique_cell_cnt, ShortType(), nullable=False),\n            StructField(ColNames.unique_location_cnt, ShortType(), nullable=False),\n            StructField(ColNames.sum_distance_m, IntegerType(), nullable=True),\n            StructField(ColNames.unique_hour_cnt, ByteType(), nullable=False),\n            StructField(ColNames.mean_time_gap, IntegerType(), nullable=True),\n            StructField(ColNames.stdev_time_gap, FloatType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_enriched_grid_data_object/","title":"silver_enriched_grid_data_object","text":""},{"location":"reference/core/data_objects/silver/silver_enriched_grid_data_object/#core.data_objects.silver.silver_enriched_grid_data_object.SilverEnrichedGridDataObject","title":"<code>SilverEnrichedGridDataObject</code>","text":"<p>               Bases: <code>GeoParquetDataObject</code></p> <p>Class that models operational grid.</p> Source code in <code>multimno/core/data_objects/silver/silver_enriched_grid_data_object.py</code> <pre><code>class SilverEnrichedGridDataObject(GeoParquetDataObject):\n    \"\"\"\n    Class that models operational grid.\n    \"\"\"\n\n    ID = \"SilverEnrichedGridDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.geometry, GeometryType(), nullable=False),\n            StructField(ColNames.grid_id, IntegerType(), nullable=False),\n            StructField(ColNames.elevation, FloatType(), nullable=True),\n            StructField(ColNames.main_landuse_category, StringType(), nullable=True),\n            StructField(ColNames.landuse_area_ratios, MapType(StringType(), FloatType()), nullable=True),\n            # partition columns\n            StructField(ColNames.quadkey, StringType(), nullable=True),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.quadkey]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_event_data_object/","title":"silver_event_data_object","text":"<p>Silver MNO Event data module</p>"},{"location":"reference/core/data_objects/silver/silver_event_data_object/#core.data_objects.silver.silver_event_data_object.SilverEventDataObject","title":"<code>SilverEventDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the cleaned MNO Event data.</p> Source code in <code>multimno/core/data_objects/silver/silver_event_data_object.py</code> <pre><code>class SilverEventDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models the cleaned MNO Event data.\n    \"\"\"\n\n    ID = \"SilverEventDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.mcc, IntegerType(), nullable=True),\n            StructField(ColNames.mnc, StringType(), nullable=True),\n            StructField(ColNames.plmn, IntegerType(), nullable=True),\n            StructField(ColNames.domain, StringType(), nullable=True),\n            StructField(ColNames.cell_id, StringType(), nullable=True),\n            StructField(ColNames.latitude, FloatType(), nullable=True),\n            StructField(ColNames.longitude, FloatType(), nullable=True),\n            StructField(ColNames.loc_error, FloatType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.user_id_modulo, IntegerType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day, ColNames.user_id_modulo]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_metrics_by_column/","title":"silver_event_data_syntactic_quality_metrics_by_column","text":"<p>Silver Event Data quality metrics.</p>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_metrics_by_column/#core.data_objects.silver.silver_event_data_syntactic_quality_metrics_by_column.SilverEventDataSyntacticQualityMetricsByColumn","title":"<code>SilverEventDataSyntacticQualityMetricsByColumn</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the Silver Event Data quality metrics DataObject.</p> Source code in <code>multimno/core/data_objects/silver/silver_event_data_syntactic_quality_metrics_by_column.py</code> <pre><code>class SilverEventDataSyntacticQualityMetricsByColumn(ParquetDataObject):\n    \"\"\"\n    Class that models the Silver Event Data quality metrics DataObject.\n    \"\"\"\n\n    ID = \"SilverEventDataSyntacticQualityMetricsByColumn\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.result_timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.variable, StringType(), nullable=True),\n            StructField(ColNames.type_of_error, ShortType(), nullable=True),\n            StructField(ColNames.value, IntegerType(), nullable=False),\n            # partition columns\n            StructField(ColNames.date, DateType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.date]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_metrics_frequency_distribution/","title":"silver_event_data_syntactic_quality_metrics_frequency_distribution","text":"<p>Silver Event Data deduplication frequency quality metrics.</p>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_metrics_frequency_distribution/#core.data_objects.silver.silver_event_data_syntactic_quality_metrics_frequency_distribution.SilverEventDataSyntacticQualityMetricsFrequencyDistribution","title":"<code>SilverEventDataSyntacticQualityMetricsFrequencyDistribution</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the Silver Event Data syntactic frequency quality metrics DataObject.</p> Source code in <code>multimno/core/data_objects/silver/silver_event_data_syntactic_quality_metrics_frequency_distribution.py</code> <pre><code>class SilverEventDataSyntacticQualityMetricsFrequencyDistribution(ParquetDataObject):\n    \"\"\"\n    Class that models the Silver Event Data syntactic\n    frequency quality metrics DataObject.\n    \"\"\"\n\n    ID = \"SilverEventDataSyntacticQualityMetricsFrequencyDistribution\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.cell_id, StringType(), nullable=True),\n            StructField(ColNames.user_id, BinaryType(), nullable=True),\n            StructField(ColNames.initial_frequency, IntegerType(), nullable=False),\n            StructField(ColNames.final_frequency, IntegerType(), nullable=False),\n            StructField(ColNames.date, DateType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.date]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_warnings_for_plots/","title":"silver_event_data_syntactic_quality_warnings_for_plots","text":"<p>Silver Event Data quality warning for plots table.</p>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_warnings_for_plots/#core.data_objects.silver.silver_event_data_syntactic_quality_warnings_for_plots.SilverEventDataSyntacticQualityWarningsForPlots","title":"<code>SilverEventDataSyntacticQualityWarningsForPlots</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that store data to plot raw, clean data sizez and error rate.</p> Source code in <code>multimno/core/data_objects/silver/silver_event_data_syntactic_quality_warnings_for_plots.py</code> <pre><code>class SilverEventDataSyntacticQualityWarningsForPlots(ParquetDataObject):\n    \"\"\"\n    Class that store data to plot raw, clean data sizez and error rate.\n    \"\"\"\n\n    ID = \"SilverEventDataSyntacticQualityWarningsForPlots\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.type_of_qw, StringType(), nullable=False),\n            StructField(ColNames.lookback_period, StringType(), nullable=False),\n            StructField(ColNames.daily_value, FloatType(), nullable=False),\n            StructField(ColNames.average, FloatType(), nullable=True),\n            StructField(ColNames.LCL, FloatType(), nullable=True),\n            StructField(ColNames.UCL, FloatType(), nullable=True),\n            # partition columns\n            StructField(ColNames.date, DateType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.date]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_warnings_log_table/","title":"silver_event_data_syntactic_quality_warnings_log_table","text":"<p>Silver Event Data Quality Warning log table.</p>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_warnings_log_table/#core.data_objects.silver.silver_event_data_syntactic_quality_warnings_log_table.SilverEventDataSyntacticQualityWarningsLogTable","title":"<code>SilverEventDataSyntacticQualityWarningsLogTable</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that stores information about Event Quallity Warnings</p> Source code in <code>multimno/core/data_objects/silver/silver_event_data_syntactic_quality_warnings_log_table.py</code> <pre><code>class SilverEventDataSyntacticQualityWarningsLogTable(ParquetDataObject):\n    \"\"\"\n    Class that stores information about Event Quallity Warnings\n    \"\"\"\n\n    ID = \"SilverEventDataSyntacticQualityWarningsLogTable\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.measure_definition, StringType(), nullable=False),\n            StructField(ColNames.lookback_period, StringType(), nullable=False),\n            StructField(ColNames.daily_value, FloatType(), nullable=False),\n            StructField(ColNames.condition_value, FloatType(), nullable=True),\n            StructField(ColNames.condition, StringType(), nullable=False),\n            StructField(ColNames.warning_text, StringType(), nullable=False),\n            # partition columns\n            StructField(ColNames.date, DateType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.date]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_event_flagged_data_object/","title":"silver_event_flagged_data_object","text":"<p>Silver MNO Event data module with flags computed in Semantic Checks</p>"},{"location":"reference/core/data_objects/silver/silver_event_flagged_data_object/#core.data_objects.silver.silver_event_flagged_data_object.SilverEventFlaggedDataObject","title":"<code>SilverEventFlaggedDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the cleaned MNO Event data, with flags computed in the semantic checks module.</p> Source code in <code>multimno/core/data_objects/silver/silver_event_flagged_data_object.py</code> <pre><code>class SilverEventFlaggedDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models the cleaned MNO Event data, with flags computed\n    in the semantic checks module.\n    \"\"\"\n\n    ID = \"SilverEventFlaggedDO\"\n\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.mcc, IntegerType(), nullable=True),\n            StructField(ColNames.mnc, StringType(), nullable=True),\n            StructField(ColNames.plmn, IntegerType(), nullable=True),\n            StructField(ColNames.domain, StringType(), nullable=True),\n            StructField(ColNames.cell_id, StringType(), nullable=True),\n            StructField(ColNames.latitude, FloatType(), nullable=True),\n            StructField(ColNames.longitude, FloatType(), nullable=True),\n            StructField(ColNames.loc_error, FloatType(), nullable=True),\n            StructField(ColNames.error_flag, IntegerType(), nullable=False),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.user_id_modulo, IntegerType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day, ColNames.user_id_modulo]\n\n    def is_data_available(self, date):\n        path = (\n            f\"{self.default_path}/{ColNames.year}={date.year}/{ColNames.month}={date.month}/{ColNames.day}={date.day}\"\n        )\n\n        return check_if_data_path_exists(self.spark, path)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_geozones_grid_map_data_object/","title":"silver_geozones_grid_map_data_object","text":""},{"location":"reference/core/data_objects/silver/silver_geozones_grid_map_data_object/#core.data_objects.silver.silver_geozones_grid_map_data_object.SilverGeozonesGridMapDataObject","title":"<code>SilverGeozonesGridMapDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models geographic and admin zones to grid mapping table.</p> Source code in <code>multimno/core/data_objects/silver/silver_geozones_grid_map_data_object.py</code> <pre><code>class SilverGeozonesGridMapDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models geographic and admin zones to grid mapping table.\n    \"\"\"\n\n    ID = \"SilverGeozonesGridMapDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.grid_id, IntegerType(), nullable=False),\n            StructField(ColNames.zone_id, StringType(), nullable=False),\n            StructField(ColNames.hierarchical_id, StringType(), nullable=False),\n            # partition columns\n            StructField(ColNames.dataset_id, StringType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            # StructField(ColNames.quadkey, StringType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.dataset_id, ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_grid_data_object/","title":"silver_grid_data_object","text":""},{"location":"reference/core/data_objects/silver/silver_grid_data_object/#core.data_objects.silver.silver_grid_data_object.SilverGridDataObject","title":"<code>SilverGridDataObject</code>","text":"<p>               Bases: <code>GeoParquetDataObject</code></p> <p>Class that models operational grid.</p> Source code in <code>multimno/core/data_objects/silver/silver_grid_data_object.py</code> <pre><code>class SilverGridDataObject(GeoParquetDataObject):\n    \"\"\"\n    Class that models operational grid.\n    \"\"\"\n\n    ID = \"SilverGridDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.geometry, GeometryType(), nullable=False),\n            StructField(ColNames.grid_id, IntegerType(), nullable=False),\n            # partition columns\n            StructField(ColNames.origin, LongType(), nullable=False),\n            StructField(ColNames.quadkey, StringType(), nullable=True),\n        ]\n    )\n\n    MANDATORY_COLUMNS = [ColNames.grid_id, ColNames.geometry]\n\n    PARTITION_COLUMNS = [ColNames.origin, ColNames.quadkey]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_group_to_tile_data_object/","title":"silver_group_to_tile_data_object","text":""},{"location":"reference/core/data_objects/silver/silver_group_to_tile_data_object/#core.data_objects.silver.silver_group_to_tile_data_object.SilverGroupToTileDataObject","title":"<code>SilverGroupToTileDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> Source code in <code>multimno/core/data_objects/silver/silver_group_to_tile_data_object.py</code> <pre><code>class SilverGroupToTileDataObject(ParquetDataObject):\n    \"\"\" \"\"\"\n\n    ID = \"SilverGroupToTileDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.group_id, StringType(), nullable=True),\n            StructField(ColNames.grid_id, IntegerType(), nullable=True),\n            StructField(ColNames.year, ShortType(), nullable=True),\n            StructField(ColNames.month, ByteType(), nullable=True),\n            StructField(ColNames.day, ByteType(), nullable=True),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_internal_migration_data_object/","title":"silver_internal_migration_data_object","text":"<p>Silver Internal Migration data object module</p>"},{"location":"reference/core/data_objects/silver/silver_internal_migration_data_object/#core.data_objects.silver.silver_internal_migration_data_object.SilverInternalMigrationDataObject","title":"<code>SilverInternalMigrationDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the Internal Migration data object.</p> Source code in <code>multimno/core/data_objects/silver/silver_internal_migration_data_object.py</code> <pre><code>class SilverInternalMigrationDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models the Internal Migration data object.\n    \"\"\"\n\n    ID = \"SilverInternalMigrationDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.previous_zone, StringType(), nullable=False),\n            StructField(ColNames.new_zone, StringType(), nullable=False),\n            StructField(ColNames.migration, FloatType(), nullable=False),\n            # partition columns\n            StructField(ColNames.dataset_id, StringType(), nullable=False),\n            StructField(ColNames.level, ByteType(), nullable=False),\n            StructField(ColNames.start_date_previous, DateType(), nullable=False),\n            StructField(ColNames.end_date_previous, DateType(), nullable=False),\n            StructField(ColNames.season_previous, StringType(), nullable=False),\n            StructField(ColNames.start_date_new, DateType(), nullable=False),\n            StructField(ColNames.end_date_new, DateType(), nullable=False),\n            StructField(ColNames.season_new, StringType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [\n        ColNames.dataset_id,\n        ColNames.level,\n        ColNames.start_date_previous,\n        ColNames.end_date_previous,\n        ColNames.season_previous,\n        ColNames.start_date_new,\n        ColNames.end_date_new,\n        ColNames.season_new,\n    ]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_internal_migration_quality_metrics_data_object/","title":"silver_internal_migration_quality_metrics_data_object","text":"<p>Silver Internal Migration data object module</p>"},{"location":"reference/core/data_objects/silver/silver_internal_migration_quality_metrics_data_object/#core.data_objects.silver.silver_internal_migration_quality_metrics_data_object.SilverInternalMigrationQualityMetricsDataObject","title":"<code>SilverInternalMigrationQualityMetricsDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the Internal Migration data object.</p> Source code in <code>multimno/core/data_objects/silver/silver_internal_migration_quality_metrics_data_object.py</code> <pre><code>class SilverInternalMigrationQualityMetricsDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models the Internal Migration data object.\n    \"\"\"\n\n    ID = \"SilverInternalMigrationQualityMetricsDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.previous_home_users, LongType(), nullable=False),\n            StructField(ColNames.new_home_users, LongType(), nullable=False),\n            StructField(ColNames.common_home_users, LongType(), nullable=False),\n            StructField(ColNames.result_timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.dataset_id, StringType(), nullable=False),\n            StructField(ColNames.level, ByteType(), nullable=False),\n            StructField(ColNames.start_date_previous, DateType(), nullable=False),\n            StructField(ColNames.end_date_previous, DateType(), nullable=False),\n            StructField(ColNames.season_previous, StringType(), nullable=False),\n            StructField(ColNames.start_date_new, DateType(), nullable=False),\n            StructField(ColNames.end_date_new, DateType(), nullable=False),\n            StructField(ColNames.season_new, StringType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [\n        ColNames.dataset_id,\n        ColNames.level,\n        ColNames.start_date_previous,\n        ColNames.end_date_previous,\n        ColNames.season_previous,\n        ColNames.start_date_new,\n        ColNames.end_date_new,\n        ColNames.season_new,\n    ]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_longterm_permanence_score_data_object/","title":"silver_longterm_permanence_score_data_object","text":"<p>Silver Mid Term Permanence Score data object module</p>"},{"location":"reference/core/data_objects/silver/silver_longterm_permanence_score_data_object/#core.data_objects.silver.silver_longterm_permanence_score_data_object.SilverLongtermPermanenceScoreDataObject","title":"<code>SilverLongtermPermanenceScoreDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the Longterm Permanence Score data object.</p> Source code in <code>multimno/core/data_objects/silver/silver_longterm_permanence_score_data_object.py</code> <pre><code>class SilverLongtermPermanenceScoreDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models the Longterm Permanence Score data object.\n    \"\"\"\n\n    ID = \"SilverLongtermPermanenceScoreDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.grid_id, IntegerType(), nullable=False),\n            StructField(ColNames.lps, IntegerType(), nullable=False),\n            StructField(ColNames.total_frequency, IntegerType(), nullable=False),\n            StructField(ColNames.frequency_mean, FloatType(), nullable=True),\n            StructField(ColNames.frequency_std, FloatType(), nullable=True),\n            StructField(ColNames.regularity_mean, FloatType(), nullable=True),\n            StructField(ColNames.regularity_std, FloatType(), nullable=True),\n            # partition columns\n            StructField(ColNames.season, StringType(), nullable=False),\n            StructField(ColNames.start_date, DateType(), nullable=False),\n            StructField(ColNames.end_date, DateType(), nullable=False),\n            StructField(ColNames.day_type, StringType(), nullable=False),\n            StructField(ColNames.time_interval, StringType(), nullable=False),\n            StructField(ColNames.id_type, StringType(), nullable=False),\n            StructField(ColNames.user_id_modulo, IntegerType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [\n        ColNames.season,\n        ColNames.start_date,\n        ColNames.end_date,\n        ColNames.day_type,\n        ColNames.time_interval,\n        ColNames.id_type,\n        ColNames.user_id_modulo,\n    ]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_midterm_permanence_score_data_object/","title":"silver_midterm_permanence_score_data_object","text":"<p>Silver Mid Term Permanence Score data object module</p>"},{"location":"reference/core/data_objects/silver/silver_midterm_permanence_score_data_object/#core.data_objects.silver.silver_midterm_permanence_score_data_object.SilverMidtermPermanenceScoreDataObject","title":"<code>SilverMidtermPermanenceScoreDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the Midterm Permanence Score data object.</p> Source code in <code>multimno/core/data_objects/silver/silver_midterm_permanence_score_data_object.py</code> <pre><code>class SilverMidtermPermanenceScoreDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models the Midterm Permanence Score data object.\n    \"\"\"\n\n    ID = \"SilverMidtermPermanenceScoreDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.grid_id, IntegerType(), nullable=False),\n            StructField(ColNames.mps, IntegerType(), nullable=False),\n            StructField(ColNames.frequency, IntegerType(), nullable=False),\n            StructField(ColNames.regularity_mean, FloatType(), nullable=True),\n            StructField(ColNames.regularity_std, FloatType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day_type, StringType(), nullable=False),\n            StructField(ColNames.time_interval, StringType(), nullable=False),\n            StructField(ColNames.id_type, StringType(), nullable=False),\n            StructField(ColNames.user_id_modulo, IntegerType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [\n        ColNames.year,\n        ColNames.month,\n        ColNames.day_type,\n        ColNames.time_interval,\n        ColNames.id_type,\n        ColNames.user_id_modulo,\n    ]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_network_data_object/","title":"silver_network_data_object","text":"<p>Silver MNO Network Topology Data module</p>"},{"location":"reference/core/data_objects/silver/silver_network_data_object/#core.data_objects.silver.silver_network_data_object.SilverNetworkDataObject","title":"<code>SilverNetworkDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the clean MNO Network Topology Data, based on the physical properties of the cells.</p> Source code in <code>multimno/core/data_objects/silver/silver_network_data_object.py</code> <pre><code>class SilverNetworkDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models the clean MNO Network Topology Data, based on the physical\n    properties of the cells.\n    \"\"\"\n\n    ID = \"SilverNetworkDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.cell_id, StringType(), nullable=False),\n            StructField(ColNames.latitude, FloatType(), nullable=False),\n            StructField(ColNames.longitude, FloatType(), nullable=False),\n            StructField(ColNames.altitude, FloatType(), nullable=True),\n            StructField(ColNames.antenna_height, FloatType(), nullable=True),\n            StructField(ColNames.directionality, IntegerType(), nullable=False),\n            StructField(ColNames.azimuth_angle, FloatType(), nullable=True),\n            StructField(ColNames.elevation_angle, FloatType(), nullable=True),\n            StructField(ColNames.horizontal_beam_width, FloatType(), nullable=True),\n            StructField(ColNames.vertical_beam_width, FloatType(), nullable=True),\n            StructField(ColNames.power, FloatType(), nullable=True),\n            StructField(ColNames.range, FloatType(), nullable=True),\n            StructField(ColNames.frequency, IntegerType(), nullable=True),\n            StructField(ColNames.technology, StringType(), nullable=True),\n            StructField(ColNames.valid_date_start, TimestampType(), nullable=True),\n            StructField(ColNames.valid_date_end, TimestampType(), nullable=True),\n            StructField(ColNames.cell_type, StringType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_network_data_syntactic_quality_metrics_by_column/","title":"silver_network_data_syntactic_quality_metrics_by_column","text":"<p>Silver Network topology quality metrics.</p>"},{"location":"reference/core/data_objects/silver/silver_network_data_syntactic_quality_metrics_by_column/#core.data_objects.silver.silver_network_data_syntactic_quality_metrics_by_column.SilverNetworkDataQualityMetricsByColumn","title":"<code>SilverNetworkDataQualityMetricsByColumn</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the Silver Network Topology data quality metrics data object.</p> Source code in <code>multimno/core/data_objects/silver/silver_network_data_syntactic_quality_metrics_by_column.py</code> <pre><code>class SilverNetworkDataQualityMetricsByColumn(ParquetDataObject):\n    \"\"\"\n    Class that models the Silver Network Topology data quality metrics data object.\n    \"\"\"\n\n    ID = \"SilverNetworkDataQualityMetricsByColumn\"\n\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.result_timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.date, DateType(), nullable=False),\n            StructField(ColNames.field_name, StringType(), nullable=True),\n            StructField(ColNames.type_code, IntegerType(), nullable=False),\n            StructField(ColNames.value, IntegerType(), nullable=False),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_network_data_top_frequent_errors_data_object/","title":"silver_network_data_top_frequent_errors_data_object","text":"<p>Silver Network Data Top Frequent Errors.</p>"},{"location":"reference/core/data_objects/silver/silver_network_data_top_frequent_errors_data_object/#core.data_objects.silver.silver_network_data_top_frequent_errors_data_object.SilverNetworkDataTopFrequentErrors","title":"<code>SilverNetworkDataTopFrequentErrors</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the Silver Network Topology Top Frequent Errors data object</p> Source code in <code>multimno/core/data_objects/silver/silver_network_data_top_frequent_errors_data_object.py</code> <pre><code>class SilverNetworkDataTopFrequentErrors(ParquetDataObject):\n    \"\"\"\n    Class that models the Silver Network Topology Top Frequent Errors data object\n    \"\"\"\n\n    ID = \"SilverNetworkDataTopFrequentErrorsDO\"\n\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.result_timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.field_name, StringType(), nullable=False),\n            StructField(ColNames.type_code, IntegerType(), nullable=False),\n            StructField(ColNames.error_value, StringType(), nullable=False),\n            StructField(ColNames.error_count, IntegerType(), nullable=False),\n            StructField(ColNames.accumulated_percentage, FloatType(), nullable=False),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_network_row_error_metrics/","title":"silver_network_row_error_metrics","text":"<p>Silver Network Data Row Error Metrics.</p>"},{"location":"reference/core/data_objects/silver/silver_network_row_error_metrics/#core.data_objects.silver.silver_network_row_error_metrics.SilverNetworkRowErrorMetrics","title":"<code>SilverNetworkRowErrorMetrics</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the Silver Network Topology Row Error Metrics data object</p> Source code in <code>multimno/core/data_objects/silver/silver_network_row_error_metrics.py</code> <pre><code>class SilverNetworkRowErrorMetrics(ParquetDataObject):\n    \"\"\"\n    Class that models the Silver Network Topology Row Error Metrics data object\n    \"\"\"\n\n    ID = \"SilverNetworkRowErrorMetricsDO\"\n\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.result_timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.variable, StringType(), nullable=False),\n            StructField(ColNames.value, IntegerType(), nullable=False),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_network_syntactic_quality_warnings_log_table/","title":"silver_network_syntactic_quality_warnings_log_table","text":"<p>Silver MNO Network Topology Quality Warnings Log Table Data Object</p>"},{"location":"reference/core/data_objects/silver/silver_network_syntactic_quality_warnings_log_table/#core.data_objects.silver.silver_network_syntactic_quality_warnings_log_table.SilverNetworkDataSyntacticQualityWarningsLogTable","title":"<code>SilverNetworkDataSyntacticQualityWarningsLogTable</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the log table keeping track of the quality warnings that may arise from the syntactic checks and cleaning of the MNO Network Topology Data.</p> Source code in <code>multimno/core/data_objects/silver/silver_network_syntactic_quality_warnings_log_table.py</code> <pre><code>class SilverNetworkDataSyntacticQualityWarningsLogTable(ParquetDataObject):\n    \"\"\"\n    Class that models the log table keeping track of the quality warnings that may arise from\n    the syntactic checks and cleaning of the MNO Network Topology Data.\n    \"\"\"\n\n    ID = \"SilverNetworkDataSyntacticQualityWarningsLogTable\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.title, StringType(), nullable=False),\n            StructField(ColNames.date, DateType(), nullable=False),  # date of study analysed\n            StructField(ColNames.timestamp, TimestampType(), nullable=False),  # moment when QW where generated\n            StructField(ColNames.measure_definition, StringType(), nullable=False),\n            StructField(ColNames.daily_value, FloatType(), nullable=False),\n            StructField(ColNames.condition, StringType(), nullable=False),\n            StructField(ColNames.lookback_period, StringType(), nullable=False),  # using same name as for events\n            StructField(ColNames.condition_value, FloatType(), nullable=False),\n            StructField(ColNames.warning_text, StringType(), nullable=False),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_network_syntactic_quality_warnings_plot_data/","title":"silver_network_syntactic_quality_warnings_plot_data","text":"<p>Silver MNO Network Topology Quality Warnings Data Object for the generation of plots</p>"},{"location":"reference/core/data_objects/silver/silver_network_syntactic_quality_warnings_plot_data/#core.data_objects.silver.silver_network_syntactic_quality_warnings_plot_data.SilverNetworkSyntacticQualityWarningsLinePlotData","title":"<code>SilverNetworkSyntacticQualityWarningsLinePlotData</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the data required to produce line plots reflecting the daily evolution of the number of rows before and after the syntactic checks, as well as the overall error rate.</p> Source code in <code>multimno/core/data_objects/silver/silver_network_syntactic_quality_warnings_plot_data.py</code> <pre><code>class SilverNetworkSyntacticQualityWarningsLinePlotData(ParquetDataObject):\n    \"\"\"\n    Class that models the data required to produce line plots reflecting the daily evolution of the number\n    of rows before and after the syntactic checks, as well as the overall error rate.\n    \"\"\"\n\n    ID = \"SilverNetworkSyntacticQualityWarningsLinePlotData\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.date, DateType(), nullable=False),\n            StructField(ColNames.daily_value, FloatType(), nullable=False),\n            StructField(ColNames.average, FloatType(), nullable=False),\n            StructField(ColNames.LCL, FloatType(), nullable=False),\n            StructField(ColNames.UCL, FloatType(), nullable=True),\n            # partition columns\n            StructField(ColNames.variable, StringType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.timestamp, TimestampType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [\n        ColNames.variable,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n        ColNames.timestamp,\n    ]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_network_syntactic_quality_warnings_plot_data/#core.data_objects.silver.silver_network_syntactic_quality_warnings_plot_data.SilverNetworkSyntacticQualityWarningsPiePlotData","title":"<code>SilverNetworkSyntacticQualityWarningsPiePlotData</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the data required to produce pie plots reflecting the percentage of each type of error for each field of the network topology data object.</p> Source code in <code>multimno/core/data_objects/silver/silver_network_syntactic_quality_warnings_plot_data.py</code> <pre><code>class SilverNetworkSyntacticQualityWarningsPiePlotData(ParquetDataObject):\n    \"\"\"\n    Class that models the data required to produce pie plots reflecting the percentage of each type of error\n    for each field of the network topology data object.\n    \"\"\"\n\n    ID = \"SilverNetworkSyntacticQualityWarningsPiePlotData\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.type_of_error, StringType(), nullable=False),\n            StructField(ColNames.value, IntegerType(), nullable=False),\n            # partition columns\n            StructField(ColNames.variable, StringType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.timestamp, TimestampType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [\n        ColNames.variable,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n        ColNames.timestamp,\n    ]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_present_population_data_object/","title":"silver_present_population_data_object","text":"<p>Silver present population estimatation per grid data object</p>"},{"location":"reference/core/data_objects/silver/silver_present_population_data_object/#core.data_objects.silver.silver_present_population_data_object.SilverPresentPopulationDataObject","title":"<code>SilverPresentPopulationDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Estimation of the population present at a given time at the grid tile level.</p> Source code in <code>multimno/core/data_objects/silver/silver_present_population_data_object.py</code> <pre><code>class SilverPresentPopulationDataObject(ParquetDataObject):\n    \"\"\"\n    Estimation of the population present at a given time at the grid tile level.\n    \"\"\"\n\n    ID = \"SilverPresentPopulationDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.grid_id, IntegerType(), nullable=False),\n            StructField(ColNames.population, FloatType(), nullable=False),\n            StructField(ColNames.timestamp, TimestampType(), nullable=False),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_present_population_zone_data_object/","title":"silver_present_population_zone_data_object","text":"<p>Silver present population estimatation per zone data object</p>"},{"location":"reference/core/data_objects/silver/silver_present_population_zone_data_object/#core.data_objects.silver.silver_present_population_zone_data_object.SilverPresentPopulationZoneDataObject","title":"<code>SilverPresentPopulationZoneDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Estimation of the population present at a given time at the level of some zoning system.</p> Source code in <code>multimno/core/data_objects/silver/silver_present_population_zone_data_object.py</code> <pre><code>class SilverPresentPopulationZoneDataObject(ParquetDataObject):\n    \"\"\"\n    Estimation of the population present at a given time at the level of some zoning system.\n    \"\"\"\n\n    ID = \"SilverPresentPopulationZoneDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.zone_id, StringType(), nullable=False),\n            StructField(ColNames.population, FloatType(), nullable=False),\n            StructField(ColNames.timestamp, TimestampType(), nullable=False),\n            # partition columns\n            StructField(ColNames.dataset_id, StringType(), nullable=False),\n            StructField(ColNames.level, ByteType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    VALUE_COLUMNS = [ColNames.population]\n\n    AGGREGATION_COLUMNS = [\n        ColNames.zone_id,\n        ColNames.timestamp,\n        ColNames.dataset_id,\n        ColNames.level,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    ]\n\n    PARTITION_COLUMNS = [ColNames.dataset_id, ColNames.level, ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_semantic_quality_metrics/","title":"silver_semantic_quality_metrics","text":""},{"location":"reference/core/data_objects/silver/silver_semantic_quality_metrics/#core.data_objects.silver.silver_semantic_quality_metrics.SilverEventSemanticQualityMetrics","title":"<code>SilverEventSemanticQualityMetrics</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> Source code in <code>multimno/core/data_objects/silver/silver_semantic_quality_metrics.py</code> <pre><code>class SilverEventSemanticQualityMetrics(ParquetDataObject):\n    \"\"\" \"\"\"\n\n    ID = \"SilverEventSemanticQualityMetrics\"\n\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.result_timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.variable, StringType(), nullable=False),\n            StructField(ColNames.type_of_error, IntegerType(), nullable=False),\n            StructField(ColNames.value, LongType(), nullable=False),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_semantic_quality_warnings_log_table/","title":"silver_semantic_quality_warnings_log_table","text":"<p>Silver MNO Network Topology Quality Warnings Log Table Data Object</p>"},{"location":"reference/core/data_objects/silver/silver_semantic_quality_warnings_log_table/#core.data_objects.silver.silver_semantic_quality_warnings_log_table.SilverEventSemanticQualityWarningsLogTable","title":"<code>SilverEventSemanticQualityWarningsLogTable</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the log table keeping track of the quality warnings that may arise from the semantic checks of the MNO Event data.</p> Source code in <code>multimno/core/data_objects/silver/silver_semantic_quality_warnings_log_table.py</code> <pre><code>class SilverEventSemanticQualityWarningsLogTable(ParquetDataObject):\n    \"\"\"\n    Class that models the log table keeping track of the quality warnings that may arise from\n    the semantic checks of the MNO Event data.\n    \"\"\"\n\n    ID = \"SilverEventSemanticQualityWarningsLogTable\"\n\n    SCHEMA = StructType(\n        [\n            StructField(\"date\", DateType(), nullable=False),\n            StructField(\"Error 1\", FloatType(), nullable=False),\n            StructField(\"Error 2\", FloatType(), nullable=False),\n            StructField(\"Error 3\", FloatType(), nullable=False),\n            StructField(\"Error 4\", FloatType(), nullable=False),\n            StructField(\"Error 5\", FloatType(), nullable=False),\n            StructField(\"Error 1 upper control limit\", FloatType(), nullable=True),\n            StructField(\"Error 2 upper control limit\", FloatType(), nullable=True),\n            StructField(\"Error 3 upper control limit\", FloatType(), nullable=True),\n            StructField(\"Error 4 upper control limit\", FloatType(), nullable=True),\n            StructField(\"Error 5 upper control limit\", FloatType(), nullable=True),\n            StructField(\"Error 1 display warning\", BooleanType(), nullable=False),\n            StructField(\"Error 2 display warning\", BooleanType(), nullable=False),\n            StructField(\"Error 3 display warning\", BooleanType(), nullable=False),\n            StructField(\"Error 4 display warning\", BooleanType(), nullable=False),\n            StructField(\"Error 5 display warning\", BooleanType(), nullable=False),\n            StructField(\"execution_id\", TimestampType(), nullable=False),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_semantic_quality_warnings_plot_data/","title":"silver_semantic_quality_warnings_plot_data","text":"<p>Silver MNO Network Topology Quality Warnings Log Table Data Object</p>"},{"location":"reference/core/data_objects/silver/silver_semantic_quality_warnings_plot_data/#core.data_objects.silver.silver_semantic_quality_warnings_plot_data.SilverEventSemanticQualityWarningsBarPlotData","title":"<code>SilverEventSemanticQualityWarningsBarPlotData</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the data required to produce line plots reflecting the daily evolution of the number of each type of error, as well as records without errors, as seen during the semantic checks of MNO Event data.</p> Source code in <code>multimno/core/data_objects/silver/silver_semantic_quality_warnings_plot_data.py</code> <pre><code>class SilverEventSemanticQualityWarningsBarPlotData(ParquetDataObject):\n    \"\"\"\n    Class that models the data required to produce line plots reflecting the daily evolution of the number\n    of each type of error, as well as records without errors, as seen during the semantic checks of MNO Event data.\n    \"\"\"\n\n    ID = \"SilverEventSemanticQualityWarningBarPlotData\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.date, DateType(), nullable=False),\n            StructField(ColNames.type_of_error, StringType(), nullable=False),\n            StructField(ColNames.value, FloatType(), nullable=False),\n            # partition columns\n            StructField(ColNames.variable, StringType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.timestamp, TimestampType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.variable, ColNames.year, ColNames.month, ColNames.day, ColNames.timestamp]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_time_segments_data_object/","title":"silver_time_segments_data_object","text":""},{"location":"reference/core/data_objects/silver/silver_time_segments_data_object/#core.data_objects.silver.silver_time_segments_data_object.SilverTimeSegmentsDataObject","title":"<code>SilverTimeSegmentsDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the cleaned MNO Event data.</p> Source code in <code>multimno/core/data_objects/silver/silver_time_segments_data_object.py</code> <pre><code>class SilverTimeSegmentsDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models the cleaned MNO Event data.\n    \"\"\"\n\n    ID = \"SilverTimeSegmentsDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.time_segment_id, StringType(), nullable=False),\n            StructField(ColNames.start_timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.end_timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.last_event_timestamp, TimestampType(), nullable=True),\n            StructField(ColNames.mcc, ShortType(), nullable=False),\n            StructField(ColNames.mnc, StringType(), nullable=False),\n            StructField(ColNames.plmn, IntegerType(), nullable=True),\n            StructField(ColNames.cells, ArrayType(StringType()), nullable=False),\n            StructField(ColNames.state, ByteType(), nullable=False),\n            StructField(ColNames.is_last, BooleanType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.user_id_modulo, IntegerType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n        ColNames.user_id_modulo,\n    ]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_tourism_outbound_nights_spent_data_object/","title":"silver_tourism_outbound_nights_spent_data_object","text":""},{"location":"reference/core/data_objects/silver/silver_tourism_outbound_nights_spent_data_object/#core.data_objects.silver.silver_tourism_outbound_nights_spent_data_object.SilverTourismOutboundNightsSpentDataObject","title":"<code>SilverTourismOutboundNightsSpentDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Data Object for monthly outbound tourism aggregations of nights spent per foreign country.</p> Source code in <code>multimno/core/data_objects/silver/silver_tourism_outbound_nights_spent_data_object.py</code> <pre><code>class SilverTourismOutboundNightsSpentDataObject(ParquetDataObject):\n    \"\"\"\n    Data Object for monthly outbound tourism aggregations of nights spent per foreign country.\n    \"\"\"\n\n    ID = \"SilverTourismOutboundNightsSpentDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.time_period, StringType(), nullable=False),\n            StructField(ColNames.country_of_destination, StringType(), nullable=False),\n            StructField(ColNames.nights_spent, FloatType(), nullable=False),\n            # partitioning columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_tourism_stays_data_object/","title":"silver_tourism_stays_data_object","text":""},{"location":"reference/core/data_objects/silver/silver_tourism_stays_data_object/#core.data_objects.silver.silver_tourism_stays_data_object.SilverTourismStaysDataObject","title":"<code>SilverTourismStaysDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Data Object for daily tourism stays.</p> Source code in <code>multimno/core/data_objects/silver/silver_tourism_stays_data_object.py</code> <pre><code>class SilverTourismStaysDataObject(ParquetDataObject):\n    \"\"\"\n    Data Object for daily tourism stays.\n    \"\"\"\n\n    ID = \"SilverTourismStaysDataObjectDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.time_segment_id, StringType(), nullable=False),\n            StructField(ColNames.start_timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.end_timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.mcc, ShortType(), nullable=False),\n            StructField(ColNames.mnc, StringType(), nullable=False),\n            StructField(ColNames.plmn, StringType(), nullable=False),\n            StructField(ColNames.zone_ids_list, ArrayType(StringType()), nullable=False),\n            StructField(ColNames.zone_weights_list, ArrayType(FloatType()), nullable=False),\n            StructField(ColNames.is_overnight, BooleanType(), nullable=False),\n            # partitioning columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.user_id_modulo, IntegerType(), nullable=False),\n            StructField(ColNames.dataset_id, StringType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day, ColNames.user_id_modulo, ColNames.dataset_id]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_tourism_trip_avg_destinations_nights_spent_data_object/","title":"silver_tourism_trip_avg_destinations_nights_spent_data_object","text":""},{"location":"reference/core/data_objects/silver/silver_tourism_trip_avg_destinations_nights_spent_data_object/#core.data_objects.silver.silver_tourism_trip_avg_destinations_nights_spent_data_object.SilverTourismTripAvgDestinationsNightsSpentDataObject","title":"<code>SilverTourismTripAvgDestinationsNightsSpentDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Data Object for monthly tourism aggregations of average number of destinations and nights spent per trip.</p> Source code in <code>multimno/core/data_objects/silver/silver_tourism_trip_avg_destinations_nights_spent_data_object.py</code> <pre><code>class SilverTourismTripAvgDestinationsNightsSpentDataObject(ParquetDataObject):\n    \"\"\"\n    Data Object for monthly tourism aggregations of average number of destinations and nights spent per trip.\n    \"\"\"\n\n    ID = \"SilverTourismTripAvgDestinationsNightsSpentDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.time_period, StringType(), nullable=False),\n            StructField(ColNames.country_of_origin, StringType(), nullable=False),\n            StructField(ColNames.avg_destinations, FloatType(), nullable=False),\n            StructField(ColNames.avg_nights_spent_per_destination, FloatType(), nullable=False),\n            # partitioning columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.level, ByteType(), nullable=False),\n            StructField(ColNames.dataset_id, StringType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.level, ColNames.dataset_id]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_tourism_trip_data_object/","title":"silver_tourism_trip_data_object","text":""},{"location":"reference/core/data_objects/silver/silver_tourism_trip_data_object/#core.data_objects.silver.silver_tourism_trip_data_object.SilverTourismTripDataObject","title":"<code>SilverTourismTripDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Data Object for tourism trips.</p> Source code in <code>multimno/core/data_objects/silver/silver_tourism_trip_data_object.py</code> <pre><code>class SilverTourismTripDataObject(ParquetDataObject):\n    \"\"\"\n    Data Object for tourism trips.\n    \"\"\"\n\n    ID = \"SilverTourismTripsDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.trip_id, StringType(), nullable=False),\n            StructField(ColNames.trip_start_timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.time_segment_ids_list, ArrayType(StringType()), nullable=False),\n            StructField(ColNames.is_trip_finished, BooleanType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.user_id_modulo, IntegerType(), nullable=False),\n            StructField(ColNames.dataset_id, StringType(), nullable=False),\n        ]\n    )\n\n    MANDATORY_COLUMNS = [\n        ColNames.user_id,\n        ColNames.user_id_modulo,\n        ColNames.trip_id,\n        ColNames.trip_start_timestamp,\n        ColNames.time_segment_ids_list,\n        ColNames.year,\n        ColNames.month,\n        ColNames.dataset_id,\n    ]\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.user_id_modulo, ColNames.dataset_id]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_tourism_zone_departures_nights_spent_data_object/","title":"silver_tourism_zone_departures_nights_spent_data_object","text":""},{"location":"reference/core/data_objects/silver/silver_tourism_zone_departures_nights_spent_data_object/#core.data_objects.silver.silver_tourism_zone_departures_nights_spent_data_object.SilverTourismZoneDeparturesNightsSpentDataObject","title":"<code>SilverTourismZoneDeparturesNightsSpentDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Data Object for monthly tourism aggregations of nights spent and departures per geographical unit.</p> Source code in <code>multimno/core/data_objects/silver/silver_tourism_zone_departures_nights_spent_data_object.py</code> <pre><code>class SilverTourismZoneDeparturesNightsSpentDataObject(ParquetDataObject):\n    \"\"\"\n    Data Object for monthly tourism aggregations of nights spent and departures per geographical unit.\n    \"\"\"\n\n    ID = \"SilverTourismDeparturesNightsSpentDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.time_period, StringType(), nullable=False),\n            StructField(ColNames.zone_id, StringType(), nullable=False),\n            StructField(ColNames.country_of_origin, StringType(), nullable=False),\n            StructField(ColNames.is_overnight, BooleanType(), nullable=False),\n            StructField(ColNames.nights_spent, FloatType(), nullable=False),\n            StructField(ColNames.num_of_departures, FloatType(), nullable=False),\n            # partitioning columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.level, ByteType(), nullable=False),\n            StructField(ColNames.dataset_id, StringType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.level, ColNames.dataset_id]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_usual_environment_labeling_quality_metrics_data_object/","title":"silver_usual_environment_labeling_quality_metrics_data_object","text":"<p>Silver Usual Environment Labeling Quality Metrics data object module</p>"},{"location":"reference/core/data_objects/silver/silver_usual_environment_labeling_quality_metrics_data_object/#core.data_objects.silver.silver_usual_environment_labeling_quality_metrics_data_object.SilverUsualEnvironmentLabelingQualityMetricsDataObject","title":"<code>SilverUsualEnvironmentLabelingQualityMetricsDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the Usual Environment Labeling Quality Metrics data object.</p> Source code in <code>multimno/core/data_objects/silver/silver_usual_environment_labeling_quality_metrics_data_object.py</code> <pre><code>class SilverUsualEnvironmentLabelingQualityMetricsDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models the Usual Environment Labeling Quality Metrics data object.\n    \"\"\"\n\n    ID = \"SilverUsualEnvironmentLabelingQualityMetricsDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.labeling_quality_metric, StringType(), nullable=False),\n            StructField(ColNames.labeling_quality_count, LongType(), nullable=False),\n            StructField(ColNames.labeling_quality_min, LongType(), nullable=False),\n            StructField(ColNames.labeling_quality_max, LongType(), nullable=False),\n            StructField(ColNames.labeling_quality_avg, LongType(), nullable=False),\n            # partition columns\n            StructField(ColNames.start_date, DateType(), nullable=False),\n            StructField(ColNames.end_date, DateType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.start_date, ColNames.end_date]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_usual_environment_labels_data_object/","title":"silver_usual_environment_labels_data_object","text":"<p>Silver Usual Environment Labels data object module</p>"},{"location":"reference/core/data_objects/silver/silver_usual_environment_labels_data_object/#core.data_objects.silver.silver_usual_environment_labels_data_object.SilverUsualEnvironmentLabelsDataObject","title":"<code>SilverUsualEnvironmentLabelsDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the Usual Environment Labels data object.</p> Source code in <code>multimno/core/data_objects/silver/silver_usual_environment_labels_data_object.py</code> <pre><code>class SilverUsualEnvironmentLabelsDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models the Usual Environment Labels data object.\n    \"\"\"\n\n    ID = \"SilverUsualEnvironmentLabelsDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.grid_id, IntegerType(), nullable=False),\n            StructField(ColNames.label, StringType(), nullable=False),\n            StructField(ColNames.label_rule, StringType(), nullable=False),\n            StructField(ColNames.id_type, StringType(), nullable=False),\n            # partition columns\n            StructField(ColNames.start_date, DateType(), nullable=False),\n            StructField(ColNames.end_date, DateType(), nullable=False),\n            StructField(ColNames.season, StringType(), nullable=False),\n            StructField(ColNames.user_id_modulo, IntegerType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.start_date, ColNames.end_date, ColNames.season, ColNames.user_id_modulo]\n</code></pre>"}]}