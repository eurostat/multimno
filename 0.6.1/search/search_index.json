{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MultiMNO","text":"<p>This repository contains code that processes MNO Data to generate population and mobility insights.</p>"},{"location":"#sections","title":"Sections","text":"<ul> <li>User Manual: Guide on how to setup, configure and execute the software.</li> <li>Dev Guide: Guidelines and best practices for contributing to the repository and setting up a development environment.</li> <li>Pipeline: View of the data processing pipeline.</li> <li>Reference: Code documentation.</li> <li>System Requirements: Mandatory requirements to execute the software.</li> <li>License: Software license - EUROPEAN UNION PUBLIC LICENCE v. 1.2.</li> </ul>"},{"location":"LICENSE/","title":"License","text":"<p>EUROPEAN UNION PUBLIC LICENCE v. 1.2  EUPL \u00a9 the European Union 2007, 2016 </p> <p>This European Union Public Licence (the \u2018EUPL\u2019) applies to the Work (as defined below) which is provided under the  terms of this Licence. Any use of the Work, other than as authorised under this Licence is prohibited (to the extent such  use is covered by a right of the copyright holder of the Work).  The Work is provided under the terms of this Licence when the Licensor (as defined below) has placed the following  notice immediately following the copyright notice for the Work:                            Licensed under the EUPL  or has expressed by any other means his willingness to license under the EUPL. </p> <p>1.Definitions  In this Licence, the following terms have the following meaning:  \u2014 \u2018The Licence\u2019:this Licence.  \u2014 \u2018The Original Work\u2019:the work or software distributed or communicated by the Licensor under this Licence, available  as Source Code and also as Executable Code as the case may be.  \u2014 \u2018Derivative Works\u2019:the works or software that could be created by the Licensee, based upon the Original Work or  modifications thereof. This Licence does not define the extent of modification or dependence on the Original Work  required in order to classify a work as a Derivative Work; this extent is determined by copyright law applicable in  the country mentioned in Article 15.  \u2014 \u2018The Work\u2019:the Original Work or its Derivative Works.  \u2014 \u2018The Source Code\u2019:the human-readable form of the Work which is the most convenient for people to study and  modify.  \u2014 \u2018The Executable Code\u2019:any code which has generally been compiled and which is meant to be interpreted by  a computer as a program.  \u2014 \u2018The Licensor\u2019:the natural or legal person that distributes or communicates the Work under the Licence.  \u2014 \u2018Contributor(s)\u2019:any natural or legal person who modifies the Work under the Licence, or otherwise contributes to  the creation of a Derivative Work.  \u2014 \u2018The Licensee\u2019 or \u2018You\u2019:any natural or legal person who makes any usage of the Work under the terms of the  Licence.  \u2014 \u2018Distribution\u2019 or \u2018Communication\u2019:any act of selling, giving, lending, renting, distributing, communicating,  transmitting, or otherwise making available, online or offline, copies of the Work or providing access to its essential  functionalities at the disposal of any other natural or legal person. </p> <p>2.Scope of the rights granted by the Licence  The Licensor hereby grants You a worldwide, royalty-free, non-exclusive, sublicensable licence to do the following, for  the duration of copyright vested in the Original Work:  \u2014 use the Work in any circumstance and for all usage,  \u2014 reproduce the Work,  \u2014 modify the Work, and make Derivative Works based upon the Work,  \u2014 communicate to the public, including the right to make available or display the Work or copies thereof to the public  and perform publicly, as the case may be, the Work,  \u2014 distribute the Work or copies thereof,  \u2014 lend and rent the Work or copies thereof,  \u2014 sublicense rights in the Work or copies thereof.  Those rights can be exercised on any media, supports and formats, whether now known or later invented, as far as the  applicable law permits so.  In the countries where moral rights apply, the Licensor waives his right to exercise his moral right to the extent allowed  by law in order to make effective the licence of the economic rights here above listed.  The Licensor grants to the Licensee royalty-free, non-exclusive usage rights to any patents held by the Licensor, to the  extent necessary to make use of the rights granted on the Work under this Licence. </p> <p>3.Communication of the Source Code  The Licensor may provide the Work either in its Source Code form, or as Executable Code. If the Work is provided as  Executable Code, the Licensor provides in addition a machine-readable copy of the Source Code of the Work along with  each copy of the Work that the Licensor distributes or indicates, in a notice following the copyright notice attached to  the Work, a repository where the Source Code is easily and freely accessible for as long as the Licensor continues to  distribute or communicate the Work. </p> <p>4.Limitations on copyright  Nothing in this Licence is intended to deprive the Licensee of the benefits from any exception or limitation to the  exclusive rights of the rights owners in the Work, of the exhaustion of those rights or of other applicable limitations  thereto. </p> <p>5.Obligations of the Licensee  The grant of the rights mentioned above is subject to some restrictions and obligations imposed on the Licensee. Those  obligations are the following: </p> <p>Attribution right: The Licensee shall keep intact all copyright, patent or trademarks notices and all notices that refer to  the Licence and to the disclaimer of warranties. The Licensee must include a copy of such notices and a copy of the  Licence with every copy of the Work he/she distributes or communicates. The Licensee must cause any Derivative Work  to carry prominent notices stating that the Work has been modified and the date of modification. </p> <p>Copyleft clause: If the Licensee distributes or communicates copies of the Original Works or Derivative Works, this  Distribution or Communication will be done under the terms of this Licence or of a later version of this Licence unless  the Original Work is expressly distributed only under this version of the Licence \u2014 for example by communicating  \u2018EUPL v. 1.2 only\u2019. The Licensee (becoming Licensor) cannot offer or impose any additional terms or conditions on the  Work or Derivative Work that alter or restrict the terms of the Licence. </p> <p>Compatibility clause: If the Licensee Distributes or Communicates Derivative Works or copies thereof based upon both  the Work and another work licensed under a Compatible Licence, this Distribution or Communication can be done  under the terms of this Compatible Licence. For the sake of this clause, \u2018Compatible Licence\u2019 refers to the licences listed  in the appendix attached to this Licence. Should the Licensee's obligations under the Compatible Licence conflict with  his/her obligations under this Licence, the obligations of the Compatible Licence shall prevail. </p> <p>Provision of Source Code: When distributing or communicating copies of the Work, the Licensee will provide  a machine-readable copy of the Source Code or indicate a repository where this Source will be easily and freely available  for as long as the Licensee continues to distribute or communicate the Work.  Legal Protection: This Licence does not grant permission to use the trade names, trademarks, service marks, or names  of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and  reproducing the content of the copyright notice. </p> <p>6.Chain of Authorship  The original Licensor warrants that the copyright in the Original Work granted hereunder is owned by him/her or  licensed to him/her and that he/she has the power and authority to grant the Licence.  Each Contributor warrants that the copyright in the modifications he/she brings to the Work are owned by him/her or  licensed to him/her and that he/she has the power and authority to grant the Licence.  Each time You accept the Licence, the original Licensor and subsequent Contributors grant You a licence to their contributions  to the Work, under the terms of this Licence. </p> <p>7.Disclaimer of Warranty  The Work is a work in progress, which is continuously improved by numerous Contributors. It is not a finished work  and may therefore contain defects or \u2018bugs\u2019 inherent to this type of development.  For the above reason, the Work is provided under the Licence on an \u2018as is\u2019 basis and without warranties of any kind  concerning the Work, including without limitation merchantability, fitness for a particular purpose, absence of defects or  errors, accuracy, non-infringement of intellectual property rights other than copyright as stated in Article 6 of this  Licence.  This disclaimer of warranty is an essential part of the Licence and a condition for the grant of any rights to the Work. </p> <p>8.Disclaimer of Liability  Except in the cases of wilful misconduct or damages directly caused to natural persons, the Licensor will in no event be  liable for any direct or indirect, material or moral, damages of any kind, arising out of the Licence or of the use of the  Work, including without limitation, damages for loss of goodwill, work stoppage, computer failure or malfunction, loss  of data or any commercial damage, even if the Licensor has been advised of the possibility of such damage. However,  the Licensor will be liable under statutory product liability laws as far such laws apply to the Work. </p> <p>9.Additional agreements  While distributing the Work, You may choose to conclude an additional agreement, defining obligations or services  consistent with this Licence. However, if accepting obligations, You may act only on your own behalf and on your sole  responsibility, not on behalf of the original Licensor or any other Contributor, and only if You agree to indemnify,  defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against such Contributor by  the fact You have accepted any warranty or additional liability. </p> <p>10.Acceptance of the Licence  The provisions of this Licence can be accepted by clicking on an icon \u2018I agree\u2019 placed under the bottom of a window  displaying the text of this Licence or by affirming consent in any other similar way, in accordance with the rules of  applicable law. Clicking on that icon indicates your clear and irrevocable acceptance of this Licence and all of its terms  and conditions.  Similarly, you irrevocably accept this Licence and all of its terms and conditions by exercising any rights granted to You  by Article 2 of this Licence, such as the use of the Work, the creation by You of a Derivative Work or the Distribution  or Communication by You of the Work or copies thereof. </p> <p>11.Information to the public  In case of any Distribution or Communication of the Work by means of electronic communication by You (for example,  by offering to download the Work from a remote location) the distribution channel or media (for example, a website)  must at least provide to the public the information requested by the applicable law regarding the Licensor, the Licence  and the way it may be accessible, concluded, stored and reproduced by the Licensee. </p> <p>12.Termination of the Licence  The Licence and the rights granted hereunder will terminate automatically upon any breach by the Licensee of the terms  of the Licence.  Such a termination will not terminate the licences of any person who has received the Work from the Licensee under  the Licence, provided such persons remain in full compliance with the Licence. </p> <p>13.Miscellaneous  Without prejudice of Article 9 above, the Licence represents the complete agreement between the Parties as to the  Work.  If any provision of the Licence is invalid or unenforceable under applicable law, this will not affect the validity or  enforceability of the Licence as a whole. Such provision will be construed or reformed so as necessary to make it valid  and enforceable.  The European Commission may publish other linguistic versions or new versions of this Licence or updated versions of  the Appendix, so far this is required and reasonable, without reducing the scope of the rights granted by the Licence.  New versions of the Licence will be published with a unique version number.  All linguistic versions of this Licence, approved by the European Commission, have identical value. Parties can take  advantage of the linguistic version of their choice. </p> <p>14.Jurisdiction  Without prejudice to specific agreement between parties,  \u2014 any litigation resulting from the interpretation of this License, arising between the European Union institutions,  bodies, offices or agencies, as a Licensor, and any Licensee, will be subject to the jurisdiction of the Court of Justice  of the European Union, as laid down in article 272 of the Treaty on the Functioning of the European Union,  \u2014 any litigation arising between other parties and resulting from the interpretation of this License, will be subject to  the exclusive jurisdiction of the competent court where the Licensor resides or conducts its primary business. </p> <p>15.Applicable Law  Without prejudice to specific agreement between parties,  \u2014 this Licence shall be governed by the law of the European Union Member State where the Licensor has his seat,  resides or has his registered office,  \u2014 this licence shall be governed by Belgian law if the Licensor has no seat, residence or registered office inside  a European Union Member State. </p> <pre><code>                                                     Appendix\n</code></pre> <p>\u2018Compatible Licences\u2019 according to Article 5 EUPL are:  \u2014 GNU General Public License (GPL) v. 2, v. 3  \u2014 GNU Affero General Public License (AGPL) v. 3  \u2014 Open Software License (OSL) v. 2.1, v. 3.0  \u2014 Eclipse Public License (EPL) v. 1.0  \u2014 CeCILL v. 2.0, v. 2.1  \u2014 Mozilla Public Licence (MPL) v. 2  \u2014 GNU Lesser General Public Licence (LGPL) v. 2.1, v. 3  \u2014 Creative Commons Attribution-ShareAlike v. 3.0 Unported (CC BY-SA 3.0) for works other than software  \u2014 European Union Public Licence (EUPL) v. 1.1, v. 1.2  \u2014 Qu\u00e9bec Free and Open-Source Licence \u2014 Reciprocity (LiLiQ-R) or Strong Reciprocity (LiLiQ-R+).</p> <p>The European Commission may update this Appendix to later versions of the above licences without producing  a new version of the EUPL, as long as they provide the rights granted in Article 2 of this Licence and protect the  covered Source Code from exclusive appropriation.  All other changes or additions to this Appendix require the production of a new EUPL version. </p>"},{"location":"pipeline/","title":"Pipeline","text":"<p>The software can perform the following pipeline:</p> <pre><code>%%{\n  init: {\n    'theme': 'base',\n    'themeVariables': {\n      'primaryColor': '#BB2528',\n      'primaryTextColor': '#000000',\n      'primaryBorderColor': '#7C0000',\n      'lineColor': '#F8B229',\n      'secondaryColor': '#006100',\n      'tertiaryColor': '#fff'\n    }\n  }\n}%%\nflowchart TD;\n    %% --- Reference Data ---\n    %% Inspire grid generation\n    subgraph Ingestion\n    InspireGridGeneration--&gt;InspireGridData[(Inspire Grid\\n)];\n    end\n    %% --- MNO Data ---\n    %% -- NETWORK --\n    subgraph Cleaning\n    %% RAW Network cleaning\n    PhysicalNetworkRAWData[(MNO-Network\\nPhysical\\nRAW)]--&gt;NetworkCleaning--&gt;PhysicalNetworkData[(MNO-Network\\nPhysical)];\n    NetworkCleaning--&gt;NetworkQAData[(MNO-Network\\nQuality Checks)];\n    %% RAW Network QA\n    NetworkQAData--&gt;NetworkQualityWarnings--&gt;NetworkWarnings[(Network\\nQuality Warnings)];\n    NetworkQualityWarnings--&gt;NetworkReports{{Network\\nQA \\ngraph data\\ncsv}};\n    %% -- EVENTS --\n    %% RAW Events cleaning\n    EventsRAWData[(MNO-Event\\nRAW)]--&gt;EventCleaning--&gt;EventsData[(MNO-Event)];\n    EventCleaning--&gt;EventsQA[(MNO-Event\\nQuality Checks)]--&gt;EventQualityWarnings;\n    EventCleaning--&gt;EventsQAfreq[(MNO-Event\\nQuality Checks\\nfrequency)];\n    %% RAW Events Warnings\n    EventsQAfreq--&gt;EventQualityWarnings;\n    EventQualityWarnings--&gt;EventsWarnings[(Events\\nQuality Warnings)];\n    EventQualityWarnings--&gt;EventsReports{{Event QA \\ngraph data\\ncsv}};\n    %% Event Semantic Checks\n    EventsData--&gt;SemanticCleaning--&gt;EventsSemanticCleaned[(Events\\nSemantic\\nCleaned)];\n    PhysicalNetworkData--&gt;SemanticCleaning;\n    SemanticCleaning--&gt;DeviceSemanticQualityMetrics[(Device\\nSemantic\\nQuality\\nMetrics)];\n    %% Event Semantic Warnings\n    EventsSemanticCleaned--&gt;SemanticQualityWarnings--&gt;EventSemanticWarnings[(Event\\nSemantic\\nQuality\\nWarnings)];\n    DeviceSemanticQualityMetrics--&gt;SemanticQualityWarnings--&gt;EventSemanticReports{{Event Semantic QA \\ngraph data\\ncsv}};\n    %% Device activity Statistics\n    EventsData--&gt;DeviceActivityStatistics--&gt;DeviceActivityStatisticsData[(Device\\nActivity\\nStatistics)];\n    PhysicalNetworkData--&gt;DeviceActivityStatistics;\n    end\n\n\n    subgraph Network Processing\n    %% Cell Footprint\n    PhysicalNetworkData--&gt;CellFootprintEstimation--&gt;CellFootprintData[(Cell Footprint\\nValues)];\n    CellFootprintEstimation--&gt;CellIntersectionGroupsData[(Cell Intersection Groups)];\n    %% Cell Connection Probability\n    CellFootprintData--&gt;CellConnectionProbabilityEstimation;\n    InspireGridData--&gt;CellConnectionProbabilityEstimation--&gt;CellConnectionProbabilityData[(Cell Connection\\nProbability)];\n    end\n\n    subgraph Daily Products\n    %% --- Daily Processing module ---\n    %% Daily Permanence Score\n    EventsSemanticCleaned--&gt;DailyPermanenceScore--&gt;DPSdata[(Daily\\nPermanence\\nScore\\nData)];\n    CellFootprintData--&gt;DailyPermanenceScore;\n    %% Continuous Time segmentation\n    EventsSemanticCleaned--&gt;ContinuousTimeSegmentation--&gt;DailyCTSdata[(Daily\\nContinuous\\nTime\\nSegmentation)];\n    CellFootprintData--&gt;ContinuousTimeSegmentation;\n    CellIntersectionGroupsData--&gt;ContinuousTimeSegmentation;\n    %% Present Population Estimation\n    EventsSemanticCleaned--&gt;PresentPopulation--&gt;PresentPopulationData[(Present\\nPopulation)];\n    CellConnectionProbabilityData--&gt;PresentPopulation;\n    InspireGridData--&gt;PresentPopulation;\n    end\n\n    %% --- Longitudinal module ---\n    subgraph MidTerm Products\n    %% Midterm Permanence Score\n    HolidayData[(Holiday\\nData)]\n    DPSdata--&gt;MidTermPermanenceScore--&gt;MPSdata[(MidTerm\\nPermanence\\nScore\\nData)];\n    HolidayData--&gt;MidTermPermanenceScore;\n    end\n\n    %% [LONGTERM]\n    subgraph LongTerm Products\n    %% Longterm Permanence Score\n    MPSdata--&gt;LongTermPermanenceScore--&gt;LPSdata[(LongTerm\\nPermanence\\nScore\\nData)];\n    LPSdata--&gt;UsualEnvironmentLabelling--&gt;UELdata[(UsualEnvironment\\nLabelling\\nData)];\n    UELdata--&gt;UsualEnvironmentAggregation--&gt;UEAdata[(UsualEnvironment\\nAggregation\\nData)];\n    InspireGridData--&gt;UsualEnvironmentAggregation;\n    end\n\n    subgraph Final Product Pipeline\n    %% Spatial Aggregation\n    PresentPopulationData--&gt;SpatialAggregation--&gt;PresentPopulationZoneData[(Present\\nPopulation\\nZone)];\n    UEAdata--&gt;SpatialAggregation--&gt;UEAZoneData[(UsualEnvironment\\nAggregation\\nZone)]\n\n    %% Estimation\n    PresentPopulationZoneData--&gt;Estimation--&gt;PresentPopulationEstimationData[(Present\\nPopulation\\Estimation)];\n    UEAZoneData--&gt;Estimation--&gt;UEAEstimationData[(UsualEnvironment\\nAggregation\\Estimation)]\n\n    %% K-Anonymity\n    PresentPopulationEstimationData--&gt;K_Anonymity--&gt;PresentPopulationK_AnonymityData[(Present\\nPopulation\\K_Anonymity)];\n    UEAEstimationData--&gt;K_Anonymity--&gt;UEAK_AnonymityData[(UsualEnvironment\\nAggregation\\K_Anonymity)]\n    end\n\n    classDef green fill:#229954,stroke:#333,stroke-width:2px;\n    classDef light_green fill:#AFE1AF,stroke:#333,stroke-width:1px;\n    classDef bronze fill:#CD7F32,stroke:#333,stroke-width:2px;\n    classDef silver fill:#adadad,stroke:#333,stroke-width:2px;\n    classDef light_silver fill:#dcdcdc,stroke:#333,stroke-width:2px;\n    classDef gold fill:#FFD700,stroke:#333,stroke-width:2px;\n\n    %% ++++++++++++++++ BRONZE ++++++++++++++++\n    class PhysicalNetworkRAWData,EventsRAWData bronze\n    class HolidayData bronze\n\n    %% ++++++++++++++++ SILVER ++++++++++++++++\n    %% --- Reference Data ---\n    class InspireGridData light_silver\n    %% --- MNO Data ---\n    %% -- NETWORK --\n    class PhysicalNetworkData light_silver\n    class NetworkQAData,NetworkWarnings silver\n    class CellFootprintData,CellConnectionProbabilityData,CellIntersectionGroupsData light_silver\n    %% -- EVENTS --\n    %% event cleaning\n    class EventsData light_silver\n    class EventsQA,EventsQAfreq,EventsWarnings silver\n\n    %% event deduplicated\n    class EventsDeduplicated light_silver\n    class EventsDeduplicatedQA,EventsDeduplicatedQAfreq,EventsDeduplicatedWarnings silver\n\n    %% device activity statistics\n    class DeviceActivityStatisticsData light_silver\n    %% events semantic clean\n    class EventsSemanticCleaned light_silver\n    class DeviceSemanticQualityMetrics,EventSemanticWarnings silver\n\n    %% Present population\n    class PresentPopulationData light_silver\n    %% --- Daily Processing module ---\n    %% Daily Permanence Score\n    class DPSdata light_silver\n    %% Continuous Time segmentation\n    class DailyCTSdata light_silver\n    %% Longitudinal data\n    class MPSdata,LPSdata light_silver\n    %% UE data\n    class UELdata,UEAdata light_silver\n    %% Final components data\n    class PresentPopulationZoneData,UEAZoneData light_silver\n    class PresentPopulationEstimationData,UEAEstimationData light_silver\n\n    %% ++++++++++++++++ GOLD ++++++++++++++++\n    class NetworkReports gold\n    class EventsDeduplicatedReports gold\n    class EventsReports gold\n    class EventSemanticReports gold\n    class PresentPopulationK_AnonymityData,UEAK_AnonymityData gold\n\n    %% ---- Components ----\n    class InspireGridGeneration light_green\n    %% Net\n    class NetworkCleaning,CellFootprintEstimation,CellConnectionProbabilityEstimation light_green\n    class NetworkQualityWarnings green\n    %% Events\n    class EventCleaning,EventDeduplication,SemanticCleaning light_green\n    class EventQualityWarnings,EventQualityWarnings2,SemanticQualityWarnings green\n    %% -&gt; Device Activity Statistics should start from semantic cleaned events\n    class DeviceActivityStatistics light_green\n    %% Daily\n    class DailyPermanenceScore,ContinuousTimeSegmentation,PresentPopulation light_green\n    %% Longitudinal - Midterm\n    class MidTermPermanenceScore light_green\n    class LongTermPermanenceScore,UsualEnvironmentLabelling,UsualEnvironmentAggregation light_green\n    %% Final products pipeline\n    class SpatialAggregation,Estimation,K_Anonymity light_green</code></pre>"},{"location":"system_requirements/","title":"System Requirements","text":"<p>Multimno is a python library which requires the installation of additional system &amp; python libraries. In this section the requirements for executing this software are defined. </p> <p>In the case of using the docker image provided, the system only needs to comply with the Hardware Requirements and Docker requirements as the docker image will have all the software requirements already installed.</p>"},{"location":"system_requirements/#hardware-requirements","title":"Hardware requirements","text":""},{"location":"system_requirements/#minimum-requirements","title":"Minimum requirements","text":"<p>The hardware specification needed will vary depending on the input data volumetry. However, we recommend this settings as minimum requirements for a single node cluster:</p> <ul> <li>Cores: 4</li> <li>RAM: 16 Gb</li> <li>Disk: 32 Gb of free space</li> <li>OS: <ul> <li>Ubuntu 22.04 (Recommended)</li> <li>Mac 12.6</li> <li>Windows 11 + WSL2 with Ubuntu 22.04 </li> </ul> </li> </ul>"},{"location":"system_requirements/#software-requirements","title":"Software Requirements","text":""},{"location":"system_requirements/#os-libraries","title":"OS Libraries","text":"Library Version Python &gt;= 3.9 Java JDK &gt;= 11 Apache Spark 3.5.1 GDAL &gt;= 3.6.2"},{"location":"system_requirements/#spark-libraries-jars","title":"Spark Libraries (jars)","text":"Library Version Apache Sedona &gt;= 1.6.0 Geotools wrapper 28.2"},{"location":"system_requirements/#python-libraries","title":"Python Libraries","text":"Library Version numpy &gt;=1.24,&lt;1.27 pandas &gt;=2.0,&lt;2.3 pyarrow &gt;=17.0 requests 2.31.0 toml 0.10 apache-sedona &gt;= 1.6.0 geopandas 0.11.1 shapely 1.8.4 pyspark 3.5.1 py4j &gt;=0.10.9.7"},{"location":"system_requirements/#docker-requirements","title":"Docker requirements","text":"<p>In the case of using the docker image provided for single node execution the following requirements must be fulfilled:   - Docker-engine: &gt;=25.X   - Docker-compose: &gt;=2.24.X   - Internet connection to Ubuntu/Spark/Docker official repositories for building the docker image  </p>"},{"location":"DevGuide/","title":"Developer Guide","text":"<p>The developer guide contains two sections: - Contribute: Guidelines on how to contribute to the software. - Developer Guidelines: Guidelines and tips on how to develop and test the software.  </p>"},{"location":"DevGuide/1_contribute/","title":"Contribute","text":"<p>In this document the general rules and guidelines for contributing to the multimno repository are detailed.</p>"},{"location":"DevGuide/1_contribute/#source-control-strategy","title":"Source control strategy","text":"<p>This repository uses three principal branches for source control:</p> <ul> <li>main: Branch where the official releases are tagged. The HEAD of the branch corresponds to the    latest release of the software.  </li> <li>integration: Branch used for preproduction testing and validation from which a release to the main branch will be generated.</li> <li>development: Branch that centralizes the latest features developed in the repository. After enough features/bugs have been delivered to this branch, a snapshot will be created in the integration branch for testing before  generating a release.</li> </ul> <p>These three branches shall only accept changes by the repository administrator. Commits shall not be performed directly in these branches except for small hotfixes in the integration branch.</p> <p>All features and bug fixes will be developed in branches that origin from the development branch.</p>"},{"location":"DevGuide/1_contribute/#forking-the-repository","title":"Forking the repository","text":"<p>Developers that want to contribute to the multimno repository shall fork the repository with all its branches. This can  be done through the github website. After creating a fork of the repository, developers can clone the forked repo in  their computers.</p>"},{"location":"DevGuide/1_contribute/#create-an-issue-with-the-development-that-will-be-performed","title":"Create an Issue with the development that will be performed","text":"<p>First of all, Check if the issue you will develop already exists.  Then, create an issue in the multimno repository stating the objective of the development that will be performed. Templates for creating issues for features or fixes are provided in the repository.</p>"},{"location":"DevGuide/1_contribute/#creating-a-featurefix-branch","title":"Creating a feature/fix branch","text":"<p>Within the forked repository developers shall create a branch that originates from the development branch.  This branch shall have the following naming convention:</p> <ul> <li>feat_\\&lt;name&gt;: If it is a new feature.</li> <li>fix_\\&lt;name&gt;: If it is a bug solution.</li> </ul> <p>Please remember to keep the forked development branch up-to-date with the latest changes.</p> <p>Don't forget to look up the developer guidelines to check the code style, testing and development practices that shall be followed to develop new code for the multimno repository.</p>"},{"location":"DevGuide/1_contribute/#opening-a-pull-request","title":"Opening a pull request","text":"<p>Before opening a pull request in the multimno repository verify: * Latest development changes are merged into the branch that performs the pull request.  * All tests pass successfully. * All the new code is documented following the Google style docstrings. * New tests for the code developed are included and pass successfully.</p> <p>Use the github web to create a pull request. The pull request must deliver your developed branch to the development branch of the multimno repository. Associate the PR(pull request) to the previously created issue.</p>"},{"location":"DevGuide/1_contribute/#the-review-process-pull-request-closure","title":"The review process &amp; pull request closure","text":"<p>The repository administrators will review the pull request performed to the development branch. </p> <ul> <li>If the changes are accepted, they will be incorporated in the development branch and the pull request will be closed. </li> <li>If the changes are not accepted, the repository administrators may indicate as a comment in the pull request feedback  and modifications needed to accept the pull request. However, the pull request may be desestimated to which it  will be closed and changes will not be incorporated.</li> </ul>"},{"location":"DevGuide/2_dev_guidelines/","title":"Developer Guidelines","text":"<p>The repository contains a devcontainer configuration compatible with VsCode. This configuration will create a docker container with all the necessary libraries and configurations to develop and execute the source code. </p>"},{"location":"DevGuide/2_dev_guidelines/#development-environment-setup","title":"\ud83d\udee0\ufe0f Development Environment Setup","text":"<p>This repository provides a docker dev-container with a system completely configured to execute the source code as well as useful libraries and tools for the development of the multimno software.  Thanks to the dev-container, it is guaranteed that all developers are developing/executing code in the same environment.</p> <p>The dev-container specification is all stored inside the <code>.devcontainer</code> directory.</p>"},{"location":"DevGuide/2_dev_guidelines/#configuring-the-docker-container","title":"Configuring the docker container","text":"<p>Configuration parameters for building the docker image and for creating the docker container are specified in the configuration file <code>.devcontainer/.env</code> file. This file contains user specific container configuration (like the path in the host machine to the data). As this file will change  for each developer it is ignored for the git version control and must be created after cloning the repository.</p> <p>A <code>template file</code> is stored in this repostiory. You can use this file as a baseline copying it to the <code>.devcontainer</code> directory.</p> <pre><code>cp resources/templates/dev_container_template.env .devcontainer/.env\n</code></pre> <p>Please edit the <code>.devcontainer/.env</code> file <code>Docker run parameters</code> section  with your preferences:</p> <pre><code># ------------------- Docker Build parameters -------------------\nPYTHON_VERSION=3.11 # Python version.\nJDK_VERSION=17 # Java version.\nSPARK_VERSION=3.4.1 # Spark/Pyspark version.\nSCALA_VERSION=2.12 # Spark dependency.\nSEDONA_VERSION=1.5.1 # Sedona\nGEOTOOLS_WRAPPER_VERSION=28.2 # Sedona dependency\n\n# ------------------- Docker run parameters -------------------\nCONTAINER_NAME=multimno_dev_container # Container name.\nDATA_DIR=../sample_data # Path of the host machine to the data to be used within the container.\nSPARK_LOGS_DIR=../sample_data/logs # Path of the host machine to where the spark logs will be stored.\nJL_PORT=8888 # Port of the host machine to deploy a jupyterlab.\nJL_CPU=4 # CPU cores of the container.\nJL_MEM=16g # RAM of the container.\n</code></pre>"},{"location":"DevGuide/2_dev_guidelines/#starting-the-dev-environment","title":"Starting the dev environment","text":""},{"location":"DevGuide/2_dev_guidelines/#vscode","title":"VsCode","text":"<p>Prerequisite: Dev-Container/Docker extension</p> <p>In VsCode: F1 -&gt; Dev Containers: Rebuild and Reopen in container</p>"},{"location":"DevGuide/2_dev_guidelines/#manual","title":"Manual","text":""},{"location":"DevGuide/2_dev_guidelines/#docker-image-creation","title":"Docker image creation","text":"<p>Execute the following command: <pre><code>docker compose -f .devcontainer/docker-compose.yml --env-file=.devcontainer/.env build\n</code></pre></p>"},{"location":"DevGuide/2_dev_guidelines/#docker-container-creation","title":"Docker container creation","text":"<p>Create a container and start a shell session in it with the commands: <pre><code>docker compose -f .devcontainer/docker-compose.yml --env-file=.devcontainer/.env up -d\ndocker exec -it multimno_dev_container bash\n</code></pre></p>"},{"location":"DevGuide/2_dev_guidelines/#stopping-the-dev-environment","title":"Stopping the dev environment","text":""},{"location":"DevGuide/2_dev_guidelines/#vscode_1","title":"VsCode","text":"<p>Closing VsCode will automatically stop the devcontainer.</p>"},{"location":"DevGuide/2_dev_guidelines/#manual_1","title":"Manual","text":"<p>Exit the terminal with:</p> <p>Ctrl+D or writing <code>exit</code></p>"},{"location":"DevGuide/2_dev_guidelines/#deleting-the-dev-environment","title":"Deleting the dev environment","text":"<p>Delete the container created with: <pre><code>docker compose -f .devcontainer/docker-compose.yml --env-file=.devcontainer/.env down\n</code></pre></p>"},{"location":"DevGuide/2_dev_guidelines/#execution","title":"\ud83d\udc0e Execution","text":""},{"location":"DevGuide/2_dev_guidelines/#launching-a-single-component","title":"Launching a single component","text":"<p>In a terminal execute the command:  </p> <pre><code>spark-submit multimno/main.py &lt;component_id&gt; &lt;path_to_general_config&gt; &lt;path_to_component_config&gt; \n</code></pre> <p>Example:  </p> <pre><code>spark-submit multimno/main.py SyntheticEvents pipe_configs/configurations/general_config.ini pipe_configs/configurations/synthetic_events/synth_config.ini \n</code></pre>"},{"location":"DevGuide/2_dev_guidelines/#launching-a-pipeline","title":"Launching a pipeline","text":"<p>In a terminal execute the command:  </p> <pre><code>python multimno/orchestrator.py &lt;pipeline_json_path&gt;\n</code></pre> <p>Example:  </p> <pre><code>python multimno/orchestrator.py pipe_configs/pipelines/pipeline.json \n</code></pre>"},{"location":"DevGuide/2_dev_guidelines/#monitoringdebug","title":"\ud83d\udd0d Monitoring/Debug","text":""},{"location":"DevGuide/2_dev_guidelines/#launching-a-spark-history-server","title":"Launching a spark history server","text":"<p>The history server will access SparkUI logs stored at the path ${SPARK_LOGS_DIR} defined in the <code>.devcontainer/.env</code> file.</p> <p>Starting the history server <pre><code>start-history-server.sh \n</code></pre> Accesing the history server * Go to the address http://localhost:18080</p>"},{"location":"DevGuide/2_dev_guidelines/#style","title":"\ud83e\udeb6 Style","text":""},{"location":"DevGuide/2_dev_guidelines/#coding-style","title":"Coding style","text":"<p>The code shall follow the standard PEP 8 which is the coding style proposed for writing clean, readable, and maintainable Python code.  It was created to promote consistency in Python code and make it easier for developers to collaborate on projects. </p> <p>PEP 8 official guide</p>"},{"location":"DevGuide/2_dev_guidelines/#docstring-style","title":"Docstring style","text":"<p>The docstrings written in the code shall follow the Google Docstrings style. Adhering  to a unique docstring style  guarantees consistency within software development in a project. Google Docstrings are the most popular convention for  docstrings which facilitates readability and collaboration in open-source projects. </p> <p>Google Docstrings official guide</p>"},{"location":"DevGuide/2_dev_guidelines/#code-cleaning","title":"\ud83e\uddfc Code cleaning","text":"<p>Developed code shall be formatted and jupyter notebooks shall be cleaned of outputs to guarantee consistency and reduce  unnecessary differences between commits.</p>"},{"location":"DevGuide/2_dev_guidelines/#code-linting","title":"Code Linting","text":"<p>The python code generated shall be formatted with black. For formatting all source code execute the following command:</p> <pre><code>black -l 120 multimno tests/test_code/\n</code></pre>"},{"location":"DevGuide/2_dev_guidelines/#clean-jupyter-notebooks","title":"Clean jupyter notebooks","text":"<pre><code>find ./notebooks/ -type f -name \\*.ipynb | xargs jupyter nbconvert --clear-output --inplace\n</code></pre>"},{"location":"DevGuide/2_dev_guidelines/#code-security-scan","title":"Code Security Scan","text":"<pre><code>bandit -r multimno\n</code></pre>"},{"location":"DevGuide/2_dev_guidelines/#testing","title":"\ud83e\uddea Testing","text":""},{"location":"DevGuide/2_dev_guidelines/#launch-tests","title":"Launch Tests","text":""},{"location":"DevGuide/2_dev_guidelines/#manual_2","title":"Manual","text":"<pre><code>pytest tests/test_code/multimno\n</code></pre>"},{"location":"DevGuide/2_dev_guidelines/#vscode_2","title":"VsCode","text":"<p>1) Open the test view at the left panel.  2) Launch tests.</p>"},{"location":"DevGuide/2_dev_guidelines/#generate-test-coverage-reports","title":"Generate test &amp; coverage reports","text":"<p>Launch the command</p> <pre><code>pytest --cov-config=tests/.coveragerc \\\n    --cov-report=\"html:docs/autodoc/coverage\" \\\n    --cov=multimno --html=docs/autodoc/test_report.md \\\n    --self-contained-html tests/test_code/multimno\n</code></pre> <p>Test reports will be stored in the dir: <code>docs/autodoc</code></p>"},{"location":"DevGuide/2_dev_guidelines/#see-coverage-in-ide-vscode-extension","title":"See coverage in IDE (VsCode extension)","text":"<p>1) Launch tests with coverage to generate the coverage report (xml) <pre><code>pytest --cov-report=\"xml\" --cov=multimno tests/test_code/multimno\n</code></pre> 1) Install the extension: Coverage Gutters 2) Right click and select Coverage Gutters: Watch</p> <p>Note: You can see the coverage percentage at the bottom bar</p>"},{"location":"DevGuide/2_dev_guidelines/#code-documentation","title":"\ud83d\udcc4 Code Documentation","text":""},{"location":"DevGuide/2_dev_guidelines/#documentation-server-debugmkdocs","title":"Documentation server Debug(Mkdocs)","text":"<p>A code documentation can be deployed using mkdocs backend. </p> <p>1) Create documentation (This will launch all tests) <pre><code>./resources/scripts/generate_docs.sh\n</code></pre> 2) Launch doc server</p> <p><pre><code>mkdocs serve\n</code></pre> and navigate to the address: http://127.0.0.1:8000</p>"},{"location":"DevGuide/2_dev_guidelines/#documentation-deploy-mike","title":"Documentation deploy (mike)","text":"<p>Set <code>latest</code> as default version <pre><code>mike set-default --push latest\n</code></pre></p> <p>Deploy a version of the documentation with:</p> <pre><code>mike deploy --push --update-aliases &lt;version&gt; latest\n</code></pre> <p>Example:</p> <pre><code>mike deploy --push --update-aliases 0.2 latest\n</code></pre>"},{"location":"UserManual/","title":"User Manual","text":"<p>This document presents the user manual of the multimno software. Three sections are included: - Configuration: Section explaining the configuration values for all the components of the pipeline. - Setup: Section explaining how to install and deploy the software. - Execution: Section explaining how to execute the software.  </p>"},{"location":"UserManual/execution/","title":"Execution","text":"<p>The multimno software is a python application that launches a single component with a given configuration.  This atomic design allows the application to be integrated with multiple orchestration software. </p> <p>At the moment a python script called <code>orchestrator_multimno.py</code> is provided which will execute a  pipeline of components sequentially using <code>spark-submit</code> commands.</p> <p>The execution process can be divided into four steps:</p>"},{"location":"UserManual/execution/#1-setting-input-data","title":"1. Setting - Input Data","text":"<p>The following input data is required to execute the multimno software addtionally from the MNO Data:</p>"},{"location":"UserManual/execution/#national-holidays-data","title":"National Holidays data","text":"<p>National holiday data is required to execute the software. This data must be in parquet format and contain the following schema:</p> Column Format Value iso2 str Country code in iso2 format date date Date of the festivity in yyyy-mm-dd format name str Festivity description <p>Example: | iso2 | date   | name           | | -------- | ---------- | ------------------ | | ES       | 2022-01-01 | A\u00f1o Nuevo          | | ES       | 2022-01-06 | Epifan\u00eda del Se\u00f1or | | ES       | 2022-04-15 | Viernes Santo      |</p> <p>The path to this data must be specified in the <code>holiday_calendar_data_bronze</code> variable under the section <code>[Paths.Bronze]</code>  in the general_configuration.ini file of the apllication.</p>"},{"location":"UserManual/execution/#2-pipeline-definition","title":"2. Pipeline definition","text":"<p>The pipeline is defined as a json file that glues all the configuration files  and defines the sequential execution order of the components. The structure is as follows:</p> <ul> <li>general_config_path: Path to the general configuration file</li> <li>spark_submit_args: List containing arguments that will be passed to the spark-submit command. It can be empty.</li> <li>pipeline: List containing the order in which the components will be executed. Each item is composed of the values:<ul> <li>component_id: Id of the component to be executed.</li> <li>component_config_path: Path to the component configuration file.</li> </ul> </li> </ul> <p>Example: <pre><code>{\n    \"general_config_path\": \"pipe_configs/configurations/general_config.ini\",\n    \"spark_submit_args\": [\n        \"--master=spark://spark-master:7077\",\n        \"--packages=org.apache.sedona:sedona-spark-3.5_2.12:1.6.0,org.datasyslab:geotools-wrapper:1.6.0-28.2\"\n    ],\n    \"pipeline\": [\n        {\n            \"component_id\": \"InspireGridGeneration\",\n            \"component_config_path\": \"pipe_configs/configurations/grid/grid_generation.ini\"\n        },\n        {\n            \"component_id\": \"EventCleaning\",\n            \"component_config_path\": \"pipe_configs/configurations/event/event_cleaning.ini\"\n        }\n    ]\n}\n</code></pre></p> <p>Configuration for executing a demo pipeline is given in the file: <code>pipe_configs/pipelines/pipeline.json</code> This file contains the order of the execution of the pipeline components and references to its demo configuration files  that are given as well in the repository.</p>"},{"location":"UserManual/execution/#3-execution-configuration","title":"3. Execution Configuration","text":"<p>Each component of the pipeline to be executed must be configured to the user desired settings. It is recommended to take  the configurations defined in <code>pipe_configs/configurations</code> as a base and refine them using the configuration guide.</p>"},{"location":"UserManual/execution/#spark-configuration","title":"Spark configuration","text":"<p>spark-submit args</p> <p>The entrypoint for the pipeline execution: <code>orchestrator_multimno.py</code>, performs <code>spark-submit</code> commands to execute each component of the pipeline as a Spark Job. To define <code>spark-submit</code> arguments edit the <code>spark_submit_args</code> variable in the pipeline.json. </p> <p>This variable follows the same syntax as <code>spark-submit</code> arguments.</p> <ul> <li>Spark submit documentation: https://spark.apache.org/docs/latest/submitting-applications.html</li> </ul> <p>Spark Configuration</p> <p>To define Spark session specific configurations, edit the <code>[Spark]</code> section in the general_configuration file. If you want to change the Spark configuration for only one component in the pipeline, you can edit the the <code>[Spark]</code> section in the component configuration file which will override values defined in the general configuration file.</p> <ul> <li>Spark configuration documentation: https://spark.apache.org/docs/latest/configuration.html</li> </ul>"},{"location":"UserManual/execution/#4-execution","title":"4. Execution","text":""},{"location":"UserManual/execution/#pipeline-execution","title":"Pipeline execution","text":"<p>For executing a pipeline the <code>orchestrator_multimno.py</code> entrypoint shall be used. This takes as input a path to a json file with the pipeline definition as defined in the previous Pipeline definition section. </p> <p>Example: <pre><code>./orchestrator_multimno.py pipe_configs/pipelines/pipeline.json\n</code></pre></p> <p>Warning</p> <p>The <code>orchestrator_multimno.py</code> must be located in the same directory as the <code>main_multimno.py</code> file.</p>"},{"location":"UserManual/execution/#single-component-execution","title":"Single component execution","text":"<p>If you want to only launch a component you can perform manually the spark-submit command from a terminal using the <code>main_multimno.py</code> entrypoint:</p> <p>The entrypoint of the application is a main.py which receives the following positional parameters: - component_id: Id of the component that will be launched. - general_config_path: Path to the general configuration file of the application. - component_config_path: Path to the component configuration file.</p> <pre><code>spark-submit multimno/main_multimno.py &lt;component_id&gt; &lt;general_config_path&gt; &lt;component_config_path&gt;\n</code></pre> <p>Example:</p> <pre><code>spark-submit multimno/main_multimno.py InspireGridGeneration pipe_configs/configurations/general_config.ini pipe_configs/configurations/grid/grid_generation.ini\n</code></pre>"},{"location":"UserManual/setup_guide/","title":"Setup Guide","text":""},{"location":"UserManual/setup_guide/#introduction","title":"Introduction","text":"<p>The multimno software is a python application using the PySpark library to harness the power of Apache Spark, a fast and general-purpose cluster-computing system. PySpark provides an interface for Apache Spark in Python, enabling developers to utilize Spark's high-level APIs and distributed computing capabilities while working in the Python programming language. The Spark framework is critical to this application as it handles the distribution of data and computation across the cluster, ensures fault tolerance, and optimizes execution for performance gains. Deployment of a PySpark application can be done in two ways:</p> <p>1) Cluster mode: On a Cluster, where Spark distributes tasks across the nodes, allowing for parallel processing and efficient handling of large-scale data workloads. Recommended for production environments. 2) Local mode: On a single node, where Spark runs on a single machine. Recommended for development and testing environments. </p>"},{"location":"UserManual/setup_guide/#cluster-mode","title":"Cluster Mode","text":"<p>There are multiple ways of deploying a Spark cluster: Standalone, YARN managed, Kubernetes, Cloud managed services...  </p> <p>This guide will not enter in the specific steps for deploying a Spark cluster and only explain the requirements  and software installation steps in an existing cluster. All nodes in the cluster created shall satisfy the OS libraries requirements.</p> <p>Spark cluster mode official documentation: https://spark.apache.org/docs/latest/cluster-overview.html</p>"},{"location":"UserManual/setup_guide/#package-deploy-the-code","title":"Package &amp; deploy the code","text":"<p>There are two ways of deploying the code:</p>"},{"location":"UserManual/setup_guide/#whl-method","title":"WHL method","text":"<p>Package the code into a whl file that contains all the code and python dependencies required. Use the following  command to package it: <pre><code>python3 -m pip build\n</code></pre></p> <p>The python version and OS used to package the code must be the same of the nodes of the spark cluster.  The python library build must be installed beforehand. To install it use: <code>pip install build --upgrade</code></p> <p>Then, just install this package with all its dependencies into every node of the cluster with: <pre><code>pip install multimno*.whl\n</code></pre></p> <p>Internet connection is required to download all needed dependencies from the pypi repository</p>"},{"location":"UserManual/setup_guide/#zip-method","title":"ZIP method","text":"<p>Zip all code under <code>multimno</code> directory into a single file. Then send it to the Spark server through the  spark-submit configuration parameter: <code>--py-files=multimno.zip</code>. When using this method, the cluster must have the  python dependencies installed beforehand.</p>"},{"location":"UserManual/setup_guide/#install-dependencies","title":"Install Dependencies","text":"<p>Multimno software is a pyspark application that needs both java and python depencies intalled to run.</p>"},{"location":"UserManual/setup_guide/#java-dependencies","title":"Java Dependencies","text":"<p>The application uses the Apache Sedona engine to perform spatial calculations. In order to install this engine,  the jar files must be downloaded to the cluster. These files can be downloaded at execution time through the maven repository, specifying  them in the spark configuration, or they can be downloaded manually into the <code>$SPARK_HOME/jars</code> dir of every node in the cluster.</p> <p>Reference - Sedona installation: https://sedona.apache.org/1.5.1/setup/cluster/</p>"},{"location":"UserManual/setup_guide/#python-dependencies","title":"Python Dependencies","text":"<p>A requirement of a pyspark application is that the python version must be alligned for all the cluster.</p> <p>The software needs a set of python dependencies to be installed in every node of the cluster. These dependencies will be  installed automatically when using the WHL method. If you are using the ZIP method  you will need to install them manually into every node of the cluster through the requirements file. Install the dependencies of the  <code>pyproject.toml</code> file into every node of the cluster.</p>"},{"location":"UserManual/setup_guide/#local-mode","title":"Local Mode","text":"<p>There are two ways of setting up a system for executing the source code in local mode:   1) Building the docker image provided. (Recommended)   2) Installing and setting up all required system libraries.  </p>"},{"location":"UserManual/setup_guide/#docker-setup","title":"Docker setup","text":"<p>A Dockerfile is provided to build a docker image with all necessary dependencies for the code execution.</p>"},{"location":"UserManual/setup_guide/#installing-docker-software","title":"Installing docker software","text":"<p>To use the docker image it is necessary to have the docker engine installed. Please follow the official docker  guide to set it up in your system: -  Official guide: Click here</p>"},{"location":"UserManual/setup_guide/#docker-image-creation","title":"Docker image creation","text":"<p>Execute the following command: <pre><code>docker build -t multimno:1.0-prod --target=multimno-prod .\n</code></pre></p>"},{"location":"UserManual/setup_guide/#docker-container-creation","title":"Docker container creation","text":"<p>Run an example pipeline within a container <pre><code>docker run --rm --name=multimno-container -v \"${PWD}/sample_data:/opt/data\" -v \"${PWD}/pipe_configs:/opt/app/pipe_configs\" multimno:1.0-prod pipe_configs/pipelines/pipeline.json\n</code></pre></p> <p>Run a container in interactive mode <pre><code>docker run -it --name=multimno-container -v \"${PWD}/sample_data:/opt/data\" -v \"${PWD}/pipe_configs:/opt/app/pipe_configs\" --entrypoint=bash multimno:1.0-prod \n</code></pre></p> <p>After performing this command your shell(command-line) will be inside the container and you can perform  the execution steps to try out the code.</p>"},{"location":"UserManual/setup_guide/#clean-up","title":"Clean up","text":"<p>Exit the terminal with:</p> <p>Ctrl+D or writing <code>exit</code></p> <p>Delete the container created with: <pre><code>docker rm multimno-container\n</code></pre></p> <p>Delete the docker image with: <pre><code>docker rmi multimno:1.0-prod\n</code></pre></p>"},{"location":"UserManual/setup_guide/#software-setup","title":"Software setup","text":"<p>The software is aimed to be executed in a Linux OS. It is recommended to use Ubuntu 22.04 LTS but these steps should also work in MAC OS 12.6(or superior) and in Windows 11 with WSL2 and setting up as the distro of WSL Ubuntu 22.04.</p>"},{"location":"UserManual/setup_guide/#install-system-libs","title":"Install system libs","text":"<pre><code>sudo apt-get update &amp;&amp; \\\n    sudo apt-get install -y --no-install-recommends \\\n      sudo \\\n      openjdk-17-jdk \\\n      build-essential \\\n      software-properties-common \\\n      openssh-client openssh-server \\\n      gdal-bin \\\n      libgdal-dev \\\n      ssh\n</code></pre>"},{"location":"UserManual/setup_guide/#download-spark-source-code","title":"Download Spark source code","text":"<pre><code>SPARK_VERSION=3.5.1\nexport SPARK_HOME=${SPARK_HOME:-\"/opt/spark\"}\nmkdir -p ${SPARK_HOME}\ncd ${SPARK_HOME}\nwget https://dlcdn.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.tgz \\\n  &amp;&amp; tar -xvzf spark-${SPARK_VERSION}-bin-hadoop3.tgz --directory /opt/spark --strip-components 1 \\\n  &amp;&amp; rm -rf spark-${SPARK_VERSION}-bin-hadoop3.tgz\nexport PATH=\"${PATH}:$SPARK_HOME/bin:$SPARK_HOME/sbin\"\n</code></pre>"},{"location":"UserManual/setup_guide/#install-python-requirements","title":"Install python requirements","text":"<pre><code>pip install --upgrade pip uv\nuv pip install -r pyproject.toml\n</code></pre> <p>You can use a virtualenv for avoiding conflicts with other python libraries.</p>"},{"location":"UserManual/setup_guide/#install-spark-dependencies","title":"Install Spark dependencies","text":"<pre><code>SCALA_VERSION=2.12\nSEDONA_VERSION=1.5.1\nGEOTOOLS_WRAPPER_VERSION=28.2\nchmod +x ./resources/scripts/install_sedona_jars.sh\n./resources/scripts/install_sedona_jars.sh ${SPARK_VERSION} ${SCALA_VERSION} ${SEDONA_VERSION} ${GEOTOOLS_WRAPPER_VERSION} \n</code></pre>"},{"location":"UserManual/setup_guide/#setup-spark-configuration","title":"Setup spark configuration","text":"<pre><code>cp conf/spark-defaults.conf \"$SPARK_HOME/conf/spark-defaults.conf\"\ncp conf/log4j2.properties \"$SPARK_HOME/conf/log4j2.properties\"\n</code></pre>"},{"location":"UserManual/configuration/","title":"Configuration","text":""},{"location":"UserManual/configuration/#configuration-section-structure","title":"Configuration Section Structure","text":"<p>The configuration section is divided in four sections:  </p> <ul> <li>Pipeline: Main pipeline components to generate indicators from Mno Data.  </li> <li>Optional: Components that are optional to the pipeline execution. They enrich some data objects which may lead to quality improvements of the final results but are not essential to the pipeline.  </li> <li>QualityWarnings: Components that analyze quality metrics of data objects.  </li> <li>SyntheticMnoData: Components that are used to create synthetic Mno Data. Mainly used for testing purposes.  </li> </ul>"},{"location":"UserManual/configuration/#configuration-files-used","title":"Configuration files used","text":"<p>The multimno application requires from multiple configuration files.  </p> <ul> <li>One general configuration file describing general parameters like file paths, logging, spark and  common values for all components in the pipeline.  </li> <li>A configuration file per each component of the pipeline with configuration parameters exclusive to the component. Values defined in these files can override values defined in the general configuration file.</li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/0_general_configuration/","title":"General Configuration","text":"<p>The general configuration file contains transversal settings for all the pipeline. It is an INI file composed of three main sections:</p> <p>General: Section containing configuration values which are pipeline wide. </p> <p>Logging: Section which contains the logger settings.</p> <p>Paths: Section containing the definition of all paths to be used.</p> <p>Spark: Apache Spark configuration values. </p>"},{"location":"UserManual/configuration/1_Pipeline/0_general_configuration/#general-section","title":"General section","text":"<ul> <li>local_timezone: str, Specifies the time zone in IANA format (e.g., Europe/Madrid) to represent regional time settings.</li> <li>local_mcc: int, MCC code of the country of study.</li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/0_general_configuration/#logging-section","title":"Logging Section","text":"<ul> <li>level: The minimum logging level for console output. Messages with a level equal to or higher than this will be output to the console.</li> <li>format: The format of the log messages for console output.</li> <li>datefmt: The format of the date/time in the log messages for console output.</li> <li>file_log_level: The minimum logging level for file output. Messages with a level equal to or higher than this will be written to the log file.</li> <li>file_format: The format of the log messages for file output.</li> <li>report_path: The path where the log file will be written.</li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/0_general_configuration/#paths-section","title":"Paths Section","text":"<ul> <li>home_dir: The main directory for the application. </li> <li>lakehouse_dir: The directory for the \"lakehouse\" data.</li> <li>exports_dir: The directory where exported data is stored.</li> <li>landing_dir: The directory for the \"landing\" data within the lakehouse.</li> <li>bronze_dir: The directory for the \"bronze\" data within the lakehouse.</li> <li>silver_dir: The directory for the \"silver\" data within the lakehouse.</li> <li>gold_dir: The directory for the \"gold\" data within the lakehouse.</li> <li>app_log_dir: The directory where application logs are stored.</li> <li>spark_log_dir: The directory where Spark logs are stored (Output for the Spark History Server).</li> <li>silver_quality_metrics_dir: The directory for quality metrics of the \"silver\" data.</li> <li>silver_quality_warnings_dir: The directory for quality warnings of the \"silver\" data.</li> <li>gold_quality_warnings_dir: The directory for quality warnings of the \"gold\" data.</li> <li>spark_checkpoint_dir: The directory for Spark checkpoints.</li> </ul> <p>All the [Paths] subsections (Paths.Landing, Paths.Bronze, Paths.Silver, Paths.Gold), contain the different paths for the output of the pipeline components. It is recommended to use the default values as all this paths are relative to the ones specified in the [Paths] section.</p>"},{"location":"UserManual/configuration/1_Pipeline/0_general_configuration/#spark-section","title":"Spark Section","text":"<p>Parameters defined in this section will be used to create the spark session. Values supported are based in: Spark configuration. As an exception, the parameter <code>session_name</code> has been included which identifies the name of the spark session.</p> <p>Example:</p> <pre><code>[General]\nlocal_timezone = Europe/Madrid\nlocal_mcc = 214\n\n[Logging]\nlevel = INFO\nformat= %(asctime)-20s %(message)s\ndatefmt = %y-%m-%d %H:%M:%S\nfile_log_level = INFO\nfile_format = %(asctime)-20s |%(name)s| %(levelname)s: %(message)s\nreport_path = /opt/data/app_log\n\n[Paths]\n# Main paths\nhome_dir = /opt/data\nlakehouse_dir = ${Paths:home_dir}/lakehouse\nexports_dir = ${Paths:home_dir}/exports\n# Lakehouse\nlanding_dir = ${Paths:lakehouse_dir}/landing\nbronze_dir = ${Paths:lakehouse_dir}/bronze\nsilver_dir = ${Paths:lakehouse_dir}/silver\ngold_dir = ${Paths:lakehouse_dir}/gold\n# Logs dir\napp_log_dir = ${Paths:home_dir}/log\nspark_log_dir = ${Paths:home_dir}/spark_log\n# Quality metrics dir\nsilver_quality_metrics_dir = ${Paths:silver_dir}/quality_metrics\n# Quality warnings dir\nsilver_quality_warnings_dir = ${Paths:silver_dir}/quality_warnings\n# Quality graphs\ngold_quality_warnings_dir = ${Paths:gold_dir}/quality_warnings\n# spark\nspark_checkpoint_dir = ${Paths:home_dir}/spark/spark_checkpoint\n\n[Spark]\n# parameters in this section are passed to Spark except session_name \nsession_name = MultiMNO\nspark.master = local[*]\nspark.driver.host = localhost\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/10_daily_permanence_score/","title":"DailyPermanenceScore Configuration","text":"<p>To initialise and run the component two configs are used -  <code>general_config.ini</code> and component\u2019s config <code>daily_permanence_score.ini</code>. In  <code>general_config.ini</code> all paths to the data objects used by DailyPermanenceScore component shall be specified. An example with the specified paths is shown below:</p> <pre><code>[Paths.Silver]\n...\nevent_data_silver_flagged = ${Paths:silver_dir}/mno_events_flagged\ncell_footprint_data_silver = ${Paths:silver_dir}/cell_footprint\ndaily_permanence_score_data_silver = ${Paths:silver_dir}/daily_permanence_score\n...\n</code></pre> <p>Below there is a description of the sub component\u2019s config  - <code>daily_permanence_score.ini</code>. </p> <p>Parameters are as follows:</p> <p>Under  <code>[DailyPermanenceScore]</code> config section: </p> <ul> <li> <p>data_period_start - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-08), the date from which start DailyPermanenceScore processing. Make sure events and cell footprint data are available for all the dates between data_period_start and data_period_end, as well as for the previous day to data_period_start and the posterior day to data_period_end. </p> </li> <li> <p>data_period_end - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-15), the date till which we will perform DailyPermanenceScore processing. Make sure events and cell footprint data are available for all the dates between data_period_start and data_period_end, as well as for the previous day to data_period_start and the posterior day to data_period_end. </p> </li> <li> <p>time_slot_number - integer, can take the values 24, 48, or 96. Number of equal-length time slots in which to divide each date for the daily permanence score calculation. Recommended value: 24, so that the day is divided in 24 1-hour time slots. The values 48 and 96 result in 30-min and 15-min time slots, respectively.</p> </li> <li> <p>max_time_thresh - integer, in seconds. In case of 2 consecutive events taking place in different cells, if the time difference between the events is lower than this threshold, then the both the first event's assigned end time and the second event's assigned start time will be equal to the average between the first and second event's timestamps. If the time difference between the 2 consecutive events is higher than this threshold, then the assigned end time for the first event will be equal to the first event's timestamp plus half the value of this threshold; and the assigned start time for the second event will be equal to the second event's timestamp minus half the value of this threshold. For the case of 2 consecutive events taking place in different cells, if the time difference between the events is higher than the corresponding threshold (either <code>max_time_thresh_day</code> or <code>max_time_thresh_night</code>), then the event timestamps are also extended half this value of <code>max_time_thresh</code>. Recommended value: 900 seconds (15 minutes).</p> </li> <li> <p>max_time_thresh_day - integer, in seconds. In case of 2 consecutive events in the same cell, if the time of at least one of the events is included in the \"day time\", i.e. from 9:00 to 22:59, then <code>max_time_thresh_day</code> is the threshold to be applied. If the time difference between the events is lower than this threshold, then the both the first event's assigned end time and the second event's assigned start time will be equal to the average between the first and second event's timestamps. Otherwise, the assigned end time for the first event will be equal to the first event's timestamp plus half the value of <code>max_time_thresh</code>; and the assigned start time for the second event will be equal to the second event's timestamp minus half the value of <code>max_time_thresh</code>. Recommended value: 7200 seconds (2 hours).</p> </li> <li> <p>max_time_thresh_night - integer, in seconds. In case of 2 consecutive events in the same cell, if the time of both events is included in the \"night time\", i.e. from 23:00 to 8:59, then <code>max_time_thresh_night</code> is the threshold to be applied. If the time difference between the events is lower than this threshold, then the both the first event's assigned end time and the second event's assigned start time will be equal to the average between the first and second event's timestamps. Otherwise, the assigned end time for the first event will be equal to the first event's timestamp plus half the value of <code>max_time_thresh</code>; and the assigned start time for the second event will be equal to the second event's timestamp minus half the value of <code>max_time_thresh</code>. Recommended value: 28800 seconds (8 hours).</p> </li> <li> <p>max_vel_thresh - float, in metres per second (m/s). In order to evaluate if an event corresponds to a \"move\", the speed between the previous event and the next event is calculated. If the speed exceeds <code>max_vel_thresh</code>, then the event is tagged as a move and will be discarded for daily permanence score calculation. Recommended value: 13.889 m/s (50 km/h).</p> </li> <li> <p>event_error_flags_to_include - set of integers, e.g. \"{0}\". It indicates the values of the \"error_flag\" column of the input event data that will be kept. Rows with \"error_flag\" values not included in this set will be discarded and will not be considered for any step of the daily permanence score component. Recommended value: {0}.</p> </li> </ul> <p>--- Optional configuration values ---</p> <ul> <li> <p>broadcast_footprints - bool, default: False. If True, broadcast cell_footprints to each executor. Only recommendeded for small countries like Luxembourg.</p> </li> <li> <p>clear_destination_directory - bool, default: False. if True, the component will clear all the data in output paths.</p> </li> <li> <p>partition_chunk_size - int, default None. Number of <code>user_id_modulo</code> partitions. Should be the same value as <code>number_of_partitions</code> in <code>EventCleaning</code> component. If not given will process all <code>user_id_modulo</code> partitions at the same time.</p> </li> <li> <p>number_of_partitions - int, default None. Number of <code>user_id_modulo</code> partitions that will be processed at the same time. This value shall be lower than <code>partition_chunk_size</code>. If not given will process all <code>user_id_modulo</code> partitions at the same time.</p> </li> <li> <p>use_200m_grid - bool, default: False. If True, the component will aggregate cell footprint data from 100m to 200m grid resolution for DPS calculations. If False, the component will use initial 100m resolution.</p> </li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/10_daily_permanence_score/#configuration-example","title":"Configuration example","text":"<pre><code>[Logging]\nlevel = DEBUG\n\n[Spark]\nsession_name = DailyPermanenceScore\n\n[DailyPermanenceScore]\ndata_period_start = 2023-01-02\ndata_period_end = 2023-01-02\n\ntime_slot_number = 24\n\nmax_time_thresh = 900  # 15 min\nmax_time_thresh_day = 7_200  # 2 h\nmax_time_thresh_night = 28_800  # 8 h\n\nmax_speed_thresh = 13.88888889  # 50 km/h\n\nevent_error_flags_to_include = {0}\n\n# Optional\nbroadcast_footprints = False\nclear_destination_directory = False\npartition_chunk_size = 256\nnumber_of_partitions = 64\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/11_midterm_permanence_score/","title":"MidtermPermanenceScore Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>midterm_permanence_score.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>...\n[Paths.Bronze]\nholiday_calendar_data_bronze = ${Paths:bronze_dir}/holiday_calendar\n\n...\n\n[Paths.Silver]\ncell_footprint_data_silver = ${Paths:silver_dir}/cell_footprint\ndaily_permanence_score_data_silver = ${Paths:silver_dir}/daily_permanence_score\nmidterm_permanence_score_data_siilver = ${Paths:silver_dir}/midterm_permanence_score\n...\n</code></pre> <p>The expected parameters in <code>midterm_permanence_score.ini</code> are as follows:  - start_month: string, with <code>YYYY-MM</code> format, indicating the first month for which mid-term permanence score and metrics are to be computed. All months between start_month and end_month, both inclusive, will be processed individually. Example: <code>2023-01</code>.  - end_month: string, with <code>YYYY-MM</code> format, indicating the last month for which mid-term permanence score and metrics are to be computed. All months between start_month and end_month, both inclusive, will be processed individually. Must be a month equal or later than start_month. Example: <code>2023-08</code>.  - before_regularity_days: positive integer, it represents the number of days previous to a month that will be loaded and used in the calculation of the mid-term regularity metrics by searching for the latest date in which a given user performed a stay in a given grid tile. Example: <code>7</code>.  - after_regularity_days: positive integer, it represents the number of days following a month that will be loaded and used in the calculation of the mid-term regularity metrics by searching for the earliest date in which a given user performed a stay in a given grid tile. Example: <code>7</code>.  - day_start_hour: integer between 0 and 23 (both inclusive) that marks the starting time of a given day in the mid-term analysis to be performed. Example: <code>4</code>. A value of <code>4</code> means that the time slots of the Daily Permanence Score contained between the 04:00 of day $D$ (inclusive) and the 04:00 of day $D+1$ (not inclusive) are considered to belong to day $D$.  - country_of_study: two-letter, upper-case string, marking the ISO Alpha 2 code of the country being studied, used to load the holiday dates of that country that are used to define the workdays and holidays day types. Example: <code>ES</code> (Spain).  - weekend_start: integer between 1 and 7, marks the first day of the week (inclusive) that belongs to the weekends day type. Monday is 1, Tuesday is 2, ..., and Sunday is 7. Example: <code>6</code>.  - weekend_end: integer between 1 and 7, marks the last day of the week (inclusive) that belongs to the weekends day type. Monday is 1, Tuesday is 2, ..., and Sunday is 7. Example: <code>7</code>.  - night_time_start: string, <code>HH:MM</code> format, marking the starting hour and minutes (inclusive) of the night_time time interval. Only allowed values for the minutes <code>MM</code> are <code>00</code>, <code>15</code>, <code>30</code>, or <code>45</code>. The time interval limits must be compatible with the time slot limits and durations of the Daily Permanence Score data used. See the Time interval additional information for additional restrictions of this parameter. Example: <code>18:00</code>.  - night_time_end: string, <code>HH:MM</code> format, marking the ending hour and minutes (exclusive) of the night_time time interval. Only allowed values for the minutes <code>MM</code> are <code>00</code>, <code>15</code>, <code>30</code>, or <code>45</code>. The time interval limits must be compatible with the time slot limits and durations of the Daily Permanence Score data used. See the Time interval additional information for additional restrictions of this parameter. Example: <code>08:00</code>.  - working_hours_start: string, <code>HH:MM</code> format, marking the starting hour and minutes (inclusive) of the working_hours time interval. Only allowed values for the minutes <code>MM</code> are <code>00</code>, <code>15</code>, <code>30</code>, or <code>45</code>. The time interval limits must be compatible with the time slot limits and durations of the Daily Permanence Score data used. See the Time interval additional information for additional restrictions of this parameter. Example: <code>07:30</code>.  - working_hours_end: string, <code>HH:MM</code> format, marking the ending hour and minutes (exclusive) of the working_hours time interval. Only allowed values for the minutes <code>MM</code> are <code>00</code>, <code>15</code>, <code>30</code>, or <code>45</code>. The time interval limits must be compatible with the time slot limits and durations of the Daily Permanence Score data used. See the Time interval additional information for additional restrictions of this parameter. Example: <code>17:30</code>.  - evening_time_start: string, <code>HH:MM</code> format, marking the starting hour and minutes (inclusive) of the evening_time time interval. Only allowed values for the minutes <code>MM</code> are <code>00</code>, <code>15</code>, <code>30</code>, or <code>45</code>. The time interval limits must be compatible with the time slot limits and durations of the Daily Permanence Score data used. See the Time interval additional information for additional restrictions of this parameter. Example: <code>18:00</code>.  - evening_time_end: string, <code>HH:MM</code> format, marking the ending hour and minutes (exclusive) of the evening_time time interval. Only allowed values for the minutes <code>MM</code> are <code>00</code>, <code>15</code>, <code>30</code>, or <code>45</code>. The time interval limits must be compatible with the time slot limits and durations of the Daily Permanence Score data used. See the Time interval additional information for additional restrictions of this parameter. Example: <code>21:00</code>.  - period_combinations: dictionary indicating the combinations of sub-monthly and sub-daily periods (i.e., day types and time intervals) that are to be considered for the mid-term permanence score and metrics computation, for each month betwen start_month and end_month. The structure is as follows (a full example can be in Configuration example):    - The keys of the dictionary must be one of the possible day type values surrounded by quotes:      - <code>\"all\"</code>: every day of the month.      - <code>\"workdays\"</code>: every day of the month that does not belong to the weekend and is not a holiday in the country of study.      - <code>\"holidays\"</code>: every day of the month that is a holiday in the country of study.      - <code>\"weekends\"</code>: every day of the month that is part of the weekend.      - <code>\"mondays\"</code>: every Monday of the month.      - <code>\"tuesdays\"</code>: every Tuesday of the month.      - <code>\"wednesdays\"</code>: every Wednesday of the month.      - <code>\"thursdays\"</code>: every Thursday of the month.      - <code>\"fridays\"</code>: every Friday of the month.      - <code>\"saturdays\"</code>: every Saturday of the month.      - <code>\"sundays\"</code>: every Sunday of the month.    - The value assigned to a key must be a non-empty list surrounded by square brackets containing some of the possible time interval values:      - <code>\"all\"</code>: all time slots of the day.      - <code>\"night_time\"</code>: time slots of the day contained in the hour interval defined by night_time_start and night_time_end.      - <code>\"working_hours\"</code>: time slots of the day contained in the hour interval defined by working_hours_start and working_hours_end.      - <code>\"evening_time\"</code>: time slots of the day contained in the hour interval defined by evening_time_start and evening_time_end.</p> <p>--- Optional configuration values ---</p> <ul> <li> <p>clear_destination_directory - bool, default: False. if True, the component will clear all the data in output paths.</p> </li> <li> <p>partition_chunk_size - int, default None. Number of <code>user_id_modulo</code> partitions. Should be the same value as <code>number_of_partitions</code> in <code>EventCleaning</code> component. If not given will process all <code>user_id_modulo</code> partitions at the same time.</p> </li> <li> <p>number_of_partitions - int, default None. Number of <code>user_id_modulo</code> partitions that will be processed at the same time. This value shall be lower than <code>partition_chunk_size</code>. If not given will process all <code>user_id_modulo</code> partitions at the same time.</p> </li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/11_midterm_permanence_score/#time-interval-additional-information","title":"Time interval additional information","text":"<p>There are some nuances and restrictions related to the definition of the different time intervals and the day_start_hour parameter:  - By definition, a time interval will belong to the date that contains its start hour. See the following example:    - Suppose that day_start_hour has been set to <code>4</code>, so that the day \"starts\" at 04:00.    - Suppose that night_time_start has been set to <code>20:15</code> and night_time_end has been set to <code>06:30</code>. Then, the night time interval starts at 20:15 in the evening of some day $D$, crosses midnight, and ends at 06:30 in the morning of the following day $D+1$.    - In this case, the start of the time interval, 20:15, is between the 04:00 of day $D$ and the 04:00 of day $D+1$. Then, this time interval will be assigned to the date $D$.</p> <ul> <li>The following configuration is not allowed for the time intervals night_time, working_hours, and evening_time:</li> <li>day_start_hour is different from <code>0</code>, and</li> <li>the start of the interval is between 00:00 (exclusive) and day_start_hour (exclusive), and</li> <li> <p>the end of the interval is between 00:00 (exclusive) and the start of the interval (exclusive).</p> <p>Example of a not-allowed configuration under this restriction:    - day_start_hour is <code>4</code>, that is, 04:00.    - night_time_start is <code>03:00</code>.    - night_time_end is <code>01:15</code>.  - The following configuration is not allowed for the time intervals working_hours and evening_time:    - the start of the interval is between 00:00 (inclusive) and day_start_hour (exclusive)    - the end of the interval is later than day_start_hour (exclusive).</p> <p>Example of a not-allowed configuration under this restriction: - day_start_hour is <code>4</code>, that is, 04:00. - working_hours_start is <code>03:00</code>. - working_hours_end is <code>18:00</code>.</p> </li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/11_midterm_permanence_score/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = MidtermPermanenceScore\n\n[MidtermPermanenceScore]\n\nstart_month = 2023-02\nend_month = 2023-03\n\nbefore_regularity_days = 7\nafter_regularity_days = 7\nday_start_hour = 4  # at what time the day starts, e.g. day gos from 4AM Mon to 4AM Tue\n\ncountry_of_study = ES\n\nnight_time_start = 18:00\nnight_time_end = 08:00\n\nworking_hours_start = 08:00\nworking_hours_end = 17:00\n\nevening_time_start = 16:00\nevening_time_end = 22:00\n\nweekend_start = 6\nweekend_end = 7\n\nperiod_combinations = {\n    \"all\": [\"all\", \"night_time\", \"evening_time\", \"working_hours\"],\n    \"workdays\": [\"night_time\", \"working_hours\"],\n    \"holidays\": [\"all\", \"night_time\"],\n    \"weekends\": [\"all\", \"night_time\"],\n    \"mondays\": [\"all\"]\n    }\n\n# Optional\nclear_destination_directory = True\nnumber_of_partitions = 8\npartition_chunk_size = 4\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/12_longterm_permanence_score/","title":"LongtermPermanenceScore Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>longterm_permanence_score.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Silver]\n...\nmidterm_permanence_score_data_siilver = ${Paths:silver_dir}/midterm_permanence_score\nlongterm_permanence_score_data_siilver = ${Paths:silver_dir}/longterm_permanence_score\n...\n</code></pre> <p>The expected parameters in <code>longterm_permanence_score.ini</code> are as follows:  - start_month: string, in <code>YYYY-MM</code> format, it indicates the first month to be included in the long-term permanence score computations and analyses. All months between start_month and end_month, both inclusive, will be considered in the long-term analyses. Example: <code>2023-01</code>.  - end_month: string, in <code>YYYY-MM</code> format, it indicates the last month to be included in the long-term permanence score computations and analyses. All months between start_month and end_month, both inclusive, will be considered in the long-term analyses. Example: <code>2023-06</code>.  - winter_months: comma-separated sequence of integers between 1 and 12, they indicate which months of the year belong to the winter season. 1 represents January, 2 represents February, ..., and 12 represents December. Example: <code>12, 1, 2</code>.  - spring_months: comma-separated sequence of integers between 1 and 12, they indicate which months of the year belong to the spring season. 1 represents January, 2 represents February, ..., and 12 represents December. Example: <code>3, 4, 5</code>.  - summer_months: comma-separated sequence of integers between 1 and 12, they indicate which months of the year belong to the summer season. 1 represents January, 2 represents February, ..., and 12 represents December. Example: <code>6, 7, 8</code>.  - autumn_months: comma-separated sequence of integers between 1 and 12, they indicate which months of the year belong to the autumn season. 1 represents January, 2 represents February, ..., and 12 represents December. Example: <code>9, 10, 11</code>.  - period_combinations: dictionary indicating the combinations of sub-yearly, sub-monthly and sub-daily periods (i.e., seasons, day types and time intervals) to consider in the long-term analyses. Each combination will result in the computation of indicators resulting from aggregating the data of months belonging to the particular season, day type and time interval defined by the combination. The structure is as follows (a full example can be found in Configuration example):     - The keys of the dictionary must be one of the possible season values:         - <code>\"all\"</code>: every month between start_month and end_month, both inclusive, are considered in the aggregation.         - <code>\"winter\"</code>: every month included in the winter_months list between start_month and end_month, both inclusive, are considered in the aggregation. If this key appears then the winter_months list must contain at least one month.         - <code>\"spring\"</code>: every month included in the spring_months list between start_month and end_month, both inclusive, are considered in the aggregation. If this key appears then the spring_months list must contain at least one month.         - <code>\"summer\"</code>: every month included in the summer_months list between start_month and end_month, both inclusive, are considered in the aggregation. If this key appears then the summer_months list must contain at least one month.         - <code>\"autumn\"</code>: every month included in the autumn_months list between start_month and end_month, both inclusive, are considered in the aggregation. If this key appears then the autumn_months list must contain at least one month.     - The values assigned to a key is, in turn, another dictionary with the following structure:         - The keys of this dictionary must be one of the possible day type values surrounded by quotes:             - <code>\"all\"</code>             - <code>\"workdays\"</code>             - <code>\"holidays\"</code>             - <code>\"weekends\"</code>             - <code>\"mondays\"</code>             - <code>\"tuesdays\"</code>             - <code>\"wednesdays\"</code>             - <code>\"thursdays\"</code>             - <code>\"fridays\"</code>             - <code>\"saturdays\"</code>             - <code>\"sundays\"</code>         - The value assigned to a key must be a non-empty list surrounded by square brackets containing some of the possible time interval values:             - <code>\"all\"</code>             - <code>\"night_time\"</code>             - <code>\"working_hours\"</code>             - <code>\"evening_time\"</code></p> <pre><code>The **period_combinations** example that appears in the [Configuration example](#configuration-example) would result in the computation of the long-term permanence metrics for the following combinations:\n\n| season | day type | time interval   |\n|--------|----------|-----------------|\n| all    | all      | all             |\n| all    | all      | night_time      |\n| winter | all      | all             |\n| spring | all      | all             |\n| spring | workdays | working_hours   |\n</code></pre> <p>--- Optional configuration values ---</p> <ul> <li> <p>partition_chunk_size - int, default None. Number of <code>user_id_modulo</code> partitions. Should be the same value as <code>number_of_partitions</code> in <code>EventCleaning</code> component. If not given will process all <code>user_id_modulo</code> partitions at the same time.</p> </li> <li> <p>number_of_partitions - int, default None. Number of <code>user_id_modulo</code> partitions that will be processed at the same time. This value shall be lower than <code>partition_chunk_size</code>. If not given will process all <code>user_id_modulo</code> partitions at the same time.</p> </li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/12_longterm_permanence_score/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = LongtermPermanenceScore\n\n[LongtermPermanenceScore]\nstart_month = 2023-01\nend_month = 2023-06\n\nwinter_months = 12, 1, 2\nspring_months = 3, 4, 5\nsummer_months = 6, 7, 8\nautumn_months = 9, 10, 11\n\nperiod_combinations = {\n    \"all\": {\n        \"all\": [\"all\", \"night_time\"]\n    },\n    \"winter\": {\n        \"all\": [\"all\"],\n    },\n    \"spring\": {\n        \"all\": [\"all\"],\n        \"workdays\": [\"working_hours\"]\n    }\n    }\n\n# Optional\nnumber_of_partitions = 8\npartition_chunk_size = 4\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/13_usual_environment_labeling/","title":"UsualEnvironmentLabeling Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>usual_environment_labeling.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Silver]\n...\nlongterm_permanence_score_data_siilver = ${Paths:silver_dir}/longterm_permanence_score\nusual_environment_labels_data_silver = ${Paths:silver_dir}/usual_environment_labels\n...\n</code></pre> <p>The expected parameters in <code>usual_environment_labeling.ini</code> are as follows:</p> <ul> <li> <p>start_month: string, in <code>YYYY-MM</code> format, it indicates the first month to be included in the Usual Environment Labeling process. Long-term permanence metrics data shall be available for the corresponding start_month, end_month and season. Example: <code>2023-01</code>.</p> </li> <li> <p>end_month: string, in <code>YYYY-MM</code> format, it indicates the last month to be included in the Usual Environment Labeling process. Long-term permanence metrics data shall be available for the corresponding start_month, end_month and season. Example: <code>2023-06</code>.</p> </li> <li> <p>season: string, it indicates the season to be included in the Usual Environment Labeling process. Long-term permanence metrics data shall be available for the corresponding start_month, end_month and season. The months that correspond to each season are defined in the configuration of the Long-term permanence score method. Example: <code>all</code>. Options:</p> <ul> <li><code>\"all\"</code>: every month between start_month and end_month, both inclusive, are considered in the aggregation.</li> <li><code>\"winter\"</code>: every month included in the winter_months list between start_month and end_month, both inclusive, are considered in the aggregation.</li> <li><code>\"spring\"</code>: every month included in the spring_months list between start_month and end_month, both inclusive, are considered in the aggregation.</li> <li><code>\"summer\"</code>: every month included in the summer_months list between start_month and end_month, both inclusive, are considered in the aggregation.</li> <li><code>\"autumn\"</code>: every month included in the autumn_months list between start_month and end_month, both inclusive, are considered in the aggregation.</li> </ul> </li> <li> <p>total_ps_threshold: int, permanence score. It represents a long-term permanence score (ps) threshold to apply to discard rarely observed devices. Rarely observed devices are defined as those devices for which the total observed device's ps is lower than this threshold. Example: <code>1500</code>.</p> </li> <li> <p>total_ndays_threshold: int, number of days. It represents a frequency (number of days) threshold to apply to discard discontinuously observed devices. Discontinuously observed devices are defined as those devices for which the total observed device's frequency is lower than this threshold. Example <code>50</code>: </p> </li> <li> <p>ue_gap_ps_threshold: float, percentage. It represents a long-term permanence score (ps) threshold to apply in the preselection of tiles that may be tagged as usual environment (ue). This threshold is relative (a percentage) to the total ps device observation in the corresponding day type and time interval. The tiles associated to each device are sorted by ps in descending order, and then those that are below a ps gap that is bigger than <code>device_observation_ps * ue_gap_ps_threshold / 100</code> are filtered out. Example: <code>20</code> (%).</p> </li> <li> <p>gap_ps_threshold: int, permanence score. It represents a long-term permanence score (ps) threshold to apply in the preselection of tiles that may be tagged as home or work locations. This threshold is an absolute ps value, and its recommended value is 1. The tiles associated to each device are sorted by ps in descending order, and then those that are below a ps gap that is bigger than <code>gap_ps_threshold</code> are filtered out. Example: <code>1</code>.</p> </li> <li> <p>ue_ps_threshold: float, percentage. It represents a long-term permanence score (ps) threshold for the selection of tiles that may be tagged as usual environment (ue). This threshold is relative (a percentage) to the total ps device observation in the corresponding day type and time interval. The tiles associated to one device that have passed the ue gap cut and which have a ps higher than <code>device_observation_ps * ue_ps_threshold / 100</code> are tagged as ue. Example: <code>70</code> (%).</p> </li> <li> <p>home_ps_threshold: float, percentage. It represents a long-term permanence score (ps) threshold for the selection of tiles that may be tagged as home location. This threshold is relative (a percentage) to the total ps device observation in the corresponding day type and time interval. The tiles associated to one device that have passed the home gap cut and which have a ps higher than <code>device_observation_ps * home_ps_threshold / 100</code> are tagged as home. Example: <code>80</code> (%).</p> </li> <li> <p>work_ps_threshold: float, percentage. It represents a long-term permanence score (ps) threshold for the selection of tiles that may be tagged as work location. This threshold is relative (a percentage) to the total ps device observation in the corresponding day type and time interval. The tiles associated to one device that have passed the work gap cut and which have a ps higher than <code>device_observation_ps * work_ps_threshold / 100</code> are tagged as work. Example: <code>80</code> (%).</p> </li> <li> <p>ue_ndays_threshold: float, percentage. It represents a frequency (number of days) threshold for the selection of tiles that may be tagged as usual environment (ue). This threshold is relative (a percentage) to the total observed frequency for the device in the corresponding day type and time interval. The tiles associated to one device that have passed the ue gap cut and which have a frequency higher than device_observation_frequency * ue_ndays_threshold / 100 are tagged as ue. Example: <code>70</code> (%).</p> </li> <li> <p>home_ndays_threshold: float, percentage. It represents a frequency (number of days) threshold for the selection of tiles that may be tagged as home location. This threshold is relative (a percentage) to the total observed frequency for the device in the corresponding day type and time interval. The tiles associated to one device that have passed the home gap cut and which have a frequency higher than device_observation_frequency * home_ndays_threshold / 100 are tagged as home. Example: <code>70</code> (%).</p> </li> <li> <p>ue_ndays_threshold: float, percentage. It represents a frequency (number of days) threshold for the selection of tiles that may be tagged as work location. This threshold is relative (a percentage) to the total observed frequency for the device in the corresponding day type and time interval. The tiles associated to one device that have passed the work gap cut and which have a frequency higher than device_observation_frequency * work_ndays_threshold / 100 are tagged as work. Example: <code>70</code> (%).</p> </li> <li> <p>disaggregate_to_100m_grid: bool, default: False. If True, disaggregate the usual environment labels from 200m to 100m grid resolution. If False, the component will use the initial resolution.</p> </li> </ul> <p>--- Optional configuration values ---</p> <ul> <li> <p>partition_chunk_size - int, default None. Number of <code>user_id_modulo</code> partitions. Should be the same value as <code>number_of_partitions</code> in <code>EventCleaning</code> component. If not given will process all <code>user_id_modulo</code> partitions at the same time.</p> </li> <li> <p>number_of_partitions - int, default None. Number of <code>user_id_modulo</code> partitions that will be processed at the same time. This value shall be lower than <code>partition_chunk_size</code>. If not given will process all <code>user_id_modulo</code> partitions at the same time.</p> </li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/13_usual_environment_labeling/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = UsualEnvironmentLabeling\n\n[UsualEnvironmentLabeling]\nstart_month = 2023-01\nend_month = 2023-04\nseason = all\n\n# filtering rarely/discontinuously observed devices\ntotal_ps_threshold = 1500\ntotal_ndays_threshold = 50\n\nue_gap_ps_threshold = 20\ngap_ps_threshold = 1\n\nue_ps_threshold = 70\nhome_ps_threshold = 80\nwork_ps_threshold = 80\n\nue_ndays_threshold = 70\nhome_ndays_threshold = 80\nwork_ndays_threshold = 80\n\n# Optional\nnumber_of_partitions = 8\npartition_chunk_size = 4\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/14_usual_environment_aggregation/","title":"UsualEnvironmentAggregation Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>ue_aggregation.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Silver]\n...\nusual_environment_labels_data_silver = ${Paths:silver_dir}/usual_environment_labels\ngrid_data_silver = ${Paths:silver_dir}/grid\naggregated_usual_environments_silver = ${Paths:silver_dir}/aggregated_usual_environment\n...\n</code></pre> <p>The expected parameters in <code>ue_aggregation.ini</code> are as follows:</p> <ul> <li> <p>start_month: string, in <code>YYYY-MM</code> format, it indicates the first month to be included in the Usual Environment Aggregation process. Long-term permanence metrics data shall be available for the corresponding start_month, end_month and season. Example: <code>2023-01</code>.</p> </li> <li> <p>end_month: string, in <code>YYYY-MM</code> format, it indicates the last month to be included in the Usual Environment Aggregation process. Long-term permanence metrics data shall be available for the corresponding start_month, end_month and season. Example: <code>2023-06</code>.</p> </li> <li> <p>season: string, it indicates the season to be included in the Usual Environment Aggregation process. Long-term permanence metrics data shall be available for the corresponding start_month, end_month and season. The months that correspond to each season are defined in the configuration of the Long-term permanence score method. Example: <code>all</code>. Options:</p> <ul> <li><code>\"all\"</code>: every month between start_month and end_month, both inclusive, are considered in the aggregation.</li> <li><code>\"winter\"</code>: every month included in the winter_months list between start_month and end_month, both inclusive, are considered in the aggregation.</li> <li><code>\"spring\"</code>: every month included in the spring_months list between start_month and end_month, both inclusive, are considered in the aggregation.</li> <li><code>\"summer\"</code>: every month included in the summer_months list between start_month and end_month, both inclusive, are considered in the aggregation.</li> <li><code>\"autumn\"</code>: every month included in the autumn_months list between start_month and end_month, both inclusive, are considered in the aggregation.</li> </ul> </li> <li> <p>clear_destination_directory: bool, if to delete all previous outputs before running the component.</p> </li> <li> <p>uniform_tile_weights: int, if to use uniform tile weights in the aggregation process. If <code>True</code>, the weights of the tiles are equal. If <code>False</code>, the weights of the tiles are calculated based on landuse information.</p> </li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/14_usual_environment_aggregation/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = UsualEnvironmentLabeling\n\n[UsualEnvironmentLabeling]\nclear_destination_directory = True\n\nstart_month = 2023-01\nend_month = 2023-03\n\nseason = all\nuniform_tile_weights = True\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/15_internal_migration/","title":"InternalMigration Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>internal_migration.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Silver]\n...\ninternal_migration_previous_ue_labels_silver = ${Paths:ue_dir}/internal_migration_previous_ue_labels\ninternal_migration_new_ue_labels_silver = ${Paths:ue_dir}/internal_migration_new_ue_labels\ninternal_migration_enriched_grid_silver = ${Paths:ue_dir}/internal_migration_enriched_grid  # only if used\ninternal_migration_silver = ${Paths:ue_dir}/internal_migration\n...\n</code></pre> <p>The expected parameters in <code>internal_migration.ini</code> are as follows:  - clear_destination_directory: bool, whether to clear the output directory before running the component. Example: <code>True</code>.  - clear_quality_metrics_directory: bool, whether to clear the quality metrics directory before running the component. Example: <code>False</code>.  - migration_threshold: float between 0.0 and 1.0, it sets the threshold for considering that a device is to be considered for internal migration based on the metric comparing the set of home tiles in the two long-term time periods being compared. Example: <code>0.5</code>.  - uniform_tile_weights: bool, whether to use uniform tile weights for the home-labelled tiles. If <code>True</code>, the weights of all home tiles of a device are equal. If <code>False</code>, the weights of the home tiles will be proportional to the provided weights, and it is mandatory to provide a path under <code>internal_migration_enriched_silver_grid</code> on the <code>general_config.in</code> file.  - zoning_dataset_id: string, ID of the zones dataset that will be used to map grid tiles to zones. Example: <code>nuts</code>.  - hierarchical_levels: comma-separated list of positive integers, they are the hierarchical levels of the zones dataset used that should be processed by this component. If the zones dataset was not hierarchical, this should just take the value <code>1</code>. Example: <code>1,2,3</code>.  - start_month_previous: string, in <code>YYYY-MM</code> format, it indicates the first month of the first long-term period used to compute the internal migration. Example: <code>2023-01</code>.  - end_month_previous: string, in <code>YYYY-MM</code> format, it indicates the last month of the first long-term period used to compute the internal migration. Example: <code>2023-06</code>.  - season_previous: string, value of the season of the first long-term period used to compute the internal migration. Allowed values are: <code>all</code>, <code>spring</code>, <code>summer</code>, <code>autumn</code>, <code>winter</code>. Example: <code>all</code>.  - start_month_new: string, in <code>YYYY-MM</code> format, it indicates the first month of the second long-term period used to compute the internal migration. Example: <code>2023-07</code>.  - end_month_new: string, in <code>YYYY-MM</code> format, it indicates the last month of the second long-term period used to compute the internal migration. Example: <code>2023-12</code>.  - season_new: string, value of the season of the second long-term period used to compute the internal migration. Allowed values are: <code>all</code>, <code>spring</code>, <code>summer</code>, <code>autumn</code>, <code>winter</code>. Example: <code>all</code>.</p>"},{"location":"UserManual/configuration/1_Pipeline/15_internal_migration/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = InternalMigration\n\n[InternalMigration]\nclear_destination_directory = True\nclear_quality_metrics_directory = False\nmigration_threshold = 0.5\nuniform_tile_weights = True\n\nzoning_dataset_id = nuts  # ID of the zoning dataset\nhierarchical_levels = 1  # Hierarchical level(s) of the zoning dataset. Comma-separated list\n\nstart_month_previous = 2023-01  # Start month (inclusive) of the first long-term period\nend_month_previous = 2023-06  # End month (inclusive) of the first long-term period\nseason_previous = all  # Allowed values: `all`, `spring`, `summer`, `autumn`, `winter`. First long term period\n\nstart_month_new = 2023-07  # Start month (inclusive) of the second long-term period\nend_month_new = 2023-12  # End month (inclusive) of the second long-term period\nseason_new = all  # Allowed values: `all`, `spring`, `summer`, `autumn`, `winter`. Second long term period\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/16_tourism_stays_estimation/","title":"TourismStaysEstimation Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>tourism_stays_estimation.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Silver]\n...\ntourism_stays_estimation_silver = ${Paths:silver_dir}/tourism_stays_estimation\n...\n</code></pre> <p>The expected parameters in <code>tourism_stays_estimation.ini</code> are as follows:  - clear_destination_directory: bool, whether to clear the output directory before running the component. Example: <code>True</code>.  - local_mcc: int, the Mobile Country Code of the processing home country. Example: <code>234</code>.  - zoning_dataset_id: str, the dataset id of the zoning dataset to map time segments cell footrpints to. Example: <code>'nuts'</code>.  - min_duration_segment_m: int, the minimum duration of a segment in minutes to be considered as tourism stay. Example: <code>180</code>.  - functional_midnight_h: int, the hour of the day to consider as the functional midnight to mark overnight tourism stays. Example: <code>4</code>.  - min_duration_segment_night_m: int, the minimum duration of a segment in minutes to be considered as overnight tourism stay. Example: <code>300</code>.  - filter_ue_segments: bool, whether to filter out all segments of users who have inbound usual environment. Example: <code>False</code>.</p>"},{"location":"UserManual/configuration/1_Pipeline/16_tourism_stays_estimation/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = TourismStaysEstimation\n\n[InternalMigration]\ndata_period_start = 2023-01-07\ndata_period_end = 2023-01-11\n\nclear_destination_directory = true\nlocal_mcc = 234\nzoning_dataset_id = 'nuts'\nmin_duration_segment_m = 180\nfunctional_midnight_h = 4\nmin_duration_segment_night_m = 300\nfilter_ue_segments = false\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/17_spatial_aggregation/","title":"SpatialAggregation Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>spatial_aggregation.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Silver]\n...\ngeozones_grid_map_data_silver = ${Paths:silver_dir}/geozones_grid_map\naggregated_usual_environments_silver = ${Paths:silver_dir}/aggregated_usual_environment\naggregated_usual_environments_zone_silver = ${Paths:silver_dir}/aggregated_usual_environment_zone\npresent_population_silver = ${Paths:silver_dir}/present_population\npresent_population_zone_silver = ${Paths:silver_dir}/present_population_zone\n...\n</code></pre> <p>The <code>geozones_grid_map_data_silver</code> dataset may not be needed for the execution in the case that the zoning dataset that is provided via configuration file (defined below) is one of the reserved dataset names:  - <code>INSPIRE_100m</code>: no operations for an execution need to be performed by this component when this zoning dataset is selected for an execution.  - <code>INSPIRE_1km</code>: no <code>geozones_grid_map_data_silver</code> needs to be provided when executing a component with this zoning dataset. The mapping to this grid is performed directly and no external mapping dataset is needed.</p>"},{"location":"UserManual/configuration/1_Pipeline/17_spatial_aggregation/#general-section","title":"General section","text":"<p>The specific component configuration file, <code>spatial_aggregation.ini</code>, has three different sections. The general section <code>[SpatialAggregation]</code> is used to indicate on what UC outputs the spatial aggregation process should be executed. Its expected parameters are:  - present_population_execution: boolean, it indicates whether the spatial aggregation process should be executed on the gridded present population output. Example: <code>True</code>.  - usual_environment_execution: boolean, it indicates whether the spatial aggregation process should be executed on the gridded usual environment output. Example: <code>True</code>.</p> <p>## Present Population section  If the configuration parameter present_population_execution has been set to <code>True</code>, the component will read the section related to Present Population, <code>[SpatialAggregation.PresentPopulation]</code>. The expected parameters in this section are as follows:  - clear_destination_directory: boolean, whether to delete all previous results in the output directory before running the component.  - zoning_dataset_id: string, ID of the zones dataset to use to aggregate the data from grid level to zone level. The user may also specify one of the reserved dataset names <code>INSPIRE_100m</code> and <code>INSPIRE_1km</code>, in which case no <code>geozones_grid_map_data_silver</code> needs to be read. See above for the execution behaviour under these dataset names.  - hierarchical_levels: comma-separated list of positive integers, they are the hierarchical levels of the zones dataset used that should be processed by this component. If the zones dataset was not hierarchical, this should just take the value <code>1</code>. The list of levels is ignored and set to be <code>1</code> if a different value is encountered when zoning_dataset_id is one of the reserved dataset names. Example: <code>1,2,3</code>.  - start_date: string, in <code>YYYY-MM-DD</code> format, indicates the starting date (inclusive) for which the present population data should be processed by this component. Example: <code>2023-01-01</code>.  - end_date: string, in <code>YYYY-MM-DD</code> format, indicates the ending date (inclusive) for which the present population data should be processed by this component. Example: <code>2023-01-01</code>.</p> <p>## Usual Environment section  If the configuration parameter usual_environment_execution has been set to <code>True</code>, the component will read the section related to Usual Environment, <code>[SpatialAggregation.UsualEnvironment]</code>. The expected parameters in this section are as follows:  - clear_destination_directory: boolean, whether to delete all previous results in the output directory before running the component.  - zoning_dataset_id: string, ID of the zones dataset to use to aggregate the data from grid level to zone level. The user may also specify one of the reserved dataset names <code>INSPIRE_100m</code> and <code>INSPIRE_1km</code>, in which case no <code>geozones_grid_map_data_silver</code> needs to be read. See above for the execution behaviour under these dataset names.  - hierarchical_levels: comma-separated list of positive integers, they are the hierarchical levels of the zones dataset used that should be processed by this component. If the zones dataset was not hierarchical, this should just take the value <code>1</code>. The list of levels is ignored and set to be <code>1</code> if a different value is encountered when zoning_dataset_id is one of the reserved dataset names. Example: <code>1,2,3</code>.  - labels: comma-separated list of strings, they indicate what usual environment labels should be processed by this component. Allowed values are <code>home</code>, <code>work</code>, <code>ue</code>. Example: <code>ue, home, work</code>.  - start_month: string, in <code>YYYY-MM</code> format, it indicates the first month of the period used to compute a specific usual environment dataset that the user desires to process through this component. Example: <code>2023-01</code>.  - end_month: string, in <code>YYYY-MM</code> format, it indicates the last month of the period used to compute a specific usual environment dataset that the user desires to process through this component. Example: <code>2023-03</code>.  - season: string, value of the season of the usual environment dataset that the user desires to prcess through this component. Allowed values are: <code>all</code>, <code>spring</code>, <code>summer</code>, <code>autumn</code>, <code>winter</code>. Example: <code>all</code>.</p> <pre><code>[Spark]\nsession_name = SpatialAggregation\n\n[SpatialAggregation]\n\npresent_population_execution = True\nusual_environment_execution = True\n\n[SpatialAggregation.PresentPopulation]\nclear_destination_directory = True\n\nzoning_dataset_id = nuts  # ID of the zoning dataset\nhierarchical_levels = 1,2,3,4  # Hierarchical level(s) of the zoning dataset. Comma-separated list\nstart_date = 2023-01-08  # start date (inclusive)\nend_date = 2023-01-09  # end date (inclusive)\n\n[SpatialAggregation.UsualEnvironment]\nclear_destination_directory = True\n\nzoning_dataset_id = nuts  # ID of the zoning dataset\nhierarchical_levels = 1,2,3,4  # Hierarchical level(s) of the zoning dataset. Comma-separated list\nlabels = ue, home, work  # Allowed values: `ue`, `home`, `work`. Comma-separated list\nstart_month = 2023-01  # Start month (inclusive)\nend_month = 2023-01  # End month (inclusive)\nseason = all  # Allowed values: `all`, `spring`, `summer`, `autumn`, `winter`.\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/18_estimation/","title":"Estimation Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>estimation.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <p><pre><code>[Paths.Silver]\n...\n# Present Population UC data objects\n# One of the two parameters below are needed depending on configuration file value `zoning_dataset_id`\npresent_population_silver = ${Paths:silver_dir}/present_population\npresent_population_zone_silver = ${Paths:silver_dir}/present_population_zone\n\nestimated_present_population_zone_silver = ${Paths:silver_dir}/estimated_present_population_zone\n\n# Usual Environment UC data objects\n# One of the two parameters below are needed depending on configuration file value `zoning_dataset_id`\naggregated_usual_environments_silver = ${Paths:silver_dir}/aggregated_usual_environment\naggregated_usual_environments_zone_silver = ${Paths:silver_dir}/aggregated_usual_environment_zone\n\nestimated_aggregated_usual_environments_zone_silver = ${Paths:silver_dir}/estimated_aggregated_usual_environment_zone\n\n# Internal Migration UC data object\ninternal_migration_silver = ${Paths:silver_dir}/internal_migration\nestimated_internal_migration_silver = ${Paths:silver_dir}/estimated_internal_migration\n...\n</code></pre> For the execution of the Estimation process for the Present Population UC and the Usual Environment (including Home Location UC), one of two possible input datasets may be needed:  - If the zoning dataset that is provided via configuration file (defined below) is the reserved dataset name <code>INSPIRE_100m</code>, the datasets <code>present_population_silver</code> and <code>aggregated_usual_environment_silver</code> are required for the present population execution and the usual environment execution, respectively.  - If any other dataset name is provided, the datasets <code>present_population_zone_silver</code> and <code>aggregated_usual_environments_zone_silver</code> are required for the present population execution and the usual environment execution, respectively.</p> <p>As of now, the Internal Migration UC execution does not accept the reserved dataset name <code>INSPIRE_100m</code>.</p>"},{"location":"UserManual/configuration/1_Pipeline/18_estimation/#general-section","title":"General section","text":"<p>The specific component configuration file, <code>estimation.ini</code>, has three different sections. The general section <code>[Estimation]</code> is used to indicate on what UC outputs the estimation process should be executed. Its expected parameters are:  - present_population_execution: boolean, it indicates whether the estimation process should be executed on the zone-aggregated present population output. Example: <code>True</code>.  - usual_environment_execution: boolean, it indicates whether the estimation process should be executed on the zone-aggregated usual environment output. Example: <code>True</code>.  - internal_migration_execution boolean, it indicates whether the estimation process should be executed on the zone-level internal migration output. Example: <code>True</code>.</p> <p>## Present Population section  If the configuration parameter present_population_execution has been set to <code>True</code>, the component will read the section related to Present Population, <code>[Estimation.PresentPopulationEstimation]</code>. The expected parameters in this section are as follows:  - clear_destination_directory: boolean, whether to delete all previous results in the output directory before running the component.  - deduplication_factor: positive float, this factor will be used to multiply the population value computed at device level to take into account the possibility that a real person might own more than one device. Example: <code>0.98</code>.  - mno_to_target_population_factor: positive, float, this factor will be used to multiply the (now deduplicated) population value computed at device level to represent the total target population. Example: <code>5.4</code>.  - zoning_dataset_id: string, ID of the zones dataset that was previously used to aggregate the data from grid level to zone level. This identifies what specific usual environment output data should be processed by this component. The user may also specify the reserved dataset names <code>INSPIRE_100m</code>, in which case the <code>present_population_silver</code> dataset will be read instead of <code>present_population_zone_silver</code>. See above for the execution behaviour under this name. Example: <code>nuts</code>.  - hierarchical_levels: comma-separated list of positive integers, they are the hierarchical levels of the zones dataset used that should be processed by this component. If the zones dataset was not hierarchical, this should just take the value <code>1</code>. The list of levels is ignored and set to be <code>1</code> if a different value is encountered when zoning_dataset_id is one of the reserved dataset names. Example: <code>1,2,3</code>.  - start_date: string, in <code>YYYY-MM-DD</code> format, indicates the starting date (inclusive) for which the present population data should be processed by this component. Example: <code>2023-01-01</code>.  - end_date: string, in <code>YYYY-MM-DD</code> format, indicates the ending date (inclusive) for which the present population data should be processed by this component. Example: <code>2023-01-01</code>.</p> <p>## Usual Environment section  If the configuration parameter usual_environment_execution has been set to <code>True</code>, the component will read the section related to Usual Environment, <code>[Estimation.UsualEnvironmentAggregation]</code>. The expected parameters in this section are as follows:  - clear_destination_directory: boolean, whether to delete all previous results in the output directory before running the component.  - deduplication_factor: positive float, this factor will be used to multiply the weighted device count value computed at device level to take into account the possibility that a real person might own more than one device. Example: <code>0.98</code>.  - mno_to_target_population_factor: positive, float, this factor will be used to multiply the (now deduplicated) weighted device count value computed at device level to represent the total target population. Example: <code>5.4</code>.  - zoning_dataset_id: string, ID of the zones dataset that was previously used to aggregate the data from grid level to zone level. This identifies what specific usual environment output data should be processed by this component. The user may also specify the reserved dataset names <code>INSPIRE_100m</code>, in which case the <code>aggregated_usual_environment_silver</code> dataset will be read instead of <code>aggregated_usual_environments_zone_silver</code>. See above for the execution behaviour under this name. Example: <code>nuts</code>.  - hierarchical_levels: comma-separated list of positive integers, they are the hierarchical levels of the zones dataset used that should be processed by this component. If the zones dataset was not hierarchical, this should just take the value <code>1</code>. The list of levels is ignored and set to be <code>1</code> if a different value is encountered when zoning_dataset_id is one of the reserved dataset names. Example: <code>1,2,3</code>.  - labels: comma-separated list of strings, they indicate what usual environment labels should be processed by this component. Allowed values are <code>home</code>, <code>work</code>, <code>ue</code>. Example: <code>ue, home, work</code>.  - start_month: string, in <code>YYYY-MM</code> format, it indicates the first month of the period used to compute a specific usual environment dataset that the user desires to process through this component. Example: <code>2023-01</code>.  - end_month: string, in <code>YYYY-MM</code> format, it indicates the last month of the period used to compute a specific usual environment dataset that the user desires to process through this component. Example: <code>2023-03</code>.  - season: string, value of the season of the usual environment dataset that the user desires to prcess through this component. Allowed values are: <code>all</code>, <code>spring</code>, <code>summer</code>, <code>autumn</code>, <code>winter</code>. Example: <code>all</code>.</p> <p>## Internal Migration section  If the configuration parameter internal_migration_execution has been set to <code>True</code>, the component will read the section related to Internal Migration, <code>[Estimation.InternalMigration]</code>. The expected parameters in this section are as follows:  - clear_destination_directory: boolean, whether to delete all previous results in the output directory before running the component.  - deduplication_factor: positive float, this factor will be used to multiply the internal migration metric at device level to take into account the possibility that a real person might own more than one device. Example: <code>0.98</code>.  - mno_to_target_population_factor: positive, float, this factor will be used to multiply the (now deduplicated) internal migration value computed at device level to represent the total target population. Example: <code>5.4</code>.  - zoning_dataset_id: string, ID of the zones dataset that was previously used to aggregate the data from grid level to zone level. This identifies what specific internal migration output data should be processed by this component. The Example: <code>nuts</code>.  - hierarchical_levels: comma-separated list of positive integers, they are the hierarchical levels of the zones dataset used that should be processed by this component. If the zones dataset was not hierarchical, this should just take the value <code>1</code>. Example: <code>1,2,3</code>.  - start_month_previous: string, in <code>YYYY-MM</code> format, it indicates the first month of the first long-term period used to compute the internal migration. Example: <code>2023-01</code>.  - end_month_previous: string, in <code>YYYY-MM</code> format, it indicates the last month of the first long-term period used to compute the internal migration. Example: <code>2023-06</code>.  - season_previous: string, value of the season of the first long-term period used to compute the internal migration. Allowed values are: <code>all</code>, <code>spring</code>, <code>summer</code>, <code>autumn</code>, <code>winter</code>. Example: <code>all</code>.  - start_month_new: string, in <code>YYYY-MM</code> format, it indicates the first month of the second long-term period used to compute the internal migration. Example: <code>2023-07</code>.  - end_month_new: string, in <code>YYYY-MM</code> format, it indicates the last month of the second long-term period used to compute the internal migration. Example: <code>2023-12</code>.  - season_new: string, value of the season of the second long-term period used to compute the internal migration. Allowed values are: <code>all</code>, <code>spring</code>, <code>summer</code>, <code>autumn</code>, <code>winter</code>. Example: <code>all</code>.</p>"},{"location":"UserManual/configuration/1_Pipeline/18_estimation/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = Estimation\n\n\n[Estimation]\npresent_population_execution = True\nusual_environment_execution = True\n\n[Estimation.PresentPopulationEstimation]\nclear_destination_directory = True\ndeduplication_factor = 0.95\nmno_to_target_population_factor = 5.4\n\n# Target PresentPopulationEstimation dataset\nzoning_dataset_id = nuts  # ID of the zoning dataset\nhierarchical_levels = 1,2,3,4  # Hierarchical level(s) of the zoning dataset. Comma-separated list\nstart_date = 2023-01-01  # start date (inclusive)\nend_date = 2023-01-01  # end date (inclusive)\n\n\n[Estimation.UsualEnvironmentAggregation]\nclear_destination_directory = True\ndeduplication_factor = 0.95\nmno_to_target_population_factor = 5.4\n\n# Target UsualEnvironmentAggregation dataset\nzoning_dataset_id = nuts  # ID of the zoning dataset\nhierarchical_levels = 1,2,3,4  # Hierarchical level(s) of the zoning dataset. Comma-separated list\nlabels = ue, home, work  # Allowed values: `ue`, `home`, `work`. Comma-separated list\nstart_month = 2023-01  # Start month (inclusive)\nend_month = 2023-01  # End month (inclusive)\nseason = all  # Allowed values: `all`, `spring`, `summer`, `autumn`, `winter`.\n\n\n[Estimation.InternalMigration]\nclear_destination_directory = True\ndeduplication_factor = 0.95\nmno_to_target_population_factor = 5.4\n\n# Target InternalMigration dataset\nzoning_dataset_id = nuts  # ID of the zoning dataset\nhierarchical_levels = 1,2,3,4  # Hierarchical level(s) of the zoning dataset. Comma-separated list\n\nstart_month_previous = 2023-01  # Start month (inclusive) of the first long-term period\nend_month_previous = 2023-06  # End month (inclusive) of the first long-term period\nseason_previous = all  # Allowed values: `all`, `spring`, `summer`, `autumn`, `winter`. First long term period\n\nstart_month_new = 2023-07  # Start month (inclusive) of the second long-term period\nend_month_new = 2023-12  # End month (inclusive) of the second long-term period\nseason_new = all  # Allowed values: `all`, `spring`, `summer`, `autumn`, `winter`. Second long term period\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/19_kanonimity/","title":"KAnonimity Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>kanonimity.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Silver]\n...\n# Present Population UC input data object\nestimated_present_population_zone_silver = ${Paths:silver_dir}/estimated_present_population_zone\n\n# Usual Environment UC input data objects\nestimated_aggregated_usual_environments_zone_silver = ${Paths:silver_dir}/estimated_aggregated_usual_environment_zone\n\n# Internal Migration UC input data object\nestimated_internal_migration_silver = ${Paths:silver_dir}/estimated_internal_migration\n...\n\n[Paths.Gold]\n# Present Population UC output data object\nkanonimity_present_population_zone_gold = ${Paths:gold_dir}/kanonimity_present_population_zone\n\n# Usual Environment UC output data object\nkanonimity_aggregated_usual_environments_zone_gold = ${Paths:gold_dir}/kanonimity_aggregated_usual_environment_zone\n\n# Internal Migration UC output data object\nkanonimity_internal_migration_gold = ${Paths:silver_dir}/kanonimity_internal_migration\n...\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/19_kanonimity/#general-section","title":"General section","text":"<p>The specific component configuration file, <code>kanonimity.ini</code>, has three different sections. The general section <code>[KAnonimity]</code> is used to indicate on what UC outputs the k-anonimity process should be executed. Its expected parameters are:  - present_population_execution: boolean, it indicates whether the k-anonimity process should be executed on the zone-aggregated present population output. Example: <code>True</code>.  - usual_environment_execution: boolean, it indicates whether the k-anonimity process should be executed on the zone-aggregated usual environment output. Example: <code>True</code>.  - internal_migration_execution boolean, it indicates whether the k-anonimity process should be executed on the zone-level internal migration output. Example: <code>True</code>.</p> <p>## Present Population section  If the configuration parameter present_population_execution has been set to <code>True</code>, the component will read the section related to Present Population, <code>[KAnonimity.PresentPopulationEstimation]</code>. The expected parameters in this section are as follows:  - clear_destination_directory: boolean, whether to delete all previous results in the output directory before running the component.  - k: integer, it indicates the threshold value to select what records should have k-anonimity applied to them, that is, all records that have a population value strictly lower than k. Example: <code>5</code>.  - anonimity_type: string, it indicates what k-anonimity metholodogy to employ. If it is set to <code>delete</code>, it will delete all records that have a population value strictly lower than k. If it is set to <code>obfuscate</code>, it will replace all population values strictly lower than k by the negative value <code>-1</code>. Example: <code>delete</code>.  - zoning_dataset_id: string, ID of the zones dataset that was previously used to aggregate the data from grid level to zone level. This identifies what specific present population output data should be processed by this component. Example: <code>nuts</code>.  - hierarchical_levels: comma-separated list of positive integers, they are the hierarchical levels of the zones dataset used that should be processed by this component. If the zones dataset was not hierarchical, this should just take the value <code>1</code>. Example: <code>1,2,3</code>.  - start_date: string, in <code>YYYY-MM-DD</code> format, indicates the starting date (inclusive) for which the present population data should be processed by this component. Example: <code>2023-01-01</code>.  - end_date: string, in <code>YYYY-MM-DD</code> format, indicates the ending date (inclusive) for which the present population data should be processed by this component. Example: <code>2023-01-01</code>.</p> <p>## Usual Environment section  If the configuration parameter usual_environment_execution has been set to <code>True</code>, the component will read the section related to Usual Environment, <code>[KAnonimity.UsualEnvironmentAggregation]</code>. The expected parameters in this section are as follows:  - clear_destination_directory: boolean, whether to delete all previous results in the output directory before running the component.  - k: integer, it indicates the threshold value to select what records should have k-anonimity applied to them, that is, all records that have a weighted device count value strictly lower than k. Example: <code>5</code>.  - anonimity_type: string, it indicates what k-anonimity metholodogy to employ. If it is set to <code>delete</code>, it will delete all records that have a weighted device count value strictly lower than k. If it is set to <code>obfuscate</code>, it will replace all weighted device count values strictly lower than k by the negative value <code>-1</code>. Example: <code>delete</code>.  - zoning_dataset_id: string, ID of the zones dataset that was previously used to aggregate the data from grid level to zone level. This identifies what specific usual environment output data should be processed by this component. Example: <code>nuts</code>.  - hierarchical_levels: comma-separated list of positive integers, they are the hierarchical levels of the zones dataset used that should be processed by this component. If the zones dataset was not hierarchical, this should just take the value <code>1</code>. Example: <code>1,2,3</code>.  - labels: comma-separated list of strings, they indicate what usual environment labels should be processed by this component. Allowed values are <code>home</code>, <code>work</code>, <code>ue</code>. Example: <code>ue, home, work</code>.  - start_month: string, in <code>YYYY-MM</code> format, it indicates the first month of the period used to compute a specific usual environment dataset that the user desires to process through this component. Example: <code>2023-01</code>.  - end_month: string, in <code>YYYY-MM</code> format, it indicates the last month of the period used to compute a specific usual environment dataset that the user desires to process through this component. Example: <code>2023-03</code>.  - season: string, value of the season of the usual environment dataset that the user desires to prcess through this component. Allowed values are: <code>all</code>, <code>spring</code>, <code>summer</code>, <code>autumn</code>, <code>winter</code>. Example: <code>all</code>.</p> <p>## Internal Migration section  If the configuration parameter internal_migration_execution has been set to <code>True</code>, the component will read the section related to Internal Migration, <code>[KAnonimity.InternalMigration]</code>. The expected parameters in this section are as follows:  - clear_destination_directory: boolean, whether to delete all previous results in the output directory before running the component.  - k: integer, it indicates the threshold value to select what records should have k-anonimity applied to them, that is, all records that have an internal migration value strictly lower than k. Example: <code>5</code>.  - anonimity_type: string, it indicates what k-anonimity metholodogy to employ. If it is set to <code>delete</code>, it will delete all records that have an internal migration value strictly lower than k. If it is set to <code>obfuscate</code>, it will replace all internal migration values strictly lower than k by the negative value <code>-1</code>. Example: <code>delete</code>.  - zoning_dataset_id: string, ID of the zones dataset that was used to map grid tiles to zones. This identifies what specific internal migration output data should be processed by this component. Example: <code>nuts</code>.  - hierarchical_levels: comma-separated list of positive integers, they are the hierarchical levels of the zones dataset used that should be processed by this component. If the zones dataset was not hierarchical, this should just take the value <code>1</code>. Example: <code>1,2,3</code>.  - start_month_previous: string, in <code>YYYY-MM</code> format, it indicates the first month of the first long-term period used to compute the internal migration. Example: <code>2023-01</code>.  - end_month_previous: string, in <code>YYYY-MM</code> format, it indicates the last month of the first long-term period used to compute the internal migration. Example: <code>2023-06</code>.  - season_previous: string, value of the season of the first long-term period used to compute the internal migration. Allowed values are: <code>all</code>, <code>spring</code>, <code>summer</code>, <code>autumn</code>, <code>winter</code>. Example: <code>all</code>.  - start_month_new: string, in <code>YYYY-MM</code> format, it indicates the first month of the second long-term period used to compute the internal migration. Example: <code>2023-07</code>.  - end_month_new: string, in <code>YYYY-MM</code> format, it indicates the last month of the second long-term period used to compute the internal migration. Example: <code>2023-12</code>.  - season_new: string, value of the season of the second long-term period used to compute the internal migration. Allowed values are: <code>all</code>, <code>spring</code>, <code>summer</code>, <code>autumn</code>, <code>winter</code>. Example: <code>all</code>.</p>"},{"location":"UserManual/configuration/1_Pipeline/19_kanonimity/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = KAnonimity\n\n[KAnonimity]\npresent_population_execution = True\nusual_environment_execution = True\ninternal_migration_execution = True\n\n\n[KAnonimity.PresentPopulationEstimation]\nclear_destination_directory = True\nanonimity_type = obfuscate  # `obfuscate`, `delete`\nk = 5\n\n# Target PresentPopulationEstimation dataset\nzoning_dataset_id = nuts  # ID of the zoning dataset\nhierarchical_levels = 1,2,3,4  # Hierarchical level(s) of the zoning dataset. Comma-separated list\nstart_date = 2023-01-01  # start date (inclusive)\nend_date = 2023-01-01  # end date (inclusive)\n\n[KAnonimity.UsualEnvironmentAggregation]\nclear_destination_directory = True\nanonimity_type = obfuscate  # `obfuscate`, `delete`\nk = 5\n\n# Target UsualEnvironmentAggregation dataset\nzoning_dataset_id = nuts  # ID of the zoning dataset\nhierarchical_levels = 1,2,3,4  # Hierarchical level(s) of the zoning dataset. Comma-separated list\nlabels = ue, home, work  # Allowed values: `ue`, `home`, `work`. Comma-separated list\nstart_month = 2023-01  # Start month (inclusive)\nend_month = 2023-03  # End month (inclusive)\nseason = all  # Allowed values: `all`, `spring`, `summer`, `autumn`, `winter`.\n\n[KAnonimity.InternalMigration]\nclear_destination_directory = True\nanonimity_type = obfuscate  # `obfuscate`, `delete`\nk = 5\n\n# Target InternalMigration dataset\nzoning_dataset_id = nuts  # ID of the zoning dataset\nhierarchical_levels = 1  # Hierarchical level(s) of the zoning dataset. Comma-separated list\n\nstart_month_previous = 2023-01  # Start month (inclusive) of the first long-term period\nend_month_previous = 2023-06  # End month (inclusive) of the first long-term period\nseason_previous = all  # Allowed values: `all`, `spring`, `summer`, `autumn`, `winter`. First long term period\n\nstart_month_new = 2023-07  # Start month (inclusive) of the second long-term period\nend_month_new = 2023-12  # End month (inclusive) of the second long-term period\nseason_new = all  # Allowed values: `all`, `spring`, `summer`, `autumn`, `winter`. Second long term period\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/1_inspire_grid_generation/","title":"InspireGridGeneration Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>grid_generation.ini</code>. In <code>general_config.ini</code> to execute the component specify all paths to its four corresponding data objects (input + output). Example: </p> <pre><code>[Paths.Silver]\ngrid_data_silver = ${Paths:silver_dir}/grid\n</code></pre> <p>In grid_generation.ini parameters are as follows: </p> <ul> <li> <p>clear_destination_directory - boolean, if True, the component will clear all the data in output paths.</p> </li> <li> <p>grid_mask - string, the mask to be used for grid generation. It can be either 'extent' or 'polygon'. If 'extent' is chosen, the extent parameter should be provided. If 'polygon' is chosen, reference country iso2 code should be provided.</p> </li> <li> <p>extent - list, the extent of the grid to be generated if 'extent' is chosen spatial mask type. The format is [min_lon, min_lat, max_lon, max_lat].</p> </li> <li> <p>reference_country - string, iso2 country code to use as a spatial mask for grid generation.</p> </li> <li> <p>country_buffer - integer, buffer distance to extend country polygon for grid generation.</p> </li> <li> <p>grid_generation_partition_size - integer, the size of the partition to be used for grid generation as a size of a side of grid subsquare. Default value is 500 grid tiles, so generation will be done with 500 by 500 grid subsquares.</p> </li> <li> <p>grid_processing_partition_quadkey_level - integer, the level of quadkey for resulted grid partitioning. Default value is level 8.  </p> </li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/1_inspire_grid_generation/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = InspireGridGeneration\n\n[InspireGridGeneration]\nclear_destination_directory = True\ngrid_mask = polygon # extent or polygon\nextent = [-4.5699,39.9101,-2.8544,40.9416] # [min_lon, min_lat, max_lon, max_lat]\nreference_country = ES # ISO A2 code\ncountry_buffer = 10000 # meters\ngrid_generation_partition_size = 500\ngrid_processing_partition_quadkey_level = 8\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/20_multimno_aggregation/","title":"MultiMNOAggregation Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>multimno_aggregation.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. In this aggregation component that brings together the data computed individually in several MNOs, the number of input data paths depends on the number of different MNOs that the user worked with. This number will be specified below in the configuration section related to the relevant use case. Example (two MNOs for both Present Population and Usual Environment):</p> <pre><code>...\n[Paths.Gold]\n# Present Population UC data objects\nsingle_mno_1_present_population_zone_gold = ${Paths:gold_dir}/kanonimity_present_population_zone_1\nsingle_mno_2_present_population_zone_gold = ${Paths:gold_dir}/kanonimity_present_population_zone_2\nmultimno_aggregated_present_population_zone_gold = ${Paths:gold_dir}/multimno_aggregated_present_population_zone\n\n# Usual Environment UC data objects\nsingle_mno_1_usual_environment_zone_gold = ${Paths:gold_dir}/kanonimity_aggregated_usual_environment_zone_1\nsingle_mno_2_usual_environment_zone_gold = ${Paths:gold_dir}/kanonimity_aggregated_usual_environment_zone_2\nmultimno_aggregated_usual_environment_zone_gold = ${Paths:gold_dir}/multimno_aggregated_usual_environment_zone\n\n# Internal Migration UC data objects\nsingle_mno_1_internal_migration_gold = ${Paths:gold_dir}/kanonimity_internal_migration_1\nsingle_mno_2_internal_migration_gold = ${Paths:gold_dir}/kanonimity_internal_migration_2\nmultimno_internal_migration_gold = ${Paths:gold_dir}/multimno_internal_migration\n...\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/20_multimno_aggregation/#general-section","title":"General section","text":"<p>The specific component configuration file, <code>multimno_aggregation.ini</code>, has three different sections. The general section <code>[MultiMNOAggregation]</code> is used to indicate on what UC outputs the multi-MNO aggregation process should be executed. Its expected parameters are:  - present_population_execution: boolean, it indicates whether the multi-MNO aggregation process should be executed on the zone-aggregated present population output. Example: <code>True</code>.  - usual_environment_execution: boolean, it indicates whether the multi-MNO aggregation process should be executed on the zone-aggregated usual environment output. Example: <code>True</code>.  - internal_migration_execution boolean, it indicates whether the multi-MNO aggregation process should be executed on the zone-level internal migration output. Example: <code>True</code>.</p> <p>## Present Population section  If the configuration parameter present_population_execution has been set to <code>True</code>, the component will read the section related to Present Population, <code>[MultiMNOAggregation.PresentPopulationEstimation]</code>. The expected parameters in this section are as follows:  - clear_destination_directory: boolean, whether to delete all previous results in the output directory before running the component.  - number_of_single_mnos: positive integer equal or greater than <code>2</code>, it indicates the number of MNOs whose present population data will be aggregated by this component. Example: <code>2</code>. This number also dictates:    - The number of input data paths that need to be specified in the general configuration. The name format of these configuration parameters is <code>single_mno_i_present_population_zone_gold</code>, where <code>i</code> must be an integer between <code>1</code> and number_of_single_mnos. All input data path parameters are mandatory. See the configuration example above for the case where there are two MNOs.    - The number of MNO factors. The name format of these configuration parameters is <code>single_mno_i_factor</code>, where <code>i</code> must be an integer between <code>1</code> and number_of_single_mnos. All factor parameters are mandatory and are described below.  - single_mno_<code>i</code>_factor: float between 0 and 1, this parameter (for all values <code>i</code> between <code>1</code> and number_of_single_mnos) represents the weight that the indicator produced by this specific MNO will have in the final aggregation. It is required that all MNO factors add up to <code>1</code>. Example: <code>0.6</code>. See the configuration example below for the case where number_of_single_mnos is equal to <code>2</code>.  - zoning_dataset_id: string, ID of the zones dataset that was previously used to aggregate the data from grid level to zone level. This identifies what specific present population output data should be processed by this component. Example: <code>nuts</code>.  - hierarchical_levels: comma-separated list of positive integers, they are the hierarchical levels of the zones dataset used that should be processed by this component. If the zones dataset was not hierarchical, this should just take the value <code>1</code>. Example: <code>1,2,3</code>.  - start_date: string, in <code>YYYY-MM-DD</code> format, indicates the starting date (inclusive) for which the present population data should be processed by this component. Example: <code>2023-01-01</code>.  - end_date: string, in <code>YYYY-MM-DD</code> format, indicates the ending date (inclusive) for which the present population data should be processed by this component. Example: <code>2023-01-01</code>.</p> <p>## Usual Environment section  If the configuration parameter usual_environment_execution has been set to <code>True</code>, the component will read the section related to Usual Environment, <code>[MultiMNOAggregation.UsualEnvironmentAggregation]</code>. The expected parameters in this section are as follows:  - clear_destination_directory: boolean, whether to delete all previous results in the output directory before running the component.  - number_of_single_mnos: positive integer equal or greater than <code>2</code>, it indicates the number of MNOs whose usual environment data will be aggregated by this component. Example: <code>2</code>. This number also dictates:    - The number of input data paths that need to be specified in the general configuration. The name format of these configuration parameters is <code>single_mno_i_usual_environment_zone_gold</code>, where <code>i</code> must be an integer between <code>1</code> and number_of_single_mnos. All input data path parameters are mandatory. See the configuration example above for the case where there are two MNOs.    - The number of MNO factors. The name format of these configuration parameters is <code>single_mno_i_factor</code>, where <code>i</code> must be an integer between <code>1</code> and number_of_single_mnos. All factor parameters are mandatory and are described below.  - single_mno_<code>i</code>_factor: float between 0 and 1, this parameter (for all values <code>i</code> between <code>1</code> and number_of_single_mnos) represents the weight that the indicator produced by this specific MNO will have in the final aggregation. It is required that all MNO factors add up to <code>1</code>. Example: <code>0.6</code>. See the configuration example below for the case where number_of_single_mnos is equal to <code>2</code>.  - zoning_dataset_id: string, ID of the zones dataset that was previously used to aggregate the data from grid level to zone level. This identifies what specific usual environment output data should be processed by this component. Example: <code>nuts</code>.  - hierarchical_levels: comma-separated list of positive integers, they are the hierarchical levels of the zones dataset used that should be processed by this component. If the zones dataset was not hierarchical, this should just take the value <code>1</code>. Example: <code>1,2,3</code>.  - labels: comma-separated list of strings, they indicate what usual environment labels should be processed by this component. Allowed values are <code>home</code>, <code>work</code>, <code>ue</code>. Example: <code>ue, home, work</code>.  - start_month: string, in <code>YYYY-MM</code> format, it indicates the first month of the period used to compute a specific usual environment dataset that the user desires to process through this component. Example: <code>2023-01</code>.  - end_month: string, in <code>YYYY-MM</code> format, it indicates the last month of the period used to compute a specific usual environment dataset that the user desires to process through this component. Example: <code>2023-03</code>.  - season: string, value of the season of the usual environment dataset that the user desires to prcess through this component. Allowed values are: <code>all</code>, <code>spring</code>, <code>summer</code>, <code>autumn</code>, <code>winter</code>. Example: <code>all</code>.</p> <p>## Internal Migration section  If the configuration parameter internal_migration_execution has been set to <code>True</code>, the component will read the section related to Internal Migration, <code>[MultiMNOAggregation.InternalMigration]</code>. The expected parameters in this section are as follows:  - clear_destination_directory: boolean, whether to delete all previous results in the output directory before running the component.  - number_of_single_mnos: positive integer equal or greater than <code>2</code>, it indicates the number of MNOs whose internal migration data will be aggregated by this component. Example: <code>2</code>. This number also dictates:    - The number of input data paths that need to be specified in the general configuration. The name format of these configuration parameters is <code>single_mno_i_internal_migration_gold</code>, where <code>i</code> must be an integer between <code>1</code> and number_of_single_mnos. All input data path parameters are mandatory. See the configuration example above for the case where there are two MNOs.    - The number of MNO factors. The name format of these configuration parameters is <code>single_mno_i_factor</code>, where <code>i</code> must be an integer between <code>1</code> and number_of_single_mnos. All factor parameters are mandatory and are described below.  - single_mno_<code>i</code>_factor: float between 0 and 1, this parameter (for all values <code>i</code> between <code>1</code> and number_of_single_mnos) represents the weight that the indicator produced by this specific MNO will have in the final aggregation. It is required that all MNO factors add up to <code>1</code>. Example: <code>0.6</code>. See the configuration example below for the case where number_of_single_mnos is equal to <code>2</code>.  - zoning_dataset_id: string, ID of the zones dataset that was used to map grid tiles to zones. This identifies what specific internal migration output data should be processed by this component. Example: <code>nuts</code>.  - hierarchical_levels: comma-separated list of positive integers, they are the hierarchical levels of the zones dataset used that should be processed by this component. If the zones dataset was not hierarchical, this should just take the value <code>1</code>. Example: <code>1,2,3</code>.  - start_month_previous: string, in <code>YYYY-MM</code> format, it indicates the first month of the first long-term period used to compute the internal migration. Example: <code>2023-01</code>.  - end_month_previous: string, in <code>YYYY-MM</code> format, it indicates the last month of the first long-term period used to compute the internal migration. Example: <code>2023-06</code>.  - season_previous: string, value of the season of the first long-term period used to compute the internal migration. Allowed values are: <code>all</code>, <code>spring</code>, <code>summer</code>, <code>autumn</code>, <code>winter</code>. Example: <code>all</code>.  - start_month_new: string, in <code>YYYY-MM</code> format, it indicates the first month of the second long-term period used to compute the internal migration. Example: <code>2023-07</code>.  - end_month_new: string, in <code>YYYY-MM</code> format, it indicates the last month of the second long-term period used to compute the internal migration. Example: <code>2023-12</code>.  - season_new: string, value of the season of the second long-term period used to compute the internal migration. Allowed values are: <code>all</code>, <code>spring</code>, <code>summer</code>, <code>autumn</code>, <code>winter</code>. Example: <code>all</code>.</p>"},{"location":"UserManual/configuration/1_Pipeline/20_multimno_aggregation/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = MultiMNOAggregation\n\n\n[MultiMNOAggregation]\npresent_population_execution = True\nusual_environment_execution = True\ninternal_migration_execution = True\n\n[MultiMNOAggregation.PresentPopulationEstimation]\nclear_destination_directory = True\nnumber_of_single_mnos = 2\nsingle_mno_1_factor = 0.6\nsingle_mno_2_factor = 0.4\n\n# Target PresentPopulationEstimation dataset\nzoning_dataset_id = nuts  # ID of the zoning dataset\nhierarchical_levels = 1,2,3,4  # Hierarchical level(s) of the zoning dataset. Comma-separated list\nstart_date = 2023-01-01  # start date (inclusive)\nend_date = 2023-01-01  # end date (inclusive)\n\n[MultiMNOAggregation.UsualEnvironmentAggregation]\nclear_destination_directory = True\nnumber_of_single_mnos = 2\nsingle_mno_1_factor = 0.6\nsingle_mno_2_factor = 0.4\n\n# Target UsualEnvironmentAggregation dataset\nzoning_dataset_id = nuts  # ID of the zoning dataset\nhierarchical_levels = 1,2,3,4  # Hierarchical level(s) of the zoning dataset. Comma-separated list\nlabels = ue, home, work  # Allowed values: `ue`, `home`, `work`. Comma-separated list\nstart_month = 2023-01  # Start month (inclusive)\nend_month = 2023-03  # End month (inclusive)\nseason = all  # Allowed values: `all`, `spring`, `summer`, `autumn`, `winter`.\n\n[MultiMNOAggregation.InternalMigration]\nclear_destination_directory = True\nnumber_of_single_mnos = 2\nsingle_mno_1_factor = 0.6\nsingle_mno_2_factor = 0.4\n\n# Target InternalMigration dataset\nzoning_dataset_id = nuts  # ID of the zoning dataset\nhierarchical_levels = 1  # Hierarchical level(s) of the zoning dataset. Comma-separated list\n\nstart_month_previous = 2023-01  # Start month (inclusive) of the first long-term period\nend_month_previous = 2023-06  # End month (inclusive) of the first long-term period\nseason_previous = all  # Allowed values: `all`, `spring`, `summer`, `autumn`, `winter`. First long term period\n\nstart_month_new = 2023-07  # Start month (inclusive) of the second long-term period\nend_month_new = 2023-12  # End month (inclusive) of the second long-term period\nseason_new = all  # Allowed values: `all`, `spring`, `summer`, `autumn`, `winter`. Second long term period\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/2_event_data_cleaning/","title":"EventCleaning Configuration","text":"<p>To initialise and run the component two configs are used - general_config.ini and event_cleaning.ini.  In general_config.ini to execute Event Cleaning component specify all paths to its four corresponding data objects (input + output). Example: </p> <pre><code>[Paths.Bronze]\nevent_data_bronze = ${Paths:bronze_dir}/mno_events\n\n[Paths.Silver]\nevent_data_silver = ${Paths:silver_dir}/mno_events\nevent_syntactic_quality_metrics_by_column = ${Paths:silver_dir}/event_syntactic_quality_metrics_by_column\nevent_syntactic_quality_metrics_frequency_distribution = ${Paths:silver_dir}/event_syntactic_quality_metrics_frequency_distribution\n</code></pre> <p>In event_cleaning.ini parameters are as follows: </p> <ul> <li> <p>data_period_start - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-01), the date from which start Event Cleaning</p> </li> <li> <p>data_period_end - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-05), the date till which perform Event Cleaning</p> </li> <li> <p>data_folder_date_format - string, to what string format convert dates so they match the naming of input data folders (it is expected that input data is divided into separate folders for each date of research period). Example: if you know that data for 2023-01-01 is stored in f\"{bronze_event_path}/20230101\", then the format to convert 2023-01-01 date to 20230101 string using strftimewill be %Y%m%d</p> </li> <li> <p>spark_data_folder_date_format - string, as for data_folder_date_format it depends on folder\u2019s naming pattern of input data but since datetime patterns in pyspark and strftime differ, it is a separate config param. Used to convert string to datetype when creating date column in frequency distribution table </p> </li> <li> <p>timestamp_format - str, expected string format of timestamp column when converting it to timestamp type</p> </li> <li> <p>do_bounding_box_filtering- boolean, True/False, decides whether to apply bounding box filtering</p> </li> <li> <p>bounding_box - dictionary, with following keys 'min_lon', 'max_lon', 'min_lat', and 'max_lat' and integer/float values, to specify coordinates of bounding box, within which records should fall, make sure that records and bounding box are in the same src </p> </li> <li> <p>number_of_partitions - an integer, that determines the value of the modulo operator. This value will determine the number expected partitions as to the last partitioning column user_id_modulo. This value does not affect the number of folders in terms of other partitioning columns (day, month, year).</p> </li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/2_event_data_cleaning/#configuration-example","title":"Configuration example","text":"<pre><code>[EventCleaning]\ndata_period_start = 2023-01-01\ndata_period_end = 2023-01-05\ndata_folder_date_format = %Y%m%d\nspark_data_folder_date_format = yyyyMMdd\ntimestamp_format = yyyy-MM-dd'T'HH:mm:ss\ndo_bounding_box_filtering = True\ndo_same_location_deduplication = True\nbounding_box = {\n    'min_lon': -180,\n    'max_lon': 180,\n    'min_lat': -90,\n    'max_lat': 90\n    }\nnumber_of_partitions = 256\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/3_network_cleaning/","title":"NetworkCleaning Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>network_cleaning.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Bronze]\n...\nnetwork_data_bronze = ${Paths:bronze_dir}/mno_network\n...\n\n[Paths.Silver]\n...\nnetwork_data_silver = ${Paths:silver_dir}/mno_network\nnetwork_syntactic_quality_metrics_by_column = ${Paths:silver_quality_metrics_dir}/network_syntactic_quality_metrics_by_column\n...\n</code></pre> <p>The expected parameters in <code>network_cleaning.ini</code> are as follows:</p> <ul> <li>latitude_min: float, minimum accepted latitude (WGS84) for the latitude of cells in the input data. Values lower than this will be treated as out of bounds/range.</li> <li>latitude_max: float, maximum accepted latitude (WGS84) for the latitude of cells in the input data. Values higher than this will be treated as out of bounds/range.</li> <li>longitude_min: float, minimum accepted longitude (WGS84) for the longitude of cells in the input data. Values lower than this will be treated as out of bounds/range.</li> <li>longitude_max: float, minimum accepted longitude (WGS84) for the longitude of cells in the input data. Values higher than this will be treated as out of bounds/range.</li> <li>cell_type_options: comma-separated list of strings, this parameter indicates the accepted values in the <code>cell_type</code> field. Other values will be treated as out of bounds/range. Example: <code>macrocell, microcell, picocell</code>.</li> <li>technology_options: comma-separated list of strings, this parameter indicates the accepted values in the <code>technology</code> field. Other values will be treated as out of bounds/range. Example: <code>5G, LTE, UMTS, GSM</code>.</li> <li>data_period_start: string, format should be the \"yyyy-MM-dd\" (e.g., <code>2023-01-01</code>), the first date for which data will be processed by the component. All dates between this one and the specified in <code>data_period_end</code> will be processed (both inclusive).</li> <li>data_period_end: string, format should be \"yyyy-MM-dd\" (e.g., <code>2023-01-09</code>), the last date for which data will be processed by the component. All dates between the specified in <code>data_period_start</code> and this one will be processed (both inclusive).</li> <li>valid_date_timestamp_format: string, the timestamp format that is expected to be in the input network data and that will be parsed with PySpark using thiis format. Example: <code>yyyy-MM-dd'T'HH:mm:ss</code></li> <li>frequent_error_criterion: string, criterion to use when computing the most frequent errors encountered. It can take two values: <code>absolute</code> if one wants to find the top k most frequent errors (e.g., <code>k=10</code>); or <code>percentage</code> if one wants to find the most frequent errors that represent <code>k</code> percentage of all errors found. Example: <code>percentage</code>.</li> <li>top_k_errors: integer if <code>frequent_error_criterion=absolute</code> or float if <code>top_k_errors</code> if <code>frequent_error_criterion=percentage</code>, represents what portion of the most frequent errors to save. Example: <code>10</code>.</li> <li>do_cell_cgi_check: boolean, default: False. If set to True, cell_id's that do not follow the CGI format will be removed.</li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/3_network_cleaning/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = NetworkCleaning\n\n[NetworkCleaning]\n# Bounding box\nlatitude_min = 40.352\nlatitude_max = 40.486\nlongitude_min = -3.751\nlongitude_max = -3.579\n\ncell_type_options = macrocell, microcell, picocell\ntechnology_options = 5G, LTE, UMTS, GSM\n# Left- and right-inclusive date range for the data to be read\ndata_period_start = 2023-01-01\ndata_period_end = 2023-01-09\n\nvalid_date_timestamp_format = yyyy-MM-dd'T'HH:mm:ss\n\nfrequent_error_criterion = percentage  # allowed values: `absolute`, `percentage`\ntop_k_errors = 40.5\ndo_cell_cgi_check = False\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/4_cell_footprint_estimation/","title":"CellFootprintEstimation Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>cell_footprint_estimation.ini</code>. In <code>general_config.ini</code> to execute the component specify all paths to its corresponding data objects (input + output). Example: </p> <pre><code>[Paths.Silver]\nnetwork_data_silver = ${Paths:silver_dir}/mno_network\ngrid_data_silver = ${Paths:silver_dir}/grid\ncell_footprint_data_silver = ${Paths:silver_dir}/cell_footprint\n</code></pre> <p>In cell_footprint_estimation.ini parameters are as follows: </p> <ul> <li> <p>clear_destination_directory - boolean, if True, the component will clear all the data in output paths.</p> </li> <li> <p>data_period_start - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-01), the date from which start Event Cleaning</p> </li> <li> <p>data_period_end - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-05), the date till which perform Event Cleaning</p> </li> <li> <p>logistic_function_steepness - float, the steepness of the logistic function used to estimate the signal dominance (cell footprint) value from signal strength. The default value is 0.2.</p> </li> <li> <p>logistic_function_midpoint - float, the midpoint of the logistic function used to estimate the signal dominance (cell footprint) value from signal strength. The default value is -92.5.</p> </li> <li> <p>use_elevation - boolean, if True, the elevation data will be used for signal strength modeling. If False, the elevation will be set to 0</p> </li> <li> <p>do_azimuth_angle_adjustments - boolean, if True, signal strengths values will be adjusted based on azimuth and antenna horizontal beam widths for directional cells. If False, no adjustments will be made.</p> </li> <li> <p>do_elevation_angle_adjustments - boolean, if True, signal strengths values will be adjusted based on tilt and antenna vertical beam widths for directional cells. If False, no adjustments will be made.</p> </li> <li> <p>cartesian_crs - integer, the coordinate reference system (CRS) to use for the cartesian coordinates. The default value is 3035.</p> </li> <li> <p>do_difference_from_best_sd_prunning - boolean, if True, the cells per grid tile with signal dominance values that are lower than the threshold percentage from the best signal dominance will be pruned. If False, no pruning of this method will be performed.</p> </li> <li> <p>difference_from_best_sd_treshold - float, the threshold percentage from the best signal dominance value. The default value is 90.</p> </li> <li> <p>do_max_cells_per_tile_prunning - boolean, if True, the maximum number of cells per grid tile will be kept, other cells will be pruned. If False, no pruning of this method will be performed.</p> </li> <li> <p>max_cells_per_grid_tile - integer, the maximum number of cells per grid tile. The default value is 10.</p> </li> <li> <p>do_dynamic_coverage_range_calculation - boolean, if True, the component will calculate the dynamic coverage range for each cell based on signal dominance threshold value. If False, default range will be used. Might be useful to reduce computational load, but might take longer to calculate.</p> </li> <li> <p>signal_dominance_treshold - float, the threshold value for signal dominance. The default value is 0.01.</p> </li> <li> <p>coverage_range_line_buffer - integer, the buffer distance of cell range line in meters for dynammic range calculation. The default value is 50. The value should be set based on reference grid resolution.</p> </li> <li> <p>repartition_number - integer, the number of partitions to use for the repartitioning of the data during dynamic range calculations. The default value is 10.</p> </li> <li> <p>do_sd_treshold_prunning - boolean, if True, the cells with signal dominance values that are lower than the threshold will be pruned. If False, no pruning of this method will be performed.</p> </li> <li> <p>default_cell_physical_properties - dictionary, the default physical properties of the cell types. These properties will be assigned to cells of corresponding type if the properties are not found in the network topology data. If cell types are not peresent in network topology data, the default type properties will be assigned to all cells.</p> </li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/4_cell_footprint_estimation/#configuration-example","title":"Configuration example","text":"<pre><code>[CellFootprintEstimation]\nclear_destination_directory = True\ndata_period_start = 2023-01-01\ndata_period_end = 2023-01-01\nlogistic_function_steepness = 0.2\nlogistic_function_midpoint = -92.5\n\nuse_elevation = False\ndo_azimuth_angle_adjustments = True\ndo_elevation_angle_adjustments = True\ncartesian_crs = 3035\n\ndo_difference_from_best_sd_prunning = False\ndifference_from_best_sd_treshold = 90 # percentage\n\ndo_max_cells_per_tile_prunning = False\nmax_cells_per_grid_tile = 100\n\ndo_dynamic_coverage_range_calculation = False\nsignal_dominance_treshold = 0.01\ncoverage_range_line_buffer = 50\nrepartition_number = 10\n\ndo_sd_treshold_prunning = True\n\ndefault_cell_physical_properties = {\n    'macrocell': {\n        'power': 10,\n        'range': 10000,\n        'path_loss_exponent': 3.75,\n        'antenna_height': 30,\n        'elevation_angle': 5,\n        'vertical_beam_width': 9,\n        'horizontal_beam_width': 65,\n        'azimuth_signal_strength_back_loss': -30,\n        'elevation_signal_strength_back_loss': -30,\n    },\n    'microcell': {\n        'power': 5,\n        'range': 1000,\n        'path_loss_exponent': 6.0,\n        'antenna_height': 8,\n        'elevation_angle': 5,\n        'vertical_beam_width': 9,\n        'horizontal_beam_width': 65,\n        'azimuth_signal_strength_back_loss': -30,\n        'elevation_signal_strength_back_loss': -30,\n    },\n    'default': {\n        'power': 5,\n        'range': 5000,\n        'path_loss_exponent': 3.75,\n        'antenna_height': 8,\n        'elevation_angle': 5,\n        'vertical_beam_width': 9,\n        'horizontal_beam_width': 65,\n        'azimuth_signal_strength_back_loss': -30,\n        'elevation_signal_strength_back_loss': -30,\n        }\n    }\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/5_cell_connection_probability/","title":"CellConnectionProbabilityEstimation Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>cell_connection_probability_estimation</code>.ini. In <code>general_config.ini</code> to execute the component specify all paths to its four corresponding data objects (input + output). Example: </p> <pre><code>[Paths.Bronze]\nevent_data_bronze = ${Paths:bronze_dir}/mno_events\n\n[Paths.Silver]\ncell_footprint_data_silver = ${Paths:silver_dir}/cell_footprint\ngrid_data_silver = ${Paths:silver_dir}/grid\ncell_connection_probabilities_data_silver = ${Paths:silver_dir}/cell_conn_probs\n</code></pre> <p>In cell_connection_probability_estimation.ini parameters are as follows: </p> <ul> <li> <p>data_period_start - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-01), the date from which start Event Cleaning</p> </li> <li> <p>data_period_end - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-05), the date till which perform Event Cleaning</p> </li> <li> <p>use_land_use_prior - boolean, if True, the land use prior will be used for cell connection posterior probability estimation. If False, the land use prior will not be used, only connection probability based on cell footprint will be estimated.</p> </li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/5_cell_connection_probability/#configuration-example","title":"Configuration example","text":"<pre><code>[CellConnectionProbabilityEstimation]\ndata_period_start = 2023-01-01\ndata_period_end = 2023-01-15\nuse_land_use_prior = False\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/6_event_semantic_cleaning/","title":"SemanticCleaning Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>event_semantic_cleaning.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Silver]\n...\nnetwork_data_silver = ${Paths:silver_dir}/mno_network\nevent_data_silver_flagged = ${Paths:silver_dir}/mno_events_flagged\nevent_device_semantic_quality_metrics = ${Paths:silver_quality_metrics_dir}/semantic_quality_metrics\n...\n</code></pre> <p>The expected parameters in <code>event_semantic_cleaning.ini</code> are as follows: - data_period_start: string, format should be the one specified <code>data_period_format</code> (e.g., <code>2023-01-01</code> for <code>%Y-%m-%d</code>), the first date for which data will be processed by the component. All dates between this one and the specified in <code>data_period_end</code> will be processed (both inclusive). - data_period_end: string, format should be \"yyyy-MM-dd\" (e.g., <code>2023-01-09</code> for <code>%Y-%m-%d</code>), the last date for which data will be processed by the component. All dates between the specified in <code>data_period_start</code> and this one will be processed (both inclusive). - data_period_format: string, it indicates the format expected in <code>data_period_start</code> and <code>data_period_end</code>. For example, use <code>%Y-%m-%d</code> for the usual \"2023-01-09\" format separated by <code>-</code>. - semantic_min_distance_m: float, minimum distance (in metres) between two consecutive events above which they will be considered for flagging as suspicious or incorrect location. Example: <code>10000</code>. - semantic_min_speed_m_s: float, minimum mean speed (in metres per second) between two consecutive events above whihc they will be considered for flagging as suspicious or incorrect location. Example: <code>55</code>. - do_different_location_deduplication: boolean, True/False. Determines whether to flag duplicates with different location information (cases where a single user has one or more rows with identical timestamp values, but non-identical values in any other columns).</p>"},{"location":"UserManual/configuration/1_Pipeline/6_event_semantic_cleaning/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = SemanticCleaning\n\n[SemanticCleaning]\ndata_period_start = 2023-01-01\ndata_period_end = 2023-01-09\ndate_format = %Y-%m-%d\n\nsemantic_min_distance_m = 10000\nsemantic_min_speed_m_s = 55\n\ndo_different_location_deduplication = True\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/7_device_activity_statistics/","title":"DeviceActivityStatistics Configuration","text":"<p>To initialise and run the component two configs are used - general_config.ini and device_activity_statistics.ini.  In general_config.ini to execute Device Activity Statistics component specify all paths to its three corresponding data objects (input + output). The local timezone must also be specified in the general config. Example: </p> <pre><code>[Timezone]\nlocal_timezone = UTC\n\n[Paths.Silver]\n# Data\nnetwork_data_silver = ${Paths:silver_dir}/mno_network\n\ndevice_activity_statistics = ${Paths:silver_dir}/device_activity_statistics\n</code></pre> <p>In device_activity_statistics.ini parameters are as follows: </p> <ul> <li> <p>data_period_start - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-01), the date from which start Event Cleaning</p> </li> <li> <p>data_period_end - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-05), the date till which perform Event Cleaning</p> </li> <li> <p>clear_destination_directory - boolean, whether to empty the destination directory before running or not</p> </li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/7_device_activity_statistics/#configuration-example","title":"Configuration example","text":"<pre><code>[DeviceActivityStatistics]\nclear_destination_directory = True\ndata_period_start = 2023-01-01\ndata_period_end = 2023-01-04\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/8_continuous_time_segmentation/","title":"ContinuousTimeSegmentation Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>time_segments.ini</code>. In <code>general_config.ini</code> to execute the component specify all paths to its four corresponding data objects (input + output). Example: </p> <pre><code>[Paths.Silver]\nevent_data_silver_flagged = ${Paths:silver_dir}/mno_events_flagged\ncell_intersection_groups_data_silver = ${Paths:silver_dir}/cell_intersection_groups\ntime_segments_silver = ${Paths:silver_dir}/time_segments\n</code></pre> <p>In time_segments.ini parameters are as follows: </p> <ul> <li> <p>data_period_start - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-01), the date from which start Event Cleaning</p> </li> <li> <p>data_period_end - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-05), the date till which perform Event Cleaning</p> </li> <li> <p>is_first_run - boolean, if True, the component won't use previously calculated time segments. If False, the component will use last calculated time segment per device.</p> </li> <li> <p>event_error_flags_to_include - list of integers, the list of error flags that should be included in the time segments processing. Default value is [0], so only events with no errors are included.</p> </li> <li> <p>min_time_stay_s - integer, the minimum dwell time in seconds for a time segments to be considered as a \"stay\". Default value is 15 minutes.</p> </li> <li> <p>max_time_missing_stay_s - integer, maximum time difference between events to be considered a \u201cstay\u201d. If larger, the time segment will be marked \u201cunknown\u201d. Default value is 12 hours to support devices being offline at home or work addresses.</p> </li> <li> <p>max_time_missing_move_s - integer, maximum time difference between events to be considered a \u201cmove\u201d. If larger, the time segment will be marked \u201cunknown\u201d. Default value is 2 hours.</p> </li> <li> <p>pad_time_s - integer, half the size of an isolated time segment: between two \u201cunknowns\u201d time segments. It expands the isolated event in time, by \u201cpadding\u201d from the \u201cunknown\u201d time segments on both sides. Default value is 5 minutes.</p> </li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/8_continuous_time_segmentation/#configuration-example","title":"Configuration example","text":"<pre><code>[ContinuousTimeSegmentation]\ndata_period_start = 2023-01-01\ndata_period_end = 2023-01-05\n\nis_first_run = true\nevent_error_flags_to_include = [0]\n\nmin_time_stay_s = 900\nmax_time_missing_stay_s = 43200\nmax_time_missing_move_s = 7200\npad_time_s = 300\n</code></pre>"},{"location":"UserManual/configuration/1_Pipeline/9_present_population_estimation/","title":"PresentPopulationEstimation Configuration","text":"<p>To initialise and run the component two configs are used -  <code>general_config.ini</code> and component\u2019s config <code>present_population_estimation.ini</code>. In  <code>general_config.ini</code> all paths to the data objects used by PresentPopulationEstimation component shall be specified. An example with the specified paths is shown below:</p> <pre><code>[Paths.Silver]\n...\ngrid_data_silver = ${Paths:silver_dir}/grid\ncell_connection_probabilities_data_silver = ${Paths:silver_dir}/cell_connection_probabilities_data_silver\nevent_data_silver_flagged = ${Paths:silver_dir}/mno_events_flagged\npresent_population_silver = ${Paths:silver_dir}/present_population\n...\n</code></pre> <p>Below there is a description of the sub component\u2019s config  - <code>present_population_estimation.ini</code>. </p> <p>Parameters are as follows:</p> <p>Under  <code>[PresentPopulationEstimation]</code> config section: </p> <ul> <li>data_period_start - string, format should be \u201cyyyy-MM-dd HH:mm:ss\u201c (e.g. 2023-01-08 12:30:00). Determines when the first time point is generated.</li> <li>data_period_end - string, format should be \u201cyyyy-MM-dd HH:mm:ss\u201c (e.g. 2023-01-08 12:30:00). No time points can be generated after this time. A time point can be generated at this exact time.</li> <li>time_point_gap_s - integer, in seconds. Determines the interval between two time points. Starting from <code>data_period_start</code>, one time point is generated after each <code>time_point_gap_s</code> seconds until <code>data_period_end</code> is reached.</li> <li>tolerance_period_s - integer, in seconds. Determines the size of the temporal window of each time point. Only events within this distance from the time point are included in the results calculation of that point. </li> <li>max_iterations - integer. Maximum number of iteration allowed for the Bayesian process for each time point.</li> <li>min_difference_threshold - float. Minumum difference between Bayesian process prior and posterior population estimates needed to continue iterating the process.</li> </ul>"},{"location":"UserManual/configuration/1_Pipeline/9_present_population_estimation/#configuration-example-grid-level-aggregation","title":"Configuration example: grid-level aggregation","text":"<pre><code>[Logging]\nlevel = DEBUG\n\n[Spark]\nsession_name = PresentPopulationEstimationGrid\n\n[PresentPopulationEstimation]\ndata_period_start = 2023-01-01 00:00:00 # Starting bound when to create time points. The first time point is created at this timestamp. \ndata_period_end = 2023-01-02 00:00:00 # Ending bound when to create time points. No time points are generated later than this timestamp. A time point can happen to be generated on this timestamp, but this is not always the case.\ntime_point_gap_s = 43200 # space between consecutive time points\ntolerance_period_s = 3600 # Maximum allowed time difference for an event to be included in a time point\nnr_of_user_id_partitions = 128 # Total number of user_id_modulo partitions. TODO should be a global conf value\nnr_of_user_id_partitions_per_slice = 32 # Number of user_id_modulo partitions to process at one time\nmax_iterations = 20 # Number of iterations allowed for the Bayesian process\nmin_difference_threshold = 10000 # Minimum total difference between Bayesian process prior and posterior needed to continue processing \n</code></pre>"},{"location":"UserManual/configuration/2_Optional/1_grid_enrichment/","title":"GridEnrichment Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>grid_enrichment.ini</code>. In <code>general_config.ini</code> to execute the component specify all paths to its corresponding data objects (input + output). </p> <pre><code>[Paths.Bronze]\ntransportation_data_bronze = ${Paths:bronze_dir}/spatial/transportation\nlanduse_data_bronze = ${Paths:bronze_dir}/spatial/landuse\n[Paths.Silver]\ngrid_data_silver = ${Paths:silver_dir}/grid\nenriched_grid_data_silver = ${Paths:silver_dir}/grid_enriched\n</code></pre> <p>In grid_enrichment.ini parameters are as follows: </p> <ul> <li> <p>clear_destination_directory - boolean, if True, the component will clear all the data in output paths.</p> </li> <li> <p>do_landcover_enrichment - boolean, if True, the component will enrich the grid with landuse prior probabilities and Path Loss Exponent environment coefficient.</p> </li> <li> <p>transportation_category_buffer_m - dictionary, buffer distance for each transportation category in meters. Used to convert transportation lines to polygons.</p> </li> <li> <p>prior_weights - dictionary, weights for each landuse category. Used to calculate prior probabilities.</p> </li> <li> <p>ple_coefficient_weights - dictionary, weights for each landuse category. Used to calculate Path Loss Exponent coefficient.</p> </li> <li> <p>do_elevation_enrichment - boolean, if True, the component will enrich the grid with elevation data. Not implemented yet.</p> </li> </ul>"},{"location":"UserManual/configuration/2_Optional/1_grid_enrichment/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = InspireGridGeneration\n\n[InspireGridGeneration]\n[Spark]\nsession_name = GridEnrichment\n\n[GridEnrichment]\nclear_destination_directory = True\ndo_landcover_enrichment = True\nprior_calculation_repartition_size = 12\ntransportation_category_buffer_m = {\n    \"primary\": 30,\n    \"secondary\": 15,\n    \"tertiary\": 5,\n    \"pedestrian\": 5,\n    \"railroad\": 15,\n    \"unknown\": 2\n    }\nprior_weights = {\n    \"residential_builtup\": 1.0,\n    \"other_builtup\": 1.0,\n    \"roads\": 0.5,\n    \"open_area\": 0.0,\n    \"forest\": 0.1,\n    \"water\": 0.0\n    }\nple_coefficient_weights = {\n    \"residential_builtup\": 1.0,\n    \"other_builtup\": 1.0,\n    \"roads\": 0.0,\n    \"open_area\": 0.0,\n    \"forest\": 1.0,\n    \"water\": 0.0\n    }\ndo_elevation_enrichment = False\n</code></pre>"},{"location":"UserManual/configuration/2_Optional/2_geozones_to_grid_mapping/","title":"GeozonesGridMapping Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>geozones_grid_mapping.ini</code>. In <code>general_config.ini</code> to execute the component specify all paths to its corresponding data objects (input + output). </p> <pre><code>[Paths.Bronze]\ngeographic_zones_data_bronze = ${Paths:bronze_dir}/spatial/geographic_zones \nadmin_units_data_bronze = ${Paths:bronze_dir}/spatial/admin_units \n[Paths.Silver]\ngrid_data_silver = ${Paths:silver_dir}/grid\ngeozones_grid_map_data_silver = ${Paths:silver_dir}/geozones_grid_map\n</code></pre> <p>In geozones_grid_mapping.ini parameters are as follows: </p> <ul> <li> <p>clear_destination_directory - boolean, if True, the component will clear all the data in output paths.</p> </li> <li> <p>zoning_type - string, type of zoning data object to be used for mapping. Possible values are \"admin\" and \"other\".</p> </li> <li> <p>dataset_ids - list, ids of zonning datasets to use for mapping. Grid mapping will be done for each dataset separately.</p> </li> </ul>"},{"location":"UserManual/configuration/2_Optional/2_geozones_to_grid_mapping/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = GeozonesGridMapping\n\n[GeozonesGridMapping]\nclear_destination_directory = True\nzoning_type = other # admin or other\ndataset_ids = ['nuts'] # list of dataset ids to map grid to\n</code></pre>"},{"location":"UserManual/configuration/3_QualityWarnings/1_network_quality_warnings/","title":"NetworkQualityWarnings Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>network_quality_warnings.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Silver]\n...\nnetwork_syntactic_quality_metrics_by_column = ${Paths:silver_quality_metrics_dir}/network_syntactic_quality_metrics_by_column\nnetwork_syntactic_quality_warnings_log_table = ${Paths:silver_quality_warnings_dir}/network_syntactic_quality_warnings_log_table\nnetwork_syntactic_quality_warnings_line_plot_data = ${Paths:silver_quality_warnings_dir}/network_syntactic_quality_warnings_line_plot_data\nnetwork_syntactic_quality_warnings_pie_plot_data = ${Paths:silver_quality_warnings_dir}/network_syntactic_quality_warnings_pie_plot_data\n...\n</code></pre> <p>The expected parameters in <code>network_quality_warnings.ini</code> are as follows: - date_of_study: string, format should be the one specified <code>date_format</code> (e.g., <code>2023-01-01</code> for <code>%Y-%m-%d</code>), the first date for which data will be processed by the component. All dates between this one and the specified in <code>data_period_end</code> will be processed (both inclusive). - date_format: string, it indicates the format expected in <code>date_of_study</code>. For example, use <code>%Y-%m-%d</code> for the usual \"2023-01-01\" format separated by <code>-</code>. - lookback_period: string, it indicates the length of the lookback period used to compare the metrics of the date of study with past data volume and error rates. Three possible values are accepted: <code>week</code>, <code>month</code>, and <code>quarter</code>. - thresholds: dictionary, indicating the different thresholds to be used for raising warnings.</p>"},{"location":"UserManual/configuration/3_QualityWarnings/1_network_quality_warnings/#thresholds","title":"Thresholds","text":"<p>The thresholds parameter is a dictionary of dictionaries, used to indicate the different thresholds to be used for each type of warning. In the case that one of the thresholds is missing, a default value will be used instead. The default values are contained in <code>multimno.core.constants.network_default_thresholds.NETWORK_DEFAULT_THRESHOLDS</code>.</p> <p>Ihe dictionary structure is as follows: - <code>\"SIZE_RAW_DATA\"</code>: refers to the size of the input data.   - <code>\"OVER_AVERAGE\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the input raw file is greater than this threshold, in percentage different, when compared to the average over the lookback period. By default, the value is <code>30</code>.   - <code>\"UNDER_AVERAGE\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the input raw file is lower than this threshold, in percentage different, when compared to the average over the lookback period. By default, the value is <code>30</code>.   - <code>\"VARIABILITY\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the input raw file differs from its average over the lookback period by more than its standard deviation multiplied by this threshold. This is, this value is a threshold of the number of standard deviations than a value can differ from its average over the lookback period, either over or under the average. By default, the value is <code>2</code>.   - <code>\"ABS_VALUE_UPPER_LIMIT\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the input raw file is greater than this value. By default, it is equal to the threshold calculated from the <code>VARIABILITY</code> parameter over the average.   - <code>\"ABS_VALUE_LOWER_LIMIT\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the input raw file is greater than this value. By default, it is equal to the threshold calculated from the <code>VARIABILITY</code> parameter under the average.</p> <ul> <li><code>\"SIZE_CLEAN_DATA\"</code>: refers to the size of the output data.</li> <li><code>\"OVER_AVERAGE\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the output processed file is greater than this threshold, in percentage different, when compared to the average over the lookback period. By default, the value is <code>30</code>.</li> <li><code>\"UNDER_AVERAGE\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the output processed file is lower than this threshold, in percentage different, when compared to the average over the lookback period. By default, the value is <code>30</code>.</li> <li><code>\"VARIABILITY\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the output processed file differs from its average over the lookback period by more than its standard deviation multiplied by this threshold. This is, this value is a threshold of the number of standard deviations than a value can differ from its average over the lookback period, either over or under the average. By default, the value is <code>2</code>.</li> <li><code>\"ABS_VALUE_UPPER_LIMIT\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the output processed file is greater than this value. by default, it is equal to the threshold calculated from the <code>VARIABILITY</code> parameter over the average.</li> <li> <p><code>\"ABS_VALUE_LOWER_LIMIT\"</code>: integer or float, indicates the threshold for a warning to be raised when the number of rows in the output processed file is greater than this value. By default, it is equal to the threshold calculated from the <code>VARIABILITY</code> parameter under the average.</p> </li> <li> <p><code>\"TOTAL_ERROR_RATE\"</code>: refers to the percentage of rows preserved from the input file, i.e., the rows that passed the cleaning/check procedure.</p> </li> <li><code>\"OVER_AVERAGE\"</code>: integer or float, indicates the threshold for a warning to be raised when the error rate is greater than this threshold, in percentage different, when compared to the average over the lookback period. By default, the value is <code>30</code>.</li> <li><code>\"VARIABILITY\"</code>: integer or float, indicates the threshold for a warning to be raised when the error rate differs from its average over the lookback period by more than its standard deviation multiplied by this threshold. This is, this value is a threshold of the number of standard deviations than a value can differ from its average over the lookback period, obly over the average. By default, the value is <code>2</code>.</li> <li> <p><code>\"ABS_VALUE_UPPER_LIMIT\"</code>: integer or float, indicates the threshold for a warning to be raised when the error rate is greater than this value. By default, the value is <code>20</code>.</p> </li> <li> <p><code>\"Missing_value_RATE\"</code>: refers to the percentage of missing/null values of a given field in the input data. The value of this dictionary key is a dictionary where the keys are field names, and the values are a dictionary containing the thresholds form this error type and field.</p> </li> <li>Admitted keys (i.e., fields) are: <code>cell_id</code>, <code>valid_date_start</code>, <code>latitude</code>, <code>longitude</code>, <code>altitude</code>, <code>antenna_height</code>, <code>directionality</code>, <code>azimuth_angle</code>, <code>elevation_angle</code>, <code>horizontal_beam_width</code>, <code>vertical_beam_width</code>, <code>power</code>, <code>frequency</code>, <code>technology</code>, and <code>cell_type</code>.</li> <li> <p>Each key (i.e., field) has the following thresholds:</p> <ul> <li><code>\"AVERAGE\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate is greater than this threshold, in percentage different, when compared to the average over the lookback period. By default, the value is <code>30</code> for <code>cell_id</code>, <code>latitude</code>, and <code>longitude</code>, and <code>60</code> otherwise.</li> <li><code>\"VARIABILITY\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate differs from its average over the lookback period by more than its standard deviation multiplied by this threshold. This is, this value is a threshold of the number of standard deviations than a value can differ from its average over the lookback period, obly over the average. By default, the value is <code>2</code> for <code>cell_id</code>, <code>latitude</code>, and <code>longitude</code>, and <code>3</code> otherwise.</li> <li><code>\"ABS_VALUE_UPPER_LIMIT\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate is greater than this value. By default, the value is <code>20</code> for <code>cell_id</code>, <code>latitude</code>, and <code>longitude</code>, and <code>50</code> otherwise.</li> </ul> </li> <li> <p><code>\"Out_of_range_RATE\"</code>: refers to the percentage of out of bounds, out of range or invalid values of a given field in the input data. The value of this dictionary key is a dictionary where the keys are field names, and the values are a dictionary containing the thresholds form this error type and field.</p> </li> <li>Admitted keys (i.e., fields) are: <code>\"cell_id\"</code>, <code>\"latitude\"</code>, <code>\"longitude\"</code>, <code>\"antenna_height\"</code>, <code>\"directionality\"</code>, <code>\"azimuth_angle\"</code>, <code>\"elevation_angle\"</code>, <code>\"horizontal_beam_width\"</code>, <code>\"vertical_beam_width\"</code>, <code>\"power\"</code>, <code>\"frequency\"</code>, <code>\"technology\"</code>, and <code>\"cell_type\"</code>. Exceptionally, the <code>None</code> value is also accepted, referring to the specific error where <code>valid_date_end</code> is a point int time earlier than <code>valid_date_start</code>.</li> <li> <p>Each key has the following thresholds:</p> <ul> <li><code>\"AVERAGE\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate is greater than this threshold, in percentage different, when compared to the average over the lookback period. By default, the value is <code>20</code> for <code>cell_id</code>, <code>latitude</code>, and <code>longitude</code>, and <code>60</code> otherwise.</li> <li><code>\"VARIABILITY\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate differs from its average over the lookback period by more than its standard deviation multiplied by this threshold. This is, this value is a threshold of the number of standard deviations than a value can differ from its average over the lookback period, obly over the average. By default, the value is <code>2</code> for <code>cell_id</code>, <code>latitude</code>, and <code>longitude</code>, and <code>3</code> otherwise.</li> <li><code>\"ABS_VALUE_UPPER_LIMIT\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate is greater than this value. By default, the value is <code>20</code> for <code>cell_id</code>, <code>latitude</code>, and <code>longitude</code>, and <code>50</code> otherwise.</li> </ul> </li> <li> <p><code>\"Parsing_error_RATE\"</code>: refers to values that could not be parsed.</p> </li> <li><code>\"valid_date_start\"</code>:<ul> <li><code>\"AVERAGE\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate is greater than this threshold, in percentage different, when compared to the average over the lookback period. By default, the value is <code>60</code>.</li> <li><code>\"VARIABILITY\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate differs from its average over the lookback period by more than its standard deviation multiplied by this threshold. This is, this value is a threshold of the number of standard deviations than a value can differ from its average over the lookback period, obly over the average. By default, the value is <code>3</code>.</li> <li><code>\"ABS_VALUE_UPPER_LIMIT\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate is greater than this value. By default, the value is <code>50</code>.</li> </ul> </li> </ul>"},{"location":"UserManual/configuration/3_QualityWarnings/1_network_quality_warnings/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = NetworkQualityWarnings\n\n[NetworkQualityWarnings]\ndate_format = %Y-%m-%d\ndate_of_study = 2023-01-08\nlookback_period = week\n\n# All values must be numeric\n# Missing parameter will take the default value\n# Incorrect value will throw an error\nthresholds = {\n    \"SIZE_RAW_DATA\": {\n        \"OVER_AVERAGE\": 30,\n        \"UNDER_AVERAGE\": 30,\n        \"VARIABILITY\": 2,\n        \"ABS_VALUE_UPPER_LIMIT\": None,\n        \"ABS_VALUE_LOWER_LIMIT\": None,\n    },\n    \"SIZE_CLEAN_DATA\": {\n        \"OVER_AVERAGE\": 30,\n        \"UNDER_AVERAGE\": 30,\n        \"VARIABILITY\": 2,\n        \"ABS_VALUE_UPPER_LIMIT\": None,\n        \"ABS_VALUE_LOWER_LIMIT\": None,\n    },\n    \"TOTAL_ERROR_RATE\": {\n        \"OVER_AVERAGE\": 30,\n        \"VARIABILITY\": 2,\n        \"ABS_VALUE_UPPER_LIMIT\": 20,\n    },\n    \"Missing_value_RATE\": {\n        \"cell_id\": {\n            \"AVERAGE\": 30,\n            \"VARIABILITY\": 2,\n            \"ABS_VALUE_UPPER_LIMIT\": 20,\n        },\n        \"valid_date_start\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"latitude\": {\n            \"AVERAGE\": 30,\n            \"VARIABILITY\": 2,\n            \"ABS_VALUE_UPPER_LIMIT\": 20,\n        },\n        \"longitude\": {\n            \"AVERAGE\": 30,\n            \"VARIABILITY\": 2,\n            \"ABS_VALUE_UPPER_LIMIT\": 20,\n        },\n        \"altitude\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"antenna_height\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"directionality\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"azimuth_angle\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"elevation_angle\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"horizontal_beam_width\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"vertical_beam_width\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"power\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"frequency\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"technology\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"cell_type\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n    },\n    \"Out_of_range_RATE\": {\n        \"cell_id\": {\n            \"AVERAGE\": 30,\n            \"VARIABILITY\": 2,\n            \"ABS_VALUE_UPPER_LIMIT\": 20,\n        },\n        \"latitude\": {\n            \"AVERAGE\": 30,\n            \"VARIABILITY\": 2,\n            \"ABS_VALUE_UPPER_LIMIT\": 20,\n        },\n        \"longitude\": {\n            \"AVERAGE\": 30,\n            \"VARIABILITY\": 2,\n            \"ABS_VALUE_UPPER_LIMIT\": 20,\n        },\n        \"antenna_height\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"directionality\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"azimuth_angle\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"elevation_angle\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"horizontal_beam_width\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"vertical_beam_width\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"power\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"frequency\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"technology\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"cell_type\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        None: {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n    },\n    \"Parsing_error_RATE\": {\n        \"valid_date_start\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        },\n        \"valid_date_end\": {\n            \"AVERAGE\": 60,\n            \"VARIABILITY\": 3,\n            \"ABS_VALUE_UPPER_LIMIT\": 50,\n        }\n    }\n    }\n</code></pre>"},{"location":"UserManual/configuration/3_QualityWarnings/2_event_quality_warnings/","title":"EventQualityWarnings Configuration","text":"<p>To initialise and run the component two configs are used -  <code>general_config.ini</code> and component\u2019s config (either  <code>event_cleaning_quality_warnings.ini</code> or  <code>event_deduplication_quality_warnings.ini</code>). In  <code>general_config.ini</code> to execute Event Quality Warnings component specify all paths to its corresponding data objects. Example with specified paths for both cases:</p> <pre><code>[Paths.Silver]\n...\n# for Event Cleaning Quality Warnings\nevent_syntactic_quality_metrics_by_column = ${Paths:silver_dir}/event_syntactic_quality_metrics_by_column\nevent_syntactic_quality_metrics_frequency_distribution = ${Paths:silver_dir}/event_syntactic_quality_metrics_frequency_distribution\nevent_syntactic_quality_warnings_log_table = ${Paths:silver_dir}/event_syntactic_quality_warnings_log_table\nevent_syntactic_quality_warnings_for_plots = ${Paths:silver_dir}/event_syntactic_quality_warnings_for_plots\n</code></pre> <p>Below there is a description of one of sub component\u2019s config  - <code>event_cleaning_quality_warnings.ini</code>. </p> <p>Parameters are as follows:</p> <p>Under <code>[EventQualityWarnings]</code> config section: </p> <ul> <li> <p>input_qm_by_column_path_key - string, key in Paths.Silver section in general config for a path to corresponding Quality Metrics By Column data</p> </li> <li> <p>input_qm_freq_distr_path_key - string, key in Paths.Silver section in general config for a path to corresponding Quality Metrics Frequency Distribution data</p> </li> <li> <p>output_qw_log_table_path_key - string, key in Paths.Silver section in general config for a path to write output Quality Warnings Log Table</p> </li> <li> <p>output_qw_for_plots_path_key - string, key in Paths.Silver section in general config for a path to write output Quality Warnings ForPLots</p> </li> <li> <p>data_period_start - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-08), the date from which start Event Quality Warnings, by now make sure the first day(s) of research period has enough previous data in in Quality Metrics Frequency Fistribution and Quality Metrics By Column </p> </li> <li> <p>data_period_end - string, format should be \u201cyyyy-MM-dd\u201c (e.g. 2023-01-15), the date till which perform Event Quality Warnings</p> </li> <li> <p>lookback_period - the length of lookback period, represented as string (could be either \u2018week' or 'month') which than will get its numeric representation in number of days</p> </li> <li> <p>do_size_raw_data_qw - boolean, whether perform QW checks on <code>initial_frequency</code> column in Quality Metrics Frequency Fistribution</p> </li> <li> <p>do_size_clean_data_qw - boolean, whether perform QW checks on <code>final_frequency</code> column in Quality Metrics Frequency Distribution</p> </li> <li> <p>data_size_tresholds - dictionary, with keys as thresholds names and values as corresponding thresholds values for  <code>do_size_raw_data_qw</code> and <code>do_size_clean_data_qw</code></p> </li> <li> <p>do_error_rate_by_date_qw - boolean, whether to perform QW checks on total error rate by <code>date</code></p> </li> <li> <p>do_error_rate_by_date_and_cell_qw -  boolean, whether to perform QW checks on total error rate by <code>date</code> and <code>cell_id</code></p> </li> <li> <p>do_error_rate_by_date_and_user_qw -  boolean, whether to perform QW checks on total error rate by <code>date</code> and <code>user_id</code></p> </li> <li> <p>do_error_rate_by_date_and_cell_user_qw - boolean, whether to perform QW checks on total error rate by <code>date</code>, <code>cell_id</code> and <code>user_id</code></p> </li> <li> <p>error_rate_tresholds - dictionary, with keys as thresholds names and values as corresponding thresholds values for  <code>do_error_rate_by_date_qw</code>, <code>do_error_rate_by_date_and_cell_qw</code>, <code>do_error_rate_by_date_and_user_qw</code>, and <code>do_error_rate_by_date_and_cell_user_qw</code></p> </li> <li> <p>error_type_qw_checks - dictionary, where the keys are names of error types (please see <code>multimno/core/constants/error_types.py</code> file) and values list of column names on which you want to perform QWs of the this error type. Example: during Event Cleaning three columns are checked for null values, if you want to check error rate of <code>missing_value</code> type for all mentioned columns specify them in the list. Some error types might have None for column names, which means that technically this kind or error do not belong to just one column but several (e.g. for <code>no_location</code> error three columns are used - cell_id, lat, lon): </p> </li> </ul> <p><pre><code>error_type_qw_checks = {\n    'missing_value': ['user_id', 'mcc', 'timestamp'],\n    'out_of_bounding_box':[None]\n    }\n</code></pre> If you do not intend to run QWs on some error type leave its corresponding list of columns empty.</p> <ul> <li>thresholds for each error_type &amp; column combination you want to compute QWs  - thresholds are combined in groups: each set of thresholds relevant to some error type is a separate config param of type dictionary, where keys are column names, values is another dictionary of structure: <code>threshold_name:threshold_value</code>. Example: </li> </ul> <p><pre><code>missing_value_thresholds = {\n    'user_id': {\"Missing_value_RATE_BYDATE_USER_AVERAGE\": 30,\n                \"Missing_value_RATE_BYDATE_USER_VARIABILITY\": 2,\n                \"Missing_value_RATE_BYDATE_USER_ABS_VALUE_UPPER_LIMIT\": 20\n               }, \n    'mcc': {\"Missing_value_RATE_BYDATE_MCC_AVERAGE\": 30,\n            \"Missing_value_RATE_BYDATE_MCC_VARIABILITY\": 2,\n            \"Missing_value_RATE_BYDATE_MCC_ABS_VALUE_UPPER_LIMIT\": 20\n           }, \n    'timestamp': {\"Missing_value_RATE_BYDATE_TIMESTAMP_AVERAGE\": 30,\n                  \"Missing_value_RATE_BYDATE_TIMESTAMP_VARIABILITY\": 2,\n                  \"Missing_value_RATE_BYDATE_TIMESTAMP_ABS_VALUE_UPPER_LIMIT\": 20\n                 }\n    }\nout_of_bounding_box_thresholds = {\n    None: {\"Out_of_bbox_RATE_BYDATE_AVERAGE\": 30,\n           \"Out_of_bbox_RATE_BYDATE_VARIABILITY\": 2,\n           \"Out_of_bbox_RATE_BYDATE_ABS_VALUE_UPPER_LIMIT\": 20\n          }\n    }\n</code></pre> Make sure, that for each column of interest specified in <code>error_type_qw_checks</code> there are corresponding thresholds. The order of thresholds is important and should be: <code>AVERAGE</code>, <code>VARIABILITY</code>, and <code>ABS_VALUE_UPPER_LIMIT</code> (at least by now all error type QWs follow the same logic and thus their computation is done within one function with ordered threshold arguments). Currently the code supports running QWs on following thresholds: </p> <p><pre><code># possible thresholds in event_cleaning_quality_warnings.ini\nmissing_value_thresholds\n\nout_of_admissible_values_thresholds\n\nnot_right_syntactic_format_thresholds\n\nno_location_thresholds\n\nno_domain_thresholds\n\nout_of_bounding_box_thresholds\n\ndeduplication_same_location_thresholds\n</code></pre> - clear_destination_directory - boolean, if True deletes all output of the Component in init stage</p>"},{"location":"UserManual/configuration/3_QualityWarnings/2_event_quality_warnings/#configuration-example","title":"Configuration example","text":"<pre><code>[EventQualityWarnings]\n# keys in Paths.Silver section in general config\ninput_qm_by_column_path_key = event_syntactic_quality_metrics_by_column\ninput_qm_freq_distr_path_key = event_syntactic_quality_metrics_frequency_distribution\noutput_qw_log_table_path_key = event_syntactic_quality_warnings_log_table\noutput_qw_for_plots_path_key = event_syntactic_quality_warnings_for_plots\n# BY NOW make sure that the first day(s) of research period has enough previous data\n# of df_qa_by_column and df_qa_freq_distribution \n# (e.g. staring from 2023-01-01, if period is a week and start period is 2023-01-08)\ndata_period_start = 2023-01-01\n# you can exceed max(df_qa_by_column.date) \n# although you will still get QWs for dates till max(df_qa_by_column.date), including\ndata_period_end = 2023-01-09\n# should be either week or month\nlookback_period = week\n# SIZE QA\ndo_size_raw_data_qw = True\ndo_size_clean_data_qw = True\ndata_size_tresholds = {\n    \"SIZE_RAW_DATA_BYDATE_VARIABILITY\": 3,\n    \"SIZE_RAW_DATA_BYDATE_ABS_VALUE_UPPER_LIMIT\": 10000000,\n    \"SIZE_RAW_DATA_BYDATE_ABS_VALUE_LOWER_LIMIT\": 0,\n    \"SIZE_CLEAN_DATA_BYDATE_VARIABILITY\": 3,\n    \"SIZE_CLEAN_DATA_BYDATE_ABS_VALUE_UPPER_LIMIT\": 10000000,\n    \"SIZE_CLEAN_DATA_BYDATE_ABS_VALUE_LOWER_LIMIT\": 0,\n    }\n# ERROR RATE QW\ndo_error_rate_by_date_qw = True\ndo_error_rate_by_date_and_cell_qw = False\ndo_error_rate_by_date_and_user_qw = True\ndo_error_rate_by_date_and_cell_user_qw = True\nerror_rate_tresholds = {\n    \"TOTAL_ERROR_RATE_BYDATE_OVER_AVERAGE\": 30,\n    \"TOTAL_ERROR_RATE_BYDATE_VARIABILITY\": 2,\n    \"TOTAL_ERROR_RATE_BYDATE_ABS_VALUE_UPPER_LIMIT\": 20,\n    \"ERROR_RATE_BYDATE_BYCELL_OVER_AVERAGE\": 30,\n    \"ERROR_RATE_BYDATE_BYCELL_VARIABILITY\": 2,\n    \"ERROR_RATE_BYDATE_BYCELL_ABS_VALUE_UPPER_LIMIT\": 20,\n    \"ERROR_RATE_BYDATE_BYUSER_OVER_AVERAGE\": 30,\n    \"ERROR_RATE_BYDATE_BYUSER_VARIABILITY\": 2,\n    \"ERROR_RATE_BYDATE_BYUSER_ABS_VALUE_UPPER_LIMIT\": 20,\n    \"ERROR_RATE_BYDATE_BYCELL_USER_OVER_AVERAGE\": 30,\n    \"ERROR_RATE_BYDATE_BYCELL_USER_VARIABILITY\": 2,\n    \"ERROR_RATE_BYDATE_BYCELL_USER_ABS_VALUE_UPPER_LIMIT\": 20,\n    }\n# ERROR TYPE QW\n# for each type of error (key), specified the colums you want to check, naming of columns must be oidentical to ColNames\n# if you do not want to run qw on some error_type leave the list empty\n# None - for no_location and out_of_bounding_box because they do not have more than one column used for this error_type\n# for more clarity please check event_cleaning.py\nerror_type_qw_checks = {\n    'missing_value': ['user_id', 'mcc', 'timestamp'],\n    'out_of_admissible_values': ['cell_id', 'mcc', 'mnc', 'plmn', 'timestamp'],\n    'not_right_syntactic_format': ['timestamp'], \n    'no_domain': [None],\n    'no_location':[None], \n    'out_of_bounding_box':[None],\n    'same_location_duplicate':[None]\n    }\n# for each dict_error_type_thresholds make sure you specified all relevant columns\n# the order of thresholds is important, should be: AVERAGE, VARIABILITY, and ABS_VALUE_UPPER_LIMIT\nmissing_value_thresholds = {\n    'user_id': {\"Missing_value_RATE_BYDATE_USER_AVERAGE\": 30,\n                \"Missing_value_RATE_BYDATE_USER_VARIABILITY\": 2,\n                \"Missing_value_RATE_BYDATE_USER_ABS_VALUE_UPPER_LIMIT\": 20\n               }, \n    'mcc': {\"Missing_value_RATE_BYDATE_MCC_AVERAGE\": 30,\n            \"Missing_value_RATE_BYDATE_MCC_VARIABILITY\": 2,\n            \"Missing_value_RATE_BYDATE_MCC_ABS_VALUE_UPPER_LIMIT\": 20\n           },\n    'mnc': {\"Missing_value_RATE_BYDATE_MNC_AVERAGE\": 30,\n            \"Missing_value_RATE_BYDATE_MNC_VARIABILITY\": 2,\n            \"Missing_value_RATE_BYDATE_MNC_ABS_VALUE_UPPER_LIMIT\": 20\n           }, \n    'plmn': {\"Missing_value_RATE_BYDATE_PLMN_AVERAGE\": 30,\n            \"Missing_value_RATE_BYDATE_PLMN_VARIABILITY\": 2,\n            \"Missing_value_RATE_BYDATE_PLMN_ABS_VALUE_UPPER_LIMIT\": 20\n           }, \n    'timestamp': {\"Missing_value_RATE_BYDATE_TIMESTAMP_AVERAGE\": 30,\n                  \"Missing_value_RATE_BYDATE_TIMESTAMP_VARIABILITY\": 2,\n                  \"Missing_value_RATE_BYDATE_TIMESTAMP_ABS_VALUE_UPPER_LIMIT\": 20\n                 }\n    }\nout_of_admissible_values_thresholds = {\n    'cell_id': {\"Out_of_range_RATE_BYDATE_CELL_AVERAGE\": 30,\n                \"Out_of_range_RATE_BYDATE_CELL_VARIABILITY\": 2,\n                \"Out_of_range_RATE_BYDATE_CELL_ABS_VALUE_UPPER_LIMIT\": 20\n               }, \n    'mcc': {\"Out_of_range_RATE_BYDATE_MCC_AVERAGE\": 30,\n            \"Out_of_range_RATE_BYDATE_MCC_VARIABILITY\": 2,\n            \"Out_of_range_RATE_BYDATE_MCC_ABS_VALUE_UPPER_LIMIT\": 20\n           }, \n    'mnc': {\"Out_of_range_RATE_BYDATE_MNC_AVERAGE\": 30,\n            \"Out_of_range_RATE_BYDATE_MNC_VARIABILITY\": 2,\n            \"Out_of_range_RATE_BYDATE_MNC_ABS_VALUE_UPPER_LIMIT\": 20\n           },\n    'plmn': {\"Out_of_range_RATE_BYDATE_PLMN_AVERAGE\": 30,\n            \"Out_of_range_RATE_BYDATE_PLMN_VARIABILITY\": 2,\n            \"Out_of_range_RATE_BYDATE_PLMN_ABS_VALUE_UPPER_LIMIT\": 20\n           },\n    'timestamp': {\"Out_of_range_RATE_BYDATE_TIMESTAMP_AVERAGE\": 30,\n                  \"Out_of_range_RATE_BYDATE_TIMESTAMP_VARIABILITY\": 2,\n                  \"Out_of_range_RATE_BYDATE_TIMESTAMP_ABS_VALUE_UPPER_LIMIT\": 20\n                 }\n    }\nnot_right_syntactic_format_thresholds = {\n    'timestamp': {\"Wrong_type_RATE_BYDATE_TIMESTAMP_AVERAGE\": 30,\n                  \"Wrong_type_RATE_BYDATE_TIMESTAMP_VARIABILITY\": 2,\n                  \"Wrong_type_RATE_BYDATE_TIMESTAMP_ABS_VALUE_UPPER_LIMIT\": 20\n                 }\n    }\nno_location_thresholds = {\n    None: {\"No_location_RATE_BYDATE_AVERAGE\": 30,\n           \"No_location_RATE_BYDATE_VARIABILITY\": 2,\n           \"No_location_RATE_BYDATE_ABS_VALUE_UPPER_LIMIT\": 20\n          }\n    }\nno_domain_thresholds = {\n    None: {\"No_domain_RATE_BYDATE_AVERAGE\": 30,\n           \"No_domain_RATE_BYDATE_VARIABILITY\": 2,\n           \"No_domain_RATE_BYDATE_ABS_VALUE_UPPER_LIMIT\": 20\n          }\n    }\nout_of_bounding_box_thresholds = {\n    None: {\"Out_of_bbox_RATE_BYDATE_AVERAGE\": 30,\n           \"Out_of_bbox_RATE_BYDATE_VARIABILITY\": 2,\n           \"Out_of_bbox_RATE_BYDATE_ABS_VALUE_UPPER_LIMIT\": 20\n          }\n    }\ndeduplication_same_location_thresholds = {\n    None: {\"Deduplication_same_location_RATE_BYDATE_AVERAGE\": 30,\n           \"Deduplication_same_location_RATE_BYDATE_VARIABILITY\": 2,\n           \"Deduplication_same_location_RATE_BYDATE_ABS_VALUE_UPPER_LIMIT\": 20\n          }\n    }\n\nclear_destination_directory = True\n</code></pre>"},{"location":"UserManual/configuration/3_QualityWarnings/3_semantic_quality_warnings/","title":"SemanticQualityWarnings Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>event_semantic_quality_warnings.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Silver]\n...\nevent_device_semantic_quality_metrics = ${Paths:silver_quality_metrics_dir}/semantic_quality_metrics\nevent_device_semantic_quality_warnings_log_table = ${Paths:silver_quality_warnings_dir}/semantic_quality_warnings_log_table\nevent_device_semantic_quality_warnings_bar_plot_data = ${Paths:silver_quality_warnings_dir}/semantic_quality_warnings_bar_plot_data\n...\n</code></pre> <p>The expected parameters in <code>event_semantic_quality_warnings.ini</code> are as follows: - date_of_study: string, format should be the one specified <code>date_format</code> (e.g., <code>2023-01-01</code> for <code>%Y-%m-%d</code>), the first date for which data will be processed by the component. All dates between this one and the specified in <code>data_period_end</code> will be processed (both inclusive). - date_format: string, it indicates the format expected in <code>date_of_study</code>. For example, use <code>%Y-%m-%d</code> for the usual \"2023-01-01\" format separated by <code>-</code>. - thresholds: dictionary, indicating the different thresholds to be used for raising warnings.</p>"},{"location":"UserManual/configuration/3_QualityWarnings/3_semantic_quality_warnings/#thresholds","title":"Thresholds","text":"<p>The thresholds parameter is a dictionary of dictionaries, used to indicate the different thresholds and lookback periods to be used for each type of warning. In the case than one of the thresholds is missing, a default value will be used instead. The default values are contained in <code>multimno.core.constants.semantic_qw_default_thresholds.SEMANTIC_DEFAULT_THRESHOLDS</code>.</p> <p>The dictionary structure is as follows:  - <code>\"CELL_ID_NON_EXISTENT\"</code>: refers to events that make reference to a non-existent cell ID.   - <code>\"sd_lookback_days\"</code>: integer, indicates the days to use as a lookback period to calculate the average and standard deviation over. By default, the value is 7.   - <code>\"min_sd\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate differs from the average over the lookback period by more than its standard deviation multiplied by this threshold. By default, the value is 2.   - <code>\"min_percentage\"</code>: integer or float, indicates the threshold for a warning to be raised for this error rate when the lookback date specified is strictly lower than 3. By default, the value is 0.  - <code>\"CELL_ID_NOT_VALID\"</code>: refers to events that make reference to an existent cell ID, but the cell is not operative.   - <code>\"sd_lookback_days\"</code>: integer, indicates the days to use as a lookback period to calculate the average and standard deviation over. By default, the value is 7.   - <code>\"min_sd\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate differs from the average over the lookback period by more than its standard deviation multiplied by this threshold. By default, the value is 2.   - <code>\"min_percentage\"</code>: integer or float, indicates the threshold for a warning to be raised for this error rate when the lookback date specified is strictly lower than 3. By default, the value is 0.  - <code>\"INCORRECT_EVENT_LOCATION\"</code>: refers to events that have been flagged as having an incorrect location.   - <code>\"sd_lookback_days\"</code>: integer, indicates the days to use as a lookback period to calculate the average and standard deviation over. By default, the value is 7.   - <code>\"min_sd\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate differs from the average over the lookback period by more than its standard deviation multiplied by this threshold. By default, the value is 2.   - <code>\"min_percentage\"</code>: integer or float, indicates the threshold for a warning to be raised for this error rate when the lookback date specified is strictly lower than 3. By default, the value is 0.  - <code>\"SUSPICIOUS_EVENT_LOCATION\"</code>: refers to events that have been flagged as having a suspicious location.   - <code>\"sd_lookback_days\"</code>: integer, indicates the days to use as a lookback period to calculate the average and standard deviation over. By default, the value is 7.   - <code>\"min_sd\"</code>: integer or float, indicates the threshold for a warning to be raised when this error rate differs from the average over the lookback period by more than its standard deviation multiplied by this threshold. By default, the value is 2.   - <code>\"min_percentage\"</code>: integer or float, indicates the threshold for a warning to be raised for this error rate when the lookback date specified is strictly lower than 3. By default, the value is 0.</p>"},{"location":"UserManual/configuration/3_QualityWarnings/3_semantic_quality_warnings/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = SemanticQualityWarnings\n\n[SemanticQualityWarnings]\n\ndate_format = %Y-%m-%d\ndate_of_study = 2023-01-08\n\nthresholds = {\n    \"CELL_ID_NON_EXISTENT\": {\n        \"sd_lookback_days\": 8,\n        \"min_sd\": 1.5,\n        \"min_percentage\": 5\n    },\n    \"CELL_ID_NOT_VALID\": {\n        \"sd_lookback_days\": 8,\n        \"min_sd\": 1.5,\n        \"min_percentage\": 5\n    },\n    \"INCORRECT_EVENT_LOCATION\": {\n        \"sd_lookback_days\": 8,\n        \"min_sd\": 1.5,\n        \"min_percentage\": 5\n    },\n    \"SUSPICIOUS_EVENT_LOCATION\": {\n        \"sd_lookback_days\": 8,\n        \"min_sd\": 1.5,\n        \"min_percentage\": 5\n    },\n    }\n</code></pre>"},{"location":"UserManual/configuration/4_SyntheticMnoData/1_synthetic_diaries/","title":"SyntheticDiaries Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>synthetic_diaries.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Bronze]\n...\ndiaries_data_bronze = ${Paths:bronze_dir}/diaries\n...\n</code></pre> <p>The expected parameters in <code>synthetic_diaries.ini</code> are as follows:  - number_of_users: integer, number of devices for which to generate synthetic diaries in each target date. Example: <code>100</code>.  - date_format: string, format for date parsing from initial_date field (1989 C standard). Example: <code>%Y-%m-%d</code>.  - initial_date: string, initial date for which to generate synthetic diaries, matching the date format specified in date_format. Example: <code>2023-01-01</code>.  - number_of_dates: integer, number of dates for which synthetic diaries will be generated. Synthetic diaries for initial_date and for the following <code>number_of_dates - 1</code> dates will be generated. If diaries are to be generated for just one date, set this parameter equal to 1. Example: <code>9</code>.</p> <ul> <li>latitude_min: float, degrees. Lower limit of the bounding box within which all locations will be generated. Example: <code>40.352</code>.</li> <li>latitude_max: float, degrees. Upper limit of the bounding box within which all locations will be generated. Must be higher than latitude_min. Example: <code>40.486</code>.</li> <li>longitude_min: float, degrees. Left limit of the bounding box within which all locations will be generated. Example: <code>-3.751</code>.</li> <li> <p>longitude_max: float, degrees. Right limit of the bounding box within which all locations will be generated. Must be higher than longitude_min. Example: <code>-3.579</code>.</p> </li> <li> <p>home_work_distance_min: float, meters. Work location of a user will be generated at a distance higher than this minimum from the user's home location. Example: <code>2000</code> (m).</p> </li> <li>home_work_distance_max: float, meters. Work location of a user will be generated at a distance lower than this maximum from the user's home location. Must be higher than home_work_distance_min. Example: <code>10000</code> (m).</li> <li>other_distance_min: float, meters. For activities of type 'other', the location of the activity will be generated at a distance higher than this minimum from the user's previous activity. Example: <code>1000</code> (m).</li> <li> <p>other_distance_max: float, meters. For activities of type 'other', the location of the activity will be generated at a distance lower than this maximum from the user's previous activity. Must be higher than other_distance_min. Example: <code>3000</code> (m).</p> </li> <li> <p>home_duration_min: float, hours. This is the minimum duration that will be considered for an activity with stay type 'home'. Example: <code>5</code> (hours).</p> </li> <li>home_duration_max: float, hours. This is the maximum duration that will be considered for an activity with stay type 'home'. Must be higher than home_duration_min. Example: <code>12</code> (hours).</li> <li>work_duration_min: float, hours. This is the minimum duration that will be considered for an activity with stay type 'work'. Example: <code>4</code> (hours).</li> <li>work_duration_max: float, hours. This is the maximum duration that will be considered for an activity with stay type 'work'. Must be higher than work_duration_min. Example: <code>8</code> (hours).</li> <li>other_duration_min: float, hours. This is the minimum duration that will be considered for an activity with stay type 'other'. Example: <code>1</code> (hours).</li> <li> <p>other_duration_max: float, hours. This is the maximum duration that will be considered for an activity with stay type 'other'. Must be higher than other_duration_min. Example: <code>3</code> (hours).</p> </li> <li> <p>displacement_speed: float, m/s. Given the location of 2 consecutive generated activities for a user, this speed helps us define the second activity's start time once we know the first activity's end time. The distance between both activities is calculated and then divided by this speed in order to calculate the trip time, which is then added to the first activity's end time in order to obtain the second activity's initial time. Example: <code>3</code> (m/s).</p> </li> </ul>"},{"location":"UserManual/configuration/4_SyntheticMnoData/1_synthetic_diaries/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\n# parameters in this section are passed to Spark except session_name \nsession_name = SyntheticDiarySession\n\n[SyntheticDiaries]\nnumber_of_users = 100\ndate_format = %Y-%m-%d\ninitial_date = 2023-01-01\nnumber_of_dates = 9\n\nlatitude_min = 40.352\nlatitude_max = 40.486\nlongitude_min = -3.751\nlongitude_max = -3.579\n\nhome_work_distance_min = 2_000  # in meters\nhome_work_distance_max = 10_000  # in meters\nother_distance_min = 1_000  # in meters\nother_distance_max = 3_000  # in meters\nhome_duration_min = 5  # in hours\nhome_duration_max = 12  # in hours\nwork_duration_min = 4  # in hours\nwork_duration_max = 8  # in hours\nother_duration_min = 1  # in hours\nother_duration_max = 3  # in hours\ndisplacement_speed = 3  # 3 m/s = 10 km/h\n\nstay_sequence_superset = home,other,work,other,other,other,home\nstay_sequence_probabilities = 1,0.1,0.6,0.4,0.2,0.1,1\n</code></pre>"},{"location":"UserManual/configuration/4_SyntheticMnoData/2_synthetic_network/","title":"LongtermPermanenceScore Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>synthetic_network.ini</code>. In <code>general_config.ini</code> all paths to the corresponding data objects shall be specified. Example:</p> <pre><code>[Paths.Bronze]\n...\nnetwork_data_bronze = ${Paths:bronze_dir}/mno_network\n...\n</code></pre> <p>The expected parameters in <code>synthetic_network.ini</code> are as follows:  - seed: integer, seed for random number generation used to generate the synthetic network topology data.  - n_cells: positive integer, number of synthetic cells that will be generated. Example: <code>500</code>.  - cell_id_generation_type: string, identifier of the generator of cell IDs to be used. Currently the only option available is <code>random_cell_id</code>, which generates a random 14- or 15- digit string. Example: <code>random_cell_id</code>.  - cell_type_options: comma-separated list of strings, it contains the values that the <code>cell_type</code> field can take for the generated synthetic data. Each option has the same probability of being assigned. Example: <code>macrocell, microcell, picocell, femtocell</code>.  - latitude_min: float, minimum latitude that defines the bounding box in which the coordinates of the synthetic cells will be generated.  - latitude_max: float, maximum latitude that defines the bounding box in which the coordinates of the synthetic cells will be generated.  - longitude_min: float, minimum longitude that defines the bounding box in which the coordinates of the synthetic cells will be generated.  - longitude_max: float, maximum longitude that defines the bounding box in which the coordinates of the synthetic cells will be generated.  - altitude_min: float, minimum value that the altitude field can take in the generated cells. Example: <code>-40</code>.  - altitude_max: float, maximum value that the altitude field can take in the generated cells. Example: <code>5000</code>.  - antenna_height_max: float, maximum value that the <code>antenna_height</code> field might take in the generated cells. Example: <code>120</code>.  - altitude_min: float, minimum value that the altitude field can take in the generated cells. Example: <code>-40</code>.  - altitude_max: float, maximum value that the altitude field can take in the generated cells. Example: <code>5000</code>.  - power_min: float, minimum value that the power field can take in the generated cells. Units are watts (W). Example: <code>0.1</code>.  - power_max: float, maximum value that the power field can take in the generated cells. Units are watts (W). Example: <code>500</code>.  - frequency_min: float, minimum value that the frequency field can take in the generated cells. Units are megahertz (MHz). Example: <code>1</code>.  - frequency_max: float, maximum value that the frequency field can take in the generated cells. Units are megahertz (MHz). Example: <code>4000</code>.  - data_period_format: string, it indicates the format expected in <code>earliest_valid_date_start</code> and <code>latest_valid_date_end</code>. For example, use <code>%Y-%m-%dT%H:%M:%S</code> for the usual \"2023-01-09T00:00:00\" format.  - earliest_valid_date_start: string, it indicates the timestamp value that the <code>valid_date_start</code> will take in all generated cells. Example: <code>2022-12-15T00:00:00</code>.  - latest_valid_date_end: string, it indicates the timestamp value that the <code>valid_date_end</code> will take in all generated cells. Example: <code>2023-01-15T00:00:00</code>.  - date_format: string, it indicates the format expected in <code>starting_date</code> and <code>ending_date</code>. For example, use <code>%Y-%m-%d</code> for the usual \"2023-01-09\" format separated by <code>-</code>. Example: <code>%Y-%m-%d</code>.  - starting_date: string, format should be the one specified <code>date_format</code> (e.g., <code>2023-01-01</code> for <code>%Y-%m-%d</code>), the first date for which data will be generated. All dates between this one and the specified in <code>ending_date</code> will be have data generated (both inclusive). The cell properties will be equal across all dates.   - ending_date: string, format should be the one specified <code>date_format</code> (e.g., <code>2023-01-09</code> for <code>%Y-%m-%d</code>), the last date for which data will be generated. All dates between the specified in <code>starting_date</code> and this one will be have data generated (both inclusive). The cell properties will be equal across all dates.   - no_optional_fields_probability: float, probability that all of the optional fields of a record take the null value. Example: <code>0.05</code>.  - mandatory_null_probability: float, probability that one of the mandatory fields of a record will take a null value. Example: <code>0.05</code>.  - out_of_bounds_values_probability: float, probability that a field of a record will take a value outside its valid values. This could be, for example, a negative power or a latitude outside the $[-90, 90]$ interval. Example: <code>0.05</code>.  - erroneous_values_probability: float, probability that one of the following erroneous values might occur:    - The <code>cell_id</code> takes a non-valid value (not a 14- or 15-digit string).    - The <code>valid_date_start</code> and <code>valid_date_end</code> fields has an invalid timestamp format.    - The <code>valid_date_end</code> is a point in time earlier than <code>valid_date_start</code>.</p> <pre><code>Example: `0.05`.\n</code></pre>"},{"location":"UserManual/configuration/4_SyntheticMnoData/2_synthetic_network/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\nsession_name = SyntheticNetworkSession\n\n[SyntheticNetwork]\nseed = 33\nn_cells = 500\ncell_id_generation_type = random_cell_id  # options: random_cell_id\ncell_type_options = macrocell, microcell, picocell, femtocell\nlatitude_min = 40.352\nlatitude_max = 40.486\nlongitude_min = -3.751\nlongitude_max = -3.579\n\naltitude_min = -40\naltitude_max = 5000\n# antenna_height_min is always 0\nantenna_height_max = 120\npower_min = 0.1\npower_max = 500\nfrequency_min = 1\nfrequency_max = 4000\ntimestamp_format = %Y-%m-%dT%H:%M:%S\nearliest_valid_date_start = 2022-12-15T00:00:00\nlatest_valid_date_end = 2023-01-15T00:00:00\ndate_format = %Y-%m-%d\nstarting_date = 2023-01-01\nending_date = 2023-01-09\n\nno_optional_fields_probability = 0.0\nmandatory_null_probability = 0.0\nout_of_bounds_values_probability = 0.0\nerroneous_values_probability = 0.0\n</code></pre>"},{"location":"UserManual/configuration/4_SyntheticMnoData/3_synthetic_events/","title":"GeozonesGridMapping Configuration","text":"<p>To initialise and run the component two configs are used - <code>general_config.ini</code> and <code>synthetic_events.ini</code>. In <code>general_config.ini</code> to execute the component specify all paths to its corresponding data objects (input + output). </p> <pre><code>[Paths.Bronze]\ndiaries_data_bronze = ${Paths:bronze_dir}/diaries\nevent_data_bronze = ${Paths:bronze_dir}/mno_events\nnetwork_data_bronze = ${Paths:bronze_dir}/mno_network\n</code></pre> <p>In synthetic_events.ini parameters are as follows: </p> <ul> <li> <p>seed -integer, the random seed value for all subprocesses that involve randomness, such as the random generation of timestamps, latitude, longitude,  random selection of cell to be linked to a point, random selection of rows and columns for null generation, random selection of rows for duplicates generations, etc.</p> </li> <li> <p>event_freq_stays - integer, the frequency in seconds for events to be generated for stays (higher means that less events will be generated for a given stay in a given time interval in synthetic diaries).</p> </li> <li> <p>event_freq_moves - integer, the frequency in seconds for events to be generated for moves (higher means that less events will be generated for a given move in a given time interval in synthetic diaries).</p> </li> <li> <p>error_location_probability - float (between 0.0 and 1.0), ratio of rows from all generated events, to be selected for generating errors in x and y coordinates.</p> </li> <li> <p>error_location_distance_min - integer, the minimum distance in meters to which generated points may reach from the original generated point, when offseting these points to be erroneous</p> </li> <li> <p>error_location_distance_max - integer, the maximum distance in meters to which generated points may reach from the original generated point, when offseting these points to be erroneous.  </p> </li> <li> <p>error_cell_id_probability - float (between 0.0 and 1.0), the ratio of rows that are to be selected from all generated events, for generating events with nonexistent cell ids. These are events that have a syntactically valid cell_id that is not present in the input network data.</p> </li> <li> <p>mcc - integer, the value for the mcc column to be applied to the data of all generated users.</p> </li> <li> <p>maximum_number_of_cells_for_event - integer, the maximimum number of cells to consider, when linking a generated point to a cell in the input network data. If several cells are within the distance provided by closest_cell_distance_max for a given generated point (erroneous or not), a single cell is selected from as many closest cells, randomly, as given by maximum_number_of_cells_for_event.</p> </li> <li> <p>closest_cell_distance_max - integer, the distance in meters to a cell from a generated point, for that cell to be included as one of the possible cells to be linked to the given point.</p> </li> <li> <p>closest_cell_distance_max_for_errors - integer, the distance in meters to a cell from an erroneously generated point, for the cell to be included as one of the possible cells to be linked to that point.</p> </li> <li> <p>closest_cell_distance_max_for_errors - integer, the distance in meters to a cell from an erroneously generated point, for the cell to be included as one of the possible cells to be linked to that point.</p> </li> <li> <p>do_event_error_generation - boolean, whether to generate syntactic errors for clean rows that have been generated.</p> </li> <li> <p>null_row_probability - float, probability to use for sampling rows for which one or more columns will be set to null. If set to 0, no null rows are generated. Which columns on the sampeld rows are selected as null is affected by the parameter column_is_null_probability.</p> </li> <li> <p>out_of_bounds_probability - float, probability to use for sampling rows for which timestamp will be replaced with a timestamp that is out of the temporal bounds set in the input synthetic diaries. These rows will not include any other errors.</p> </li> <li> <p>data_type_error_probability - float, probability to use for sampling rows for which a random selection of columns will be modified as erroneous. Modifications are column specific, for instance the cell_id column will be replaced by random string. </p> </li> <li> <p>column_is_null_probability - float, probability that a given column will be set as null for the rows that have been sampled for null selection. If column_is_null_probability=1, all columns in the rows selected for null generation are replaced with nulls. If null_row_probability=0, this parameter is not used.</p> </li> <li> <p>same_location_duplicates_probability - float, probability for selecting rows that will be transformed into same location duplicates. The value corresponds to the approximate ratio of rows in the final output that will include pair-wise full duplicates. This process generates at a maximum two duplicate records for a given user, not more.</p> </li> <li> <p>different_location_duplicates_probability - float, probability for selecting rows that will be transformed into different location duplicates. The value corresponds to the approximate ratio of rows in the final output that will include pair-wise duplicates in all columns, except for longitude and latitude. This process generates at a maximum two duplicate records for a given user, not more.</p> </li> <li> <p>order_output_by_timestamp - boolean, whether to order the final output by the timestamp column.</p> </li> <li> <p>cartesian_crs - integer, the coordinate reference system (CRS) to use for the cartesian coordinates. The default value is 3035.</p> </li> </ul> <p>Note on probability parameters: These parameters do not necessarily translate to the exact ratios for each probability type in the output data object, as the exact number of rows selected is affected by the random seed, in addition to the probability value itself.</p>"},{"location":"UserManual/configuration/4_SyntheticMnoData/3_synthetic_events/#configuration-example","title":"Configuration example","text":"<pre><code>[Spark]\n\nsession_name = SyntheticEventsSession\n\n\n[SyntheticEvents]\nseed = 999\nevent_freq_stays = 2400 # s\nevent_freq_moves = 1200  # s\nerror_location_probability = 0.0\nerror_location_distance_min = 1000 # m\nerror_location_distance_max = 10000 # m\nerror_cell_id_probability  = 0.0\nmcc = 214\nmaximum_number_of_cells_for_event = 3\nclosest_cell_distance_max = 5000 # m\nclosest_cell_distance_max_for_errors = 10000 # m\ncartesian_crs = 3035\n\ndo_event_error_generation = True \nnull_row_probability = 0.3 \nout_of_bounds_probability = 0.0 \ndata_type_error_probability = 0.3 \ncolumn_is_null_probability = 0.5 \ndifferent_location_duplicates_probability = 0.3\nsame_location_duplicates_probability = 0.25\n\norder_output_by_timestamp = True\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>components<ul> <li>execution<ul> <li>cell_connection_probability<ul> <li>cell_connection_probability</li> </ul> </li> <li>cell_footprint<ul> <li>cell_footprint_estimation</li> </ul> </li> <li>cell_proximity_estimation<ul> <li>cell_proximity_estimation</li> </ul> </li> <li>daily_permanence_score<ul> <li>daily_permanence_score</li> </ul> </li> <li>device_activity_statistics<ul> <li>device_activity_statistics</li> </ul> </li> <li>estimation<ul> <li>estimation</li> </ul> </li> <li>event_cleaning<ul> <li>event_cleaning</li> </ul> </li> <li>event_semantic_cleaning<ul> <li>event_semantic_cleaning</li> </ul> </li> <li>geozones_grid_mapping<ul> <li>geozones_grid_mapping</li> </ul> </li> <li>grid_enrichment<ul> <li>grid_enrichment</li> </ul> </li> <li>internal_migration<ul> <li>internal_migration</li> </ul> </li> <li>kanonimity<ul> <li>kanonimity</li> </ul> </li> <li>longterm_permanence_score<ul> <li>longterm_permanence_score</li> </ul> </li> <li>midterm_permanence_score<ul> <li>midterm_permanence_score</li> </ul> </li> <li>multimno_aggregation<ul> <li>multimno_aggregation</li> </ul> </li> <li>network_cleaning<ul> <li>network_cleaning</li> </ul> </li> <li>present_population<ul> <li>present_population_estimation</li> </ul> </li> <li>spatial_aggregation<ul> <li>spatial_aggregation</li> </ul> </li> <li>time_segments<ul> <li>continuous_time_segmentation</li> </ul> </li> <li>tourism_stays_estimation<ul> <li>tourism_stays_estimation</li> </ul> </li> <li>usual_environment_aggregation<ul> <li>usual_environment_aggregation</li> </ul> </li> <li>usual_environment_labeling<ul> <li>usual_environment_labeling</li> </ul> </li> </ul> </li> <li>ingestion<ul> <li>data_filtering<ul> <li>data_filtering</li> </ul> </li> <li>grid_generation<ul> <li>inspire_grid_generation</li> </ul> </li> <li>spatial_data_ingestion<ul> <li>gisco_data_ingestion</li> <li>overture_data_ingestion</li> </ul> </li> <li>synthetic<ul> <li>synthetic_diaries</li> <li>synthetic_events</li> <li>synthetic_network</li> </ul> </li> </ul> </li> <li>quality<ul> <li>event_quality_warnings<ul> <li>event_quality_warnings</li> </ul> </li> <li>network_quality_warnings<ul> <li>network_quality_warnings</li> </ul> </li> <li>semantic_quality_warnings<ul> <li>semantic_quality_warnings</li> </ul> </li> </ul> </li> </ul> </li> <li>core<ul> <li>component</li> <li>configuration</li> <li>constants<ul> <li>columns</li> <li>conditions</li> <li>error_types</li> <li>measure_definitions</li> <li>network_default_thresholds</li> <li>period_names</li> <li>reserved_dataset_ids</li> <li>semantic_qw_default_thresholds</li> <li>spatial</li> <li>transformations</li> <li>warnings</li> </ul> </li> <li>data_objects<ul> <li>bronze<ul> <li>bronze_admin_units_data_object</li> <li>bronze_countries_data_object</li> <li>bronze_event_data_object</li> <li>bronze_geographic_zones_data_object</li> <li>bronze_holiday_calendar_data_object</li> <li>bronze_landuse_data_object</li> <li>bronze_network_physical_data_object</li> <li>bronze_synthetic_diaries_data_object</li> <li>bronze_transportation_data_object</li> </ul> </li> <li>data_object</li> <li>landing<ul> <li>landing_geoparquet_data_object</li> <li>landing_http_geojson_data_object</li> </ul> </li> <li>silver<ul> <li>event_cache_data_object</li> <li>silver_aggregated_usual_environments_data_object</li> <li>silver_aggregated_usual_environments_zones_data_object</li> <li>silver_cell_connection_probabilities_data_object</li> <li>silver_cell_distance_data_object</li> <li>silver_cell_footprint_data_object</li> <li>silver_cell_intersection_groups_data_object</li> <li>silver_daily_permanence_score_data_object</li> <li>silver_device_activity_statistics</li> <li>silver_enriched_grid_data_object</li> <li>silver_event_data_object</li> <li>silver_event_data_syntactic_quality_metrics_by_column</li> <li>silver_event_data_syntactic_quality_metrics_frequency_distribution</li> <li>silver_event_data_syntactic_quality_warnings_for_plots</li> <li>silver_event_data_syntactic_quality_warnings_log_table</li> <li>silver_event_flagged_data_object</li> <li>silver_geozones_grid_map_data_object</li> <li>silver_grid_data_object</li> <li>silver_internal_migration_data_object</li> <li>silver_internal_migration_quality_metrics_data_object</li> <li>silver_longterm_permanence_score_data_object</li> <li>silver_midterm_permanence_score_data_object</li> <li>silver_network_data_object</li> <li>silver_network_data_syntactic_quality_metrics_by_column</li> <li>silver_network_data_top_frequent_errors_data_object</li> <li>silver_network_row_error_metrics</li> <li>silver_network_syntactic_quality_warnings_log_table</li> <li>silver_network_syntactic_quality_warnings_plot_data</li> <li>silver_present_population_data_object</li> <li>silver_present_population_zone_data_object</li> <li>silver_semantic_quality_metrics</li> <li>silver_semantic_quality_warnings_log_table</li> <li>silver_semantic_quality_warnings_plot_data</li> <li>silver_time_segments_data_object</li> <li>silver_tourism_stays_data_object</li> <li>silver_usual_environment_labeling_quality_metrics_data_object</li> <li>silver_usual_environment_labels_data_object</li> </ul> </li> </ul> </li> <li>grid</li> <li>io_interface</li> <li>log</li> <li>settings</li> <li>spark_session</li> <li>utils</li> </ul> </li> <li>main_multimno</li> <li>orchestrator_multimno</li> </ul>"},{"location":"reference/main_multimno/","title":"main_multimno","text":"<p>Application entrypoint for launching a single component.</p> <p>Usage:</p> <pre><code>python multimno/main.py &lt;component_id&gt; &lt;general_config_path&gt; &lt;component_config_path&gt;\n</code></pre> <ul> <li>component_id: ID of the component to be executed.</li> <li>general_config_path: Path to a INI file with the general configuration of the execution.</li> <li>component_config_path: Path to a INI file with the specific configuration of the component.</li> </ul>"},{"location":"reference/main_multimno/#main_multimno.build","title":"<code>build(component_id, general_config_path, component_config_path)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>component_id</code> <code>str</code> <p>id of the component</p> required <code>general_config_path</code> <code>str</code> <p>general config path</p> required <code>component_config_path</code> <code>str</code> <p>component config path</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the component_id is not supported.</p> <p>Returns:</p> Type Description <code>Component</code> <p>Component constructor.</p> Source code in <code>multimno/main_multimno.py</code> <pre><code>def build(component_id: str, general_config_path: str, component_config_path: str):\n    \"\"\"\n\n\n    Args:\n        component_id (str): id of the component\n        general_config_path (str): general config path\n        component_config_path (str): component config path\n\n    Raises:\n        ValueError: If the component_id is not supported.\n\n    Returns:\n        (multimno.core.component.Component): Component constructor.\n    \"\"\"\n    try:\n        constructor = CONSTRUCTORS[component_id]\n    except KeyError as e:\n        raise ValueError(f\"Component {component_id} is not supported.\") from e\n\n    return constructor(general_config_path, component_config_path)\n</code></pre>"},{"location":"reference/orchestrator_multimno/","title":"orchestrator_multimno","text":"<p>Module that orchestrates MultiMNO pipeline components. A spark-submit will be performed for each  component in the pipeline.</p> <p>Usage: </p> <pre><code>python multimno/orchestrator.py &lt;pipeline.json&gt;\n</code></pre> <ul> <li>pipeline.json: Path to a json file with the pipeline configuration.</li> </ul>"},{"location":"reference/orchestrator_multimno/#orchestrator_multimno.create_logger","title":"<code>create_logger(general_config_path)</code>","text":"<p>Create a logger with the specified configuration.</p> <p>Parameters:</p> Name Type Description Default <code>general_config_path</code> <code>str</code> <p>The path to the general configuration file.</p> required <p>Returns:</p> Type Description <p>logging.Logger: The created logger object.</p> Source code in <code>multimno/orchestrator_multimno.py</code> <pre><code>def create_logger(general_config_path: str):\n    \"\"\"\n    Create a logger with the specified configuration.\n\n    Args:\n        general_config_path (str): The path to the general configuration file.\n\n    Returns:\n        logging.Logger: The created logger object.\n    \"\"\"\n    # Get log configuration\n    config = parse_configuration(general_config_path)\n\n    report_path = config.get(LoggerKeys.LOG_CONFIG_KEY, LoggerKeys.REPORT_PATH, fallback=None)\n    file_format = config.get(LoggerKeys.LOG_CONFIG_KEY, LoggerKeys.FILE_FORMAT, fallback=None)\n    datefmt = config.get(LoggerKeys.LOG_CONFIG_KEY, LoggerKeys.DATEFMT, fallback=None)\n\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.DEBUG)  # Set the logging level\n\n    # Get log path\n    today = datetime.now().strftime(\"%y%m%d_%H%M%S\")\n    log_path = f\"{report_path}/multimno_{today}.log\"\n    # Make report path + log dir\n    os.makedirs(os.path.dirname(log_path), exist_ok=True)\n\n    # Create a file handler\n    file_handler = logging.FileHandler(log_path)\n    file_handler.setLevel(logging.DEBUG)\n\n    # Create a console handler\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(logging.DEBUG)\n\n    # Create a formatter and add it to the handlers\n    formatter = logging.Formatter(fmt=file_format, datefmt=datefmt)\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n\n    # Add the handlers to the logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n\n    return logger\n</code></pre>"},{"location":"reference/components/","title":"components","text":""},{"location":"reference/components/execution/","title":"execution","text":""},{"location":"reference/components/execution/cell_connection_probability/","title":"cell_connection_probability","text":""},{"location":"reference/components/execution/cell_connection_probability/cell_connection_probability/","title":"cell_connection_probability","text":"<p>Module that calculates cell connection probabilities and posterior probabilities.</p>"},{"location":"reference/components/execution/cell_connection_probability/cell_connection_probability/#components.execution.cell_connection_probability.cell_connection_probability.CellConnectionProbabilityEstimation","title":"<code>CellConnectionProbabilityEstimation</code>","text":"<p>               Bases: <code>Component</code></p> <p>Estimates the cell connection probabilities and posterior probabilities for each grid tile. Cell connection probabilities are calculated based on footprint per grid. Posterior probabilities are calculated based on the cell connection probabilities and grid prior probabilities.</p> <p>This class reads in cell footprint estimation and the grid model wit prior probabilities. The output is a DataFrame that represents cell connection probabilities and  posterior probabilities for each cell and grid id combination for a given date.</p> Source code in <code>multimno/components/execution/cell_connection_probability/cell_connection_probability.py</code> <pre><code>class CellConnectionProbabilityEstimation(Component):\n    \"\"\"\n    Estimates the cell connection probabilities and posterior probabilities for each grid tile.\n    Cell connection probabilities are calculated based on footprint per grid.\n    Posterior probabilities are calculated based on the cell connection probabilities\n    and grid prior probabilities.\n\n    This class reads in cell footprint estimation and the grid model wit prior probabilities.\n    The output is a DataFrame that represents cell connection probabilities and\n     posterior probabilities for each cell and grid id combination for a given date.\n    \"\"\"\n\n    COMPONENT_ID = \"CellConnectionProbabilityEstimation\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.data_period_start = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n\n        self.data_period_end = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n\n        self.data_period_dates = [\n            self.data_period_start + datetime.timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n        self.current_date = None\n        self.current_cell_footprint = None\n        self.partition_number = self.config.getint(self.COMPONENT_ID, \"partition_number\")\n\n    def initalize_data_objects(self):\n\n        self.clear_destination_directory = self.config.getboolean(self.COMPONENT_ID, \"clear_destination_directory\")\n\n        # Input\n        self.input_data_objects = {}\n        self.use_land_use_prior = self.config.getboolean(self.COMPONENT_ID, \"use_land_use_prior\")\n\n        inputs = {\n            \"cell_footprint_data_silver\": SilverCellFootprintDataObject,\n        }\n\n        if self.use_land_use_prior:\n            inputs[\"enriched_grid_data_silver\"] = SilverEnrichedGridDataObject\n\n        for key, value in inputs.items():\n            path = self.config.get(CONFIG_SILVER_PATHS_KEY, key)\n            if check_if_data_path_exists(self.spark, path):\n                self.input_data_objects[value.ID] = value(self.spark, path)\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")\n                raise ValueError(f\"Invalid path for {value.ID} in component {self.COMPONENT_ID} initialization\")\n\n        # Output\n        self.output_data_objects = {}\n        silver_cell_probabilities_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"cell_connection_probabilities_data_silver\"\n        )\n\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, silver_cell_probabilities_path)\n\n        self.output_data_objects[SilverCellConnectionProbabilitiesDataObject.ID] = (\n            SilverCellConnectionProbabilitiesDataObject(\n                self.spark,\n                silver_cell_probabilities_path,\n            )\n        )\n\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n        for current_date in self.data_period_dates:\n\n            self.logger.info(f\"Processing cell footprint for {current_date.strftime('%Y-%m-%d')}\")\n\n            self.current_date = current_date\n\n            self.current_cell_footprint = self.input_data_objects[SilverCellFootprintDataObject.ID].df.filter(\n                (F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(current_date))\n            )\n\n            self.transform()\n            self.write()\n            self.spark.catalog.clearCache()\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n\n        cell_footprint_df = self.current_cell_footprint\n\n        # Calculate the cell connection probabilities\n\n        window_spec = Window.partitionBy(ColNames.year, ColNames.month, ColNames.day, ColNames.grid_id)\n\n        cell_conn_probs_df = cell_footprint_df.withColumn(\n            ColNames.cell_connection_probability,\n            F.col(ColNames.signal_dominance) / F.sum(ColNames.signal_dominance).over(window_spec),\n        )\n        # Calculate the posterior probabilities\n\n        if self.use_land_use_prior:\n            grid_model_df = self.input_data_objects[SilverEnrichedGridDataObject.ID].df.select(\n                ColNames.grid_id, ColNames.prior_probability\n            )\n            cell_conn_probs_df = cell_conn_probs_df.join(grid_model_df, on=ColNames.grid_id)\n            cell_conn_probs_df = cell_conn_probs_df.withColumn(\n                ColNames.posterior_probability,\n                F.col(ColNames.cell_connection_probability) * F.col(ColNames.prior_probability),\n            )\n\n        elif not self.use_land_use_prior:\n            cell_conn_probs_df = cell_conn_probs_df.withColumn(\n                ColNames.posterior_probability,\n                F.col(ColNames.cell_connection_probability),\n            )\n\n        # Normalize the posterior probabilities\n\n        window_spec = Window.partitionBy(ColNames.year, ColNames.month, ColNames.day, ColNames.cell_id)\n\n        cell_conn_probs_df = cell_conn_probs_df.withColumn(\n            ColNames.posterior_probability,\n            F.col(ColNames.posterior_probability) / F.sum(ColNames.posterior_probability).over(window_spec),\n        )\n\n        cell_conn_probs_df = utils.apply_schema_casting(\n            cell_conn_probs_df, SilverCellConnectionProbabilitiesDataObject.SCHEMA\n        )\n        cell_conn_probs_df = cell_conn_probs_df.coalesce(self.partition_number)\n\n        self.output_data_objects[SilverCellConnectionProbabilitiesDataObject.ID].df = cell_conn_probs_df\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/","title":"cell_footprint","text":""},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/","title":"cell_footprint_estimation","text":"<p>Module that cleans RAW MNO Event data.</p>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation","title":"<code>CellFootprintEstimation</code>","text":"<p>               Bases: <code>Component</code></p> <p>This class is responsible for modeling the signal strength of a cellular network.</p> <p>It takes as input a configuration file and a set of data representing the network's cells and their properties. The class then calculates the signal strength at various points of a grid, taking into account factors such as the distance to the cell, the azimuth and elevation angles, and the directionality of the cell.</p> <p>The class provides methods for adjusting the signal strength based on the horizontal and vertical angles, imputing default cell properties.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>class CellFootprintEstimation(Component):\n    \"\"\"\n    This class is responsible for modeling the signal strength of a cellular network.\n\n    It takes as input a configuration file and a set of data representing the network's cells and their properties.\n    The class then calculates the signal strength at various points of a grid, taking into account factors such\n    as the distance to the cell, the azimuth and elevation angles, and the directionality of the cell.\n\n    The class provides methods for adjusting the signal strength based on the horizontal and vertical angles,\n    imputing default cell properties.\n    \"\"\"\n\n    COMPONENT_ID = \"CellFootprintEstimation\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.data_period_start = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n        self.data_period_end = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n\n        self.do_azimuth_angle_adjustments = self.config.getboolean(self.COMPONENT_ID, \"do_azimuth_angle_adjustments\")\n        self.do_elevation_angle_adjustments = self.config.getboolean(\n            self.COMPONENT_ID, \"do_elevation_angle_adjustments\"\n        )\n        self.default_cell_properties = self.config.geteval(self.COMPONENT_ID, \"default_cell_physical_properties\")\n\n        self.logistic_function_steepness = self.config.getfloat(self.COMPONENT_ID, \"logistic_function_steepness\")\n        self.logistic_function_midpoint = self.config.getfloat(self.COMPONENT_ID, \"logistic_function_midpoint\")\n        self.signal_dominance_treshold = self.config.getfloat(self.COMPONENT_ID, \"signal_dominance_treshold\")\n        self.max_cells_per_grid_tile = self.config.getfloat(self.COMPONENT_ID, \"max_cells_per_grid_tile\")\n\n        self.do_dynamic_coverage_range_calculation = self.config.getboolean(\n            self.COMPONENT_ID, \"do_dynamic_coverage_range_calculation\"\n        )\n        self.repartition_number = self.config.getint(self.COMPONENT_ID, \"repartition_number\")\n        self.coverage_range_line_buffer = self.config.getint(self.COMPONENT_ID, \"coverage_range_line_buffer\")\n\n        self.difference_from_best_sd_treshold = self.config.getfloat(\n            self.COMPONENT_ID, \"difference_from_best_sd_treshold\"\n        )\n        self.do_sd_treshold_prunning = self.config.getboolean(self.COMPONENT_ID, \"do_sd_treshold_prunning\")\n        self.do_max_cells_per_tile_prunning = self.config.getboolean(\n            self.COMPONENT_ID, \"do_max_cells_per_tile_prunning\"\n        )\n        self.do_difference_from_best_sd_prunning = self.config.getboolean(\n            self.COMPONENT_ID, \"do_difference_from_best_sd_prunning\"\n        )\n\n        self.data_period_dates = [\n            self.data_period_start + datetime.timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n        self.sd_azimuth_mapping_sdf = None\n        self.sd_elevation_mapping_sdf = None\n        self.current_date = None\n        self.current_cells_sdf = None\n\n        self.partition_number = self.config.getint(self.COMPONENT_ID, \"partition_number\")\n\n    def initalize_data_objects(self):\n\n        self.clear_destination_directory = self.config.getboolean(self.COMPONENT_ID, \"clear_destination_directory\")\n        self.cartesian_crs = self.config.get(CellFootprintEstimation.COMPONENT_ID, \"cartesian_crs\")\n        self.use_elevation = self.config.getboolean(CellFootprintEstimation.COMPONENT_ID, \"use_elevation\")\n\n        self.input_data_objects = {}\n        # Input\n        inputs = {\n            \"network_data_silver\": SilverNetworkDataObject,\n        }\n        if self.use_elevation:\n            inputs[\"enriched_grid_data_silver\"] = SilverEnrichedGridDataObject\n        else:\n            inputs[\"grid_data_silver\"] = SilverGridDataObject\n\n        for key, value in inputs.items():\n            path = self.config.get(CONFIG_SILVER_PATHS_KEY, key)\n            if check_if_data_path_exists(self.spark, path):\n                self.input_data_objects[value.ID] = value(self.spark, path)\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")\n                raise ValueError(f\"Invalid path for {value.ID}: {path}\")\n\n        # Output\n        self.output_data_objects = {}\n        cell_footprint_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"cell_footprint_data_silver\")\n\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, cell_footprint_path)\n\n        self.output_data_objects[SilverCellFootprintDataObject.ID] = SilverCellFootprintDataObject(\n            self.spark, cell_footprint_path\n        )\n\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n\n        # for every date in the data period, get the events\n        # for that date and calculate the time segments\n        for current_date in self.data_period_dates:\n\n            self.logger.info(f\"Processing cell plan for {current_date.strftime('%Y-%m-%d')}\")\n\n            self.current_date = current_date\n\n            self.current_cells_sdf = (\n                self.input_data_objects[SilverNetworkDataObject.ID]\n                .df.filter(\n                    (\n                        F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n                        == F.lit(current_date)\n                    )\n                )\n                .select(\n                    ColNames.cell_id,\n                    ColNames.cell_type,\n                    ColNames.antenna_height,\n                    ColNames.power,\n                    ColNames.range,\n                    ColNames.horizontal_beam_width,\n                    ColNames.vertical_beam_width,\n                    ColNames.altitude,\n                    ColNames.latitude,\n                    ColNames.longitude,\n                    ColNames.directionality,\n                    ColNames.azimuth_angle,\n                    ColNames.elevation_angle,\n                )\n            )\n\n            self.transform()\n            self.write()\n            self.spark.catalog.clearCache()\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n\n        if self.use_elevation:\n            grid_sdf = self.input_data_objects[SilverEnrichedGridDataObject.ID].df.select(\n                ColNames.grid_id, ColNames.geometry, ColNames.elevation\n            )\n        else:\n            grid_sdf = self.input_data_objects[SilverGridDataObject.ID].df.select(ColNames.grid_id, ColNames.geometry)\n\n        grid_sdf = self.add_z_to_point_geometry(grid_sdf, ColNames.geometry, self.use_elevation)\n        grid_sdf = grid_sdf.withColumnRenamed(ColNames.geometry, ColNames.joined_geometry)\n\n        current_cells_sdf = self.current_cells_sdf\n\n        current_cells_sdf = self.impute_default_cell_properties(current_cells_sdf)\n\n        current_cells_sdf = self.watt_to_dbm(current_cells_sdf)\n\n        # TODO: Add Path Loss Exponent calculation based on grid landuse data\n\n        # Create geometries\n        current_cells_sdf = self.create_cell_point_geometry(current_cells_sdf, self.use_elevation)\n        current_cells_sdf = utils.project_to_crs(current_cells_sdf, 4326, self.cartesian_crs)\n\n        if self.do_azimuth_angle_adjustments:\n            # get standard deviation mapping table for azimuth beam width azimuth back loss pairs\n            self.sd_azimuth_mapping_sdf = self.get_angular_adjustments_sd_mapping(\n                current_cells_sdf,\n                ColNames.horizontal_beam_width,\n                ColNames.azimuth_signal_strength_back_loss,\n                \"azimuth\",\n            )\n\n        self.sd_azimuth_mapping_sdf = F.broadcast(self.sd_azimuth_mapping_sdf)\n\n        if self.do_elevation_angle_adjustments:\n            # get standard deviation mapping table for elevation beam width elevation back loss pairs\n            self.sd_elevation_mapping_sdf = self.get_angular_adjustments_sd_mapping(\n                current_cells_sdf,\n                ColNames.vertical_beam_width,\n                ColNames.elevation_signal_strength_back_loss,\n                \"elevation\",\n            )\n\n        self.sd_elevation_mapping_sdf = F.broadcast(self.sd_elevation_mapping_sdf)\n\n        if self.do_dynamic_coverage_range_calculation:\n            current_cells_sdf = self.calculate_effective_coverage(current_cells_sdf, grid_sdf)\n            current_cells_sdf = current_cells_sdf.dropna(subset=[\"coverage_center\"])\n            current_cells_sdf = current_cells_sdf.filter(F.col(\"coverage_effective_range\") &gt; 100)\n            current_cells_sdf = current_cells_sdf.repartition(self.repartition_number)\n            current_cells_sdf.cache()\n            current_cells_sdf.count()\n            current_cell_grid_sdf = self.spatial_join_within_distance(\n                current_cells_sdf, grid_sdf, \"coverage_center\", \"coverage_effective_range\"\n            )\n        else:\n            current_cell_grid_sdf = self.spatial_join_within_distance(current_cells_sdf, grid_sdf, \"geometry\", \"range\")\n        # Calculate planar and 3D distances\n        current_cell_grid_sdf = self.calculate_cartesian_distances(current_cell_grid_sdf)\n\n        current_cell_grid_sdf = self.calculate_signal_dominance(\n            current_cell_grid_sdf, self.do_azimuth_angle_adjustments, self.do_elevation_angle_adjustments\n        )\n\n        current_cell_grid_sdf = current_cell_grid_sdf.drop(\n            ColNames.distance_to_cell_3D,\n            ColNames.distance_to_cell,\n            ColNames.azimuth_angle,\n            ColNames.elevation_angle,\n            ColNames.horizontal_beam_width,\n            ColNames.vertical_beam_width,\n            ColNames.azimuth_signal_strength_back_loss,\n            ColNames.elevation_signal_strength_back_loss,\n        )\n\n        # Prune small signal dominance values\n        if self.do_sd_treshold_prunning:\n            current_cell_grid_sdf = self.prune_small_signal_dominance(\n                current_cell_grid_sdf, self.signal_dominance_treshold\n            )\n\n        # Prune signal dominance difference from best\n        if self.do_difference_from_best_sd_prunning:\n            current_cell_grid_sdf = self.prune_signal_difference_from_best(\n                current_cell_grid_sdf, self.difference_from_best_sd_treshold\n            )\n\n        # Prune max cells per grid tile\n        if self.do_max_cells_per_tile_prunning:\n            current_cell_grid_sdf = self.prune_max_cells_per_grid_tile(\n                current_cell_grid_sdf, self.max_cells_per_grid_tile\n            )\n\n        # get year, month, day from current date\n\n        current_cell_grid_sdf = current_cell_grid_sdf.withColumns(\n            {\n                ColNames.year: F.lit(self.current_date.year),\n                ColNames.month: F.lit(self.current_date.month),\n                ColNames.day: F.lit(self.current_date.day),\n            }\n        )\n\n        current_cell_grid_sdf = utils.apply_schema_casting(current_cell_grid_sdf, SilverCellFootprintDataObject.SCHEMA)\n\n        current_cell_grid_sdf = current_cell_grid_sdf.coalesce(self.partition_number)\n\n        self.output_data_objects[SilverCellFootprintDataObject.ID].df = current_cell_grid_sdf\n\n    def impute_default_cell_properties(self, sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Imputes default cell properties for null values in the input DataFrame using\n        default properties for cell types from config.\n\n        Args:\n            sdf (DataFrame): Input DataFrame.\n\n        Returns:\n            DataFrame: DataFrame with imputed default cell properties.\n        \"\"\"\n        default_properties_df = self.create_default_properties_df()\n\n        # add default prefix to the columns of default_properties_df\n        default_properties_df = default_properties_df.select(\n            [F.col(col).alias(f\"default_{col}\") for col in default_properties_df.columns]\n        )\n\n        # assign default cell type to cell types not present in config\n        sdf = sdf.withColumn(\n            ColNames.cell_type,\n            F.when(\n                F.col(ColNames.cell_type).isin(list(self.default_cell_properties.keys())),\n                F.col(ColNames.cell_type),\n            ).otherwise(\"default\"),\n        )\n\n        # all cell types which are absent from the default_properties_df will be assigned default values\n        sdf = sdf.join(\n            default_properties_df,\n            sdf[ColNames.cell_type] == default_properties_df[f\"default_{ColNames.cell_type}\"],\n            how=\"inner\",\n        )\n        # if orignal column is null, assign the default value\n        for col in default_properties_df.columns:\n            col = col.replace(\"default_\", \"\")\n            if col not in sdf.columns:\n                sdf = sdf.withColumn(col, F.lit(None))\n            sdf = sdf.withColumn(col, F.coalesce(F.col(col), F.col(f\"default_{col}\")))\n\n        return sdf.drop(*default_properties_df.columns)\n\n    def create_default_properties_df(self) -&gt; DataFrame:\n        \"\"\"\n        Creates a DataFrame with default cell properties from config dict.\n\n        Returns:\n            DataFrame: A DataFrame with default cell properties.\n        \"\"\"\n\n        rows = [Row(cell_type=k, **v) for k, v in self.default_cell_properties.items()]\n        return self.spark.createDataFrame(rows).withColumnRenamed(\"cell_type\", ColNames.cell_type)\n\n    # create geometry for cells. Set Z values if elevation is taken into account from z column, otherwise to 0\n    @staticmethod\n    def create_cell_point_geometry(sdf: DataFrame, use_elevation: bool) -&gt; DataFrame:\n        \"\"\"\n        Creates cell point geometry.\n        If elevation is taken into account, set Z values from z column, otherwise to 0.\n\n        Args:\n            sdf (DataFrame): Input DataFrame.\n            use_elevation (bool): Whether to use elevation.\n\n        Returns:\n            DataFrame: DataFrame with cell point geometry.\n        \"\"\"\n        if use_elevation:\n            sdf = sdf.withColumn(\n                ColNames.geometry,\n                STC.ST_MakePoint(\n                    F.col(ColNames.longitude),\n                    F.col(ColNames.latitude),\n                    F.col(ColNames.altitude) + F.col(ColNames.antenna_height),\n                ),\n            )\n        else:\n            sdf = sdf.withColumn(\n                ColNames.geometry,\n                STC.ST_MakePoint(\n                    F.col(ColNames.longitude),\n                    F.col(ColNames.latitude),\n                    F.col(ColNames.antenna_height),\n                ),\n            )\n        # assign crs\n        sdf = sdf.withColumn(ColNames.geometry, STF.ST_SetSRID(F.col(ColNames.geometry), F.lit(4326)))\n\n        return sdf.drop(ColNames.latitude, ColNames.longitude, ColNames.altitude, ColNames.antenna_height)\n\n    # add z value to the grid geometry if elevation is taken into account from z column in the grid otherwise set to 0\n    @staticmethod\n    def add_z_to_point_geometry(sdf: DataFrame, geometry_col: str, use_elevation: bool) -&gt; DataFrame:\n        \"\"\"\n        Adds z value to the point geometry (grid centroids).\n        If elevation is taken into account, set Z values from z column, otherwise to 0.\n\n        Args:\n            sdf (DataFrame): Input DataFrame.\n            use_elevation (bool): Whether to use elevation.\n\n        Returns:\n            DataFrame: DataFrame with z value added to point geometry.\n        \"\"\"\n        if use_elevation:\n            sdf = sdf.withColumn(\n                geometry_col,\n                STC.ST_MakePoint(\n                    STF.ST_X(F.col(geometry_col)),\n                    STF.ST_Y(F.col(geometry_col)),\n                    F.col(ColNames.elevation),\n                ),\n            )\n        else:\n            sdf = sdf.withColumn(\n                geometry_col,\n                STC.ST_MakePoint(\n                    STF.ST_X(F.col(geometry_col)),\n                    STF.ST_Y(F.col(geometry_col)),\n                    F.lit(0.0),\n                ),\n            )\n\n        return sdf.drop(ColNames.elevation)\n\n    @staticmethod\n    def spatial_join_within_distance(\n        sdf_from: DataFrame, sdf_to: DataFrame, geometry_col: str, within_distance_col: str\n    ) -&gt; DataFrame:\n        \"\"\"\n        Performs a spatial join within a specified distance.\n\n        Args:\n            sdf_from (DataFrame): Input DataFrame.\n            sdf_to (DataFrame): DataFrame to join with.\n            within_distance_col (str): Column name for the within distance.\n\n        Returns:\n            DataFrame: DataFrame after performing the spatial join.\n        \"\"\"\n\n        sdf_merged = (\n            sdf_from.alias(\"a\")\n            .join(\n                sdf_to.alias(\"b\"),\n                STP.ST_Intersects(\n                    STF.ST_Buffer(f\"a.{geometry_col}\", f\"a.{within_distance_col}\"),\n                    f\"b.{ColNames.joined_geometry}\",\n                ),\n            )\n            .drop(f\"a.{within_distance_col}\")\n        )\n\n        return sdf_merged\n\n    @staticmethod\n    def calculate_cartesian_distances(sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Calculates cartesian distances.\n\n        Args:\n            sdf (DataFrame): Input DataFrame.\n\n        Returns:\n            DataFrame: DataFrame with calculated cartesian distances.\n        \"\"\"\n\n        sdf = sdf.withColumns(\n            {\n                ColNames.distance_to_cell_3D: STF.ST_3DDistance(\n                    F.col(ColNames.geometry), F.col(ColNames.joined_geometry)\n                ),\n                ColNames.distance_to_cell: STF.ST_Distance(F.col(ColNames.geometry), F.col(ColNames.joined_geometry)),\n            }\n        )\n\n        return sdf\n\n    @staticmethod\n    def watt_to_dbm(sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Converts power from watt to dBm.\n\n        Args:\n            sdf (DataFrame): Input DataFrame.\n\n        Returns:\n            DataFrame: DataFrame with power converted to dBm.\n        \"\"\"\n        return sdf.withColumn(ColNames.power, 10.0 * F.log10(F.col(ColNames.power)) + 30.0)\n\n    @staticmethod\n    def join_sd_mapping(\n        sdf: DataFrame,\n        sd_mapping_sdf: DataFrame,\n        beam_width_col: str,\n        signal_front_back_difference_col: str,\n        sd_col: str,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Joins DataFrame with standard deviation mapping.\n\n        Args:\n            sdf (DataFrame): Input DataFrame.\n            sd_mapping_sdf (DataFrame): DataFrame with standard deviation mapping.\n            beam_width_col (str): Column name for the beam width.\n            signal_front_back_difference_col (str): Column name for the signal front-back difference.\n\n        Returns:\n            DataFrame: DataFrame after joining with standard deviation mapping.\n        \"\"\"\n\n        join_condition = (F.col(f\"a.{beam_width_col}\") == F.col(f\"b.{beam_width_col}\")) &amp; (\n            F.col(f\"a.{signal_front_back_difference_col}\") == F.col(f\"b.{signal_front_back_difference_col}\")\n        )\n\n        sdf = sdf.alias(\"a\").join(sd_mapping_sdf.alias(\"b\"), join_condition).select(f\"a.*\", f\"b.{sd_col}\")\n\n        return sdf\n\n    def get_angular_adjustments_sd_mapping(\n        self,\n        cells_sdf: DataFrame,\n        beam_width_col: str,\n        signal_front_back_difference_col: str,\n        angular_adjustment_type: str,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Calculates standard deviations in signal strength based on beam width and front-back cell signal difference.\n\n        Args:\n            cells_sdf (DataFrame): Input DataFrame.\n            beam_width_col (str): Column name for the beam width.\n            signal_front_back_difference_col (str): Column name for the signal front-back difference.\n            angular_adjustment_type (str): Type of angular adjustment.\n\n        Returns:\n            DataFrame: DataFrame with angular adjustments standard deviation mapping.\n        \"\"\"\n\n        sd_mappings = CellFootprintEstimation.get_sd_to_signal_back_loss_mappings(\n            cells_sdf, signal_front_back_difference_col\n        )\n        beam_widths_diff = (\n            cells_sdf.select(F.col(beam_width_col), F.col(signal_front_back_difference_col))\n            .where(F.col(ColNames.directionality) == 1)\n            .distinct()\n        )\n        beam_widths_diff = [row.asDict() for row in beam_widths_diff.collect()]\n\n        beam_sds = []\n        for item in beam_widths_diff:\n            item[f\"sd_{angular_adjustment_type}\"] = self.find_sd(\n                item[beam_width_col],\n                sd_mappings[sd_mappings[signal_front_back_difference_col] == item[signal_front_back_difference_col]],\n            )\n            beam_sds.append(item)\n\n        beam_sd_sdf = self.spark.createDataFrame(beam_sds)\n\n        return beam_sd_sdf\n\n    @staticmethod\n    def find_sd(beam_width: float, mapping: DataFrame) -&gt; float:\n        \"\"\"\n        Finds the standard deviation corresponding to the given beam width using the provided mapping.\n\n        Args:\n            beam_width (float): The width of the beam in degrees.\n            mapping (DataFrame): A DataFrame where each row corresponds to a standard deviation\n                and contains the corresponding angle.\n\n        Returns:\n            float: The standard deviation corresponding to the given beam width.\n        \"\"\"\n        min_diff_index = (abs(mapping[\"deg\"] - beam_width / 2)).idxmin()\n        return float(mapping.loc[min_diff_index, \"sd\"])\n\n    @staticmethod\n    def get_sd_to_signal_back_loss_mappings(cells_sdf: DataFrame, signal_front_back_difference_col: str) -&gt; DataFrame:\n        \"\"\"\n        Generates a DataFrame with mapping of signal strength standard deviation for each\n            elevation/azimuth angle degree.\n\n        Parameters:\n        cells_sdf (DataFrame): A Spark DataFrame containing information about the cells.\n        signal_front_back_difference_col (str): The name of the column that contains the difference\n            in signal strength between\n        the front and back of the cell.\n\n        Returns:\n        DataFrame: A pandas DataFrame with standard deviation mappings.\n\n        \"\"\"\n        db_back_diffs = (\n            cells_sdf.select(F.col(signal_front_back_difference_col))\n            .where(F.col(ColNames.directionality) == 1)\n            .distinct()\n        )\n        db_back_diffs = [row.asDict()[signal_front_back_difference_col] for row in db_back_diffs.collect()]\n        mappings = [\n            CellFootprintEstimation.create_mapping(item, signal_front_back_difference_col) for item in db_back_diffs\n        ]\n\n        return pd.concat(mappings)\n\n    @staticmethod\n    def create_mapping(db_back: float, signal_front_back_difference_col) -&gt; DataFrame:\n        \"\"\"\n        Creates a mapping between standard deviation and the angle\n        at which the signal strength falls to 3 dB below its maximum value.\n\n        Args:\n            db_back (float): The difference in signal strength in dB between the front and back of the signal.\n\n        Returns:\n            DataFrame: A DataFrame where each row corresponds to a\n            standard deviation and contains the corresponding angle.\n        \"\"\"\n        idf = pd.DataFrame({\"sd\": np.arange(180 / 1000, 180, 180 / 1000)})\n        idf[\"deg\"] = idf[\"sd\"].apply(CellFootprintEstimation.get_min3db, db_back=db_back)\n        df = pd.DataFrame({\"deg\": np.arange(1, 181)})\n        df[\"sd\"] = df[\"deg\"].apply(lambda dg: idf.loc[np.abs(idf[\"deg\"] - dg).idxmin(), \"sd\"])\n        df[signal_front_back_difference_col] = db_back\n        return df\n\n    @staticmethod\n    def get_min3db(sd: float, db_back: float) -&gt; float:\n        \"\"\"\n        Finds the angle at which the signal strength falls to 3 dB below its maximum value.\n\n        Args:\n            sd (float): The standard deviation of the normal distribution modeling the signal strength.\n            db_back (float): The difference in signal strength in dB between the front and back of the signal.\n\n        Returns:\n            float: The angle at which the signal strength falls to 3 dB below its maximum value.\n        \"\"\"\n        df = pd.DataFrame({\"a\": np.linspace(0, 180, 720)})\n        df[\"dbLoss\"] = CellFootprintEstimation.norm_dBloss(df[\"a\"], db_back=db_back, sd=sd)\n        return df.loc[np.abs(-3 - df[\"dbLoss\"]).idxmin(), \"a\"]\n\n    @staticmethod\n    def norm_dBloss(a: float, sd: float, db_back: float) -&gt; float:\n        \"\"\"\n        Computes the loss in signal strength in dB as a function of\n        angle from the direction of maximum signal strength.\n\n        Args:\n            a (float): The angle from the direction of maximum signal strength.\n            sd (float): The standard deviation of the normal distribution modeling the signal strength.\n            db_back (float): The difference in signal strength in dB between the front and back of the signal.\n\n        Returns:\n            float: The loss in signal strength in dB at the given angle.\n        \"\"\"\n        a = ((a + 180) % 360) - 180\n        inflate = -db_back / (\n            CellFootprintEstimation.normal_distribution(0, 0, sd)\n            - CellFootprintEstimation.normal_distribution(180, 0, sd)\n        )\n        return (\n            CellFootprintEstimation.normal_distribution(a, 0, sd)\n            - CellFootprintEstimation.normal_distribution(0, 0, sd)\n        ) * inflate\n\n    @staticmethod\n    def normal_distribution(x: float, mean: float, sd: float) -&gt; Union[np.array, list]:\n        \"\"\"\n        Computes the value of the normal distribution with the given mean\n        and standard deviation at the given point.\n\n        Args:\n            x (float): The point at which to evaluate the normal distribution.\n            mean (float): The mean of the normal distribution.\n            sd (float): The standard deviation of the normal distribution.\n            return_type (str): The desired return type, either 'np_array' or 'list'.\n\n        Returns:\n            np.array or list: The value of the normal distribution at the given point,\n            returned as either a numpy array or a list.\n        \"\"\"\n        n_dist = (1.0 / (sqrt(2.0 * pi) * sd)) * np.exp(-0.5 * ((x - mean) / sd) ** 2)\n\n        return n_dist\n\n    def calculate_effective_coverage(self, cells_sdf: DataFrame, grid_sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Calculates effective cell coverage center and range based on desired signal domincance threshold.\n\n        The function first separates the cells into omnidirectional and directional types,\n        then calculates the signal dominance threshold points for each type.\n        The function then calculates the effective coverage center and range for each cell\n        based on the signal dominance threshold points.\n\n        Parameters:\n        cells_sdf (pyspark.sql.DataFrame): A Spark DataFrame containing cell information, including directionality.\n        grid_sdf (pyspark.sql.DataFrame): A Spark DataFrame containing grid used for calculating signal dominance.\n\n        Returns:\n        pyspark.sql.DataFrame: A Spark DataFrame with the effective coverage information for each cell, including the\n                            coverage center and effective range. The DataFrame excludes intermediate columns used\n                            during the calculation.\n        \"\"\"\n        # omnidirectional cells\n        cells_omni_sdf = cells_sdf.filter(F.col(ColNames.directionality) == 0)\n\n        if cells_omni_sdf.rdd.isEmpty():\n            cells_omni_sdf = self.spark.createDataFrame([], cells_sdf.schema)\n        else:\n            cells_omni_sdf = self.get_signal_dominance_threshold_point(cells_omni_sdf, grid_sdf, \"omni\")\n\n            cells_omni_sdf = cells_omni_sdf.withColumn(\"coverage_center\", F.col(\"geometry\")).withColumn(\n                \"coverage_effective_range\", STF.ST_Distance(F.col(\"coverage_center\"), F.col(\"omni\"))\n            )\n        cells_omni_sdf.cache()\n        cells_omni_sdf.count()\n\n        # directional cells\n        cells_directional_sdf = cells_sdf.filter(F.col(ColNames.directionality) == 1)\n\n        if cells_directional_sdf.rdd.isEmpty():\n            cells_directional_sdf = self.spark.createDataFrame([], cells_sdf.schema)\n        else:\n            cells_directional_sdf = self.get_signal_dominance_threshold_point(\n                cells_directional_sdf, grid_sdf, \"directional_front\"\n            )\n\n            cells_directional_sdf.cache()\n            cells_directional_sdf.count()\n\n            cells_directional_sdf = self.get_signal_dominance_threshold_point(\n                cells_directional_sdf, grid_sdf, \"directional_back\"\n            )\n\n            cells_directional_sdf = cells_directional_sdf.withColumn(\n                \"coverage_center\",\n                STF.ST_LineInterpolatePoint(\n                    STF.ST_MakeLine(\n                        cells_directional_sdf[\"directional_front\"], cells_directional_sdf[\"directional_back\"]\n                    ),\n                    0.5,\n                ),\n            )\n            cells_directional_sdf = cells_directional_sdf.withColumn(\n                \"coverage_effective_range\",\n                STF.ST_Distance(cells_directional_sdf[\"coverage_center\"], cells_directional_sdf[\"directional_front\"]),\n            )\n\n        cells_sdf = cells_omni_sdf.unionByName(cells_directional_sdf, allowMissingColumns=True)\n\n        return cells_sdf.drop(\"omni\", \"directional_front\", \"directional_back\")\n\n    def get_signal_dominance_threshold_point(self, cells_sdf, grid_sdf, point_type):\n        \"\"\"\n        Calculates the signal dominance threshold points in cell maximum range.\n\n        For omnidirectional cell types, the signal dominance threshold point is calculated as\n        the furthest point along 90 degrees azimuth direction where signal dominance is less than the threshold.\n        For directional cells types, two signal dominance threshold points are calculated:\n            1. the furthest point along the directionality angle direction where signal dominance is less than the threshold\n            2. the furthest point opposite to the directionality angle direction where signal dominance is less than the threshold\n\n        Args:\n            cells_sdf (DataFrame): The DataFrame containing cell information.\n            grid_sdf (DataFrame): The DataFrame containing grid information.\n            point_type (str): The type of point to calculate the signal dominance threshold for.\n\n        Returns:\n            DataFrame: The updated cells DataFrame with the signal dominance threshold point added.\n        \"\"\"\n\n        do_azimuth_angle_adjustments = True\n        do_elevation_angle_adjustments = True\n\n        if point_type == \"directional_front\":\n            # Calculate range line for directional front point type\n            cells_sdf = cells_sdf.withColumn(\n                \"range_line\",\n                STF.ST_MakeLine(\n                    \"geometry\",\n                    STC.ST_Point(\n                        STF.ST_X(\"geometry\") + cells_sdf[\"range\"] * F.sin(F.radians(cells_sdf[\"azimuth_angle\"])),\n                        STF.ST_Y(\"geometry\") + cells_sdf[\"range\"] * F.cos(F.radians(cells_sdf[\"azimuth_angle\"])),\n                    ),\n                ),\n            )\n\n        elif point_type == \"directional_back\":\n            # Calculate range line for directional back point type\n            cells_sdf = cells_sdf.withColumn(\n                \"range_line\",\n                STF.ST_MakeLine(\n                    \"geometry\",\n                    STC.ST_Point(\n                        STF.ST_X(\"geometry\") + cells_sdf[\"range\"] * F.sin(F.radians(cells_sdf[\"azimuth_angle\"] + 180)),\n                        STF.ST_Y(\"geometry\") + cells_sdf[\"range\"] * F.cos(F.radians(cells_sdf[\"azimuth_angle\"] + 180)),\n                    ),\n                ),\n            )\n\n        elif point_type == \"omni\":\n            # Calculate range line for omni point type\n            cells_sdf = cells_sdf.withColumn(\n                \"range_line\",\n                STF.ST_MakeLine(\n                    \"geometry\",\n                    STC.ST_Point(\n                        STF.ST_X(\"geometry\") + cells_sdf[\"range\"] * F.sin(F.radians(F.lit(90))),\n                        STF.ST_Y(\"geometry\") + cells_sdf[\"range\"] * F.cos(F.radians(F.lit(90))),\n                    ),\n                ),\n            )\n\n            do_azimuth_angle_adjustments = False\n            do_elevation_angle_adjustments = False\n\n        cells_sdf = cells_sdf.withColumn(\"coverage_range_line_buffer\", F.lit(self.coverage_range_line_buffer))\n\n        cell_grid = self.spatial_join_within_distance(cells_sdf, grid_sdf, \"range_line\", \"coverage_range_line_buffer\")\n\n        # calculate 3d distance and planar distance from cell id to grid ids\n        cell_grid = self.calculate_cartesian_distances(cell_grid)\n\n        cell_grid = self.calculate_signal_dominance(\n            cell_grid, do_elevation_angle_adjustments, do_azimuth_angle_adjustments\n        )\n\n        cell_grid_filtered = cell_grid.filter(F.col(ColNames.signal_dominance) &gt; self.signal_dominance_treshold)\n        cell_counts = cell_grid_filtered.groupBy(\"cell_id\").count().withColumnRenamed(\"count\", \"filtered_count\")\n        cell_grid_joined = cell_grid.join(cell_counts, on=\"cell_id\", how=\"left\").fillna(0, subset=[\"filtered_count\"])\n\n        # In case if desired threshold filter removes all grid tiles in given direction, keep closest grid tile\n        # Otherwise keep furthest point where threshold condition meet\n        window_min = Window.partitionBy(\"cell_id\").orderBy(F.col(\"distance_to_cell\"))\n        window_max = Window.partitionBy(\"cell_id\").orderBy(F.desc(F.col(\"distance_to_cell\")))\n\n        cell_grid_max = (\n            cell_grid_joined.withColumn(\n                \"rank\",\n                F.when(F.col(\"filtered_count\") == 0, F.row_number().over(window_min)).otherwise(\n                    F.row_number().over(window_max)\n                ),\n            )\n            .filter(F.col(\"rank\") == 1)\n            .drop(\"rank\", \"filtered_count\")\n        )\n\n        cells_sdf = cells_sdf.join(\n            cell_grid_max.select(\"cell_id\", \"joined_geometry\").withColumnRenamed(\"joined_geometry\", point_type),\n            \"cell_id\",\n            \"left\",\n        )\n\n        return cells_sdf.drop(\n            \"range_line\", \"distance_to_cell_3d\", \"distance_to_cell\", \"signal_strength\", \"signal_dominance\", \"grid_id\"\n        )\n\n    @staticmethod\n    def calculate_distance_power_loss(sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Calculates distance power loss caluclated as\n        power - path_loss_exponent * 10 * log10(distance_to_cell_3D).\n\n        Args:\n            sdf (DataFrame): Input DataFrame.\n\n        Returns:\n            DataFrame: DataFrame with calculated distance power loss.\n        \"\"\"\n        sdf = sdf.withColumn(\n            ColNames.signal_strength,\n            F.col(ColNames.power)\n            - F.col(ColNames.path_loss_exponent) * 10 * F.log10(F.col(ColNames.distance_to_cell_3D)),\n        )\n\n        return sdf.drop(ColNames.power, ColNames.path_loss_exponent)\n\n    def calculate_signal_dominance(self, cell_grid_gdf, do_elevation_angle_adjustments, do_azimuth_angle_adjustments):\n        \"\"\"\n        Calculates the signal dominance for each cell in a grid DataFrame, optionally adjusting for elevation and azimuth angles.\n\n        This function performs a series of adjustments on the signal strength of each cell in the provided grid DataFrame to calculate the signal dominance. The adjustments include distance power loss, horizontal angle power adjustment (azimuth), and vertical angle power adjustment (elevation), based on the provided flags. Finally, it converts the adjusted signal strength into signal dominance using a logistic function.\n\n        Parameters:\n            cell_grid_gdf (GeoDataFrame): A GeoDataFrame containing the cell grid data.\n            do_elevation_angle_adjustments (bool): Flag to indicate whether to perform elevation angle adjustments.\n            do_azimuth_angle_adjustments (bool): Flag to indicate whether to perform azimuth angle adjustments.\n\n        Returns:\n            GeoDataFrame: The input GeoDataFrame with an additional column for signal dominance, after applying the specified adjustments.\n        \"\"\"\n\n        # calculate distance power loss\n        cell_grid_gdf = self.calculate_distance_power_loss(cell_grid_gdf)\n\n        # calculate horizontal angle power adjustment\n        if do_azimuth_angle_adjustments:\n\n            cell_grid_gdf_directional = cell_grid_gdf.filter(F.col(ColNames.directionality) == 1)\n\n            cell_grid_gdf_directional = self.join_sd_mapping(\n                cell_grid_gdf_directional,\n                self.sd_azimuth_mapping_sdf,\n                ColNames.horizontal_beam_width,\n                ColNames.azimuth_signal_strength_back_loss,\n                \"sd_azimuth\",\n            )\n            cell_grid_gdf_directional = self.calculate_horizontal_angle_power_adjustment(cell_grid_gdf_directional)\n\n            cell_grid_gdf = cell_grid_gdf_directional.unionByName(\n                cell_grid_gdf.where(F.col(ColNames.directionality) == 0),\n                allowMissingColumns=True,\n            )\n\n        # calculate vertical angle power adjustment\n        if do_elevation_angle_adjustments:\n\n            cell_grid_gdf_directional = cell_grid_gdf.filter(F.col(ColNames.directionality) == 1)\n\n            cell_grid_gdf_directional = self.join_sd_mapping(\n                cell_grid_gdf_directional,\n                self.sd_elevation_mapping_sdf,\n                ColNames.vertical_beam_width,\n                ColNames.elevation_signal_strength_back_loss,\n                \"sd_elevation\",\n            )\n\n            cell_grid_gdf_directional = self.calculate_vertical_angle_power_adjustment(cell_grid_gdf_directional)\n            cell_grid_gdf = cell_grid_gdf_directional.unionByName(\n                cell_grid_gdf.where(F.col(ColNames.directionality) == 0),\n                allowMissingColumns=True,\n            )\n\n        # calculate signal dominance\n        cell_grid_gdf = self.signal_strength_to_signal_dominance(\n            cell_grid_gdf, self.logistic_function_steepness, self.logistic_function_midpoint\n        )\n\n        return cell_grid_gdf\n\n    @staticmethod\n    def calculate_horizontal_angle_power_adjustment(sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Adjusts the signal strength of each cell in the input DataFrame based on the horizontal angle.\n\n        This function calculates the azimuth angle between each cell and a reference point,\n        projects the data to the elevation plane, and adjusts the signal strength based on the\n        relative azimuth angle and the distance to the cell. The adjustment is calculated using\n        a normal distribution model of signal strength.\n\n        Based on https://github.com/MobilePhoneESSnetBigData/mobloc/blob/master/R/signal_strength.R\n\n        Args:\n            sdf (DataFrame): A Spark DataFrame\n\n        Returns:\n            DataFrame: The input DataFrame with the 'signal_strength' column adjusted and intermediate columns dropped.\n        \"\"\"\n\n        # TODO: simplify math in this function by using Sedona built in spatial methods\n        sdf = sdf.withColumn(\n            \"theta_azim\",\n            (\n                90\n                - F.degrees(\n                    (\n                        F.atan2(\n                            STF.ST_Y(F.col(ColNames.joined_geometry)) - STF.ST_Y(F.col(ColNames.geometry)),\n                            STF.ST_X(F.col(ColNames.joined_geometry)) - STF.ST_X(F.col(ColNames.geometry)),\n                        )\n                    )\n                )\n            ),\n        )\n        sdf = sdf.withColumn(\n            \"theta_azim\",\n            F.when(F.col(\"theta_azim\") &lt; 0, F.col(\"theta_azim\") + 360).otherwise(F.col(\"theta_azim\")),\n        )\n        sdf = sdf.withColumn(\"azim\", (F.col(\"theta_azim\") - F.col(ColNames.azimuth_angle)) % 360)\n        sdf = sdf.withColumn(\n            \"azim\",\n            F.when(F.col(\"azim\") &gt; 180, F.col(\"azim\") - 360).otherwise(\n                F.when(F.col(\"azim\") &lt; -180, F.col(\"azim\") + 360).otherwise(F.col(\"azim\"))\n            ),\n        )\n        sdf = sdf.withColumn(\"a\", F.sin(F.radians(F.col(\"azim\"))) * F.col(ColNames.distance_to_cell))\n\n        # project to elevation plane\n        sdf = sdf.withColumn(\"b\", F.cos(F.radians(F.col(\"azim\"))) * F.col(ColNames.distance_to_cell))\n        sdf = sdf.withColumn(\"c\", STF.ST_Z(ColNames.geometry) - STF.ST_Z(ColNames.joined_geometry))\n\n        sdf = sdf.withColumn(\"d\", F.sqrt(F.col(\"b\") ** 2 + F.col(\"c\") ** 2))\n        sdf = sdf.withColumn(\"lambda\", F.degrees(F.atan2(F.col(\"c\"), F.abs(F.col(\"b\")))))\n        sdf = sdf.withColumn(\n            \"cases\",\n            F.when(\n                F.col(\"b\") &gt; 0,\n                F.when(F.col(ColNames.elevation_angle) &lt; F.col(\"lambda\"), F.lit(1)).otherwise(F.lit(2)),\n            ).otherwise(F.when(F.col(\"lambda\") + F.col(ColNames.elevation_angle) &lt; 90, F.lit(3)).otherwise(F.lit(4))),\n        )\n\n        sdf = sdf.withColumn(\n            \"e\",\n            F.when(\n                F.col(\"cases\") == 1,\n                F.cos(F.radians(F.col(\"lambda\") - F.col(ColNames.elevation_angle))) * F.col(\"d\"),\n            )\n            .when(\n                F.col(\"cases\") == 2,\n                F.cos(F.radians(F.col(ColNames.elevation_angle) - F.col(\"lambda\"))) * F.col(\"d\"),\n            )\n            .when(\n                F.col(\"cases\") == 3,\n                -F.cos(F.radians(F.col(\"lambda\") + F.col(ColNames.elevation_angle))) * F.col(\"d\"),\n            )\n            .otherwise(F.cos(F.radians(180 - F.col(\"lambda\") - F.col(ColNames.elevation_angle))) * F.col(\"d\")),\n        )\n\n        sdf = sdf.withColumn(\"azim2\", F.degrees(F.atan2(F.col(\"a\"), F.col(\"e\"))))\n\n        # finally get power adjustments\n        sdf = CellFootprintEstimation.norm_dBloss_spark(\n            sdf, \"azim2\", \"sd_azimuth\", ColNames.azimuth_signal_strength_back_loss\n        )\n\n        sdf = sdf.withColumn(\"signal_strength\", F.col(\"signal_strength\") + F.col(\"normalized_dBloss\"))\n\n        # cleanup\n        sdf = sdf.drop(\n            \"theta_azim\",\n            \"azim\",\n            \"a\",\n            \"b\",\n            \"c\",\n            \"d\",\n            \"_lambda\",\n            \"cases\",\n            \"e\",\n            \"azim2\",\n            \"sd_azimuth\",\n            \"normalized_dBloss\",\n        )\n\n        return sdf\n\n    @staticmethod\n    def calculate_vertical_angle_power_adjustment(sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Adjusts the signal strength of each cell in the input DataFrame based on the vertical angle.\n\n        This function calculates the elevation angle between each cell and a reference point,\n        and adjusts the signal strength based on the relative elevation angle and the distance to the cell.\n        The adjustment is calculated using a normal distribution model of signal strength.\n\n        Based on https://github.com/MobilePhoneESSnetBigData/mobloc/blob/master/R/signal_strength.R\n\n        Args:\n            sdf (DataFrame): A Spark DataFrame\n\n        Returns:\n            DataFrame: The input DataFrame with the 'signal_strength' column adjusted and intermediate columns dropped.\n        \"\"\"\n\n        sdf = sdf.withColumn(\n            \"gamma_elev\",\n            F.degrees(\n                F.atan2(\n                    STF.ST_Z(ColNames.geometry) - STF.ST_Z(ColNames.joined_geometry),\n                    F.col(ColNames.distance_to_cell),\n                )\n            ),\n        )\n        sdf = sdf.withColumn(\"elev\", (F.col(\"gamma_elev\") - F.col(ColNames.elevation_angle)) % 360)\n\n        sdf = sdf.withColumn(\n            \"elev\",\n            F.when(F.col(\"elev\") &gt; 180, F.col(\"elev\") - 360).otherwise(\n                F.when(F.col(\"elev\") &lt; -180, F.col(\"elev\") + 360).otherwise(F.col(\"elev\"))\n            ),\n        )\n\n        # finally get power adjustments\n        sdf = CellFootprintEstimation.norm_dBloss_spark(\n            sdf, \"elev\", \"sd_elevation\", ColNames.elevation_signal_strength_back_loss\n        )\n\n        sdf = sdf.withColumn(\n            ColNames.signal_strength,\n            F.col(\n                ColNames.signal_strength,\n            )\n            + F.col(\"normalized_dBloss\"),\n        )\n\n        # cleanup\n        sdf = sdf.drop(\"gamma_elev\", \"elev\", \"sd_elevation\", \"normalized_dBloss\")\n\n        return sdf\n\n    @staticmethod\n    def normal_distribution_col(x_col: Column, mean_col: Column, sd_col: Column) -&gt; Column:\n        \"\"\"\n        Computes the value of the normal distribution for each row in a DataFrame based on\n        the provided columns for the point, mean, and standard deviation.\n\n        This function applies the normal distribution formula to each row of the DataFrame using\n        the specified columns for the point (x), mean, and standard deviation (sd).\n        The normal distribution formula used is:\n\n            f(x) = (1 / (sqrt(2 * pi) * sd)) * exp(-0.5 * ((x - mean) / sd)^2)\n\n        where `x` is the value at which the normal distribution is evaluated,\n        `mean` is the mean of the distribution, and `sd` is the standard deviation.\n\n        Parameters:\n            x_col (Column): A Spark DataFrame column representing the point at which to evaluate the normal distribution.\n            mean_col (Column): A Spark DataFrame column representing the mean of the normal distribution.\n            sd_col (Column): A Spark DataFrame column representing the standard deviation of the normal distribution.\n\n        Returns:\n            Column: A Spark DataFrame column with the computed normal distribution values for each row.\n        \"\"\"\n        return (1.0 / (F.sqrt(2.0 * F.lit(pi)) * sd_col)) * F.exp(-0.5 * ((x_col - mean_col) / sd_col) ** 2)\n\n    @staticmethod\n    def norm_dBloss_spark(sdf: DataFrame, angle_col: str, sd_col: str, db_back_col: str) -&gt; DataFrame:\n        \"\"\"\n        Normalizes the dB loss in a Spark DataFrame based on the angle, standard deviation, and dB back loss.\n\n        This method performs several operations to normalize the dB loss for each row in the Spark DataFrame:\n        1. Normalizes the angle to a range of [-180, 180) degrees.\n        2. Calculates the normal distribution of the normalized angles with a mean of 0 and a standard deviation specified by `sd_col`.\n        3. Calculates the normal distribution for angles 0 and 180 degrees using precomputed values, with a mean of 0 and the same standard deviation.\n        4. Computes an inflation factor based on the dB back loss column and the difference between the normal distributions at 0 and 180 degrees.\n        5. Calculates the normalized dB loss by adjusting the normal distribution of the angle with the inflation factor.\n\n        Parameters:\n            sdf (DataFrame): The input Spark DataFrame containing the data.\n            angle_col (str): The name of the column that contains the angles to be normalized.\n            sd_col (str): The name of the column that contains the standard deviation values for the normal distribution calculation.\n            db_back_col (str): The name of the column in that contains the dB back loss values used to calculate the inflation factor.\n\n        Returns:\n            DataFrame: A Spark DataFrame with the normalized dB loss added and intermediate columns removed.\n        \"\"\"\n        # Normalizing angles\n\n        sdf = sdf.withColumn(\"angle_normalized\", ((F.col(angle_col) + 180) % 360) - 180)\n\n        # Calculate the normal distribution for the normalized angles\n        sdf = sdf.withColumn(\n            \"norm_dist_angle\",\n            CellFootprintEstimation.normal_distribution_col(F.col(\"angle_normalized\"), F.lit(0), F.col(sd_col)),\n        )\n\n        # Calculate the normal distribution for 0 and 180 degrees using precomputed values\n        n_dist_0 = CellFootprintEstimation.normal_distribution_col(F.lit(0), F.lit(0), F.col(sd_col))\n        n_dist_180 = CellFootprintEstimation.normal_distribution_col(F.lit(180), F.lit(0), F.col(sd_col))\n\n        # Calculate the inflated factor\n        sdf = sdf.withColumn(\"inflate\", -F.col(db_back_col) / (n_dist_0 - n_dist_180))\n\n        # Calculate the normalized dB loss\n        sdf = sdf.withColumn(\"normalized_dBloss\", (F.col(\"norm_dist_angle\") - n_dist_0) * F.col(\"inflate\"))\n\n        return sdf.drop(\"angle_normalized\", \"norm_dist_angle\", \"inflate\")\n\n    @staticmethod\n    def signal_strength_to_signal_dominance(\n        sdf: DataFrame,\n        logistic_function_steepness: float,\n        logistic_function_midpoint: float,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Converts signal strength to signal dominance using a logistic function.\n        Methodology from A Bayesian approach to location estimation of mobile devices\n        from mobile network operator data. Tennekes and Gootzen (2022).\n\n        The logistic function is defined as 1 / (1 + exp(-scale)),\n        where scale is (signal_strength - logistic_function_midpoint) * logistic_function_steepness.\n\n        Parameters:\n        sdf (DataFrame): SignalStrenghtDataObject Spark DataFrame.\n        logistic_function_steepness (float): The steepness parameter for the logistic function.\n        logistic_function_midpoint (float): The midpoint parameter for the logistic function.\n\n        Returns:\n        DataFrame: A Spark DataFrame with the signal dominance added as a new column.\n        \"\"\"\n        sdf = sdf.withColumn(\n            \"scale\",\n            (F.col(ColNames.signal_strength) - logistic_function_midpoint) * logistic_function_steepness,\n        )\n        sdf = sdf.withColumn(ColNames.signal_dominance, 1 / (1 + F.exp(-F.col(\"scale\"))))\n        sdf = sdf.drop(\"scale\")\n\n        return sdf\n\n    @staticmethod\n    def prune_small_signal_dominance(sdf: DataFrame, signal_dominance_threshold: float) -&gt; DataFrame:\n        \"\"\"\n        Prunes rows from the DataFrame where the signal dominance is less than or equal to the provided threshold.\n\n        Parameters:\n        sdf (DataFrame): A Spark DataFrame containing the signal dominance data.\n        signal_dominance_threshold (float): The threshold for pruning small signal dominance values.\n\n        Returns:\n        DataFrame: A DataFrame with rows having signal dominance less than or equal to the threshold removed.\n        \"\"\"\n        sdf = sdf.filter(F.col(ColNames.signal_dominance) &gt; signal_dominance_threshold)\n        return sdf\n\n    @staticmethod\n    def prune_max_cells_per_grid_tile(sdf: DataFrame, max_cells_per_grid_tile: int) -&gt; DataFrame:\n        \"\"\"\n        Prunes rows from the DataFrame exceeding the maximum number of cells allowed per grid tile.\n\n        The rows are ordered by signal dominance in descending order,\n        and only the top 'max_cells_per_grid_tile' rows are kept for each grid tile.\n\n        Parameters:\n        sdf (DataFrame): A Spark DataFrame containing the signal dominance data.\n        max_cells_per_grid_tile (int): The maximum number of cells allowed per grid tile.\n\n        Returns:\n        DataFrame: A DataFrame with rows exceeding the maximum number of cells per grid tile removed.\n        \"\"\"\n        window = Window.partitionBy(ColNames.grid_id).orderBy(F.desc(ColNames.signal_dominance))\n\n        sdf = sdf.withColumn(\"row_number\", F.row_number().over(window))\n        sdf = sdf.filter(F.col(\"row_number\") &lt;= max_cells_per_grid_tile)\n        sdf = sdf.drop(\"row_number\")\n\n        return sdf\n\n    @staticmethod\n    def prune_signal_difference_from_best(sdf: DataFrame, difference_threshold: float) -&gt; DataFrame:\n        \"\"\"\n        Prunes rows from the DataFrame based on a threshold of signal dominance difference.\n\n        The rows are ordered by signal dominance in descending order, and only the rows where the difference\n        in signal dominance from the maximum is less than the threshold are kept for each grid tile.\n\n        Parameters:\n        sdf (DataFrame): A Spark DataFrame containing the signal dominance data.\n        threshold (float): The threshold for signal dominance difference in percentage.\n\n        Returns:\n        DataFrame: A DataFrame with rows pruned based on the signal dominance difference threshold.\n        \"\"\"\n        window = Window.partitionBy(ColNames.grid_id).orderBy(F.desc(ColNames.signal_dominance))\n\n        sdf = sdf.withColumn(\"row_number\", F.row_number().over(window))\n\n        sdf = sdf.withColumn(\"max_signal_dominance\", F.first(ColNames.signal_dominance).over(window))\n        sdf = sdf.withColumn(\n            \"signal_dominance_diff_percentage\",\n            (sdf[\"max_signal_dominance\"] - sdf[ColNames.signal_dominance]) / sdf[\"max_signal_dominance\"] * 100,\n        )\n\n        sdf = sdf.filter(\n            (F.col(\"row_number\") == 1) | (F.col(\"signal_dominance_diff_percentage\") &lt;= difference_threshold)\n        )\n\n        sdf = sdf.drop(\"row_number\", \"max_signal_dominance\", \"signal_dominance_diff_percentage\")\n\n        return sdf\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.add_z_to_point_geometry","title":"<code>add_z_to_point_geometry(sdf, geometry_col, use_elevation)</code>  <code>staticmethod</code>","text":"<p>Adds z value to the point geometry (grid centroids). If elevation is taken into account, set Z values from z column, otherwise to 0.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>use_elevation</code> <code>bool</code> <p>Whether to use elevation.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with z value added to point geometry.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef add_z_to_point_geometry(sdf: DataFrame, geometry_col: str, use_elevation: bool) -&gt; DataFrame:\n    \"\"\"\n    Adds z value to the point geometry (grid centroids).\n    If elevation is taken into account, set Z values from z column, otherwise to 0.\n\n    Args:\n        sdf (DataFrame): Input DataFrame.\n        use_elevation (bool): Whether to use elevation.\n\n    Returns:\n        DataFrame: DataFrame with z value added to point geometry.\n    \"\"\"\n    if use_elevation:\n        sdf = sdf.withColumn(\n            geometry_col,\n            STC.ST_MakePoint(\n                STF.ST_X(F.col(geometry_col)),\n                STF.ST_Y(F.col(geometry_col)),\n                F.col(ColNames.elevation),\n            ),\n        )\n    else:\n        sdf = sdf.withColumn(\n            geometry_col,\n            STC.ST_MakePoint(\n                STF.ST_X(F.col(geometry_col)),\n                STF.ST_Y(F.col(geometry_col)),\n                F.lit(0.0),\n            ),\n        )\n\n    return sdf.drop(ColNames.elevation)\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.calculate_cartesian_distances","title":"<code>calculate_cartesian_distances(sdf)</code>  <code>staticmethod</code>","text":"<p>Calculates cartesian distances.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with calculated cartesian distances.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef calculate_cartesian_distances(sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Calculates cartesian distances.\n\n    Args:\n        sdf (DataFrame): Input DataFrame.\n\n    Returns:\n        DataFrame: DataFrame with calculated cartesian distances.\n    \"\"\"\n\n    sdf = sdf.withColumns(\n        {\n            ColNames.distance_to_cell_3D: STF.ST_3DDistance(\n                F.col(ColNames.geometry), F.col(ColNames.joined_geometry)\n            ),\n            ColNames.distance_to_cell: STF.ST_Distance(F.col(ColNames.geometry), F.col(ColNames.joined_geometry)),\n        }\n    )\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.calculate_distance_power_loss","title":"<code>calculate_distance_power_loss(sdf)</code>  <code>staticmethod</code>","text":"<p>Calculates distance power loss caluclated as power - path_loss_exponent * 10 * log10(distance_to_cell_3D).</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with calculated distance power loss.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef calculate_distance_power_loss(sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Calculates distance power loss caluclated as\n    power - path_loss_exponent * 10 * log10(distance_to_cell_3D).\n\n    Args:\n        sdf (DataFrame): Input DataFrame.\n\n    Returns:\n        DataFrame: DataFrame with calculated distance power loss.\n    \"\"\"\n    sdf = sdf.withColumn(\n        ColNames.signal_strength,\n        F.col(ColNames.power)\n        - F.col(ColNames.path_loss_exponent) * 10 * F.log10(F.col(ColNames.distance_to_cell_3D)),\n    )\n\n    return sdf.drop(ColNames.power, ColNames.path_loss_exponent)\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.calculate_effective_coverage","title":"<code>calculate_effective_coverage(cells_sdf, grid_sdf)</code>","text":"<p>Calculates effective cell coverage center and range based on desired signal domincance threshold.</p> <p>The function first separates the cells into omnidirectional and directional types, then calculates the signal dominance threshold points for each type. The function then calculates the effective coverage center and range for each cell based on the signal dominance threshold points.</p> <p>Parameters: cells_sdf (pyspark.sql.DataFrame): A Spark DataFrame containing cell information, including directionality. grid_sdf (pyspark.sql.DataFrame): A Spark DataFrame containing grid used for calculating signal dominance.</p> <p>pyspark.sql.DataFrame: A Spark DataFrame with the effective coverage information for each cell, including the                     coverage center and effective range. The DataFrame excludes intermediate columns used                     during the calculation.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>def calculate_effective_coverage(self, cells_sdf: DataFrame, grid_sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Calculates effective cell coverage center and range based on desired signal domincance threshold.\n\n    The function first separates the cells into omnidirectional and directional types,\n    then calculates the signal dominance threshold points for each type.\n    The function then calculates the effective coverage center and range for each cell\n    based on the signal dominance threshold points.\n\n    Parameters:\n    cells_sdf (pyspark.sql.DataFrame): A Spark DataFrame containing cell information, including directionality.\n    grid_sdf (pyspark.sql.DataFrame): A Spark DataFrame containing grid used for calculating signal dominance.\n\n    Returns:\n    pyspark.sql.DataFrame: A Spark DataFrame with the effective coverage information for each cell, including the\n                        coverage center and effective range. The DataFrame excludes intermediate columns used\n                        during the calculation.\n    \"\"\"\n    # omnidirectional cells\n    cells_omni_sdf = cells_sdf.filter(F.col(ColNames.directionality) == 0)\n\n    if cells_omni_sdf.rdd.isEmpty():\n        cells_omni_sdf = self.spark.createDataFrame([], cells_sdf.schema)\n    else:\n        cells_omni_sdf = self.get_signal_dominance_threshold_point(cells_omni_sdf, grid_sdf, \"omni\")\n\n        cells_omni_sdf = cells_omni_sdf.withColumn(\"coverage_center\", F.col(\"geometry\")).withColumn(\n            \"coverage_effective_range\", STF.ST_Distance(F.col(\"coverage_center\"), F.col(\"omni\"))\n        )\n    cells_omni_sdf.cache()\n    cells_omni_sdf.count()\n\n    # directional cells\n    cells_directional_sdf = cells_sdf.filter(F.col(ColNames.directionality) == 1)\n\n    if cells_directional_sdf.rdd.isEmpty():\n        cells_directional_sdf = self.spark.createDataFrame([], cells_sdf.schema)\n    else:\n        cells_directional_sdf = self.get_signal_dominance_threshold_point(\n            cells_directional_sdf, grid_sdf, \"directional_front\"\n        )\n\n        cells_directional_sdf.cache()\n        cells_directional_sdf.count()\n\n        cells_directional_sdf = self.get_signal_dominance_threshold_point(\n            cells_directional_sdf, grid_sdf, \"directional_back\"\n        )\n\n        cells_directional_sdf = cells_directional_sdf.withColumn(\n            \"coverage_center\",\n            STF.ST_LineInterpolatePoint(\n                STF.ST_MakeLine(\n                    cells_directional_sdf[\"directional_front\"], cells_directional_sdf[\"directional_back\"]\n                ),\n                0.5,\n            ),\n        )\n        cells_directional_sdf = cells_directional_sdf.withColumn(\n            \"coverage_effective_range\",\n            STF.ST_Distance(cells_directional_sdf[\"coverage_center\"], cells_directional_sdf[\"directional_front\"]),\n        )\n\n    cells_sdf = cells_omni_sdf.unionByName(cells_directional_sdf, allowMissingColumns=True)\n\n    return cells_sdf.drop(\"omni\", \"directional_front\", \"directional_back\")\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.calculate_horizontal_angle_power_adjustment","title":"<code>calculate_horizontal_angle_power_adjustment(sdf)</code>  <code>staticmethod</code>","text":"<p>Adjusts the signal strength of each cell in the input DataFrame based on the horizontal angle.</p> <p>This function calculates the azimuth angle between each cell and a reference point, projects the data to the elevation plane, and adjusts the signal strength based on the relative azimuth angle and the distance to the cell. The adjustment is calculated using a normal distribution model of signal strength.</p> <p>Based on https://github.com/MobilePhoneESSnetBigData/mobloc/blob/master/R/signal_strength.R</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>A Spark DataFrame</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The input DataFrame with the 'signal_strength' column adjusted and intermediate columns dropped.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef calculate_horizontal_angle_power_adjustment(sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Adjusts the signal strength of each cell in the input DataFrame based on the horizontal angle.\n\n    This function calculates the azimuth angle between each cell and a reference point,\n    projects the data to the elevation plane, and adjusts the signal strength based on the\n    relative azimuth angle and the distance to the cell. The adjustment is calculated using\n    a normal distribution model of signal strength.\n\n    Based on https://github.com/MobilePhoneESSnetBigData/mobloc/blob/master/R/signal_strength.R\n\n    Args:\n        sdf (DataFrame): A Spark DataFrame\n\n    Returns:\n        DataFrame: The input DataFrame with the 'signal_strength' column adjusted and intermediate columns dropped.\n    \"\"\"\n\n    # TODO: simplify math in this function by using Sedona built in spatial methods\n    sdf = sdf.withColumn(\n        \"theta_azim\",\n        (\n            90\n            - F.degrees(\n                (\n                    F.atan2(\n                        STF.ST_Y(F.col(ColNames.joined_geometry)) - STF.ST_Y(F.col(ColNames.geometry)),\n                        STF.ST_X(F.col(ColNames.joined_geometry)) - STF.ST_X(F.col(ColNames.geometry)),\n                    )\n                )\n            )\n        ),\n    )\n    sdf = sdf.withColumn(\n        \"theta_azim\",\n        F.when(F.col(\"theta_azim\") &lt; 0, F.col(\"theta_azim\") + 360).otherwise(F.col(\"theta_azim\")),\n    )\n    sdf = sdf.withColumn(\"azim\", (F.col(\"theta_azim\") - F.col(ColNames.azimuth_angle)) % 360)\n    sdf = sdf.withColumn(\n        \"azim\",\n        F.when(F.col(\"azim\") &gt; 180, F.col(\"azim\") - 360).otherwise(\n            F.when(F.col(\"azim\") &lt; -180, F.col(\"azim\") + 360).otherwise(F.col(\"azim\"))\n        ),\n    )\n    sdf = sdf.withColumn(\"a\", F.sin(F.radians(F.col(\"azim\"))) * F.col(ColNames.distance_to_cell))\n\n    # project to elevation plane\n    sdf = sdf.withColumn(\"b\", F.cos(F.radians(F.col(\"azim\"))) * F.col(ColNames.distance_to_cell))\n    sdf = sdf.withColumn(\"c\", STF.ST_Z(ColNames.geometry) - STF.ST_Z(ColNames.joined_geometry))\n\n    sdf = sdf.withColumn(\"d\", F.sqrt(F.col(\"b\") ** 2 + F.col(\"c\") ** 2))\n    sdf = sdf.withColumn(\"lambda\", F.degrees(F.atan2(F.col(\"c\"), F.abs(F.col(\"b\")))))\n    sdf = sdf.withColumn(\n        \"cases\",\n        F.when(\n            F.col(\"b\") &gt; 0,\n            F.when(F.col(ColNames.elevation_angle) &lt; F.col(\"lambda\"), F.lit(1)).otherwise(F.lit(2)),\n        ).otherwise(F.when(F.col(\"lambda\") + F.col(ColNames.elevation_angle) &lt; 90, F.lit(3)).otherwise(F.lit(4))),\n    )\n\n    sdf = sdf.withColumn(\n        \"e\",\n        F.when(\n            F.col(\"cases\") == 1,\n            F.cos(F.radians(F.col(\"lambda\") - F.col(ColNames.elevation_angle))) * F.col(\"d\"),\n        )\n        .when(\n            F.col(\"cases\") == 2,\n            F.cos(F.radians(F.col(ColNames.elevation_angle) - F.col(\"lambda\"))) * F.col(\"d\"),\n        )\n        .when(\n            F.col(\"cases\") == 3,\n            -F.cos(F.radians(F.col(\"lambda\") + F.col(ColNames.elevation_angle))) * F.col(\"d\"),\n        )\n        .otherwise(F.cos(F.radians(180 - F.col(\"lambda\") - F.col(ColNames.elevation_angle))) * F.col(\"d\")),\n    )\n\n    sdf = sdf.withColumn(\"azim2\", F.degrees(F.atan2(F.col(\"a\"), F.col(\"e\"))))\n\n    # finally get power adjustments\n    sdf = CellFootprintEstimation.norm_dBloss_spark(\n        sdf, \"azim2\", \"sd_azimuth\", ColNames.azimuth_signal_strength_back_loss\n    )\n\n    sdf = sdf.withColumn(\"signal_strength\", F.col(\"signal_strength\") + F.col(\"normalized_dBloss\"))\n\n    # cleanup\n    sdf = sdf.drop(\n        \"theta_azim\",\n        \"azim\",\n        \"a\",\n        \"b\",\n        \"c\",\n        \"d\",\n        \"_lambda\",\n        \"cases\",\n        \"e\",\n        \"azim2\",\n        \"sd_azimuth\",\n        \"normalized_dBloss\",\n    )\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.calculate_signal_dominance","title":"<code>calculate_signal_dominance(cell_grid_gdf, do_elevation_angle_adjustments, do_azimuth_angle_adjustments)</code>","text":"<p>Calculates the signal dominance for each cell in a grid DataFrame, optionally adjusting for elevation and azimuth angles.</p> <p>This function performs a series of adjustments on the signal strength of each cell in the provided grid DataFrame to calculate the signal dominance. The adjustments include distance power loss, horizontal angle power adjustment (azimuth), and vertical angle power adjustment (elevation), based on the provided flags. Finally, it converts the adjusted signal strength into signal dominance using a logistic function.</p> <p>Parameters:</p> Name Type Description Default <code>cell_grid_gdf</code> <code>GeoDataFrame</code> <p>A GeoDataFrame containing the cell grid data.</p> required <code>do_elevation_angle_adjustments</code> <code>bool</code> <p>Flag to indicate whether to perform elevation angle adjustments.</p> required <code>do_azimuth_angle_adjustments</code> <code>bool</code> <p>Flag to indicate whether to perform azimuth angle adjustments.</p> required <p>Returns:</p> Name Type Description <code>GeoDataFrame</code> <p>The input GeoDataFrame with an additional column for signal dominance, after applying the specified adjustments.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>def calculate_signal_dominance(self, cell_grid_gdf, do_elevation_angle_adjustments, do_azimuth_angle_adjustments):\n    \"\"\"\n    Calculates the signal dominance for each cell in a grid DataFrame, optionally adjusting for elevation and azimuth angles.\n\n    This function performs a series of adjustments on the signal strength of each cell in the provided grid DataFrame to calculate the signal dominance. The adjustments include distance power loss, horizontal angle power adjustment (azimuth), and vertical angle power adjustment (elevation), based on the provided flags. Finally, it converts the adjusted signal strength into signal dominance using a logistic function.\n\n    Parameters:\n        cell_grid_gdf (GeoDataFrame): A GeoDataFrame containing the cell grid data.\n        do_elevation_angle_adjustments (bool): Flag to indicate whether to perform elevation angle adjustments.\n        do_azimuth_angle_adjustments (bool): Flag to indicate whether to perform azimuth angle adjustments.\n\n    Returns:\n        GeoDataFrame: The input GeoDataFrame with an additional column for signal dominance, after applying the specified adjustments.\n    \"\"\"\n\n    # calculate distance power loss\n    cell_grid_gdf = self.calculate_distance_power_loss(cell_grid_gdf)\n\n    # calculate horizontal angle power adjustment\n    if do_azimuth_angle_adjustments:\n\n        cell_grid_gdf_directional = cell_grid_gdf.filter(F.col(ColNames.directionality) == 1)\n\n        cell_grid_gdf_directional = self.join_sd_mapping(\n            cell_grid_gdf_directional,\n            self.sd_azimuth_mapping_sdf,\n            ColNames.horizontal_beam_width,\n            ColNames.azimuth_signal_strength_back_loss,\n            \"sd_azimuth\",\n        )\n        cell_grid_gdf_directional = self.calculate_horizontal_angle_power_adjustment(cell_grid_gdf_directional)\n\n        cell_grid_gdf = cell_grid_gdf_directional.unionByName(\n            cell_grid_gdf.where(F.col(ColNames.directionality) == 0),\n            allowMissingColumns=True,\n        )\n\n    # calculate vertical angle power adjustment\n    if do_elevation_angle_adjustments:\n\n        cell_grid_gdf_directional = cell_grid_gdf.filter(F.col(ColNames.directionality) == 1)\n\n        cell_grid_gdf_directional = self.join_sd_mapping(\n            cell_grid_gdf_directional,\n            self.sd_elevation_mapping_sdf,\n            ColNames.vertical_beam_width,\n            ColNames.elevation_signal_strength_back_loss,\n            \"sd_elevation\",\n        )\n\n        cell_grid_gdf_directional = self.calculate_vertical_angle_power_adjustment(cell_grid_gdf_directional)\n        cell_grid_gdf = cell_grid_gdf_directional.unionByName(\n            cell_grid_gdf.where(F.col(ColNames.directionality) == 0),\n            allowMissingColumns=True,\n        )\n\n    # calculate signal dominance\n    cell_grid_gdf = self.signal_strength_to_signal_dominance(\n        cell_grid_gdf, self.logistic_function_steepness, self.logistic_function_midpoint\n    )\n\n    return cell_grid_gdf\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.calculate_vertical_angle_power_adjustment","title":"<code>calculate_vertical_angle_power_adjustment(sdf)</code>  <code>staticmethod</code>","text":"<p>Adjusts the signal strength of each cell in the input DataFrame based on the vertical angle.</p> <p>This function calculates the elevation angle between each cell and a reference point, and adjusts the signal strength based on the relative elevation angle and the distance to the cell. The adjustment is calculated using a normal distribution model of signal strength.</p> <p>Based on https://github.com/MobilePhoneESSnetBigData/mobloc/blob/master/R/signal_strength.R</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>A Spark DataFrame</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The input DataFrame with the 'signal_strength' column adjusted and intermediate columns dropped.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef calculate_vertical_angle_power_adjustment(sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Adjusts the signal strength of each cell in the input DataFrame based on the vertical angle.\n\n    This function calculates the elevation angle between each cell and a reference point,\n    and adjusts the signal strength based on the relative elevation angle and the distance to the cell.\n    The adjustment is calculated using a normal distribution model of signal strength.\n\n    Based on https://github.com/MobilePhoneESSnetBigData/mobloc/blob/master/R/signal_strength.R\n\n    Args:\n        sdf (DataFrame): A Spark DataFrame\n\n    Returns:\n        DataFrame: The input DataFrame with the 'signal_strength' column adjusted and intermediate columns dropped.\n    \"\"\"\n\n    sdf = sdf.withColumn(\n        \"gamma_elev\",\n        F.degrees(\n            F.atan2(\n                STF.ST_Z(ColNames.geometry) - STF.ST_Z(ColNames.joined_geometry),\n                F.col(ColNames.distance_to_cell),\n            )\n        ),\n    )\n    sdf = sdf.withColumn(\"elev\", (F.col(\"gamma_elev\") - F.col(ColNames.elevation_angle)) % 360)\n\n    sdf = sdf.withColumn(\n        \"elev\",\n        F.when(F.col(\"elev\") &gt; 180, F.col(\"elev\") - 360).otherwise(\n            F.when(F.col(\"elev\") &lt; -180, F.col(\"elev\") + 360).otherwise(F.col(\"elev\"))\n        ),\n    )\n\n    # finally get power adjustments\n    sdf = CellFootprintEstimation.norm_dBloss_spark(\n        sdf, \"elev\", \"sd_elevation\", ColNames.elevation_signal_strength_back_loss\n    )\n\n    sdf = sdf.withColumn(\n        ColNames.signal_strength,\n        F.col(\n            ColNames.signal_strength,\n        )\n        + F.col(\"normalized_dBloss\"),\n    )\n\n    # cleanup\n    sdf = sdf.drop(\"gamma_elev\", \"elev\", \"sd_elevation\", \"normalized_dBloss\")\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.create_cell_point_geometry","title":"<code>create_cell_point_geometry(sdf, use_elevation)</code>  <code>staticmethod</code>","text":"<p>Creates cell point geometry. If elevation is taken into account, set Z values from z column, otherwise to 0.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>use_elevation</code> <code>bool</code> <p>Whether to use elevation.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with cell point geometry.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef create_cell_point_geometry(sdf: DataFrame, use_elevation: bool) -&gt; DataFrame:\n    \"\"\"\n    Creates cell point geometry.\n    If elevation is taken into account, set Z values from z column, otherwise to 0.\n\n    Args:\n        sdf (DataFrame): Input DataFrame.\n        use_elevation (bool): Whether to use elevation.\n\n    Returns:\n        DataFrame: DataFrame with cell point geometry.\n    \"\"\"\n    if use_elevation:\n        sdf = sdf.withColumn(\n            ColNames.geometry,\n            STC.ST_MakePoint(\n                F.col(ColNames.longitude),\n                F.col(ColNames.latitude),\n                F.col(ColNames.altitude) + F.col(ColNames.antenna_height),\n            ),\n        )\n    else:\n        sdf = sdf.withColumn(\n            ColNames.geometry,\n            STC.ST_MakePoint(\n                F.col(ColNames.longitude),\n                F.col(ColNames.latitude),\n                F.col(ColNames.antenna_height),\n            ),\n        )\n    # assign crs\n    sdf = sdf.withColumn(ColNames.geometry, STF.ST_SetSRID(F.col(ColNames.geometry), F.lit(4326)))\n\n    return sdf.drop(ColNames.latitude, ColNames.longitude, ColNames.altitude, ColNames.antenna_height)\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.create_default_properties_df","title":"<code>create_default_properties_df()</code>","text":"<p>Creates a DataFrame with default cell properties from config dict.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame with default cell properties.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>def create_default_properties_df(self) -&gt; DataFrame:\n    \"\"\"\n    Creates a DataFrame with default cell properties from config dict.\n\n    Returns:\n        DataFrame: A DataFrame with default cell properties.\n    \"\"\"\n\n    rows = [Row(cell_type=k, **v) for k, v in self.default_cell_properties.items()]\n    return self.spark.createDataFrame(rows).withColumnRenamed(\"cell_type\", ColNames.cell_type)\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.create_mapping","title":"<code>create_mapping(db_back, signal_front_back_difference_col)</code>  <code>staticmethod</code>","text":"<p>Creates a mapping between standard deviation and the angle at which the signal strength falls to 3 dB below its maximum value.</p> <p>Parameters:</p> Name Type Description Default <code>db_back</code> <code>float</code> <p>The difference in signal strength in dB between the front and back of the signal.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame where each row corresponds to a</p> <code>DataFrame</code> <p>standard deviation and contains the corresponding angle.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef create_mapping(db_back: float, signal_front_back_difference_col) -&gt; DataFrame:\n    \"\"\"\n    Creates a mapping between standard deviation and the angle\n    at which the signal strength falls to 3 dB below its maximum value.\n\n    Args:\n        db_back (float): The difference in signal strength in dB between the front and back of the signal.\n\n    Returns:\n        DataFrame: A DataFrame where each row corresponds to a\n        standard deviation and contains the corresponding angle.\n    \"\"\"\n    idf = pd.DataFrame({\"sd\": np.arange(180 / 1000, 180, 180 / 1000)})\n    idf[\"deg\"] = idf[\"sd\"].apply(CellFootprintEstimation.get_min3db, db_back=db_back)\n    df = pd.DataFrame({\"deg\": np.arange(1, 181)})\n    df[\"sd\"] = df[\"deg\"].apply(lambda dg: idf.loc[np.abs(idf[\"deg\"] - dg).idxmin(), \"sd\"])\n    df[signal_front_back_difference_col] = db_back\n    return df\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.find_sd","title":"<code>find_sd(beam_width, mapping)</code>  <code>staticmethod</code>","text":"<p>Finds the standard deviation corresponding to the given beam width using the provided mapping.</p> <p>Parameters:</p> Name Type Description Default <code>beam_width</code> <code>float</code> <p>The width of the beam in degrees.</p> required <code>mapping</code> <code>DataFrame</code> <p>A DataFrame where each row corresponds to a standard deviation and contains the corresponding angle.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The standard deviation corresponding to the given beam width.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef find_sd(beam_width: float, mapping: DataFrame) -&gt; float:\n    \"\"\"\n    Finds the standard deviation corresponding to the given beam width using the provided mapping.\n\n    Args:\n        beam_width (float): The width of the beam in degrees.\n        mapping (DataFrame): A DataFrame where each row corresponds to a standard deviation\n            and contains the corresponding angle.\n\n    Returns:\n        float: The standard deviation corresponding to the given beam width.\n    \"\"\"\n    min_diff_index = (abs(mapping[\"deg\"] - beam_width / 2)).idxmin()\n    return float(mapping.loc[min_diff_index, \"sd\"])\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.get_angular_adjustments_sd_mapping","title":"<code>get_angular_adjustments_sd_mapping(cells_sdf, beam_width_col, signal_front_back_difference_col, angular_adjustment_type)</code>","text":"<p>Calculates standard deviations in signal strength based on beam width and front-back cell signal difference.</p> <p>Parameters:</p> Name Type Description Default <code>cells_sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>beam_width_col</code> <code>str</code> <p>Column name for the beam width.</p> required <code>signal_front_back_difference_col</code> <code>str</code> <p>Column name for the signal front-back difference.</p> required <code>angular_adjustment_type</code> <code>str</code> <p>Type of angular adjustment.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with angular adjustments standard deviation mapping.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>def get_angular_adjustments_sd_mapping(\n    self,\n    cells_sdf: DataFrame,\n    beam_width_col: str,\n    signal_front_back_difference_col: str,\n    angular_adjustment_type: str,\n) -&gt; DataFrame:\n    \"\"\"\n    Calculates standard deviations in signal strength based on beam width and front-back cell signal difference.\n\n    Args:\n        cells_sdf (DataFrame): Input DataFrame.\n        beam_width_col (str): Column name for the beam width.\n        signal_front_back_difference_col (str): Column name for the signal front-back difference.\n        angular_adjustment_type (str): Type of angular adjustment.\n\n    Returns:\n        DataFrame: DataFrame with angular adjustments standard deviation mapping.\n    \"\"\"\n\n    sd_mappings = CellFootprintEstimation.get_sd_to_signal_back_loss_mappings(\n        cells_sdf, signal_front_back_difference_col\n    )\n    beam_widths_diff = (\n        cells_sdf.select(F.col(beam_width_col), F.col(signal_front_back_difference_col))\n        .where(F.col(ColNames.directionality) == 1)\n        .distinct()\n    )\n    beam_widths_diff = [row.asDict() for row in beam_widths_diff.collect()]\n\n    beam_sds = []\n    for item in beam_widths_diff:\n        item[f\"sd_{angular_adjustment_type}\"] = self.find_sd(\n            item[beam_width_col],\n            sd_mappings[sd_mappings[signal_front_back_difference_col] == item[signal_front_back_difference_col]],\n        )\n        beam_sds.append(item)\n\n    beam_sd_sdf = self.spark.createDataFrame(beam_sds)\n\n    return beam_sd_sdf\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.get_min3db","title":"<code>get_min3db(sd, db_back)</code>  <code>staticmethod</code>","text":"<p>Finds the angle at which the signal strength falls to 3 dB below its maximum value.</p> <p>Parameters:</p> Name Type Description Default <code>sd</code> <code>float</code> <p>The standard deviation of the normal distribution modeling the signal strength.</p> required <code>db_back</code> <code>float</code> <p>The difference in signal strength in dB between the front and back of the signal.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The angle at which the signal strength falls to 3 dB below its maximum value.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef get_min3db(sd: float, db_back: float) -&gt; float:\n    \"\"\"\n    Finds the angle at which the signal strength falls to 3 dB below its maximum value.\n\n    Args:\n        sd (float): The standard deviation of the normal distribution modeling the signal strength.\n        db_back (float): The difference in signal strength in dB between the front and back of the signal.\n\n    Returns:\n        float: The angle at which the signal strength falls to 3 dB below its maximum value.\n    \"\"\"\n    df = pd.DataFrame({\"a\": np.linspace(0, 180, 720)})\n    df[\"dbLoss\"] = CellFootprintEstimation.norm_dBloss(df[\"a\"], db_back=db_back, sd=sd)\n    return df.loc[np.abs(-3 - df[\"dbLoss\"]).idxmin(), \"a\"]\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.get_sd_to_signal_back_loss_mappings","title":"<code>get_sd_to_signal_back_loss_mappings(cells_sdf, signal_front_back_difference_col)</code>  <code>staticmethod</code>","text":"<p>Generates a DataFrame with mapping of signal strength standard deviation for each     elevation/azimuth angle degree.</p> <p>Parameters: cells_sdf (DataFrame): A Spark DataFrame containing information about the cells. signal_front_back_difference_col (str): The name of the column that contains the difference     in signal strength between the front and back of the cell.</p> <p>Returns: DataFrame: A pandas DataFrame with standard deviation mappings.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef get_sd_to_signal_back_loss_mappings(cells_sdf: DataFrame, signal_front_back_difference_col: str) -&gt; DataFrame:\n    \"\"\"\n    Generates a DataFrame with mapping of signal strength standard deviation for each\n        elevation/azimuth angle degree.\n\n    Parameters:\n    cells_sdf (DataFrame): A Spark DataFrame containing information about the cells.\n    signal_front_back_difference_col (str): The name of the column that contains the difference\n        in signal strength between\n    the front and back of the cell.\n\n    Returns:\n    DataFrame: A pandas DataFrame with standard deviation mappings.\n\n    \"\"\"\n    db_back_diffs = (\n        cells_sdf.select(F.col(signal_front_back_difference_col))\n        .where(F.col(ColNames.directionality) == 1)\n        .distinct()\n    )\n    db_back_diffs = [row.asDict()[signal_front_back_difference_col] for row in db_back_diffs.collect()]\n    mappings = [\n        CellFootprintEstimation.create_mapping(item, signal_front_back_difference_col) for item in db_back_diffs\n    ]\n\n    return pd.concat(mappings)\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.get_signal_dominance_threshold_point","title":"<code>get_signal_dominance_threshold_point(cells_sdf, grid_sdf, point_type)</code>","text":"<p>Calculates the signal dominance threshold points in cell maximum range.</p> <p>For omnidirectional cell types, the signal dominance threshold point is calculated as the furthest point along 90 degrees azimuth direction where signal dominance is less than the threshold. For directional cells types, two signal dominance threshold points are calculated:     1. the furthest point along the directionality angle direction where signal dominance is less than the threshold     2. the furthest point opposite to the directionality angle direction where signal dominance is less than the threshold</p> <p>Parameters:</p> Name Type Description Default <code>cells_sdf</code> <code>DataFrame</code> <p>The DataFrame containing cell information.</p> required <code>grid_sdf</code> <code>DataFrame</code> <p>The DataFrame containing grid information.</p> required <code>point_type</code> <code>str</code> <p>The type of point to calculate the signal dominance threshold for.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <p>The updated cells DataFrame with the signal dominance threshold point added.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>def get_signal_dominance_threshold_point(self, cells_sdf, grid_sdf, point_type):\n    \"\"\"\n    Calculates the signal dominance threshold points in cell maximum range.\n\n    For omnidirectional cell types, the signal dominance threshold point is calculated as\n    the furthest point along 90 degrees azimuth direction where signal dominance is less than the threshold.\n    For directional cells types, two signal dominance threshold points are calculated:\n        1. the furthest point along the directionality angle direction where signal dominance is less than the threshold\n        2. the furthest point opposite to the directionality angle direction where signal dominance is less than the threshold\n\n    Args:\n        cells_sdf (DataFrame): The DataFrame containing cell information.\n        grid_sdf (DataFrame): The DataFrame containing grid information.\n        point_type (str): The type of point to calculate the signal dominance threshold for.\n\n    Returns:\n        DataFrame: The updated cells DataFrame with the signal dominance threshold point added.\n    \"\"\"\n\n    do_azimuth_angle_adjustments = True\n    do_elevation_angle_adjustments = True\n\n    if point_type == \"directional_front\":\n        # Calculate range line for directional front point type\n        cells_sdf = cells_sdf.withColumn(\n            \"range_line\",\n            STF.ST_MakeLine(\n                \"geometry\",\n                STC.ST_Point(\n                    STF.ST_X(\"geometry\") + cells_sdf[\"range\"] * F.sin(F.radians(cells_sdf[\"azimuth_angle\"])),\n                    STF.ST_Y(\"geometry\") + cells_sdf[\"range\"] * F.cos(F.radians(cells_sdf[\"azimuth_angle\"])),\n                ),\n            ),\n        )\n\n    elif point_type == \"directional_back\":\n        # Calculate range line for directional back point type\n        cells_sdf = cells_sdf.withColumn(\n            \"range_line\",\n            STF.ST_MakeLine(\n                \"geometry\",\n                STC.ST_Point(\n                    STF.ST_X(\"geometry\") + cells_sdf[\"range\"] * F.sin(F.radians(cells_sdf[\"azimuth_angle\"] + 180)),\n                    STF.ST_Y(\"geometry\") + cells_sdf[\"range\"] * F.cos(F.radians(cells_sdf[\"azimuth_angle\"] + 180)),\n                ),\n            ),\n        )\n\n    elif point_type == \"omni\":\n        # Calculate range line for omni point type\n        cells_sdf = cells_sdf.withColumn(\n            \"range_line\",\n            STF.ST_MakeLine(\n                \"geometry\",\n                STC.ST_Point(\n                    STF.ST_X(\"geometry\") + cells_sdf[\"range\"] * F.sin(F.radians(F.lit(90))),\n                    STF.ST_Y(\"geometry\") + cells_sdf[\"range\"] * F.cos(F.radians(F.lit(90))),\n                ),\n            ),\n        )\n\n        do_azimuth_angle_adjustments = False\n        do_elevation_angle_adjustments = False\n\n    cells_sdf = cells_sdf.withColumn(\"coverage_range_line_buffer\", F.lit(self.coverage_range_line_buffer))\n\n    cell_grid = self.spatial_join_within_distance(cells_sdf, grid_sdf, \"range_line\", \"coverage_range_line_buffer\")\n\n    # calculate 3d distance and planar distance from cell id to grid ids\n    cell_grid = self.calculate_cartesian_distances(cell_grid)\n\n    cell_grid = self.calculate_signal_dominance(\n        cell_grid, do_elevation_angle_adjustments, do_azimuth_angle_adjustments\n    )\n\n    cell_grid_filtered = cell_grid.filter(F.col(ColNames.signal_dominance) &gt; self.signal_dominance_treshold)\n    cell_counts = cell_grid_filtered.groupBy(\"cell_id\").count().withColumnRenamed(\"count\", \"filtered_count\")\n    cell_grid_joined = cell_grid.join(cell_counts, on=\"cell_id\", how=\"left\").fillna(0, subset=[\"filtered_count\"])\n\n    # In case if desired threshold filter removes all grid tiles in given direction, keep closest grid tile\n    # Otherwise keep furthest point where threshold condition meet\n    window_min = Window.partitionBy(\"cell_id\").orderBy(F.col(\"distance_to_cell\"))\n    window_max = Window.partitionBy(\"cell_id\").orderBy(F.desc(F.col(\"distance_to_cell\")))\n\n    cell_grid_max = (\n        cell_grid_joined.withColumn(\n            \"rank\",\n            F.when(F.col(\"filtered_count\") == 0, F.row_number().over(window_min)).otherwise(\n                F.row_number().over(window_max)\n            ),\n        )\n        .filter(F.col(\"rank\") == 1)\n        .drop(\"rank\", \"filtered_count\")\n    )\n\n    cells_sdf = cells_sdf.join(\n        cell_grid_max.select(\"cell_id\", \"joined_geometry\").withColumnRenamed(\"joined_geometry\", point_type),\n        \"cell_id\",\n        \"left\",\n    )\n\n    return cells_sdf.drop(\n        \"range_line\", \"distance_to_cell_3d\", \"distance_to_cell\", \"signal_strength\", \"signal_dominance\", \"grid_id\"\n    )\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.impute_default_cell_properties","title":"<code>impute_default_cell_properties(sdf)</code>","text":"<p>Imputes default cell properties for null values in the input DataFrame using default properties for cell types from config.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with imputed default cell properties.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>def impute_default_cell_properties(self, sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Imputes default cell properties for null values in the input DataFrame using\n    default properties for cell types from config.\n\n    Args:\n        sdf (DataFrame): Input DataFrame.\n\n    Returns:\n        DataFrame: DataFrame with imputed default cell properties.\n    \"\"\"\n    default_properties_df = self.create_default_properties_df()\n\n    # add default prefix to the columns of default_properties_df\n    default_properties_df = default_properties_df.select(\n        [F.col(col).alias(f\"default_{col}\") for col in default_properties_df.columns]\n    )\n\n    # assign default cell type to cell types not present in config\n    sdf = sdf.withColumn(\n        ColNames.cell_type,\n        F.when(\n            F.col(ColNames.cell_type).isin(list(self.default_cell_properties.keys())),\n            F.col(ColNames.cell_type),\n        ).otherwise(\"default\"),\n    )\n\n    # all cell types which are absent from the default_properties_df will be assigned default values\n    sdf = sdf.join(\n        default_properties_df,\n        sdf[ColNames.cell_type] == default_properties_df[f\"default_{ColNames.cell_type}\"],\n        how=\"inner\",\n    )\n    # if orignal column is null, assign the default value\n    for col in default_properties_df.columns:\n        col = col.replace(\"default_\", \"\")\n        if col not in sdf.columns:\n            sdf = sdf.withColumn(col, F.lit(None))\n        sdf = sdf.withColumn(col, F.coalesce(F.col(col), F.col(f\"default_{col}\")))\n\n    return sdf.drop(*default_properties_df.columns)\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.join_sd_mapping","title":"<code>join_sd_mapping(sdf, sd_mapping_sdf, beam_width_col, signal_front_back_difference_col, sd_col)</code>  <code>staticmethod</code>","text":"<p>Joins DataFrame with standard deviation mapping.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>sd_mapping_sdf</code> <code>DataFrame</code> <p>DataFrame with standard deviation mapping.</p> required <code>beam_width_col</code> <code>str</code> <p>Column name for the beam width.</p> required <code>signal_front_back_difference_col</code> <code>str</code> <p>Column name for the signal front-back difference.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame after joining with standard deviation mapping.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef join_sd_mapping(\n    sdf: DataFrame,\n    sd_mapping_sdf: DataFrame,\n    beam_width_col: str,\n    signal_front_back_difference_col: str,\n    sd_col: str,\n) -&gt; DataFrame:\n    \"\"\"\n    Joins DataFrame with standard deviation mapping.\n\n    Args:\n        sdf (DataFrame): Input DataFrame.\n        sd_mapping_sdf (DataFrame): DataFrame with standard deviation mapping.\n        beam_width_col (str): Column name for the beam width.\n        signal_front_back_difference_col (str): Column name for the signal front-back difference.\n\n    Returns:\n        DataFrame: DataFrame after joining with standard deviation mapping.\n    \"\"\"\n\n    join_condition = (F.col(f\"a.{beam_width_col}\") == F.col(f\"b.{beam_width_col}\")) &amp; (\n        F.col(f\"a.{signal_front_back_difference_col}\") == F.col(f\"b.{signal_front_back_difference_col}\")\n    )\n\n    sdf = sdf.alias(\"a\").join(sd_mapping_sdf.alias(\"b\"), join_condition).select(f\"a.*\", f\"b.{sd_col}\")\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.norm_dBloss","title":"<code>norm_dBloss(a, sd, db_back)</code>  <code>staticmethod</code>","text":"<p>Computes the loss in signal strength in dB as a function of angle from the direction of maximum signal strength.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>The angle from the direction of maximum signal strength.</p> required <code>sd</code> <code>float</code> <p>The standard deviation of the normal distribution modeling the signal strength.</p> required <code>db_back</code> <code>float</code> <p>The difference in signal strength in dB between the front and back of the signal.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The loss in signal strength in dB at the given angle.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef norm_dBloss(a: float, sd: float, db_back: float) -&gt; float:\n    \"\"\"\n    Computes the loss in signal strength in dB as a function of\n    angle from the direction of maximum signal strength.\n\n    Args:\n        a (float): The angle from the direction of maximum signal strength.\n        sd (float): The standard deviation of the normal distribution modeling the signal strength.\n        db_back (float): The difference in signal strength in dB between the front and back of the signal.\n\n    Returns:\n        float: The loss in signal strength in dB at the given angle.\n    \"\"\"\n    a = ((a + 180) % 360) - 180\n    inflate = -db_back / (\n        CellFootprintEstimation.normal_distribution(0, 0, sd)\n        - CellFootprintEstimation.normal_distribution(180, 0, sd)\n    )\n    return (\n        CellFootprintEstimation.normal_distribution(a, 0, sd)\n        - CellFootprintEstimation.normal_distribution(0, 0, sd)\n    ) * inflate\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.norm_dBloss_spark","title":"<code>norm_dBloss_spark(sdf, angle_col, sd_col, db_back_col)</code>  <code>staticmethod</code>","text":"<p>Normalizes the dB loss in a Spark DataFrame based on the angle, standard deviation, and dB back loss.</p> <p>This method performs several operations to normalize the dB loss for each row in the Spark DataFrame: 1. Normalizes the angle to a range of [-180, 180) degrees. 2. Calculates the normal distribution of the normalized angles with a mean of 0 and a standard deviation specified by <code>sd_col</code>. 3. Calculates the normal distribution for angles 0 and 180 degrees using precomputed values, with a mean of 0 and the same standard deviation. 4. Computes an inflation factor based on the dB back loss column and the difference between the normal distributions at 0 and 180 degrees. 5. Calculates the normalized dB loss by adjusting the normal distribution of the angle with the inflation factor.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The input Spark DataFrame containing the data.</p> required <code>angle_col</code> <code>str</code> <p>The name of the column that contains the angles to be normalized.</p> required <code>sd_col</code> <code>str</code> <p>The name of the column that contains the standard deviation values for the normal distribution calculation.</p> required <code>db_back_col</code> <code>str</code> <p>The name of the column in that contains the dB back loss values used to calculate the inflation factor.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A Spark DataFrame with the normalized dB loss added and intermediate columns removed.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef norm_dBloss_spark(sdf: DataFrame, angle_col: str, sd_col: str, db_back_col: str) -&gt; DataFrame:\n    \"\"\"\n    Normalizes the dB loss in a Spark DataFrame based on the angle, standard deviation, and dB back loss.\n\n    This method performs several operations to normalize the dB loss for each row in the Spark DataFrame:\n    1. Normalizes the angle to a range of [-180, 180) degrees.\n    2. Calculates the normal distribution of the normalized angles with a mean of 0 and a standard deviation specified by `sd_col`.\n    3. Calculates the normal distribution for angles 0 and 180 degrees using precomputed values, with a mean of 0 and the same standard deviation.\n    4. Computes an inflation factor based on the dB back loss column and the difference between the normal distributions at 0 and 180 degrees.\n    5. Calculates the normalized dB loss by adjusting the normal distribution of the angle with the inflation factor.\n\n    Parameters:\n        sdf (DataFrame): The input Spark DataFrame containing the data.\n        angle_col (str): The name of the column that contains the angles to be normalized.\n        sd_col (str): The name of the column that contains the standard deviation values for the normal distribution calculation.\n        db_back_col (str): The name of the column in that contains the dB back loss values used to calculate the inflation factor.\n\n    Returns:\n        DataFrame: A Spark DataFrame with the normalized dB loss added and intermediate columns removed.\n    \"\"\"\n    # Normalizing angles\n\n    sdf = sdf.withColumn(\"angle_normalized\", ((F.col(angle_col) + 180) % 360) - 180)\n\n    # Calculate the normal distribution for the normalized angles\n    sdf = sdf.withColumn(\n        \"norm_dist_angle\",\n        CellFootprintEstimation.normal_distribution_col(F.col(\"angle_normalized\"), F.lit(0), F.col(sd_col)),\n    )\n\n    # Calculate the normal distribution for 0 and 180 degrees using precomputed values\n    n_dist_0 = CellFootprintEstimation.normal_distribution_col(F.lit(0), F.lit(0), F.col(sd_col))\n    n_dist_180 = CellFootprintEstimation.normal_distribution_col(F.lit(180), F.lit(0), F.col(sd_col))\n\n    # Calculate the inflated factor\n    sdf = sdf.withColumn(\"inflate\", -F.col(db_back_col) / (n_dist_0 - n_dist_180))\n\n    # Calculate the normalized dB loss\n    sdf = sdf.withColumn(\"normalized_dBloss\", (F.col(\"norm_dist_angle\") - n_dist_0) * F.col(\"inflate\"))\n\n    return sdf.drop(\"angle_normalized\", \"norm_dist_angle\", \"inflate\")\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.normal_distribution","title":"<code>normal_distribution(x, mean, sd)</code>  <code>staticmethod</code>","text":"<p>Computes the value of the normal distribution with the given mean and standard deviation at the given point.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float</code> <p>The point at which to evaluate the normal distribution.</p> required <code>mean</code> <code>float</code> <p>The mean of the normal distribution.</p> required <code>sd</code> <code>float</code> <p>The standard deviation of the normal distribution.</p> required <code>return_type</code> <code>str</code> <p>The desired return type, either 'np_array' or 'list'.</p> required <p>Returns:</p> Type Description <code>Union[array, list]</code> <p>np.array or list: The value of the normal distribution at the given point,</p> <code>Union[array, list]</code> <p>returned as either a numpy array or a list.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef normal_distribution(x: float, mean: float, sd: float) -&gt; Union[np.array, list]:\n    \"\"\"\n    Computes the value of the normal distribution with the given mean\n    and standard deviation at the given point.\n\n    Args:\n        x (float): The point at which to evaluate the normal distribution.\n        mean (float): The mean of the normal distribution.\n        sd (float): The standard deviation of the normal distribution.\n        return_type (str): The desired return type, either 'np_array' or 'list'.\n\n    Returns:\n        np.array or list: The value of the normal distribution at the given point,\n        returned as either a numpy array or a list.\n    \"\"\"\n    n_dist = (1.0 / (sqrt(2.0 * pi) * sd)) * np.exp(-0.5 * ((x - mean) / sd) ** 2)\n\n    return n_dist\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.normal_distribution_col","title":"<code>normal_distribution_col(x_col, mean_col, sd_col)</code>  <code>staticmethod</code>","text":"<p>Computes the value of the normal distribution for each row in a DataFrame based on the provided columns for the point, mean, and standard deviation.</p> <p>This function applies the normal distribution formula to each row of the DataFrame using the specified columns for the point (x), mean, and standard deviation (sd). The normal distribution formula used is:</p> <pre><code>f(x) = (1 / (sqrt(2 * pi) * sd)) * exp(-0.5 * ((x - mean) / sd)^2)\n</code></pre> <p>where <code>x</code> is the value at which the normal distribution is evaluated, <code>mean</code> is the mean of the distribution, and <code>sd</code> is the standard deviation.</p> <p>Parameters:</p> Name Type Description Default <code>x_col</code> <code>Column</code> <p>A Spark DataFrame column representing the point at which to evaluate the normal distribution.</p> required <code>mean_col</code> <code>Column</code> <p>A Spark DataFrame column representing the mean of the normal distribution.</p> required <code>sd_col</code> <code>Column</code> <p>A Spark DataFrame column representing the standard deviation of the normal distribution.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A Spark DataFrame column with the computed normal distribution values for each row.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef normal_distribution_col(x_col: Column, mean_col: Column, sd_col: Column) -&gt; Column:\n    \"\"\"\n    Computes the value of the normal distribution for each row in a DataFrame based on\n    the provided columns for the point, mean, and standard deviation.\n\n    This function applies the normal distribution formula to each row of the DataFrame using\n    the specified columns for the point (x), mean, and standard deviation (sd).\n    The normal distribution formula used is:\n\n        f(x) = (1 / (sqrt(2 * pi) * sd)) * exp(-0.5 * ((x - mean) / sd)^2)\n\n    where `x` is the value at which the normal distribution is evaluated,\n    `mean` is the mean of the distribution, and `sd` is the standard deviation.\n\n    Parameters:\n        x_col (Column): A Spark DataFrame column representing the point at which to evaluate the normal distribution.\n        mean_col (Column): A Spark DataFrame column representing the mean of the normal distribution.\n        sd_col (Column): A Spark DataFrame column representing the standard deviation of the normal distribution.\n\n    Returns:\n        Column: A Spark DataFrame column with the computed normal distribution values for each row.\n    \"\"\"\n    return (1.0 / (F.sqrt(2.0 * F.lit(pi)) * sd_col)) * F.exp(-0.5 * ((x_col - mean_col) / sd_col) ** 2)\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.prune_max_cells_per_grid_tile","title":"<code>prune_max_cells_per_grid_tile(sdf, max_cells_per_grid_tile)</code>  <code>staticmethod</code>","text":"<p>Prunes rows from the DataFrame exceeding the maximum number of cells allowed per grid tile.</p> <p>The rows are ordered by signal dominance in descending order, and only the top 'max_cells_per_grid_tile' rows are kept for each grid tile.</p> <p>Parameters: sdf (DataFrame): A Spark DataFrame containing the signal dominance data. max_cells_per_grid_tile (int): The maximum number of cells allowed per grid tile.</p> <p>Returns: DataFrame: A DataFrame with rows exceeding the maximum number of cells per grid tile removed.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef prune_max_cells_per_grid_tile(sdf: DataFrame, max_cells_per_grid_tile: int) -&gt; DataFrame:\n    \"\"\"\n    Prunes rows from the DataFrame exceeding the maximum number of cells allowed per grid tile.\n\n    The rows are ordered by signal dominance in descending order,\n    and only the top 'max_cells_per_grid_tile' rows are kept for each grid tile.\n\n    Parameters:\n    sdf (DataFrame): A Spark DataFrame containing the signal dominance data.\n    max_cells_per_grid_tile (int): The maximum number of cells allowed per grid tile.\n\n    Returns:\n    DataFrame: A DataFrame with rows exceeding the maximum number of cells per grid tile removed.\n    \"\"\"\n    window = Window.partitionBy(ColNames.grid_id).orderBy(F.desc(ColNames.signal_dominance))\n\n    sdf = sdf.withColumn(\"row_number\", F.row_number().over(window))\n    sdf = sdf.filter(F.col(\"row_number\") &lt;= max_cells_per_grid_tile)\n    sdf = sdf.drop(\"row_number\")\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.prune_signal_difference_from_best","title":"<code>prune_signal_difference_from_best(sdf, difference_threshold)</code>  <code>staticmethod</code>","text":"<p>Prunes rows from the DataFrame based on a threshold of signal dominance difference.</p> <p>The rows are ordered by signal dominance in descending order, and only the rows where the difference in signal dominance from the maximum is less than the threshold are kept for each grid tile.</p> <p>Parameters: sdf (DataFrame): A Spark DataFrame containing the signal dominance data. threshold (float): The threshold for signal dominance difference in percentage.</p> <p>Returns: DataFrame: A DataFrame with rows pruned based on the signal dominance difference threshold.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef prune_signal_difference_from_best(sdf: DataFrame, difference_threshold: float) -&gt; DataFrame:\n    \"\"\"\n    Prunes rows from the DataFrame based on a threshold of signal dominance difference.\n\n    The rows are ordered by signal dominance in descending order, and only the rows where the difference\n    in signal dominance from the maximum is less than the threshold are kept for each grid tile.\n\n    Parameters:\n    sdf (DataFrame): A Spark DataFrame containing the signal dominance data.\n    threshold (float): The threshold for signal dominance difference in percentage.\n\n    Returns:\n    DataFrame: A DataFrame with rows pruned based on the signal dominance difference threshold.\n    \"\"\"\n    window = Window.partitionBy(ColNames.grid_id).orderBy(F.desc(ColNames.signal_dominance))\n\n    sdf = sdf.withColumn(\"row_number\", F.row_number().over(window))\n\n    sdf = sdf.withColumn(\"max_signal_dominance\", F.first(ColNames.signal_dominance).over(window))\n    sdf = sdf.withColumn(\n        \"signal_dominance_diff_percentage\",\n        (sdf[\"max_signal_dominance\"] - sdf[ColNames.signal_dominance]) / sdf[\"max_signal_dominance\"] * 100,\n    )\n\n    sdf = sdf.filter(\n        (F.col(\"row_number\") == 1) | (F.col(\"signal_dominance_diff_percentage\") &lt;= difference_threshold)\n    )\n\n    sdf = sdf.drop(\"row_number\", \"max_signal_dominance\", \"signal_dominance_diff_percentage\")\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.prune_small_signal_dominance","title":"<code>prune_small_signal_dominance(sdf, signal_dominance_threshold)</code>  <code>staticmethod</code>","text":"<p>Prunes rows from the DataFrame where the signal dominance is less than or equal to the provided threshold.</p> <p>Parameters: sdf (DataFrame): A Spark DataFrame containing the signal dominance data. signal_dominance_threshold (float): The threshold for pruning small signal dominance values.</p> <p>Returns: DataFrame: A DataFrame with rows having signal dominance less than or equal to the threshold removed.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef prune_small_signal_dominance(sdf: DataFrame, signal_dominance_threshold: float) -&gt; DataFrame:\n    \"\"\"\n    Prunes rows from the DataFrame where the signal dominance is less than or equal to the provided threshold.\n\n    Parameters:\n    sdf (DataFrame): A Spark DataFrame containing the signal dominance data.\n    signal_dominance_threshold (float): The threshold for pruning small signal dominance values.\n\n    Returns:\n    DataFrame: A DataFrame with rows having signal dominance less than or equal to the threshold removed.\n    \"\"\"\n    sdf = sdf.filter(F.col(ColNames.signal_dominance) &gt; signal_dominance_threshold)\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.signal_strength_to_signal_dominance","title":"<code>signal_strength_to_signal_dominance(sdf, logistic_function_steepness, logistic_function_midpoint)</code>  <code>staticmethod</code>","text":"<p>Converts signal strength to signal dominance using a logistic function. Methodology from A Bayesian approach to location estimation of mobile devices from mobile network operator data. Tennekes and Gootzen (2022).</p> <p>The logistic function is defined as 1 / (1 + exp(-scale)), where scale is (signal_strength - logistic_function_midpoint) * logistic_function_steepness.</p> <p>Parameters: sdf (DataFrame): SignalStrenghtDataObject Spark DataFrame. logistic_function_steepness (float): The steepness parameter for the logistic function. logistic_function_midpoint (float): The midpoint parameter for the logistic function.</p> <p>Returns: DataFrame: A Spark DataFrame with the signal dominance added as a new column.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef signal_strength_to_signal_dominance(\n    sdf: DataFrame,\n    logistic_function_steepness: float,\n    logistic_function_midpoint: float,\n) -&gt; DataFrame:\n    \"\"\"\n    Converts signal strength to signal dominance using a logistic function.\n    Methodology from A Bayesian approach to location estimation of mobile devices\n    from mobile network operator data. Tennekes and Gootzen (2022).\n\n    The logistic function is defined as 1 / (1 + exp(-scale)),\n    where scale is (signal_strength - logistic_function_midpoint) * logistic_function_steepness.\n\n    Parameters:\n    sdf (DataFrame): SignalStrenghtDataObject Spark DataFrame.\n    logistic_function_steepness (float): The steepness parameter for the logistic function.\n    logistic_function_midpoint (float): The midpoint parameter for the logistic function.\n\n    Returns:\n    DataFrame: A Spark DataFrame with the signal dominance added as a new column.\n    \"\"\"\n    sdf = sdf.withColumn(\n        \"scale\",\n        (F.col(ColNames.signal_strength) - logistic_function_midpoint) * logistic_function_steepness,\n    )\n    sdf = sdf.withColumn(ColNames.signal_dominance, 1 / (1 + F.exp(-F.col(\"scale\"))))\n    sdf = sdf.drop(\"scale\")\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.spatial_join_within_distance","title":"<code>spatial_join_within_distance(sdf_from, sdf_to, geometry_col, within_distance_col)</code>  <code>staticmethod</code>","text":"<p>Performs a spatial join within a specified distance.</p> <p>Parameters:</p> Name Type Description Default <code>sdf_from</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>sdf_to</code> <code>DataFrame</code> <p>DataFrame to join with.</p> required <code>within_distance_col</code> <code>str</code> <p>Column name for the within distance.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame after performing the spatial join.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef spatial_join_within_distance(\n    sdf_from: DataFrame, sdf_to: DataFrame, geometry_col: str, within_distance_col: str\n) -&gt; DataFrame:\n    \"\"\"\n    Performs a spatial join within a specified distance.\n\n    Args:\n        sdf_from (DataFrame): Input DataFrame.\n        sdf_to (DataFrame): DataFrame to join with.\n        within_distance_col (str): Column name for the within distance.\n\n    Returns:\n        DataFrame: DataFrame after performing the spatial join.\n    \"\"\"\n\n    sdf_merged = (\n        sdf_from.alias(\"a\")\n        .join(\n            sdf_to.alias(\"b\"),\n            STP.ST_Intersects(\n                STF.ST_Buffer(f\"a.{geometry_col}\", f\"a.{within_distance_col}\"),\n                f\"b.{ColNames.joined_geometry}\",\n            ),\n        )\n        .drop(f\"a.{within_distance_col}\")\n    )\n\n    return sdf_merged\n</code></pre>"},{"location":"reference/components/execution/cell_footprint/cell_footprint_estimation/#components.execution.cell_footprint.cell_footprint_estimation.CellFootprintEstimation.watt_to_dbm","title":"<code>watt_to_dbm(sdf)</code>  <code>staticmethod</code>","text":"<p>Converts power from watt to dBm.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with power converted to dBm.</p> Source code in <code>multimno/components/execution/cell_footprint/cell_footprint_estimation.py</code> <pre><code>@staticmethod\ndef watt_to_dbm(sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Converts power from watt to dBm.\n\n    Args:\n        sdf (DataFrame): Input DataFrame.\n\n    Returns:\n        DataFrame: DataFrame with power converted to dBm.\n    \"\"\"\n    return sdf.withColumn(ColNames.power, 10.0 * F.log10(F.col(ColNames.power)) + 30.0)\n</code></pre>"},{"location":"reference/components/execution/cell_proximity_estimation/","title":"cell_proximity_estimation","text":""},{"location":"reference/components/execution/cell_proximity_estimation/cell_proximity_estimation/","title":"cell_proximity_estimation","text":"<p>Module for calculating which cells have overlapping coverage areas, as well calculating distances between coverage areas.</p>"},{"location":"reference/components/execution/cell_proximity_estimation/cell_proximity_estimation/#components.execution.cell_proximity_estimation.cell_proximity_estimation.CellProximityEstimation","title":"<code>CellProximityEstimation</code>","text":"<p>               Bases: <code>Component</code></p> Source code in <code>multimno/components/execution/cell_proximity_estimation/cell_proximity_estimation.py</code> <pre><code>class CellProximityEstimation(Component):\n    COMPONENT_ID = \"CellProximityEstimation\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.max_nearby_cell_distance_m = self.config.getfloat(self.COMPONENT_ID, \"max_nearby_cell_distance_m\")\n        self.footprint_buffer_distance_m = self.config.getfloat(self.COMPONENT_ID, \"footprint_buffer_distance_m\")\n        self.n_output_partitions = self.config.getint(self.COMPONENT_ID, \"n_output_partitions\")\n        self.data_period_start = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n        self.data_period_end = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n\n        self.data_period_dates = [\n            self.data_period_start + timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n        self.grid_gen = InspireGridGenerator(self.spark)\n\n        self.current_date = None\n        self.prev_date = None\n\n    def initalize_data_objects(self):\n        # Input paths\n        input_cell_footprint_silver_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"cell_footprint_data_silver\")\n        # Output paths\n        output_cell_intersection_groups_silver_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"cell_intersection_groups_data_silver\"\n        )\n        output_cell_distance_silver_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"cell_distance_data_silver\")\n        clear_destination_directory = self.config.getboolean(self.COMPONENT_ID, \"clear_destination_directory\")\n\n        # Input data objects\n        silver_cell_footprint = SilverCellFootprintDataObject(self.spark, input_cell_footprint_silver_path)\n        self.input_data_objects = {SilverCellFootprintDataObject.ID: silver_cell_footprint}\n        # Output data objects\n        silver_cell_intersection_groups = SilverCellIntersectionGroupsDataObject(\n            self.spark, output_cell_intersection_groups_silver_path\n        )\n        silver_cell_distance = SilverCellDistanceDataObject(self.spark, output_cell_distance_silver_path)\n        self.output_data_objects = {\n            SilverCellIntersectionGroupsDataObject.ID: silver_cell_intersection_groups,\n            SilverCellDistanceDataObject.ID: silver_cell_distance,\n        }\n\n        if clear_destination_directory:\n            delete_file_or_folder(self.spark, output_cell_intersection_groups_silver_path)\n            delete_file_or_folder(self.spark, output_cell_distance_silver_path)\n\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n        for current_date in self.data_period_dates:\n            self.current_date = current_date\n            self.transform()\n            self.write()\n            self.prev_date = current_date\n            self.spark.catalog.clearCache()\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        current_date = self.current_date\n        self.logger.info(f\"Starting Transform method of {self.COMPONENT_ID} for date {current_date} ...\")\n        # Get cell grid_ids for current date.\n        cell_grid_ids_df = (\n            self.input_data_objects[SilverCellFootprintDataObject.ID]\n            .df.filter(\n                (F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(current_date))\n            )\n            .select(ColNames.cell_id, ColNames.grid_id)\n        )\n        # Shortcut to next date if no data is present.\n        if cell_grid_ids_df.isEmpty():\n            self.logger.info(f\"No input data for date {current_date}.\")\n            self.output_data_objects[SilverCellIntersectionGroupsDataObject.ID].df = self.output_data_objects[\n                SilverCellIntersectionGroupsDataObject.ID\n            ].df.limit(0)\n            self.output_data_objects[SilverCellDistanceDataObject.ID].df = self.output_data_objects[\n                SilverCellDistanceDataObject.ID\n            ].df.limit(0)\n            return\n\n        # Aggregate and sort list of grid ids per cell.\n        df = cell_grid_ids_df.groupBy(ColNames.cell_id).agg(\n            F.array_sort(F.collect_list(ColNames.grid_id)).alias(\"grid_id_list\")\n        )\n        # For each cell, calculate the geometry from grid (concave hull of grid centroids, with buffer zone).\n        df_with_geom = self.calculate_cell_geometry_with_buffer(df)\n        # Repartition to avoid exploding partition count on following self join.\n        # Note: using a partition count other than the output partition count may be desirable.\n        df_with_geom = df_with_geom.repartition(self.n_output_partitions, ColNames.cell_id)\n\n        # Perform dataframe self join to get cell pairs where distance is below max distance threshold.\n        df = self.calculate_nearby_cell_pairs(df_with_geom)\n        # Zero distance between two geoms indicates an overlap.\n        # We currently do not account for the amount of overlap, simply its existence.\n        df = df.withColumn(\"is_overlap\", F.col(ColNames.distance) &lt;= 0.0)\n        df = df.cache()\n\n        # Collect overlapping cell ids for output\n        cell_intersection_groups_df = self.create_cell_intersection_groups_dataframe(df, current_date=current_date)\n        cell_intersection_groups_df = cell_intersection_groups_df.repartition(\n            self.n_output_partitions, ColNames.cell_id\n        )\n        self.output_data_objects[SilverCellIntersectionGroupsDataObject.ID].df = cell_intersection_groups_df\n\n        # Collect cell distances for output\n        cell_distance_df = self.create_cell_distance_dataframe(df, current_date=current_date)\n        cell_distance_df = cell_distance_df.repartition(self.n_output_partitions, ColNames.cell_id_a)\n        self.output_data_objects[SilverCellDistanceDataObject.ID].df = cell_distance_df\n        return\n\n    def create_cell_distance_dataframe(self, df: DataFrame, current_date) -&gt; DataFrame:\n        \"\"\"Creates dataframe matching the cell distance data object structure.\n\n        Args:\n            df (DataFrame): (cell_id_a, cell_id_b, distance)\n            current_date: date of data\n\n        Returns:\n            DataFrame: (cell_id_a ,cell_id_b, distance, year, month, day)\n        \"\"\"\n        cell_distance_df = df.select(\n            ColNames.cell_id_a,\n            ColNames.cell_id_b,\n            F.col(ColNames.distance).cast(FloatType()),\n            F.lit(current_date.year).cast(ShortType()).alias(ColNames.year),\n            F.lit(current_date.month).cast(ByteType()).alias(ColNames.month),\n            F.lit(current_date.day).cast(ByteType()).alias(ColNames.day),\n        ).where(F.col(ColNames.cell_id_a) != F.col(ColNames.cell_id_b))\n\n        return cell_distance_df\n\n    def create_cell_intersection_groups_dataframe(self, df: DataFrame, current_date) -&gt; DataFrame:\n        \"\"\"Creates dataframe matching the cell intersection groups data object structure.\n\n        Args:\n            df (DataFrame): (cell_id_a, cell_id_b, is_overlap)\n            current_date: date of data\n\n        Returns:\n            DataFrame: (cell_id_a, overlapping_cell_ids, year, month, day)\n        \"\"\"\n        cell_intersection_groups_df = (\n            df.where(\"is_overlap\")\n            .groupBy(ColNames.cell_id_a)\n            .agg(F.array_sort(F.collect_list(ColNames.cell_id_b)).alias(ColNames.overlapping_cell_ids))\n        )\n        cell_intersection_groups_df = cell_intersection_groups_df.withColumnRenamed(\n            ColNames.cell_id_a, ColNames.cell_id\n        )\n        cell_intersection_groups_df = cell_intersection_groups_df.select(\n            ColNames.cell_id,\n            ColNames.overlapping_cell_ids,\n            F.lit(current_date.year).cast(ShortType()).alias(ColNames.year),\n            F.lit(current_date.month).cast(ByteType()).alias(ColNames.month),\n            F.lit(current_date.day).cast(ByteType()).alias(ColNames.day),\n        )\n        return cell_intersection_groups_df\n\n    def calculate_nearby_cell_pairs(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Calculates pairs of cells in the dataframe which are within parameter-defined distance of each other.\n        Pairs between the same cell_id have the cell_id_b set to None.\n        Args:\n            df (DataFrame): (cell_id, geometry)\n\n        Returns:\n            DataFrame: (cell_id_a, cell_id_b, distance)\n        \"\"\"\n        df = (\n            df.alias(\"df1\")\n            .join(\n                df.alias(\"df2\"),\n                on=STP.ST_DWithin(\n                    F.col(f\"df1.{ColNames.geometry}\"),\n                    F.col(f\"df2.{ColNames.geometry}\"),\n                    distance=self.max_nearby_cell_distance_m,\n                ),\n            )\n            .select(\n                F.col(f\"df1.{ColNames.cell_id}\").alias(ColNames.cell_id_a),\n                F.when((F.col(f\"df1.{ColNames.cell_id}\") == F.col(f\"df2.{ColNames.cell_id}\")), F.lit(None))\n                .otherwise(F.col(f\"df2.{ColNames.cell_id}\"))\n                .alias(ColNames.cell_id_b),\n                STF.ST_Distance(F.col(f\"df1.{ColNames.geometry}\"), F.col(f\"df2.{ColNames.geometry}\")).alias(\n                    ColNames.distance\n                ),\n            )\n        )\n        return df\n\n    def calculate_cell_geometry_with_buffer(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Calculates cell coverage geometries for cell rows, including added buffer zone.\n        Returns (cell_id, geometry) for each row.\n\n        Args:\n            df (DataFrame): (cell_id, is_new_geometry, grid_id_list)\n\n        Returns:\n            DataFrame: (cell_id, geometry)\n        \"\"\"\n        df_with_geom = df.select(ColNames.cell_id, F.explode(\"grid_id_list\").alias(ColNames.grid_id))\n        df_with_geom = self.grid_gen.grid_ids_to_centroids(df_with_geom)\n        df_with_geom = df_with_geom.groupBy([ColNames.cell_id]).agg(\n            STF.ST_ConcaveHull(STF.ST_Collect(F.collect_list(ColNames.geometry)), 0.5).alias(ColNames.geometry)\n        )\n        # Add buffer\n        df_with_geom = df_with_geom.withColumn(\n            ColNames.geometry, STF.ST_Buffer(ColNames.geometry, self.footprint_buffer_distance_m)\n        )\n\n        return df_with_geom\n</code></pre>"},{"location":"reference/components/execution/cell_proximity_estimation/cell_proximity_estimation/#components.execution.cell_proximity_estimation.cell_proximity_estimation.CellProximityEstimation.calculate_cell_geometry_with_buffer","title":"<code>calculate_cell_geometry_with_buffer(df)</code>","text":"<p>Calculates cell coverage geometries for cell rows, including added buffer zone. Returns (cell_id, geometry) for each row.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>(cell_id, is_new_geometry, grid_id_list)</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>(cell_id, geometry)</p> Source code in <code>multimno/components/execution/cell_proximity_estimation/cell_proximity_estimation.py</code> <pre><code>def calculate_cell_geometry_with_buffer(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Calculates cell coverage geometries for cell rows, including added buffer zone.\n    Returns (cell_id, geometry) for each row.\n\n    Args:\n        df (DataFrame): (cell_id, is_new_geometry, grid_id_list)\n\n    Returns:\n        DataFrame: (cell_id, geometry)\n    \"\"\"\n    df_with_geom = df.select(ColNames.cell_id, F.explode(\"grid_id_list\").alias(ColNames.grid_id))\n    df_with_geom = self.grid_gen.grid_ids_to_centroids(df_with_geom)\n    df_with_geom = df_with_geom.groupBy([ColNames.cell_id]).agg(\n        STF.ST_ConcaveHull(STF.ST_Collect(F.collect_list(ColNames.geometry)), 0.5).alias(ColNames.geometry)\n    )\n    # Add buffer\n    df_with_geom = df_with_geom.withColumn(\n        ColNames.geometry, STF.ST_Buffer(ColNames.geometry, self.footprint_buffer_distance_m)\n    )\n\n    return df_with_geom\n</code></pre>"},{"location":"reference/components/execution/cell_proximity_estimation/cell_proximity_estimation/#components.execution.cell_proximity_estimation.cell_proximity_estimation.CellProximityEstimation.calculate_nearby_cell_pairs","title":"<code>calculate_nearby_cell_pairs(df)</code>","text":"<p>Calculates pairs of cells in the dataframe which are within parameter-defined distance of each other. Pairs between the same cell_id have the cell_id_b set to None. Args:     df (DataFrame): (cell_id, geometry)</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>(cell_id_a, cell_id_b, distance)</p> Source code in <code>multimno/components/execution/cell_proximity_estimation/cell_proximity_estimation.py</code> <pre><code>def calculate_nearby_cell_pairs(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Calculates pairs of cells in the dataframe which are within parameter-defined distance of each other.\n    Pairs between the same cell_id have the cell_id_b set to None.\n    Args:\n        df (DataFrame): (cell_id, geometry)\n\n    Returns:\n        DataFrame: (cell_id_a, cell_id_b, distance)\n    \"\"\"\n    df = (\n        df.alias(\"df1\")\n        .join(\n            df.alias(\"df2\"),\n            on=STP.ST_DWithin(\n                F.col(f\"df1.{ColNames.geometry}\"),\n                F.col(f\"df2.{ColNames.geometry}\"),\n                distance=self.max_nearby_cell_distance_m,\n            ),\n        )\n        .select(\n            F.col(f\"df1.{ColNames.cell_id}\").alias(ColNames.cell_id_a),\n            F.when((F.col(f\"df1.{ColNames.cell_id}\") == F.col(f\"df2.{ColNames.cell_id}\")), F.lit(None))\n            .otherwise(F.col(f\"df2.{ColNames.cell_id}\"))\n            .alias(ColNames.cell_id_b),\n            STF.ST_Distance(F.col(f\"df1.{ColNames.geometry}\"), F.col(f\"df2.{ColNames.geometry}\")).alias(\n                ColNames.distance\n            ),\n        )\n    )\n    return df\n</code></pre>"},{"location":"reference/components/execution/cell_proximity_estimation/cell_proximity_estimation/#components.execution.cell_proximity_estimation.cell_proximity_estimation.CellProximityEstimation.create_cell_distance_dataframe","title":"<code>create_cell_distance_dataframe(df, current_date)</code>","text":"<p>Creates dataframe matching the cell distance data object structure.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>(cell_id_a, cell_id_b, distance)</p> required <code>current_date</code> <p>date of data</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>(cell_id_a ,cell_id_b, distance, year, month, day)</p> Source code in <code>multimno/components/execution/cell_proximity_estimation/cell_proximity_estimation.py</code> <pre><code>def create_cell_distance_dataframe(self, df: DataFrame, current_date) -&gt; DataFrame:\n    \"\"\"Creates dataframe matching the cell distance data object structure.\n\n    Args:\n        df (DataFrame): (cell_id_a, cell_id_b, distance)\n        current_date: date of data\n\n    Returns:\n        DataFrame: (cell_id_a ,cell_id_b, distance, year, month, day)\n    \"\"\"\n    cell_distance_df = df.select(\n        ColNames.cell_id_a,\n        ColNames.cell_id_b,\n        F.col(ColNames.distance).cast(FloatType()),\n        F.lit(current_date.year).cast(ShortType()).alias(ColNames.year),\n        F.lit(current_date.month).cast(ByteType()).alias(ColNames.month),\n        F.lit(current_date.day).cast(ByteType()).alias(ColNames.day),\n    ).where(F.col(ColNames.cell_id_a) != F.col(ColNames.cell_id_b))\n\n    return cell_distance_df\n</code></pre>"},{"location":"reference/components/execution/cell_proximity_estimation/cell_proximity_estimation/#components.execution.cell_proximity_estimation.cell_proximity_estimation.CellProximityEstimation.create_cell_intersection_groups_dataframe","title":"<code>create_cell_intersection_groups_dataframe(df, current_date)</code>","text":"<p>Creates dataframe matching the cell intersection groups data object structure.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>(cell_id_a, cell_id_b, is_overlap)</p> required <code>current_date</code> <p>date of data</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>(cell_id_a, overlapping_cell_ids, year, month, day)</p> Source code in <code>multimno/components/execution/cell_proximity_estimation/cell_proximity_estimation.py</code> <pre><code>def create_cell_intersection_groups_dataframe(self, df: DataFrame, current_date) -&gt; DataFrame:\n    \"\"\"Creates dataframe matching the cell intersection groups data object structure.\n\n    Args:\n        df (DataFrame): (cell_id_a, cell_id_b, is_overlap)\n        current_date: date of data\n\n    Returns:\n        DataFrame: (cell_id_a, overlapping_cell_ids, year, month, day)\n    \"\"\"\n    cell_intersection_groups_df = (\n        df.where(\"is_overlap\")\n        .groupBy(ColNames.cell_id_a)\n        .agg(F.array_sort(F.collect_list(ColNames.cell_id_b)).alias(ColNames.overlapping_cell_ids))\n    )\n    cell_intersection_groups_df = cell_intersection_groups_df.withColumnRenamed(\n        ColNames.cell_id_a, ColNames.cell_id\n    )\n    cell_intersection_groups_df = cell_intersection_groups_df.select(\n        ColNames.cell_id,\n        ColNames.overlapping_cell_ids,\n        F.lit(current_date.year).cast(ShortType()).alias(ColNames.year),\n        F.lit(current_date.month).cast(ByteType()).alias(ColNames.month),\n        F.lit(current_date.day).cast(ByteType()).alias(ColNames.day),\n    )\n    return cell_intersection_groups_df\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/","title":"daily_permanence_score","text":""},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/","title":"daily_permanence_score","text":"<p>Module that implements the Daily Permanence Score functionality</p>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore","title":"<code>DailyPermanenceScore</code>","text":"<p>               Bases: <code>Component</code></p> <p>A class to calculate the daily permanence score of each user per interval and grid tile.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>class DailyPermanenceScore(Component):\n    \"\"\"\n    A class to calculate the daily permanence score of each user per interval and grid tile.\n    \"\"\"\n\n    COMPONENT_ID = \"DailyPermanenceScore\"\n    # Only 24, 48, and 96 are allowed as values for the number of time slots. These values correspond to intervals\n    # of length 60, 30, and 15 minutes respectively.\n    ALLOWED_NUMBER_OF_TIME_SLOTS = [24, 48, 96]\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.data_period_start = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n        self.data_period_end = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n\n        self.time_slot_number = self.config.getint(self.COMPONENT_ID, \"time_slot_number\")\n        if self.time_slot_number not in self.ALLOWED_NUMBER_OF_TIME_SLOTS:\n            raise ValueError(\n                f\"Accepted values for `time_slot_number` are {str(self.ALLOWED_NUMBER_OF_TIME_SLOTS)} -- found {self.time_slot_number}\"\n            )\n\n        self.max_time_thresh = timedelta(seconds=self.config.getint(self.COMPONENT_ID, \"max_time_thresh\"))\n        self.max_time_thresh_day = timedelta(seconds=self.config.getint(self.COMPONENT_ID, \"max_time_thresh_day\"))\n        self.max_time_thresh_night = timedelta(seconds=self.config.getint(self.COMPONENT_ID, \"max_time_thresh_night\"))\n        self.max_speed_thresh = self.config.getfloat(self.COMPONENT_ID, \"max_speed_thresh\")\n        self.broadcast_footprints = self.config.getboolean(self.COMPONENT_ID, \"broadcast_footprints\", fallback=False)\n        self.use_200m_grid = self.config.getboolean(self.COMPONENT_ID, \"use_200m_grid\", fallback=False)\n\n        self.event_error_flags_to_include = self.config.geteval(self.COMPONENT_ID, \"event_error_flags_to_include\")\n\n        self.data_period_dates = [\n            self.data_period_start + timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n        self.grid_gen = InspireGridGenerator(self.spark)\n        self.events = None\n        self.cell_footprint = None\n        self.time_slots = None\n        self.current_date = None\n\n    def initalize_data_objects(self):\n        # Get paths\n        input_events_silver_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"event_data_silver_flagged\")\n        input_events_cache_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"event_cache\")\n        input_cell_footprint_silver_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"cell_footprint_data_silver\")\n        output_dps_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"daily_permanence_score_data_silver\")\n\n        # Clear destination directory if needed\n        clear_destination_directory = self.config.getboolean(\n            self.COMPONENT_ID, \"clear_destination_directory\", fallback=False\n        )\n        if clear_destination_directory:\n            self.logger.warning(f\"Deleting: {output_dps_path}\")\n            delete_file_or_folder(self.spark, output_dps_path)\n\n        # ------------------ Data objects ------------------\n        silver_events = SilverEventFlaggedDataObject(self.spark, input_events_silver_path)\n        silver_cell_footprint = SilverCellFootprintDataObject(self.spark, input_cell_footprint_silver_path)\n        events_cache = EventCacheDataObject(self.spark, input_events_cache_path)\n\n        silver_dps = SilverDailyPermanenceScoreDataObject(self.spark, output_dps_path)\n\n        self.input_data_objects = {\n            silver_events.ID: silver_events,\n            silver_cell_footprint.ID: silver_cell_footprint,\n            events_cache.ID: events_cache,\n        }\n\n        self.output_data_objects = {silver_dps.ID: silver_dps}\n\n    # ------------------ Assert existence of input data and read+filter it. ------------------\n\n    def check_needed_dates(self):\n        \"\"\"\n        Method that checks if both the dates of study and the dates necessary to generate\n        the daily permanence scores are present in the input data (events + cell footprint).\n\n        Raises:\n            ValueError: If there is no data for one or more of the needed dates.\n        \"\"\"\n        # needed dates: for each date D, we also need D-1 and D+1\n        # this is built this way so it would also support definition of study\n        # dates that are not consecutive\n        needed_dates = (\n            {d + timedelta(days=1) for d in self.data_period_dates}\n            | set(self.data_period_dates)\n            | {d - timedelta(days=1) for d in self.data_period_dates}\n        )\n        self.logger.info(needed_dates)\n        # Assert needed dates in event data:\n        self.assert_needed_dates_events()\n\n        # Assert needed dates in cell footprint data:\n        self.assert_needed_dates_data_object(SilverCellFootprintDataObject.ID, needed_dates)\n\n    def assert_needed_dates_events(self):\n        extended_dates = {d + timedelta(days=1) for d in self.data_period_dates} | {\n            d - timedelta(days=1) for d in self.data_period_dates\n        }\n\n        for dates_to_check, event_type_do in zip(\n            [self.data_period_dates, extended_dates], [SilverEventFlaggedDataObject, EventCacheDataObject]\n        ):\n            # Check event data for data_period_dates\n            missing_dates = [\n                date for date in dates_to_check if not self.input_data_objects[event_type_do.ID].is_data_available(date)\n            ]\n            # Report missing dates\n            if missing_dates:\n                error_msg = f\"Missing {event_type_do.ID} data for dates {sorted(list(missing_dates))}\"\n                self.logger.error(error_msg)\n                raise ValueError(error_msg)\n\n        # Check event cache data for extended_dates\n        missing_dates = [\n            date\n            for date in extended_dates\n            if not self.input_data_objects[EventCacheDataObject.ID].is_data_available(date)\n        ]\n\n    def assert_needed_dates_data_object(self, data_object_id: str, needed_dates: List[datetime]):\n        \"\"\"\n        Method that checks if data for a set of dates exists for a data object.\n\n        Args:\n            data_object_id (str): name of the data object to check.\n            needed_dates (List[datetime]): list of the dates for which data shall be available.\n\n        Raises:\n            ValueError: If there is no data for one or more of the needed dates.\n        \"\"\"\n        # Load data\n        df = self.input_data_objects[data_object_id].df\n\n        # Find dates that match the needed dates:\n        dates = (\n            df.withColumn(ColNames.date, F.make_date(ColNames.year, ColNames.month, ColNames.day))\n            .select(F.col(ColNames.date))\n            .filter(F.col(ColNames.date).isin(needed_dates))\n            .distinct()\n            .collect()\n        )\n        available_dates = {row[ColNames.date] for row in dates}\n\n        # If missing needed dates, raise error:\n        missing_dates = needed_dates.difference(available_dates)\n        if missing_dates:\n            error_msg = f\"Missing {data_object_id} data for dates {sorted(list(missing_dates))}\"\n            self.logger.error(error_msg)\n            raise ValueError(error_msg)\n\n    def filter_events(self, current_date: date, partition_chunk=None) -&gt; DataFrame:\n        \"\"\"\n        Load events with no errors for a specific date.\n\n        Args:\n            current_date (date): current date.\n\n        Returns:\n            DataFrame: filtered events dataframe.\n        \"\"\"\n        df = (\n            self.input_data_objects[SilverEventFlaggedDataObject.ID]\n            .df.filter(\n                (F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(current_date))\n                &amp; (F.col(ColNames.error_flag).isin(self.event_error_flags_to_include))\n            )\n            .select(\n                ColNames.user_id,\n                ColNames.cell_id,\n                ColNames.timestamp,\n                ColNames.year,\n                ColNames.month,\n                ColNames.day,\n                ColNames.user_id_modulo,\n            )\n        )\n\n        if partition_chunk is not None:\n            df = df.filter(F.col(ColNames.user_id_modulo).isin(partition_chunk))\n\n        return df\n\n    def get_cache_events(self, current_date: date, partition_chunk=None, last_event: bool = True) -&gt; DataFrame:\n        \"\"\"\n        Load cache events with for a specific date.\n\n        Args:\n            current_date (date): current date.\n            last_event (bool): flag to get last event or first.\n\n        Returns:\n            DataFrame: filtered events dataframe.\n        \"\"\"\n        df = (\n            self.input_data_objects[EventCacheDataObject.ID]\n            .df.filter(\n                (F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(current_date))\n            )\n            .filter(F.col(ColNames.is_last_event) == last_event)\n            .drop(ColNames.is_last_event)\n        ).select(\n            ColNames.user_id,\n            ColNames.cell_id,\n            ColNames.timestamp,\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            ColNames.user_id_modulo,\n        )\n\n        if partition_chunk is not None:\n            df = df.filter(F.col(ColNames.user_id_modulo).isin(partition_chunk))\n\n        return df\n\n    def filter_cell_footprint(self, current_date: date, cells: DataFrame = None) -&gt; DataFrame:\n        \"\"\"\n        Load cell footprints for a specific date.\n\n        Args:\n            current_date (date): current date.\n\n        Returns:\n            DataFrame: filtered cell footprint dataframe.\n        \"\"\"\n        df = self.input_data_objects[SilverCellFootprintDataObject.ID].df.filter(\n            (F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(current_date))\n        )\n\n        if cells is not None:\n            df = df.join(cells, ColNames.cell_id, \"inner\")\n\n        return df\n\n    # ------------------ Execute ------------------\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n        self.check_needed_dates()\n        partition_chunks = self._get_partition_chunks()\n\n        for current_date in self.data_period_dates:\n            self.current_date = current_date  # for use in other methods\n            self.logger.info(f\"Processing events for {current_date.strftime('%Y-%m-%d')}\")\n            self.build_time_slots_table(current_date)\n\n            for i, partition_chunk in enumerate(partition_chunks):\n                self.logger.info(f\"Processing partition chunk {i}\")\n                self.logger.debug(f\"Partition chunk: {partition_chunk}\")\n\n                # Build input data\n                self.build_day_data(current_date, partition_chunk)\n\n                # Transform\n                self.transform()\n\n                # Write\n                self.write()\n                self.spark.catalog.clearCache()\n                self.logger.info(f\"Finished partition chunk {i}\")\n\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def _get_partition_chunks(self) -&gt; List[List[int]]:\n        \"\"\"\n        Method that returns the partition chunks for the current date.\n\n        Returns:\n            List[List[int, int]]: list of partition chunks. If the partition_chunk_size is not defined in the config or\n                the number of partitions is less than the desired chunk size, it will return a list with a single None element.\n        \"\"\"\n        # Get partitions desired\n        partition_chunk_size = self.config.getint(self.COMPONENT_ID, \"partition_chunk_size\", fallback=None)\n        number_of_partitions = self.config.getint(self.COMPONENT_ID, \"number_of_partitions\", fallback=None)\n\n        if partition_chunk_size is None or number_of_partitions is None or partition_chunk_size &lt;= 0:\n            return [None]\n\n        if number_of_partitions &lt;= partition_chunk_size:\n            self.logger.warning(\n                f\"Available Partition number ({number_of_partitions}) is \"\n                f\"less than the desired chunk size ({partition_chunk_size}). \"\n                f\"Using all partitions.\"\n            )\n            return [None]\n        partition_chunks = [\n            list(range(i, min(i + partition_chunk_size, number_of_partitions)))\n            for i in range(0, number_of_partitions, partition_chunk_size)\n        ]\n        # NOTE: Generate chunks if partition_values were read for each day\n        # getting exactly the amount of partitions for that day\n\n        # partition_chunks = [\n        #     partition_values[i : i + partition_chunk_size]\n        #     for i in range(0, partition_values_size, partition_chunk_size)\n        # ]\n\n        return partition_chunks\n\n    def build_day_data(self, current_date, partition_chunk=None):\n        \"\"\"\n        Load events data for date D, also adding last event of each\n        user from date D-1 and first event of each user from D+1.\n\n        Args:\n            current_date (date): current date.\n\n        Returns:\n            DataFrame: events dataframe.\n        \"\"\"\n        previous_date = current_date - timedelta(days=1)\n        next_date = current_date + timedelta(days=1)\n\n        current_events = self.filter_events(current_date, partition_chunk)\n        previous_events = self.get_cache_events(previous_date, partition_chunk, last_event=True)\n        next_events = self.get_cache_events(next_date, partition_chunk, last_event=False)\n\n        # concat all events together (last of D-1 + all D + first of D+1):\n        events = (\n            previous_events.union(current_events)\n            .union(next_events)\n            .drop(ColNames.longitude, ColNames.latitude, ColNames.loc_error, ColNames.error_flag)\n        )\n\n        # Get distinct cell_ids for previous and next events for filtering footprints\n        previous_cells = previous_events.select(ColNames.cell_id).distinct()\n        next_cells = next_events.select(ColNames.cell_id).distinct()\n\n        current_cell_footprint = self.filter_cell_footprint(current_date)\n        previous_cell_footprint = self.filter_cell_footprint(previous_date, previous_cells)\n        next_cell_footprint = self.filter_cell_footprint(next_date, next_cells)\n\n        cell_footprint = previous_cell_footprint.union(current_cell_footprint).union(next_cell_footprint)\n\n        cell_footprint = self.grid_gen.grid_ids_to_centroids(cell_footprint)\n\n        if self.use_200m_grid:\n            self.logger.info(\"Using 200m grid for DPS calculation\")\n            cell_footprint = self.grid_gen.get_parent_grid_ids(cell_footprint, 200, parent_col_name=ColNames.grid_id)\n\n        cell_footprint = cell_footprint.groupBy([ColNames.cell_id, ColNames.year, ColNames.month, ColNames.day]).agg(\n            F.collect_list(ColNames.geometry).alias(ColNames.geometry),\n            F.array_sort(F.collect_set(ColNames.grid_id)).alias(\"grid_ids\"),\n        )\n\n        cell_footprint = cell_footprint.withColumn(\n            ColNames.geometry,\n            STF.ST_ConcaveHull(STF.ST_Collect(F.col(ColNames.geometry)), 0.8),\n        )\n\n        # Load class attributes\n        self.events = events\n\n        # TODO: This is questionable, use with care\n        if self.broadcast_footprints:\n            cell_footprint = F.broadcast(cell_footprint)\n\n        self.cell_footprint = cell_footprint.persist(StorageLevel.MEMORY_AND_DISK)\n\n    def build_time_slots_table(self, current_date) -&gt; DataFrame:\n        \"\"\"\n        Build a dataframe with the specified time slots for the current date.\n\n        Returns:\n            DataFrame: time slots dataframe.\n        \"\"\"\n        time_slot_length = timedelta(days=1) / self.time_slot_number\n\n        time_slots_list = []\n        previous_end_time = datetime(\n            year=current_date.year,\n            month=current_date.month,\n            day=current_date.day,\n            hour=0,\n            minute=0,\n            second=0,\n        )\n\n        while previous_end_time.date() == current_date:\n            init_time = previous_end_time\n            end_time = init_time + time_slot_length\n            time_slot = (init_time, end_time)\n            time_slots_list.append(time_slot)\n            previous_end_time = end_time\n\n        schema = StructType(\n            [\n                StructField(ColNames.time_slot_initial_time, TimestampType(), True),\n                StructField(ColNames.time_slot_end_time, TimestampType(), True),\n            ]\n        )\n\n        self.time_slots = self.spark.createDataFrame(time_slots_list, schema=schema)\n\n    # ------------------ Main transformations ------------------\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n\n        # differentiate 'move' events:\n        events = self.detect_move_events(self.events, self.cell_footprint)\n\n        # Determine stay durations:\n        stays = self.determine_stay_durations(events)\n\n        # generate time slots per user\n        # TODO: this might be an issue with big countries, probably will need to rethink\n        unique_users = stays.select(ColNames.user_id, ColNames.user_id_modulo).distinct()\n        unique_users = unique_users.repartition(ColNames.user_id_modulo)\n        user_time_slots = unique_users.crossJoin(F.broadcast(self.time_slots))\n\n        # Assign stay time slot, assign duration to time slots:\n        stays_slots = self.calculate_time_slots_durations(stays, user_time_slots)\n\n        # Filter out durations that would result in 0 DPS\n        stays_slots = self.filter_zero_dps_durations(stays_slots)\n        stays_slots = stays_slots.persist(StorageLevel.MEMORY_AND_DISK)\n        stays_slots.count()  # for some reason better to force computation\n\n        # collect grid_ids where DPS is 1\n        dps = self.calculate_dps(stays_slots, self.cell_footprint)\n\n        dps = (\n            dps\n            # since some stays may come from events in previous date, fix and always set current date:\n            .withColumn(ColNames.year, F.lit(self.current_date.year).cast(ShortType()))\n            .withColumn(ColNames.month, F.lit(self.current_date.month).cast(ByteType()))\n            .withColumn(ColNames.day, F.lit(self.current_date.day).cast(ByteType()))\n        )\n\n        dps = apply_schema_casting(dps, SilverDailyPermanenceScoreDataObject.SCHEMA)\n\n        dps = dps.repartition(*SilverDailyPermanenceScoreDataObject.PARTITION_COLUMNS)\n        self.output_data_objects[SilverDailyPermanenceScoreDataObject.ID].df = dps\n\n    def detect_move_events(self, events: DataFrame, cell_footprint: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Detect which of the events are associated to moves according to the\n        distances/times from previous to posterior event and a speed threshold.\n\n        Args:\n            events (DataFrame): events dataframe.\n            cell_footprint (DataFrame): cells footprint dataframe.\n\n        Returns:\n            DataFrame: events dataframe, with an additional 'is_move' boolean column.\n        \"\"\"\n        # inner join -&gt; bring cell footprints to events data discarding events for which there is no cell footprint\n        events = events.join(\n            cell_footprint.select(ColNames.cell_id, ColNames.year, ColNames.month, ColNames.day, ColNames.geometry),\n            (events[ColNames.cell_id] == cell_footprint[ColNames.cell_id])\n            &amp; (events[ColNames.year] == cell_footprint[ColNames.year])\n            &amp; (events[ColNames.month] == cell_footprint[ColNames.month])\n            &amp; (events[ColNames.day] == cell_footprint[ColNames.day]),\n            \"inner\",\n        ).drop(\n            cell_footprint[ColNames.cell_id],\n            cell_footprint[ColNames.year],\n            cell_footprint[ColNames.month],\n            cell_footprint[ColNames.day],\n        )\n\n        # Add lags of timestamp, cell_id and grid_ids:\n        window = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id).orderBy(ColNames.timestamp)\n        lag_fields = [ColNames.timestamp, ColNames.cell_id, ColNames.geometry]\n        for lf in lag_fields:\n            events = events.withColumn(f\"{lf}_+1\", F.lag(lf, -1).over(window)).withColumn(\n                f\"{lf}_-1\", F.lag(lf, 1).over(window)\n            )\n\n        # Calculate distance between grid tiles associated to events -1, 0 and +1:\n        # Calculate speeds and determine which rows are moves:\n        events = (\n            events.withColumn(\n                \"dist_0_+1\",\n                STF.ST_Distance(F.col(ColNames.geometry), F.col(f\"{ColNames.geometry}_+1\")),\n            )\n            # .withColumn(\n            #     \"dist_-1_0\",\n            #     STF.ST_Distance(F.col(f\"{ColNames.geometry}_-1\"), F.col(ColNames.geometry)),\n            # )\n            .withColumn(  # repeating the distance calculation is not necessary, a lagged column works:\n                \"dist_-1_0\", F.lag(\"dist_0_+1\", 1).over(window)\n            )\n            .withColumn(\n                \"dist_-1_+1\",\n                STF.ST_Distance(F.col(f\"{ColNames.geometry}_-1\"), F.col(f\"{ColNames.geometry}_+1\")),\n            )\n            .withColumn(\n                \"time_difference\",\n                F.unix_timestamp(events[f\"{ColNames.timestamp}_+1\"])\n                - F.unix_timestamp(events[f\"{ColNames.timestamp}_-1\"]),\n            )\n            .withColumn(\"max_dist\", F.greatest(F.col(\"dist_-1_0\") + F.col(\"dist_0_+1\"), F.col(\"dist_-1_+1\")))\n            .withColumn(\"speed\", F.col(\"max_dist\") / F.col(\"time_difference\"))\n            .withColumn(\"is_move\", F.when(F.col(\"speed\") &gt; self.max_speed_thresh, True).otherwise(False))\n            .drop(\n                \"dist_0_+1\",\n                \"dist_-1_0\",\n                \"dist_-1_+1\",\n                f\"{ColNames.geometry}_-1\",\n                f\"{ColNames.geometry}_+1\",\n                ColNames.geometry,\n                \"time_difference\",\n                \"max_dist\",\n                \"speed\",\n            )\n        )\n\n        return events\n\n    def determine_stay_durations(self, events: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Determine the start time and end time for each stay event.\n\n        Args:\n            events (DataFrame): events dataframe.\n\n        Returns:\n            DataFrame: stays dataframe (filtering out moves).\n        \"\"\"\n        current_datetime = datetime(self.current_date.year, self.current_date.month, self.current_date.day)\n        # TODO: night interval could be a config parameter instead of hardcoded!\n        night_start_time = current_datetime - timedelta(hours=1)\n        night_end_time = current_datetime + timedelta(hours=9)\n\n        stays = (\n            events\n            # Filter out 'move' events (keep only stays), and also drop unneeded columns:\n            .filter(F.col(\"is_move\") == False)\n            # Set applicable time thresholds:\n            .withColumn(\n                \"threshold_-1\",\n                F.when(\n                    (F.col(\"cell_id\") == F.col(\"cell_id_-1\"))\n                    &amp; (F.col(\"timestamp_-1\") &gt;= night_start_time)\n                    &amp; (F.col(\"timestamp\") &lt;= night_end_time),\n                    self.max_time_thresh_night,\n                ).otherwise(\n                    F.when(F.col(\"cell_id\") == F.col(\"cell_id_-1\"), self.max_time_thresh_day).otherwise(\n                        self.max_time_thresh\n                    )\n                ),\n            )\n            .withColumn(\n                \"threshold_+1\",\n                F.when(\n                    (F.col(\"cell_id\") == F.col(\"cell_id_+1\"))\n                    &amp; (F.col(\"timestamp\") &gt;= night_start_time)\n                    &amp; (F.col(\"timestamp_+1\") &lt;= night_end_time),\n                    self.max_time_thresh_night,\n                ).otherwise(\n                    F.when(F.col(\"cell_id\") == F.col(\"cell_id_+1\"), self.max_time_thresh_day).otherwise(\n                        self.max_time_thresh\n                    )\n                ),\n            )\n            # Calculate init_time and end_time according to thresholds and time differences between events:\n            .withColumn(\n                \"init_time\",\n                F.when(\n                    F.col(ColNames.timestamp) - F.col(f\"{ColNames.timestamp}_-1\") &lt;= F.col(\"threshold_-1\"),\n                    F.col(ColNames.timestamp) - (F.col(ColNames.timestamp) - F.col(f\"{ColNames.timestamp}_-1\")) / 2,\n                ).otherwise(F.col(ColNames.timestamp) - self.max_time_thresh / 2),\n            )\n            .withColumn(\n                \"end_time\",\n                F.when(\n                    F.col(f\"{ColNames.timestamp}_+1\") - F.col(ColNames.timestamp) &lt;= F.col(\"threshold_+1\"),\n                    F.col(f\"{ColNames.timestamp}_+1\")\n                    - (F.col(f\"{ColNames.timestamp}_+1\") - F.col(ColNames.timestamp)) / 2,\n                ).otherwise(F.col(ColNames.timestamp) + self.max_time_thresh / 2),\n            )\n            .drop(\n                f\"{ColNames.cell_id}_-1\",\n                f\"{ColNames.cell_id}_+1\",\n                ColNames.timestamp,\n                f\"{ColNames.timestamp}_-1\",\n                f\"{ColNames.timestamp}_+1\",\n                ColNames.mcc,\n                \"is_move\",\n                \"threshold_-1\",\n                \"threshold_+1\",\n            )\n        )\n\n        return stays\n\n    def calculate_time_slots_durations(self, stays: DataFrame, user_time_slots: DataFrame) -&gt; DataFrame:\n        \"\"\"Calculates duration of user stays within defined time slots.\n\n        Joins stay records with time slot definitions and computes overlapping durations.\n        For time slots with no stays, assigns a default duration of time_slot_number (1/24th day).\n        Finally aggregates total stay duration per user, cell and time slot.\n\n        Args:\n            stays (DataFrame): User mobility stay records\n            user_time_slots (DataFrame): User time slot definitions\n\n        Returns:\n            DataFrame: Aggregated stay durations per time slot\n        \"\"\"\n        stays = (\n            stays.join(\n                user_time_slots.select(\n                    F.col(ColNames.user_id).alias(\"time_slot_user_id\"),\n                    F.col(ColNames.user_id_modulo).alias(\"time_slot_user_id_modulo\"),\n                    ColNames.time_slot_end_time,\n                    ColNames.time_slot_initial_time,\n                ),\n                (\n                    (F.col(\"init_time\") &lt; F.col(ColNames.time_slot_end_time))\n                    &amp; (\n                        F.col(\"end_time\")\n                        &gt; F.col(\n                            ColNames.time_slot_initial_time,\n                        )\n                    )\n                    &amp; (F.col(ColNames.user_id) == F.col(\"time_slot_user_id\"))\n                    &amp; (F.col(ColNames.user_id_modulo) == F.col(\"time_slot_user_id_modulo\"))\n                ),\n                how=\"right\",\n            )\n            .withColumn(\"init_time\", F.greatest(F.col(\"init_time\"), F.col(ColNames.time_slot_initial_time)))\n            .withColumn(\"end_time\", F.least(F.col(\"end_time\"), F.col(ColNames.time_slot_end_time)))\n            .withColumn(\n                ColNames.stay_duration,\n                F.when(\n                    F.col(ColNames.cell_id).isNotNull(),\n                    F.unix_timestamp(F.col(\"end_time\")) - F.unix_timestamp(F.col(\"init_time\")),\n                ).otherwise(F.lit((timedelta(days=1) / self.time_slot_number).total_seconds()).cast(IntegerType())),\n            )\n            .select(\n                F.coalesce(ColNames.user_id, \"time_slot_user_id\").alias(ColNames.user_id),\n                F.coalesce(ColNames.cell_id, F.lit(UeGridIdType.UNKNOWN)).alias(ColNames.cell_id),\n                ColNames.time_slot_initial_time,\n                ColNames.time_slot_end_time,\n                ColNames.stay_duration,\n                ColNames.year,\n                ColNames.month,\n                ColNames.day,\n                F.coalesce(ColNames.user_id_modulo, \"time_slot_user_id_modulo\").alias(ColNames.user_id_modulo),\n            )\n        )\n\n        stays = stays.groupBy(\n            ColNames.user_id,\n            ColNames.cell_id,\n            ColNames.time_slot_initial_time,\n            ColNames.time_slot_end_time,\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            ColNames.user_id_modulo,\n        ).agg((F.sum(ColNames.stay_duration).cast(FloatType()).alias(ColNames.stay_duration)))\n\n        return stays\n\n    def filter_zero_dps_durations(self, stays: DataFrame) -&gt; DataFrame:\n        \"\"\"Filters out stay records that would result in zero Daily Permanence Score.\n\n        Removes records where the total stay duration is less than half of a time slot's\n        duration (1/24th of a day). This pre-filtering step optimizes performance by\n        eliminating records that would not contribute to the final score.\n\n        Args:\n            stays (DataFrame): DataFrame containing stay records with durations\n\n        Returns:\n            DataFrame: Filtered stay records above minimum duration threshold\n        \"\"\"\n        window_spec = Window.partitionBy(\n            ColNames.user_id,\n            ColNames.user_id_modulo,\n            ColNames.time_slot_initial_time,\n        )\n\n        # Count distinct cell_id within each window\n        stays = stays.withColumn(\n            \"distinct_cell_count\", F.size(F.collect_set(ColNames.cell_id).over(window_spec))\n        ).withColumn(\"duration_sum\", F.sum(ColNames.stay_duration).over(window_spec))\n\n        # remove all records where DPS will be 0\n        stays = stays.filter(\n            F.col(\"duration_sum\") &gt;= F.lit(int((timedelta(days=1) / self.time_slot_number).total_seconds() / 2))\n        ).drop(\"duration_sum\")\n\n        return stays\n\n    def calculate_dps(self, stay_intervals: DataFrame, cell_footprint: DataFrame) -&gt; DataFrame:\n        \"\"\"Calculate Daily Permanence Score (DPS) from user stay intervals.\n\n        Processes stay intervals to determine user's presence in grid locations:\n        - For multiple cell stays: explodes footprints and aggregates overlapping areas\n        - For single cell stays: directly maps to corresponding grid IDs\n        - For unknown locations: assigns special unknown identifier\n\n        Args:\n            stay_intervals (DataFrame): User stay intervals with duration and cell information\n            cell_footprint (DataFrame): Mapping between cells and their grid coverage areas\n\n        Returns:\n            DataFrame: Daily Permanence Score results with grid IDs and type identification\n        \"\"\"\n\n        # for intervals with multiple cells, we have to explode footprints to calculate sum of durations for grids\n        # in case if there is overalap in coverage areas.\n        # TODO: this is the most expensive part\n        stay_intervals_multiple_cells = stay_intervals.filter(F.col(\"distinct_cell_count\") &gt; 1)\n\n        stay_intervals_multiple_cells = self.join_footprint_grids(stay_intervals_multiple_cells, cell_footprint)\n\n        stay_intervals_multiple_cells = (\n            stay_intervals_multiple_cells.withColumn(ColNames.grid_id, F.explode(F.col(\"grid_ids\")))\n            .drop(\"grid_ids\")\n            .groupBy(\n                ColNames.user_id,\n                ColNames.time_slot_initial_time,\n                ColNames.time_slot_end_time,\n                ColNames.grid_id,\n                ColNames.user_id_modulo,\n            )\n            .agg(F.sum(ColNames.stay_duration).alias(ColNames.stay_duration))\n            .filter(\n                F.col(ColNames.stay_duration)\n                &gt;= F.lit(int((timedelta(days=1) / self.time_slot_number).total_seconds() / 2))\n            )\n        )\n\n        stay_intervals_multiple_cells = (\n            stay_intervals_multiple_cells.groupBy(\n                ColNames.user_id,\n                ColNames.time_slot_initial_time,\n                ColNames.time_slot_end_time,\n                ColNames.user_id_modulo,\n            )\n            .agg(F.array_sort(F.collect_set(\"grid_id\")).alias(ColNames.dps))\n            .select(\n                ColNames.user_id,\n                ColNames.dps,\n                ColNames.time_slot_initial_time,\n                ColNames.time_slot_end_time,\n                ColNames.user_id_modulo,\n            )\n        )\n\n        # For intervals with single cell no need to explode, just join grid ids as is\n        stay_intervals_single_cell = stay_intervals.filter(\n            (F.col(\"distinct_cell_count\") == 1)\n            &amp; (F.col(ColNames.cell_id) != F.lit(UeGridIdType.UNKNOWN).cast(StringType()))\n        )\n        stay_intervals_single_cell = self.join_footprint_grids(stay_intervals_single_cell, cell_footprint)\n\n        stay_intervals_single_cell = stay_intervals_single_cell.select(\n            ColNames.user_id,\n            F.col(\"grid_ids\").alias(ColNames.dps),\n            ColNames.time_slot_initial_time,\n            ColNames.time_slot_end_time,\n            ColNames.user_id_modulo,\n        )\n\n        uknown_intervals = stay_intervals.filter(F.col(ColNames.cell_id) == F.lit(UeGridIdType.UNKNOWN))\n\n        uknown_intervals = uknown_intervals.withColumn(\n            ColNames.dps,\n            F.array(F.lit(UeGridIdType.UNKNOWN).cast(LongType())),\n        ).select(\n            ColNames.user_id,\n            ColNames.dps,\n            ColNames.time_slot_initial_time,\n            ColNames.time_slot_end_time,\n            ColNames.user_id_modulo,\n        )\n\n        # uknown_intervals = uknown_intervals.persist(StorageLevel.MEMORY_AND_DISK)\n\n        dps = stay_intervals_single_cell.union(stay_intervals_multiple_cells).union(uknown_intervals)\n\n        dps = dps.withColumn(\n            ColNames.id_type,\n            F.when(\n                F.col(ColNames.dps) == F.array(F.lit(UeGridIdType.UNKNOWN).cast(LongType())),\n                F.lit(UeGridIdType.UKNOWN_STR),\n            ).otherwise(F.lit(UeGridIdType.GRID_STR)),\n        )\n\n        return dps\n\n    def join_footprint_grids(self, stay_intervals: DataFrame, cell_footprint: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Join the stay_intervals dataframe with the cell_footprint dataframe.\n\n        Args:\n            stay_intervals (DataFrame): stay_intervals dataframe.\n            cell_footprint (DataFrame): cell_footprint dataframe.\n\n        Returns:\n            DataFrame: stay_intervals dataframe with the grid_ids column.\n        \"\"\"\n        stay_intervals = stay_intervals.join(\n            cell_footprint.select(\n                F.col(ColNames.cell_id).alias(\"footprints_cell_id\"),\n                F.col(ColNames.year).alias(\"footprints_year\"),\n                F.col(ColNames.month).alias(\"footprints_month\"),\n                F.col(ColNames.day).alias(\"footprints_day\"),\n                (\"grid_ids\"),\n            ),\n            (F.col(ColNames.cell_id) == F.col(\"footprints_cell_id\"))\n            &amp; (F.col(ColNames.year) == F.col(\"footprints_year\"))\n            &amp; (F.col(ColNames.month) == F.col(\"footprints_month\"))\n            &amp; (F.col(ColNames.day) == F.col(\"footprints_day\")),\n        ).drop(\n            \"footprints_cell_id\",\n            \"footprints_year\",\n            \"footprints_month\",\n            \"footprints_day\",\n            ColNames.cell_id,\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n        )\n\n        return stay_intervals\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore._get_partition_chunks","title":"<code>_get_partition_chunks()</code>","text":"<p>Method that returns the partition chunks for the current date.</p> <p>Returns:</p> Type Description <code>List[List[int]]</code> <p>List[List[int, int]]: list of partition chunks. If the partition_chunk_size is not defined in the config or the number of partitions is less than the desired chunk size, it will return a list with a single None element.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def _get_partition_chunks(self) -&gt; List[List[int]]:\n    \"\"\"\n    Method that returns the partition chunks for the current date.\n\n    Returns:\n        List[List[int, int]]: list of partition chunks. If the partition_chunk_size is not defined in the config or\n            the number of partitions is less than the desired chunk size, it will return a list with a single None element.\n    \"\"\"\n    # Get partitions desired\n    partition_chunk_size = self.config.getint(self.COMPONENT_ID, \"partition_chunk_size\", fallback=None)\n    number_of_partitions = self.config.getint(self.COMPONENT_ID, \"number_of_partitions\", fallback=None)\n\n    if partition_chunk_size is None or number_of_partitions is None or partition_chunk_size &lt;= 0:\n        return [None]\n\n    if number_of_partitions &lt;= partition_chunk_size:\n        self.logger.warning(\n            f\"Available Partition number ({number_of_partitions}) is \"\n            f\"less than the desired chunk size ({partition_chunk_size}). \"\n            f\"Using all partitions.\"\n        )\n        return [None]\n    partition_chunks = [\n        list(range(i, min(i + partition_chunk_size, number_of_partitions)))\n        for i in range(0, number_of_partitions, partition_chunk_size)\n    ]\n    # NOTE: Generate chunks if partition_values were read for each day\n    # getting exactly the amount of partitions for that day\n\n    # partition_chunks = [\n    #     partition_values[i : i + partition_chunk_size]\n    #     for i in range(0, partition_values_size, partition_chunk_size)\n    # ]\n\n    return partition_chunks\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.assert_needed_dates_data_object","title":"<code>assert_needed_dates_data_object(data_object_id, needed_dates)</code>","text":"<p>Method that checks if data for a set of dates exists for a data object.</p> <p>Parameters:</p> Name Type Description Default <code>data_object_id</code> <code>str</code> <p>name of the data object to check.</p> required <code>needed_dates</code> <code>List[datetime]</code> <p>list of the dates for which data shall be available.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If there is no data for one or more of the needed dates.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def assert_needed_dates_data_object(self, data_object_id: str, needed_dates: List[datetime]):\n    \"\"\"\n    Method that checks if data for a set of dates exists for a data object.\n\n    Args:\n        data_object_id (str): name of the data object to check.\n        needed_dates (List[datetime]): list of the dates for which data shall be available.\n\n    Raises:\n        ValueError: If there is no data for one or more of the needed dates.\n    \"\"\"\n    # Load data\n    df = self.input_data_objects[data_object_id].df\n\n    # Find dates that match the needed dates:\n    dates = (\n        df.withColumn(ColNames.date, F.make_date(ColNames.year, ColNames.month, ColNames.day))\n        .select(F.col(ColNames.date))\n        .filter(F.col(ColNames.date).isin(needed_dates))\n        .distinct()\n        .collect()\n    )\n    available_dates = {row[ColNames.date] for row in dates}\n\n    # If missing needed dates, raise error:\n    missing_dates = needed_dates.difference(available_dates)\n    if missing_dates:\n        error_msg = f\"Missing {data_object_id} data for dates {sorted(list(missing_dates))}\"\n        self.logger.error(error_msg)\n        raise ValueError(error_msg)\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.build_day_data","title":"<code>build_day_data(current_date, partition_chunk=None)</code>","text":"<p>Load events data for date D, also adding last event of each user from date D-1 and first event of each user from D+1.</p> <p>Parameters:</p> Name Type Description Default <code>current_date</code> <code>date</code> <p>current date.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <p>events dataframe.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def build_day_data(self, current_date, partition_chunk=None):\n    \"\"\"\n    Load events data for date D, also adding last event of each\n    user from date D-1 and first event of each user from D+1.\n\n    Args:\n        current_date (date): current date.\n\n    Returns:\n        DataFrame: events dataframe.\n    \"\"\"\n    previous_date = current_date - timedelta(days=1)\n    next_date = current_date + timedelta(days=1)\n\n    current_events = self.filter_events(current_date, partition_chunk)\n    previous_events = self.get_cache_events(previous_date, partition_chunk, last_event=True)\n    next_events = self.get_cache_events(next_date, partition_chunk, last_event=False)\n\n    # concat all events together (last of D-1 + all D + first of D+1):\n    events = (\n        previous_events.union(current_events)\n        .union(next_events)\n        .drop(ColNames.longitude, ColNames.latitude, ColNames.loc_error, ColNames.error_flag)\n    )\n\n    # Get distinct cell_ids for previous and next events for filtering footprints\n    previous_cells = previous_events.select(ColNames.cell_id).distinct()\n    next_cells = next_events.select(ColNames.cell_id).distinct()\n\n    current_cell_footprint = self.filter_cell_footprint(current_date)\n    previous_cell_footprint = self.filter_cell_footprint(previous_date, previous_cells)\n    next_cell_footprint = self.filter_cell_footprint(next_date, next_cells)\n\n    cell_footprint = previous_cell_footprint.union(current_cell_footprint).union(next_cell_footprint)\n\n    cell_footprint = self.grid_gen.grid_ids_to_centroids(cell_footprint)\n\n    if self.use_200m_grid:\n        self.logger.info(\"Using 200m grid for DPS calculation\")\n        cell_footprint = self.grid_gen.get_parent_grid_ids(cell_footprint, 200, parent_col_name=ColNames.grid_id)\n\n    cell_footprint = cell_footprint.groupBy([ColNames.cell_id, ColNames.year, ColNames.month, ColNames.day]).agg(\n        F.collect_list(ColNames.geometry).alias(ColNames.geometry),\n        F.array_sort(F.collect_set(ColNames.grid_id)).alias(\"grid_ids\"),\n    )\n\n    cell_footprint = cell_footprint.withColumn(\n        ColNames.geometry,\n        STF.ST_ConcaveHull(STF.ST_Collect(F.col(ColNames.geometry)), 0.8),\n    )\n\n    # Load class attributes\n    self.events = events\n\n    # TODO: This is questionable, use with care\n    if self.broadcast_footprints:\n        cell_footprint = F.broadcast(cell_footprint)\n\n    self.cell_footprint = cell_footprint.persist(StorageLevel.MEMORY_AND_DISK)\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.build_time_slots_table","title":"<code>build_time_slots_table(current_date)</code>","text":"<p>Build a dataframe with the specified time slots for the current date.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>time slots dataframe.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def build_time_slots_table(self, current_date) -&gt; DataFrame:\n    \"\"\"\n    Build a dataframe with the specified time slots for the current date.\n\n    Returns:\n        DataFrame: time slots dataframe.\n    \"\"\"\n    time_slot_length = timedelta(days=1) / self.time_slot_number\n\n    time_slots_list = []\n    previous_end_time = datetime(\n        year=current_date.year,\n        month=current_date.month,\n        day=current_date.day,\n        hour=0,\n        minute=0,\n        second=0,\n    )\n\n    while previous_end_time.date() == current_date:\n        init_time = previous_end_time\n        end_time = init_time + time_slot_length\n        time_slot = (init_time, end_time)\n        time_slots_list.append(time_slot)\n        previous_end_time = end_time\n\n    schema = StructType(\n        [\n            StructField(ColNames.time_slot_initial_time, TimestampType(), True),\n            StructField(ColNames.time_slot_end_time, TimestampType(), True),\n        ]\n    )\n\n    self.time_slots = self.spark.createDataFrame(time_slots_list, schema=schema)\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.calculate_dps","title":"<code>calculate_dps(stay_intervals, cell_footprint)</code>","text":"<p>Calculate Daily Permanence Score (DPS) from user stay intervals.</p> <p>Processes stay intervals to determine user's presence in grid locations: - For multiple cell stays: explodes footprints and aggregates overlapping areas - For single cell stays: directly maps to corresponding grid IDs - For unknown locations: assigns special unknown identifier</p> <p>Parameters:</p> Name Type Description Default <code>stay_intervals</code> <code>DataFrame</code> <p>User stay intervals with duration and cell information</p> required <code>cell_footprint</code> <code>DataFrame</code> <p>Mapping between cells and their grid coverage areas</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Daily Permanence Score results with grid IDs and type identification</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def calculate_dps(self, stay_intervals: DataFrame, cell_footprint: DataFrame) -&gt; DataFrame:\n    \"\"\"Calculate Daily Permanence Score (DPS) from user stay intervals.\n\n    Processes stay intervals to determine user's presence in grid locations:\n    - For multiple cell stays: explodes footprints and aggregates overlapping areas\n    - For single cell stays: directly maps to corresponding grid IDs\n    - For unknown locations: assigns special unknown identifier\n\n    Args:\n        stay_intervals (DataFrame): User stay intervals with duration and cell information\n        cell_footprint (DataFrame): Mapping between cells and their grid coverage areas\n\n    Returns:\n        DataFrame: Daily Permanence Score results with grid IDs and type identification\n    \"\"\"\n\n    # for intervals with multiple cells, we have to explode footprints to calculate sum of durations for grids\n    # in case if there is overalap in coverage areas.\n    # TODO: this is the most expensive part\n    stay_intervals_multiple_cells = stay_intervals.filter(F.col(\"distinct_cell_count\") &gt; 1)\n\n    stay_intervals_multiple_cells = self.join_footprint_grids(stay_intervals_multiple_cells, cell_footprint)\n\n    stay_intervals_multiple_cells = (\n        stay_intervals_multiple_cells.withColumn(ColNames.grid_id, F.explode(F.col(\"grid_ids\")))\n        .drop(\"grid_ids\")\n        .groupBy(\n            ColNames.user_id,\n            ColNames.time_slot_initial_time,\n            ColNames.time_slot_end_time,\n            ColNames.grid_id,\n            ColNames.user_id_modulo,\n        )\n        .agg(F.sum(ColNames.stay_duration).alias(ColNames.stay_duration))\n        .filter(\n            F.col(ColNames.stay_duration)\n            &gt;= F.lit(int((timedelta(days=1) / self.time_slot_number).total_seconds() / 2))\n        )\n    )\n\n    stay_intervals_multiple_cells = (\n        stay_intervals_multiple_cells.groupBy(\n            ColNames.user_id,\n            ColNames.time_slot_initial_time,\n            ColNames.time_slot_end_time,\n            ColNames.user_id_modulo,\n        )\n        .agg(F.array_sort(F.collect_set(\"grid_id\")).alias(ColNames.dps))\n        .select(\n            ColNames.user_id,\n            ColNames.dps,\n            ColNames.time_slot_initial_time,\n            ColNames.time_slot_end_time,\n            ColNames.user_id_modulo,\n        )\n    )\n\n    # For intervals with single cell no need to explode, just join grid ids as is\n    stay_intervals_single_cell = stay_intervals.filter(\n        (F.col(\"distinct_cell_count\") == 1)\n        &amp; (F.col(ColNames.cell_id) != F.lit(UeGridIdType.UNKNOWN).cast(StringType()))\n    )\n    stay_intervals_single_cell = self.join_footprint_grids(stay_intervals_single_cell, cell_footprint)\n\n    stay_intervals_single_cell = stay_intervals_single_cell.select(\n        ColNames.user_id,\n        F.col(\"grid_ids\").alias(ColNames.dps),\n        ColNames.time_slot_initial_time,\n        ColNames.time_slot_end_time,\n        ColNames.user_id_modulo,\n    )\n\n    uknown_intervals = stay_intervals.filter(F.col(ColNames.cell_id) == F.lit(UeGridIdType.UNKNOWN))\n\n    uknown_intervals = uknown_intervals.withColumn(\n        ColNames.dps,\n        F.array(F.lit(UeGridIdType.UNKNOWN).cast(LongType())),\n    ).select(\n        ColNames.user_id,\n        ColNames.dps,\n        ColNames.time_slot_initial_time,\n        ColNames.time_slot_end_time,\n        ColNames.user_id_modulo,\n    )\n\n    # uknown_intervals = uknown_intervals.persist(StorageLevel.MEMORY_AND_DISK)\n\n    dps = stay_intervals_single_cell.union(stay_intervals_multiple_cells).union(uknown_intervals)\n\n    dps = dps.withColumn(\n        ColNames.id_type,\n        F.when(\n            F.col(ColNames.dps) == F.array(F.lit(UeGridIdType.UNKNOWN).cast(LongType())),\n            F.lit(UeGridIdType.UKNOWN_STR),\n        ).otherwise(F.lit(UeGridIdType.GRID_STR)),\n    )\n\n    return dps\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.calculate_time_slots_durations","title":"<code>calculate_time_slots_durations(stays, user_time_slots)</code>","text":"<p>Calculates duration of user stays within defined time slots.</p> <p>Joins stay records with time slot definitions and computes overlapping durations. For time slots with no stays, assigns a default duration of time_slot_number (1/24th day). Finally aggregates total stay duration per user, cell and time slot.</p> <p>Parameters:</p> Name Type Description Default <code>stays</code> <code>DataFrame</code> <p>User mobility stay records</p> required <code>user_time_slots</code> <code>DataFrame</code> <p>User time slot definitions</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Aggregated stay durations per time slot</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def calculate_time_slots_durations(self, stays: DataFrame, user_time_slots: DataFrame) -&gt; DataFrame:\n    \"\"\"Calculates duration of user stays within defined time slots.\n\n    Joins stay records with time slot definitions and computes overlapping durations.\n    For time slots with no stays, assigns a default duration of time_slot_number (1/24th day).\n    Finally aggregates total stay duration per user, cell and time slot.\n\n    Args:\n        stays (DataFrame): User mobility stay records\n        user_time_slots (DataFrame): User time slot definitions\n\n    Returns:\n        DataFrame: Aggregated stay durations per time slot\n    \"\"\"\n    stays = (\n        stays.join(\n            user_time_slots.select(\n                F.col(ColNames.user_id).alias(\"time_slot_user_id\"),\n                F.col(ColNames.user_id_modulo).alias(\"time_slot_user_id_modulo\"),\n                ColNames.time_slot_end_time,\n                ColNames.time_slot_initial_time,\n            ),\n            (\n                (F.col(\"init_time\") &lt; F.col(ColNames.time_slot_end_time))\n                &amp; (\n                    F.col(\"end_time\")\n                    &gt; F.col(\n                        ColNames.time_slot_initial_time,\n                    )\n                )\n                &amp; (F.col(ColNames.user_id) == F.col(\"time_slot_user_id\"))\n                &amp; (F.col(ColNames.user_id_modulo) == F.col(\"time_slot_user_id_modulo\"))\n            ),\n            how=\"right\",\n        )\n        .withColumn(\"init_time\", F.greatest(F.col(\"init_time\"), F.col(ColNames.time_slot_initial_time)))\n        .withColumn(\"end_time\", F.least(F.col(\"end_time\"), F.col(ColNames.time_slot_end_time)))\n        .withColumn(\n            ColNames.stay_duration,\n            F.when(\n                F.col(ColNames.cell_id).isNotNull(),\n                F.unix_timestamp(F.col(\"end_time\")) - F.unix_timestamp(F.col(\"init_time\")),\n            ).otherwise(F.lit((timedelta(days=1) / self.time_slot_number).total_seconds()).cast(IntegerType())),\n        )\n        .select(\n            F.coalesce(ColNames.user_id, \"time_slot_user_id\").alias(ColNames.user_id),\n            F.coalesce(ColNames.cell_id, F.lit(UeGridIdType.UNKNOWN)).alias(ColNames.cell_id),\n            ColNames.time_slot_initial_time,\n            ColNames.time_slot_end_time,\n            ColNames.stay_duration,\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            F.coalesce(ColNames.user_id_modulo, \"time_slot_user_id_modulo\").alias(ColNames.user_id_modulo),\n        )\n    )\n\n    stays = stays.groupBy(\n        ColNames.user_id,\n        ColNames.cell_id,\n        ColNames.time_slot_initial_time,\n        ColNames.time_slot_end_time,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n        ColNames.user_id_modulo,\n    ).agg((F.sum(ColNames.stay_duration).cast(FloatType()).alias(ColNames.stay_duration)))\n\n    return stays\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.check_needed_dates","title":"<code>check_needed_dates()</code>","text":"<p>Method that checks if both the dates of study and the dates necessary to generate the daily permanence scores are present in the input data (events + cell footprint).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there is no data for one or more of the needed dates.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def check_needed_dates(self):\n    \"\"\"\n    Method that checks if both the dates of study and the dates necessary to generate\n    the daily permanence scores are present in the input data (events + cell footprint).\n\n    Raises:\n        ValueError: If there is no data for one or more of the needed dates.\n    \"\"\"\n    # needed dates: for each date D, we also need D-1 and D+1\n    # this is built this way so it would also support definition of study\n    # dates that are not consecutive\n    needed_dates = (\n        {d + timedelta(days=1) for d in self.data_period_dates}\n        | set(self.data_period_dates)\n        | {d - timedelta(days=1) for d in self.data_period_dates}\n    )\n    self.logger.info(needed_dates)\n    # Assert needed dates in event data:\n    self.assert_needed_dates_events()\n\n    # Assert needed dates in cell footprint data:\n    self.assert_needed_dates_data_object(SilverCellFootprintDataObject.ID, needed_dates)\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.detect_move_events","title":"<code>detect_move_events(events, cell_footprint)</code>","text":"<p>Detect which of the events are associated to moves according to the distances/times from previous to posterior event and a speed threshold.</p> <p>Parameters:</p> Name Type Description Default <code>events</code> <code>DataFrame</code> <p>events dataframe.</p> required <code>cell_footprint</code> <code>DataFrame</code> <p>cells footprint dataframe.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>events dataframe, with an additional 'is_move' boolean column.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def detect_move_events(self, events: DataFrame, cell_footprint: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Detect which of the events are associated to moves according to the\n    distances/times from previous to posterior event and a speed threshold.\n\n    Args:\n        events (DataFrame): events dataframe.\n        cell_footprint (DataFrame): cells footprint dataframe.\n\n    Returns:\n        DataFrame: events dataframe, with an additional 'is_move' boolean column.\n    \"\"\"\n    # inner join -&gt; bring cell footprints to events data discarding events for which there is no cell footprint\n    events = events.join(\n        cell_footprint.select(ColNames.cell_id, ColNames.year, ColNames.month, ColNames.day, ColNames.geometry),\n        (events[ColNames.cell_id] == cell_footprint[ColNames.cell_id])\n        &amp; (events[ColNames.year] == cell_footprint[ColNames.year])\n        &amp; (events[ColNames.month] == cell_footprint[ColNames.month])\n        &amp; (events[ColNames.day] == cell_footprint[ColNames.day]),\n        \"inner\",\n    ).drop(\n        cell_footprint[ColNames.cell_id],\n        cell_footprint[ColNames.year],\n        cell_footprint[ColNames.month],\n        cell_footprint[ColNames.day],\n    )\n\n    # Add lags of timestamp, cell_id and grid_ids:\n    window = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id).orderBy(ColNames.timestamp)\n    lag_fields = [ColNames.timestamp, ColNames.cell_id, ColNames.geometry]\n    for lf in lag_fields:\n        events = events.withColumn(f\"{lf}_+1\", F.lag(lf, -1).over(window)).withColumn(\n            f\"{lf}_-1\", F.lag(lf, 1).over(window)\n        )\n\n    # Calculate distance between grid tiles associated to events -1, 0 and +1:\n    # Calculate speeds and determine which rows are moves:\n    events = (\n        events.withColumn(\n            \"dist_0_+1\",\n            STF.ST_Distance(F.col(ColNames.geometry), F.col(f\"{ColNames.geometry}_+1\")),\n        )\n        # .withColumn(\n        #     \"dist_-1_0\",\n        #     STF.ST_Distance(F.col(f\"{ColNames.geometry}_-1\"), F.col(ColNames.geometry)),\n        # )\n        .withColumn(  # repeating the distance calculation is not necessary, a lagged column works:\n            \"dist_-1_0\", F.lag(\"dist_0_+1\", 1).over(window)\n        )\n        .withColumn(\n            \"dist_-1_+1\",\n            STF.ST_Distance(F.col(f\"{ColNames.geometry}_-1\"), F.col(f\"{ColNames.geometry}_+1\")),\n        )\n        .withColumn(\n            \"time_difference\",\n            F.unix_timestamp(events[f\"{ColNames.timestamp}_+1\"])\n            - F.unix_timestamp(events[f\"{ColNames.timestamp}_-1\"]),\n        )\n        .withColumn(\"max_dist\", F.greatest(F.col(\"dist_-1_0\") + F.col(\"dist_0_+1\"), F.col(\"dist_-1_+1\")))\n        .withColumn(\"speed\", F.col(\"max_dist\") / F.col(\"time_difference\"))\n        .withColumn(\"is_move\", F.when(F.col(\"speed\") &gt; self.max_speed_thresh, True).otherwise(False))\n        .drop(\n            \"dist_0_+1\",\n            \"dist_-1_0\",\n            \"dist_-1_+1\",\n            f\"{ColNames.geometry}_-1\",\n            f\"{ColNames.geometry}_+1\",\n            ColNames.geometry,\n            \"time_difference\",\n            \"max_dist\",\n            \"speed\",\n        )\n    )\n\n    return events\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.determine_stay_durations","title":"<code>determine_stay_durations(events)</code>","text":"<p>Determine the start time and end time for each stay event.</p> <p>Parameters:</p> Name Type Description Default <code>events</code> <code>DataFrame</code> <p>events dataframe.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>stays dataframe (filtering out moves).</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def determine_stay_durations(self, events: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Determine the start time and end time for each stay event.\n\n    Args:\n        events (DataFrame): events dataframe.\n\n    Returns:\n        DataFrame: stays dataframe (filtering out moves).\n    \"\"\"\n    current_datetime = datetime(self.current_date.year, self.current_date.month, self.current_date.day)\n    # TODO: night interval could be a config parameter instead of hardcoded!\n    night_start_time = current_datetime - timedelta(hours=1)\n    night_end_time = current_datetime + timedelta(hours=9)\n\n    stays = (\n        events\n        # Filter out 'move' events (keep only stays), and also drop unneeded columns:\n        .filter(F.col(\"is_move\") == False)\n        # Set applicable time thresholds:\n        .withColumn(\n            \"threshold_-1\",\n            F.when(\n                (F.col(\"cell_id\") == F.col(\"cell_id_-1\"))\n                &amp; (F.col(\"timestamp_-1\") &gt;= night_start_time)\n                &amp; (F.col(\"timestamp\") &lt;= night_end_time),\n                self.max_time_thresh_night,\n            ).otherwise(\n                F.when(F.col(\"cell_id\") == F.col(\"cell_id_-1\"), self.max_time_thresh_day).otherwise(\n                    self.max_time_thresh\n                )\n            ),\n        )\n        .withColumn(\n            \"threshold_+1\",\n            F.when(\n                (F.col(\"cell_id\") == F.col(\"cell_id_+1\"))\n                &amp; (F.col(\"timestamp\") &gt;= night_start_time)\n                &amp; (F.col(\"timestamp_+1\") &lt;= night_end_time),\n                self.max_time_thresh_night,\n            ).otherwise(\n                F.when(F.col(\"cell_id\") == F.col(\"cell_id_+1\"), self.max_time_thresh_day).otherwise(\n                    self.max_time_thresh\n                )\n            ),\n        )\n        # Calculate init_time and end_time according to thresholds and time differences between events:\n        .withColumn(\n            \"init_time\",\n            F.when(\n                F.col(ColNames.timestamp) - F.col(f\"{ColNames.timestamp}_-1\") &lt;= F.col(\"threshold_-1\"),\n                F.col(ColNames.timestamp) - (F.col(ColNames.timestamp) - F.col(f\"{ColNames.timestamp}_-1\")) / 2,\n            ).otherwise(F.col(ColNames.timestamp) - self.max_time_thresh / 2),\n        )\n        .withColumn(\n            \"end_time\",\n            F.when(\n                F.col(f\"{ColNames.timestamp}_+1\") - F.col(ColNames.timestamp) &lt;= F.col(\"threshold_+1\"),\n                F.col(f\"{ColNames.timestamp}_+1\")\n                - (F.col(f\"{ColNames.timestamp}_+1\") - F.col(ColNames.timestamp)) / 2,\n            ).otherwise(F.col(ColNames.timestamp) + self.max_time_thresh / 2),\n        )\n        .drop(\n            f\"{ColNames.cell_id}_-1\",\n            f\"{ColNames.cell_id}_+1\",\n            ColNames.timestamp,\n            f\"{ColNames.timestamp}_-1\",\n            f\"{ColNames.timestamp}_+1\",\n            ColNames.mcc,\n            \"is_move\",\n            \"threshold_-1\",\n            \"threshold_+1\",\n        )\n    )\n\n    return stays\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.filter_cell_footprint","title":"<code>filter_cell_footprint(current_date, cells=None)</code>","text":"<p>Load cell footprints for a specific date.</p> <p>Parameters:</p> Name Type Description Default <code>current_date</code> <code>date</code> <p>current date.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>filtered cell footprint dataframe.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def filter_cell_footprint(self, current_date: date, cells: DataFrame = None) -&gt; DataFrame:\n    \"\"\"\n    Load cell footprints for a specific date.\n\n    Args:\n        current_date (date): current date.\n\n    Returns:\n        DataFrame: filtered cell footprint dataframe.\n    \"\"\"\n    df = self.input_data_objects[SilverCellFootprintDataObject.ID].df.filter(\n        (F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(current_date))\n    )\n\n    if cells is not None:\n        df = df.join(cells, ColNames.cell_id, \"inner\")\n\n    return df\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.filter_events","title":"<code>filter_events(current_date, partition_chunk=None)</code>","text":"<p>Load events with no errors for a specific date.</p> <p>Parameters:</p> Name Type Description Default <code>current_date</code> <code>date</code> <p>current date.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>filtered events dataframe.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def filter_events(self, current_date: date, partition_chunk=None) -&gt; DataFrame:\n    \"\"\"\n    Load events with no errors for a specific date.\n\n    Args:\n        current_date (date): current date.\n\n    Returns:\n        DataFrame: filtered events dataframe.\n    \"\"\"\n    df = (\n        self.input_data_objects[SilverEventFlaggedDataObject.ID]\n        .df.filter(\n            (F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(current_date))\n            &amp; (F.col(ColNames.error_flag).isin(self.event_error_flags_to_include))\n        )\n        .select(\n            ColNames.user_id,\n            ColNames.cell_id,\n            ColNames.timestamp,\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            ColNames.user_id_modulo,\n        )\n    )\n\n    if partition_chunk is not None:\n        df = df.filter(F.col(ColNames.user_id_modulo).isin(partition_chunk))\n\n    return df\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.filter_zero_dps_durations","title":"<code>filter_zero_dps_durations(stays)</code>","text":"<p>Filters out stay records that would result in zero Daily Permanence Score.</p> <p>Removes records where the total stay duration is less than half of a time slot's duration (1/24th of a day). This pre-filtering step optimizes performance by eliminating records that would not contribute to the final score.</p> <p>Parameters:</p> Name Type Description Default <code>stays</code> <code>DataFrame</code> <p>DataFrame containing stay records with durations</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered stay records above minimum duration threshold</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def filter_zero_dps_durations(self, stays: DataFrame) -&gt; DataFrame:\n    \"\"\"Filters out stay records that would result in zero Daily Permanence Score.\n\n    Removes records where the total stay duration is less than half of a time slot's\n    duration (1/24th of a day). This pre-filtering step optimizes performance by\n    eliminating records that would not contribute to the final score.\n\n    Args:\n        stays (DataFrame): DataFrame containing stay records with durations\n\n    Returns:\n        DataFrame: Filtered stay records above minimum duration threshold\n    \"\"\"\n    window_spec = Window.partitionBy(\n        ColNames.user_id,\n        ColNames.user_id_modulo,\n        ColNames.time_slot_initial_time,\n    )\n\n    # Count distinct cell_id within each window\n    stays = stays.withColumn(\n        \"distinct_cell_count\", F.size(F.collect_set(ColNames.cell_id).over(window_spec))\n    ).withColumn(\"duration_sum\", F.sum(ColNames.stay_duration).over(window_spec))\n\n    # remove all records where DPS will be 0\n    stays = stays.filter(\n        F.col(\"duration_sum\") &gt;= F.lit(int((timedelta(days=1) / self.time_slot_number).total_seconds() / 2))\n    ).drop(\"duration_sum\")\n\n    return stays\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.get_cache_events","title":"<code>get_cache_events(current_date, partition_chunk=None, last_event=True)</code>","text":"<p>Load cache events with for a specific date.</p> <p>Parameters:</p> Name Type Description Default <code>current_date</code> <code>date</code> <p>current date.</p> required <code>last_event</code> <code>bool</code> <p>flag to get last event or first.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>filtered events dataframe.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def get_cache_events(self, current_date: date, partition_chunk=None, last_event: bool = True) -&gt; DataFrame:\n    \"\"\"\n    Load cache events with for a specific date.\n\n    Args:\n        current_date (date): current date.\n        last_event (bool): flag to get last event or first.\n\n    Returns:\n        DataFrame: filtered events dataframe.\n    \"\"\"\n    df = (\n        self.input_data_objects[EventCacheDataObject.ID]\n        .df.filter(\n            (F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(current_date))\n        )\n        .filter(F.col(ColNames.is_last_event) == last_event)\n        .drop(ColNames.is_last_event)\n    ).select(\n        ColNames.user_id,\n        ColNames.cell_id,\n        ColNames.timestamp,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n        ColNames.user_id_modulo,\n    )\n\n    if partition_chunk is not None:\n        df = df.filter(F.col(ColNames.user_id_modulo).isin(partition_chunk))\n\n    return df\n</code></pre>"},{"location":"reference/components/execution/daily_permanence_score/daily_permanence_score/#components.execution.daily_permanence_score.daily_permanence_score.DailyPermanenceScore.join_footprint_grids","title":"<code>join_footprint_grids(stay_intervals, cell_footprint)</code>","text":"<p>Join the stay_intervals dataframe with the cell_footprint dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>stay_intervals</code> <code>DataFrame</code> <p>stay_intervals dataframe.</p> required <code>cell_footprint</code> <code>DataFrame</code> <p>cell_footprint dataframe.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>stay_intervals dataframe with the grid_ids column.</p> Source code in <code>multimno/components/execution/daily_permanence_score/daily_permanence_score.py</code> <pre><code>def join_footprint_grids(self, stay_intervals: DataFrame, cell_footprint: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Join the stay_intervals dataframe with the cell_footprint dataframe.\n\n    Args:\n        stay_intervals (DataFrame): stay_intervals dataframe.\n        cell_footprint (DataFrame): cell_footprint dataframe.\n\n    Returns:\n        DataFrame: stay_intervals dataframe with the grid_ids column.\n    \"\"\"\n    stay_intervals = stay_intervals.join(\n        cell_footprint.select(\n            F.col(ColNames.cell_id).alias(\"footprints_cell_id\"),\n            F.col(ColNames.year).alias(\"footprints_year\"),\n            F.col(ColNames.month).alias(\"footprints_month\"),\n            F.col(ColNames.day).alias(\"footprints_day\"),\n            (\"grid_ids\"),\n        ),\n        (F.col(ColNames.cell_id) == F.col(\"footprints_cell_id\"))\n        &amp; (F.col(ColNames.year) == F.col(\"footprints_year\"))\n        &amp; (F.col(ColNames.month) == F.col(\"footprints_month\"))\n        &amp; (F.col(ColNames.day) == F.col(\"footprints_day\")),\n    ).drop(\n        \"footprints_cell_id\",\n        \"footprints_year\",\n        \"footprints_month\",\n        \"footprints_day\",\n        ColNames.cell_id,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    )\n\n    return stay_intervals\n</code></pre>"},{"location":"reference/components/execution/device_activity_statistics/","title":"device_activity_statistics","text":""},{"location":"reference/components/execution/device_activity_statistics/device_activity_statistics/","title":"device_activity_statistics","text":"<p>Module that cleans RAW MNO Event data.</p>"},{"location":"reference/components/execution/device_activity_statistics/device_activity_statistics/#components.execution.device_activity_statistics.device_activity_statistics.DeviceActivityStatistics","title":"<code>DeviceActivityStatistics</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class that removes duplicates from clean MNO Event data</p> Source code in <code>multimno/components/execution/device_activity_statistics/device_activity_statistics.py</code> <pre><code>class DeviceActivityStatistics(Component):\n    \"\"\"\n    Class that removes duplicates from clean MNO Event data\n    \"\"\"\n\n    COMPONENT_ID = \"DeviceActivityStatistics\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n        self.current_date = None\n        self.current_input_events = None\n        self.current_input_network = None\n        self.statistics_df = None\n\n    def initalize_data_objects(self):\n        # Input\n        # TODO: update this to semantically cleaned files after merge\n        self.input_events_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"event_data_silver\")\n        # TODO: Figure out how this would work with coverage areas. We don't have location of cells in those cases\n        self.input_topology_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"network_data_silver\")\n        self.output_statistics_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"device_activity_statistics\")\n\n        self.data_period_start = self.config.get(DeviceActivityStatistics.COMPONENT_ID, \"data_period_start\")\n        self.data_period_end = self.config.get(DeviceActivityStatistics.COMPONENT_ID, \"data_period_end\")\n        self.clear_destination_directory = self.config.get(\n            DeviceActivityStatistics.COMPONENT_ID, \"clear_destination_directory\"\n        )\n        self.local_timezone_str = self.config.get(GENERAL_CONFIG_KEY, \"local_timezone\")\n\n        # Create all possible dates between start and end\n        sdate = pd.to_datetime(self.data_period_start)\n        edate = pd.to_datetime(self.data_period_end)\n        self.to_process_dates = list(pd.date_range(sdate, edate, freq=\"d\"))\n\n        # Create all input data objects\n        self.input_data_objects = {SilverEventDataObject.ID: None}\n        if check_if_data_path_exists(self.spark, self.input_events_path):\n            # Single input option, with a filter follwed by it\n            self.input_data_objects[SilverEventDataObject.ID] = SilverEventDataObject(\n                self.spark, self.input_events_path\n            )\n        else:\n            self.logger.warning(f\"Expected path {self.input_events_path} to exist but it does not\")\n\n        self.input_data_objects[SilverNetworkDataObject.ID] = None\n\n        if check_if_data_path_exists(self.spark, self.input_topology_path):\n            # Single input option, with a filter follwed by it\n            self.input_data_objects[SilverNetworkDataObject.ID] = SilverNetworkDataObject(\n                self.spark, self.input_topology_path\n            )\n        else:\n            self.logger.warning(f\"Expected path {self.input_topology_path} to exist but it does not\")\n\n        # Output data objects dictionary\n        self.output_data_objects = {}\n        self.output_data_objects[SilverDeviceActivityStatistics.ID] = SilverDeviceActivityStatistics(\n            self.spark, self.output_statistics_path\n        )\n\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, self.output_statistics_path)\n\n        # Create timezones for transformations\n        self.local_tz = pytz.timezone(self.local_timezone_str)\n        self.utc_tz = pytz.timezone(\"UTC\")\n\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n\n        self.read()\n\n        for current_date in self.to_process_dates:\n            self.current_date = current_date\n\n            start = datetime(year=current_date.year, month=current_date.month, day=current_date.day)\n            start_utc = self.local_tz.localize(start).astimezone(self.utc_tz)\n            end_utc = start_utc + timedelta(days=1) - timedelta(seconds=1)\n\n            # TODO: should this selection also be based on user_id_modulo?\n            self.current_input_events = self.input_data_objects[SilverEventDataObject.ID].df.filter(\n                (F.col(ColNames.timestamp).between(start_utc, end_utc))\n            )\n\n            self.current_input_network = self.input_data_objects[SilverNetworkDataObject.ID].df.filter(\n                (\n                    (F.col(ColNames.year) == current_date.year)\n                    &amp; (F.col(ColNames.month) == current_date.month)\n                    &amp; (F.col(ColNames.day) == current_date.day)\n                )\n            )\n\n            self.transform()\n\n            self.write()\n\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n        # Prepare everything for metrics calculations\n        df_events = self.preprocess_events(self.current_input_events, self.current_input_network)\n\n        # Calculate count of events per user\n        self.statistics_df = df_events.groupby(ColNames.user_id).count().withColumnRenamed(\"count\", ColNames.event_cnt)\n\n        # Calculate count of unique cells per user\n        unique_cell_counts = (\n            df_events.groupBy(ColNames.user_id)\n            .agg(F.countDistinct(ColNames.cell_id))\n            .withColumnRenamed(f\"count(DISTINCT {ColNames.cell_id})\", ColNames.unique_cell_cnt)\n        )\n        self.statistics_df = self.statistics_df.join(unique_cell_counts, on=\"user_id\")\n        # Calculate count of unique locations per user\n        unique_location_counts = df_events.groupBy(ColNames.user_id).agg(\n            F.countDistinct(ColNames.latitude, ColNames.longitude).alias(ColNames.unique_location_cnt)\n        )\n        self.statistics_df = self.statistics_df.join(unique_location_counts, on=\"user_id\")\n\n        # Calculate sum of distances between cells\n        distance_per_user = df_events.groupBy(ColNames.user_id).agg(\n            F.sum(\"distance\").cast(IntegerType()).alias(ColNames.sum_distance_m)\n        )\n        self.statistics_df = self.statistics_df.join(distance_per_user, on=\"user_id\")\n\n        # Calculate number of unique hours in data per user\n        hourly_events_df = df_events.withColumn(\n            ColNames.timestamp, F.date_trunc(\"hour\", F.col(ColNames.timestamp))\n        ).select([ColNames.user_id, ColNames.timestamp])\n        hourly_counts = hourly_events_df.groupBy(ColNames.user_id).agg(\n            F.countDistinct(ColNames.timestamp).alias(ColNames.unique_hour_cnt)\n        )\n        self.statistics_df = self.statistics_df.join(hourly_counts, on=\"user_id\")\n\n        # mean_time_gap\n        mean_time_gaps = df_events.groupBy(ColNames.user_id).agg(\n            F.mean(\"time_gap_s\").cast(IntegerType()).alias(ColNames.mean_time_gap)\n        )\n        self.statistics_df = self.statistics_df.join(mean_time_gaps, on=\"user_id\")\n\n        # stdev_time_gap\n        stddev_time_gaps = df_events.groupBy(ColNames.user_id).agg(\n            F.stddev(\"time_gap_s\").alias(ColNames.stdev_time_gap)\n        )\n        self.statistics_df = self.statistics_df.join(stddev_time_gaps, on=\"user_id\")\n        # Add date to statistics\n        self.statistics_df = self.statistics_df.withColumns(\n            {\n                ColNames.year: F.lit(self.current_date.year).cast(\"smallint\"),\n                ColNames.month: F.lit(self.current_date.month).cast(\"tinyint\"),\n                ColNames.day: F.lit(self.current_date.day).cast(\"tinyint\"),\n            }\n        )\n        # Reorder rows\n        self.statistics_df = self.statistics_df.select([col.name for col in SilverDeviceActivityStatistics.SCHEMA])\n        for col in SilverDeviceActivityStatistics.SCHEMA:\n            self.statistics_df = self.statistics_df.withColumn(\n                col.name, self.statistics_df[col.name].cast(col.dataType)\n            )\n        self.output_data_objects[SilverDeviceActivityStatistics.ID].df = self.statistics_df\n\n        # after each chunk processing clear all Cache to free memory and disk\n        self.spark.catalog.clearCache()\n\n    def preprocess_events(\n        self,\n        df_events: pyspark.sql.dataframe.DataFrame,\n        df_network: pyspark.sql.dataframe.DataFrame,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Preprocesses events dataframe to be able to calculate all metrics. Steps:\n        1. Converts timestamp from UTC to local\n        2. Fills latitude and longitude columns from the location of the cell\n        3. Gets location of next event for each event\n        4. Gets timestamp of next event for each event\n        5. Calculates time gap to next event\n        6. Calculates distance to next event\n\n        Args:\n            df_events (pyspark.sql.dataframe.DataFrame): Events relating to the day that is currently being processed\n            df_network (pyspark.sql.dataframe.DataFrame): Network relating to the day that is currently being processed\n\n        Returns:\n            df_events: Events relating to the day that is currently being processed with\n                extra columns for metric calculation\n        \"\"\"\n\n        # Convert timestamp to local\n        df_events.withColumn(\n            ColNames.timestamp,\n            F.to_utc_timestamp(F.from_utc_timestamp(ColNames.timestamp, self.local_timezone_str), \"UTC\"),\n        )\n        df_events = df_events.withColumns(\n            {\n                ColNames.year: F.year(ColNames.timestamp).cast(\"smallint\"),\n                ColNames.month: F.month(ColNames.timestamp).cast(\"tinyint\"),\n                ColNames.day: F.dayofmonth(ColNames.timestamp).cast(\"tinyint\"),\n            }\n        )\n\n        # Join events with topology data to enable checking unique locations and travelled distances\n        df_events = df_events.join(\n            df_network.select(\n                [\n                    F.col(ColNames.cell_id),\n                    F.col(ColNames.latitude).alias(\"cell_lat\"),\n                    F.col(ColNames.longitude).alias(\"cell_lon\"),\n                ]\n            ),\n            on=ColNames.cell_id,\n            how=\"left\",\n        )\n\n        # Use latitude and longitude if they exist, otherwise use cells location\n        df_events = df_events.withColumn(ColNames.latitude, F.coalesce(ColNames.latitude, \"cell_lat\"))\n        df_events = df_events.withColumn(ColNames.longitude, F.coalesce(ColNames.longitude, \"cell_lon\"))\n\n        # Add timestamp and location of next record\n        window = Window.partitionBy(*[ColNames.user_id_modulo, ColNames.user_id]).orderBy(ColNames.timestamp)\n        df_events = df_events.withColumn(\n            f\"next_{ColNames.timestamp}\",\n            F.lead(F.col(ColNames.timestamp), 1).over(window),\n        )\n        df_events = df_events.withColumn(\n            f\"next_{ColNames.latitude}\",\n            F.lead(F.col(ColNames.latitude), 1).over(window),\n        )\n        df_events = df_events.withColumn(\n            f\"next_{ColNames.longitude}\",\n            F.lead(F.col(ColNames.longitude), 1).over(window),\n        )\n\n        # Calculate time gap to next event\n        df_events = df_events.withColumn(\n            \"time_gap_s\",\n            F.unix_timestamp(f\"next_{ColNames.timestamp}\") - F.unix_timestamp(ColNames.timestamp),\n        )\n\n        # Calculate the distance between current and next event\n        # TODO: check if this is the correct way to calculate, got varying results\n        # There are many ways to calculate distance between points and all of them give different results\n        df_events = df_events.withColumn(\n            \"source_geom\",\n            STF.ST_SetSRID(\n                STC.ST_Point(df_events[ColNames.latitude], df_events[ColNames.longitude]),\n                4326,\n            ),\n        )\n        df_events = df_events.withColumn(\n            \"destination_geom\",\n            STF.ST_SetSRID(\n                STC.ST_Point(\n                    df_events[f\"next_{ColNames.latitude}\"],\n                    df_events[f\"next_{ColNames.longitude}\"],\n                ),\n                4326,\n            ),\n        )\n        df_events = df_events.withColumn(\n            \"distance\",\n            STF.ST_DistanceSphere(df_events[\"source_geom\"], df_events[\"destination_geom\"]),\n        )\n\n        return df_events\n</code></pre>"},{"location":"reference/components/execution/device_activity_statistics/device_activity_statistics/#components.execution.device_activity_statistics.device_activity_statistics.DeviceActivityStatistics.preprocess_events","title":"<code>preprocess_events(df_events, df_network)</code>","text":"<p>Preprocesses events dataframe to be able to calculate all metrics. Steps: 1. Converts timestamp from UTC to local 2. Fills latitude and longitude columns from the location of the cell 3. Gets location of next event for each event 4. Gets timestamp of next event for each event 5. Calculates time gap to next event 6. Calculates distance to next event</p> <p>Parameters:</p> Name Type Description Default <code>df_events</code> <code>DataFrame</code> <p>Events relating to the day that is currently being processed</p> required <code>df_network</code> <code>DataFrame</code> <p>Network relating to the day that is currently being processed</p> required <p>Returns:</p> Name Type Description <code>df_events</code> <code>DataFrame</code> <p>Events relating to the day that is currently being processed with extra columns for metric calculation</p> Source code in <code>multimno/components/execution/device_activity_statistics/device_activity_statistics.py</code> <pre><code>def preprocess_events(\n    self,\n    df_events: pyspark.sql.dataframe.DataFrame,\n    df_network: pyspark.sql.dataframe.DataFrame,\n) -&gt; DataFrame:\n    \"\"\"\n    Preprocesses events dataframe to be able to calculate all metrics. Steps:\n    1. Converts timestamp from UTC to local\n    2. Fills latitude and longitude columns from the location of the cell\n    3. Gets location of next event for each event\n    4. Gets timestamp of next event for each event\n    5. Calculates time gap to next event\n    6. Calculates distance to next event\n\n    Args:\n        df_events (pyspark.sql.dataframe.DataFrame): Events relating to the day that is currently being processed\n        df_network (pyspark.sql.dataframe.DataFrame): Network relating to the day that is currently being processed\n\n    Returns:\n        df_events: Events relating to the day that is currently being processed with\n            extra columns for metric calculation\n    \"\"\"\n\n    # Convert timestamp to local\n    df_events.withColumn(\n        ColNames.timestamp,\n        F.to_utc_timestamp(F.from_utc_timestamp(ColNames.timestamp, self.local_timezone_str), \"UTC\"),\n    )\n    df_events = df_events.withColumns(\n        {\n            ColNames.year: F.year(ColNames.timestamp).cast(\"smallint\"),\n            ColNames.month: F.month(ColNames.timestamp).cast(\"tinyint\"),\n            ColNames.day: F.dayofmonth(ColNames.timestamp).cast(\"tinyint\"),\n        }\n    )\n\n    # Join events with topology data to enable checking unique locations and travelled distances\n    df_events = df_events.join(\n        df_network.select(\n            [\n                F.col(ColNames.cell_id),\n                F.col(ColNames.latitude).alias(\"cell_lat\"),\n                F.col(ColNames.longitude).alias(\"cell_lon\"),\n            ]\n        ),\n        on=ColNames.cell_id,\n        how=\"left\",\n    )\n\n    # Use latitude and longitude if they exist, otherwise use cells location\n    df_events = df_events.withColumn(ColNames.latitude, F.coalesce(ColNames.latitude, \"cell_lat\"))\n    df_events = df_events.withColumn(ColNames.longitude, F.coalesce(ColNames.longitude, \"cell_lon\"))\n\n    # Add timestamp and location of next record\n    window = Window.partitionBy(*[ColNames.user_id_modulo, ColNames.user_id]).orderBy(ColNames.timestamp)\n    df_events = df_events.withColumn(\n        f\"next_{ColNames.timestamp}\",\n        F.lead(F.col(ColNames.timestamp), 1).over(window),\n    )\n    df_events = df_events.withColumn(\n        f\"next_{ColNames.latitude}\",\n        F.lead(F.col(ColNames.latitude), 1).over(window),\n    )\n    df_events = df_events.withColumn(\n        f\"next_{ColNames.longitude}\",\n        F.lead(F.col(ColNames.longitude), 1).over(window),\n    )\n\n    # Calculate time gap to next event\n    df_events = df_events.withColumn(\n        \"time_gap_s\",\n        F.unix_timestamp(f\"next_{ColNames.timestamp}\") - F.unix_timestamp(ColNames.timestamp),\n    )\n\n    # Calculate the distance between current and next event\n    # TODO: check if this is the correct way to calculate, got varying results\n    # There are many ways to calculate distance between points and all of them give different results\n    df_events = df_events.withColumn(\n        \"source_geom\",\n        STF.ST_SetSRID(\n            STC.ST_Point(df_events[ColNames.latitude], df_events[ColNames.longitude]),\n            4326,\n        ),\n    )\n    df_events = df_events.withColumn(\n        \"destination_geom\",\n        STF.ST_SetSRID(\n            STC.ST_Point(\n                df_events[f\"next_{ColNames.latitude}\"],\n                df_events[f\"next_{ColNames.longitude}\"],\n            ),\n            4326,\n        ),\n    )\n    df_events = df_events.withColumn(\n        \"distance\",\n        STF.ST_DistanceSphere(df_events[\"source_geom\"], df_events[\"destination_geom\"]),\n    )\n\n    return df_events\n</code></pre>"},{"location":"reference/components/execution/estimation/","title":"estimation","text":""},{"location":"reference/components/execution/estimation/estimation/","title":"estimation","text":"<p>Module that implements the Estimation component</p>"},{"location":"reference/components/execution/estimation/estimation/#components.execution.estimation.estimation.Estimation","title":"<code>Estimation</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class responsible for the estimation of the actual population volumes of different indicators, starting from values referring to devices, then 1) applying a deduplication factor to account for people carrying multiple devices and 2) translating the observed MNO population to the target population.</p> <p>At this moment, both the deduplication factor and MNO-&gt;target population factor are constant across time and space, i.e. same factor is used for all values.</p> Source code in <code>multimno/components/execution/estimation/estimation.py</code> <pre><code>class Estimation(Component):\n    \"\"\"\n    Class responsible for the estimation of the actual population volumes of different indicators, starting from\n    values referring to devices, then 1) applying a deduplication factor to account for people carrying multiple devices\n    and 2) translating the observed MNO population to the target population.\n\n    At this moment, both the deduplication factor and MNO-&gt;target population factor are constant across time and space,\n    i.e. same factor is used for all values.\n    \"\"\"\n\n    COMPONENT_ID = \"Estimation\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.current_component_id: str = None\n        self.target_column: str = None\n        self.deduplication_factor: float = None\n        self.mno_to_target_population_factor: float = None\n        self.zoning_dataset: str = None\n        self.levels: list[int] = None\n\n    def _configure_execution(self, target_component_id: str, class_mapping: dict):\n        \"\"\"Method  that sets up the input and output data objects based on the target input data to be processed,\n        and whether to remain at the INSPIRE 100m grid level or use a proper zoning system\n\n        Args:\n            target_component_id (str): COMPONENT_ID attribute of the target\n            class_mapping (dict): dict with constants for the appropiate target\n\n        Raises:\n            ValueError: input data path does not exist\n        \"\"\"\n        input_do_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, class_mapping[target_component_id][\"input_path_config_key\"]\n        )\n        output_do_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, class_mapping[target_component_id][\"output_path_config_key\"]\n        )\n\n        if not check_if_data_path_exists(self.spark, input_do_path):\n            self.logger.warning(f\"Expected path {input_do_path} to exist but it does not\")\n            raise ValueError(\n                f\"Invalid path for {class_mapping[target_component_id]['constructor'].ID}: {input_do_path}\"\n            )\n\n        clear_destination_directory = self.config.getboolean(\n            f\"{self.COMPONENT_ID}.{target_component_id}\", \"clear_destination_directory\"\n        )\n        if clear_destination_directory:\n            delete_file_or_folder(self.spark, output_do_path)\n\n        input_do = class_mapping[target_component_id][\"constructor\"](self.spark, input_do_path)\n        output_do = CLASS_MAPPING[target_component_id][\"constructor\"](self.spark, output_do_path)\n\n        self.data_objects[target_component_id] = {\"input\": input_do, \"output\": output_do}\n\n    def initalize_data_objects(self):\n\n        self.execute_present_population = self.config.getboolean(self.COMPONENT_ID, \"present_population_execution\")\n        self.execute_usual_environment = self.config.getboolean(self.COMPONENT_ID, \"usual_environment_execution\")\n        self.execute_internal_migration = self.config.getboolean(self.COMPONENT_ID, \"internal_migration_execution\")\n\n        self.data_objects = {}\n\n        if self.execute_present_population:\n            target_component_id = PresentPopulationEstimation.COMPONENT_ID\n            zoning_dataset = self.config.get(f\"{self.COMPONENT_ID}.{target_component_id}\", \"zoning_dataset_id\")\n            if zoning_dataset == ReservedDatasetIDs.INSPIRE_100m:\n                self._configure_execution(target_component_id, CLASS_MAPPING_100m)\n            else:\n                self._configure_execution(target_component_id, CLASS_MAPPING)\n\n        if self.execute_usual_environment:\n            target_component_id = UsualEnvironmentAggregation.COMPONENT_ID\n            zoning_dataset = self.config.get(f\"{self.COMPONENT_ID}.{target_component_id}\", \"zoning_dataset_id\")\n            if zoning_dataset == ReservedDatasetIDs.INSPIRE_100m:\n                self._configure_execution(target_component_id, CLASS_MAPPING_100m)\n            else:\n                self._configure_execution(target_component_id, CLASS_MAPPING)\n\n        if self.execute_internal_migration:\n            target_component_id = InternalMigration.COMPONENT_ID\n            zoning_dataset = self.config.get(f\"{self.COMPONENT_ID}.{target_component_id}\", \"zoning_dataset_id\")\n            if zoning_dataset == ReservedDatasetIDs.INSPIRE_100m:\n                raise NotImplementedError(\"Estimation process for InternalMigration and INSPIRE 100m not implemented\")\n            else:\n                self._configure_execution(target_component_id, CLASS_MAPPING)\n\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        if len(self.data_objects) == 0:\n            self.logger.info(\"No execution requested in config file -- finishing without performing any operation...\")\n            self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n            return\n\n        for component_id in self.data_objects:\n            self.logger.info(f\"Working on {self.COMPONENT_ID}.{component_id}...\")\n            self.current_component_id = component_id\n            self.target_column = CLASS_MAPPING[self.current_component_id][\"target_column\"]\n            self.deduplication_factor = self.config.getfloat(\n                f\"{self.COMPONENT_ID}.{component_id}\", \"deduplication_factor\"\n            )\n            self.mno_to_target_population_factor = self.config.getfloat(\n                f\"{self.COMPONENT_ID}.{component_id}\", \"mno_to_target_population_factor\"\n            )\n\n            self.zoning_dataset = self.config.get(\n                f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"zoning_dataset_id\"\n            )\n            if self.zoning_dataset in ReservedDatasetIDs():\n                self.logger.info(\n                    f\"{component_id}: zoning_dataset_id is {self.zoning_dataset} -- forcing hierarchical levels to `[1]`\"\n                )\n                self.levels = [1]\n            else:\n                levels = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"hierarchical_levels\")\n                self.levels = list(int(x.strip()) for x in levels.split(\",\"))\n\n            self.input_data_objects = {\n                self.data_objects[component_id][\"input\"].ID: self.data_objects[component_id][\"input\"]\n            }\n\n            self.output_data_objects = {\n                self.data_objects[component_id][\"output\"].ID: self.data_objects[component_id][\"output\"]\n            }\n\n            self.read()\n            self.transform()\n            self.write()\n\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def filter_dataframe(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Filtering function that takes the partitions of the dataframe specified via configuration file\n\n        Args:\n            df (DataFrame): original DataFrame\n\n        Raises:\n            ValueError: if `season` value in configuration file is not one of allowed values\n\n        Returns:\n            DataFrame: filtered DataFrame\n        \"\"\"\n        # TODO: move config reading and validation to __init__ (?)\n        if self.current_component_id == PresentPopulationEstimation.COMPONENT_ID:\n            start_date = dt.datetime.strptime(\n                self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"start_date\"), \"%Y-%m-%d\"\n            )\n            end_date = dt.datetime.strptime(\n                self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"end_date\"), \"%Y-%m-%d\"\n            )\n\n            df = df.where(F.make_date(ColNames.year, ColNames.month, ColNames.day).between(start_date, end_date))\n\n            if self.zoning_dataset == ReservedDatasetIDs.INSPIRE_100m:\n                df = df.withColumn(ColNames.dataset_id, F.lit(ReservedDatasetIDs.INSPIRE_100m)).withColumn(\n                    ColNames.level, F.lit(1)\n                )\n                grid_gen = InspireGridGenerator(spark=self.spark)\n                df = grid_gen.convert_internal_id_to_inspire_specs(df, resolution=100, grid_id_col=ColNames.grid_id)\n                df = df.withColumnRenamed(ColNames.grid_id, ColNames.zone_id)\n            else:\n                df = df.where(F.col(ColNames.dataset_id) == F.lit(self.zoning_dataset)).where(\n                    F.col(ColNames.level).isin(self.levels)\n                )\n\n        if self.current_component_id == UsualEnvironmentAggregation.COMPONENT_ID:\n            labels = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"labels\")\n            labels = list(x.strip() for x in labels.split(\",\"))\n\n            start_date = dt.datetime.strptime(\n                self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"start_month\"), \"%Y-%m\"\n            )\n            end_date = dt.datetime.strptime(\n                self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"end_month\"), \"%Y-%m\"\n            )\n            end_date = end_date + dt.timedelta(days=cal.monthrange(end_date.year, end_date.month)[1] - 1)\n            season = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"season\")\n            if season not in SEASONS:\n                raise ValueError(f\"Unknown season {season} -- valid values are {SEASONS}\")\n\n            df = (\n                df.where(F.col(ColNames.label).isin(labels))\n                .where(F.col(ColNames.start_date) == start_date)\n                .where(F.col(ColNames.end_date) == end_date)\n                .where(F.col(ColNames.season) == season)\n            )\n            if self.zoning_dataset == ReservedDatasetIDs.INSPIRE_100m:\n                df = df.withColumn(ColNames.dataset_id, F.lit(ReservedDatasetIDs.INSPIRE_100m)).withColumn(\n                    ColNames.level, F.lit(1)\n                )\n\n                grid_gen = InspireGridGenerator(spark=self.spark)\n                df = grid_gen.convert_internal_id_to_inspire_specs(df, resolution=100, grid_id_col=ColNames.grid_id)\n                df = df.withColumnRenamed(ColNames.grid_id, ColNames.zone_id)\n            else:\n                df = df.where(F.col(ColNames.dataset_id) == self.zoning_dataset).where(\n                    F.col(ColNames.level).isin(self.levels)\n                )\n\n        if self.current_component_id == InternalMigration.COMPONENT_ID:\n            start_date_prev = dt.datetime.strptime(\n                self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"start_month_previous\"), \"%Y-%m\"\n            )\n            end_date_prev = dt.datetime.strptime(\n                self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"end_month_previous\"), \"%Y-%m\"\n            )\n            end_date_prev = end_date_prev + dt.timedelta(\n                days=cal.monthrange(end_date_prev.year, end_date_prev.month)[1] - 1\n            )\n            season_prev = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"season_previous\")\n            if season_prev not in SEASONS:\n                raise ValueError(f\"Unknown season {season_prev} -- valid values are {SEASONS}\")\n\n            start_date_new = dt.datetime.strptime(\n                self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"start_month_new\"), \"%Y-%m\"\n            )\n            end_date_new = dt.datetime.strptime(\n                self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"end_month_new\"), \"%Y-%m\"\n            )\n            end_date_new = end_date_new + dt.timedelta(\n                days=cal.monthrange(end_date_new.year, end_date_new.month)[1] - 1\n            )\n            season_new = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"season_new\")\n            if season_new not in SEASONS:\n                raise ValueError(f\"Unknown season {season_new} -- valid values are {SEASONS}\")\n\n            df = (\n                df.where(F.col(ColNames.dataset_id) == self.zoning_dataset)\n                .where(F.col(ColNames.level).isin(self.levels))\n                .where(F.col(ColNames.start_date_previous) == start_date_prev)\n                .where(F.col(ColNames.end_date_previous) == end_date_prev)\n                .where(F.col(ColNames.season_previous) == season_prev)\n                .where(F.col(ColNames.start_date_new) == start_date_new)\n                .where(F.col(ColNames.end_date_new) == end_date_new)\n                .where(F.col(ColNames.season_new) == season_new)\n            )\n\n        return df\n\n    def apply_deduplication_factor(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Applies the device deduplication factor to the target value column. Currently applies a constant factor\n        to all rows/records.\n\n        Args:\n            df (DataFrame): original DataFrame containing the target column\n\n        Returns:\n            DataFrame: DataFrame after applying the deduplication factor\n        \"\"\"\n        df = df.withColumn(\n            self.target_column,\n            F.col(self.target_column)\n            * F.lit(self.deduplication_factor).cast(\n                CLASS_MAPPING[self.current_component_id][\"constructor\"].SCHEMA[self.target_column].dataType\n            ),\n        )\n        return df\n\n    def apply_mno_to_target_population_factor(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Applies the MNO to target population factor to the target value column. Currently applies a constant factor\n        to all rows/records\n\n        Args:\n            df (DataFrame): original DataFrame containing the target column\n\n        Returns:\n            DataFrame: DataFrame after applying the MNO to target population factor\n        \"\"\"\n        df = df.withColumn(\n            self.target_column,\n            F.col(self.target_column)\n            * F.lit(self.mno_to_target_population_factor).cast(\n                CLASS_MAPPING[self.current_component_id][\"constructor\"].SCHEMA[self.target_column].dataType\n            ),\n        )\n        return df\n\n    def transform(self):\n        if self.zoning_dataset == ReservedDatasetIDs.INSPIRE_100m:\n            constructor_id = CLASS_MAPPING_100m[self.current_component_id][\"constructor\"].ID\n        else:\n            constructor_id = CLASS_MAPPING[self.current_component_id][\"constructor\"].ID\n\n        df = self.input_data_objects[constructor_id].df\n        # First, filter based on partition and dates\n        df = self.filter_dataframe(df)\n\n        # Apply deduplication factor\n        df = self.apply_deduplication_factor(df)\n\n        # Apply MNO to Target population factor\n        df = self.apply_mno_to_target_population_factor(df)\n\n        self.output_data_objects[CLASS_MAPPING[self.current_component_id][\"constructor\"].ID].df = df\n</code></pre>"},{"location":"reference/components/execution/estimation/estimation/#components.execution.estimation.estimation.Estimation._configure_execution","title":"<code>_configure_execution(target_component_id, class_mapping)</code>","text":"<p>Method  that sets up the input and output data objects based on the target input data to be processed, and whether to remain at the INSPIRE 100m grid level or use a proper zoning system</p> <p>Parameters:</p> Name Type Description Default <code>target_component_id</code> <code>str</code> <p>COMPONENT_ID attribute of the target</p> required <code>class_mapping</code> <code>dict</code> <p>dict with constants for the appropiate target</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>input data path does not exist</p> Source code in <code>multimno/components/execution/estimation/estimation.py</code> <pre><code>def _configure_execution(self, target_component_id: str, class_mapping: dict):\n    \"\"\"Method  that sets up the input and output data objects based on the target input data to be processed,\n    and whether to remain at the INSPIRE 100m grid level or use a proper zoning system\n\n    Args:\n        target_component_id (str): COMPONENT_ID attribute of the target\n        class_mapping (dict): dict with constants for the appropiate target\n\n    Raises:\n        ValueError: input data path does not exist\n    \"\"\"\n    input_do_path = self.config.get(\n        CONFIG_SILVER_PATHS_KEY, class_mapping[target_component_id][\"input_path_config_key\"]\n    )\n    output_do_path = self.config.get(\n        CONFIG_SILVER_PATHS_KEY, class_mapping[target_component_id][\"output_path_config_key\"]\n    )\n\n    if not check_if_data_path_exists(self.spark, input_do_path):\n        self.logger.warning(f\"Expected path {input_do_path} to exist but it does not\")\n        raise ValueError(\n            f\"Invalid path for {class_mapping[target_component_id]['constructor'].ID}: {input_do_path}\"\n        )\n\n    clear_destination_directory = self.config.getboolean(\n        f\"{self.COMPONENT_ID}.{target_component_id}\", \"clear_destination_directory\"\n    )\n    if clear_destination_directory:\n        delete_file_or_folder(self.spark, output_do_path)\n\n    input_do = class_mapping[target_component_id][\"constructor\"](self.spark, input_do_path)\n    output_do = CLASS_MAPPING[target_component_id][\"constructor\"](self.spark, output_do_path)\n\n    self.data_objects[target_component_id] = {\"input\": input_do, \"output\": output_do}\n</code></pre>"},{"location":"reference/components/execution/estimation/estimation/#components.execution.estimation.estimation.Estimation.apply_deduplication_factor","title":"<code>apply_deduplication_factor(df)</code>","text":"<p>Applies the device deduplication factor to the target value column. Currently applies a constant factor to all rows/records.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>original DataFrame containing the target column</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame after applying the deduplication factor</p> Source code in <code>multimno/components/execution/estimation/estimation.py</code> <pre><code>def apply_deduplication_factor(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Applies the device deduplication factor to the target value column. Currently applies a constant factor\n    to all rows/records.\n\n    Args:\n        df (DataFrame): original DataFrame containing the target column\n\n    Returns:\n        DataFrame: DataFrame after applying the deduplication factor\n    \"\"\"\n    df = df.withColumn(\n        self.target_column,\n        F.col(self.target_column)\n        * F.lit(self.deduplication_factor).cast(\n            CLASS_MAPPING[self.current_component_id][\"constructor\"].SCHEMA[self.target_column].dataType\n        ),\n    )\n    return df\n</code></pre>"},{"location":"reference/components/execution/estimation/estimation/#components.execution.estimation.estimation.Estimation.apply_mno_to_target_population_factor","title":"<code>apply_mno_to_target_population_factor(df)</code>","text":"<p>Applies the MNO to target population factor to the target value column. Currently applies a constant factor to all rows/records</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>original DataFrame containing the target column</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame after applying the MNO to target population factor</p> Source code in <code>multimno/components/execution/estimation/estimation.py</code> <pre><code>def apply_mno_to_target_population_factor(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Applies the MNO to target population factor to the target value column. Currently applies a constant factor\n    to all rows/records\n\n    Args:\n        df (DataFrame): original DataFrame containing the target column\n\n    Returns:\n        DataFrame: DataFrame after applying the MNO to target population factor\n    \"\"\"\n    df = df.withColumn(\n        self.target_column,\n        F.col(self.target_column)\n        * F.lit(self.mno_to_target_population_factor).cast(\n            CLASS_MAPPING[self.current_component_id][\"constructor\"].SCHEMA[self.target_column].dataType\n        ),\n    )\n    return df\n</code></pre>"},{"location":"reference/components/execution/estimation/estimation/#components.execution.estimation.estimation.Estimation.filter_dataframe","title":"<code>filter_dataframe(df)</code>","text":"<p>Filtering function that takes the partitions of the dataframe specified via configuration file</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>original DataFrame</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>season</code> value in configuration file is not one of allowed values</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>filtered DataFrame</p> Source code in <code>multimno/components/execution/estimation/estimation.py</code> <pre><code>def filter_dataframe(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Filtering function that takes the partitions of the dataframe specified via configuration file\n\n    Args:\n        df (DataFrame): original DataFrame\n\n    Raises:\n        ValueError: if `season` value in configuration file is not one of allowed values\n\n    Returns:\n        DataFrame: filtered DataFrame\n    \"\"\"\n    # TODO: move config reading and validation to __init__ (?)\n    if self.current_component_id == PresentPopulationEstimation.COMPONENT_ID:\n        start_date = dt.datetime.strptime(\n            self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"start_date\"), \"%Y-%m-%d\"\n        )\n        end_date = dt.datetime.strptime(\n            self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"end_date\"), \"%Y-%m-%d\"\n        )\n\n        df = df.where(F.make_date(ColNames.year, ColNames.month, ColNames.day).between(start_date, end_date))\n\n        if self.zoning_dataset == ReservedDatasetIDs.INSPIRE_100m:\n            df = df.withColumn(ColNames.dataset_id, F.lit(ReservedDatasetIDs.INSPIRE_100m)).withColumn(\n                ColNames.level, F.lit(1)\n            )\n            grid_gen = InspireGridGenerator(spark=self.spark)\n            df = grid_gen.convert_internal_id_to_inspire_specs(df, resolution=100, grid_id_col=ColNames.grid_id)\n            df = df.withColumnRenamed(ColNames.grid_id, ColNames.zone_id)\n        else:\n            df = df.where(F.col(ColNames.dataset_id) == F.lit(self.zoning_dataset)).where(\n                F.col(ColNames.level).isin(self.levels)\n            )\n\n    if self.current_component_id == UsualEnvironmentAggregation.COMPONENT_ID:\n        labels = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"labels\")\n        labels = list(x.strip() for x in labels.split(\",\"))\n\n        start_date = dt.datetime.strptime(\n            self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"start_month\"), \"%Y-%m\"\n        )\n        end_date = dt.datetime.strptime(\n            self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"end_month\"), \"%Y-%m\"\n        )\n        end_date = end_date + dt.timedelta(days=cal.monthrange(end_date.year, end_date.month)[1] - 1)\n        season = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"season\")\n        if season not in SEASONS:\n            raise ValueError(f\"Unknown season {season} -- valid values are {SEASONS}\")\n\n        df = (\n            df.where(F.col(ColNames.label).isin(labels))\n            .where(F.col(ColNames.start_date) == start_date)\n            .where(F.col(ColNames.end_date) == end_date)\n            .where(F.col(ColNames.season) == season)\n        )\n        if self.zoning_dataset == ReservedDatasetIDs.INSPIRE_100m:\n            df = df.withColumn(ColNames.dataset_id, F.lit(ReservedDatasetIDs.INSPIRE_100m)).withColumn(\n                ColNames.level, F.lit(1)\n            )\n\n            grid_gen = InspireGridGenerator(spark=self.spark)\n            df = grid_gen.convert_internal_id_to_inspire_specs(df, resolution=100, grid_id_col=ColNames.grid_id)\n            df = df.withColumnRenamed(ColNames.grid_id, ColNames.zone_id)\n        else:\n            df = df.where(F.col(ColNames.dataset_id) == self.zoning_dataset).where(\n                F.col(ColNames.level).isin(self.levels)\n            )\n\n    if self.current_component_id == InternalMigration.COMPONENT_ID:\n        start_date_prev = dt.datetime.strptime(\n            self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"start_month_previous\"), \"%Y-%m\"\n        )\n        end_date_prev = dt.datetime.strptime(\n            self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"end_month_previous\"), \"%Y-%m\"\n        )\n        end_date_prev = end_date_prev + dt.timedelta(\n            days=cal.monthrange(end_date_prev.year, end_date_prev.month)[1] - 1\n        )\n        season_prev = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"season_previous\")\n        if season_prev not in SEASONS:\n            raise ValueError(f\"Unknown season {season_prev} -- valid values are {SEASONS}\")\n\n        start_date_new = dt.datetime.strptime(\n            self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"start_month_new\"), \"%Y-%m\"\n        )\n        end_date_new = dt.datetime.strptime(\n            self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"end_month_new\"), \"%Y-%m\"\n        )\n        end_date_new = end_date_new + dt.timedelta(\n            days=cal.monthrange(end_date_new.year, end_date_new.month)[1] - 1\n        )\n        season_new = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"season_new\")\n        if season_new not in SEASONS:\n            raise ValueError(f\"Unknown season {season_new} -- valid values are {SEASONS}\")\n\n        df = (\n            df.where(F.col(ColNames.dataset_id) == self.zoning_dataset)\n            .where(F.col(ColNames.level).isin(self.levels))\n            .where(F.col(ColNames.start_date_previous) == start_date_prev)\n            .where(F.col(ColNames.end_date_previous) == end_date_prev)\n            .where(F.col(ColNames.season_previous) == season_prev)\n            .where(F.col(ColNames.start_date_new) == start_date_new)\n            .where(F.col(ColNames.end_date_new) == end_date_new)\n            .where(F.col(ColNames.season_new) == season_new)\n        )\n\n    return df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/","title":"event_cleaning","text":""},{"location":"reference/components/execution/event_cleaning/event_cleaning/","title":"event_cleaning","text":"<p>Module that cleans RAW MNO Event data.</p>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning","title":"<code>EventCleaning</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class that cleans MNO Event data</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>class EventCleaning(Component):\n    \"\"\"\n    Class that cleans MNO Event data\n    \"\"\"\n\n    COMPONENT_ID = \"EventCleaning\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.do_same_location_deduplication = self.config.getboolean(\n            self.COMPONENT_ID,\n            \"do_same_location_deduplication\",\n            fallback=False,\n        )\n\n        self.do_bounding_box_filtering = self.config.getboolean(\n            self.COMPONENT_ID,\n            \"do_bounding_box_filtering\",\n            fallback=False,\n        )\n\n        self.do_cell_id_length_filtering = self.config.getboolean(\n            self.COMPONENT_ID,\n            \"do_cell_id_length_filtering\",\n            fallback=False,\n        )\n\n        self.do_timestamp_conversion_to_utc = self.config.getboolean(\n            self.COMPONENT_ID,\n            \"do_timestamp_conversion_to_utc\",\n            fallback=False,\n        )\n\n        self.timestamp_format = self.config.get(self.COMPONENT_ID, \"timestamp_format\")\n        self.local_timezone = self.config.get(GENERAL_CONFIG_KEY, \"local_timezone\")\n        self.local_mcc = self.config.getint(GENERAL_CONFIG_KEY, \"local_mcc\")\n        self.bbox = self.config.geteval(self.COMPONENT_ID, \"bounding_box\")\n\n    def initalize_data_objects(self):\n        # Input\n        self.bronze_event_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"event_data_bronze\")\n\n        self.clear_destination_directory = self.config.getboolean(self.COMPONENT_ID, \"clear_destination_directory\")\n        self.number_of_partitions = self.config.get(self.COMPONENT_ID, \"number_of_partitions\")\n\n        self.data_period_start = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n        self.data_period_end = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n\n        # List of datetime.dates for which to perform cleaning of the raw network input data.\n        # Notice that date_period_end is included\n        self.data_period_dates = [\n            self.data_period_start + datetime.timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n        # Create all input data objects\n        self.input_event_data_objects = []\n        self.dates_to_process = []\n        for date in self.data_period_dates:\n            path = f\"{self.bronze_event_path}/year={date.year}/month={date.month}/day={date.day}\"\n            if check_if_data_path_exists(self.spark, path):\n                self.dates_to_process.append(date)\n                self.input_event_data_objects.append(BronzeEventDataObject(self.spark, path))\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")\n\n        # Output\n        self.output_data_objects = {}\n\n        outputs = {\n            \"event_syntactic_quality_metrics_by_column\": SilverEventDataSyntacticQualityMetricsByColumn,\n            \"event_syntactic_quality_metrics_frequency_distribution\": SilverEventDataSyntacticQualityMetricsFrequencyDistribution,\n            \"event_data_silver\": SilverEventDataObject,\n        }\n\n        for key, value in outputs.items():\n            path = self.config.get(CONFIG_SILVER_PATHS_KEY, key)\n            if self.clear_destination_directory:\n                delete_file_or_folder(self.spark, path)\n            self.output_data_objects[value.ID] = value(self.spark, path)\n\n    def read(self):\n        self.current_input_do.read()\n\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        for input_do, current_date in zip(self.input_event_data_objects, self.dates_to_process):\n            self.current_date = current_date\n            self.logger.info(f\"Reading from path {input_do.default_path}\")\n            self.current_input_do = input_do\n            self.read()\n            self.transform()  # Transforms the input_df\n            self.write()\n            # after each chunk processing clear all Cache to free memory and disk\n            self.spark.catalog.clearCache()\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n        df_events = self.current_input_do.df\n\n        # check nulls\n        check_null_columns = [ColNames.user_id, ColNames.timestamp, ColNames.mcc, ColNames.mnc]\n\n        df_events = self.flag_nulls(df_events, check_null_columns)\n\n        # add user_id_modulo\n        # The concept of device demultiplex is implemented here\n        # 1) Creates a modulo column, 2) repartitions according to it 3) sorts data within partitions\n\n        df_events = self.calculate_user_id_modulo(df_events, self.number_of_partitions)\n        df_events = df_events.repartition(ColNames.user_id_modulo)\n        df_events = df_events.sortWithinPartitions(ColNames.user_id, ColNames.timestamp)\n\n        # parse timestamp\n        df_events = self.parse_timestamp(df_events, self.timestamp_format)\n\n        # flag out of range limits\n        cols_range_limits = {\n            ColNames.mcc: [100, 999],\n            ColNames.plmn: [10000, 99999],\n        }\n\n        df_events = self.flag_out_of_range_limits(df_events, cols_range_limits)\n\n        if self.do_bounding_box_filtering:\n            cols_range_limits = {\n                ColNames.latitude: [self.bbox[1], self.bbox[3]],\n                ColNames.longitude: [self.bbox[0], self.bbox[2]],\n            }\n            df_events = self.flag_out_of_range_limits(df_events, cols_range_limits)\n\n        # flag out of length limits\n        cols_length_limits = {ColNames.mnc: [2, 3]}\n\n        df_events = self.flag_out_of_length_limits(df_events, cols_length_limits)\n\n        if self.do_cell_id_length_filtering:\n            cols_length_limits = {ColNames.cell_id: [14, 15]}\n            df_events = self.flag_out_of_length_limits(df_events, cols_length_limits)\n\n        # flag timestamp out of bounds\n        df_events = self.flag_timestamp_out_of_bounds(\n            df_events, self.current_date, self.current_date + datetime.timedelta(days=1)\n        )\n\n        # flag missing network info\n        df_events = self.flag_missing_mno_info(df_events)\n\n        # assign domain\n        df_events = self.assign_domain(df_events, self.local_mcc)\n\n        # flag missing location\n        df_events = self.flag_missing_location(df_events)\n\n        # flag same location duplicates\n        if self.do_same_location_deduplication:\n            df_events = self.flag_same_location_duplicates(df_events)\n\n        # add no error flag columns\n        df_events = self.add_no_error_flag_columns(df_events)\n\n        df_events.persist(StorageLevel.MEMORY_AND_DISK)\n\n        # get metrics by flag column\n        output_qa_by_column = self.get_metrics_by_flag_column(df_events)\n\n        output_qa_by_column = output_qa_by_column.withColumns(\n            {\n                ColNames.result_timestamp: F.lit(F.current_timestamp()),\n                ColNames.date: F.to_date(F.lit(self.current_date)),\n            }\n        )\n\n        output_qa_by_column = utils.apply_schema_casting(\n            output_qa_by_column, SilverEventDataSyntacticQualityMetricsByColumn.SCHEMA\n        )\n        self.output_data_objects[SilverEventDataSyntacticQualityMetricsByColumn.ID].df = output_qa_by_column\n\n        # get frequency metrics\n        frequency_metrics = self.get_frequency_metrics(df_events)\n\n        frequency_metrics = frequency_metrics.withColumns(\n            {\n                ColNames.result_timestamp: F.lit(F.current_timestamp()),\n                ColNames.date: F.to_date(F.lit(self.current_date)),\n            }\n        )\n\n        frequency_metrics = utils.apply_schema_casting(\n            frequency_metrics, SilverEventDataSyntacticQualityMetricsFrequencyDistribution.SCHEMA\n        )\n        self.output_data_objects[SilverEventDataSyntacticQualityMetricsFrequencyDistribution.ID].df = frequency_metrics\n\n        df_events = df_events.filter(F.col(\"to_preserve\"))\n\n        if self.do_timestamp_conversion_to_utc:\n            df_events = self.convert_to_utc(df_events, self.local_timezone)\n\n        df_events = df_events.withColumns(\n            {\n                ColNames.year: F.year(ColNames.timestamp).cast(\"smallint\"),\n                ColNames.month: F.month(ColNames.timestamp).cast(\"tinyint\"),\n                ColNames.day: F.dayofmonth(ColNames.timestamp).cast(\"tinyint\"),\n            }\n        )\n\n        df_events = utils.apply_schema_casting(df_events, SilverEventDataObject.SCHEMA)\n        df_events = df_events.repartition(*SilverEventDataObject.PARTITION_COLUMNS)\n\n        self.output_data_objects[SilverEventDataObject.ID].df = df_events\n\n    @staticmethod\n    def flag_nulls(\n        sdf: DataFrame,\n        filter_columns: List[str],\n    ) -&gt; DataFrame:\n        \"\"\"\n        Marks rows which include nulls in the filter columns\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): df events with possible nulls values\n            filter_columns (List[str], optional): columns to check for nulls\n        Returns:\n            pyspark.sql.dataframe.DataFrame: df where rows with null values in filter columns are flagged\n        \"\"\"\n\n        sdf = sdf.withColumns({f\"{col}_flag_{ErrorTypes.NULL_VALUE}\": F.col(col).isNull() for col in filter_columns})\n\n        return sdf\n\n    @staticmethod\n    def parse_timestamp(\n        sdf: DataFrame,\n        timestamp_format: str,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Based on config params timestampt format and input timezone\n        convert timestamp column from string to timestamp type.\n        Flag succesful timestamp transformations and errors.\n\n        Args:\n            sdf (pyspark.sql.dataframe.DataFrame): df with timestamp column\n            timestamp_format (str): expected string format to use in time conversion\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: df with time column parsed to timestamp\n        \"\"\"\n\n        sdf = sdf.withColumn(\n            f\"{ColNames.timestamp}_parsed\",\n            F.to_timestamp(ColNames.timestamp, timestamp_format),\n        )\n\n        sdf = sdf.withColumn(\n            f\"{ColNames.timestamp}_flag_{ErrorTypes.CANNOT_PARSE}\",\n            F.when(\n                F.col(ColNames.timestamp).isNotNull() &amp; F.col(f\"{ColNames.timestamp}_parsed\").isNull(),\n                F.lit(True),\n            ).otherwise(F.lit(False)),\n        )\n\n        sdf = sdf.withColumn(\n            ColNames.timestamp,\n            F.col(f\"{ColNames.timestamp}_parsed\"),\n        ).drop(f\"{ColNames.timestamp}_parsed\")\n\n        return sdf\n\n    @staticmethod\n    def flag_out_of_range_limits(\n        sdf: DataFrame,\n        cols_range_limits: Dict[str, List[int]],\n    ) -&gt; DataFrame:\n        \"\"\"\n        Checks if values in columns are within the specified range\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): df with columns to check\n            cols_range_limits (Dict[str, List[int]]): dictionary with column names as keys and lists of min and max values as values\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: df with columns checked for range limits\n        \"\"\"\n\n        for col_name, limits in cols_range_limits.items():\n            sdf = sdf.withColumn(\n                f\"{col_name}_flag_{ErrorTypes.OUT_OF_RANGE}\",\n                F.when(\n                    F.col(col_name).isNotNull() &amp; ~F.col(col_name).between(limits[0], limits[1]),\n                    F.lit(True),\n                ).otherwise(F.lit(False)),\n            )\n        return sdf\n\n    @staticmethod\n    def flag_out_of_length_limits(\n        sdf: DataFrame,\n        cols_length_limits: Dict[str, List[int]],\n    ) -&gt; DataFrame:\n        \"\"\"\n        Checks if values in columns are within the specified characters length\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): df with columns to check\n            cols_length_limits (Dict[str, List[int]]): dictionary with column names as keys and lists of min and max values as values\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: df with columns checked for range limits\n        \"\"\"\n\n        for col_name, limits in cols_length_limits.items():\n            sdf = sdf.withColumn(\n                f\"{col_name}_flag_{ErrorTypes.OUT_OF_RANGE}\",\n                F.when(\n                    F.col(col_name).isNotNull() &amp; ~F.length(F.col(col_name)).between(limits[0], limits[1]),\n                    F.lit(True),\n                ).otherwise(F.lit(False)),\n            )\n\n        return sdf\n\n    @staticmethod\n    def flag_timestamp_out_of_bounds(\n        sdf: DataFrame,\n        start_date: str,\n        end_date: str,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Flags rows with timestamps outside of the specified date bounds\n\n        Args:\n            sdf (pyspark.sql.dataframe.DataFrame): df with timestamp column\n            start_date (str): start date of the data\n            end_date (str): end date of the data\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: df with rows flagged if timestamp is out of bounds\n        \"\"\"\n\n        sdf = sdf.withColumn(\n            f\"{ColNames.timestamp}_flag_{ErrorTypes.OUT_OF_RANGE}\",\n            F.when(\n                F.col(f\"{ColNames.timestamp}\").isNotNull()\n                &amp; ~F.col(f\"{ColNames.timestamp}\").between(start_date, end_date),\n                F.lit(True),\n            ).otherwise(F.lit(False)),\n        )\n\n        return sdf\n\n    @staticmethod\n    def flag_missing_mno_info(\n        sdf: DataFrame,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Flags rows with missing mno network information\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): df with network columns\n            network_columns (List[str]): columns with network information\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: df with rows flagged if network info is missing\n        \"\"\"\n\n        sdf = sdf.withColumn(\n            f\"no_mno_info_flag_{ErrorTypes.NO_MNO_INFO}\",\n            F.when(\n                F.col(ColNames.mcc).isNull() &amp; F.col(ColNames.mnc).isNull() &amp; F.col(ColNames.plmn).isNull(),\n                F.lit(True),\n            ).otherwise(F.lit(False)),\n        )\n\n        return sdf\n\n    @staticmethod\n    def assign_domain(\n        sdf: DataFrame,\n        local_mcc: int,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Assigns domain to rows\n\n        Args:\n            sdf (pyspark.sql.dataframe.DataFrame): df with domain column\n            local_mcc (int): local_mcc value\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: df with domain column assigned\n        \"\"\"\n        sdf = sdf.withColumn(\n            ColNames.domain,\n            F.when(F.col(ColNames.plmn).isNotNull(), ColNames.outbound).otherwise(\n                F.when(F.col(ColNames.mcc) == local_mcc, ColNames.domestic).otherwise(ColNames.inbound)\n            ),\n        )\n        return sdf\n\n    @staticmethod\n    def flag_missing_location(\n        sdf: DataFrame,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Flags rows with missing location information\n\n        Args:\n            sdf (pyspark.sql.dataframe.DataFrame): df with location columns\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: df with rows flagged if location info is missing\n        \"\"\"\n\n        sdf = sdf.withColumn(\n            f\"no_location_flag_{ErrorTypes.NO_LOCATION_INFO}\",\n            F.when(\n                (\n                    (\n                        F.col(ColNames.latitude).isNull()\n                        &amp; F.col(ColNames.longitude).isNull()\n                        &amp; F.col(ColNames.cell_id).isNull()\n                        &amp; ((F.col(ColNames.domain) == ColNames.domestic) | (F.col(ColNames.domain) == ColNames.inbound))\n                    )\n                ),\n                F.lit(True),\n            ).otherwise(F.lit(False)),\n        )\n\n        return sdf\n\n    @staticmethod\n    def flag_same_location_duplicates(\n        df: DataFrame,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Flag rows that have identical records for\n        timestamp, cell_id, longitude, latitude, plmn and user_id.\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): df events with possible duplicates\n        Returns:\n            pyspark.sql.dataframe.DataFrame: df duplicate\n            in terms of user_id, cell_id, latitutde, longitude, plmn\n            and timestamp combination flagged\n        \"\"\"\n        window_spec = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id).orderBy(F.col(ColNames.timestamp))\n        # Use lag to get the previous row's values\n        df = df.withColumns(\n            {\n                \"prev_timestamp\": F.lag(ColNames.timestamp).over(window_spec),\n                \"prev_cell_id\": F.lag(ColNames.cell_id).over(window_spec),\n                \"prev_latitude\": F.lag(ColNames.latitude).over(window_spec),\n                \"prev_longitude\": F.lag(ColNames.longitude).over(window_spec),\n                \"prev_plmn\": F.lag(ColNames.plmn).over(window_spec),\n            }\n        )\n        # Flag the current row if the conditions are met\n        df = df.withColumn(\n            f\"duplicated_flag_{ErrorTypes.DUPLICATED}\",\n            F.when(\n                (F.col(ColNames.timestamp) == F.col(\"prev_timestamp\"))\n                &amp; (\n                    F.coalesce(F.col(ColNames.cell_id), F.lit(\"\")).cast(\"string\")\n                    == F.coalesce(F.col(\"prev_cell_id\"), F.lit(\"\")).cast(\"string\")\n                )\n                &amp; (\n                    F.coalesce(F.col(ColNames.latitude), F.lit(0.0)).cast(\"double\")\n                    == F.coalesce(F.col(\"prev_latitude\"), F.lit(0.0)).cast(\"double\")\n                )\n                &amp; (\n                    F.coalesce(F.col(ColNames.longitude), F.lit(0.0)).cast(\"double\")\n                    == F.coalesce(F.col(\"prev_longitude\"), F.lit(0.0)).cast(\"double\")\n                )\n                &amp; (\n                    F.coalesce(F.col(ColNames.plmn), F.lit(\"\")).cast(\"string\")\n                    == F.coalesce(F.col(\"prev_plmn\"), F.lit(\"\")).cast(\"string\")\n                ),\n                F.lit(True),\n            ).otherwise(F.lit(False)),\n        ).drop(\"prev_timestamp\", \"prev_cell_id\", \"prev_latitude\", \"prev_longitude\", \"prev_plmn\")\n\n        return df\n\n    @staticmethod\n    def add_no_error_flag_columns(sdf: DataFrame):\n        \"\"\"\n        Add columns with 'no error' flags for each column\n\n        Args:\n            sdf (pyspark.sql.dataframe.DataFrame): df with error flag columns\n\n        Returns:\n            List[str]: list of columns that are not flagged as errors\n        \"\"\"\n\n        flag_columns = [col for col in sdf.columns if \"flag\" in col]\n\n        base_columns = list(set(col.split(\"_flag\")[0] for col in flag_columns))\n\n        column_groups = dict()\n        for col in base_columns:\n            # for each auxiliar column\n            column_groups[col] = []\n            for cc in flag_columns:\n                # if it is related to the DO's column:\n                if col == cc.split(\"_flag\")[0]:\n                    column_groups[col].append(F.col(cc))\n\n        column_conditions = {col: reduce(lambda a, b: a | b, column_groups[col]) for col in column_groups}\n\n        field_without_errors = {col: ~column_conditions[col] for col in column_conditions}\n\n        sdf = sdf.withColumns(\n            {f\"{col}_flag_{ErrorTypes.NO_ERROR}\": field_without_errors[col] for col in field_without_errors}\n        )\n\n        sdf = sdf.withColumn(\n            \"to_preserve\", reduce(lambda a, b: a &amp; b, [field_without_errors[col] for col in field_without_errors])\n        )\n\n        return sdf\n\n    @staticmethod\n    def get_metrics_by_flag_column(sdf):\n        \"\"\"\n        Get the count of flagged errors for each column\n\n        Args:\n            sdf (pyspark.sql.dataframe.DataFrame): df with error columns\n\n        Returns:\n            dict: dictionary with column names as keys and error counts as values\n        \"\"\"\n        flag_columns = [col for col in sdf.columns if \"flag\" in col]\n\n        flag_counts = sdf.agg(*[F.sum(F.col(col).cast(\"int\")).alias(col) for col in flag_columns])\n\n        # Unpivot the true_counts DataFrame\n        metrics_sdf = flag_counts.unpivot([], flag_columns, \"flag_column\", ColNames.value)\n\n        metrics_sdf = (\n            metrics_sdf.withColumns(\n                {\n                    ColNames.variable: F.split(F.col(\"flag_column\"), \"_flag_\").getItem(0),\n                    ColNames.type_of_error: F.split(F.col(\"flag_column\"), \"_flag_\").getItem(1),\n                }\n            )\n            .drop(\"flag_column\")\n            .orderBy(ColNames.variable)\n        )\n\n        return metrics_sdf\n\n    @staticmethod\n    def get_frequency_metrics(sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Get total row counts per user_id and cell_id before\n        filtering and after filtering (rows without errors)\n\n        Args:\n            sdf (pyspark.sql.dataframe.DataFrame): df with columns to count\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: dataframe with frequency counts\n        \"\"\"\n\n        frequency_metrics = sdf.groupBy(ColNames.user_id, ColNames.cell_id).agg(\n            F.count(\"*\").alias(ColNames.initial_frequency),\n            F.sum(F.col(\"to_preserve\").cast(\"int\")).alias(ColNames.final_frequency),\n        )\n\n        return frequency_metrics\n\n    @staticmethod\n    def convert_to_utc(\n        sdf: DataFrame,\n        local_timezone: str,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Converts timestamp column to UTC timezone\n\n        Args:\n            sdf (pyspark.sql.dataframe.DataFrame): df with timestamp column\n            local_timezone (str): timezone of the input data\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: df with timestamp column converted to UTC\n        \"\"\"\n\n        sdf = sdf.withColumn(\n            ColNames.timestamp,\n            F.to_utc_timestamp(ColNames.timestamp, local_timezone),\n        )\n\n        return sdf\n\n    @staticmethod\n    def calculate_user_id_modulo(\n        df: DataFrame,\n        modulo_value: int,\n        hex_truncation_end: int = 12,\n    ) -&gt; DataFrame:\n        \"\"\"Calculates the extra column user_id_modulo, as the result of the modulo function\n        applied on the binary user id column. The modulo value will affect the number of\n        partitions in the final output.\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): df with user_id column\n            modulo_value (int): modulo value to be used when dividing user id.\n            hex_truncation_end (int): to which character truncate the hex, before sending it to conv function\n                and then to modulo. Anything upward of 13 is likely to result in distributional issues,\n                as the modulo value might not correspond to the number of final partitions.\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: dataframe with user_id_modulo column.\n        \"\"\"\n\n        # TODO make hex truncation (substring parameters) as configurable by user?\n\n        df = df.withColumn(\n            ColNames.user_id_modulo,\n            F.conv(\n                F.substring(F.hex(F.col(ColNames.user_id)), 1, hex_truncation_end),\n                16,\n                10,\n            ).cast(\"long\")\n            % F.lit(modulo_value).cast(\"bigint\"),\n        )\n\n        return df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.add_no_error_flag_columns","title":"<code>add_no_error_flag_columns(sdf)</code>  <code>staticmethod</code>","text":"<p>Add columns with 'no error' flags for each column</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>df with error flag columns</p> required <p>Returns:</p> Type Description <p>List[str]: list of columns that are not flagged as errors</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>@staticmethod\ndef add_no_error_flag_columns(sdf: DataFrame):\n    \"\"\"\n    Add columns with 'no error' flags for each column\n\n    Args:\n        sdf (pyspark.sql.dataframe.DataFrame): df with error flag columns\n\n    Returns:\n        List[str]: list of columns that are not flagged as errors\n    \"\"\"\n\n    flag_columns = [col for col in sdf.columns if \"flag\" in col]\n\n    base_columns = list(set(col.split(\"_flag\")[0] for col in flag_columns))\n\n    column_groups = dict()\n    for col in base_columns:\n        # for each auxiliar column\n        column_groups[col] = []\n        for cc in flag_columns:\n            # if it is related to the DO's column:\n            if col == cc.split(\"_flag\")[0]:\n                column_groups[col].append(F.col(cc))\n\n    column_conditions = {col: reduce(lambda a, b: a | b, column_groups[col]) for col in column_groups}\n\n    field_without_errors = {col: ~column_conditions[col] for col in column_conditions}\n\n    sdf = sdf.withColumns(\n        {f\"{col}_flag_{ErrorTypes.NO_ERROR}\": field_without_errors[col] for col in field_without_errors}\n    )\n\n    sdf = sdf.withColumn(\n        \"to_preserve\", reduce(lambda a, b: a &amp; b, [field_without_errors[col] for col in field_without_errors])\n    )\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.assign_domain","title":"<code>assign_domain(sdf, local_mcc)</code>  <code>staticmethod</code>","text":"<p>Assigns domain to rows</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>df with domain column</p> required <code>local_mcc</code> <code>int</code> <p>local_mcc value</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: df with domain column assigned</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>@staticmethod\ndef assign_domain(\n    sdf: DataFrame,\n    local_mcc: int,\n) -&gt; DataFrame:\n    \"\"\"\n    Assigns domain to rows\n\n    Args:\n        sdf (pyspark.sql.dataframe.DataFrame): df with domain column\n        local_mcc (int): local_mcc value\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: df with domain column assigned\n    \"\"\"\n    sdf = sdf.withColumn(\n        ColNames.domain,\n        F.when(F.col(ColNames.plmn).isNotNull(), ColNames.outbound).otherwise(\n            F.when(F.col(ColNames.mcc) == local_mcc, ColNames.domestic).otherwise(ColNames.inbound)\n        ),\n    )\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.calculate_user_id_modulo","title":"<code>calculate_user_id_modulo(df, modulo_value, hex_truncation_end=12)</code>  <code>staticmethod</code>","text":"<p>Calculates the extra column user_id_modulo, as the result of the modulo function applied on the binary user id column. The modulo value will affect the number of partitions in the final output.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>df with user_id column</p> required <code>modulo_value</code> <code>int</code> <p>modulo value to be used when dividing user id.</p> required <code>hex_truncation_end</code> <code>int</code> <p>to which character truncate the hex, before sending it to conv function and then to modulo. Anything upward of 13 is likely to result in distributional issues, as the modulo value might not correspond to the number of final partitions.</p> <code>12</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: dataframe with user_id_modulo column.</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>@staticmethod\ndef calculate_user_id_modulo(\n    df: DataFrame,\n    modulo_value: int,\n    hex_truncation_end: int = 12,\n) -&gt; DataFrame:\n    \"\"\"Calculates the extra column user_id_modulo, as the result of the modulo function\n    applied on the binary user id column. The modulo value will affect the number of\n    partitions in the final output.\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): df with user_id column\n        modulo_value (int): modulo value to be used when dividing user id.\n        hex_truncation_end (int): to which character truncate the hex, before sending it to conv function\n            and then to modulo. Anything upward of 13 is likely to result in distributional issues,\n            as the modulo value might not correspond to the number of final partitions.\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: dataframe with user_id_modulo column.\n    \"\"\"\n\n    # TODO make hex truncation (substring parameters) as configurable by user?\n\n    df = df.withColumn(\n        ColNames.user_id_modulo,\n        F.conv(\n            F.substring(F.hex(F.col(ColNames.user_id)), 1, hex_truncation_end),\n            16,\n            10,\n        ).cast(\"long\")\n        % F.lit(modulo_value).cast(\"bigint\"),\n    )\n\n    return df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.convert_to_utc","title":"<code>convert_to_utc(sdf, local_timezone)</code>  <code>staticmethod</code>","text":"<p>Converts timestamp column to UTC timezone</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>df with timestamp column</p> required <code>local_timezone</code> <code>str</code> <p>timezone of the input data</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: df with timestamp column converted to UTC</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>@staticmethod\ndef convert_to_utc(\n    sdf: DataFrame,\n    local_timezone: str,\n) -&gt; DataFrame:\n    \"\"\"\n    Converts timestamp column to UTC timezone\n\n    Args:\n        sdf (pyspark.sql.dataframe.DataFrame): df with timestamp column\n        local_timezone (str): timezone of the input data\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: df with timestamp column converted to UTC\n    \"\"\"\n\n    sdf = sdf.withColumn(\n        ColNames.timestamp,\n        F.to_utc_timestamp(ColNames.timestamp, local_timezone),\n    )\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.flag_missing_location","title":"<code>flag_missing_location(sdf)</code>  <code>staticmethod</code>","text":"<p>Flags rows with missing location information</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>df with location columns</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: df with rows flagged if location info is missing</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>@staticmethod\ndef flag_missing_location(\n    sdf: DataFrame,\n) -&gt; DataFrame:\n    \"\"\"\n    Flags rows with missing location information\n\n    Args:\n        sdf (pyspark.sql.dataframe.DataFrame): df with location columns\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: df with rows flagged if location info is missing\n    \"\"\"\n\n    sdf = sdf.withColumn(\n        f\"no_location_flag_{ErrorTypes.NO_LOCATION_INFO}\",\n        F.when(\n            (\n                (\n                    F.col(ColNames.latitude).isNull()\n                    &amp; F.col(ColNames.longitude).isNull()\n                    &amp; F.col(ColNames.cell_id).isNull()\n                    &amp; ((F.col(ColNames.domain) == ColNames.domestic) | (F.col(ColNames.domain) == ColNames.inbound))\n                )\n            ),\n            F.lit(True),\n        ).otherwise(F.lit(False)),\n    )\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.flag_missing_mno_info","title":"<code>flag_missing_mno_info(sdf)</code>  <code>staticmethod</code>","text":"<p>Flags rows with missing mno network information</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>df with network columns</p> required <code>network_columns</code> <code>List[str]</code> <p>columns with network information</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: df with rows flagged if network info is missing</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>@staticmethod\ndef flag_missing_mno_info(\n    sdf: DataFrame,\n) -&gt; DataFrame:\n    \"\"\"\n    Flags rows with missing mno network information\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): df with network columns\n        network_columns (List[str]): columns with network information\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: df with rows flagged if network info is missing\n    \"\"\"\n\n    sdf = sdf.withColumn(\n        f\"no_mno_info_flag_{ErrorTypes.NO_MNO_INFO}\",\n        F.when(\n            F.col(ColNames.mcc).isNull() &amp; F.col(ColNames.mnc).isNull() &amp; F.col(ColNames.plmn).isNull(),\n            F.lit(True),\n        ).otherwise(F.lit(False)),\n    )\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.flag_nulls","title":"<code>flag_nulls(sdf, filter_columns)</code>  <code>staticmethod</code>","text":"<p>Marks rows which include nulls in the filter columns</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>df events with possible nulls values</p> required <code>filter_columns</code> <code>List[str]</code> <p>columns to check for nulls</p> required <p>Returns:     pyspark.sql.dataframe.DataFrame: df where rows with null values in filter columns are flagged</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>@staticmethod\ndef flag_nulls(\n    sdf: DataFrame,\n    filter_columns: List[str],\n) -&gt; DataFrame:\n    \"\"\"\n    Marks rows which include nulls in the filter columns\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): df events with possible nulls values\n        filter_columns (List[str], optional): columns to check for nulls\n    Returns:\n        pyspark.sql.dataframe.DataFrame: df where rows with null values in filter columns are flagged\n    \"\"\"\n\n    sdf = sdf.withColumns({f\"{col}_flag_{ErrorTypes.NULL_VALUE}\": F.col(col).isNull() for col in filter_columns})\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.flag_out_of_length_limits","title":"<code>flag_out_of_length_limits(sdf, cols_length_limits)</code>  <code>staticmethod</code>","text":"<p>Checks if values in columns are within the specified characters length</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>df with columns to check</p> required <code>cols_length_limits</code> <code>Dict[str, List[int]]</code> <p>dictionary with column names as keys and lists of min and max values as values</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: df with columns checked for range limits</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>@staticmethod\ndef flag_out_of_length_limits(\n    sdf: DataFrame,\n    cols_length_limits: Dict[str, List[int]],\n) -&gt; DataFrame:\n    \"\"\"\n    Checks if values in columns are within the specified characters length\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): df with columns to check\n        cols_length_limits (Dict[str, List[int]]): dictionary with column names as keys and lists of min and max values as values\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: df with columns checked for range limits\n    \"\"\"\n\n    for col_name, limits in cols_length_limits.items():\n        sdf = sdf.withColumn(\n            f\"{col_name}_flag_{ErrorTypes.OUT_OF_RANGE}\",\n            F.when(\n                F.col(col_name).isNotNull() &amp; ~F.length(F.col(col_name)).between(limits[0], limits[1]),\n                F.lit(True),\n            ).otherwise(F.lit(False)),\n        )\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.flag_out_of_range_limits","title":"<code>flag_out_of_range_limits(sdf, cols_range_limits)</code>  <code>staticmethod</code>","text":"<p>Checks if values in columns are within the specified range</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>df with columns to check</p> required <code>cols_range_limits</code> <code>Dict[str, List[int]]</code> <p>dictionary with column names as keys and lists of min and max values as values</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: df with columns checked for range limits</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>@staticmethod\ndef flag_out_of_range_limits(\n    sdf: DataFrame,\n    cols_range_limits: Dict[str, List[int]],\n) -&gt; DataFrame:\n    \"\"\"\n    Checks if values in columns are within the specified range\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): df with columns to check\n        cols_range_limits (Dict[str, List[int]]): dictionary with column names as keys and lists of min and max values as values\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: df with columns checked for range limits\n    \"\"\"\n\n    for col_name, limits in cols_range_limits.items():\n        sdf = sdf.withColumn(\n            f\"{col_name}_flag_{ErrorTypes.OUT_OF_RANGE}\",\n            F.when(\n                F.col(col_name).isNotNull() &amp; ~F.col(col_name).between(limits[0], limits[1]),\n                F.lit(True),\n            ).otherwise(F.lit(False)),\n        )\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.flag_same_location_duplicates","title":"<code>flag_same_location_duplicates(df)</code>  <code>staticmethod</code>","text":"<p>Flag rows that have identical records for timestamp, cell_id, longitude, latitude, plmn and user_id. Args:     df (pyspark.sql.dataframe.DataFrame): df events with possible duplicates Returns:     pyspark.sql.dataframe.DataFrame: df duplicate     in terms of user_id, cell_id, latitutde, longitude, plmn     and timestamp combination flagged</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>@staticmethod\ndef flag_same_location_duplicates(\n    df: DataFrame,\n) -&gt; DataFrame:\n    \"\"\"\n    Flag rows that have identical records for\n    timestamp, cell_id, longitude, latitude, plmn and user_id.\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): df events with possible duplicates\n    Returns:\n        pyspark.sql.dataframe.DataFrame: df duplicate\n        in terms of user_id, cell_id, latitutde, longitude, plmn\n        and timestamp combination flagged\n    \"\"\"\n    window_spec = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id).orderBy(F.col(ColNames.timestamp))\n    # Use lag to get the previous row's values\n    df = df.withColumns(\n        {\n            \"prev_timestamp\": F.lag(ColNames.timestamp).over(window_spec),\n            \"prev_cell_id\": F.lag(ColNames.cell_id).over(window_spec),\n            \"prev_latitude\": F.lag(ColNames.latitude).over(window_spec),\n            \"prev_longitude\": F.lag(ColNames.longitude).over(window_spec),\n            \"prev_plmn\": F.lag(ColNames.plmn).over(window_spec),\n        }\n    )\n    # Flag the current row if the conditions are met\n    df = df.withColumn(\n        f\"duplicated_flag_{ErrorTypes.DUPLICATED}\",\n        F.when(\n            (F.col(ColNames.timestamp) == F.col(\"prev_timestamp\"))\n            &amp; (\n                F.coalesce(F.col(ColNames.cell_id), F.lit(\"\")).cast(\"string\")\n                == F.coalesce(F.col(\"prev_cell_id\"), F.lit(\"\")).cast(\"string\")\n            )\n            &amp; (\n                F.coalesce(F.col(ColNames.latitude), F.lit(0.0)).cast(\"double\")\n                == F.coalesce(F.col(\"prev_latitude\"), F.lit(0.0)).cast(\"double\")\n            )\n            &amp; (\n                F.coalesce(F.col(ColNames.longitude), F.lit(0.0)).cast(\"double\")\n                == F.coalesce(F.col(\"prev_longitude\"), F.lit(0.0)).cast(\"double\")\n            )\n            &amp; (\n                F.coalesce(F.col(ColNames.plmn), F.lit(\"\")).cast(\"string\")\n                == F.coalesce(F.col(\"prev_plmn\"), F.lit(\"\")).cast(\"string\")\n            ),\n            F.lit(True),\n        ).otherwise(F.lit(False)),\n    ).drop(\"prev_timestamp\", \"prev_cell_id\", \"prev_latitude\", \"prev_longitude\", \"prev_plmn\")\n\n    return df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.flag_timestamp_out_of_bounds","title":"<code>flag_timestamp_out_of_bounds(sdf, start_date, end_date)</code>  <code>staticmethod</code>","text":"<p>Flags rows with timestamps outside of the specified date bounds</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>df with timestamp column</p> required <code>start_date</code> <code>str</code> <p>start date of the data</p> required <code>end_date</code> <code>str</code> <p>end date of the data</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: df with rows flagged if timestamp is out of bounds</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>@staticmethod\ndef flag_timestamp_out_of_bounds(\n    sdf: DataFrame,\n    start_date: str,\n    end_date: str,\n) -&gt; DataFrame:\n    \"\"\"\n    Flags rows with timestamps outside of the specified date bounds\n\n    Args:\n        sdf (pyspark.sql.dataframe.DataFrame): df with timestamp column\n        start_date (str): start date of the data\n        end_date (str): end date of the data\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: df with rows flagged if timestamp is out of bounds\n    \"\"\"\n\n    sdf = sdf.withColumn(\n        f\"{ColNames.timestamp}_flag_{ErrorTypes.OUT_OF_RANGE}\",\n        F.when(\n            F.col(f\"{ColNames.timestamp}\").isNotNull()\n            &amp; ~F.col(f\"{ColNames.timestamp}\").between(start_date, end_date),\n            F.lit(True),\n        ).otherwise(F.lit(False)),\n    )\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.get_frequency_metrics","title":"<code>get_frequency_metrics(sdf)</code>  <code>staticmethod</code>","text":"<p>Get total row counts per user_id and cell_id before filtering and after filtering (rows without errors)</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>df with columns to count</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: dataframe with frequency counts</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>@staticmethod\ndef get_frequency_metrics(sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Get total row counts per user_id and cell_id before\n    filtering and after filtering (rows without errors)\n\n    Args:\n        sdf (pyspark.sql.dataframe.DataFrame): df with columns to count\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: dataframe with frequency counts\n    \"\"\"\n\n    frequency_metrics = sdf.groupBy(ColNames.user_id, ColNames.cell_id).agg(\n        F.count(\"*\").alias(ColNames.initial_frequency),\n        F.sum(F.col(\"to_preserve\").cast(\"int\")).alias(ColNames.final_frequency),\n    )\n\n    return frequency_metrics\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.get_metrics_by_flag_column","title":"<code>get_metrics_by_flag_column(sdf)</code>  <code>staticmethod</code>","text":"<p>Get the count of flagged errors for each column</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>df with error columns</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>dictionary with column names as keys and error counts as values</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>@staticmethod\ndef get_metrics_by_flag_column(sdf):\n    \"\"\"\n    Get the count of flagged errors for each column\n\n    Args:\n        sdf (pyspark.sql.dataframe.DataFrame): df with error columns\n\n    Returns:\n        dict: dictionary with column names as keys and error counts as values\n    \"\"\"\n    flag_columns = [col for col in sdf.columns if \"flag\" in col]\n\n    flag_counts = sdf.agg(*[F.sum(F.col(col).cast(\"int\")).alias(col) for col in flag_columns])\n\n    # Unpivot the true_counts DataFrame\n    metrics_sdf = flag_counts.unpivot([], flag_columns, \"flag_column\", ColNames.value)\n\n    metrics_sdf = (\n        metrics_sdf.withColumns(\n            {\n                ColNames.variable: F.split(F.col(\"flag_column\"), \"_flag_\").getItem(0),\n                ColNames.type_of_error: F.split(F.col(\"flag_column\"), \"_flag_\").getItem(1),\n            }\n        )\n        .drop(\"flag_column\")\n        .orderBy(ColNames.variable)\n    )\n\n    return metrics_sdf\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning/#components.execution.event_cleaning.event_cleaning.EventCleaning.parse_timestamp","title":"<code>parse_timestamp(sdf, timestamp_format)</code>  <code>staticmethod</code>","text":"<p>Based on config params timestampt format and input timezone convert timestamp column from string to timestamp type. Flag succesful timestamp transformations and errors.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>df with timestamp column</p> required <code>timestamp_format</code> <code>str</code> <p>expected string format to use in time conversion</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: df with time column parsed to timestamp</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>@staticmethod\ndef parse_timestamp(\n    sdf: DataFrame,\n    timestamp_format: str,\n) -&gt; DataFrame:\n    \"\"\"\n    Based on config params timestampt format and input timezone\n    convert timestamp column from string to timestamp type.\n    Flag succesful timestamp transformations and errors.\n\n    Args:\n        sdf (pyspark.sql.dataframe.DataFrame): df with timestamp column\n        timestamp_format (str): expected string format to use in time conversion\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: df with time column parsed to timestamp\n    \"\"\"\n\n    sdf = sdf.withColumn(\n        f\"{ColNames.timestamp}_parsed\",\n        F.to_timestamp(ColNames.timestamp, timestamp_format),\n    )\n\n    sdf = sdf.withColumn(\n        f\"{ColNames.timestamp}_flag_{ErrorTypes.CANNOT_PARSE}\",\n        F.when(\n            F.col(ColNames.timestamp).isNotNull() &amp; F.col(f\"{ColNames.timestamp}_parsed\").isNull(),\n            F.lit(True),\n        ).otherwise(F.lit(False)),\n    )\n\n    sdf = sdf.withColumn(\n        ColNames.timestamp,\n        F.col(f\"{ColNames.timestamp}_parsed\"),\n    ).drop(f\"{ColNames.timestamp}_parsed\")\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/event_semantic_cleaning/","title":"event_semantic_cleaning","text":""},{"location":"reference/components/execution/event_semantic_cleaning/event_semantic_cleaning/","title":"event_semantic_cleaning","text":"<p>Module that computes semantic checks on event data and adds error flags</p>"},{"location":"reference/components/execution/event_semantic_cleaning/event_semantic_cleaning/#components.execution.event_semantic_cleaning.event_semantic_cleaning.SemanticCleaning","title":"<code>SemanticCleaning</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class that performs semantic checks on event data and adds error flags</p> Source code in <code>multimno/components/execution/event_semantic_cleaning/event_semantic_cleaning.py</code> <pre><code>class SemanticCleaning(Component):\n    \"\"\"\n    Class that performs semantic checks on event data and adds error flags\n    \"\"\"\n\n    COMPONENT_ID = \"SemanticCleaning\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n        self.timestamp = datetime.datetime.now()\n\n        # Read date of study\n        self.date_format = self.config.get(self.COMPONENT_ID, \"date_format\")\n        data_period_start = self.config.get(self.COMPONENT_ID, \"data_period_start\")\n        data_period_end = self.config.get(self.COMPONENT_ID, \"data_period_end\")\n        self.date_of_study: datetime.date = None\n\n        self.do_different_location_deduplication = self.config.getboolean(\n            self.COMPONENT_ID, \"do_different_location_deduplication\"\n        )\n\n        try:\n            self.data_period_start = datetime.datetime.strptime(data_period_start, self.date_format).date()\n        except ValueError as e:\n            self.logger.error(str(e), exc_info=True)\n            raise e\n\n        try:\n            self.data_period_end = datetime.datetime.strptime(data_period_end, self.date_format).date()\n        except ValueError as e:\n            self.logger.error(str(e), exc_info=True)\n            raise e\n\n        self.data_period_dates = [\n            self.data_period_start + datetime.timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n        # Unit: metre\n        self.semantic_min_distance = self.config.getfloat(self.COMPONENT_ID, \"semantic_min_distance_m\")\n\n        # Unit: metre / second\n        self.semantic_min_speed = self.config.getfloat(self.COMPONENT_ID, \"semantic_min_speed_m_s\")\n\n    def initalize_data_objects(self):\n        self.clear_destination_directory = self.config.getboolean(self.COMPONENT_ID, \"clear_destination_directory\")\n        input_silver_event_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"event_data_silver\")\n        input_silver_network_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"network_data_silver\")\n        output_silver_event_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"event_data_silver_flagged\")\n        output_silver_semantic_metrics_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"event_device_semantic_quality_metrics\"\n        )\n        output_event_cache_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"event_cache\")\n\n        input_silver_network = SilverNetworkDataObject(\n            self.spark,\n            input_silver_network_path,\n        )\n        input_silver_event = SilverEventDataObject(self.spark, input_silver_event_path)\n\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, output_silver_event_path)\n            delete_file_or_folder(self.spark, output_event_cache_path)\n\n        output_silver_event = SilverEventFlaggedDataObject(self.spark, output_silver_event_path)\n        output_silver_semantic_metrics = SilverEventSemanticQualityMetrics(\n            self.spark,\n            output_silver_semantic_metrics_path,\n        )\n        output_event_cache = EventCacheDataObject(self.spark, output_event_cache_path)\n\n        self.input_data_objects = {\n            SilverNetworkDataObject.ID: input_silver_network,\n            SilverEventDataObject.ID: input_silver_event,\n        }\n        self.output_data_objects = {\n            SilverEventFlaggedDataObject.ID: output_silver_event,\n            SilverEventSemanticQualityMetrics.ID: output_silver_semantic_metrics,\n            EventCacheDataObject.ID: output_event_cache,\n        }\n\n    def transform(self):\n        events_df = self.events_df\n        cells_df = self.cells_df\n\n        # Partition filter\n        events_df = events_df.filter(\n            F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(self.date_of_study)\n        )\n\n        cells_df = (\n            cells_df.filter(\n                F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n                == F.lit(self.date_of_study)\n            )\n            # Point geometry auxiliar column\n            .withColumn(\n                \"geometry\",\n                STC.ST_Point(F.col(ColNames.latitude), F.col(ColNames.latitude)),\n            ).select(\n                [\n                    F.col(ColNames.cell_id),\n                    F.col(ColNames.latitude).alias(\"cell_lat\"),\n                    F.col(ColNames.longitude).alias(\"cell_lon\"),\n                    F.col(ColNames.valid_date_start),\n                    F.col(ColNames.valid_date_end),\n                    F.col(\"geometry\"),\n                ]\n            )\n        )\n\n        # Perform a left join between events and cell IDs. Non-existent cell IDs will be matched\n        # with null values\n        df = events_df.join(cells_df, on=ColNames.cell_id, how=\"left\")\n        df = df.withColumn(ColNames.error_flag, F.lit(None))\n\n        # Flag outbound records as NO_ERROR to omit them from checks.\n        # NOTE: Different location duplicates checking is still performed as normal.\n        df = self._flag_outbound(df)\n\n        # Error flag: Check rows for cell ids with no matching cell entry\n        df = self._flag_non_existent_cell_ids(df)\n\n        # Optional flagging of duplicates with different location info\n        if self.do_different_location_deduplication:\n            df = self._flag_different_location_duplicates(df)\n\n        # Error flag: Check rows which have valid date start and/or valid date end, and flag when timestamp is incompatible\n        df = self._flag_invalid_cell_ids(df)\n\n        # Error flag: suspicious and incorrect events based on location change distance and speed\n        df = self._flag_by_event_location(df)\n\n        # Set NO_ERROR flag for all unmarked records\n        df = self._flag_non_errors(df)\n\n        # Keep only the necessary columns and remove auxiliar ones\n        df = df.select(SilverEventFlaggedDataObject.SCHEMA.names)\n\n        df = df.repartition(*SilverEventFlaggedDataObject.PARTITION_COLUMNS)\n\n        df.persist(StorageLevel.MEMORY_AND_DISK)\n\n        # Semantic metrics\n        metrics_df = self._compute_semantic_metrics(df)\n\n        # Event cache: Calculate first and last events\n        cache_events_df = self._mark_first_last_events(df)\n\n        self.output_data_objects[SilverEventFlaggedDataObject.ID].df = df\n        self.output_data_objects[SilverEventSemanticQualityMetrics.ID].df = metrics_df\n        self.output_data_objects[EventCacheDataObject.ID].df = cache_events_df\n\n    def _mark_first_last_events(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Marks first and last event per user per day.\n        - False for first event\n        - True for last event\n\n        Args:\n            df: Input DataFrame with user_id, timestamp, and date-related columns.\n\n        Returns:\n            DataFrame in EventCacheDataObject format.\n        \"\"\"\n        temp_column = \"temp\"\n\n        # Define window specification partitioned by user and day, ordered by timestamp\n        window_spec = Window.partitionBy(\n            [\n                ColNames.year,\n                ColNames.month,\n                ColNames.day,\n                ColNames.user_id_modulo,\n                ColNames.user_id,\n            ]\n        )\n        # Get the first and last event timestamps in the window\n        first_event_ts = F.first(ColNames.timestamp).over(window_spec)\n        last_event_ts = F.last(ColNames.timestamp).over(window_spec)\n\n        # Mark first and last events of event_error_flag = 0\n        df = df.filter(F.col(ColNames.error_flag) == SemanticErrorType.NO_ERROR).withColumn(\n            temp_column,\n            F.when(F.col(ColNames.timestamp) == first_event_ts, 1)  # First event\n            .when(F.col(ColNames.timestamp) == last_event_ts, 2)  # Last event\n            .otherwise(0),  # Other events\n        )\n\n        # Filter out events that are not first or last\n        df = df.filter(F.col(temp_column) &gt; 0)\n        df = df.withColumn(ColNames.is_last_event, F.when(F.col(temp_column) == 2, True).otherwise(False)).drop(\n            temp_column\n        )\n\n        return df\n\n    def _flag_outbound(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Method to mark all outbound records as valid.\n        Outbound records have a PLMN value where the MCC component differs from the MCC of the record.\n        Outbound records are exempt from other error checks (except same location duplicates).\n\n        Args:\n            df (DataFrame): Spark DataFrame of event records.\n\n        Returns:\n            DataFrame: Same DataFram with added error_flag column, with outbound records marked as NO_ERROR\n        \"\"\"\n        local_mcc = self.config.getint(GENERAL_CONFIG_KEY, \"local_mcc\")\n\n        is_outbound_record_cond = (F.col(ColNames.plmn).isNotNull()) &amp; (\n            F.substring(F.col(ColNames.plmn), 0, 3).cast(IntegerType()) != local_mcc\n        )\n        df = df.withColumn(\n            ColNames.error_flag,\n            F.when(F.col(ColNames.error_flag).isNotNull(), F.col(ColNames.error_flag)).when(\n                is_outbound_record_cond, F.lit(SemanticErrorType.NO_ERROR)\n            ),\n        )\n        return df\n\n    def _flag_non_existent_cell_ids(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Method that creates a new integer column with the name of ColNames.error_flag, and\n        sets the corresponding flags to events that refer to non-existent cell IDs. The rest of\n        the column's values are left as null.\n\n        Args:\n            df (DataFrame): PySpark DataFrame resulting from the left join of events and cells\n\n        Returns:\n            DataFrame: same DataFrame as the input with a new error flag column, containing\n                flags for the events with a non existent cell ID\n        \"\"\"\n        df = df.withColumn(\n            ColNames.error_flag,\n            F.when(F.col(ColNames.error_flag).isNotNull(), F.col(ColNames.error_flag)).when(\n                F.col(\"geometry\").isNull(), F.lit(SemanticErrorType.CELL_ID_NON_EXISTENT)\n            ),\n        )\n        return df\n\n    def _flag_invalid_cell_ids(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Method that finds and flags events which refer to an existent cell ID, but that happened outside the\n        time interval during which the cell was operationals. This flag cannot occur at the same time as a\n        non-existent cell. The rest of values (i.e., those with no previous flags) in the error flag column are left\n        as null.\n        The auxiliar Point geometry column will be set to null for these flagged events.\n\n        Args:\n            df (DataFrame): DataFrame in which invalid cells will be flagged\n\n        Returns:\n            DataFrame: DataFrame with flagged invalid cells\n        \"\"\"\n        df = df.withColumn(\n            ColNames.error_flag,\n            # Leave already flagged rows as is\n            F.when(\n                F.col(ColNames.error_flag).isNotNull(), F.col(ColNames.error_flag)\n            ).when(  # event happened before the cell was operational, or after the cell was operational\n                (\n                    (\n                        F.col(ColNames.valid_date_start).isNotNull()\n                        &amp; (F.col(ColNames.timestamp) &lt; F.col(ColNames.valid_date_start))\n                    )\n                    | (\n                        F.col(ColNames.valid_date_end).isNotNull()\n                        &amp; (F.col(ColNames.timestamp) &gt; F.col(ColNames.valid_date_end))\n                    )\n                ),\n                F.lit(SemanticErrorType.CELL_ID_NOT_VALID),\n            ),\n        )\n\n        df = df.withColumn(\n            \"geometry\",\n            F.when(F.col(ColNames.error_flag).isNotNull(), F.lit(None)).otherwise(F.col(\"geometry\")),\n        )\n        return df\n\n    def _flag_by_event_location(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Method that finds and flags events that are considered to be suspicious or incorrect in\n        terms of their timestamp and cell location with respect to their previous and/or following\n        events.\n        It is assumed that these are the last flags to be raised. Thus, non-flagged events are also\n        set to the no-error-flag value within this method.\n        Args:\n            df (DataFrame): DataFrame in which suspicious and/or incorrect events based on\n                location are to be found and flagged\n\n        Returns:\n            DataFrame: flagged DataFrame with suspicious and/or incorrect events\n        \"\"\"\n        # Windows that comprise all previous (following) rows ordered by time for each user.\n        # Partition pruning\n        # These windows have to be used, as all records have to be kept, and we skip them\n        forward_window = (\n            Window.partitionBy(\n                [\n                    ColNames.year,\n                    ColNames.month,\n                    ColNames.day,\n                    ColNames.user_id_modulo,\n                    ColNames.user_id,\n                ]\n            )\n            .orderBy(ColNames.timestamp)\n            .rowsBetween(Window.currentRow + 1, Window.unboundedFollowing)\n        )\n        backward_window = (\n            Window.partitionBy(\n                [\n                    ColNames.year,\n                    ColNames.month,\n                    ColNames.day,\n                    ColNames.user_id_modulo,\n                    ColNames.user_id,\n                ]\n            )\n            .orderBy(ColNames.timestamp)\n            .rowsBetween(Window.unboundedPreceding, Window.currentRow - 1)\n        )\n\n        # Columns to be evaluated for flags\n        # The order of the definition of these columns appears to affect the physical plan\n        # TODO: find best ordering\n        df = (\n            df\n            # auxiliar column containing timestamps of non-flagged events, and null for flagged events\n            .withColumn(\n                \"filtered_ts\",\n                F.when(F.col(ColNames.error_flag).isNull(), F.col(ColNames.timestamp)).otherwise(None),\n            )\n            .withColumn(\n                \"next_timediff\",  # time b/w curr event and first following non-flagged event timestamp\n                F.when(\n                    F.col(ColNames.error_flag).isNull(),\n                    (\n                        F.first(F.col(\"filtered_ts\"), ignorenulls=True).over(forward_window).cast(LongType())\n                        - F.col(ColNames.timestamp).cast(LongType())\n                    ),\n                ).otherwise(F.lit(None)),\n            )\n            .withColumn(\n                \"prev_timediff\",  # time b/w curr event and last previous non-flagged event timestamp\n                F.when(\n                    F.col(ColNames.error_flag).isNull(),\n                    (\n                        F.col(ColNames.timestamp).cast(LongType())\n                        - F.last(F.col(\"filtered_ts\"), ignorenulls=True).over(backward_window).cast(LongType())\n                    ),\n                ).otherwise(F.lit(None)),\n            )\n            .withColumn(\n                \"next_distance\",  # distance b/w curr location and first following non-flagged event location\n                F.when(\n                    F.col(ColNames.error_flag).isNull(),  # rows not flagged (yet)\n                    STF.ST_DistanceSpheroid(\n                        F.col(\"geometry\"),\n                        F.first(F.col(\"geometry\"), ignorenulls=True).over(forward_window),\n                    ),\n                ).otherwise(F.lit(None)),\n            )\n            .withColumn(\n                \"prev_distance\",  # distance b/w curr location and last previous non-flagged event location\n                F.when(\n                    F.col(ColNames.error_flag).isNull(),  # rows not flagged (yet)\n                    STF.ST_DistanceSpheroid(\n                        F.col(\"geometry\"),\n                        F.last(F.col(\"geometry\"), ignorenulls=True).over(backward_window),\n                    ),\n                ).otherwise(F.lit(None)),\n            )\n            .withColumn(  # mean speed in m/s for euclidean displacemente b/w curr and next non-flagged events\n                \"next_speed\", F.col(\"next_distance\") / F.col(\"next_timediff\")\n            )\n            .withColumn(  # mean speed in m/s for euclidean displacemente b/w curr and previous non-flagged events\n                \"prev_speed\", F.col(\"prev_distance\") / F.col(\"prev_timediff\")\n            )\n        )\n\n        # Conditions that must occur for the two location related error flags\n        incorrect_location_cond = (\n            (F.col(\"prev_speed\") &gt; self.semantic_min_speed)\n            &amp; (F.col(\"prev_distance\") &gt; self.semantic_min_distance)\n            &amp; (F.col(\"next_speed\") &gt; self.semantic_min_speed)\n            &amp; (F.col(\"next_distance\") &gt; self.semantic_min_distance)\n        )\n\n        suspicious_location_cond = F.coalesce(\n            (F.col(\"prev_speed\") &gt; self.semantic_min_speed) &amp; (F.col(\"prev_distance\") &gt; self.semantic_min_distance),\n            F.lit(False),\n        ) | F.coalesce(\n            (F.col(\"next_speed\") &gt; self.semantic_min_speed) &amp; (F.col(\"next_distance\") &gt; self.semantic_min_distance),\n            F.lit(False),\n        )\n        # Set the error flags.\n        # NOTE: it is assumed that this is the last flag to be computed. Thus, all non-flagged events\n        # will be set to the code corresponding to no error flags. If new flags are to be added, one might\n        # want to change this.\n        df = df.withColumn(\n            ColNames.error_flag,\n            F.when(F.col(ColNames.error_flag).isNotNull(), F.col(ColNames.error_flag))\n            .when(incorrect_location_cond, F.lit(SemanticErrorType.INCORRECT_EVENT_LOCATION))\n            .when(suspicious_location_cond, F.lit(SemanticErrorType.SUSPICIOUS_EVENT_LOCATION)),\n        )\n\n        return df\n\n    def _flag_non_errors(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Marks all rows where error type is null as NO_ERROR.\n\n        Args:\n            df (DataFrame): Dataframe of event records with error_flag column.\n\n        Returns:\n            DataFrame: Same dataframe with error_flag set to NO_ERROR code where it was null before.\n        \"\"\"\n        df = df.withColumn(\n            ColNames.error_flag,\n            F.when(F.col(ColNames.error_flag).isNull(), F.lit(SemanticErrorType.NO_ERROR)).otherwise(\n                F.col(ColNames.error_flag)\n            ),\n        )\n        return df\n\n    def _flag_different_location_duplicates(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Method that checkes for duplicates of a different location type and flags these rows with the corresponding error flag.\n        A different location duplicate is such where user_id and timestamp columns are identical,\n        but any of the cell_id, latitude or longitude columns are different.\n        In the current implementation, all column rows are counted for a given partition of user_id_modulo, user_id and timestamp.\n\n        Args:\n            df (DataFrame): PySpark DataFrame resulting from the left join of events and cells\n\n        Returns:\n            DataFrame: same DataFrame as the input with a new error flag column, containing\n                flags for the events with identical timestamps, but different cell_id or latitude or longitude column values\n        \"\"\"\n\n        # Hash the columns that define a unique location\n        df = df.withColumn(\n            \"location_hash\", F.hash(ColNames.cell_id, ColNames.latitude, ColNames.longitude, ColNames.plmn)\n        )\n\n        # Define a window partitioned by user_id only\n        window = Window.partitionBy(\n            ColNames.year, ColNames.month, ColNames.day, ColNames.user_id_modulo, ColNames.user_id\n        ).orderBy(ColNames.timestamp)\n\n        # Mark rows with the same user_id and timestamp but different location hashes as duplicates\n        df = df.withColumn(\n            ColNames.error_flag,\n            F.when(\n                (\n                    (F.lag(\"location_hash\", 1).over(window) != F.col(\"location_hash\"))\n                    &amp; (F.lag(ColNames.timestamp, 1).over(window) == F.col(ColNames.timestamp))\n                )\n                | (\n                    (F.lead(\"location_hash\", 1).over(window) != F.col(\"location_hash\"))\n                    &amp; (F.lead(ColNames.timestamp, 1).over(window) == F.col(ColNames.timestamp))\n                ),\n                F.lit(SemanticErrorType.DIFFERENT_LOCATION_DUPLICATE),\n            ).otherwise(F.col(ColNames.error_flag)),\n        )\n\n        return df\n\n    def _compute_semantic_metrics(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Method that computes the semantic quality metrics of the semantic checks.\n        This amounts to counting the number of flagged events after the semantic checks.\n\n        Args:\n            df (DataFrame): Flagged event DataFrame\n\n        Returns:\n            DataFrame: semantic metrics DataFrame\n        \"\"\"\n        metrics_df = (\n            df.groupby(ColNames.error_flag)\n            .agg(F.count(F.col(ColNames.error_flag)).alias(ColNames.value))\n            .withColumnRenamed(ColNames.error_flag, ColNames.type_of_error)\n            .withColumns(\n                {\n                    ColNames.variable: F.lit(ColNames.cell_id),  # currently, only cell_id here\n                    ColNames.year: F.lit(self.date_of_study.year).cast(ShortType()),\n                    ColNames.month: F.lit(self.date_of_study.month).cast(ByteType()),\n                    ColNames.day: F.lit(self.date_of_study.day).cast(ByteType()),\n                }\n            )\n        )\n\n        all_error_codes = [error_name for error_name in dir(SemanticErrorType) if not error_name.startswith(\"__\")]\n        all_error_codes = [\n            Row(\n                **{\n                    ColNames.type_of_error: getattr(SemanticErrorType, error_name),\n                }\n            )\n            for error_name in all_error_codes\n        ]\n\n        all_errors_df = self.spark.createDataFrame(\n            all_error_codes,\n            schema=StructType([SilverEventSemanticQualityMetrics.SCHEMA[ColNames.type_of_error]]),\n        )\n\n        metrics_df = (\n            metrics_df.join(all_errors_df, on=ColNames.type_of_error, how=\"right\")\n            .fillna(\n                {\n                    ColNames.variable: ColNames.cell_id,\n                    ColNames.value: 0,\n                    ColNames.year: self.date_of_study.year,\n                    ColNames.month: self.date_of_study.month,\n                    ColNames.day: self.date_of_study.day,\n                }\n            )\n            .withColumn(ColNames.result_timestamp, F.lit(self.timestamp))\n        )\n\n        return metrics_df\n\n    @get_execution_stats\n    def execute(self):\n        \"\"\"\n        Method that performs the read, transform and write methods of the component.\n        \"\"\"\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n\n        for date in self.data_period_dates:\n            self.date_of_study = date\n\n            self.events_df = self.input_data_objects[SilverEventDataObject.ID].df.filter(\n                F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n                == F.lit(self.date_of_study)\n            )\n            self.cells_df = self.input_data_objects[SilverNetworkDataObject.ID].df.filter(\n                F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n                == F.lit(self.date_of_study)\n            )\n\n            self.logger.info(f\"Processing data for {date}\")\n            self.transform()\n            self.write()\n            self.spark.catalog.clearCache()\n            self.logger.info(f\"Finished {date}\")\n\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n</code></pre>"},{"location":"reference/components/execution/event_semantic_cleaning/event_semantic_cleaning/#components.execution.event_semantic_cleaning.event_semantic_cleaning.SemanticCleaning._compute_semantic_metrics","title":"<code>_compute_semantic_metrics(df)</code>","text":"<p>Method that computes the semantic quality metrics of the semantic checks. This amounts to counting the number of flagged events after the semantic checks.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Flagged event DataFrame</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>semantic metrics DataFrame</p> Source code in <code>multimno/components/execution/event_semantic_cleaning/event_semantic_cleaning.py</code> <pre><code>def _compute_semantic_metrics(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Method that computes the semantic quality metrics of the semantic checks.\n    This amounts to counting the number of flagged events after the semantic checks.\n\n    Args:\n        df (DataFrame): Flagged event DataFrame\n\n    Returns:\n        DataFrame: semantic metrics DataFrame\n    \"\"\"\n    metrics_df = (\n        df.groupby(ColNames.error_flag)\n        .agg(F.count(F.col(ColNames.error_flag)).alias(ColNames.value))\n        .withColumnRenamed(ColNames.error_flag, ColNames.type_of_error)\n        .withColumns(\n            {\n                ColNames.variable: F.lit(ColNames.cell_id),  # currently, only cell_id here\n                ColNames.year: F.lit(self.date_of_study.year).cast(ShortType()),\n                ColNames.month: F.lit(self.date_of_study.month).cast(ByteType()),\n                ColNames.day: F.lit(self.date_of_study.day).cast(ByteType()),\n            }\n        )\n    )\n\n    all_error_codes = [error_name for error_name in dir(SemanticErrorType) if not error_name.startswith(\"__\")]\n    all_error_codes = [\n        Row(\n            **{\n                ColNames.type_of_error: getattr(SemanticErrorType, error_name),\n            }\n        )\n        for error_name in all_error_codes\n    ]\n\n    all_errors_df = self.spark.createDataFrame(\n        all_error_codes,\n        schema=StructType([SilverEventSemanticQualityMetrics.SCHEMA[ColNames.type_of_error]]),\n    )\n\n    metrics_df = (\n        metrics_df.join(all_errors_df, on=ColNames.type_of_error, how=\"right\")\n        .fillna(\n            {\n                ColNames.variable: ColNames.cell_id,\n                ColNames.value: 0,\n                ColNames.year: self.date_of_study.year,\n                ColNames.month: self.date_of_study.month,\n                ColNames.day: self.date_of_study.day,\n            }\n        )\n        .withColumn(ColNames.result_timestamp, F.lit(self.timestamp))\n    )\n\n    return metrics_df\n</code></pre>"},{"location":"reference/components/execution/event_semantic_cleaning/event_semantic_cleaning/#components.execution.event_semantic_cleaning.event_semantic_cleaning.SemanticCleaning._flag_by_event_location","title":"<code>_flag_by_event_location(df)</code>","text":"<p>Method that finds and flags events that are considered to be suspicious or incorrect in terms of their timestamp and cell location with respect to their previous and/or following events. It is assumed that these are the last flags to be raised. Thus, non-flagged events are also set to the no-error-flag value within this method. Args:     df (DataFrame): DataFrame in which suspicious and/or incorrect events based on         location are to be found and flagged</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>flagged DataFrame with suspicious and/or incorrect events</p> Source code in <code>multimno/components/execution/event_semantic_cleaning/event_semantic_cleaning.py</code> <pre><code>def _flag_by_event_location(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Method that finds and flags events that are considered to be suspicious or incorrect in\n    terms of their timestamp and cell location with respect to their previous and/or following\n    events.\n    It is assumed that these are the last flags to be raised. Thus, non-flagged events are also\n    set to the no-error-flag value within this method.\n    Args:\n        df (DataFrame): DataFrame in which suspicious and/or incorrect events based on\n            location are to be found and flagged\n\n    Returns:\n        DataFrame: flagged DataFrame with suspicious and/or incorrect events\n    \"\"\"\n    # Windows that comprise all previous (following) rows ordered by time for each user.\n    # Partition pruning\n    # These windows have to be used, as all records have to be kept, and we skip them\n    forward_window = (\n        Window.partitionBy(\n            [\n                ColNames.year,\n                ColNames.month,\n                ColNames.day,\n                ColNames.user_id_modulo,\n                ColNames.user_id,\n            ]\n        )\n        .orderBy(ColNames.timestamp)\n        .rowsBetween(Window.currentRow + 1, Window.unboundedFollowing)\n    )\n    backward_window = (\n        Window.partitionBy(\n            [\n                ColNames.year,\n                ColNames.month,\n                ColNames.day,\n                ColNames.user_id_modulo,\n                ColNames.user_id,\n            ]\n        )\n        .orderBy(ColNames.timestamp)\n        .rowsBetween(Window.unboundedPreceding, Window.currentRow - 1)\n    )\n\n    # Columns to be evaluated for flags\n    # The order of the definition of these columns appears to affect the physical plan\n    # TODO: find best ordering\n    df = (\n        df\n        # auxiliar column containing timestamps of non-flagged events, and null for flagged events\n        .withColumn(\n            \"filtered_ts\",\n            F.when(F.col(ColNames.error_flag).isNull(), F.col(ColNames.timestamp)).otherwise(None),\n        )\n        .withColumn(\n            \"next_timediff\",  # time b/w curr event and first following non-flagged event timestamp\n            F.when(\n                F.col(ColNames.error_flag).isNull(),\n                (\n                    F.first(F.col(\"filtered_ts\"), ignorenulls=True).over(forward_window).cast(LongType())\n                    - F.col(ColNames.timestamp).cast(LongType())\n                ),\n            ).otherwise(F.lit(None)),\n        )\n        .withColumn(\n            \"prev_timediff\",  # time b/w curr event and last previous non-flagged event timestamp\n            F.when(\n                F.col(ColNames.error_flag).isNull(),\n                (\n                    F.col(ColNames.timestamp).cast(LongType())\n                    - F.last(F.col(\"filtered_ts\"), ignorenulls=True).over(backward_window).cast(LongType())\n                ),\n            ).otherwise(F.lit(None)),\n        )\n        .withColumn(\n            \"next_distance\",  # distance b/w curr location and first following non-flagged event location\n            F.when(\n                F.col(ColNames.error_flag).isNull(),  # rows not flagged (yet)\n                STF.ST_DistanceSpheroid(\n                    F.col(\"geometry\"),\n                    F.first(F.col(\"geometry\"), ignorenulls=True).over(forward_window),\n                ),\n            ).otherwise(F.lit(None)),\n        )\n        .withColumn(\n            \"prev_distance\",  # distance b/w curr location and last previous non-flagged event location\n            F.when(\n                F.col(ColNames.error_flag).isNull(),  # rows not flagged (yet)\n                STF.ST_DistanceSpheroid(\n                    F.col(\"geometry\"),\n                    F.last(F.col(\"geometry\"), ignorenulls=True).over(backward_window),\n                ),\n            ).otherwise(F.lit(None)),\n        )\n        .withColumn(  # mean speed in m/s for euclidean displacemente b/w curr and next non-flagged events\n            \"next_speed\", F.col(\"next_distance\") / F.col(\"next_timediff\")\n        )\n        .withColumn(  # mean speed in m/s for euclidean displacemente b/w curr and previous non-flagged events\n            \"prev_speed\", F.col(\"prev_distance\") / F.col(\"prev_timediff\")\n        )\n    )\n\n    # Conditions that must occur for the two location related error flags\n    incorrect_location_cond = (\n        (F.col(\"prev_speed\") &gt; self.semantic_min_speed)\n        &amp; (F.col(\"prev_distance\") &gt; self.semantic_min_distance)\n        &amp; (F.col(\"next_speed\") &gt; self.semantic_min_speed)\n        &amp; (F.col(\"next_distance\") &gt; self.semantic_min_distance)\n    )\n\n    suspicious_location_cond = F.coalesce(\n        (F.col(\"prev_speed\") &gt; self.semantic_min_speed) &amp; (F.col(\"prev_distance\") &gt; self.semantic_min_distance),\n        F.lit(False),\n    ) | F.coalesce(\n        (F.col(\"next_speed\") &gt; self.semantic_min_speed) &amp; (F.col(\"next_distance\") &gt; self.semantic_min_distance),\n        F.lit(False),\n    )\n    # Set the error flags.\n    # NOTE: it is assumed that this is the last flag to be computed. Thus, all non-flagged events\n    # will be set to the code corresponding to no error flags. If new flags are to be added, one might\n    # want to change this.\n    df = df.withColumn(\n        ColNames.error_flag,\n        F.when(F.col(ColNames.error_flag).isNotNull(), F.col(ColNames.error_flag))\n        .when(incorrect_location_cond, F.lit(SemanticErrorType.INCORRECT_EVENT_LOCATION))\n        .when(suspicious_location_cond, F.lit(SemanticErrorType.SUSPICIOUS_EVENT_LOCATION)),\n    )\n\n    return df\n</code></pre>"},{"location":"reference/components/execution/event_semantic_cleaning/event_semantic_cleaning/#components.execution.event_semantic_cleaning.event_semantic_cleaning.SemanticCleaning._flag_different_location_duplicates","title":"<code>_flag_different_location_duplicates(df)</code>","text":"<p>Method that checkes for duplicates of a different location type and flags these rows with the corresponding error flag. A different location duplicate is such where user_id and timestamp columns are identical, but any of the cell_id, latitude or longitude columns are different. In the current implementation, all column rows are counted for a given partition of user_id_modulo, user_id and timestamp.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>PySpark DataFrame resulting from the left join of events and cells</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>same DataFrame as the input with a new error flag column, containing flags for the events with identical timestamps, but different cell_id or latitude or longitude column values</p> Source code in <code>multimno/components/execution/event_semantic_cleaning/event_semantic_cleaning.py</code> <pre><code>def _flag_different_location_duplicates(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Method that checkes for duplicates of a different location type and flags these rows with the corresponding error flag.\n    A different location duplicate is such where user_id and timestamp columns are identical,\n    but any of the cell_id, latitude or longitude columns are different.\n    In the current implementation, all column rows are counted for a given partition of user_id_modulo, user_id and timestamp.\n\n    Args:\n        df (DataFrame): PySpark DataFrame resulting from the left join of events and cells\n\n    Returns:\n        DataFrame: same DataFrame as the input with a new error flag column, containing\n            flags for the events with identical timestamps, but different cell_id or latitude or longitude column values\n    \"\"\"\n\n    # Hash the columns that define a unique location\n    df = df.withColumn(\n        \"location_hash\", F.hash(ColNames.cell_id, ColNames.latitude, ColNames.longitude, ColNames.plmn)\n    )\n\n    # Define a window partitioned by user_id only\n    window = Window.partitionBy(\n        ColNames.year, ColNames.month, ColNames.day, ColNames.user_id_modulo, ColNames.user_id\n    ).orderBy(ColNames.timestamp)\n\n    # Mark rows with the same user_id and timestamp but different location hashes as duplicates\n    df = df.withColumn(\n        ColNames.error_flag,\n        F.when(\n            (\n                (F.lag(\"location_hash\", 1).over(window) != F.col(\"location_hash\"))\n                &amp; (F.lag(ColNames.timestamp, 1).over(window) == F.col(ColNames.timestamp))\n            )\n            | (\n                (F.lead(\"location_hash\", 1).over(window) != F.col(\"location_hash\"))\n                &amp; (F.lead(ColNames.timestamp, 1).over(window) == F.col(ColNames.timestamp))\n            ),\n            F.lit(SemanticErrorType.DIFFERENT_LOCATION_DUPLICATE),\n        ).otherwise(F.col(ColNames.error_flag)),\n    )\n\n    return df\n</code></pre>"},{"location":"reference/components/execution/event_semantic_cleaning/event_semantic_cleaning/#components.execution.event_semantic_cleaning.event_semantic_cleaning.SemanticCleaning._flag_invalid_cell_ids","title":"<code>_flag_invalid_cell_ids(df)</code>","text":"<p>Method that finds and flags events which refer to an existent cell ID, but that happened outside the time interval during which the cell was operationals. This flag cannot occur at the same time as a non-existent cell. The rest of values (i.e., those with no previous flags) in the error flag column are left as null. The auxiliar Point geometry column will be set to null for these flagged events.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame in which invalid cells will be flagged</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with flagged invalid cells</p> Source code in <code>multimno/components/execution/event_semantic_cleaning/event_semantic_cleaning.py</code> <pre><code>def _flag_invalid_cell_ids(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Method that finds and flags events which refer to an existent cell ID, but that happened outside the\n    time interval during which the cell was operationals. This flag cannot occur at the same time as a\n    non-existent cell. The rest of values (i.e., those with no previous flags) in the error flag column are left\n    as null.\n    The auxiliar Point geometry column will be set to null for these flagged events.\n\n    Args:\n        df (DataFrame): DataFrame in which invalid cells will be flagged\n\n    Returns:\n        DataFrame: DataFrame with flagged invalid cells\n    \"\"\"\n    df = df.withColumn(\n        ColNames.error_flag,\n        # Leave already flagged rows as is\n        F.when(\n            F.col(ColNames.error_flag).isNotNull(), F.col(ColNames.error_flag)\n        ).when(  # event happened before the cell was operational, or after the cell was operational\n            (\n                (\n                    F.col(ColNames.valid_date_start).isNotNull()\n                    &amp; (F.col(ColNames.timestamp) &lt; F.col(ColNames.valid_date_start))\n                )\n                | (\n                    F.col(ColNames.valid_date_end).isNotNull()\n                    &amp; (F.col(ColNames.timestamp) &gt; F.col(ColNames.valid_date_end))\n                )\n            ),\n            F.lit(SemanticErrorType.CELL_ID_NOT_VALID),\n        ),\n    )\n\n    df = df.withColumn(\n        \"geometry\",\n        F.when(F.col(ColNames.error_flag).isNotNull(), F.lit(None)).otherwise(F.col(\"geometry\")),\n    )\n    return df\n</code></pre>"},{"location":"reference/components/execution/event_semantic_cleaning/event_semantic_cleaning/#components.execution.event_semantic_cleaning.event_semantic_cleaning.SemanticCleaning._flag_non_errors","title":"<code>_flag_non_errors(df)</code>","text":"<p>Marks all rows where error type is null as NO_ERROR.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe of event records with error_flag column.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Same dataframe with error_flag set to NO_ERROR code where it was null before.</p> Source code in <code>multimno/components/execution/event_semantic_cleaning/event_semantic_cleaning.py</code> <pre><code>def _flag_non_errors(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Marks all rows where error type is null as NO_ERROR.\n\n    Args:\n        df (DataFrame): Dataframe of event records with error_flag column.\n\n    Returns:\n        DataFrame: Same dataframe with error_flag set to NO_ERROR code where it was null before.\n    \"\"\"\n    df = df.withColumn(\n        ColNames.error_flag,\n        F.when(F.col(ColNames.error_flag).isNull(), F.lit(SemanticErrorType.NO_ERROR)).otherwise(\n            F.col(ColNames.error_flag)\n        ),\n    )\n    return df\n</code></pre>"},{"location":"reference/components/execution/event_semantic_cleaning/event_semantic_cleaning/#components.execution.event_semantic_cleaning.event_semantic_cleaning.SemanticCleaning._flag_non_existent_cell_ids","title":"<code>_flag_non_existent_cell_ids(df)</code>","text":"<p>Method that creates a new integer column with the name of ColNames.error_flag, and sets the corresponding flags to events that refer to non-existent cell IDs. The rest of the column's values are left as null.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>PySpark DataFrame resulting from the left join of events and cells</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>same DataFrame as the input with a new error flag column, containing flags for the events with a non existent cell ID</p> Source code in <code>multimno/components/execution/event_semantic_cleaning/event_semantic_cleaning.py</code> <pre><code>def _flag_non_existent_cell_ids(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Method that creates a new integer column with the name of ColNames.error_flag, and\n    sets the corresponding flags to events that refer to non-existent cell IDs. The rest of\n    the column's values are left as null.\n\n    Args:\n        df (DataFrame): PySpark DataFrame resulting from the left join of events and cells\n\n    Returns:\n        DataFrame: same DataFrame as the input with a new error flag column, containing\n            flags for the events with a non existent cell ID\n    \"\"\"\n    df = df.withColumn(\n        ColNames.error_flag,\n        F.when(F.col(ColNames.error_flag).isNotNull(), F.col(ColNames.error_flag)).when(\n            F.col(\"geometry\").isNull(), F.lit(SemanticErrorType.CELL_ID_NON_EXISTENT)\n        ),\n    )\n    return df\n</code></pre>"},{"location":"reference/components/execution/event_semantic_cleaning/event_semantic_cleaning/#components.execution.event_semantic_cleaning.event_semantic_cleaning.SemanticCleaning._flag_outbound","title":"<code>_flag_outbound(df)</code>","text":"<p>Method to mark all outbound records as valid. Outbound records have a PLMN value where the MCC component differs from the MCC of the record. Outbound records are exempt from other error checks (except same location duplicates).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Spark DataFrame of event records.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Same DataFram with added error_flag column, with outbound records marked as NO_ERROR</p> Source code in <code>multimno/components/execution/event_semantic_cleaning/event_semantic_cleaning.py</code> <pre><code>def _flag_outbound(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Method to mark all outbound records as valid.\n    Outbound records have a PLMN value where the MCC component differs from the MCC of the record.\n    Outbound records are exempt from other error checks (except same location duplicates).\n\n    Args:\n        df (DataFrame): Spark DataFrame of event records.\n\n    Returns:\n        DataFrame: Same DataFram with added error_flag column, with outbound records marked as NO_ERROR\n    \"\"\"\n    local_mcc = self.config.getint(GENERAL_CONFIG_KEY, \"local_mcc\")\n\n    is_outbound_record_cond = (F.col(ColNames.plmn).isNotNull()) &amp; (\n        F.substring(F.col(ColNames.plmn), 0, 3).cast(IntegerType()) != local_mcc\n    )\n    df = df.withColumn(\n        ColNames.error_flag,\n        F.when(F.col(ColNames.error_flag).isNotNull(), F.col(ColNames.error_flag)).when(\n            is_outbound_record_cond, F.lit(SemanticErrorType.NO_ERROR)\n        ),\n    )\n    return df\n</code></pre>"},{"location":"reference/components/execution/event_semantic_cleaning/event_semantic_cleaning/#components.execution.event_semantic_cleaning.event_semantic_cleaning.SemanticCleaning._mark_first_last_events","title":"<code>_mark_first_last_events(df)</code>","text":"<p>Marks first and last event per user per day. - False for first event - True for last event</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame with user_id, timestamp, and date-related columns.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame in EventCacheDataObject format.</p> Source code in <code>multimno/components/execution/event_semantic_cleaning/event_semantic_cleaning.py</code> <pre><code>def _mark_first_last_events(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Marks first and last event per user per day.\n    - False for first event\n    - True for last event\n\n    Args:\n        df: Input DataFrame with user_id, timestamp, and date-related columns.\n\n    Returns:\n        DataFrame in EventCacheDataObject format.\n    \"\"\"\n    temp_column = \"temp\"\n\n    # Define window specification partitioned by user and day, ordered by timestamp\n    window_spec = Window.partitionBy(\n        [\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            ColNames.user_id_modulo,\n            ColNames.user_id,\n        ]\n    )\n    # Get the first and last event timestamps in the window\n    first_event_ts = F.first(ColNames.timestamp).over(window_spec)\n    last_event_ts = F.last(ColNames.timestamp).over(window_spec)\n\n    # Mark first and last events of event_error_flag = 0\n    df = df.filter(F.col(ColNames.error_flag) == SemanticErrorType.NO_ERROR).withColumn(\n        temp_column,\n        F.when(F.col(ColNames.timestamp) == first_event_ts, 1)  # First event\n        .when(F.col(ColNames.timestamp) == last_event_ts, 2)  # Last event\n        .otherwise(0),  # Other events\n    )\n\n    # Filter out events that are not first or last\n    df = df.filter(F.col(temp_column) &gt; 0)\n    df = df.withColumn(ColNames.is_last_event, F.when(F.col(temp_column) == 2, True).otherwise(False)).drop(\n        temp_column\n    )\n\n    return df\n</code></pre>"},{"location":"reference/components/execution/event_semantic_cleaning/event_semantic_cleaning/#components.execution.event_semantic_cleaning.event_semantic_cleaning.SemanticCleaning.execute","title":"<code>execute()</code>","text":"<p>Method that performs the read, transform and write methods of the component.</p> Source code in <code>multimno/components/execution/event_semantic_cleaning/event_semantic_cleaning.py</code> <pre><code>@get_execution_stats\ndef execute(self):\n    \"\"\"\n    Method that performs the read, transform and write methods of the component.\n    \"\"\"\n    self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n    self.read()\n\n    for date in self.data_period_dates:\n        self.date_of_study = date\n\n        self.events_df = self.input_data_objects[SilverEventDataObject.ID].df.filter(\n            F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n            == F.lit(self.date_of_study)\n        )\n        self.cells_df = self.input_data_objects[SilverNetworkDataObject.ID].df.filter(\n            F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n            == F.lit(self.date_of_study)\n        )\n\n        self.logger.info(f\"Processing data for {date}\")\n        self.transform()\n        self.write()\n        self.spark.catalog.clearCache()\n        self.logger.info(f\"Finished {date}\")\n\n    self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n</code></pre>"},{"location":"reference/components/execution/geozones_grid_mapping/","title":"geozones_grid_mapping","text":""},{"location":"reference/components/execution/geozones_grid_mapping/geozones_grid_mapping/","title":"geozones_grid_mapping","text":"<p>This module is responsible for enrichment of the operational grid with elevation and landuse data.</p>"},{"location":"reference/components/execution/geozones_grid_mapping/geozones_grid_mapping/#components.execution.geozones_grid_mapping.geozones_grid_mapping.GeozonesGridMapping","title":"<code>GeozonesGridMapping</code>","text":"<p>               Bases: <code>Component</code></p> <p>This class is responsible for mapping of zoning data to the operational grid.</p> Source code in <code>multimno/components/execution/geozones_grid_mapping/geozones_grid_mapping.py</code> <pre><code>class GeozonesGridMapping(Component):\n    \"\"\"\n    This class is responsible for mapping of zoning data to the operational grid.\n    \"\"\"\n\n    COMPONENT_ID = \"GeozonesGridMapping\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.zoning_dataset_ids = self.config.geteval(GeozonesGridMapping.COMPONENT_ID, \"dataset_ids\")\n\n    def initalize_data_objects(self):\n\n        # inputs\n        self.clear_destination_directory = self.config.getboolean(\n            GeozonesGridMapping.COMPONENT_ID, \"clear_destination_directory\"\n        )\n        self.zoning_type = self.config.get(GeozonesGridMapping.COMPONENT_ID, \"zoning_type\")\n\n        if self.zoning_type == \"admin\":\n            zoning_data = {\"admin_units_data_bronze\": BronzeAdminUnitsDataObject}\n        elif self.zoning_type == \"other\":\n            zoning_data = {\"geographic_zones_data_bronze\": BronzeGeographicZonesDataObject}\n\n        self.input_data_objects = {}\n        inputs = {\n            \"grid_data_silver\": SilverGridDataObject,\n        } | zoning_data\n\n        for key, value in inputs.items():\n            if self.config.has_option(CONFIG_BRONZE_PATHS_KEY, key):\n                path = self.config.get(CONFIG_BRONZE_PATHS_KEY, key)\n            else:\n                path = self.config.get(CONFIG_SILVER_PATHS_KEY, key)\n            if check_if_data_path_exists(self.spark, path):\n                self.input_data_objects[value.ID] = value(self.spark, path)\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")\n                raise ValueError(f\"Invalid path for {value.ID}: {path}\")\n\n        # outputs\n\n        grid_do_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"geozones_grid_map_data_silver\")\n\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, grid_do_path)\n\n        self.output_data_objects = {}\n        self.output_data_objects[SilverGeozonesGridMapDataObject.ID] = SilverGeozonesGridMapDataObject(\n            self.spark,\n            grid_do_path,\n            [\n                ColNames.dataset_id,\n                ColNames.year,\n                ColNames.month,\n                ColNames.day,\n            ],\n        )\n\n    @get_execution_stats\n    def execute(self):\n\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n\n        # iterate over each dataset_id and do mapping separately\n        for dataset_id in self.zoning_dataset_ids:\n            self.logger.info(f\"Starting mapping for {dataset_id} dataset...\")\n            self.current_dataset_id = dataset_id\n            self.read()\n            self.transform()\n            self.write()\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}...\")\n\n        current_zoning_sdf = self.input_data_objects[BronzeGeographicZonesDataObject.ID].df\n        current_zoning_sdf = current_zoning_sdf.filter(\n            current_zoning_sdf[ColNames.dataset_id].isin(self.current_dataset_id)\n        )\n        grid_sdf = self.input_data_objects[SilverGridDataObject.ID].df\n\n        zoning_levels = self.get_hierarchy_levels(current_zoning_sdf)\n\n        zone_grid_sdf = self.map_zoning_units_to_grid(grid_sdf, current_zoning_sdf, zoning_levels)\n\n        zone_grid_sdf = self.extract_hierarchy_ids(zone_grid_sdf, current_zoning_sdf, zoning_levels)\n\n        # get year, month, day from the current_zone_sdf year, month, day columns, assign to the zone_grid_sdf\n        first_row = current_zoning_sdf.first()\n\n        zone_grid_sdf = zone_grid_sdf.withColumn(ColNames.year, F.lit(first_row[ColNames.year]))\n        zone_grid_sdf = zone_grid_sdf.withColumn(ColNames.month, F.lit(first_row[ColNames.month]))\n        zone_grid_sdf = zone_grid_sdf.withColumn(ColNames.day, F.lit(first_row[ColNames.day]))\n\n        zone_grid_sdf = zone_grid_sdf.withColumn(ColNames.dataset_id, F.lit(self.current_dataset_id))\n\n        zone_grid_sdf = utils.apply_schema_casting(zone_grid_sdf, SilverGeozonesGridMapDataObject.SCHEMA)\n\n        self.output_data_objects[SilverGeozonesGridMapDataObject.ID].df = zone_grid_sdf\n\n    @staticmethod\n    def get_hierarchy_levels(zone_units_df: DataFrame) -&gt; Dict[str, int]:\n        \"\"\"\n        Returns the distinct hierarchy levels of the zoning units in a sorted order.\n\n        This method takes a DataFrame of zoning units, selects the distinct values in the 'level' column,\n        and returns these levels in a sorted list.\n\n        Args:\n            zone_units_df (DataFrame): A DataFrame containing zoning units data.\n                                    It is expected to have a column named 'level' which\n                                    indicates the hierarchy level of each zoning unit.\n\n        Returns:\n            list: A sorted list of distinct hierarchy levels of the zoning units.\n        \"\"\"\n        levels = [row[ColNames.level] for row in zone_units_df.select(ColNames.level).distinct().collect()]\n\n        return sorted(levels)\n\n    def map_zoning_units_to_grid(\n        self, grid_sdf: DataFrame, zoning_units_df: DataFrame, zoning_levels: list\n    ) -&gt; DataFrame:\n        \"\"\"\n        Maps zoning units to a grid.\n\n        This method takes a DataFrame of grid data and a DataFrame of zoning units data,\n        and maps the zoning units to the grid. The mapping is done based on the maximum zoning level.\n        The method returns a DataFrame that contains the grid data with an additional column for the zoning unit ID.\n        If a grid cell does not intersect with any zoning unit, the zoning unit ID for that cell is set to 'undefined'.\n\n        Args:\n            grid_sdf (DataFrame): A DataFrame containing grid data.\n                                  It is expected to have a column named 'geometry' which indicates the geometry of each grid cell.\n            zoning_units_df (DataFrame): A DataFrame containing zoning units data.\n                                         It is expected to have columns named 'level' and 'geometry'\n                                         which indicate the hierarchy level and the geometry of each zoning unit, respectively.\n            zoning_levels (list): A list of zoning levels.\n\n        Returns:\n            DataFrame: A DataFrame that contains the grid data with an additional column for the zoning unit ID.\n        \"\"\"\n        zoning_units_df = zoning_units_df.filter(F.col(ColNames.level) == max(zoning_levels))\n\n        intersection_sdf = (\n            grid_sdf.alias(\"a\")\n            .join(\n                F.broadcast(zoning_units_df.alias(\"b\")),\n                STP.ST_Intersects(f\"a.{ColNames.geometry}\", f\"b.{ColNames.geometry}\"),\n            )\n            .select(\"a.*\", f\"b.{ColNames.zone_id}\")\n        )\n\n        non_intersection_sdf = grid_sdf.alias(\"a\").join(\n            intersection_sdf.alias(\"b\").select(\n                ColNames.grid_id,\n            ),\n            F.col(f\"a.{ColNames.grid_id}\") == F.col(f\"b.{ColNames.grid_id}\"),\n            \"left_anti\",\n        )\n\n        non_intersection_sdf = non_intersection_sdf.withColumn(ColNames.zone_id, F.lit(\"undefined\"))\n\n        lowest_zone_grid_sdf = intersection_sdf.union(non_intersection_sdf)\n\n        return lowest_zone_grid_sdf\n\n    def extract_hierarchy_ids(\n        self, zone_grid_sdf: DataFrame, zone_units_sdf: DataFrame, zoning_levels: list\n    ) -&gt; DataFrame:\n        \"\"\"\n        Extracts the hierarchy IDs for all zoning levels.\n\n        This method takes a DataFrame of grid data mapped to zoning units and a DataFrame of zoning units data,\n        and extracts the hierarchy IDs for all zoning levels. The hierarchy ID for a grid tile is a string that\n        contains the IDs of the zoning units that the tile intersects with, ordered by the zoning level.\n\n        Args:\n            zone_grid_sdf (DataFrame): A DataFrame containing grid data mapped to zoning units.\n                                       It is expected to have column named 'zone_id' which represents zone id on the lowest level.\n            zone_units_sdf (DataFrame): A DataFrame containing zoning units data.\n                                        It is expected to have columns named 'level', 'zone_id', and 'parent_id'.\n            zoning_levels (int): The number of zoning levels.\n\n        Returns:\n            DataFrame: A DataFrame that contains the grid data with an additional column for the hierarchy ID.\n        \"\"\"\n\n        for level in reversed(zoning_levels):\n\n            current_level_zone_units_sdf = zone_units_sdf.filter(F.col(ColNames.level) == level)\n            if level == max(zoning_levels):\n\n                zone_grid_sdf = zone_grid_sdf.withColumn(ColNames.hierarchical_id, F.col(ColNames.zone_id))\n                zone_grid_sdf = zone_grid_sdf.alias(\"a\").join(\n                    current_level_zone_units_sdf.alias(\"b\"),\n                    F.col(f\"a.{ColNames.zone_id}\") == F.col(f\"b.{ColNames.zone_id}\"),\n                    \"left\",\n                )\n                zone_grid_sdf = zone_grid_sdf.withColumn(\"next_level_join_id\", F.col(f\"b.{ColNames.parent_id}\"))\n\n                zone_grid_sdf = zone_grid_sdf.select(\"a.*\", \"next_level_join_id\")\n\n            else:\n                zone_grid_sdf = zone_grid_sdf.alias(\"a\").join(\n                    current_level_zone_units_sdf.alias(\"b\"),\n                    F.col(f\"a.next_level_join_id\") == F.col(f\"b.{ColNames.zone_id}\"),\n                    \"left\",\n                )\n\n                zone_grid_sdf = zone_grid_sdf.withColumn(\n                    ColNames.hierarchical_id,\n                    F.when(\n                        F.col(f\"b.{ColNames.zone_id}\").isNotNull(),\n                        F.concat(\n                            F.col(f\"b.{ColNames.zone_id}\"),\n                            F.lit(\"|\"),\n                            F.col(f\"{ColNames.hierarchical_id}\"),\n                        ),\n                    ).otherwise(\n                        F.concat(\n                            F.lit(\"undefined\"),\n                            F.lit(\"|\"),\n                            F.col(f\"a.{ColNames.hierarchical_id}\"),\n                        )\n                    ),\n                )\n                zone_grid_sdf = zone_grid_sdf.withColumn(\"next_level_join_id\", F.col(f\"b.{ColNames.parent_id}\"))\n                zone_grid_sdf = zone_grid_sdf.select(\"a.*\", \"next_level_join_id\", ColNames.hierarchical_id)\n\n        return zone_grid_sdf\n</code></pre>"},{"location":"reference/components/execution/geozones_grid_mapping/geozones_grid_mapping/#components.execution.geozones_grid_mapping.geozones_grid_mapping.GeozonesGridMapping.extract_hierarchy_ids","title":"<code>extract_hierarchy_ids(zone_grid_sdf, zone_units_sdf, zoning_levels)</code>","text":"<p>Extracts the hierarchy IDs for all zoning levels.</p> <p>This method takes a DataFrame of grid data mapped to zoning units and a DataFrame of zoning units data, and extracts the hierarchy IDs for all zoning levels. The hierarchy ID for a grid tile is a string that contains the IDs of the zoning units that the tile intersects with, ordered by the zoning level.</p> <p>Parameters:</p> Name Type Description Default <code>zone_grid_sdf</code> <code>DataFrame</code> <p>A DataFrame containing grid data mapped to zoning units.                        It is expected to have column named 'zone_id' which represents zone id on the lowest level.</p> required <code>zone_units_sdf</code> <code>DataFrame</code> <p>A DataFrame containing zoning units data.                         It is expected to have columns named 'level', 'zone_id', and 'parent_id'.</p> required <code>zoning_levels</code> <code>int</code> <p>The number of zoning levels.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame that contains the grid data with an additional column for the hierarchy ID.</p> Source code in <code>multimno/components/execution/geozones_grid_mapping/geozones_grid_mapping.py</code> <pre><code>def extract_hierarchy_ids(\n    self, zone_grid_sdf: DataFrame, zone_units_sdf: DataFrame, zoning_levels: list\n) -&gt; DataFrame:\n    \"\"\"\n    Extracts the hierarchy IDs for all zoning levels.\n\n    This method takes a DataFrame of grid data mapped to zoning units and a DataFrame of zoning units data,\n    and extracts the hierarchy IDs for all zoning levels. The hierarchy ID for a grid tile is a string that\n    contains the IDs of the zoning units that the tile intersects with, ordered by the zoning level.\n\n    Args:\n        zone_grid_sdf (DataFrame): A DataFrame containing grid data mapped to zoning units.\n                                   It is expected to have column named 'zone_id' which represents zone id on the lowest level.\n        zone_units_sdf (DataFrame): A DataFrame containing zoning units data.\n                                    It is expected to have columns named 'level', 'zone_id', and 'parent_id'.\n        zoning_levels (int): The number of zoning levels.\n\n    Returns:\n        DataFrame: A DataFrame that contains the grid data with an additional column for the hierarchy ID.\n    \"\"\"\n\n    for level in reversed(zoning_levels):\n\n        current_level_zone_units_sdf = zone_units_sdf.filter(F.col(ColNames.level) == level)\n        if level == max(zoning_levels):\n\n            zone_grid_sdf = zone_grid_sdf.withColumn(ColNames.hierarchical_id, F.col(ColNames.zone_id))\n            zone_grid_sdf = zone_grid_sdf.alias(\"a\").join(\n                current_level_zone_units_sdf.alias(\"b\"),\n                F.col(f\"a.{ColNames.zone_id}\") == F.col(f\"b.{ColNames.zone_id}\"),\n                \"left\",\n            )\n            zone_grid_sdf = zone_grid_sdf.withColumn(\"next_level_join_id\", F.col(f\"b.{ColNames.parent_id}\"))\n\n            zone_grid_sdf = zone_grid_sdf.select(\"a.*\", \"next_level_join_id\")\n\n        else:\n            zone_grid_sdf = zone_grid_sdf.alias(\"a\").join(\n                current_level_zone_units_sdf.alias(\"b\"),\n                F.col(f\"a.next_level_join_id\") == F.col(f\"b.{ColNames.zone_id}\"),\n                \"left\",\n            )\n\n            zone_grid_sdf = zone_grid_sdf.withColumn(\n                ColNames.hierarchical_id,\n                F.when(\n                    F.col(f\"b.{ColNames.zone_id}\").isNotNull(),\n                    F.concat(\n                        F.col(f\"b.{ColNames.zone_id}\"),\n                        F.lit(\"|\"),\n                        F.col(f\"{ColNames.hierarchical_id}\"),\n                    ),\n                ).otherwise(\n                    F.concat(\n                        F.lit(\"undefined\"),\n                        F.lit(\"|\"),\n                        F.col(f\"a.{ColNames.hierarchical_id}\"),\n                    )\n                ),\n            )\n            zone_grid_sdf = zone_grid_sdf.withColumn(\"next_level_join_id\", F.col(f\"b.{ColNames.parent_id}\"))\n            zone_grid_sdf = zone_grid_sdf.select(\"a.*\", \"next_level_join_id\", ColNames.hierarchical_id)\n\n    return zone_grid_sdf\n</code></pre>"},{"location":"reference/components/execution/geozones_grid_mapping/geozones_grid_mapping/#components.execution.geozones_grid_mapping.geozones_grid_mapping.GeozonesGridMapping.get_hierarchy_levels","title":"<code>get_hierarchy_levels(zone_units_df)</code>  <code>staticmethod</code>","text":"<p>Returns the distinct hierarchy levels of the zoning units in a sorted order.</p> <p>This method takes a DataFrame of zoning units, selects the distinct values in the 'level' column, and returns these levels in a sorted list.</p> <p>Parameters:</p> Name Type Description Default <code>zone_units_df</code> <code>DataFrame</code> <p>A DataFrame containing zoning units data.                     It is expected to have a column named 'level' which                     indicates the hierarchy level of each zoning unit.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>Dict[str, int]</code> <p>A sorted list of distinct hierarchy levels of the zoning units.</p> Source code in <code>multimno/components/execution/geozones_grid_mapping/geozones_grid_mapping.py</code> <pre><code>@staticmethod\ndef get_hierarchy_levels(zone_units_df: DataFrame) -&gt; Dict[str, int]:\n    \"\"\"\n    Returns the distinct hierarchy levels of the zoning units in a sorted order.\n\n    This method takes a DataFrame of zoning units, selects the distinct values in the 'level' column,\n    and returns these levels in a sorted list.\n\n    Args:\n        zone_units_df (DataFrame): A DataFrame containing zoning units data.\n                                It is expected to have a column named 'level' which\n                                indicates the hierarchy level of each zoning unit.\n\n    Returns:\n        list: A sorted list of distinct hierarchy levels of the zoning units.\n    \"\"\"\n    levels = [row[ColNames.level] for row in zone_units_df.select(ColNames.level).distinct().collect()]\n\n    return sorted(levels)\n</code></pre>"},{"location":"reference/components/execution/geozones_grid_mapping/geozones_grid_mapping/#components.execution.geozones_grid_mapping.geozones_grid_mapping.GeozonesGridMapping.map_zoning_units_to_grid","title":"<code>map_zoning_units_to_grid(grid_sdf, zoning_units_df, zoning_levels)</code>","text":"<p>Maps zoning units to a grid.</p> <p>This method takes a DataFrame of grid data and a DataFrame of zoning units data, and maps the zoning units to the grid. The mapping is done based on the maximum zoning level. The method returns a DataFrame that contains the grid data with an additional column for the zoning unit ID. If a grid cell does not intersect with any zoning unit, the zoning unit ID for that cell is set to 'undefined'.</p> <p>Parameters:</p> Name Type Description Default <code>grid_sdf</code> <code>DataFrame</code> <p>A DataFrame containing grid data.                   It is expected to have a column named 'geometry' which indicates the geometry of each grid cell.</p> required <code>zoning_units_df</code> <code>DataFrame</code> <p>A DataFrame containing zoning units data.                          It is expected to have columns named 'level' and 'geometry'                          which indicate the hierarchy level and the geometry of each zoning unit, respectively.</p> required <code>zoning_levels</code> <code>list</code> <p>A list of zoning levels.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame that contains the grid data with an additional column for the zoning unit ID.</p> Source code in <code>multimno/components/execution/geozones_grid_mapping/geozones_grid_mapping.py</code> <pre><code>def map_zoning_units_to_grid(\n    self, grid_sdf: DataFrame, zoning_units_df: DataFrame, zoning_levels: list\n) -&gt; DataFrame:\n    \"\"\"\n    Maps zoning units to a grid.\n\n    This method takes a DataFrame of grid data and a DataFrame of zoning units data,\n    and maps the zoning units to the grid. The mapping is done based on the maximum zoning level.\n    The method returns a DataFrame that contains the grid data with an additional column for the zoning unit ID.\n    If a grid cell does not intersect with any zoning unit, the zoning unit ID for that cell is set to 'undefined'.\n\n    Args:\n        grid_sdf (DataFrame): A DataFrame containing grid data.\n                              It is expected to have a column named 'geometry' which indicates the geometry of each grid cell.\n        zoning_units_df (DataFrame): A DataFrame containing zoning units data.\n                                     It is expected to have columns named 'level' and 'geometry'\n                                     which indicate the hierarchy level and the geometry of each zoning unit, respectively.\n        zoning_levels (list): A list of zoning levels.\n\n    Returns:\n        DataFrame: A DataFrame that contains the grid data with an additional column for the zoning unit ID.\n    \"\"\"\n    zoning_units_df = zoning_units_df.filter(F.col(ColNames.level) == max(zoning_levels))\n\n    intersection_sdf = (\n        grid_sdf.alias(\"a\")\n        .join(\n            F.broadcast(zoning_units_df.alias(\"b\")),\n            STP.ST_Intersects(f\"a.{ColNames.geometry}\", f\"b.{ColNames.geometry}\"),\n        )\n        .select(\"a.*\", f\"b.{ColNames.zone_id}\")\n    )\n\n    non_intersection_sdf = grid_sdf.alias(\"a\").join(\n        intersection_sdf.alias(\"b\").select(\n            ColNames.grid_id,\n        ),\n        F.col(f\"a.{ColNames.grid_id}\") == F.col(f\"b.{ColNames.grid_id}\"),\n        \"left_anti\",\n    )\n\n    non_intersection_sdf = non_intersection_sdf.withColumn(ColNames.zone_id, F.lit(\"undefined\"))\n\n    lowest_zone_grid_sdf = intersection_sdf.union(non_intersection_sdf)\n\n    return lowest_zone_grid_sdf\n</code></pre>"},{"location":"reference/components/execution/grid_enrichment/","title":"grid_enrichment","text":""},{"location":"reference/components/execution/grid_enrichment/grid_enrichment/","title":"grid_enrichment","text":"<p>This module is responsible for enrichment of the operational grid with elevation and landuse data.</p>"},{"location":"reference/components/execution/grid_enrichment/grid_enrichment/#components.execution.grid_enrichment.grid_enrichment.GridEnrichment","title":"<code>GridEnrichment</code>","text":"<p>               Bases: <code>Component</code></p> <p>This class is responsible for enrichment of the operational grid with elevation and landuse data.</p> Source code in <code>multimno/components/execution/grid_enrichment/grid_enrichment.py</code> <pre><code>class GridEnrichment(Component):\n    \"\"\"\n    This class is responsible for enrichment of the operational grid with elevation and landuse data.\n    \"\"\"\n\n    COMPONENT_ID = \"GridEnrichment\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.do_land_cover_enrichment = self.config.getboolean(GridEnrichment.COMPONENT_ID, \"do_landcover_enrichment\")\n        self.transportation_category_buffer_m = self.config.geteval(\n            GridEnrichment.COMPONENT_ID, \"transportation_category_buffer_m\"\n        )\n        self.prior_weights = self.config.geteval(GridEnrichment.COMPONENT_ID, \"prior_weights\")\n        self.ple_coefficient_weights = self.config.geteval(GridEnrichment.COMPONENT_ID, \"ple_coefficient_weights\")\n        # self.spatial_repartition_size_rows = self.config.getint(\n        #     InspireGridGeneration.COMPONENT_ID, \"spatial_repartition_size_rows\"\n        # )\n\n        self.do_elevation_enrichment = self.config.getboolean(GridEnrichment.COMPONENT_ID, \"do_elevation_enrichment\")\n\n        self.prior_calculation_repartition_size = self.config.getint(\n            GridEnrichment.COMPONENT_ID, \"prior_calculation_repartition_size\"\n        )\n\n        self.quadkey_udf = F.udf(utils.latlon_to_quadkey)\n\n        self.spark.sparkContext.setCheckpointDir(self.config.get(CONFIG_PATHS_KEY, \"spark_checkpoint_dir\"))\n\n        # TODO: For now set to default 100, but can be dynamic from config in future\n        self.grid_resolution = 100\n\n        self.grid_generator = InspireGridGenerator(\n            self.spark,\n            self.grid_resolution,\n            ColNames.geometry,\n            ColNames.grid_id,\n            1000,\n        )\n\n    def initalize_data_objects(self):\n\n        # inputs\n        self.clear_destination_directory = self.config.getboolean(\n            GridEnrichment.COMPONENT_ID, \"clear_destination_directory\"\n        )\n\n        self.input_data_objects = {}\n        inputs = {\n            \"grid_data_silver\": SilverGridDataObject,\n            \"transportation_data_bronze\": BronzeTransportationDataObject,\n            \"landuse_data_bronze\": BronzeLanduseDataObject,\n        }\n\n        for key, value in inputs.items():\n            if self.config.has_option(CONFIG_BRONZE_PATHS_KEY, key):\n                path = self.config.get(CONFIG_BRONZE_PATHS_KEY, key)\n            else:\n                path = self.config.get(CONFIG_SILVER_PATHS_KEY, key)\n            if check_if_data_path_exists(self.spark, path):\n                self.input_data_objects[value.ID] = value(self.spark, path)\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")\n                raise ValueError(f\"Invalid path for {value.ID}: {path}\")\n\n        # outputs\n\n        grid_do_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"enriched_grid_data_silver\")\n\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, grid_do_path)\n\n        self.output_data_objects = {}\n        self.output_data_objects[SilverEnrichedGridDataObject.ID] = SilverEnrichedGridDataObject(\n            self.spark, grid_do_path, [ColNames.quadkey]\n        )\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}...\")\n\n        grid_sdf = self.input_data_objects[SilverGridDataObject.ID].df\n\n        if self.do_elevation_enrichment:\n            grid_sdf = self.add_elevation_to_grid()\n\n        if self.do_land_cover_enrichment:\n\n            # Depending on avaliable resources and data size of an area of interest\n            # we might need to split computations into multiple parts using quadkey level\n            grid_parts = []\n            unique_quadkeys = grid_sdf.select(ColNames.quadkey).distinct().collect()\n            self.logger.info(f\"Calculating landuse prior...\")\n            self.logger.info(f\"Unique quadkeys to process: {len(unique_quadkeys)}\")\n            for quadkey in unique_quadkeys:\n                self.logger.info(f\"Processing quadkey {quadkey[ColNames.quadkey]}\")\n                self.current_quadkey = quadkey[ColNames.quadkey]\n                current_grid_part = grid_sdf.filter(F.col(ColNames.quadkey) == F.lit(self.current_quadkey))\n\n                # map landuse and roads to grid tiles and get landuse ratios\n                current_grid_part = self.map_landuse_to_grid(current_grid_part)\n\n                # Calculate the weighted sums\n                current_grid_part = self.calculated_landuse_ratios_weighted_sum(\n                    current_grid_part, self.prior_weights, \"weighted_sum\"\n                )\n\n                # calculate PLE coefficient\n                current_grid_part = self.calculated_landuse_ratios_weighted_sum(\n                    current_grid_part,\n                    self.ple_coefficient_weights,\n                    ColNames.ple_coefficient,\n                )\n\n                # TODO: asses if it would be possible to use persist instead of checkpoint\n                current_grid_part = current_grid_part.checkpoint()\n\n                # clear the cache\n                self.spark.catalog.clearCache()\n\n                grid_parts.append(current_grid_part)\n\n            grid_sdf = reduce(lambda x, y: x.union(y), grid_parts)\n            grid_sdf = grid_sdf.dropDuplicates([ColNames.grid_id])\n\n            grid_sdf = self.calculate_landuse_prior(grid_sdf)\n\n        grid_sdf = self.grid_generator.grid_ids_to_centroids(grid_sdf)\n\n        grid_sdf = grid_sdf.orderBy(\"quadkey\")\n        grid_sdf = grid_sdf.repartition(\"quadkey\")\n\n        # Cast column types to DO schema, add missing columns manually\n        df_columns = set(grid_sdf.columns)\n        schema_columns = set(field.name for field in SilverEnrichedGridDataObject.SCHEMA.fields)\n        missing_columns = schema_columns - df_columns\n\n        for column in missing_columns:\n            grid_sdf = grid_sdf.withColumn(\n                column,\n                F.lit(None).cast(SilverEnrichedGridDataObject.SCHEMA[column].dataType),\n            )\n\n        grid_sdf = utils.apply_schema_casting(grid_sdf, SilverEnrichedGridDataObject.SCHEMA)\n\n        self.output_data_objects[SilverEnrichedGridDataObject.ID].df = grid_sdf\n\n    def add_elevation_to_grid(self):\n        # TODO: implement elevation enrichment\n        pass\n\n    def map_landuse_to_grid(self, grid_sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Maps land use and transportation data to a grid.\n\n        This function takes a grid DataFrame, prepares the transportation and land use data\n        by filtering and cutting it to the extent of the current quadkey,\n        and then merges the prepared data with the grid. It also calculates the land use ratios per tile in the grid.\n\n        Args:\n            grid_sdf (pyspark.sql.DataFrame): The grid DataFrame to which the land use and transportation data will be mapped.\n\n        Returns:\n            pyspark.sql.DataFrame: The grid DataFrame with the mapped land use\n            and transportation data as ratios of a total area.\n\n        Raises:\n            Warning: If no data is found for the current quadkey,\n            a warning is logged and the function returns the grid DataFrame populated with empty ratios.\n        \"\"\"\n\n        current_quadkey_extent = utils.quadkey_to_extent(self.current_quadkey)\n        # prepare roads\n        transportation_sdf = self.input_data_objects[BronzeTransportationDataObject.ID].df.select(\n            ColNames.category,\n            ColNames.geometry,\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n        )\n\n        transportation_sdf = utils.filter_geodata_to_extent(transportation_sdf, current_quadkey_extent, 3035)\n\n        transportation_sdf = self.calculate_transportation_buffer(\n            transportation_sdf, self.transportation_category_buffer_m\n        )\n        transportation_sdf = transportation_sdf.withColumn(ColNames.category, F.lit(\"roads\"))\n        # This is needed to reduce number of geometries so all small roads are merged into one multipolygon based on quadkey\n        transportation_sdf = self.merge_transportation_by_grid(transportation_sdf, 1000)\n\n        transportation_sdf.persist(StorageLevel.MEMORY_AND_DISK)\n        transportation_sdf.count()\n        transportation_sdf = transportation_sdf.withColumn(\"quadkey\", F.lit(self.current_quadkey))\n        transportation_sdf.coalesce(1).write.mode(\"overwrite\").format(\"geoparquet\").partitionBy(\"quadkey\").save(\n            \"/opt/mobloc_data/roads_merged_test\"\n        )\n\n        transportation_sdf = transportation_sdf.select(\n            ColNames.category,\n            ColNames.geometry,\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n        )\n\n        self.logger.info(\"Roads prepared\")\n\n        # prepare landuse\n        landuse_sdf = self.input_data_objects[BronzeLanduseDataObject.ID].df.select(\n            ColNames.category,\n            ColNames.geometry,\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n        )\n\n        landuse_sdf = utils.filter_geodata_to_extent(landuse_sdf, current_quadkey_extent, 3035)\n        landuse_sdf = utils.cut_geodata_to_extent(landuse_sdf, current_quadkey_extent, 3035)\n\n        landuse_sdf.persist(StorageLevel.MEMORY_AND_DISK)\n        landuse_sdf.count()\n        self.logger.info(\"Landuse prepared\")\n\n        # merge roads with landuse\n        landuse_sdf = utils.cut_polygons_with_mask_polygons(\n            landuse_sdf,\n            transportation_sdf,\n            [\n                ColNames.category,\n                ColNames.geometry,\n                ColNames.year,\n                ColNames.month,\n                ColNames.day,\n            ],\n            False,\n            ColNames.geometry,\n        )\n\n        landuse_roads_sdf = landuse_sdf.union(transportation_sdf)\n\n        # blow up too big landuse polygons\n        # TODO: make vertices number parameter\n        landuse_roads_sdf = landuse_roads_sdf.withColumn(\n            ColNames.geometry, STF.ST_SubDivideExplode(ColNames.geometry, 1000)\n        )\n        landuse_roads_sdf = utils.fix_geometry(landuse_roads_sdf, 3)\n\n        landuse_roads_sdf.persist(StorageLevel.MEMORY_AND_DISK)\n        land_use_count = landuse_roads_sdf.count()\n\n        if land_use_count == 0:\n            self.logger.warning(f\"No data found for quadkey {self.current_quadkey}. Skipping\")\n            return self.populate_grid_tiles_with_empty_ratios(grid_sdf).drop(\"geometry\")\n\n        self.logger.info(\"Landuse and roads merged\")\n\n        # count landuse types ratios in grid cells\n        grid_tiles = self.grid_generator.grid_ids_to_tiles(grid_sdf)\n        grid_tiles = grid_tiles.withColumn(\"area\", STF.ST_Area(ColNames.geometry))\n\n        grid_tiles = grid_tiles.persist(StorageLevel.MEMORY_AND_DISK)\n        grid_tiles.count()\n\n        grid_tiles = self.calculate_landuse_ratios_per_tile(grid_tiles, landuse_roads_sdf)\n\n        transportation_sdf.unpersist()\n        landuse_roads_sdf.unpersist()\n        landuse_sdf.unpersist()\n\n        return grid_tiles\n\n    def calculated_landuse_ratios_weighted_sum(\n        self,\n        grid_tiles: DataFrame,\n        weights_dict: Dict[str, float],\n        sum_column_name: str,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Calculates the weighted sum of land use ratios for each grid tile.\n\n        This function takes a DataFrame of grid tiles and a dictionary of weights for each land use ratio.\n        It multiplies each land use ratio by its corresponding weight and then sums these weighted ratios\n        to calculate a weighted sum for each grid tile.\n\n        Args:\n            grid_tiles (pyspark.sql.DataFrame): The DataFrame of grid tiles with landuse ratios.\n            weights_dict (Dict[str, float]): A dictionary where the keys are the names of the land use ratios\n            and the values are the corresponding weights.\n            sum_column_name (str): The name of the new column that will contain the weighted sums.\n\n        Returns:\n            pyspark.sql.DataFrame: The DataFrame of grid tiles with the new column of weighted sums.\n        \"\"\"\n\n        weighted_columns = [F.col(f\"{ratio}_ratio\") * weight for ratio, weight in weights_dict.items()]\n        grid_tiles = grid_tiles.withColumn(sum_column_name, sum(weighted_columns))\n\n        return grid_tiles\n\n    def calculate_landuse_ratios_per_tile(self, grid_tiles: DataFrame, landuse_sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Calculates the land use ratios for each tile in a grid.\n\n        This function takes a DataFrame of grid tiles and a DataFrame of land use data.\n        For each land use class, it calculates the ratio of the area of the class that intersects\n        with each grid tile to the area of the tile.\n        If a tile does not intersect with a land use class, the ratio for that class is set to 0.0.\n        If a tile does not intersect with any land use class, the 'open_area_ratio' is set to 1.0.\n\n        Args:\n            grid_tiles (pyspark.sql.DataFrame): The DataFrame of grid tiles.\n            landuse_sdf (pyspark.sql.DataFrame): The DataFrame of land use data.\n\n        Returns:\n            pyspark.sql.DataFrame: The DataFrame of grid tiles with the new columns of land use ratios.\n        \"\"\"\n\n        classes = [prior_weights for prior_weights in self.prior_weights.keys()]\n\n        for landuse_class in classes:\n\n            class_area_ratio_sdf = self.find_intersection_ratio(grid_tiles, landuse_sdf, landuse_class)\n            if class_area_ratio_sdf.count() == 0:\n                grid_tiles = grid_tiles.withColumn(f\"{landuse_class}_ratio\", F.lit(0.0))\n            else:\n                grid_tiles = grid_tiles.join(class_area_ratio_sdf, \"grid_id\", \"left\")\n            grid_tiles = grid_tiles.persist(StorageLevel.MEMORY_AND_DISK)\n            grid_tiles.count()\n            self.logger.info(f\"Landuse class {landuse_class} ratio per tile calculated\")\n\n        # Fill null values in the DataFrame with a specific float number, e.g., 0.0\n        grid_tiles = grid_tiles.fillna(0.0)\n\n        # Update 'open_areas_ratio' to 1.0 if all other specific ratio columns are 0.0\n        # TODO: remove hardcoded values\n        grid_tiles = grid_tiles.withColumn(\n            \"open_area_ratio\",\n            F.when(\n                (F.col(\"roads_ratio\") == 0.0)\n                &amp; (F.col(\"residential_builtup_ratio\") == 0.0)\n                &amp; (F.col(\"other_builtup_ratio\") == 0.0)\n                &amp; (F.col(\"forest_ratio\") == 0.0)\n                &amp; (F.col(\"water_ratio\") == 0.0),\n                1.0,\n            ).otherwise(F.col(\"open_area_ratio\")),\n        ).drop(\"geometry\", \"area\")\n\n        return grid_tiles\n\n    def populate_grid_tiles_with_empty_ratios(self, grid_tiles: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Populates grid tiles with empty land use ratios.\n\n        This function takes a DataFrame of grid tiles and adds a new column for each land use class in weights dict.\n        Each new column is initialized with a value of 0.0, representing an empty land use ratio.\n\n        Args:\n            grid_tiles (pyspark.sql.DataFrame): The DataFrame of grid tiles.\n\n        Returns:\n            pyspark.sql.DataFrame: The DataFrame of grid tiles with the new columns of empty land use ratios.\n        \"\"\"\n\n        classes = [prior_weights for prior_weights in self.prior_weights.keys()]\n\n        for landuse_class in classes:\n            grid_tiles = grid_tiles.withColumn(f\"{landuse_class}_ratio\", F.lit(0.0))\n\n        return grid_tiles\n\n    def calculate_landuse_prior(self, grid_tiles: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Calculates the prior probability of land use for each tile in a grid.\n\n        This function takes a DataFrame of grid tiles, each with a 'weighted_sum' column representing the weighted sum of land use ratios.\n        It calculates the total weighted sum across all tiles and then divides the weighted sum of each tile by this\n        total to calculate the prior probability of land use for each tile.\n\n        Args:\n            grid_tiles (pyspark.sql.DataFrame): The DataFrame of grid tiles.\n\n        Returns:\n            pyspark.sql.DataFrame: The DataFrame of grid tiles with the new column of prior probabilities.\n        \"\"\"\n\n        # Compute total weighted sum for normalization\n        total_weighted_sum = grid_tiles.select(F.sum(\"weighted_sum\").alias(\"total_weighted_sum\")).collect()[0][\n            \"total_weighted_sum\"\n        ]\n\n        # Normalize the weighted sum across all grid tiles\n        grid_tiles = grid_tiles.withColumn(ColNames.prior_probability, F.col(\"weighted_sum\") / total_weighted_sum)\n\n        return grid_tiles\n\n    def calculate_transportation_buffer(\n        self, transportation_sdf: DataFrame, category_buffer_m: Dict[str, float]\n    ) -&gt; DataFrame:\n        \"\"\"\n        Calculates the buffer for each transportation feature based on its category.\n\n        This function takes a DataFrame of transportation features and a dictionary mapping categories to buffer distances.\n        It assigns the appropriate buffer distance to each feature based on its category and then calculates the buffer geometry.\n        The buffer geometry replaces the original geometry of each feature.\n\n        Args:\n            transportation_sdf (pyspark.sql.DataFrame): The DataFrame of transportation features.\n            category_buffer_m (dict): A dictionary mapping categories to buffer distances.\n\n        Returns:\n            pyspark.sql.DataFrame: The DataFrame of transportation features with the buffer geometries.\n        \"\"\"\n\n        transportation_sdf = self.assign_mapping_values(\n            transportation_sdf,\n            category_buffer_m,\n            ColNames.category,\n            \"buffer_dist\",\n            category_buffer_m[\"unknown\"],\n        )\n\n        transportation_sdf = transportation_sdf.filter(F.col(\"buffer_dist\") != 0.0)\n\n        transportation_sdf = transportation_sdf.withColumn(\n            ColNames.geometry,\n            STF.ST_Buffer(ColNames.geometry, F.col(\"buffer_dist\")),\n        ).drop(\"buffer_dist\")\n\n        return transportation_sdf\n\n    def merge_transportation_by_grid(self, transportation_sdf: DataFrame, resolution: int) -&gt; DataFrame:\n        \"\"\"\n        Merges transportation data with a generated grid based on the specified resolution.\n\n        This function takes a DataFrame containing transportation data and a grid resolution.\n        It first generates a grid that covers the extent of the transportation data. Then, it\n        intersects the transportation data with this grid, merging the transportation geometries\n        that fall within each grid cell. The result is a DataFrame where each row represents a\n        grid cell, aggregated by transportation category and date, with a merged geometry for\n        all transportation data within that cell.\n\n        Parameters:\n        - transportation_sdf (DataFrame): A Spark DataFrame containing transportation data.\n        - resolution (int): The resolution of the grid to generate, specified as the length\n        of the side of each square grid cell in meters.\n\n        Returns:\n        - DataFrame: A Spark DataFrame where each row represents merged transportation data.\n        \"\"\"\n        grid_gen = InspireGridGenerator(self.spark, resolution)\n        for_extent = transportation_sdf.groupBy().agg(STA.ST_Envelope_Aggr(\"geometry\").alias(\"envelope\"))\n        for_extent = for_extent.withColumn(\n            \"envelope\",\n            STF.ST_Transform(\"envelope\", F.lit(\"epsg:3035\"), F.lit(\"epsg:4326\")),\n        )\n        extent = (\n            for_extent.withColumn(\n                \"envelope\",\n                F.array(\n                    STF.ST_XMin(\"envelope\"),\n                    STF.ST_YMin(\"envelope\"),\n                    STF.ST_XMax(\"envelope\"),\n                    STF.ST_YMax(\"envelope\"),\n                ),\n            )\n            .collect()[0]\n            .envelope\n        )\n        for_extent = grid_gen.cover_extent_with_grid_tiles(extent)\n\n        intersection = (\n            transportation_sdf.alias(\"a\")\n            .join(\n                for_extent.alias(\"b\"),\n                STP.ST_Intersects(\"a.geometry\", \"b.geometry\"),\n            )\n            .withColumn(\"merge_geometry\", STF.ST_Intersection(\"a.geometry\", \"b.geometry\"))\n            .groupBy(\"category\", \"year\", \"month\", \"day\", \"b.grid_id\")\n            .agg(STA.ST_Union_Aggr(\"merge_geometry\").alias(\"geometry\"))\n        )\n\n        return intersection\n\n    def find_intersection_ratio(self, grid_tiles: DataFrame, landuse: DataFrame, landuse_class: str) -&gt; DataFrame:\n        \"\"\"\n        Finds the intersection ratio of a specific land use class for each tile in a grid.\n\n        This function takes a DataFrame of grid tiles, a DataFrame of land use data, and a land use class.\n        It finds the intersection of each grid tile with the land use data for the specified class,\n        calculates the area of the intersection, and divides this by the area of the tile to find the intersection ratio.\n        The intersection ratio is added as a new column to the grid DataFrame.\n\n        Args:\n            grid_tiles (pyspark.sql.DataFrame): The DataFrame of grid tiles.\n            landuse (pyspark.sql.DataFrame): The DataFrame of land use data.\n            landuse_class (str): The land use class to find the intersection ratio for.\n\n        Returns:\n            pyspark.sql.DataFrame: The DataFrame of grid tiles with the new column of intersection ratios.\n        \"\"\"\n\n        grid_tiles = grid_tiles.alias(\"a\").join(\n            landuse.filter(F.col(ColNames.category) == F.lit(landuse_class)).alias(\"b\"),\n            STP.ST_Intersects(f\"a.{ColNames.geometry}\", f\"b.{ColNames.geometry}\"),\n        )\n\n        grid_tiles = grid_tiles.withColumn(\n            \"shared_geom\",\n            STF.ST_Intersection(f\"a.{ColNames.geometry}\", f\"b.{ColNames.geometry}\"),\n        ).select(f\"b.{ColNames.category}\", f\"a.{ColNames.grid_id}\", \"a.area\", \"shared_geom\")\n\n        # grid_tiles = utils.fix_polygon_geometry(grid_tiles, \"shared_geom\")\n\n        grid_tiles = grid_tiles.groupBy(f\"b.{ColNames.category}\", f\"a.{ColNames.grid_id}\", \"a.area\").agg(\n            F.sum(STF.ST_Area(\"shared_geom\")).alias(f\"{landuse_class}_area\")\n        )\n\n        grid_tiles = grid_tiles.withColumn(f\"{landuse_class}_ratio\", F.col(f\"{landuse_class}_area\") / F.col(\"area\"))\n\n        return grid_tiles.select(ColNames.grid_id, f\"{landuse_class}_ratio\")\n\n    @staticmethod\n    def assign_mapping_values(\n        sdf: DataFrame,\n        values_map: Dict[Any, Any],\n        map_column: str,\n        values_column: str,\n        default_value: Any = 2,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Assigns mapping values to a DataFrame based on a specified column.\n\n        This function takes a DataFrame, a dictionary of mapping values, a column to map from,\n        a column to map to, and a default value. It creates a new column in the DataFrame by mapping values\n        from the map column to the values column using the values map.\n        If a value in the map column is not found in the values map, the default value is used.\n\n        Args:\n            sdf (pyspark.sql.DataFrame): The DataFrame to assign mapping values to.\n            values_map (dict): The dictionary of mapping values.\n            map_column (str): The column to map from.\n            values_column (str): The column to map to.\n            default_value (any, optional): The default value to use if a value in the map column is not found in the values map. Defaults to 2.\n\n        Returns:\n            pyspark.sql.DataFrame: The DataFrame with the new column of mapped values.\n        \"\"\"\n\n        keys = list(values_map.keys())\n        reclass_expr = F.when(F.col(map_column) == (keys[0]), values_map[keys[0]])\n\n        for key in keys[1:]:\n            reclass_expr = reclass_expr.when(F.col(map_column) == (key), values_map[key])\n\n        reclass_expr = reclass_expr.otherwise(default_value)\n        sdf = sdf.withColumn(values_column, reclass_expr)\n\n        return sdf\n</code></pre>"},{"location":"reference/components/execution/grid_enrichment/grid_enrichment/#components.execution.grid_enrichment.grid_enrichment.GridEnrichment.assign_mapping_values","title":"<code>assign_mapping_values(sdf, values_map, map_column, values_column, default_value=2)</code>  <code>staticmethod</code>","text":"<p>Assigns mapping values to a DataFrame based on a specified column.</p> <p>This function takes a DataFrame, a dictionary of mapping values, a column to map from, a column to map to, and a default value. It creates a new column in the DataFrame by mapping values from the map column to the values column using the values map. If a value in the map column is not found in the values map, the default value is used.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The DataFrame to assign mapping values to.</p> required <code>values_map</code> <code>dict</code> <p>The dictionary of mapping values.</p> required <code>map_column</code> <code>str</code> <p>The column to map from.</p> required <code>values_column</code> <code>str</code> <p>The column to map to.</p> required <code>default_value</code> <code>any</code> <p>The default value to use if a value in the map column is not found in the values map. Defaults to 2.</p> <code>2</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: The DataFrame with the new column of mapped values.</p> Source code in <code>multimno/components/execution/grid_enrichment/grid_enrichment.py</code> <pre><code>@staticmethod\ndef assign_mapping_values(\n    sdf: DataFrame,\n    values_map: Dict[Any, Any],\n    map_column: str,\n    values_column: str,\n    default_value: Any = 2,\n) -&gt; DataFrame:\n    \"\"\"\n    Assigns mapping values to a DataFrame based on a specified column.\n\n    This function takes a DataFrame, a dictionary of mapping values, a column to map from,\n    a column to map to, and a default value. It creates a new column in the DataFrame by mapping values\n    from the map column to the values column using the values map.\n    If a value in the map column is not found in the values map, the default value is used.\n\n    Args:\n        sdf (pyspark.sql.DataFrame): The DataFrame to assign mapping values to.\n        values_map (dict): The dictionary of mapping values.\n        map_column (str): The column to map from.\n        values_column (str): The column to map to.\n        default_value (any, optional): The default value to use if a value in the map column is not found in the values map. Defaults to 2.\n\n    Returns:\n        pyspark.sql.DataFrame: The DataFrame with the new column of mapped values.\n    \"\"\"\n\n    keys = list(values_map.keys())\n    reclass_expr = F.when(F.col(map_column) == (keys[0]), values_map[keys[0]])\n\n    for key in keys[1:]:\n        reclass_expr = reclass_expr.when(F.col(map_column) == (key), values_map[key])\n\n    reclass_expr = reclass_expr.otherwise(default_value)\n    sdf = sdf.withColumn(values_column, reclass_expr)\n\n    return sdf\n</code></pre>"},{"location":"reference/components/execution/grid_enrichment/grid_enrichment/#components.execution.grid_enrichment.grid_enrichment.GridEnrichment.calculate_landuse_prior","title":"<code>calculate_landuse_prior(grid_tiles)</code>","text":"<p>Calculates the prior probability of land use for each tile in a grid.</p> <p>This function takes a DataFrame of grid tiles, each with a 'weighted_sum' column representing the weighted sum of land use ratios. It calculates the total weighted sum across all tiles and then divides the weighted sum of each tile by this total to calculate the prior probability of land use for each tile.</p> <p>Parameters:</p> Name Type Description Default <code>grid_tiles</code> <code>DataFrame</code> <p>The DataFrame of grid tiles.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: The DataFrame of grid tiles with the new column of prior probabilities.</p> Source code in <code>multimno/components/execution/grid_enrichment/grid_enrichment.py</code> <pre><code>def calculate_landuse_prior(self, grid_tiles: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Calculates the prior probability of land use for each tile in a grid.\n\n    This function takes a DataFrame of grid tiles, each with a 'weighted_sum' column representing the weighted sum of land use ratios.\n    It calculates the total weighted sum across all tiles and then divides the weighted sum of each tile by this\n    total to calculate the prior probability of land use for each tile.\n\n    Args:\n        grid_tiles (pyspark.sql.DataFrame): The DataFrame of grid tiles.\n\n    Returns:\n        pyspark.sql.DataFrame: The DataFrame of grid tiles with the new column of prior probabilities.\n    \"\"\"\n\n    # Compute total weighted sum for normalization\n    total_weighted_sum = grid_tiles.select(F.sum(\"weighted_sum\").alias(\"total_weighted_sum\")).collect()[0][\n        \"total_weighted_sum\"\n    ]\n\n    # Normalize the weighted sum across all grid tiles\n    grid_tiles = grid_tiles.withColumn(ColNames.prior_probability, F.col(\"weighted_sum\") / total_weighted_sum)\n\n    return grid_tiles\n</code></pre>"},{"location":"reference/components/execution/grid_enrichment/grid_enrichment/#components.execution.grid_enrichment.grid_enrichment.GridEnrichment.calculate_landuse_ratios_per_tile","title":"<code>calculate_landuse_ratios_per_tile(grid_tiles, landuse_sdf)</code>","text":"<p>Calculates the land use ratios for each tile in a grid.</p> <p>This function takes a DataFrame of grid tiles and a DataFrame of land use data. For each land use class, it calculates the ratio of the area of the class that intersects with each grid tile to the area of the tile. If a tile does not intersect with a land use class, the ratio for that class is set to 0.0. If a tile does not intersect with any land use class, the 'open_area_ratio' is set to 1.0.</p> <p>Parameters:</p> Name Type Description Default <code>grid_tiles</code> <code>DataFrame</code> <p>The DataFrame of grid tiles.</p> required <code>landuse_sdf</code> <code>DataFrame</code> <p>The DataFrame of land use data.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: The DataFrame of grid tiles with the new columns of land use ratios.</p> Source code in <code>multimno/components/execution/grid_enrichment/grid_enrichment.py</code> <pre><code>def calculate_landuse_ratios_per_tile(self, grid_tiles: DataFrame, landuse_sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Calculates the land use ratios for each tile in a grid.\n\n    This function takes a DataFrame of grid tiles and a DataFrame of land use data.\n    For each land use class, it calculates the ratio of the area of the class that intersects\n    with each grid tile to the area of the tile.\n    If a tile does not intersect with a land use class, the ratio for that class is set to 0.0.\n    If a tile does not intersect with any land use class, the 'open_area_ratio' is set to 1.0.\n\n    Args:\n        grid_tiles (pyspark.sql.DataFrame): The DataFrame of grid tiles.\n        landuse_sdf (pyspark.sql.DataFrame): The DataFrame of land use data.\n\n    Returns:\n        pyspark.sql.DataFrame: The DataFrame of grid tiles with the new columns of land use ratios.\n    \"\"\"\n\n    classes = [prior_weights for prior_weights in self.prior_weights.keys()]\n\n    for landuse_class in classes:\n\n        class_area_ratio_sdf = self.find_intersection_ratio(grid_tiles, landuse_sdf, landuse_class)\n        if class_area_ratio_sdf.count() == 0:\n            grid_tiles = grid_tiles.withColumn(f\"{landuse_class}_ratio\", F.lit(0.0))\n        else:\n            grid_tiles = grid_tiles.join(class_area_ratio_sdf, \"grid_id\", \"left\")\n        grid_tiles = grid_tiles.persist(StorageLevel.MEMORY_AND_DISK)\n        grid_tiles.count()\n        self.logger.info(f\"Landuse class {landuse_class} ratio per tile calculated\")\n\n    # Fill null values in the DataFrame with a specific float number, e.g., 0.0\n    grid_tiles = grid_tiles.fillna(0.0)\n\n    # Update 'open_areas_ratio' to 1.0 if all other specific ratio columns are 0.0\n    # TODO: remove hardcoded values\n    grid_tiles = grid_tiles.withColumn(\n        \"open_area_ratio\",\n        F.when(\n            (F.col(\"roads_ratio\") == 0.0)\n            &amp; (F.col(\"residential_builtup_ratio\") == 0.0)\n            &amp; (F.col(\"other_builtup_ratio\") == 0.0)\n            &amp; (F.col(\"forest_ratio\") == 0.0)\n            &amp; (F.col(\"water_ratio\") == 0.0),\n            1.0,\n        ).otherwise(F.col(\"open_area_ratio\")),\n    ).drop(\"geometry\", \"area\")\n\n    return grid_tiles\n</code></pre>"},{"location":"reference/components/execution/grid_enrichment/grid_enrichment/#components.execution.grid_enrichment.grid_enrichment.GridEnrichment.calculate_transportation_buffer","title":"<code>calculate_transportation_buffer(transportation_sdf, category_buffer_m)</code>","text":"<p>Calculates the buffer for each transportation feature based on its category.</p> <p>This function takes a DataFrame of transportation features and a dictionary mapping categories to buffer distances. It assigns the appropriate buffer distance to each feature based on its category and then calculates the buffer geometry. The buffer geometry replaces the original geometry of each feature.</p> <p>Parameters:</p> Name Type Description Default <code>transportation_sdf</code> <code>DataFrame</code> <p>The DataFrame of transportation features.</p> required <code>category_buffer_m</code> <code>dict</code> <p>A dictionary mapping categories to buffer distances.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: The DataFrame of transportation features with the buffer geometries.</p> Source code in <code>multimno/components/execution/grid_enrichment/grid_enrichment.py</code> <pre><code>def calculate_transportation_buffer(\n    self, transportation_sdf: DataFrame, category_buffer_m: Dict[str, float]\n) -&gt; DataFrame:\n    \"\"\"\n    Calculates the buffer for each transportation feature based on its category.\n\n    This function takes a DataFrame of transportation features and a dictionary mapping categories to buffer distances.\n    It assigns the appropriate buffer distance to each feature based on its category and then calculates the buffer geometry.\n    The buffer geometry replaces the original geometry of each feature.\n\n    Args:\n        transportation_sdf (pyspark.sql.DataFrame): The DataFrame of transportation features.\n        category_buffer_m (dict): A dictionary mapping categories to buffer distances.\n\n    Returns:\n        pyspark.sql.DataFrame: The DataFrame of transportation features with the buffer geometries.\n    \"\"\"\n\n    transportation_sdf = self.assign_mapping_values(\n        transportation_sdf,\n        category_buffer_m,\n        ColNames.category,\n        \"buffer_dist\",\n        category_buffer_m[\"unknown\"],\n    )\n\n    transportation_sdf = transportation_sdf.filter(F.col(\"buffer_dist\") != 0.0)\n\n    transportation_sdf = transportation_sdf.withColumn(\n        ColNames.geometry,\n        STF.ST_Buffer(ColNames.geometry, F.col(\"buffer_dist\")),\n    ).drop(\"buffer_dist\")\n\n    return transportation_sdf\n</code></pre>"},{"location":"reference/components/execution/grid_enrichment/grid_enrichment/#components.execution.grid_enrichment.grid_enrichment.GridEnrichment.calculated_landuse_ratios_weighted_sum","title":"<code>calculated_landuse_ratios_weighted_sum(grid_tiles, weights_dict, sum_column_name)</code>","text":"<p>Calculates the weighted sum of land use ratios for each grid tile.</p> <p>This function takes a DataFrame of grid tiles and a dictionary of weights for each land use ratio. It multiplies each land use ratio by its corresponding weight and then sums these weighted ratios to calculate a weighted sum for each grid tile.</p> <p>Parameters:</p> Name Type Description Default <code>grid_tiles</code> <code>DataFrame</code> <p>The DataFrame of grid tiles with landuse ratios.</p> required <code>weights_dict</code> <code>Dict[str, float]</code> <p>A dictionary where the keys are the names of the land use ratios</p> required <code>sum_column_name</code> <code>str</code> <p>The name of the new column that will contain the weighted sums.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: The DataFrame of grid tiles with the new column of weighted sums.</p> Source code in <code>multimno/components/execution/grid_enrichment/grid_enrichment.py</code> <pre><code>def calculated_landuse_ratios_weighted_sum(\n    self,\n    grid_tiles: DataFrame,\n    weights_dict: Dict[str, float],\n    sum_column_name: str,\n) -&gt; DataFrame:\n    \"\"\"\n    Calculates the weighted sum of land use ratios for each grid tile.\n\n    This function takes a DataFrame of grid tiles and a dictionary of weights for each land use ratio.\n    It multiplies each land use ratio by its corresponding weight and then sums these weighted ratios\n    to calculate a weighted sum for each grid tile.\n\n    Args:\n        grid_tiles (pyspark.sql.DataFrame): The DataFrame of grid tiles with landuse ratios.\n        weights_dict (Dict[str, float]): A dictionary where the keys are the names of the land use ratios\n        and the values are the corresponding weights.\n        sum_column_name (str): The name of the new column that will contain the weighted sums.\n\n    Returns:\n        pyspark.sql.DataFrame: The DataFrame of grid tiles with the new column of weighted sums.\n    \"\"\"\n\n    weighted_columns = [F.col(f\"{ratio}_ratio\") * weight for ratio, weight in weights_dict.items()]\n    grid_tiles = grid_tiles.withColumn(sum_column_name, sum(weighted_columns))\n\n    return grid_tiles\n</code></pre>"},{"location":"reference/components/execution/grid_enrichment/grid_enrichment/#components.execution.grid_enrichment.grid_enrichment.GridEnrichment.find_intersection_ratio","title":"<code>find_intersection_ratio(grid_tiles, landuse, landuse_class)</code>","text":"<p>Finds the intersection ratio of a specific land use class for each tile in a grid.</p> <p>This function takes a DataFrame of grid tiles, a DataFrame of land use data, and a land use class. It finds the intersection of each grid tile with the land use data for the specified class, calculates the area of the intersection, and divides this by the area of the tile to find the intersection ratio. The intersection ratio is added as a new column to the grid DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>grid_tiles</code> <code>DataFrame</code> <p>The DataFrame of grid tiles.</p> required <code>landuse</code> <code>DataFrame</code> <p>The DataFrame of land use data.</p> required <code>landuse_class</code> <code>str</code> <p>The land use class to find the intersection ratio for.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: The DataFrame of grid tiles with the new column of intersection ratios.</p> Source code in <code>multimno/components/execution/grid_enrichment/grid_enrichment.py</code> <pre><code>def find_intersection_ratio(self, grid_tiles: DataFrame, landuse: DataFrame, landuse_class: str) -&gt; DataFrame:\n    \"\"\"\n    Finds the intersection ratio of a specific land use class for each tile in a grid.\n\n    This function takes a DataFrame of grid tiles, a DataFrame of land use data, and a land use class.\n    It finds the intersection of each grid tile with the land use data for the specified class,\n    calculates the area of the intersection, and divides this by the area of the tile to find the intersection ratio.\n    The intersection ratio is added as a new column to the grid DataFrame.\n\n    Args:\n        grid_tiles (pyspark.sql.DataFrame): The DataFrame of grid tiles.\n        landuse (pyspark.sql.DataFrame): The DataFrame of land use data.\n        landuse_class (str): The land use class to find the intersection ratio for.\n\n    Returns:\n        pyspark.sql.DataFrame: The DataFrame of grid tiles with the new column of intersection ratios.\n    \"\"\"\n\n    grid_tiles = grid_tiles.alias(\"a\").join(\n        landuse.filter(F.col(ColNames.category) == F.lit(landuse_class)).alias(\"b\"),\n        STP.ST_Intersects(f\"a.{ColNames.geometry}\", f\"b.{ColNames.geometry}\"),\n    )\n\n    grid_tiles = grid_tiles.withColumn(\n        \"shared_geom\",\n        STF.ST_Intersection(f\"a.{ColNames.geometry}\", f\"b.{ColNames.geometry}\"),\n    ).select(f\"b.{ColNames.category}\", f\"a.{ColNames.grid_id}\", \"a.area\", \"shared_geom\")\n\n    # grid_tiles = utils.fix_polygon_geometry(grid_tiles, \"shared_geom\")\n\n    grid_tiles = grid_tiles.groupBy(f\"b.{ColNames.category}\", f\"a.{ColNames.grid_id}\", \"a.area\").agg(\n        F.sum(STF.ST_Area(\"shared_geom\")).alias(f\"{landuse_class}_area\")\n    )\n\n    grid_tiles = grid_tiles.withColumn(f\"{landuse_class}_ratio\", F.col(f\"{landuse_class}_area\") / F.col(\"area\"))\n\n    return grid_tiles.select(ColNames.grid_id, f\"{landuse_class}_ratio\")\n</code></pre>"},{"location":"reference/components/execution/grid_enrichment/grid_enrichment/#components.execution.grid_enrichment.grid_enrichment.GridEnrichment.map_landuse_to_grid","title":"<code>map_landuse_to_grid(grid_sdf)</code>","text":"<p>Maps land use and transportation data to a grid.</p> <p>This function takes a grid DataFrame, prepares the transportation and land use data by filtering and cutting it to the extent of the current quadkey, and then merges the prepared data with the grid. It also calculates the land use ratios per tile in the grid.</p> <p>Parameters:</p> Name Type Description Default <code>grid_sdf</code> <code>DataFrame</code> <p>The grid DataFrame to which the land use and transportation data will be mapped.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: The grid DataFrame with the mapped land use</p> <code>DataFrame</code> <p>and transportation data as ratios of a total area.</p> <p>Raises:</p> Type Description <code>Warning</code> <p>If no data is found for the current quadkey,</p> Source code in <code>multimno/components/execution/grid_enrichment/grid_enrichment.py</code> <pre><code>def map_landuse_to_grid(self, grid_sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Maps land use and transportation data to a grid.\n\n    This function takes a grid DataFrame, prepares the transportation and land use data\n    by filtering and cutting it to the extent of the current quadkey,\n    and then merges the prepared data with the grid. It also calculates the land use ratios per tile in the grid.\n\n    Args:\n        grid_sdf (pyspark.sql.DataFrame): The grid DataFrame to which the land use and transportation data will be mapped.\n\n    Returns:\n        pyspark.sql.DataFrame: The grid DataFrame with the mapped land use\n        and transportation data as ratios of a total area.\n\n    Raises:\n        Warning: If no data is found for the current quadkey,\n        a warning is logged and the function returns the grid DataFrame populated with empty ratios.\n    \"\"\"\n\n    current_quadkey_extent = utils.quadkey_to_extent(self.current_quadkey)\n    # prepare roads\n    transportation_sdf = self.input_data_objects[BronzeTransportationDataObject.ID].df.select(\n        ColNames.category,\n        ColNames.geometry,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    )\n\n    transportation_sdf = utils.filter_geodata_to_extent(transportation_sdf, current_quadkey_extent, 3035)\n\n    transportation_sdf = self.calculate_transportation_buffer(\n        transportation_sdf, self.transportation_category_buffer_m\n    )\n    transportation_sdf = transportation_sdf.withColumn(ColNames.category, F.lit(\"roads\"))\n    # This is needed to reduce number of geometries so all small roads are merged into one multipolygon based on quadkey\n    transportation_sdf = self.merge_transportation_by_grid(transportation_sdf, 1000)\n\n    transportation_sdf.persist(StorageLevel.MEMORY_AND_DISK)\n    transportation_sdf.count()\n    transportation_sdf = transportation_sdf.withColumn(\"quadkey\", F.lit(self.current_quadkey))\n    transportation_sdf.coalesce(1).write.mode(\"overwrite\").format(\"geoparquet\").partitionBy(\"quadkey\").save(\n        \"/opt/mobloc_data/roads_merged_test\"\n    )\n\n    transportation_sdf = transportation_sdf.select(\n        ColNames.category,\n        ColNames.geometry,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    )\n\n    self.logger.info(\"Roads prepared\")\n\n    # prepare landuse\n    landuse_sdf = self.input_data_objects[BronzeLanduseDataObject.ID].df.select(\n        ColNames.category,\n        ColNames.geometry,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    )\n\n    landuse_sdf = utils.filter_geodata_to_extent(landuse_sdf, current_quadkey_extent, 3035)\n    landuse_sdf = utils.cut_geodata_to_extent(landuse_sdf, current_quadkey_extent, 3035)\n\n    landuse_sdf.persist(StorageLevel.MEMORY_AND_DISK)\n    landuse_sdf.count()\n    self.logger.info(\"Landuse prepared\")\n\n    # merge roads with landuse\n    landuse_sdf = utils.cut_polygons_with_mask_polygons(\n        landuse_sdf,\n        transportation_sdf,\n        [\n            ColNames.category,\n            ColNames.geometry,\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n        ],\n        False,\n        ColNames.geometry,\n    )\n\n    landuse_roads_sdf = landuse_sdf.union(transportation_sdf)\n\n    # blow up too big landuse polygons\n    # TODO: make vertices number parameter\n    landuse_roads_sdf = landuse_roads_sdf.withColumn(\n        ColNames.geometry, STF.ST_SubDivideExplode(ColNames.geometry, 1000)\n    )\n    landuse_roads_sdf = utils.fix_geometry(landuse_roads_sdf, 3)\n\n    landuse_roads_sdf.persist(StorageLevel.MEMORY_AND_DISK)\n    land_use_count = landuse_roads_sdf.count()\n\n    if land_use_count == 0:\n        self.logger.warning(f\"No data found for quadkey {self.current_quadkey}. Skipping\")\n        return self.populate_grid_tiles_with_empty_ratios(grid_sdf).drop(\"geometry\")\n\n    self.logger.info(\"Landuse and roads merged\")\n\n    # count landuse types ratios in grid cells\n    grid_tiles = self.grid_generator.grid_ids_to_tiles(grid_sdf)\n    grid_tiles = grid_tiles.withColumn(\"area\", STF.ST_Area(ColNames.geometry))\n\n    grid_tiles = grid_tiles.persist(StorageLevel.MEMORY_AND_DISK)\n    grid_tiles.count()\n\n    grid_tiles = self.calculate_landuse_ratios_per_tile(grid_tiles, landuse_roads_sdf)\n\n    transportation_sdf.unpersist()\n    landuse_roads_sdf.unpersist()\n    landuse_sdf.unpersist()\n\n    return grid_tiles\n</code></pre>"},{"location":"reference/components/execution/grid_enrichment/grid_enrichment/#components.execution.grid_enrichment.grid_enrichment.GridEnrichment.merge_transportation_by_grid","title":"<code>merge_transportation_by_grid(transportation_sdf, resolution)</code>","text":"<p>Merges transportation data with a generated grid based on the specified resolution.</p> <p>This function takes a DataFrame containing transportation data and a grid resolution. It first generates a grid that covers the extent of the transportation data. Then, it intersects the transportation data with this grid, merging the transportation geometries that fall within each grid cell. The result is a DataFrame where each row represents a grid cell, aggregated by transportation category and date, with a merged geometry for all transportation data within that cell.</p> <p>Parameters: - transportation_sdf (DataFrame): A Spark DataFrame containing transportation data. - resolution (int): The resolution of the grid to generate, specified as the length of the side of each square grid cell in meters.</p> <p>Returns: - DataFrame: A Spark DataFrame where each row represents merged transportation data.</p> Source code in <code>multimno/components/execution/grid_enrichment/grid_enrichment.py</code> <pre><code>def merge_transportation_by_grid(self, transportation_sdf: DataFrame, resolution: int) -&gt; DataFrame:\n    \"\"\"\n    Merges transportation data with a generated grid based on the specified resolution.\n\n    This function takes a DataFrame containing transportation data and a grid resolution.\n    It first generates a grid that covers the extent of the transportation data. Then, it\n    intersects the transportation data with this grid, merging the transportation geometries\n    that fall within each grid cell. The result is a DataFrame where each row represents a\n    grid cell, aggregated by transportation category and date, with a merged geometry for\n    all transportation data within that cell.\n\n    Parameters:\n    - transportation_sdf (DataFrame): A Spark DataFrame containing transportation data.\n    - resolution (int): The resolution of the grid to generate, specified as the length\n    of the side of each square grid cell in meters.\n\n    Returns:\n    - DataFrame: A Spark DataFrame where each row represents merged transportation data.\n    \"\"\"\n    grid_gen = InspireGridGenerator(self.spark, resolution)\n    for_extent = transportation_sdf.groupBy().agg(STA.ST_Envelope_Aggr(\"geometry\").alias(\"envelope\"))\n    for_extent = for_extent.withColumn(\n        \"envelope\",\n        STF.ST_Transform(\"envelope\", F.lit(\"epsg:3035\"), F.lit(\"epsg:4326\")),\n    )\n    extent = (\n        for_extent.withColumn(\n            \"envelope\",\n            F.array(\n                STF.ST_XMin(\"envelope\"),\n                STF.ST_YMin(\"envelope\"),\n                STF.ST_XMax(\"envelope\"),\n                STF.ST_YMax(\"envelope\"),\n            ),\n        )\n        .collect()[0]\n        .envelope\n    )\n    for_extent = grid_gen.cover_extent_with_grid_tiles(extent)\n\n    intersection = (\n        transportation_sdf.alias(\"a\")\n        .join(\n            for_extent.alias(\"b\"),\n            STP.ST_Intersects(\"a.geometry\", \"b.geometry\"),\n        )\n        .withColumn(\"merge_geometry\", STF.ST_Intersection(\"a.geometry\", \"b.geometry\"))\n        .groupBy(\"category\", \"year\", \"month\", \"day\", \"b.grid_id\")\n        .agg(STA.ST_Union_Aggr(\"merge_geometry\").alias(\"geometry\"))\n    )\n\n    return intersection\n</code></pre>"},{"location":"reference/components/execution/grid_enrichment/grid_enrichment/#components.execution.grid_enrichment.grid_enrichment.GridEnrichment.populate_grid_tiles_with_empty_ratios","title":"<code>populate_grid_tiles_with_empty_ratios(grid_tiles)</code>","text":"<p>Populates grid tiles with empty land use ratios.</p> <p>This function takes a DataFrame of grid tiles and adds a new column for each land use class in weights dict. Each new column is initialized with a value of 0.0, representing an empty land use ratio.</p> <p>Parameters:</p> Name Type Description Default <code>grid_tiles</code> <code>DataFrame</code> <p>The DataFrame of grid tiles.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: The DataFrame of grid tiles with the new columns of empty land use ratios.</p> Source code in <code>multimno/components/execution/grid_enrichment/grid_enrichment.py</code> <pre><code>def populate_grid_tiles_with_empty_ratios(self, grid_tiles: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Populates grid tiles with empty land use ratios.\n\n    This function takes a DataFrame of grid tiles and adds a new column for each land use class in weights dict.\n    Each new column is initialized with a value of 0.0, representing an empty land use ratio.\n\n    Args:\n        grid_tiles (pyspark.sql.DataFrame): The DataFrame of grid tiles.\n\n    Returns:\n        pyspark.sql.DataFrame: The DataFrame of grid tiles with the new columns of empty land use ratios.\n    \"\"\"\n\n    classes = [prior_weights for prior_weights in self.prior_weights.keys()]\n\n    for landuse_class in classes:\n        grid_tiles = grid_tiles.withColumn(f\"{landuse_class}_ratio\", F.lit(0.0))\n\n    return grid_tiles\n</code></pre>"},{"location":"reference/components/execution/internal_migration/","title":"internal_migration","text":""},{"location":"reference/components/execution/internal_migration/internal_migration/","title":"internal_migration","text":"<p>Module that implements the InternalMigration component</p>"},{"location":"reference/components/execution/internal_migration/internal_migration/#components.execution.internal_migration.internal_migration.InternalMigration","title":"<code>InternalMigration</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class responsible for calculating the internal home location changes produced between two long-term periods and for a given zoning system. First, devices with a significant change in home location tiles are found. Then, based on tile weights, weights for migration between different zones where the home tiles are contained are computed for each device, and these weights are summed up to result in a final weighted device count of devices that migrated from one zone to another</p> Source code in <code>multimno/components/execution/internal_migration/internal_migration.py</code> <pre><code>class InternalMigration(Component):\n    \"\"\"\n    Class responsible for calculating the internal home location changes produced between two long-term periods and for\n    a given zoning system. First, devices with a significant change in home location tiles are found. Then, based on\n    tile weights, weights for migration between different zones where the home tiles are contained are computed\n    for each device, and these weights are summed up to result in a final weighted device count of devices that migrated\n    from one zone to another\n    \"\"\"\n\n    COMPONENT_ID = \"InternalMigration\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        # Read and validate migration threshold\n        self.migration_threshold = self.config.getfloat(self.COMPONENT_ID, \"migration_threshold\")\n        if self.migration_threshold &lt; 0 or self.migration_threshold &gt; 1:\n            msg = f\"migration_threshold should be a value between 0 and 1, found {self.migration_threshold}\"\n            self.logger.error(msg)\n            raise ValueError(msg)\n\n        # Read zoning dataset and hierarchical levels\n        self.zoning_dataset = self.config.get(self.COMPONENT_ID, \"zoning_dataset_id\")\n        levels = self.config.get(self.COMPONENT_ID, \"hierarchical_levels\")\n        try:\n            levels = list(int(x.strip()) for x in levels.split(\",\"))\n        except BaseException as e:\n            msg = f\"hierarchical_levels expected a comma-separated list of integers, but found {levels}\"\n            self.logger.error(msg)\n            raise e(msg)\n        self.levels = levels\n\n        # Read and validate identifying values for the first period's home labels\n        self.start_date_prev = dt.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"start_month_previous\"), \"%Y-%m\"\n        ).date()\n        end_date_prev = dt.datetime.strptime(self.config.get(self.COMPONENT_ID, \"end_month_previous\"), \"%Y-%m\")\n        self.end_date_prev = (\n            end_date_prev + dt.timedelta(days=cal.monthrange(end_date_prev.year, end_date_prev.month)[1] - 1)\n        ).date()\n        if self.start_date_prev &gt; self.end_date_prev:\n            msg = f\"start_month_previous {self.start_date_prev.strftime('%Y-%m')} must be earlier than end_month_previous {end_date_prev.strftime('%Y-%m')}\"\n            self.logger.error(msg)\n            raise ValueError(msg)\n\n        self.season_prev = self.config.get(self.COMPONENT_ID, \"season_previous\")\n        if self.season_prev not in SEASONS:\n            msg = f\"Unknown season_previous {self.season_prev} -- valid values are {SEASONS}\"\n            self.logger.error(msg)\n            raise ValueError(msg)\n\n        # Read and validate identifying values for the second period's home labels\n        self.start_date_new = dt.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"start_month_new\"), \"%Y-%m\"\n        ).date()\n        end_date_new = dt.datetime.strptime(self.config.get(self.COMPONENT_ID, \"end_month_new\"), \"%Y-%m\")\n        self.end_date_new = (\n            end_date_new + dt.timedelta(days=cal.monthrange(end_date_new.year, end_date_new.month)[1] - 1)\n        ).date()\n        if self.start_date_new &gt; self.end_date_new:\n            msg = f\"start_month_new {self.start_date_new.strftime('%Y-%m')} must be earlier than end_month_new {end_date_new.strftime('%Y-%m')}\"\n            self.logger.error(msg)\n            raise ValueError(msg)\n\n        self.season_new = self.config.get(self.COMPONENT_ID, \"season_new\")\n        if self.season_new not in SEASONS:\n            msg = f\"Unknown season_new {self.season_new} -- valid values are {SEASONS}\"\n            self.logger.error(msg)\n            raise ValueError(msg)\n\n        # Initialise other variables\n        self.current_level: int = None\n        self.quality_metrics_computed: bool = False  # quality metrics need only be computed once\n        self.prev_home_tiles: DataFrame = None  # reuse dataframe if already computed for one hierarchical level\n        self.new_home_tiles: DataFrame = None  # reuse dataframe if already computed for one hierarchical level\n        self.migrating_users: DataFrame = None  # reuse dataframe if already computed for one hierarchical level\n\n    def initalize_data_objects(self):\n        input_ue_labels_prev_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"internal_migration_previous_ue_labels_silver\"\n        )\n        input_ue_labels_new_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"internal_migration_new_ue_labels_silver\")\n        input_grid_map_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"geozones_grid_map_data_silver\")\n        output_do_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"internal_migration_silver\")\n        quality_metrics_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"internal_migration_quality_metrics\")\n\n        # Check uniform for getting the grid or the enriched grid data\n        self.uniform_tile_weights = self.config.getboolean(self.COMPONENT_ID, \"uniform_tile_weights\")\n        if not self.uniform_tile_weights:\n            input_enriched_grid_path = self.config.get(\n                CONFIG_SILVER_PATHS_KEY, \"internal_migration_enriched_grid_silver\"\n            )\n            if not check_if_data_path_exists(self.spark, input_enriched_grid_path):\n                self.logger.warning(f\"Expected path {input_enriched_grid_path} to exist but it does not\")\n                raise ValueError(f\"Invalid path for {SilverEnrichedGridDataObject.ID}: {input_enriched_grid_path}\")\n\n        if not check_if_data_path_exists(self.spark, input_ue_labels_prev_path):\n            self.logger.warning(f\"Expected path {input_ue_labels_prev_path} to exist but it does not\")\n            raise ValueError(f\"Invalid path for {SilverInternalMigrationDataObject.ID}: {input_ue_labels_prev_path}\")\n        if not check_if_data_path_exists(self.spark, input_ue_labels_new_path):\n            self.logger.warning(f\"Expected path {input_ue_labels_new_path} to exist but it does not\")\n            raise ValueError(f\"Invalid path for {SilverInternalMigrationDataObject.ID}: {input_ue_labels_new_path}\")\n        if not check_if_data_path_exists(self.spark, input_grid_map_path):\n            self.logger.warning(f\"Expected path {input_grid_map_path} to exist but it does not\")\n            raise ValueError(f\"Invalid path for {SilverGeozonesGridMapDataObject.ID}: {input_grid_map_path}\")\n\n        clear_destination_directory = self.config.getboolean(self.COMPONENT_ID, \"clear_destination_directory\")\n        if clear_destination_directory:\n            delete_file_or_folder(self.spark, output_do_path)\n\n        clear_quality_metrics_directory = self.config.getboolean(self.COMPONENT_ID, \"clear_quality_metrics_directory\")\n        if clear_quality_metrics_directory:\n            delete_file_or_folder(self.spark, quality_metrics_path)\n\n        if not self.uniform_tile_weights:\n            enriched_grid = SilverEnrichedGridDataObject(self.spark, input_enriched_grid_path)\n        prev_ue_labels = SilverUsualEnvironmentLabelsDataObject(self.spark, input_ue_labels_prev_path)\n        new_ue_labels = SilverUsualEnvironmentLabelsDataObject(self.spark, input_ue_labels_new_path)\n        grid_map_zones = SilverGeozonesGridMapDataObject(self.spark, input_grid_map_path)\n        output_do = SilverInternalMigrationDataObject(self.spark, output_do_path)\n        quality_metrics_do = SilverInternalMigrationQualityMetricsDataObject(self.spark, quality_metrics_path)\n\n        self.input_data_objects = {\n            f\"prev_{SilverUsualEnvironmentLabelsDataObject.ID}\": prev_ue_labels,\n            f\"new_{SilverUsualEnvironmentLabelsDataObject.ID}\": new_ue_labels,\n            SilverGeozonesGridMapDataObject.ID: grid_map_zones,\n        }\n        if not self.uniform_tile_weights:\n            self.input_data_objects[SilverEnrichedGridDataObject.ID] = enriched_grid\n\n        self.output_data_objects = {output_do.ID: output_do, quality_metrics_do.ID: quality_metrics_do}\n\n    @staticmethod\n    def get_migrating_users(\n        prev_home_tiles: DataFrame, new_home_tiles: DataFrame, migration_threshold: float\n    ) -&gt; tuple[DataFrame, DataFrame]:\n        \"\"\"Computes the overlap index for each device in order to determine if it will be used as a migrating device\n        or not. This method also computes the number of unique devices that have home tiles in the first long-term\n        period, in the second long-term period (quality metrics).\n\n        Args:\n            prev_home_tiles (DataFrame): home tiles of devices in the first long-term period\n            new_home_tiles (DataFrame): home tiles of devices in the second long-term period\n            migration_threshold (float): threshold to classify a device as migrating or not\n\n        Returns:\n            tuple[DataFrame, DataFrame]:\n                - First dataframe is the list of devices that have been classified as migrating\n                - Second dataframe contains the quality metrics\n        \"\"\"\n        joint_home_tiles = (\n            prev_home_tiles.join(\n                new_home_tiles,\n                how=\"full\",\n                on=(\n                    (prev_home_tiles[ColNames.user_id_modulo] == new_home_tiles[ColNames.user_id_modulo])\n                    &amp; (prev_home_tiles[ColNames.user_id] == new_home_tiles[ColNames.user_id])\n                    &amp; (prev_home_tiles[ColNames.grid_id] == new_home_tiles[ColNames.grid_id])\n                ),\n            )\n            .withColumn(\n                \"coalesced_user_id\", F.coalesce(prev_home_tiles[ColNames.user_id], new_home_tiles[ColNames.user_id])\n            )\n            .withColumn(\n                \"coalesced_modulo\",\n                F.coalesce(prev_home_tiles[ColNames.user_id_modulo], new_home_tiles[ColNames.user_id_modulo]),\n            )\n        )\n        joint_home_tiles.cache()\n\n        quality_metrics = joint_home_tiles.groupBy().agg(\n            F.count_distinct(prev_home_tiles[ColNames.user_id]).alias(\"previous_home_users\"),  # users in first LT\n            F.count_distinct(new_home_tiles[ColNames.user_id]).alias(\"new_home_users\"),  # users in second LT\n            F.count_distinct(\"coalesced_user_id\").alias(\"common_home_users\"),  # users in both LTs\n        )\n\n        migrating_users = (\n            joint_home_tiles.groupBy(\"coalesced_modulo\", \"coalesced_user_id\")\n            .agg(\n                (F.count(prev_home_tiles[ColNames.grid_id]) + F.count(new_home_tiles[ColNames.grid_id])).alias(\n                    \"total_count\"\n                ),\n                F.count(\n                    F.when(\n                        prev_home_tiles[ColNames.grid_id] == new_home_tiles[ColNames.grid_id],\n                        prev_home_tiles[ColNames.grid_id],\n                    )\n                ).alias(\"common_count\"),\n            )\n            .select(\n                F.col(\"coalesced_modulo\").alias(ColNames.user_id_modulo),\n                F.col(\"coalesced_user_id\").alias(ColNames.user_id),\n                (F.lit(2) * F.col(\"common_count\") / F.col(\"total_count\")).alias(\"overlap_index\"),\n            )\n            .where(F.col(\"overlap_index\") &lt; migration_threshold)\n            .select(ColNames.user_id_modulo, ColNames.user_id)\n        )\n        migrating_users.cache()\n\n        return migrating_users, quality_metrics\n\n    def get_zone_home_weights(\n        self, migrating_prev_home_tiles: DataFrame, migrating_new_home_tiles: DataFrame\n    ) -&gt; tuple[DataFrame, DataFrame]:\n        \"\"\"Computes the home weight that each user has from a set of zones to a different set of zones, based\n        on the weights assigned to its home tiles.\n\n        Args:\n            migrating_prev_home_tiles (DataFrame): home tiles of the first long-term period mapped to their resp. zones\n            migrating_new_home_tiles (DataFrame): home tiles of the second long-term period mapped to their resp. zones\n\n        Returns:\n            tuple[DataFrame, DataFrame]: dataframes that contain the device-level home weights for the first and\n                second long-term periods respectively.\n        \"\"\"\n        # Compute tile weight of each user-tile, and add up weight to zones\n        window = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id)\n\n        # If we are not using uniform weights, get land-use or prior probabilities to use as tile weights\n        if not self.uniform_tile_weights:\n            grid_sdf = self.input_data_objects[SilverEnrichedGridDataObject.ID].df\n            grid_sdf = grid_sdf.select(ColNames.grid_id, ColNames.prior_probability)\n            prev_weights = migrating_prev_home_tiles.join(grid_sdf, on=ColNames.grid_id, how=\"inner\").withColumn(\n                \"tile_weight\", ColNames.prior_probability / F.sum(ColNames.prior_probability).over(window)\n            )\n\n            new_weights = migrating_new_home_tiles.join(grid_sdf, on=ColNames.grid_id, how=\"inner\").withColumn(\n                \"tile_weight\", ColNames.prior_probability / F.sum(ColNames.prior_probability).over(window)\n            )\n        else:  # using uniform tile weights\n            prev_weights = migrating_prev_home_tiles.withColumn(\"tile_weight\", F.lit(1) / F.count(\"*\").over(window))\n\n            new_weights = migrating_new_home_tiles.withColumn(\"tile_weight\", F.lit(1) / F.count(\"*\").over(window))\n\n        prev_zones = (\n            prev_weights.groupBy(ColNames.user_id_modulo, ColNames.user_id, ColNames.zone_id)\n            .agg(F.sum(\"tile_weight\").alias(\"prev_weight\"))\n            .withColumnRenamed(ColNames.zone_id, ColNames.previous_zone)\n        )\n        new_zones = (\n            new_weights.groupBy(ColNames.user_id_modulo, ColNames.user_id, ColNames.zone_id)\n            .agg(F.sum(\"tile_weight\").alias(\"new_weight\"))\n            .withColumnRenamed(ColNames.zone_id, ColNames.new_zone)\n        )\n        return prev_zones, new_zones\n\n    def transform(self):\n        grid_to_zone = self.input_data_objects[\"SilverGeozonesGridMapDO\"].df\n        zone_to_grid_map_sdf = grid_to_zone.withColumn(\n            ColNames.zone_id,\n            F.element_at(F.split(F.col(ColNames.hierarchical_id), pattern=\"\\\\|\"), self.current_level),\n        )\n\n        # Read home labels for first long-term period\n        if self.prev_home_tiles is None:\n            self.prev_home_tiles = (\n                self.input_data_objects[f\"prev_{SilverUsualEnvironmentLabelsDataObject.ID}\"]\n                .df.where(F.col(ColNames.start_date) == F.lit(self.start_date_prev))\n                .where(F.col(ColNames.end_date) == F.lit(self.end_date_prev))\n                .where(F.col(ColNames.season) == F.lit(self.season_prev))\n                .where(F.col(ColNames.label) == F.lit(\"home\"))\n                .select(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id)\n            )\n            self.prev_home_tiles.cache()\n        # Read home labels for second long-term period\n        if self.new_home_tiles is None:\n            self.new_home_tiles = (\n                self.input_data_objects[f\"new_{SilverUsualEnvironmentLabelsDataObject.ID}\"]\n                .df.where(F.col(ColNames.start_date) == F.lit(self.start_date_new))\n                .where(F.col(ColNames.end_date) == F.lit(self.end_date_new))\n                .where(F.col(ColNames.season) == F.lit(self.season_new))\n                .where(F.col(ColNames.label) == F.lit(\"home\"))\n                .select(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id)\n            )\n            self.new_home_tiles.cache()\n\n        # Get list of users that are under the migration threshold, plus quality metrics\n        if self.migrating_users is None:\n            self.migrating_users, quality_metrics = self.get_migrating_users(\n                self.prev_home_tiles, self.new_home_tiles, self.migration_threshold\n            )\n            self.migrating_users.cache()\n\n        # Join with list of migrating users, and then join with the grid-to-zone mapping\n        migrating_prev_home_tiles = (\n            self.prev_home_tiles.withColumnRenamed(\"prev_grid_id\", ColNames.grid_id)\n            .join(self.migrating_users, on=[ColNames.user_id_modulo, ColNames.user_id])\n            .join(zone_to_grid_map_sdf, on=ColNames.grid_id)\n        )\n        migrating_new_home_tiles = (\n            self.new_home_tiles.withColumnRenamed(\"new_grid_id\", ColNames.grid_id)\n            .join(self.migrating_users, on=[ColNames.user_id_modulo, ColNames.user_id])\n            .join(zone_to_grid_map_sdf, on=ColNames.grid_id)\n        )\n\n        prev_zone_weights, new_zone_weights = self.get_zone_home_weights(\n            migrating_prev_home_tiles, migrating_new_home_tiles\n        )\n\n        # Join before and after zones with their weights\n        migration = (\n            prev_zone_weights.join(new_zone_weights, on=[ColNames.user_id_modulo, ColNames.user_id])\n            .where(F.col(ColNames.previous_zone) != F.col(ColNames.new_zone))  # we don't care about same-zone migration\n            .withColumn(ColNames.migration, F.col(\"prev_weight\") * F.col(\"new_weight\"))\n            .groupBy(ColNames.previous_zone, ColNames.new_zone)\n            .agg(F.sum(ColNames.migration).alias(ColNames.migration))\n        )\n\n        # Add additional partition key columns\n        migration = migration.withColumns(\n            {\n                ColNames.dataset_id: F.lit(self.zoning_dataset),\n                ColNames.level: F.lit(self.current_level),\n                ColNames.start_date_previous: F.lit(self.start_date_prev),\n                ColNames.end_date_previous: F.lit(self.end_date_prev),\n                ColNames.season_previous: F.lit(self.season_prev),\n                ColNames.start_date_new: F.lit(self.start_date_new),\n                ColNames.end_date_new: F.lit(self.end_date_new),\n                ColNames.season_new: F.lit(self.season_new),\n            }\n        )\n\n        migration = apply_schema_casting(migration, SilverInternalMigrationDataObject.SCHEMA)\n        self.output_data_objects[SilverInternalMigrationDataObject.ID].df = migration\n\n        # If quality metrics have not been written yet\n        if not self.quality_metrics_computed:\n            self.quality_metrics_computed = True\n            quality_metrics = quality_metrics.withColumns(\n                {\n                    ColNames.result_timestamp: F.current_timestamp(),\n                    ColNames.dataset_id: F.lit(self.zoning_dataset),\n                    ColNames.level: F.lit(self.current_level),\n                    ColNames.start_date_previous: F.lit(self.start_date_prev),\n                    ColNames.end_date_previous: F.lit(self.end_date_prev),\n                    ColNames.season_previous: F.lit(self.season_prev),\n                    ColNames.start_date_new: F.lit(self.start_date_new),\n                    ColNames.end_date_new: F.lit(self.end_date_new),\n                    ColNames.season_new: F.lit(self.season_new),\n                }\n            )\n            quality_metrics = apply_schema_casting(\n                quality_metrics, SilverInternalMigrationQualityMetricsDataObject.SCHEMA\n            )\n            self.output_data_objects[SilverInternalMigrationQualityMetricsDataObject.ID].df = quality_metrics\n        else:\n            # If it has already been written, we remove the key from the output DO dictionary so it is not written\n            # in the self.write() method\n            if SilverInternalMigrationQualityMetricsDataObject.ID in self.output_data_objects:\n                del self.output_data_objects[SilverInternalMigrationQualityMetricsDataObject.ID]\n\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n\n        for level in self.levels:\n            self.logger.info(f\"Starting migration estimation for hierarchical level {level}...\")\n            self.current_level = level\n            self.transform()\n            self.write()\n\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n</code></pre>"},{"location":"reference/components/execution/internal_migration/internal_migration/#components.execution.internal_migration.internal_migration.InternalMigration.get_migrating_users","title":"<code>get_migrating_users(prev_home_tiles, new_home_tiles, migration_threshold)</code>  <code>staticmethod</code>","text":"<p>Computes the overlap index for each device in order to determine if it will be used as a migrating device or not. This method also computes the number of unique devices that have home tiles in the first long-term period, in the second long-term period (quality metrics).</p> <p>Parameters:</p> Name Type Description Default <code>prev_home_tiles</code> <code>DataFrame</code> <p>home tiles of devices in the first long-term period</p> required <code>new_home_tiles</code> <code>DataFrame</code> <p>home tiles of devices in the second long-term period</p> required <code>migration_threshold</code> <code>float</code> <p>threshold to classify a device as migrating or not</p> required <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame]</code> <p>tuple[DataFrame, DataFrame]: - First dataframe is the list of devices that have been classified as migrating - Second dataframe contains the quality metrics</p> Source code in <code>multimno/components/execution/internal_migration/internal_migration.py</code> <pre><code>@staticmethod\ndef get_migrating_users(\n    prev_home_tiles: DataFrame, new_home_tiles: DataFrame, migration_threshold: float\n) -&gt; tuple[DataFrame, DataFrame]:\n    \"\"\"Computes the overlap index for each device in order to determine if it will be used as a migrating device\n    or not. This method also computes the number of unique devices that have home tiles in the first long-term\n    period, in the second long-term period (quality metrics).\n\n    Args:\n        prev_home_tiles (DataFrame): home tiles of devices in the first long-term period\n        new_home_tiles (DataFrame): home tiles of devices in the second long-term period\n        migration_threshold (float): threshold to classify a device as migrating or not\n\n    Returns:\n        tuple[DataFrame, DataFrame]:\n            - First dataframe is the list of devices that have been classified as migrating\n            - Second dataframe contains the quality metrics\n    \"\"\"\n    joint_home_tiles = (\n        prev_home_tiles.join(\n            new_home_tiles,\n            how=\"full\",\n            on=(\n                (prev_home_tiles[ColNames.user_id_modulo] == new_home_tiles[ColNames.user_id_modulo])\n                &amp; (prev_home_tiles[ColNames.user_id] == new_home_tiles[ColNames.user_id])\n                &amp; (prev_home_tiles[ColNames.grid_id] == new_home_tiles[ColNames.grid_id])\n            ),\n        )\n        .withColumn(\n            \"coalesced_user_id\", F.coalesce(prev_home_tiles[ColNames.user_id], new_home_tiles[ColNames.user_id])\n        )\n        .withColumn(\n            \"coalesced_modulo\",\n            F.coalesce(prev_home_tiles[ColNames.user_id_modulo], new_home_tiles[ColNames.user_id_modulo]),\n        )\n    )\n    joint_home_tiles.cache()\n\n    quality_metrics = joint_home_tiles.groupBy().agg(\n        F.count_distinct(prev_home_tiles[ColNames.user_id]).alias(\"previous_home_users\"),  # users in first LT\n        F.count_distinct(new_home_tiles[ColNames.user_id]).alias(\"new_home_users\"),  # users in second LT\n        F.count_distinct(\"coalesced_user_id\").alias(\"common_home_users\"),  # users in both LTs\n    )\n\n    migrating_users = (\n        joint_home_tiles.groupBy(\"coalesced_modulo\", \"coalesced_user_id\")\n        .agg(\n            (F.count(prev_home_tiles[ColNames.grid_id]) + F.count(new_home_tiles[ColNames.grid_id])).alias(\n                \"total_count\"\n            ),\n            F.count(\n                F.when(\n                    prev_home_tiles[ColNames.grid_id] == new_home_tiles[ColNames.grid_id],\n                    prev_home_tiles[ColNames.grid_id],\n                )\n            ).alias(\"common_count\"),\n        )\n        .select(\n            F.col(\"coalesced_modulo\").alias(ColNames.user_id_modulo),\n            F.col(\"coalesced_user_id\").alias(ColNames.user_id),\n            (F.lit(2) * F.col(\"common_count\") / F.col(\"total_count\")).alias(\"overlap_index\"),\n        )\n        .where(F.col(\"overlap_index\") &lt; migration_threshold)\n        .select(ColNames.user_id_modulo, ColNames.user_id)\n    )\n    migrating_users.cache()\n\n    return migrating_users, quality_metrics\n</code></pre>"},{"location":"reference/components/execution/internal_migration/internal_migration/#components.execution.internal_migration.internal_migration.InternalMigration.get_zone_home_weights","title":"<code>get_zone_home_weights(migrating_prev_home_tiles, migrating_new_home_tiles)</code>","text":"<p>Computes the home weight that each user has from a set of zones to a different set of zones, based on the weights assigned to its home tiles.</p> <p>Parameters:</p> Name Type Description Default <code>migrating_prev_home_tiles</code> <code>DataFrame</code> <p>home tiles of the first long-term period mapped to their resp. zones</p> required <code>migrating_new_home_tiles</code> <code>DataFrame</code> <p>home tiles of the second long-term period mapped to their resp. zones</p> required <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame]</code> <p>tuple[DataFrame, DataFrame]: dataframes that contain the device-level home weights for the first and second long-term periods respectively.</p> Source code in <code>multimno/components/execution/internal_migration/internal_migration.py</code> <pre><code>def get_zone_home_weights(\n    self, migrating_prev_home_tiles: DataFrame, migrating_new_home_tiles: DataFrame\n) -&gt; tuple[DataFrame, DataFrame]:\n    \"\"\"Computes the home weight that each user has from a set of zones to a different set of zones, based\n    on the weights assigned to its home tiles.\n\n    Args:\n        migrating_prev_home_tiles (DataFrame): home tiles of the first long-term period mapped to their resp. zones\n        migrating_new_home_tiles (DataFrame): home tiles of the second long-term period mapped to their resp. zones\n\n    Returns:\n        tuple[DataFrame, DataFrame]: dataframes that contain the device-level home weights for the first and\n            second long-term periods respectively.\n    \"\"\"\n    # Compute tile weight of each user-tile, and add up weight to zones\n    window = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id)\n\n    # If we are not using uniform weights, get land-use or prior probabilities to use as tile weights\n    if not self.uniform_tile_weights:\n        grid_sdf = self.input_data_objects[SilverEnrichedGridDataObject.ID].df\n        grid_sdf = grid_sdf.select(ColNames.grid_id, ColNames.prior_probability)\n        prev_weights = migrating_prev_home_tiles.join(grid_sdf, on=ColNames.grid_id, how=\"inner\").withColumn(\n            \"tile_weight\", ColNames.prior_probability / F.sum(ColNames.prior_probability).over(window)\n        )\n\n        new_weights = migrating_new_home_tiles.join(grid_sdf, on=ColNames.grid_id, how=\"inner\").withColumn(\n            \"tile_weight\", ColNames.prior_probability / F.sum(ColNames.prior_probability).over(window)\n        )\n    else:  # using uniform tile weights\n        prev_weights = migrating_prev_home_tiles.withColumn(\"tile_weight\", F.lit(1) / F.count(\"*\").over(window))\n\n        new_weights = migrating_new_home_tiles.withColumn(\"tile_weight\", F.lit(1) / F.count(\"*\").over(window))\n\n    prev_zones = (\n        prev_weights.groupBy(ColNames.user_id_modulo, ColNames.user_id, ColNames.zone_id)\n        .agg(F.sum(\"tile_weight\").alias(\"prev_weight\"))\n        .withColumnRenamed(ColNames.zone_id, ColNames.previous_zone)\n    )\n    new_zones = (\n        new_weights.groupBy(ColNames.user_id_modulo, ColNames.user_id, ColNames.zone_id)\n        .agg(F.sum(\"tile_weight\").alias(\"new_weight\"))\n        .withColumnRenamed(ColNames.zone_id, ColNames.new_zone)\n    )\n    return prev_zones, new_zones\n</code></pre>"},{"location":"reference/components/execution/kanonimity/","title":"kanonimity","text":""},{"location":"reference/components/execution/kanonimity/kanonimity/","title":"kanonimity","text":"<p>Module that implements the k-anonimity component</p>"},{"location":"reference/components/execution/kanonimity/kanonimity/#components.execution.kanonimity.kanonimity.KAnonimity","title":"<code>KAnonimity</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class responsible for applying k-anonimity of the target column of a specific data object, either by obfuscating or deleting rows/registers where the target value is lower than a specified value k.</p> Source code in <code>multimno/components/execution/kanonimity/kanonimity.py</code> <pre><code>class KAnonimity(Component):\n    \"\"\"\n    Class responsible for applying k-anonimity of the target column of a specific data object, either by obfuscating\n    or deleting rows/registers where the target value is lower than a specified value k.\n    \"\"\"\n\n    COMPONENT_ID = \"KAnonimity\"\n    ANONIMITY_TYPES = [\"obfuscate\", \"delete\"]\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.current_component_id: str = None\n        self.target_column: str = None\n        self.k: int = None\n        self.anonimity_type: str = None\n\n    def initalize_data_objects(self):\n\n        self.execute_present_population = self.config.getboolean(self.COMPONENT_ID, \"present_population_execution\")\n        self.execute_usual_environment = self.config.getboolean(self.COMPONENT_ID, \"usual_environment_execution\")\n        self.execute_internal_migration = self.config.getboolean(self.COMPONENT_ID, \"internal_migration_execution\")\n\n        self.data_objects = {}\n\n        if self.execute_present_population:\n            input_do_path = self.config.get(\n                CONFIG_SILVER_PATHS_KEY,\n                CLASS_MAPPING[PresentPopulationEstimation.COMPONENT_ID][\"input_path_config_key\"],\n            )\n            output_do_path = self.config.get(\n                CONFIG_GOLD_PATHS_KEY,\n                CLASS_MAPPING[PresentPopulationEstimation.COMPONENT_ID][\"output_path_config_key\"],\n            )\n\n            if not check_if_data_path_exists(self.spark, input_do_path):\n                self.logger.warning(f\"Expected path {input_do_path} to exist but it does not\")\n                raise ValueError(\n                    f\"Invalid path for {CLASS_MAPPING[PresentPopulationEstimation.COMPONENT_ID]['constructor'].ID}: {input_do_path}\"\n                )\n\n            clear_destination_directory = self.config.getboolean(\n                f\"{self.COMPONENT_ID}.PresentPopulationEstimation\", \"clear_destination_directory\"\n            )\n            if clear_destination_directory:\n                delete_file_or_folder(self.spark, output_do_path)\n\n            input_do = CLASS_MAPPING[PresentPopulationEstimation.COMPONENT_ID][\"constructor\"](self.spark, input_do_path)\n            output_do = CLASS_MAPPING[PresentPopulationEstimation.COMPONENT_ID][\"constructor\"](\n                self.spark, output_do_path\n            )\n\n            self.data_objects[PresentPopulationEstimation.COMPONENT_ID] = {\"input\": input_do, \"output\": output_do}\n\n        if self.execute_usual_environment:\n            input_do_path = self.config.get(\n                CONFIG_SILVER_PATHS_KEY,\n                CLASS_MAPPING[UsualEnvironmentAggregation.COMPONENT_ID][\"input_path_config_key\"],\n            )\n            output_do_path = self.config.get(\n                CONFIG_GOLD_PATHS_KEY,\n                CLASS_MAPPING[UsualEnvironmentAggregation.COMPONENT_ID][\"output_path_config_key\"],\n            )\n\n            if not check_if_data_path_exists(self.spark, input_do_path):\n                self.logger.warning(f\"Expected path {input_do_path} to exist but it does not\")\n                raise ValueError(\n                    f\"Invalid path for {CLASS_MAPPING[UsualEnvironmentAggregation.COMPONENT_ID]['constructor'].ID}: {input_do_path}\"\n                )\n\n            clear_destination_directory = self.config.getboolean(\n                f\"{self.COMPONENT_ID}.UsualEnvironmentAggregation\", \"clear_destination_directory\"\n            )\n            if clear_destination_directory:\n                delete_file_or_folder(self.spark, output_do_path)\n\n            input_do = CLASS_MAPPING[UsualEnvironmentAggregation.COMPONENT_ID][\"constructor\"](self.spark, input_do_path)\n            output_do = CLASS_MAPPING[UsualEnvironmentAggregation.COMPONENT_ID][\"constructor\"](\n                self.spark, output_do_path\n            )\n\n            self.data_objects[UsualEnvironmentAggregation.COMPONENT_ID] = {\"input\": input_do, \"output\": output_do}\n\n        if self.execute_internal_migration:\n            input_do_path = self.config.get(\n                CONFIG_SILVER_PATHS_KEY,\n                CLASS_MAPPING[InternalMigration.COMPONENT_ID][\"input_path_config_key\"],\n            )\n            output_do_path = self.config.get(\n                CONFIG_GOLD_PATHS_KEY,\n                CLASS_MAPPING[InternalMigration.COMPONENT_ID][\"output_path_config_key\"],\n            )\n\n            if not check_if_data_path_exists(self.spark, input_do_path):\n                self.logger.warning(f\"Expected path {input_do_path} to exist but it does not\")\n                raise ValueError(\n                    f\"Invalid path for {CLASS_MAPPING[InternalMigration.COMPONENT_ID]['constructor'].ID}: {input_do_path}\"\n                )\n\n            clear_destination_directory = self.config.getboolean(\n                f\"{self.COMPONENT_ID}.InternalMigration\", \"clear_destination_directory\"\n            )\n            if clear_destination_directory:\n                delete_file_or_folder(self.spark, output_do_path)\n\n            input_do = CLASS_MAPPING[InternalMigration.COMPONENT_ID][\"constructor\"](self.spark, input_do_path)\n            output_do = CLASS_MAPPING[InternalMigration.COMPONENT_ID][\"constructor\"](self.spark, output_do_path)\n\n            self.data_objects[InternalMigration.COMPONENT_ID] = {\"input\": input_do, \"output\": output_do}\n\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        if len(self.data_objects) == 0:\n            self.logger.info(\"No execution requested in config file -- finishing without performing any operation...\")\n            self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n            return\n\n        for component_id in self.data_objects:\n            self.logger.info(f\"Working on {self.COMPONENT_ID}.{component_id}...\")\n            self.current_component_id = component_id\n            self.target_column = CLASS_MAPPING[self.current_component_id][\"target_column\"]\n\n            self.k = self.config.getint(f\"{self.COMPONENT_ID}.{component_id}\", \"k\")\n            self.anonimity_type = self.config.get(f\"{self.COMPONENT_ID}.{component_id}\", \"anonimity_type\")\n            if self.anonimity_type not in self.ANONIMITY_TYPES:\n                raise ValueError(\n                    f\"unknown anonimity type `{self.anonimity_type}` -- must be one of {self.ANONIMITY_TYPES}\"\n                )\n\n            self.input_data_objects = {\n                self.data_objects[component_id][\"input\"].ID: self.data_objects[component_id][\"input\"]\n            }\n\n            self.output_data_objects = {\n                self.data_objects[component_id][\"output\"].ID: self.data_objects[component_id][\"output\"]\n            }\n\n            self.read()\n            self.transform()\n            self.write()\n\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def filter_dataframe(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Filtering function that takes the partitions of the dataframe specified via configuration file\n\n        Args:\n            df (DataFrame): original DataFrame\n\n        Raises:\n            ValueError: if `season` value in configuration file is not one of allowed values\n\n        Returns:\n            DataFrame: filtered DataFrame\n        \"\"\"\n        # TODO: move config reading and validation to __init__ (?)\n        if self.current_component_id == PresentPopulationEstimation.COMPONENT_ID:\n            zoning_dataset = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"zoning_dataset_id\")\n            levels = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"hierarchical_levels\")\n            levels = list(int(x.strip()) for x in levels.split(\",\"))\n\n            start_date = dt.datetime.strptime(\n                self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"start_date\"), \"%Y-%m-%d\"\n            )\n            end_date = dt.datetime.strptime(\n                self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"end_date\"), \"%Y-%m-%d\"\n            )\n\n            df = (\n                df.where(F.col(ColNames.dataset_id) == F.lit(zoning_dataset))\n                .where(F.col(ColNames.level).isin(levels))\n                .where(F.make_date(ColNames.year, ColNames.month, ColNames.day).between(start_date, end_date))\n            )\n\n        if self.current_component_id == UsualEnvironmentAggregation.COMPONENT_ID:\n            zoning_dataset = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"zoning_dataset_id\")\n            levels = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"hierarchical_levels\")\n            levels = list(int(x.strip()) for x in levels.split(\",\"))\n            labels = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"labels\")\n            labels = list(x.strip() for x in labels.split(\",\"))\n\n            start_date = dt.datetime.strptime(\n                self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"start_month\"), \"%Y-%m\"\n            )\n            end_date = dt.datetime.strptime(\n                self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"end_month\"), \"%Y-%m\"\n            )\n            end_date = end_date + dt.timedelta(days=cal.monthrange(end_date.year, end_date.month)[1] - 1)\n            season = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"season\")\n            if season not in SEASONS:\n                raise ValueError(f\"Unknown season {season} -- valid values are {SEASONS}\")\n\n            df = (\n                df.where(F.col(ColNames.dataset_id) == zoning_dataset)\n                .where(F.col(ColNames.level).isin(levels))\n                .where(F.col(ColNames.label).isin(labels))\n                .where(F.col(ColNames.start_date) == start_date)\n                .where(F.col(ColNames.end_date) == end_date)\n                .where(F.col(ColNames.season) == season)\n            )\n\n        if self.current_component_id == InternalMigration.COMPONENT_ID:\n            zoning_dataset = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"zoning_dataset_id\")\n            levels = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"hierarchical_levels\")\n            levels = list(int(x.strip()) for x in levels.split(\",\"))\n\n            start_date_prev = dt.datetime.strptime(\n                self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"start_month_previous\"), \"%Y-%m\"\n            )\n            end_date_prev = dt.datetime.strptime(\n                self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"end_month_previous\"), \"%Y-%m\"\n            )\n            end_date_prev = end_date_prev + dt.timedelta(\n                days=cal.monthrange(end_date_prev.year, end_date_prev.month)[1] - 1\n            )\n            season_prev = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"season_previous\")\n            if season_prev not in SEASONS:\n                raise ValueError(f\"Unknown season {season_prev} -- valid values are {SEASONS}\")\n\n            start_date_new = dt.datetime.strptime(\n                self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"start_month_new\"), \"%Y-%m\"\n            )\n            end_date_new = dt.datetime.strptime(\n                self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"end_month_new\"), \"%Y-%m\"\n            )\n            end_date_new = end_date_new + dt.timedelta(\n                days=cal.monthrange(end_date_new.year, end_date_new.month)[1] - 1\n            )\n            season_new = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"season_new\")\n            if season_new not in SEASONS:\n                raise ValueError(f\"Unknown season {season_new} -- valid values are {SEASONS}\")\n\n            df = (\n                df.where(F.col(ColNames.dataset_id) == zoning_dataset)\n                .where(F.col(ColNames.level).isin(levels))\n                .where(F.col(ColNames.start_date_previous) == start_date_prev)\n                .where(F.col(ColNames.end_date_previous) == end_date_prev)\n                .where(F.col(ColNames.season_previous) == season_prev)\n                .where(F.col(ColNames.start_date_new) == start_date_new)\n                .where(F.col(ColNames.end_date_new) == end_date_new)\n                .where(F.col(ColNames.season_new) == season_new)\n            )\n        return df\n\n    def apply_anonimity_obfuscation(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Apply obfuscation at the target column, so all values in the target column that are strictly lower than\n        the threshold value k will be replaced by the value `-1`.\n\n        Args:\n            df (DataFrame): original DataFrame to perform k-anonimity to\n\n        Returns:\n            DataFrame: DataFrame after applying obfuscation\n        \"\"\"\n        df = df.withColumn(\n            self.target_column,\n            F.when(\n                F.col(self.target_column) &lt; F.lit(self.k),\n                F.lit(-1).cast(\n                    CLASS_MAPPING[self.current_component_id][\"constructor\"].SCHEMA[self.target_column].dataType\n                ),\n            ).otherwise(F.col(self.target_column)),\n        )\n        return df\n\n    def apply_anonimity_deletion(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Apply deletion at the target column, deleting all rows where the value in the target column is strictly lower\n        than the threshold value k.\n\n        Args:\n            df (DataFrame): original DataFrame to perform k-anonimity to\n\n        Returns:\n            DataFrame: DataFrame after applying deletion\n        \"\"\"\n        df = df.where(F.col(self.target_column) &gt;= F.lit(self.k))\n        return df\n\n    def transform(self):\n        df = self.input_data_objects[CLASS_MAPPING[self.current_component_id][\"constructor\"].ID].df\n\n        # First, filter based on partition and dates\n        df = self.filter_dataframe(df)\n\n        if self.anonimity_type == \"obfuscate\":\n            df = self.apply_anonimity_obfuscation(df)\n        elif self.anonimity_type == \"delete\":\n            df = self.apply_anonimity_deletion(df)\n        else:\n            error_message = (\n                f\"Unknown anonimity type `{self.anonimity_type}`. Supported types are {self.ANONIMITY_TYPES}\"\n            )\n            self.logger.error(error_message)\n            raise ValueError(error_message)\n\n        self.output_data_objects[CLASS_MAPPING[self.current_component_id][\"constructor\"].ID].df = df\n</code></pre>"},{"location":"reference/components/execution/kanonimity/kanonimity/#components.execution.kanonimity.kanonimity.KAnonimity.apply_anonimity_deletion","title":"<code>apply_anonimity_deletion(df)</code>","text":"<p>Apply deletion at the target column, deleting all rows where the value in the target column is strictly lower than the threshold value k.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>original DataFrame to perform k-anonimity to</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame after applying deletion</p> Source code in <code>multimno/components/execution/kanonimity/kanonimity.py</code> <pre><code>def apply_anonimity_deletion(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Apply deletion at the target column, deleting all rows where the value in the target column is strictly lower\n    than the threshold value k.\n\n    Args:\n        df (DataFrame): original DataFrame to perform k-anonimity to\n\n    Returns:\n        DataFrame: DataFrame after applying deletion\n    \"\"\"\n    df = df.where(F.col(self.target_column) &gt;= F.lit(self.k))\n    return df\n</code></pre>"},{"location":"reference/components/execution/kanonimity/kanonimity/#components.execution.kanonimity.kanonimity.KAnonimity.apply_anonimity_obfuscation","title":"<code>apply_anonimity_obfuscation(df)</code>","text":"<p>Apply obfuscation at the target column, so all values in the target column that are strictly lower than the threshold value k will be replaced by the value <code>-1</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>original DataFrame to perform k-anonimity to</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame after applying obfuscation</p> Source code in <code>multimno/components/execution/kanonimity/kanonimity.py</code> <pre><code>def apply_anonimity_obfuscation(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Apply obfuscation at the target column, so all values in the target column that are strictly lower than\n    the threshold value k will be replaced by the value `-1`.\n\n    Args:\n        df (DataFrame): original DataFrame to perform k-anonimity to\n\n    Returns:\n        DataFrame: DataFrame after applying obfuscation\n    \"\"\"\n    df = df.withColumn(\n        self.target_column,\n        F.when(\n            F.col(self.target_column) &lt; F.lit(self.k),\n            F.lit(-1).cast(\n                CLASS_MAPPING[self.current_component_id][\"constructor\"].SCHEMA[self.target_column].dataType\n            ),\n        ).otherwise(F.col(self.target_column)),\n    )\n    return df\n</code></pre>"},{"location":"reference/components/execution/kanonimity/kanonimity/#components.execution.kanonimity.kanonimity.KAnonimity.filter_dataframe","title":"<code>filter_dataframe(df)</code>","text":"<p>Filtering function that takes the partitions of the dataframe specified via configuration file</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>original DataFrame</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>season</code> value in configuration file is not one of allowed values</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>filtered DataFrame</p> Source code in <code>multimno/components/execution/kanonimity/kanonimity.py</code> <pre><code>def filter_dataframe(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Filtering function that takes the partitions of the dataframe specified via configuration file\n\n    Args:\n        df (DataFrame): original DataFrame\n\n    Raises:\n        ValueError: if `season` value in configuration file is not one of allowed values\n\n    Returns:\n        DataFrame: filtered DataFrame\n    \"\"\"\n    # TODO: move config reading and validation to __init__ (?)\n    if self.current_component_id == PresentPopulationEstimation.COMPONENT_ID:\n        zoning_dataset = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"zoning_dataset_id\")\n        levels = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"hierarchical_levels\")\n        levels = list(int(x.strip()) for x in levels.split(\",\"))\n\n        start_date = dt.datetime.strptime(\n            self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"start_date\"), \"%Y-%m-%d\"\n        )\n        end_date = dt.datetime.strptime(\n            self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"end_date\"), \"%Y-%m-%d\"\n        )\n\n        df = (\n            df.where(F.col(ColNames.dataset_id) == F.lit(zoning_dataset))\n            .where(F.col(ColNames.level).isin(levels))\n            .where(F.make_date(ColNames.year, ColNames.month, ColNames.day).between(start_date, end_date))\n        )\n\n    if self.current_component_id == UsualEnvironmentAggregation.COMPONENT_ID:\n        zoning_dataset = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"zoning_dataset_id\")\n        levels = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"hierarchical_levels\")\n        levels = list(int(x.strip()) for x in levels.split(\",\"))\n        labels = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"labels\")\n        labels = list(x.strip() for x in labels.split(\",\"))\n\n        start_date = dt.datetime.strptime(\n            self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"start_month\"), \"%Y-%m\"\n        )\n        end_date = dt.datetime.strptime(\n            self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"end_month\"), \"%Y-%m\"\n        )\n        end_date = end_date + dt.timedelta(days=cal.monthrange(end_date.year, end_date.month)[1] - 1)\n        season = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"season\")\n        if season not in SEASONS:\n            raise ValueError(f\"Unknown season {season} -- valid values are {SEASONS}\")\n\n        df = (\n            df.where(F.col(ColNames.dataset_id) == zoning_dataset)\n            .where(F.col(ColNames.level).isin(levels))\n            .where(F.col(ColNames.label).isin(labels))\n            .where(F.col(ColNames.start_date) == start_date)\n            .where(F.col(ColNames.end_date) == end_date)\n            .where(F.col(ColNames.season) == season)\n        )\n\n    if self.current_component_id == InternalMigration.COMPONENT_ID:\n        zoning_dataset = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"zoning_dataset_id\")\n        levels = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"hierarchical_levels\")\n        levels = list(int(x.strip()) for x in levels.split(\",\"))\n\n        start_date_prev = dt.datetime.strptime(\n            self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"start_month_previous\"), \"%Y-%m\"\n        )\n        end_date_prev = dt.datetime.strptime(\n            self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"end_month_previous\"), \"%Y-%m\"\n        )\n        end_date_prev = end_date_prev + dt.timedelta(\n            days=cal.monthrange(end_date_prev.year, end_date_prev.month)[1] - 1\n        )\n        season_prev = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"season_previous\")\n        if season_prev not in SEASONS:\n            raise ValueError(f\"Unknown season {season_prev} -- valid values are {SEASONS}\")\n\n        start_date_new = dt.datetime.strptime(\n            self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"start_month_new\"), \"%Y-%m\"\n        )\n        end_date_new = dt.datetime.strptime(\n            self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"end_month_new\"), \"%Y-%m\"\n        )\n        end_date_new = end_date_new + dt.timedelta(\n            days=cal.monthrange(end_date_new.year, end_date_new.month)[1] - 1\n        )\n        season_new = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"season_new\")\n        if season_new not in SEASONS:\n            raise ValueError(f\"Unknown season {season_new} -- valid values are {SEASONS}\")\n\n        df = (\n            df.where(F.col(ColNames.dataset_id) == zoning_dataset)\n            .where(F.col(ColNames.level).isin(levels))\n            .where(F.col(ColNames.start_date_previous) == start_date_prev)\n            .where(F.col(ColNames.end_date_previous) == end_date_prev)\n            .where(F.col(ColNames.season_previous) == season_prev)\n            .where(F.col(ColNames.start_date_new) == start_date_new)\n            .where(F.col(ColNames.end_date_new) == end_date_new)\n            .where(F.col(ColNames.season_new) == season_new)\n        )\n    return df\n</code></pre>"},{"location":"reference/components/execution/longterm_permanence_score/","title":"longterm_permanence_score","text":""},{"location":"reference/components/execution/longterm_permanence_score/longterm_permanence_score/","title":"longterm_permanence_score","text":"<p>Module that computes the Long-term Permanence Score</p>"},{"location":"reference/components/execution/longterm_permanence_score/longterm_permanence_score/#components.execution.longterm_permanence_score.longterm_permanence_score.LongtermPermanenceScore","title":"<code>LongtermPermanenceScore</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class that computes the long term permanence score and related metrics, for different combinations of seasons, day types and time intervals.</p> Source code in <code>multimno/components/execution/longterm_permanence_score/longterm_permanence_score.py</code> <pre><code>class LongtermPermanenceScore(Component):\n    \"\"\"\n    Class that computes the long term permanence score and related metrics, for different combinations of\n    seasons, day types and time intervals.\n    \"\"\"\n\n    COMPONENT_ID = \"LongtermPermanenceScore\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        # Months that compose the long-term period, at least one\n        start_month = self.config.get(self.COMPONENT_ID, \"start_month\")\n        try:\n            self.start_date = dt.datetime.strptime(start_month, \"%Y-%m\").date()\n        except ValueError as e:\n            self.logger.error(f\"Could not parse start_month = `{start_month}`. Expected format: YYYY-MM\")\n            raise e\n\n        end_month = self.config.get(self.COMPONENT_ID, \"end_month\")\n        try:\n            self.end_date = dt.datetime.strptime(end_month, \"%Y-%m\").date()\n        except ValueError as e:\n            self.logger.error(f\"Could not parse end_month = `{end_month}`. Expected format: YYYY-MM\")\n            raise e\n        self.end_date = self.end_date.replace(day=cal.monthrange(self.end_date.year, self.end_date.month)[1])\n\n        if self.end_date &lt; self.start_date:\n            raise ValueError(f\"End month `{end_month}` should not be an earlier than start month `{start_month}`\")\n\n        # Get months of the year in each season\n        self.winter_months = self._get_month_list(self.config.get(self.COMPONENT_ID, \"winter_months\"), \"winter\")\n        self.spring_months = self._get_month_list(self.config.get(self.COMPONENT_ID, \"spring_months\"), \"spring\")\n        self.summer_months = self._get_month_list(self.config.get(self.COMPONENT_ID, \"summer_months\"), \"summer\")\n        self.autumn_months = self._get_month_list(self.config.get(self.COMPONENT_ID, \"autumn_months\"), \"autumn\")\n\n        # Check for possible repeated months\n        all_season_months = self.winter_months + self.spring_months + self.summer_months + self.autumn_months\n        if len(all_season_months) != len(set(all_season_months)):\n            raise ValueError(\"at least one month belongs to more than one season -- please correct input parameters\")\n\n        # Read all subyearly-submonthly-subdaily combinations\n        period_combinations = self.config.geteval(self.COMPONENT_ID, \"period_combinations\")\n        self.period_combinations = {}\n        for season, day_types_dict in period_combinations.items():\n            if season.lower() not in SEASONS:\n                raise ValueError(f\"Unknown season `{season}` in period_combinations\")\n            self.period_combinations[season.lower()] = {}\n            for day_type, time_intervals in day_types_dict.items():\n                if day_type.lower() not in DAY_TYPES:\n                    raise ValueError(f\"Uknown day_type `{day_type}` under `{season}` in period_combinations\")\n                self.period_combinations[season.lower()][day_type.lower()] = []\n                if len(time_intervals) != len(set(time_intervals)):\n                    raise ValueError(\n                        f\"Repeated values for time_interval in under `{season}` and `{day_type}`:\",\n                        str(period_combinations[season][day_type]),\n                    )\n                for time_interval in time_intervals:\n                    if time_interval not in TIME_INTERVALS:\n                        raise ValueError(f\"Unknown time_interval `{time_interval}` under `{season}` and `{day_type}`\")\n                    self.period_combinations[season.lower()][day_type.lower()].append(time_interval)\n\n        # Check that all seasons for which an analysis is to be performed have been assigned at least one month\n        for season, periods in self.period_combinations.items():\n            if season == \"all\":\n                continue\n            if len(periods) &gt; 0 and len(getattr(self, f\"{season}_months\")) == 0:\n                raise ValueError(\n                    f\"Some period combinations have been requested for season `{season}` but no \"\n                    \"month has been included in this season -- please correct the configuration \"\n                    \"parameters\"\n                )\n\n        # Get list of all the months that will be used, together with the seasons they belong to\n        self.longterm_months = []\n        self.season_months = {}\n\n        for season in self.period_combinations:\n            self.season_months[season] = []\n\n        month_start_date = self.start_date\n        while month_start_date &lt; self.end_date:\n            self.longterm_months.append(month_start_date)\n            if \"all\" in self.season_months:\n                self.season_months[\"all\"].append(month_start_date)\n            if \"winter\" in self.season_months and month_start_date.month in self.winter_months:\n                self.season_months[\"winter\"].append(month_start_date)\n            if \"spring\" in self.season_months and month_start_date.month in self.spring_months:\n                self.season_months[\"spring\"].append(month_start_date)\n            if \"summer\" in self.season_months and month_start_date.month in self.summer_months:\n                self.season_months[\"summer\"].append(month_start_date)\n            if \"autumn\" in self.season_months and month_start_date.month in self.autumn_months:\n                self.season_months[\"autumn\"].append(month_start_date)\n\n            month_start_date += dt.timedelta(days=cal.monthrange(month_start_date.year, month_start_date.month)[1])\n\n        # Initialise variables for working with each longterm analysis\n        self.current_lt_analysis = None\n        self.longterm_analyses = None\n\n    def _get_month_list(self, months_input: str, context: str) -&gt; List[int]:\n        \"\"\"Read and parse a comma-separated list of months that will be assigned to a particular season. Months are\n        represented by an integer from 1 to 12 and must not be repeated within the list\n\n\n        Args:\n            months_input (str): comma-separated list of months to be parsed\n            context (str): name of the season, used for error tracking\n\n        Raises:\n            e: could not parse month to integer\n            ValueError: integer is not one between 1 and 12, inclusive\n            ValueError: repeated integers\n\n        Returns:\n            List[int]: list of integers representing the months of the year that will constitute a season\n        \"\"\"\n        months_input = months_input.replace(\" \", \"\").replace(\"\\t\", \"\")\n        if months_input == \"\":\n            return []\n\n        months_input = months_input.split(\",\")\n        months = []\n\n        for mm in months_input:\n            try:\n                mm = int(mm)\n            except ValueError as e:\n                self.logger.error(f\"expected integer as a month for {context} season, but found `{mm}`\")\n                raise e\n            if mm &lt; 1 or mm &gt; 12:\n                raise ValueError(\n                    f\"expected integer between 1 and 12 to represent a month for {context} season, \" f\"but found `{mm}`\"\n                )\n            months.append(mm)\n\n        if len(months) != len(set(months)):\n            raise ValueError(f\"found repeated month {months} in season {context} -- please remove any duplicates\")\n\n        return months\n\n    def initalize_data_objects(self):\n        input_silver_midterm_ps_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"midterm_permanence_score_data_silver\")\n        output_silver_longterm_ps_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"longterm_permanence_score_data_silver\"\n        )\n\n        midterm_ps = SilverMidtermPermanenceScoreDataObject(self.spark, input_silver_midterm_ps_path)\n        longterm_ps = SilverLongtermPermanenceScoreDataObject(self.spark, output_silver_longterm_ps_path)\n\n        self.input_data_objects = {midterm_ps.ID: midterm_ps}\n\n        self.output_data_objects = {longterm_ps.ID: longterm_ps}\n\n    def _check_midterm_data_exist(self) -&gt; List[dict]:\n        \"\"\"Checks that the mid-term permanence score data necessary for carrying out all long-term analyses exist,\n        based on the day_types and time_intervals requested for each analysis. Returns a list of dictionaries\n        containing the information necessary to filter the required mid-term analysis data of each analysis.\n\n        Raises:\n            FileNotFoundError: Whenever a certain (day_type, time_interval) combination has not been found in the\n                data of a particular month, required to compute some long-term analysis.\n\n        Returns:\n            List[dict]: information of each long-term analysis to be performed\n        \"\"\"\n        midterm_df = self.input_data_objects[SilverMidtermPermanenceScoreDataObject.ID].df\n        longterm_analyses = []\n        already_checked = set()\n        # For each season\n        for season, months in self.season_months.items():\n            # Check that all months of that season have the mid-term PS data of the required day type and time interval\n            for day_type, time_intervals in self.period_combinations[season].items():\n                for time_interval in time_intervals:\n                    for month in months:\n                        if (month, day_type, time_interval) in already_checked:\n                            continue\n\n                        partition_filter = (\n                            (F.col(ColNames.year) == F.lit(month.year))\n                            &amp; (F.col(ColNames.month) == F.lit(month.month))\n                            &amp; (F.col(ColNames.day_type) == F.lit(day_type))\n                            &amp; (F.col(ColNames.time_interval) == F.lit(time_interval))\n                        )\n\n                        data_exists = midterm_df.where(partition_filter).limit(1).count() &gt; 0\n                        if not data_exists:\n                            raise FileNotFoundError(\n                                \"No Mid-term Permanence Score data has been found for month \"\n                                f\"{month.strftime('%Y-%m')}, day_type `{day_type}` and time_interval `{time_interval}`\"\n                            )\n                        already_checked.add((month, day_type, time_interval))\n\n                    longterm_analyses.append(\n                        {\n                            \"season\": season,\n                            \"months\": months,\n                            \"day_type\": day_type,\n                            \"time_interval\": time_interval,\n                        }\n                    )\n\n        return longterm_analyses\n\n    def filter_longterm_analysis_data(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Applies the push-up filters on the Mid-term Permanence Score data to only consider the data from the\n        months, day_type, and time_interval required for the current long-term analysis combination.\n\n        Args:\n            df (DataFrame): Mid-term Permanence Score DataFrame before filtering\n\n        Returns:\n            DataFrame: Mid-term Permanence Score DataFrame after filtering\n        \"\"\"\n        month_filters = [\n            (F.col(ColNames.year) == F.lit(month.year)) &amp; (F.col(ColNames.month) == F.lit(month.month))\n            for month in self.current_lt_analysis[\"months\"]\n        ]\n\n        def logical_or(x, y):\n            return x | y\n\n        month_filter = reduce(logical_or, month_filters)\n\n        day_type_filter = F.col(ColNames.day_type) == F.lit(self.current_lt_analysis[\"day_type\"])\n\n        time_interval_filter = F.col(ColNames.time_interval) == F.lit(self.current_lt_analysis[\"time_interval\"])\n\n        filtered_df = df.filter(month_filter &amp; day_type_filter &amp; time_interval_filter)\n\n        return filtered_df\n\n    def compute_longterm_metrics(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Compute the Long-term Permanence Score and metrics from the filtered Mid-term Permanence Score data\n\n        Args:\n            df (DataFrame): Mid-term Permanence Score DataFrame\n\n        Returns:\n            DataFrame: dataframe with the long-term permanence score aend metrics\n        \"\"\"\n        # Split dataframes by id_type\n        grid_df = df.filter(F.col(ColNames.id_type) == F.lit(\"grid\"))\n        unknown_df = df.filter(F.col(ColNames.id_type) == F.lit(\"unknown\"))\n        obs_df = df.filter(F.col(ColNames.id_type) == F.lit(\"device_observation\"))\n\n        grid_df = grid_df.groupby(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id).agg(\n            F.sum(ColNames.mps).cast(IntegerType()).alias(ColNames.lps),\n            F.sum(ColNames.frequency).cast(IntegerType()).alias(ColNames.total_frequency),\n            F.mean(ColNames.frequency).cast(FloatType()).alias(ColNames.frequency_mean),\n            F.stddev_samp(ColNames.frequency).cast(FloatType()).alias(ColNames.frequency_std),\n            F.mean(ColNames.regularity_mean).cast(FloatType()).alias(ColNames.regularity_mean),\n            F.stddev_samp(ColNames.regularity_mean).cast(FloatType()).alias(ColNames.regularity_std),\n        )\n\n        unknown_df = unknown_df.groupby(ColNames.user_id_modulo, ColNames.user_id).agg(\n            F.sum(ColNames.mps).cast(IntegerType()).alias(ColNames.lps),\n            F.sum(ColNames.frequency).cast(IntegerType()).alias(ColNames.total_frequency),\n            F.mean(ColNames.frequency).cast(FloatType()).alias(ColNames.frequency_mean),\n            F.stddev_samp(ColNames.frequency).cast(FloatType()).alias(ColNames.frequency_std),\n            F.mean(ColNames.regularity_mean).cast(FloatType()).alias(ColNames.regularity_mean),\n            F.stddev_samp(ColNames.regularity_mean).cast(FloatType()).alias(ColNames.regularity_std),\n        )\n\n        obs_df = obs_df.groupby(ColNames.user_id_modulo, ColNames.user_id).agg(\n            F.sum(ColNames.mps).cast(IntegerType()).alias(ColNames.lps),\n            F.sum(ColNames.frequency).cast(IntegerType()).alias(ColNames.total_frequency),\n        )\n\n        grid_df = grid_df.select(\n            ColNames.user_id_modulo,\n            ColNames.user_id,\n            ColNames.grid_id,\n            ColNames.lps,\n            ColNames.total_frequency,\n            ColNames.frequency_mean,\n            ColNames.frequency_std,\n            ColNames.regularity_mean,\n            ColNames.regularity_std,\n            F.lit(\"grid\").alias(ColNames.id_type),\n        )\n\n        unknown_df = unknown_df.select(\n            ColNames.user_id_modulo,\n            ColNames.user_id,\n            F.lit(-99).alias(ColNames.grid_id),\n            ColNames.lps,\n            ColNames.total_frequency,\n            ColNames.frequency_mean,\n            ColNames.frequency_std,\n            ColNames.regularity_mean,\n            ColNames.regularity_std,\n            F.lit(\"unknown\").alias(ColNames.id_type),\n        )\n\n        obs_df = obs_df.select(\n            ColNames.user_id_modulo,\n            ColNames.user_id,\n            F.lit(UeGridIdType.DEVICE_OBSERVATION).alias(ColNames.grid_id),\n            ColNames.lps,\n            ColNames.total_frequency,\n            F.lit(None).cast(FloatType()).alias(ColNames.frequency_mean),\n            F.lit(None).cast(FloatType()).alias(ColNames.frequency_std),\n            F.lit(None).cast(FloatType()).alias(ColNames.regularity_mean),\n            F.lit(None).cast(FloatType()).alias(ColNames.regularity_std),\n            F.lit(\"device_observation\").alias(ColNames.id_type),\n        )\n\n        return grid_df.union(unknown_df).union(obs_df)\n\n    def transform(self):\n        midterm_df = self.input_data_objects[SilverMidtermPermanenceScoreDataObject.ID].df\n\n        df = self.filter_longterm_analysis_data(midterm_df)\n\n        # Filter partition chunk\n        if self.partition_chunk is not None:\n            df = df.filter(F.col(ColNames.user_id_modulo).isin(self.partition_chunk))\n\n        longterm_df = self.compute_longterm_metrics(df)\n\n        start_date = min(self.current_lt_analysis[\"months\"])\n        end_date = max(self.current_lt_analysis[\"months\"])\n        end_date = end_date.replace(day=cal.monthrange(end_date.year, end_date.month)[1])\n\n        longterm_df = longterm_df.withColumns(\n            {\n                ColNames.start_date: F.lit(start_date),\n                ColNames.end_date: F.lit(end_date),\n                ColNames.season: F.lit(self.current_lt_analysis[\"season\"]),\n                ColNames.day_type: F.lit(self.current_lt_analysis[\"day_type\"]),\n                ColNames.time_interval: F.lit(self.current_lt_analysis[\"time_interval\"]),\n            }\n        )\n\n        longterm_df = longterm_df.repartition(*SilverLongtermPermanenceScoreDataObject.PARTITION_COLUMNS)\n\n        self.output_data_objects[SilverLongtermPermanenceScoreDataObject.ID].df = longterm_df\n\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(\"Reading data objects...\")\n        self.read()\n        self.logger.info(\"... data objects read!\")\n        self.logger.info(\"Checking that all required mid-term permanence score data exists...\")\n        self.longterm_analyses = self._check_midterm_data_exist()\n        self.logger.info(\"... check successful!\")\n        self.logger.info(\"Starting long-term analyses...\")\n\n        partition_chunks = self._get_partition_chunks()\n        for i, partition_chunk in enumerate(partition_chunks):\n            self.partition_chunk = partition_chunk\n            self.logger.info(f\"Processing partition chunk {i}\")\n            self.logger.debug(f\"Partition chunk {partition_chunk}\")\n            for lt_analysis in self.longterm_analyses:\n                self.current_lt_analysis = lt_analysis\n                self.logger.info(\n                    f\"Starting analysis for season `{self.current_lt_analysis['season']}`, \"\n                    f\"{min(self.current_lt_analysis['months']).strftime('%Y-%m')} to \"\n                    f\"{max(self.current_lt_analysis['months']).strftime('%Y-%m')}, \"\n                    f\"day_type `{self.current_lt_analysis['day_type']}` and \"\n                    f\"time_interval `{self.current_lt_analysis['time_interval']}`...\"\n                )\n                self.transform()\n                self.write()\n                self.logger.info(\"... results saved\")\n\n        self.logger.info(\"... all analyses finished!\")\n\n    # TODO: Refactor iterative processing as an Interface\n    def _get_partition_chunks(self) -&gt; List[List[int]]:\n        \"\"\"\n        Method that returns the partition chunks for the current date.\n\n        Returns:\n            List[List[int, int]]: list of partition chunks. If the partition_chunk_size is not defined in the config or\n                the number of partitions is less than the desired chunk size, it will return a list with a single None element.\n        \"\"\"\n        # Get partitions desired\n        partition_chunk_size = self.config.getint(self.COMPONENT_ID, \"partition_chunk_size\", fallback=None)\n        number_of_partitions = self.config.getint(self.COMPONENT_ID, \"number_of_partitions\", fallback=None)\n\n        if partition_chunk_size is None or number_of_partitions is None or partition_chunk_size &lt;= 0:\n            return [None]\n\n        if number_of_partitions &lt;= partition_chunk_size:\n            self.logger.warning(\n                f\"Available Partition number ({number_of_partitions}) is \"\n                f\"less than the desired chunk size ({partition_chunk_size}). \"\n                f\"Using all partitions.\"\n            )\n            return [None]\n        partition_chunks = [\n            list(range(i, min(i + partition_chunk_size, number_of_partitions)))\n            for i in range(0, number_of_partitions, partition_chunk_size)\n        ]\n        # NOTE: Generate chunks if partition_values were read for each day\n        # getting exactly the amount of partitions for that day\n\n        # partition_chunks = [\n        #     partition_values[i : i + partition_chunk_size]\n        #     for i in range(0, partition_values_size, partition_chunk_size)\n        # ]\n\n        return partition_chunks\n</code></pre>"},{"location":"reference/components/execution/longterm_permanence_score/longterm_permanence_score/#components.execution.longterm_permanence_score.longterm_permanence_score.LongtermPermanenceScore._check_midterm_data_exist","title":"<code>_check_midterm_data_exist()</code>","text":"<p>Checks that the mid-term permanence score data necessary for carrying out all long-term analyses exist, based on the day_types and time_intervals requested for each analysis. Returns a list of dictionaries containing the information necessary to filter the required mid-term analysis data of each analysis.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>Whenever a certain (day_type, time_interval) combination has not been found in the data of a particular month, required to compute some long-term analysis.</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: information of each long-term analysis to be performed</p> Source code in <code>multimno/components/execution/longterm_permanence_score/longterm_permanence_score.py</code> <pre><code>def _check_midterm_data_exist(self) -&gt; List[dict]:\n    \"\"\"Checks that the mid-term permanence score data necessary for carrying out all long-term analyses exist,\n    based on the day_types and time_intervals requested for each analysis. Returns a list of dictionaries\n    containing the information necessary to filter the required mid-term analysis data of each analysis.\n\n    Raises:\n        FileNotFoundError: Whenever a certain (day_type, time_interval) combination has not been found in the\n            data of a particular month, required to compute some long-term analysis.\n\n    Returns:\n        List[dict]: information of each long-term analysis to be performed\n    \"\"\"\n    midterm_df = self.input_data_objects[SilverMidtermPermanenceScoreDataObject.ID].df\n    longterm_analyses = []\n    already_checked = set()\n    # For each season\n    for season, months in self.season_months.items():\n        # Check that all months of that season have the mid-term PS data of the required day type and time interval\n        for day_type, time_intervals in self.period_combinations[season].items():\n            for time_interval in time_intervals:\n                for month in months:\n                    if (month, day_type, time_interval) in already_checked:\n                        continue\n\n                    partition_filter = (\n                        (F.col(ColNames.year) == F.lit(month.year))\n                        &amp; (F.col(ColNames.month) == F.lit(month.month))\n                        &amp; (F.col(ColNames.day_type) == F.lit(day_type))\n                        &amp; (F.col(ColNames.time_interval) == F.lit(time_interval))\n                    )\n\n                    data_exists = midterm_df.where(partition_filter).limit(1).count() &gt; 0\n                    if not data_exists:\n                        raise FileNotFoundError(\n                            \"No Mid-term Permanence Score data has been found for month \"\n                            f\"{month.strftime('%Y-%m')}, day_type `{day_type}` and time_interval `{time_interval}`\"\n                        )\n                    already_checked.add((month, day_type, time_interval))\n\n                longterm_analyses.append(\n                    {\n                        \"season\": season,\n                        \"months\": months,\n                        \"day_type\": day_type,\n                        \"time_interval\": time_interval,\n                    }\n                )\n\n    return longterm_analyses\n</code></pre>"},{"location":"reference/components/execution/longterm_permanence_score/longterm_permanence_score/#components.execution.longterm_permanence_score.longterm_permanence_score.LongtermPermanenceScore._get_month_list","title":"<code>_get_month_list(months_input, context)</code>","text":"<p>Read and parse a comma-separated list of months that will be assigned to a particular season. Months are represented by an integer from 1 to 12 and must not be repeated within the list</p> <p>Parameters:</p> Name Type Description Default <code>months_input</code> <code>str</code> <p>comma-separated list of months to be parsed</p> required <code>context</code> <code>str</code> <p>name of the season, used for error tracking</p> required <p>Raises:</p> Type Description <code>e</code> <p>could not parse month to integer</p> <code>ValueError</code> <p>integer is not one between 1 and 12, inclusive</p> <code>ValueError</code> <p>repeated integers</p> <p>Returns:</p> Type Description <code>List[int]</code> <p>List[int]: list of integers representing the months of the year that will constitute a season</p> Source code in <code>multimno/components/execution/longterm_permanence_score/longterm_permanence_score.py</code> <pre><code>def _get_month_list(self, months_input: str, context: str) -&gt; List[int]:\n    \"\"\"Read and parse a comma-separated list of months that will be assigned to a particular season. Months are\n    represented by an integer from 1 to 12 and must not be repeated within the list\n\n\n    Args:\n        months_input (str): comma-separated list of months to be parsed\n        context (str): name of the season, used for error tracking\n\n    Raises:\n        e: could not parse month to integer\n        ValueError: integer is not one between 1 and 12, inclusive\n        ValueError: repeated integers\n\n    Returns:\n        List[int]: list of integers representing the months of the year that will constitute a season\n    \"\"\"\n    months_input = months_input.replace(\" \", \"\").replace(\"\\t\", \"\")\n    if months_input == \"\":\n        return []\n\n    months_input = months_input.split(\",\")\n    months = []\n\n    for mm in months_input:\n        try:\n            mm = int(mm)\n        except ValueError as e:\n            self.logger.error(f\"expected integer as a month for {context} season, but found `{mm}`\")\n            raise e\n        if mm &lt; 1 or mm &gt; 12:\n            raise ValueError(\n                f\"expected integer between 1 and 12 to represent a month for {context} season, \" f\"but found `{mm}`\"\n            )\n        months.append(mm)\n\n    if len(months) != len(set(months)):\n        raise ValueError(f\"found repeated month {months} in season {context} -- please remove any duplicates\")\n\n    return months\n</code></pre>"},{"location":"reference/components/execution/longterm_permanence_score/longterm_permanence_score/#components.execution.longterm_permanence_score.longterm_permanence_score.LongtermPermanenceScore._get_partition_chunks","title":"<code>_get_partition_chunks()</code>","text":"<p>Method that returns the partition chunks for the current date.</p> <p>Returns:</p> Type Description <code>List[List[int]]</code> <p>List[List[int, int]]: list of partition chunks. If the partition_chunk_size is not defined in the config or the number of partitions is less than the desired chunk size, it will return a list with a single None element.</p> Source code in <code>multimno/components/execution/longterm_permanence_score/longterm_permanence_score.py</code> <pre><code>def _get_partition_chunks(self) -&gt; List[List[int]]:\n    \"\"\"\n    Method that returns the partition chunks for the current date.\n\n    Returns:\n        List[List[int, int]]: list of partition chunks. If the partition_chunk_size is not defined in the config or\n            the number of partitions is less than the desired chunk size, it will return a list with a single None element.\n    \"\"\"\n    # Get partitions desired\n    partition_chunk_size = self.config.getint(self.COMPONENT_ID, \"partition_chunk_size\", fallback=None)\n    number_of_partitions = self.config.getint(self.COMPONENT_ID, \"number_of_partitions\", fallback=None)\n\n    if partition_chunk_size is None or number_of_partitions is None or partition_chunk_size &lt;= 0:\n        return [None]\n\n    if number_of_partitions &lt;= partition_chunk_size:\n        self.logger.warning(\n            f\"Available Partition number ({number_of_partitions}) is \"\n            f\"less than the desired chunk size ({partition_chunk_size}). \"\n            f\"Using all partitions.\"\n        )\n        return [None]\n    partition_chunks = [\n        list(range(i, min(i + partition_chunk_size, number_of_partitions)))\n        for i in range(0, number_of_partitions, partition_chunk_size)\n    ]\n    # NOTE: Generate chunks if partition_values were read for each day\n    # getting exactly the amount of partitions for that day\n\n    # partition_chunks = [\n    #     partition_values[i : i + partition_chunk_size]\n    #     for i in range(0, partition_values_size, partition_chunk_size)\n    # ]\n\n    return partition_chunks\n</code></pre>"},{"location":"reference/components/execution/longterm_permanence_score/longterm_permanence_score/#components.execution.longterm_permanence_score.longterm_permanence_score.LongtermPermanenceScore.compute_longterm_metrics","title":"<code>compute_longterm_metrics(df)</code>","text":"<p>Compute the Long-term Permanence Score and metrics from the filtered Mid-term Permanence Score data</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Mid-term Permanence Score DataFrame</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>dataframe with the long-term permanence score aend metrics</p> Source code in <code>multimno/components/execution/longterm_permanence_score/longterm_permanence_score.py</code> <pre><code>def compute_longterm_metrics(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Compute the Long-term Permanence Score and metrics from the filtered Mid-term Permanence Score data\n\n    Args:\n        df (DataFrame): Mid-term Permanence Score DataFrame\n\n    Returns:\n        DataFrame: dataframe with the long-term permanence score aend metrics\n    \"\"\"\n    # Split dataframes by id_type\n    grid_df = df.filter(F.col(ColNames.id_type) == F.lit(\"grid\"))\n    unknown_df = df.filter(F.col(ColNames.id_type) == F.lit(\"unknown\"))\n    obs_df = df.filter(F.col(ColNames.id_type) == F.lit(\"device_observation\"))\n\n    grid_df = grid_df.groupby(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id).agg(\n        F.sum(ColNames.mps).cast(IntegerType()).alias(ColNames.lps),\n        F.sum(ColNames.frequency).cast(IntegerType()).alias(ColNames.total_frequency),\n        F.mean(ColNames.frequency).cast(FloatType()).alias(ColNames.frequency_mean),\n        F.stddev_samp(ColNames.frequency).cast(FloatType()).alias(ColNames.frequency_std),\n        F.mean(ColNames.regularity_mean).cast(FloatType()).alias(ColNames.regularity_mean),\n        F.stddev_samp(ColNames.regularity_mean).cast(FloatType()).alias(ColNames.regularity_std),\n    )\n\n    unknown_df = unknown_df.groupby(ColNames.user_id_modulo, ColNames.user_id).agg(\n        F.sum(ColNames.mps).cast(IntegerType()).alias(ColNames.lps),\n        F.sum(ColNames.frequency).cast(IntegerType()).alias(ColNames.total_frequency),\n        F.mean(ColNames.frequency).cast(FloatType()).alias(ColNames.frequency_mean),\n        F.stddev_samp(ColNames.frequency).cast(FloatType()).alias(ColNames.frequency_std),\n        F.mean(ColNames.regularity_mean).cast(FloatType()).alias(ColNames.regularity_mean),\n        F.stddev_samp(ColNames.regularity_mean).cast(FloatType()).alias(ColNames.regularity_std),\n    )\n\n    obs_df = obs_df.groupby(ColNames.user_id_modulo, ColNames.user_id).agg(\n        F.sum(ColNames.mps).cast(IntegerType()).alias(ColNames.lps),\n        F.sum(ColNames.frequency).cast(IntegerType()).alias(ColNames.total_frequency),\n    )\n\n    grid_df = grid_df.select(\n        ColNames.user_id_modulo,\n        ColNames.user_id,\n        ColNames.grid_id,\n        ColNames.lps,\n        ColNames.total_frequency,\n        ColNames.frequency_mean,\n        ColNames.frequency_std,\n        ColNames.regularity_mean,\n        ColNames.regularity_std,\n        F.lit(\"grid\").alias(ColNames.id_type),\n    )\n\n    unknown_df = unknown_df.select(\n        ColNames.user_id_modulo,\n        ColNames.user_id,\n        F.lit(-99).alias(ColNames.grid_id),\n        ColNames.lps,\n        ColNames.total_frequency,\n        ColNames.frequency_mean,\n        ColNames.frequency_std,\n        ColNames.regularity_mean,\n        ColNames.regularity_std,\n        F.lit(\"unknown\").alias(ColNames.id_type),\n    )\n\n    obs_df = obs_df.select(\n        ColNames.user_id_modulo,\n        ColNames.user_id,\n        F.lit(UeGridIdType.DEVICE_OBSERVATION).alias(ColNames.grid_id),\n        ColNames.lps,\n        ColNames.total_frequency,\n        F.lit(None).cast(FloatType()).alias(ColNames.frequency_mean),\n        F.lit(None).cast(FloatType()).alias(ColNames.frequency_std),\n        F.lit(None).cast(FloatType()).alias(ColNames.regularity_mean),\n        F.lit(None).cast(FloatType()).alias(ColNames.regularity_std),\n        F.lit(\"device_observation\").alias(ColNames.id_type),\n    )\n\n    return grid_df.union(unknown_df).union(obs_df)\n</code></pre>"},{"location":"reference/components/execution/longterm_permanence_score/longterm_permanence_score/#components.execution.longterm_permanence_score.longterm_permanence_score.LongtermPermanenceScore.filter_longterm_analysis_data","title":"<code>filter_longterm_analysis_data(df)</code>","text":"<p>Applies the push-up filters on the Mid-term Permanence Score data to only consider the data from the months, day_type, and time_interval required for the current long-term analysis combination.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Mid-term Permanence Score DataFrame before filtering</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Mid-term Permanence Score DataFrame after filtering</p> Source code in <code>multimno/components/execution/longterm_permanence_score/longterm_permanence_score.py</code> <pre><code>def filter_longterm_analysis_data(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Applies the push-up filters on the Mid-term Permanence Score data to only consider the data from the\n    months, day_type, and time_interval required for the current long-term analysis combination.\n\n    Args:\n        df (DataFrame): Mid-term Permanence Score DataFrame before filtering\n\n    Returns:\n        DataFrame: Mid-term Permanence Score DataFrame after filtering\n    \"\"\"\n    month_filters = [\n        (F.col(ColNames.year) == F.lit(month.year)) &amp; (F.col(ColNames.month) == F.lit(month.month))\n        for month in self.current_lt_analysis[\"months\"]\n    ]\n\n    def logical_or(x, y):\n        return x | y\n\n    month_filter = reduce(logical_or, month_filters)\n\n    day_type_filter = F.col(ColNames.day_type) == F.lit(self.current_lt_analysis[\"day_type\"])\n\n    time_interval_filter = F.col(ColNames.time_interval) == F.lit(self.current_lt_analysis[\"time_interval\"])\n\n    filtered_df = df.filter(month_filter &amp; day_type_filter &amp; time_interval_filter)\n\n    return filtered_df\n</code></pre>"},{"location":"reference/components/execution/midterm_permanence_score/","title":"midterm_permanence_score","text":""},{"location":"reference/components/execution/midterm_permanence_score/midterm_permanence_score/","title":"midterm_permanence_score","text":"<p>Module that computes the Mid-term Permanence Score.</p>"},{"location":"reference/components/execution/midterm_permanence_score/midterm_permanence_score/#components.execution.midterm_permanence_score.midterm_permanence_score.MidtermPermanenceScore","title":"<code>MidtermPermanenceScore</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class that computes the mid term permanence score and related metrics, for different combinations of day types and time intervals in the day</p> Source code in <code>multimno/components/execution/midterm_permanence_score/midterm_permanence_score.py</code> <pre><code>class MidtermPermanenceScore(Component):\n    \"\"\"\n    Class that computes the mid term permanence score and related metrics, for different\n    combinations of day types and time intervals in the day\n    \"\"\"\n\n    COMPONENT_ID = \"MidtermPermanenceScore\"\n\n    night_time_start, night_time_end = None, None\n    working_hours_start, working_hours_end = None, None\n    evening_time_start, evening_time_end = None, None\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        # Months to process as each mid-term period\n        start_month = self.config.get(self.COMPONENT_ID, \"start_month\")\n        try:\n            self.start_date = dt.datetime.strptime(start_month, \"%Y-%m\").date()\n        except ValueError as e:\n            self.logger.error(f\"Could not parse start_month = `{start_month}`. Expected format: YYYY-MM\")\n            raise e\n\n        end_month = self.config.get(self.COMPONENT_ID, \"end_month\")\n        try:\n            self.end_date = dt.datetime.strptime(end_month, \"%Y-%m\").date()\n        except ValueError as e:\n            self.logger.error(f\"Could not parse end_month = `{end_month}`. Expected format: YYYY-MM\")\n            raise e\n        self.end_date = self.end_date.replace(day=cal.monthrange(self.end_date.year, self.end_date.month)[1])\n\n        if end_month &lt; start_month:\n            raise ValueError(f\"End month `{end_month}` should not be an earlier than start month `{start_month}`\")\n\n        # Additional days before and after each month to use for calculating regularity metrics\n        self.before_reg_days = self.config.getint(self.COMPONENT_ID, \"before_regularity_days\")\n        if self.before_reg_days &lt; 0:\n            raise ValueError(f\"`before_reg_days` must be a non-negative integer, found {self.before_reg_days}\")\n\n        self.after_reg_days = self.config.getint(self.COMPONENT_ID, \"after_regularity_days\")\n        if self.after_reg_days &lt; 0:\n            raise ValueError(f\"`after_reg_days` must be a non-negative integer, found {self.after_reg_days}\")\n\n        # list of dictionaries with each mid-term to be analysed\n        self.midterm_periods = self._get_midterm_periods()\n        # Hour used to define the start of a day, e.g. 4 means that a Monday starts at 4AM Monday and ends at\n        # 4AM Tuesday\n        self.day_start_hour = self.config.getint(self.COMPONENT_ID, \"day_start_hour\")\n        if self.day_start_hour &lt; 0 or self.day_start_hour &gt;= 24:\n            raise ValueError(f\"`day_start_hour` must be between 0 and 23 inclusive, found {self.day_start_hour} \")\n\n        # Read the definition of each sub-daily period (or time interval) to be studied.\n        # Set to keep track of the minutes in each the intervals start/end, which must be compared with\n        # daily permanence score to verify compatibility\n        self.midterm_minutes = set()\n        for time_interval in TIME_INTERVALS:  # night, work, evening, all\n            if time_interval == \"all\":\n                continue\n            interval_start = self.config.get(self.COMPONENT_ID, f\"{time_interval}_start\")\n            interval_start = self._check_time_interval(interval_start, name=f\"{time_interval}_start\")\n            setattr(self, f\"{time_interval}_start\", interval_start)\n\n            interval_end = self.config.get(self.COMPONENT_ID, f\"{time_interval}_end\")\n            interval_end = self._check_time_interval(interval_end, name=f\"{time_interval}_end\")\n            setattr(self, f\"{time_interval}_end\", interval_end)\n\n            if interval_start == interval_end:\n                raise ValueError(\n                    f\"{time_interval}_start and {time_interval}_end are equal, when they must be strictly \"\n                    \"different -- please provide a valid time interval\"\n                )\n\n            # Non-allowed time interval limits. Example:\n            # self.day_start_hour = 4 (4AM)\n            # interval_start = 03:30, interval_end = 01:00\n            # The time interval starts at 03:30 of day D-1 and ends at 01:00 of day D, but would belong to day D-1\n            if (\n                interval_start.hour &lt; self.day_start_hour\n                and interval_end != dt.time(0, 0)\n                and (interval_end &lt; interval_start)\n            ):\n                raise ValueError(\n                    \"Invalid configuration: the following order of of parameters is not allowed:\\n\"\n                    f\"\\t {time_interval}_end ({interval_end}) &lt; {time_interval}_start ({interval_start}) &lt; \"\n                    f\"day_start_hour ({self.day_start_hour})\"\n                )\n\n            # Additional prohibited time interval (except for nights): time interval must not cross the self.dat_start_hour\n            if time_interval != \"night_time\":\n                if interval_start.hour &lt; self.day_start_hour and (\n                    interval_end.hour &gt; self.day_start_hour\n                    or (interval_end.hour == self.day_start_hour and interval_end.minute != 0)\n                ):\n                    raise ValueError(\n                        \"Invalid configuration: the following order of parameters is not allowed:\\n\"\n                        f\"{time_interval}_start ({interval_start}) &lt; day_start_hour ({self.day_start_hour}) &lt; {time_interval}_end ({interval_end})\"\n                    )\n\n            self.midterm_minutes.add(interval_start.minute)\n            self.midterm_minutes.add(interval_end.minute)\n\n        # Day of the week marking the start of the weekend, (starting in self.day_start_hour)\n        weekend_start_str = self.config.get(self.COMPONENT_ID, \"weekend_start\")\n        self.weekend_start_day = self._check_weekday_number(weekend_start_str, context=weekend_start_str)\n\n        # Day of the week marking the end of the weekend, date included, (ending right before self.day_start_hour)\n        weekend_end_str = self.config.get(self.COMPONENT_ID, \"weekend_end\")\n        self.weekend_end_day = self._check_weekday_number(weekend_end_str, context=weekend_end_str)\n\n        # List of days of the week composing the weekend\n        self.weekend_days = []\n        dd = self.weekend_start_day\n        while dd != self.weekend_end_day:\n            self.weekend_days.append(dd)\n            dd = (dd) % 7 + 1\n        self.weekend_days.append(dd)\n\n        # Work days are those that are not part of the weekend (also excluding holidays later on)\n        self.work_days = sorted(list({1, 2, 3, 4, 5, 6, 7}.difference(self.weekend_days)))\n\n        # Read from configuration the combination of sub-monthly and sub-daily pairs, i.e. day types and time intervals,\n        # to compute\n        period_combinations = self.config.geteval(self.COMPONENT_ID, \"period_combinations\")\n        self.period_combinations = {}\n        for key, vals in period_combinations.items():\n            if key.lower() not in DAY_TYPES:\n                raise ValueError(f\"Unknown day type `{key}` in period_combinations\")\n            self.period_combinations[key.lower()] = []\n            if len(vals) != len(set(vals)):\n                raise ValueError(\n                    f\"Repeated values for time interval in period_combinations under `{key}`:\",\n                    str(period_combinations[key]),\n                )\n            for val in vals:\n                if val not in TIME_INTERVALS:\n                    raise ValueError(f\"Unknown time interval `{val}` in period_combinations under `{key}`\")\n                self.period_combinations[key.lower()].append(val)\n\n        # Score interval, for DPS calculation, currently set = 2\n        self.score_interval = 2\n\n        # Country of study, used to load its holidays\n        self.country_of_study = self.config.get(self.COMPONENT_ID, \"country_of_study\")\n\n        # Initialise variable for working in each midterm_period\n        self.day_type = None\n        self.time_interval = None\n        self.current_mt_period = None\n        self.current_dps_data = None\n        self.current_dps_data_chunk = None\n        self.footprint = None\n\n    def _get_midterm_periods(self) -&gt; List[dict]:\n        \"\"\"Computes the date limits of each mid-term period, together with the limits of the regularity metrics' extra\n        dates\n\n        Returns:\n            List[dict]: list of dictionaries with the information of dates of each mid-term period\n        \"\"\"\n        midterm_periods = []\n        start_of_the_month = self.start_date\n\n        while True:\n            end_of_the_month = start_of_the_month.replace(\n                day=cal.monthrange(start_of_the_month.year, start_of_the_month.month)[1]\n            )\n            before_reg_date = start_of_the_month - dt.timedelta(days=self.before_reg_days)\n            after_reg_date = end_of_the_month + dt.timedelta(days=self.after_reg_days)\n\n            midterm_periods.append(\n                {\n                    \"month_start\": start_of_the_month,\n                    \"month_end\": end_of_the_month,\n                    \"extended_month_start\": before_reg_date,\n                    \"extended_month_end\": after_reg_date,\n                }\n            )\n\n            if end_of_the_month == self.end_date:\n                return midterm_periods\n\n            start_of_the_month = end_of_the_month + dt.timedelta(days=1)\n\n    def _check_weekday_number(self, num: str, context: str) -&gt; int:\n        \"\"\"Parses and validates a day of the week\n\n        Args:\n            num (str): string to be parsed to integer between 1 and 7\n            context (str): string for error tracking\n\n        Raises:\n            e: Error in parsing num to int\n            ValueError: num is not a valid day of the week (between 1 and 7 inclusive)\n\n        Returns:\n            int: integer representing a day of the week\n        \"\"\"\n        try:\n            num = int(num)\n        except Exception as e:\n            self.logger.error(f\"Must specify a day as an integer between 1 and 7, but found `{num}` in `{context}`\")\n            raise e\n        if num &lt; 1 or num &gt; 7:\n            raise ValueError(\n                f\"Days must take a value between 1 for Monday and 7 for Sunday, found {num} in `{context}`\"\n            )\n        return num\n\n    def _check_time_interval(self, interval: str, name: str) -&gt; dt.time:\n        \"\"\"Tries to parse time interval's start/end time from configuration file and check if it has\n        valid minutes (00, 15, 30, or 45). If so, returns the corresponding dt.time object\n\n        Args:\n            interval (str): interval string to be parsed to dt.datetime\n            name (str): name of the interval being parsed, used for error tracking\n\n        Raises:\n            e: Formatting error, cannot parse time as HH:MM (24h format)\n            ValueError: interval ends in non-allowed minutes\n\n        Returns:\n            dt.time: time of the start or end of the time interval\n        \"\"\"\n        try:\n            interval = dt.datetime.strptime(interval, \"%H:%M\")\n        except ValueError as e:\n            self.logger.error(f\"Could not parse {name}, expected HH:MM format, found {interval}\")\n            raise e\n\n        interval = interval.time()\n        if interval.minute not in [0, 15, 30, 45]:\n            raise ValueError(f\"Time interval {name} must have :00, :15, :30, or :45 minutes, found :{interval.minute}\")\n        return interval\n\n    def initalize_data_objects(self):\n        # Initialize data objects\n        input_silver_daily_ps_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"daily_permanence_score_data_silver\")\n        input_bronze_holiday_calendar_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"holiday_calendar_data_bronze\")\n        input_silver_cell_footprint = self.config.get(CONFIG_SILVER_PATHS_KEY, \"cell_footprint_data_silver\")\n        output_silver_midterm_ps_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"midterm_permanence_score_data_silver\")\n\n        # Clear destination directory if needed\n        clear_destination_directory = self.config.getboolean(\n            self.COMPONENT_ID, \"clear_destination_directory\", fallback=False\n        )\n        if clear_destination_directory:\n            self.logger.warning(f\"Deleting: {output_silver_midterm_ps_path}\")\n            delete_file_or_folder(self.spark, output_silver_midterm_ps_path)\n\n        daily_ps = SilverDailyPermanenceScoreDataObject(self.spark, input_silver_daily_ps_path)\n        holiday_calendar = BronzeHolidayCalendarDataObject(self.spark, input_bronze_holiday_calendar_path)\n        cell_footprint = SilverCellFootprintDataObject(self.spark, input_silver_cell_footprint)\n        midterm_ps = SilverMidtermPermanenceScoreDataObject(self.spark, output_silver_midterm_ps_path)\n\n        self.input_data_objects = {\n            holiday_calendar.ID: holiday_calendar,\n            daily_ps.ID: daily_ps,\n            cell_footprint.ID: cell_footprint,\n        }\n        self.output_data_objects = {midterm_ps.ID: midterm_ps}\n\n    def _validate_and_load_daily_permanence_score(self, mt_period: dict) -&gt; DataFrame:\n        \"\"\"Loads the Daily Permanence Score data to be used for the calculation of the Mid-term Permanence Score metrics\n        of a particular mid-term period. Filters out DPS values equal to zero and checks that the time slots are\n        compatible with the configuration-provided time intervals.\n\n        Raises:\n            ValueError: If DPS data has a time slot duration different from 15, 30, or 60 minutes.\n            ValueError: If DPS data has 60-min slots but 15- or 30-min lengths are required.\n            ValueError: If DPS data has 30-min slots but 15-min lengths are required.\n\n        Returns:\n            dps: DataFrame of all DPS data necessary to calcualte the mid-term permanence score &amp; metrics of\n                self.current_mt_period\n        \"\"\"\n        dps = self.input_data_objects[SilverDailyPermanenceScoreDataObject.ID].df\n\n        # Add a one-day buffer, as later on the definition of a day does not match the midnight definition\n        dps = dps.filter(\n            F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n            &gt;= F.lit(mt_period[\"extended_month_start\"] - dt.timedelta(days=1))\n        ).filter(\n            F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n            &lt;= F.lit(mt_period[\"extended_month_end\"]) + dt.timedelta(days=1)\n        )\n\n        # If all time_intervals match a whole hour, no check needs to be done here.\n        if self.midterm_minutes == {0}:\n            return dps\n\n        # If the previous set was not disjoint, then the user has entered some time_interval that does not match a\n        # whole hour. We check, for all loaded dates, what their time slot length was. We only need to check the length\n        # of one time slot per DPS date.\n        time_slot_duration = (\n            dps.groupby([ColNames.year, ColNames.month, ColNames.day])\n            .agg(\n                F.first(F.col(ColNames.time_slot_end_time) - F.col(ColNames.time_slot_initial_time)).alias(\n                    \"time_slot_duration\"\n                )\n            )\n            .collect()\n        )\n        daily_duration = {\n            dt.date(row[\"year\"], row[\"month\"], row[\"day\"]): row[\"time_slot_duration\"].seconds // 60\n            for row in time_slot_duration\n        }\n\n        # If disjoint, user has specified intervals ending in :00 or :30. Then we admit 15- or 30-min time slot\n        # durations, but not whole hours.\n        # If not disjoint, user has specified some :15 or :45 intervals, and we can only admit 15-min time slots\n        check_for_half_hour_only = {15, 45}.isdisjoint(self.midterm_minutes)\n\n        for date, duration in daily_duration.items():\n            if duration not in {15, 30, 60}:\n                raise ValueError(\n                    f\"Found time_slot duration of {duration} min in DailyPermanenceScore of {date}, when \"\n                    \"accepted values are 15, 30, or 60 minutes.\"\n                )\n            if duration == 60:\n                if check_for_half_hour_only:\n                    msg = (\n                        f\"Found time_slot duration of 60 min in DailyPermanenceScore of {date}, when only accepted \"\n                        \"values are 15 and 30\"\n                    )\n                else:\n                    msg = (\n                        f\"Found time_slot duration of 60 min in DailyPermanenceScore of {date}, when only accepted \"\n                        \"value is 15\"\n                    )\n                raise ValueError(msg)\n            if duration == 30 and not check_for_half_hour_only:\n                raise ValueError(\n                    f\"Found time_slot duration of 30 min in DailyPermanenceScore of {date}, when only accepted \"\n                    \"value is 15\"\n                )\n        return dps\n\n    def filter_dps_by_time_interval(\n        self, df: DataFrame, subdaily_period: str, start: dt.time, end: dt.time\n    ) -&gt; DataFrame:\n        \"\"\"Filter DPS matrix by keeping the time slots that belong to the subdaily period or time interval\n        specified. Also create a new column, `date`, which contains to which day the time slot belongs to, as it\n        does not necessarily match the day the time slot belongs to (i.e. the `midnight` definition)\n\n        Args:\n            df (DataFrame): DPS dataframe to be filtered.\n            subdaily_period (str): name of the time interval or subdaily period. Must be one of the values in\n                MidtermPermanenceScore.TIME_INTERVALS.\n            start (dt.time): earliest time of accepted time slots that will not be filtered out\n            end (dt.time): latest time of accepted time slots that will not be filtered out\n\n        Raises:\n            ValueError: Whenever an unknown subdaily period is specified\n\n        Returns:\n            DataFrame: filtered DPS dataframe\n        \"\"\"\n        if subdaily_period not in TIME_INTERVALS:\n            raise ValueError(f\"Unknown subdaily/time_interval {subdaily_period}\")\n\n        # Auxiliary variables\n        start_hour = F.lit(start.hour)\n        start_min = F.lit(start.minute)\n        end_hour = end.hour\n        if end_hour == 0:\n            end_hour = F.lit(24)\n        else:\n            end_hour = F.lit(end_hour)\n        end_min = F.lit(end.minute)\n\n        slot_initial_hour = F.hour(ColNames.time_slot_initial_time)\n        slot_initial_min = F.minute(ColNames.time_slot_initial_time)\n        slot_end_hour = F.hour(ColNames.time_slot_end_time)\n        slot_end_hour = F.when(slot_end_hour == F.lit(0), F.lit(24)).otherwise(slot_end_hour)\n        slot_end_min = F.minute(ColNames.time_slot_end_time)\n\n        # Global time interval, taking all time slots\n        if subdaily_period == \"all\":\n            if start != end:\n                raise ValueError(\n                    \"`all` time interval must have matching start and end times to not overlap with \"\n                    f\"different dates, found start={start} and end={end}\"\n                )\n            # If the day starts and ends at midnight, no additional logic needs to be done for assigning the correct\n            # date\n            if start == dt.time(0, 0):\n                df = df.withColumn(ColNames.date, F.col(ColNames.time_slot_initial_time).cast(DateType()))\n                return df\n            # If not: the hour that defines the day always belongs to that day.\n            # e.g., if subdaily_start=4, then day D is defined as 4:00AM of day D to 4:00 AM of day D+1\n            df = df.withColumn(\n                ColNames.date,\n                F.when(\n                    F.lit(60) * (slot_initial_hour - start_hour) &gt;= (start_min - slot_initial_min),\n                    F.col(ColNames.time_slot_initial_time),\n                )\n                .otherwise(F.date_add(F.col(ColNames.time_slot_initial_time), -1))\n                .cast(DateType()),\n            )\n            return df\n\n        # Rest of time intervals: night_time, evening_time, working_hours.\n        # Filter out time slots not contained in the time interval\n        if start &lt; end or end == dt.time(0, 0):\n            df = df.filter(\n                (F.lit(60) * (slot_initial_hour - start_hour) &gt;= (start_min - slot_initial_min))\n                &amp; (F.lit(60) * (slot_end_hour - end_hour) &lt;= (end_min - slot_end_min))\n            )\n        else:\n            df = df.filter(\n                (F.lit(60) * (slot_initial_hour - start_hour) &gt;= (start_min - slot_initial_min))\n                | (F.lit(60) * (slot_end_hour - end_hour) &lt;= (end_min - slot_end_min))\n            )\n\n        # consider self.day_start_hour = 4 for the following examples\n        if subdaily_period == \"night_time\":\n            if start.hour &gt;= self.day_start_hour and (end &gt; start or end == dt.time(0, 0)):\n                # night-interval stays in the day, example [20:15 to 23:30] or [19:45 to 00:00]\n                df = df.withColumn(ColNames.date, F.col(ColNames.time_slot_initial_time).cast(DateType()))\n            elif start.hour &gt;= self.day_start_hour and end &lt; start and end != dt.time(0, 0):\n                # night-interval starts in the day and crosses midnight, example [20:15 to 03:30] or [19:00 to 18:45]\n                df = df.withColumn(\n                    ColNames.date,\n                    F.when(\n                        F.lit(60) * (slot_initial_hour - start_hour) &gt;= (start_min - slot_initial_min),\n                        F.col(ColNames.time_slot_initial_time),\n                    )\n                    .otherwise(F.date_add(F.col(ColNames.time_slot_initial_time), -1))\n                    .cast(DateType()),\n                )\n            elif start.hour &lt; self.day_start_hour and (end &gt; start or end == dt.time(0, 0)):\n                # night-interval starts in the next day and does not cross midnight, example [3:15 to 6:00] or [2:30 to 10:45]\n                df = df.withColumn(\n                    ColNames.date, F.date_add(F.col(ColNames.time_slot_initial_time), -1).cast(DateType())\n                )\n            return df\n\n        # if subdaily_period in (\"working_hours\", \"evening_time\"):\n        if start &gt;= end and end != dt.time(0, 0):\n            self.logger.log(\n                msg=f\"'inverted' start ({start}) and end ({end}) times found for subdaily period {subdaily_period}\"\n                \" -- the whole period will belong to the day of start of the interval\",\n                level=logging.INFO,\n            )\n        df = df.withColumn(\n            ColNames.date,\n            F.when(\n                F.lit(60) * (slot_initial_hour - start_hour) &gt;= (start_min - slot_initial_min),\n                F.col(ColNames.time_slot_initial_time),\n            )\n            .otherwise(F.date_add(F.col(ColNames.time_slot_initial_time), -1))\n            .cast(DateType()),\n        )\n        return df\n\n    def filter_dps_by_day_type(self, df: DataFrame, submonthly_period: str) -&gt; DataFrame:\n        \"\"\"Filter DPS matrix, with an assigned date column, keeping the time slots that belong to the day type or\n        submonthly period specified.\n\n        Args:\n            df (DataFrame): DPS dataframe, with assigned `date` column\n            submonthly_period (str): submonthly period or day type. Must be one of the values in\n                MidtermPermanenceScore.DAY_TYPE\n\n        Raises:\n            ValueError: Whenever an unknown submonthly period is specified\n\n        Returns:\n            DataFrame: Filtered dataframe\n        \"\"\"\n        # Now that we have the date column, we filter unneeded dates again, as we took one extra day before and after\n        df = df.filter(\n            F.col(ColNames.date).between(\n                lowerBound=self.current_mt_period[\"extended_month_start\"],\n                upperBound=self.current_mt_period[\"extended_month_end\"],\n            )\n        )\n\n        if submonthly_period not in DAY_TYPES:\n            raise ValueError(f\"Unknown submonthly period/day type `{submonthly_period}`\")\n        if submonthly_period == \"all\":\n            return df\n\n        if submonthly_period == \"weekends\":\n            df = df.filter((F.weekday(ColNames.date) + F.lit(1)).isin(self.weekend_days))\n            return df\n        if submonthly_period == \"mondays\":\n            df = df.filter((F.weekday(ColNames.date)).isin([0]))\n            return df\n\n        if submonthly_period == \"tuesdays\":\n            df = df.filter((F.weekday(ColNames.date)).isin([1]))\n            return df\n        if submonthly_period == \"wednesdays\":\n            df = df.filter((F.weekday(ColNames.date)).isin([2]))\n            return df\n        if submonthly_period == \"thursdays\":\n            df = df.filter((F.weekday(ColNames.date)).isin([3]))\n            return df\n        if submonthly_period == \"fridays\":\n            df = df.filter((F.weekday(ColNames.date)).isin([4]))\n            return df\n        if submonthly_period == \"saturdays\":\n            df = df.filter((F.weekday(ColNames.date)).isin([5]))\n            return df\n        if submonthly_period == \"sundays\":\n            df = df.filter((F.weekday(ColNames.date)).isin([6]))\n            return df\n\n        holidays = F.broadcast(self.input_data_objects[\"BronzeHolidayCalendarInfoDO\"].df)\n        holidays = holidays.filter(F.col(ColNames.iso2) == F.lit(self.country_of_study))\n        if submonthly_period == \"holidays\":\n            df = df.join(holidays.select(ColNames.date), on=ColNames.date, how=\"inner\")\n        # Workdays are all days falling in one of self.work_days and not being a holiday\n        if submonthly_period == \"workdays\":\n            df = df.join(holidays.select(ColNames.date), on=ColNames.date, how=\"left_anti\").filter(\n                (F.weekday(ColNames.date) + F.lit(1)).isin(self.work_days)\n            )\n        return df\n\n    def compute_midterm_metrics(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Compute the mid-term permanence score and metrics of the current mid-term period, submonthly (i.e. day type)\n        and subdaily (i.e. time interval) combination.\n\n        Args:\n            df (DataFrame): filtered DPS DataFrame with added `date` column\n\n        Returns:\n            DataFrame: resulting DataFrame\n        \"\"\"\n        # Find latest stay of the user in each grid tile during the lookback period for regularity metrics\n        # (if it exists)\n        before_reg = (\n            df.filter(F.col(ColNames.date) &lt; F.lit(self.current_mt_period[\"month_start\"]))\n            .withColumn(ColNames.grid_id, F.explode(ColNames.dps))\n            .groupBy(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id)\n            .agg(F.max(ColNames.date).alias(ColNames.date))\n        )\n\n        # Find earliest stay of the user in each grid tile during the lookback period for regularity metrics\n        # (if it exists)\n        after_reg = (\n            df.filter(F.col(ColNames.date) &gt; F.lit(self.current_mt_period[\"month_end\"]))\n            .withColumn(ColNames.grid_id, F.explode(ColNames.dps))\n            .groupBy(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id)\n            .agg(F.min(ColNames.date).alias(ColNames.date))\n        )\n\n        # Current month data\n        study_df = df.filter(\n            F.col(ColNames.date).between(\n                lowerBound=self.current_mt_period[\"month_start\"], upperBound=self.current_mt_period[\"month_end\"]\n            )\n        )\n\n        # Device observation metric\n        observation_df = self._generate_device_observation_df(study_df)\n        # Calculate regularity metrics\n        regularity_df = self._generate_midterm_metrics_df(study_df, before_reg, after_reg)\n\n        combined_df = regularity_df.union(observation_df)\n\n        return combined_df\n\n    def _generate_device_observation_df(self, study_df: DataFrame) -&gt; DataFrame:\n        return (\n            study_df.filter(F.col(ColNames.id_type) == F.lit(UeGridIdType.GRID_STR))\n            .groupby(ColNames.user_id_modulo, ColNames.user_id, ColNames.date)\n            .agg(F.count_distinct(ColNames.time_slot_initial_time).alias(\"observed_day_dps\"))\n            .groupby(ColNames.user_id_modulo, ColNames.user_id)\n            .agg(\n                F.sum(\"observed_day_dps\").alias(ColNames.mps),\n                F.count_distinct(ColNames.date).cast(IntegerType()).alias(ColNames.frequency),\n            )\n            .select(\n                ColNames.user_id_modulo,\n                ColNames.user_id,\n                F.lit(UeGridIdType.DEVICE_OBSERVATION).alias(ColNames.grid_id),\n                F.col(ColNames.mps).cast(IntegerType()).alias(ColNames.mps),\n                ColNames.frequency,\n                F.lit(None).cast(FloatType()).alias(ColNames.regularity_mean),\n                F.lit(None).cast(FloatType()).alias(ColNames.regularity_std),\n                F.lit(UeGridIdType.DEVICE_OBSERVATION_STR).alias(ColNames.id_type),\n            )\n        )\n\n    def _generate_midterm_metrics_df(\n        self, study_df: DataFrame, before_reg: DataFrame, after_reg: DataFrame\n    ) -&gt; DataFrame:\n        df = (\n            study_df.withColumn(ColNames.grid_id, F.explode(ColNames.dps))\n            .withColumn(ColNames.dps, F.lit(1))\n            .select(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id, ColNames.date, ColNames.dps)\n            .union(before_reg.withColumn(ColNames.dps, F.lit(0)))\n            .union(after_reg.withColumn(ColNames.dps, F.lit(0)))\n            .groupby(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id)\n            .agg(\n                F.sum(ColNames.dps).cast(IntegerType()).alias(ColNames.mps),\n                F.array_sort(F.collect_set(ColNames.date)).alias(\"dates\"),\n            )\n            .filter(F.col(ColNames.mps) &gt; 0)\n        )\n\n        return self._calculate_midterm_metrics(df)\n\n    def _calculate_midterm_metrics(self, study_df: DataFrame) -&gt; DataFrame:\n        # Temp cols\n        size_days_colname = \"size_days\"\n        dates_col = \"dates\"\n\n        # First, calculate frequency as size of dates col\n        df = study_df.withColumn(ColNames.frequency, F.size(F.col(dates_col)))\n\n        # --------------- Handle Extended start/end buffer ---------------\n\n        # Add extended_start and extended_end to dates if no date found in buffer\n        month_start = self.current_mt_period[\"month_start\"]\n        month_end = self.current_mt_period[\"month_end\"]\n        extended_start = self.current_mt_period[\"extended_month_start\"]\n        extended_end = self.current_mt_period[\"extended_month_end\"]\n\n        # Add extended_start and extended_end to dates if no date found in buffer\n        df = (\n            df\n            # --- Start buffer ---\n            .withColumn(\n                dates_col,\n                # If earliest date is not in start buffer, add buffer start bound\n                F.when(\n                    F.element_at(F.col(dates_col), 1) &gt;= F.lit(month_start),\n                    F.array_union(F.array(F.lit(extended_start)), F.col(dates_col)),\n                ).otherwise(F.col(dates_col)),\n            )\n            # --- End buffer ---\n            .withColumn(\n                dates_col,\n                # If latest date is not in end buffer, add buffer end bound\n                F.when(\n                    F.element_at(F.col(dates_col), -1) &lt;= F.lit(month_end),\n                    F.array_union(F.col(dates_col), F.array(F.lit(extended_end))),\n                ).otherwise(F.col(dates_col)),\n            )\n        )\n\n        # --------------- Date distances calculation ---------------\n        day_distances_col = \"day_distances\"\n        df = df.withColumn(\n            day_distances_col,\n            F.expr(\n                f\"\"\"\n                    transform(\n                        slice({dates_col}, 2, size({dates_col})),\n                        (current_day, idx) -&gt; datediff(current_day, element_at({dates_col}, idx + 1))\n                    )\n                \"\"\"\n            ),\n        )\n\n        # --------------- Metrics calculation ---------------\n        df = (\n            df\n            # --- Frequency ---\n            .withColumn(size_days_colname, F.size(dates_col))\n            # --- Regularity mean ---\n            .withColumn(\n                # Optimized way to calculate regularity mean\n                # (latest_date - earliest_date).days / (array_length - 1)\n                ColNames.regularity_mean,\n                F.when(\n                    F.col(size_days_colname) &gt; 0,\n                    F.date_diff(F.element_at(dates_col, -1), F.element_at(dates_col, 1))\n                    / (F.col(size_days_colname) - 1),\n                ).otherwise(0.0),\n            )\n            .drop(dates_col)\n            # --- Regularity deviation ---\n            .withColumn(\n                ColNames.regularity_std,\n                F.when(\n                    F.col(size_days_colname) &gt; 1,  # Due to interval array being 1 less than the size of the dates array\n                    # (sum((dd - mean) ** 2 for dd in diffs) / (array_length - 2)) ** 0.5\n                    F.sqrt(\n                        F.expr(\n                            f\"\"\"\n                            aggregate(\n                                {day_distances_col},\n                                CAST(0 AS DOUBLE),\n                                (acc, x) -&gt; acc + POWER(x - {ColNames.regularity_mean}, 2)\n                            ) / (size({day_distances_col}) - 1)\n                        \"\"\"\n                        )\n                    ),\n                ).otherwise(0.0),\n            )\n            # Remove temp columns\n            .drop(day_distances_col)\n            .drop(size_days_colname)\n        )\n\n        # --------------- Calculate Unknown ---------------\n        df = df.withColumn(\n            ColNames.id_type,\n            F.when(F.col(ColNames.grid_id) == F.lit(UeGridIdType.UNKNOWN), F.lit(UeGridIdType.UKNOWN_STR)).otherwise(\n                F.lit(UeGridIdType.GRID_STR)\n            ),\n        )\n\n        return df\n\n    def transform(self):\n        # Load all needed dps\n        if self.time_interval == \"all\":\n            time_interval_start = dt.time(hour=self.day_start_hour)\n            time_interval_end = dt.time(hour=self.day_start_hour)\n        else:\n            time_interval_start = getattr(self, f\"{self.time_interval}_start\")\n            time_interval_end = getattr(self, f\"{self.time_interval}_end\")\n\n        # Keep only time slots belonging to the time interval\n        filtered = self.filter_dps_by_time_interval(\n            self.current_dps_data_chunk, self.time_interval, time_interval_start, time_interval_end\n        )\n\n        # Keep only time slots belonging to the day type\n        filtered = self.filter_dps_by_day_type(filtered, self.day_type)\n\n        # Compute metrics\n        mps = self.compute_midterm_metrics(filtered)\n\n        mps = (\n            mps.withColumn(ColNames.day_type, F.lit(self.day_type))\n            .withColumn(ColNames.time_interval, F.lit(self.time_interval))\n            .withColumn(ColNames.year, F.lit(self.current_mt_period[\"month_start\"].year).cast(ShortType()))\n            .withColumn(ColNames.month, F.lit(self.current_mt_period[\"month_start\"].month).cast(ByteType()))\n        )\n\n        mps = apply_schema_casting(mps, SilverMidtermPermanenceScoreDataObject.SCHEMA)\n\n        mps = mps.repartition(*SilverMidtermPermanenceScoreDataObject.PARTITION_COLUMNS)\n\n        self.output_data_objects[SilverMidtermPermanenceScoreDataObject.ID].df = mps\n\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(\"Reading data objects...\")\n        self.read()\n        self.logger.info(\"... data objects read!\")\n\n        partition_chunks = self._get_partition_chunks()\n\n        midterm_daily_data = []\n\n        self.logger.info(\"Validating DPS data for each mid-term period...\")\n        for i, mt_period in enumerate(self.midterm_periods):\n            self.logger.info(\n                f\"... validating {mt_period['extended_month_start']} to {mt_period['extended_month_end']} ...\"\n            )\n            midterm_daily_data.append(self._validate_and_load_daily_permanence_score(mt_period))\n        self.logger.info(\"... all mid-term periods validated!\")\n\n        self.logger.info(\"Starting mid-term permanece score &amp; metrics computation...\")\n        for i, mt_period in enumerate(self.midterm_periods):\n            self.current_mt_period = mt_period\n            self.current_dps_data = midterm_daily_data[i]\n            self.logger.info(f\"... working on month {mt_period['month_start']} to {mt_period['month_end']}\")\n\n            self.footprint = self.input_data_objects[SilverCellFootprintDataObject.ID].df\n            self.footprint = self.footprint.select(\n                ColNames.cell_id, ColNames.grid_id, ColNames.year, ColNames.month, ColNames.day\n            ).filter(\n                F.make_date(ColNames.year, ColNames.month, ColNames.day).between(\n                    self.current_mt_period[\"extended_month_start\"] - dt.timedelta(days=1),\n                    self.current_mt_period[\"extended_month_end\"] + dt.timedelta(days=1),\n                )\n            )\n\n            for day_type, time_intervals in self.period_combinations.items():\n                self.day_type = day_type\n\n                for time_interval in time_intervals:\n                    self.time_interval = time_interval\n\n                    for i, partition_chunk in enumerate(partition_chunks):\n                        self.logger.info(f\"Processing partition chunk {i}\")\n                        self.logger.debug(f\"Partition chunk: {partition_chunk}\")\n                        if partition_chunk is not None:\n                            self.current_dps_data_chunk = self.current_dps_data.filter(\n                                F.col(ColNames.user_id_modulo).isin(partition_chunk)\n                            )\n                        else:\n                            self.current_dps_data_chunk = self.current_dps_data\n\n                        self.transform()\n                        self.write()\n                        self.logger.info(f\"Finished processing partition chunk {i}\")\n                    self.logger.info(\n                        f\"... finished saving results for day_type `{self.day_type}` and time_interval `{self.time_interval}`\"\n                    )\n            self.logger.info(\n                f\"... finished saving results for month {mt_period['month_start']} to {mt_period['month_end']}\"\n            )\n\n        self.logger.info(\"... Finished!\")\n\n    def _get_partition_chunks(self) -&gt; List[List[int]]:\n        \"\"\"\n        Method that returns the partition chunks for the current date.\n\n        Returns:\n            List[List[int, int]]: list of partition chunks. If the partition_chunk_size is not defined in the config or\n                the number of partitions is less than the desired chunk size, it will return a list with a single None element.\n        \"\"\"\n        # Get partitions desired\n        partition_chunk_size = self.config.getint(self.COMPONENT_ID, \"partition_chunk_size\", fallback=None)\n        number_of_partitions = self.config.getint(self.COMPONENT_ID, \"number_of_partitions\", fallback=None)\n\n        if partition_chunk_size is None or number_of_partitions is None or partition_chunk_size &lt;= 0:\n            return [None]\n\n        if number_of_partitions &lt;= partition_chunk_size:\n            self.logger.warning(\n                f\"Available Partition number ({number_of_partitions}) is \"\n                f\"less than the desired chunk size ({partition_chunk_size}). \"\n                f\"Using all partitions.\"\n            )\n            return [None]\n        partition_chunks = [\n            list(range(i, min(i + partition_chunk_size, number_of_partitions)))\n            for i in range(0, number_of_partitions, partition_chunk_size)\n        ]\n        # NOTE: Generate chunks if partition_values were read for each day\n        # getting exactly the amount of partitions for that day\n\n        # partition_chunks = [\n        #     partition_values[i : i + partition_chunk_size]\n        #     for i in range(0, partition_values_size, partition_chunk_size)\n        # ]\n\n        return partition_chunks\n</code></pre>"},{"location":"reference/components/execution/midterm_permanence_score/midterm_permanence_score/#components.execution.midterm_permanence_score.midterm_permanence_score.MidtermPermanenceScore._check_time_interval","title":"<code>_check_time_interval(interval, name)</code>","text":"<p>Tries to parse time interval's start/end time from configuration file and check if it has valid minutes (00, 15, 30, or 45). If so, returns the corresponding dt.time object</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>str</code> <p>interval string to be parsed to dt.datetime</p> required <code>name</code> <code>str</code> <p>name of the interval being parsed, used for error tracking</p> required <p>Raises:</p> Type Description <code>e</code> <p>Formatting error, cannot parse time as HH:MM (24h format)</p> <code>ValueError</code> <p>interval ends in non-allowed minutes</p> <p>Returns:</p> Type Description <code>time</code> <p>dt.time: time of the start or end of the time interval</p> Source code in <code>multimno/components/execution/midterm_permanence_score/midterm_permanence_score.py</code> <pre><code>def _check_time_interval(self, interval: str, name: str) -&gt; dt.time:\n    \"\"\"Tries to parse time interval's start/end time from configuration file and check if it has\n    valid minutes (00, 15, 30, or 45). If so, returns the corresponding dt.time object\n\n    Args:\n        interval (str): interval string to be parsed to dt.datetime\n        name (str): name of the interval being parsed, used for error tracking\n\n    Raises:\n        e: Formatting error, cannot parse time as HH:MM (24h format)\n        ValueError: interval ends in non-allowed minutes\n\n    Returns:\n        dt.time: time of the start or end of the time interval\n    \"\"\"\n    try:\n        interval = dt.datetime.strptime(interval, \"%H:%M\")\n    except ValueError as e:\n        self.logger.error(f\"Could not parse {name}, expected HH:MM format, found {interval}\")\n        raise e\n\n    interval = interval.time()\n    if interval.minute not in [0, 15, 30, 45]:\n        raise ValueError(f\"Time interval {name} must have :00, :15, :30, or :45 minutes, found :{interval.minute}\")\n    return interval\n</code></pre>"},{"location":"reference/components/execution/midterm_permanence_score/midterm_permanence_score/#components.execution.midterm_permanence_score.midterm_permanence_score.MidtermPermanenceScore._check_weekday_number","title":"<code>_check_weekday_number(num, context)</code>","text":"<p>Parses and validates a day of the week</p> <p>Parameters:</p> Name Type Description Default <code>num</code> <code>str</code> <p>string to be parsed to integer between 1 and 7</p> required <code>context</code> <code>str</code> <p>string for error tracking</p> required <p>Raises:</p> Type Description <code>e</code> <p>Error in parsing num to int</p> <code>ValueError</code> <p>num is not a valid day of the week (between 1 and 7 inclusive)</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>integer representing a day of the week</p> Source code in <code>multimno/components/execution/midterm_permanence_score/midterm_permanence_score.py</code> <pre><code>def _check_weekday_number(self, num: str, context: str) -&gt; int:\n    \"\"\"Parses and validates a day of the week\n\n    Args:\n        num (str): string to be parsed to integer between 1 and 7\n        context (str): string for error tracking\n\n    Raises:\n        e: Error in parsing num to int\n        ValueError: num is not a valid day of the week (between 1 and 7 inclusive)\n\n    Returns:\n        int: integer representing a day of the week\n    \"\"\"\n    try:\n        num = int(num)\n    except Exception as e:\n        self.logger.error(f\"Must specify a day as an integer between 1 and 7, but found `{num}` in `{context}`\")\n        raise e\n    if num &lt; 1 or num &gt; 7:\n        raise ValueError(\n            f\"Days must take a value between 1 for Monday and 7 for Sunday, found {num} in `{context}`\"\n        )\n    return num\n</code></pre>"},{"location":"reference/components/execution/midterm_permanence_score/midterm_permanence_score/#components.execution.midterm_permanence_score.midterm_permanence_score.MidtermPermanenceScore._get_midterm_periods","title":"<code>_get_midterm_periods()</code>","text":"<p>Computes the date limits of each mid-term period, together with the limits of the regularity metrics' extra dates</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: list of dictionaries with the information of dates of each mid-term period</p> Source code in <code>multimno/components/execution/midterm_permanence_score/midterm_permanence_score.py</code> <pre><code>def _get_midterm_periods(self) -&gt; List[dict]:\n    \"\"\"Computes the date limits of each mid-term period, together with the limits of the regularity metrics' extra\n    dates\n\n    Returns:\n        List[dict]: list of dictionaries with the information of dates of each mid-term period\n    \"\"\"\n    midterm_periods = []\n    start_of_the_month = self.start_date\n\n    while True:\n        end_of_the_month = start_of_the_month.replace(\n            day=cal.monthrange(start_of_the_month.year, start_of_the_month.month)[1]\n        )\n        before_reg_date = start_of_the_month - dt.timedelta(days=self.before_reg_days)\n        after_reg_date = end_of_the_month + dt.timedelta(days=self.after_reg_days)\n\n        midterm_periods.append(\n            {\n                \"month_start\": start_of_the_month,\n                \"month_end\": end_of_the_month,\n                \"extended_month_start\": before_reg_date,\n                \"extended_month_end\": after_reg_date,\n            }\n        )\n\n        if end_of_the_month == self.end_date:\n            return midterm_periods\n\n        start_of_the_month = end_of_the_month + dt.timedelta(days=1)\n</code></pre>"},{"location":"reference/components/execution/midterm_permanence_score/midterm_permanence_score/#components.execution.midterm_permanence_score.midterm_permanence_score.MidtermPermanenceScore._get_partition_chunks","title":"<code>_get_partition_chunks()</code>","text":"<p>Method that returns the partition chunks for the current date.</p> <p>Returns:</p> Type Description <code>List[List[int]]</code> <p>List[List[int, int]]: list of partition chunks. If the partition_chunk_size is not defined in the config or the number of partitions is less than the desired chunk size, it will return a list with a single None element.</p> Source code in <code>multimno/components/execution/midterm_permanence_score/midterm_permanence_score.py</code> <pre><code>def _get_partition_chunks(self) -&gt; List[List[int]]:\n    \"\"\"\n    Method that returns the partition chunks for the current date.\n\n    Returns:\n        List[List[int, int]]: list of partition chunks. If the partition_chunk_size is not defined in the config or\n            the number of partitions is less than the desired chunk size, it will return a list with a single None element.\n    \"\"\"\n    # Get partitions desired\n    partition_chunk_size = self.config.getint(self.COMPONENT_ID, \"partition_chunk_size\", fallback=None)\n    number_of_partitions = self.config.getint(self.COMPONENT_ID, \"number_of_partitions\", fallback=None)\n\n    if partition_chunk_size is None or number_of_partitions is None or partition_chunk_size &lt;= 0:\n        return [None]\n\n    if number_of_partitions &lt;= partition_chunk_size:\n        self.logger.warning(\n            f\"Available Partition number ({number_of_partitions}) is \"\n            f\"less than the desired chunk size ({partition_chunk_size}). \"\n            f\"Using all partitions.\"\n        )\n        return [None]\n    partition_chunks = [\n        list(range(i, min(i + partition_chunk_size, number_of_partitions)))\n        for i in range(0, number_of_partitions, partition_chunk_size)\n    ]\n    # NOTE: Generate chunks if partition_values were read for each day\n    # getting exactly the amount of partitions for that day\n\n    # partition_chunks = [\n    #     partition_values[i : i + partition_chunk_size]\n    #     for i in range(0, partition_values_size, partition_chunk_size)\n    # ]\n\n    return partition_chunks\n</code></pre>"},{"location":"reference/components/execution/midterm_permanence_score/midterm_permanence_score/#components.execution.midterm_permanence_score.midterm_permanence_score.MidtermPermanenceScore._validate_and_load_daily_permanence_score","title":"<code>_validate_and_load_daily_permanence_score(mt_period)</code>","text":"<p>Loads the Daily Permanence Score data to be used for the calculation of the Mid-term Permanence Score metrics of a particular mid-term period. Filters out DPS values equal to zero and checks that the time slots are compatible with the configuration-provided time intervals.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If DPS data has a time slot duration different from 15, 30, or 60 minutes.</p> <code>ValueError</code> <p>If DPS data has 60-min slots but 15- or 30-min lengths are required.</p> <code>ValueError</code> <p>If DPS data has 30-min slots but 15-min lengths are required.</p> <p>Returns:</p> Name Type Description <code>dps</code> <code>DataFrame</code> <p>DataFrame of all DPS data necessary to calcualte the mid-term permanence score &amp; metrics of self.current_mt_period</p> Source code in <code>multimno/components/execution/midterm_permanence_score/midterm_permanence_score.py</code> <pre><code>def _validate_and_load_daily_permanence_score(self, mt_period: dict) -&gt; DataFrame:\n    \"\"\"Loads the Daily Permanence Score data to be used for the calculation of the Mid-term Permanence Score metrics\n    of a particular mid-term period. Filters out DPS values equal to zero and checks that the time slots are\n    compatible with the configuration-provided time intervals.\n\n    Raises:\n        ValueError: If DPS data has a time slot duration different from 15, 30, or 60 minutes.\n        ValueError: If DPS data has 60-min slots but 15- or 30-min lengths are required.\n        ValueError: If DPS data has 30-min slots but 15-min lengths are required.\n\n    Returns:\n        dps: DataFrame of all DPS data necessary to calcualte the mid-term permanence score &amp; metrics of\n            self.current_mt_period\n    \"\"\"\n    dps = self.input_data_objects[SilverDailyPermanenceScoreDataObject.ID].df\n\n    # Add a one-day buffer, as later on the definition of a day does not match the midnight definition\n    dps = dps.filter(\n        F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n        &gt;= F.lit(mt_period[\"extended_month_start\"] - dt.timedelta(days=1))\n    ).filter(\n        F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n        &lt;= F.lit(mt_period[\"extended_month_end\"]) + dt.timedelta(days=1)\n    )\n\n    # If all time_intervals match a whole hour, no check needs to be done here.\n    if self.midterm_minutes == {0}:\n        return dps\n\n    # If the previous set was not disjoint, then the user has entered some time_interval that does not match a\n    # whole hour. We check, for all loaded dates, what their time slot length was. We only need to check the length\n    # of one time slot per DPS date.\n    time_slot_duration = (\n        dps.groupby([ColNames.year, ColNames.month, ColNames.day])\n        .agg(\n            F.first(F.col(ColNames.time_slot_end_time) - F.col(ColNames.time_slot_initial_time)).alias(\n                \"time_slot_duration\"\n            )\n        )\n        .collect()\n    )\n    daily_duration = {\n        dt.date(row[\"year\"], row[\"month\"], row[\"day\"]): row[\"time_slot_duration\"].seconds // 60\n        for row in time_slot_duration\n    }\n\n    # If disjoint, user has specified intervals ending in :00 or :30. Then we admit 15- or 30-min time slot\n    # durations, but not whole hours.\n    # If not disjoint, user has specified some :15 or :45 intervals, and we can only admit 15-min time slots\n    check_for_half_hour_only = {15, 45}.isdisjoint(self.midterm_minutes)\n\n    for date, duration in daily_duration.items():\n        if duration not in {15, 30, 60}:\n            raise ValueError(\n                f\"Found time_slot duration of {duration} min in DailyPermanenceScore of {date}, when \"\n                \"accepted values are 15, 30, or 60 minutes.\"\n            )\n        if duration == 60:\n            if check_for_half_hour_only:\n                msg = (\n                    f\"Found time_slot duration of 60 min in DailyPermanenceScore of {date}, when only accepted \"\n                    \"values are 15 and 30\"\n                )\n            else:\n                msg = (\n                    f\"Found time_slot duration of 60 min in DailyPermanenceScore of {date}, when only accepted \"\n                    \"value is 15\"\n                )\n            raise ValueError(msg)\n        if duration == 30 and not check_for_half_hour_only:\n            raise ValueError(\n                f\"Found time_slot duration of 30 min in DailyPermanenceScore of {date}, when only accepted \"\n                \"value is 15\"\n            )\n    return dps\n</code></pre>"},{"location":"reference/components/execution/midterm_permanence_score/midterm_permanence_score/#components.execution.midterm_permanence_score.midterm_permanence_score.MidtermPermanenceScore.compute_midterm_metrics","title":"<code>compute_midterm_metrics(df)</code>","text":"<p>Compute the mid-term permanence score and metrics of the current mid-term period, submonthly (i.e. day type) and subdaily (i.e. time interval) combination.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>filtered DPS DataFrame with added <code>date</code> column</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>resulting DataFrame</p> Source code in <code>multimno/components/execution/midterm_permanence_score/midterm_permanence_score.py</code> <pre><code>def compute_midterm_metrics(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Compute the mid-term permanence score and metrics of the current mid-term period, submonthly (i.e. day type)\n    and subdaily (i.e. time interval) combination.\n\n    Args:\n        df (DataFrame): filtered DPS DataFrame with added `date` column\n\n    Returns:\n        DataFrame: resulting DataFrame\n    \"\"\"\n    # Find latest stay of the user in each grid tile during the lookback period for regularity metrics\n    # (if it exists)\n    before_reg = (\n        df.filter(F.col(ColNames.date) &lt; F.lit(self.current_mt_period[\"month_start\"]))\n        .withColumn(ColNames.grid_id, F.explode(ColNames.dps))\n        .groupBy(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id)\n        .agg(F.max(ColNames.date).alias(ColNames.date))\n    )\n\n    # Find earliest stay of the user in each grid tile during the lookback period for regularity metrics\n    # (if it exists)\n    after_reg = (\n        df.filter(F.col(ColNames.date) &gt; F.lit(self.current_mt_period[\"month_end\"]))\n        .withColumn(ColNames.grid_id, F.explode(ColNames.dps))\n        .groupBy(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id)\n        .agg(F.min(ColNames.date).alias(ColNames.date))\n    )\n\n    # Current month data\n    study_df = df.filter(\n        F.col(ColNames.date).between(\n            lowerBound=self.current_mt_period[\"month_start\"], upperBound=self.current_mt_period[\"month_end\"]\n        )\n    )\n\n    # Device observation metric\n    observation_df = self._generate_device_observation_df(study_df)\n    # Calculate regularity metrics\n    regularity_df = self._generate_midterm_metrics_df(study_df, before_reg, after_reg)\n\n    combined_df = regularity_df.union(observation_df)\n\n    return combined_df\n</code></pre>"},{"location":"reference/components/execution/midterm_permanence_score/midterm_permanence_score/#components.execution.midterm_permanence_score.midterm_permanence_score.MidtermPermanenceScore.filter_dps_by_day_type","title":"<code>filter_dps_by_day_type(df, submonthly_period)</code>","text":"<p>Filter DPS matrix, with an assigned date column, keeping the time slots that belong to the day type or submonthly period specified.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DPS dataframe, with assigned <code>date</code> column</p> required <code>submonthly_period</code> <code>str</code> <p>submonthly period or day type. Must be one of the values in MidtermPermanenceScore.DAY_TYPE</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Whenever an unknown submonthly period is specified</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Filtered dataframe</p> Source code in <code>multimno/components/execution/midterm_permanence_score/midterm_permanence_score.py</code> <pre><code>def filter_dps_by_day_type(self, df: DataFrame, submonthly_period: str) -&gt; DataFrame:\n    \"\"\"Filter DPS matrix, with an assigned date column, keeping the time slots that belong to the day type or\n    submonthly period specified.\n\n    Args:\n        df (DataFrame): DPS dataframe, with assigned `date` column\n        submonthly_period (str): submonthly period or day type. Must be one of the values in\n            MidtermPermanenceScore.DAY_TYPE\n\n    Raises:\n        ValueError: Whenever an unknown submonthly period is specified\n\n    Returns:\n        DataFrame: Filtered dataframe\n    \"\"\"\n    # Now that we have the date column, we filter unneeded dates again, as we took one extra day before and after\n    df = df.filter(\n        F.col(ColNames.date).between(\n            lowerBound=self.current_mt_period[\"extended_month_start\"],\n            upperBound=self.current_mt_period[\"extended_month_end\"],\n        )\n    )\n\n    if submonthly_period not in DAY_TYPES:\n        raise ValueError(f\"Unknown submonthly period/day type `{submonthly_period}`\")\n    if submonthly_period == \"all\":\n        return df\n\n    if submonthly_period == \"weekends\":\n        df = df.filter((F.weekday(ColNames.date) + F.lit(1)).isin(self.weekend_days))\n        return df\n    if submonthly_period == \"mondays\":\n        df = df.filter((F.weekday(ColNames.date)).isin([0]))\n        return df\n\n    if submonthly_period == \"tuesdays\":\n        df = df.filter((F.weekday(ColNames.date)).isin([1]))\n        return df\n    if submonthly_period == \"wednesdays\":\n        df = df.filter((F.weekday(ColNames.date)).isin([2]))\n        return df\n    if submonthly_period == \"thursdays\":\n        df = df.filter((F.weekday(ColNames.date)).isin([3]))\n        return df\n    if submonthly_period == \"fridays\":\n        df = df.filter((F.weekday(ColNames.date)).isin([4]))\n        return df\n    if submonthly_period == \"saturdays\":\n        df = df.filter((F.weekday(ColNames.date)).isin([5]))\n        return df\n    if submonthly_period == \"sundays\":\n        df = df.filter((F.weekday(ColNames.date)).isin([6]))\n        return df\n\n    holidays = F.broadcast(self.input_data_objects[\"BronzeHolidayCalendarInfoDO\"].df)\n    holidays = holidays.filter(F.col(ColNames.iso2) == F.lit(self.country_of_study))\n    if submonthly_period == \"holidays\":\n        df = df.join(holidays.select(ColNames.date), on=ColNames.date, how=\"inner\")\n    # Workdays are all days falling in one of self.work_days and not being a holiday\n    if submonthly_period == \"workdays\":\n        df = df.join(holidays.select(ColNames.date), on=ColNames.date, how=\"left_anti\").filter(\n            (F.weekday(ColNames.date) + F.lit(1)).isin(self.work_days)\n        )\n    return df\n</code></pre>"},{"location":"reference/components/execution/midterm_permanence_score/midterm_permanence_score/#components.execution.midterm_permanence_score.midterm_permanence_score.MidtermPermanenceScore.filter_dps_by_time_interval","title":"<code>filter_dps_by_time_interval(df, subdaily_period, start, end)</code>","text":"<p>Filter DPS matrix by keeping the time slots that belong to the subdaily period or time interval specified. Also create a new column, <code>date</code>, which contains to which day the time slot belongs to, as it does not necessarily match the day the time slot belongs to (i.e. the <code>midnight</code> definition)</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DPS dataframe to be filtered.</p> required <code>subdaily_period</code> <code>str</code> <p>name of the time interval or subdaily period. Must be one of the values in MidtermPermanenceScore.TIME_INTERVALS.</p> required <code>start</code> <code>time</code> <p>earliest time of accepted time slots that will not be filtered out</p> required <code>end</code> <code>time</code> <p>latest time of accepted time slots that will not be filtered out</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Whenever an unknown subdaily period is specified</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>filtered DPS dataframe</p> Source code in <code>multimno/components/execution/midterm_permanence_score/midterm_permanence_score.py</code> <pre><code>def filter_dps_by_time_interval(\n    self, df: DataFrame, subdaily_period: str, start: dt.time, end: dt.time\n) -&gt; DataFrame:\n    \"\"\"Filter DPS matrix by keeping the time slots that belong to the subdaily period or time interval\n    specified. Also create a new column, `date`, which contains to which day the time slot belongs to, as it\n    does not necessarily match the day the time slot belongs to (i.e. the `midnight` definition)\n\n    Args:\n        df (DataFrame): DPS dataframe to be filtered.\n        subdaily_period (str): name of the time interval or subdaily period. Must be one of the values in\n            MidtermPermanenceScore.TIME_INTERVALS.\n        start (dt.time): earliest time of accepted time slots that will not be filtered out\n        end (dt.time): latest time of accepted time slots that will not be filtered out\n\n    Raises:\n        ValueError: Whenever an unknown subdaily period is specified\n\n    Returns:\n        DataFrame: filtered DPS dataframe\n    \"\"\"\n    if subdaily_period not in TIME_INTERVALS:\n        raise ValueError(f\"Unknown subdaily/time_interval {subdaily_period}\")\n\n    # Auxiliary variables\n    start_hour = F.lit(start.hour)\n    start_min = F.lit(start.minute)\n    end_hour = end.hour\n    if end_hour == 0:\n        end_hour = F.lit(24)\n    else:\n        end_hour = F.lit(end_hour)\n    end_min = F.lit(end.minute)\n\n    slot_initial_hour = F.hour(ColNames.time_slot_initial_time)\n    slot_initial_min = F.minute(ColNames.time_slot_initial_time)\n    slot_end_hour = F.hour(ColNames.time_slot_end_time)\n    slot_end_hour = F.when(slot_end_hour == F.lit(0), F.lit(24)).otherwise(slot_end_hour)\n    slot_end_min = F.minute(ColNames.time_slot_end_time)\n\n    # Global time interval, taking all time slots\n    if subdaily_period == \"all\":\n        if start != end:\n            raise ValueError(\n                \"`all` time interval must have matching start and end times to not overlap with \"\n                f\"different dates, found start={start} and end={end}\"\n            )\n        # If the day starts and ends at midnight, no additional logic needs to be done for assigning the correct\n        # date\n        if start == dt.time(0, 0):\n            df = df.withColumn(ColNames.date, F.col(ColNames.time_slot_initial_time).cast(DateType()))\n            return df\n        # If not: the hour that defines the day always belongs to that day.\n        # e.g., if subdaily_start=4, then day D is defined as 4:00AM of day D to 4:00 AM of day D+1\n        df = df.withColumn(\n            ColNames.date,\n            F.when(\n                F.lit(60) * (slot_initial_hour - start_hour) &gt;= (start_min - slot_initial_min),\n                F.col(ColNames.time_slot_initial_time),\n            )\n            .otherwise(F.date_add(F.col(ColNames.time_slot_initial_time), -1))\n            .cast(DateType()),\n        )\n        return df\n\n    # Rest of time intervals: night_time, evening_time, working_hours.\n    # Filter out time slots not contained in the time interval\n    if start &lt; end or end == dt.time(0, 0):\n        df = df.filter(\n            (F.lit(60) * (slot_initial_hour - start_hour) &gt;= (start_min - slot_initial_min))\n            &amp; (F.lit(60) * (slot_end_hour - end_hour) &lt;= (end_min - slot_end_min))\n        )\n    else:\n        df = df.filter(\n            (F.lit(60) * (slot_initial_hour - start_hour) &gt;= (start_min - slot_initial_min))\n            | (F.lit(60) * (slot_end_hour - end_hour) &lt;= (end_min - slot_end_min))\n        )\n\n    # consider self.day_start_hour = 4 for the following examples\n    if subdaily_period == \"night_time\":\n        if start.hour &gt;= self.day_start_hour and (end &gt; start or end == dt.time(0, 0)):\n            # night-interval stays in the day, example [20:15 to 23:30] or [19:45 to 00:00]\n            df = df.withColumn(ColNames.date, F.col(ColNames.time_slot_initial_time).cast(DateType()))\n        elif start.hour &gt;= self.day_start_hour and end &lt; start and end != dt.time(0, 0):\n            # night-interval starts in the day and crosses midnight, example [20:15 to 03:30] or [19:00 to 18:45]\n            df = df.withColumn(\n                ColNames.date,\n                F.when(\n                    F.lit(60) * (slot_initial_hour - start_hour) &gt;= (start_min - slot_initial_min),\n                    F.col(ColNames.time_slot_initial_time),\n                )\n                .otherwise(F.date_add(F.col(ColNames.time_slot_initial_time), -1))\n                .cast(DateType()),\n            )\n        elif start.hour &lt; self.day_start_hour and (end &gt; start or end == dt.time(0, 0)):\n            # night-interval starts in the next day and does not cross midnight, example [3:15 to 6:00] or [2:30 to 10:45]\n            df = df.withColumn(\n                ColNames.date, F.date_add(F.col(ColNames.time_slot_initial_time), -1).cast(DateType())\n            )\n        return df\n\n    # if subdaily_period in (\"working_hours\", \"evening_time\"):\n    if start &gt;= end and end != dt.time(0, 0):\n        self.logger.log(\n            msg=f\"'inverted' start ({start}) and end ({end}) times found for subdaily period {subdaily_period}\"\n            \" -- the whole period will belong to the day of start of the interval\",\n            level=logging.INFO,\n        )\n    df = df.withColumn(\n        ColNames.date,\n        F.when(\n            F.lit(60) * (slot_initial_hour - start_hour) &gt;= (start_min - slot_initial_min),\n            F.col(ColNames.time_slot_initial_time),\n        )\n        .otherwise(F.date_add(F.col(ColNames.time_slot_initial_time), -1))\n        .cast(DateType()),\n    )\n    return df\n</code></pre>"},{"location":"reference/components/execution/multimno_aggregation/","title":"multimno_aggregation","text":""},{"location":"reference/components/execution/multimno_aggregation/multimno_aggregation/","title":"multimno_aggregation","text":"<p>Module that implements the MultiMNO aggregation component</p>"},{"location":"reference/components/execution/multimno_aggregation/multimno_aggregation/#components.execution.multimno_aggregation.multimno_aggregation.MultiMNOAggregation","title":"<code>MultiMNOAggregation</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class responsible for the aggregation of indicators computed in different MNOs into a single, aggregate MultiMNO indicator.</p> Source code in <code>multimno/components/execution/multimno_aggregation/multimno_aggregation.py</code> <pre><code>class MultiMNOAggregation(Component):\n    \"\"\"\n    Class responsible for the aggregation of indicators computed in different MNOs into a single, aggregate MultiMNO\n    indicator.\n    \"\"\"\n\n    COMPONENT_ID = \"MultiMNOAggregation\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.current_component_id: str = None\n        self.target_column: str = None\n        self.single_mno_factors: list[float] = None\n        self.number_of_single_mnos: int = None\n\n    def initalize_data_objects(self):\n\n        self.execute_present_population = self.config.getboolean(self.COMPONENT_ID, \"present_population_execution\")\n        self.execute_usual_environment = self.config.getboolean(self.COMPONENT_ID, \"usual_environment_execution\")\n        self.execute_internal_migration = self.config.getboolean(self.COMPONENT_ID, \"internal_migration_execution\")\n\n        self.data_objects = {}\n\n        if self.execute_present_population:\n            number_of_single_mnos = self.config.getint(\n                f\"{self.COMPONENT_ID}.{PresentPopulationEstimation.COMPONENT_ID}\", \"number_of_single_mnos\"\n            )\n            input_do_paths = [\n                self.config.get(\n                    CONFIG_GOLD_PATHS_KEY,\n                    CLASS_MAPPING[PresentPopulationEstimation.COMPONENT_ID][\"input_path_config_key\"](i),\n                )\n                for i in range(1, number_of_single_mnos + 1)\n            ]\n            output_do_path = self.config.get(\n                CONFIG_GOLD_PATHS_KEY,\n                CLASS_MAPPING[PresentPopulationEstimation.COMPONENT_ID][\"output_path_config_key\"],\n            )\n\n            for i, input_do_path in enumerate(input_do_paths):\n                if not check_if_data_path_exists(self.spark, input_do_path):\n                    self.logger.warning(f\"Expected path {input_do_path} to exist but it does not\")\n                    raise ValueError(\n                        f\"Invalid path for {CLASS_MAPPING[PresentPopulationEstimation.COMPONENT_ID]['constructor'].ID}: {input_do_path}\"\n                    )\n            input_dos = [\n                CLASS_MAPPING[PresentPopulationEstimation.COMPONENT_ID][\"constructor\"](self.spark, path)\n                for path in input_do_paths\n            ]\n\n            clear_destination_directory = self.config.getboolean(\n                f\"{self.COMPONENT_ID}.PresentPopulationEstimation\", \"clear_destination_directory\"\n            )\n            if clear_destination_directory:\n                delete_file_or_folder(self.spark, output_do_path)\n\n            output_do = CLASS_MAPPING[PresentPopulationEstimation.COMPONENT_ID][\"constructor\"](\n                self.spark, output_do_path\n            )\n\n            self.data_objects[PresentPopulationEstimation.COMPONENT_ID] = {\"input\": input_dos, \"output\": output_do}\n\n        if self.execute_usual_environment:\n            number_of_single_mnos = self.config.getint(\n                f\"{self.COMPONENT_ID}.{UsualEnvironmentAggregation.COMPONENT_ID}\", \"number_of_single_mnos\"\n            )\n            input_do_paths = [\n                self.config.get(\n                    CONFIG_GOLD_PATHS_KEY,\n                    CLASS_MAPPING[UsualEnvironmentAggregation.COMPONENT_ID][\"input_path_config_key\"](i),\n                )\n                for i in range(1, number_of_single_mnos + 1)\n            ]\n            output_do_path = self.config.get(\n                CONFIG_GOLD_PATHS_KEY,\n                CLASS_MAPPING[UsualEnvironmentAggregation.COMPONENT_ID][\"output_path_config_key\"],\n            )\n\n            for i, input_do_path in enumerate(input_do_paths):\n                if not check_if_data_path_exists(self.spark, input_do_path):\n                    self.logger.warning(f\"Expected path {input_do_path} to exist but it does not\")\n                    raise ValueError(\n                        f\"Invalid path for {CLASS_MAPPING[UsualEnvironmentAggregation.COMPONENT_ID]['constructor'].ID}: {input_do_path}\"\n                    )\n            input_dos = [\n                CLASS_MAPPING[UsualEnvironmentAggregation.COMPONENT_ID][\"constructor\"](self.spark, path)\n                for path in input_do_paths\n            ]\n\n            clear_destination_directory = self.config.getboolean(\n                f\"{self.COMPONENT_ID}.UsualEnvironmentAggregation\", \"clear_destination_directory\"\n            )\n            if clear_destination_directory:\n                delete_file_or_folder(self.spark, output_do_path)\n\n            output_do = CLASS_MAPPING[UsualEnvironmentAggregation.COMPONENT_ID][\"constructor\"](\n                self.spark, output_do_path\n            )\n\n            self.data_objects[UsualEnvironmentAggregation.COMPONENT_ID] = {\"input\": input_dos, \"output\": output_do}\n\n        if self.execute_internal_migration:\n            number_of_single_mnos = self.config.getint(\n                f\"{self.COMPONENT_ID}.{InternalMigration.COMPONENT_ID}\", \"number_of_single_mnos\"\n            )\n            input_do_paths = [\n                self.config.get(\n                    CONFIG_GOLD_PATHS_KEY,\n                    CLASS_MAPPING[InternalMigration.COMPONENT_ID][\"input_path_config_key\"](i),\n                )\n                for i in range(1, number_of_single_mnos + 1)\n            ]\n            output_do_path = self.config.get(\n                CONFIG_GOLD_PATHS_KEY,\n                CLASS_MAPPING[InternalMigration.COMPONENT_ID][\"output_path_config_key\"],\n            )\n\n            for i, input_do_path in enumerate(input_do_paths):\n                if not check_if_data_path_exists(self.spark, input_do_path):\n                    self.logger.warning(f\"Expected path {input_do_path} to exist but it does not\")\n                    raise ValueError(\n                        f\"Invalid path for {CLASS_MAPPING[InternalMigration.COMPONENT_ID]['constructor'].ID}: {input_do_path}\"\n                    )\n            input_dos = [\n                CLASS_MAPPING[InternalMigration.COMPONENT_ID][\"constructor\"](self.spark, path)\n                for path in input_do_paths\n            ]\n\n            clear_destination_directory = self.config.getboolean(\n                f\"{self.COMPONENT_ID}.InternalMigration\", \"clear_destination_directory\"\n            )\n            if clear_destination_directory:\n                delete_file_or_folder(self.spark, output_do_path)\n\n            output_do = CLASS_MAPPING[InternalMigration.COMPONENT_ID][\"constructor\"](self.spark, output_do_path)\n\n            self.data_objects[InternalMigration.COMPONENT_ID] = {\"input\": input_dos, \"output\": output_do}\n\n    def filter_dataframe(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Filtering function that takes the partitions of the dataframe specified via configuration file\n\n        Args:\n            df (DataFrame): original DataFrame\n\n        Raises:\n            ValueError: if `season` value in configuration file is not one of allowed values\n\n        Returns:\n            DataFrame: filtered DataFrame\n        \"\"\"\n        # TODO: move config reading and validation to __init__ (?)\n        if self.current_component_id == PresentPopulationEstimation.COMPONENT_ID:\n            zoning_dataset = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"zoning_dataset_id\")\n            levels = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"hierarchical_levels\")\n            levels = list(int(x.strip()) for x in levels.split(\",\"))\n\n            start_date = dt.datetime.strptime(\n                self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"start_date\"), \"%Y-%m-%d\"\n            )\n            end_date = dt.datetime.strptime(\n                self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"end_date\"), \"%Y-%m-%d\"\n            )\n\n            df = (\n                df.where(F.col(ColNames.dataset_id) == F.lit(zoning_dataset))\n                .where(F.col(ColNames.level).isin(levels))\n                .where(F.make_date(ColNames.year, ColNames.month, ColNames.day).between(start_date, end_date))\n            )\n\n        if self.current_component_id == UsualEnvironmentAggregation.COMPONENT_ID:\n            zoning_dataset = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"zoning_dataset_id\")\n            levels = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"hierarchical_levels\")\n            levels = list(int(x.strip()) for x in levels.split(\",\"))\n            labels = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"labels\")\n            labels = list(x.strip() for x in labels.split(\",\"))\n\n            start_date = dt.datetime.strptime(\n                self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"start_month\"), \"%Y-%m\"\n            )\n            end_date = dt.datetime.strptime(\n                self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"end_month\"), \"%Y-%m\"\n            )\n            end_date = end_date + dt.timedelta(days=cal.monthrange(end_date.year, end_date.month)[1] - 1)\n            season = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"season\")\n            if season not in SEASONS:\n                raise ValueError(f\"Unknown season {season} -- valid values are {SEASONS}\")\n\n            df = (\n                df.where(F.col(ColNames.dataset_id) == zoning_dataset)\n                .where(F.col(ColNames.level).isin(levels))\n                .where(F.col(ColNames.label).isin(labels))\n                .where(F.col(ColNames.start_date) == start_date)\n                .where(F.col(ColNames.end_date) == end_date)\n                .where(F.col(ColNames.season) == season)\n            )\n\n        if self.current_component_id == InternalMigration.COMPONENT_ID:\n            zoning_dataset = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"zoning_dataset_id\")\n            levels = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"hierarchical_levels\")\n            levels = list(int(x.strip()) for x in levels.split(\",\"))\n\n            start_date_prev = dt.datetime.strptime(\n                self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"start_month_previous\"), \"%Y-%m\"\n            )\n            end_date_prev = dt.datetime.strptime(\n                self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"end_month_previous\"), \"%Y-%m\"\n            )\n            end_date_prev = end_date_prev + dt.timedelta(\n                days=cal.monthrange(end_date_prev.year, end_date_prev.month)[1] - 1\n            )\n            season_prev = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"season_previous\")\n            if season_prev not in SEASONS:\n                raise ValueError(f\"Unknown season {season_prev} -- valid values are {SEASONS}\")\n\n            start_date_new = dt.datetime.strptime(\n                self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"start_month_new\"), \"%Y-%m\"\n            )\n            end_date_new = dt.datetime.strptime(\n                self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"end_month_new\"), \"%Y-%m\"\n            )\n            end_date_new = end_date_new + dt.timedelta(\n                days=cal.monthrange(end_date_new.year, end_date_new.month)[1] - 1\n            )\n            season_new = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"season_new\")\n            if season_new not in SEASONS:\n                raise ValueError(f\"Unknown season {season_new} -- valid values are {SEASONS}\")\n\n            df = (\n                df.where(F.col(ColNames.dataset_id) == zoning_dataset)\n                .where(F.col(ColNames.level).isin(levels))\n                .where(F.col(ColNames.start_date_previous) == start_date_prev)\n                .where(F.col(ColNames.end_date_previous) == end_date_prev)\n                .where(F.col(ColNames.season_previous) == season_prev)\n                .where(F.col(ColNames.start_date_new) == start_date_new)\n                .where(F.col(ColNames.end_date_new) == end_date_new)\n                .where(F.col(ColNames.season_new) == season_new)\n            )\n        return df\n        return df\n\n    @staticmethod\n    def filter_out_obfuscated_records(dfs: list[DataFrame], target_column: str) -&gt; list[DataFrame]:\n        \"\"\"Filter out obfuscated records, which are records flagged by the k-anonimity process to have values lower\n        than k, for some value k. Specific treatment for these obfuscated values is not yet defined, so we simply\n        remove them in this version of the code.\n\n        Args:\n            dfs (list[DataFrame]): list of single MNO dataframes with possibly obfuscated values\n            target_column (str): name of the target column containig the value of the indicator\n\n        Returns:\n            list[DataFrame]: list of single MNO dataframes without obfuscated values\n        \"\"\"\n        output_dfs = [df.where(F.col(target_column) &gt; F.lit(0)) for df in dfs]\n        return output_dfs\n\n    def aggregate_single_mno_indicators(self, dfs: list[DataFrame]) -&gt; DataFrame:\n        \"\"\"Aggregate the indicator over all single MNOs' data. First, the value of each MNO is multiplied by some\n        weight, and then these values are added up, such that the sum of these weights add up to 1.\n\n        Args:\n            dfs (list[DataFrame]): list of single MNO dataframes\n\n        Returns:\n            DataFrame: final dataframe with the weighted sum indicator of all MNOs\n        \"\"\"\n        weighted_dfs = [\n            df.withColumn(self.target_column, F.col(self.target_column) * F.lit(factor))\n            for (df, factor) in zip(dfs, self.single_mno_factors)\n        ]\n\n        groupby_cols = [\n            column\n            for column in CLASS_MAPPING[self.current_component_id][\"constructor\"].SCHEMA.names\n            if column != self.target_column\n        ]\n\n        agg_df = (\n            reduce(DataFrame.union, weighted_dfs)\n            .groupBy(*groupby_cols)\n            .agg(F.sum(self.target_column).alias(self.target_column))\n        )\n\n        agg_df = apply_schema_casting(agg_df, CLASS_MAPPING[self.current_component_id][\"constructor\"].SCHEMA)\n\n        return agg_df\n\n    def transform(self):\n        # Get list of input single MNO indicators\n        single_mno_dfs = [\n            self.input_data_objects[f\"{CLASS_MAPPING[self.current_component_id]['constructor'].ID}_{i}\"].df\n            for i in range(1, self.number_of_single_mnos + 1)\n        ]\n\n        single_mno_dfs = [self.filter_dataframe(df) for df in single_mno_dfs]\n\n        # Filter out obfuscated records, if they exist. This is a first-version operation where we don't deal with\n        # obfuscated values and just ignore them.\n        filtered_dfs = self.filter_out_obfuscated_records(single_mno_dfs, self.target_column)\n\n        # Aggregate values across MNOs\n        aggregated_df = self.aggregate_single_mno_indicators(filtered_dfs)\n\n        self.output_data_objects[CLASS_MAPPING[self.current_component_id][\"constructor\"].ID].df = aggregated_df\n\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        if len(self.data_objects) == 0:\n            self.logger.info(\"No execution requested in config file -- finishing without performing any operation...\")\n            self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n            return\n\n        for component_id in self.data_objects:\n            self.logger.info(f\"Working on {self.COMPONENT_ID}.{component_id}...\")\n            self.current_component_id = component_id\n            self.target_column = CLASS_MAPPING[self.current_component_id][\"target_column\"]\n\n            # Number of MNOs for which we will aggregate data\n            self.number_of_single_mnos = self.config.getint(\n                f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"number_of_single_mnos\"\n            )\n\n            # There must be 2 or more MNOs\n            if self.number_of_single_mnos &lt;= 1:\n                raise ValueError(\n                    f\"Number of single MNOs to aggregate must be 2 or greater -- got {self.number_of_single_mnos}\"\n                )\n\n            self.single_mno_factors = [\n                self.config.getfloat(f\"{self.COMPONENT_ID}.{self.current_component_id}\", f\"single_mno_{i}_factor\")\n                for i in range(1, self.number_of_single_mnos + 1)\n            ]\n\n            # Sum of weights must add up to 1.\n            if sum(self.single_mno_factors) != 1:\n                raise ValueError(\n                    f\"Single MNO factors {self.single_mno_factors} add up to {sum(self.single_mno_factors)} -- must add up to 1\"\n                )\n\n            self.input_data_objects = {\n                f\"{input_do.ID}_{i+1}\": input_do\n                for i, input_do in enumerate(self.data_objects[self.current_component_id][\"input\"])\n            }\n\n            self.output_data_objects = {\n                self.data_objects[component_id][\"output\"].ID: self.data_objects[self.current_component_id][\"output\"]\n            }\n\n            self.read()\n            self.transform()\n            self.write()\n\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n</code></pre>"},{"location":"reference/components/execution/multimno_aggregation/multimno_aggregation/#components.execution.multimno_aggregation.multimno_aggregation.MultiMNOAggregation.aggregate_single_mno_indicators","title":"<code>aggregate_single_mno_indicators(dfs)</code>","text":"<p>Aggregate the indicator over all single MNOs' data. First, the value of each MNO is multiplied by some weight, and then these values are added up, such that the sum of these weights add up to 1.</p> <p>Parameters:</p> Name Type Description Default <code>dfs</code> <code>list[DataFrame]</code> <p>list of single MNO dataframes</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>final dataframe with the weighted sum indicator of all MNOs</p> Source code in <code>multimno/components/execution/multimno_aggregation/multimno_aggregation.py</code> <pre><code>def aggregate_single_mno_indicators(self, dfs: list[DataFrame]) -&gt; DataFrame:\n    \"\"\"Aggregate the indicator over all single MNOs' data. First, the value of each MNO is multiplied by some\n    weight, and then these values are added up, such that the sum of these weights add up to 1.\n\n    Args:\n        dfs (list[DataFrame]): list of single MNO dataframes\n\n    Returns:\n        DataFrame: final dataframe with the weighted sum indicator of all MNOs\n    \"\"\"\n    weighted_dfs = [\n        df.withColumn(self.target_column, F.col(self.target_column) * F.lit(factor))\n        for (df, factor) in zip(dfs, self.single_mno_factors)\n    ]\n\n    groupby_cols = [\n        column\n        for column in CLASS_MAPPING[self.current_component_id][\"constructor\"].SCHEMA.names\n        if column != self.target_column\n    ]\n\n    agg_df = (\n        reduce(DataFrame.union, weighted_dfs)\n        .groupBy(*groupby_cols)\n        .agg(F.sum(self.target_column).alias(self.target_column))\n    )\n\n    agg_df = apply_schema_casting(agg_df, CLASS_MAPPING[self.current_component_id][\"constructor\"].SCHEMA)\n\n    return agg_df\n</code></pre>"},{"location":"reference/components/execution/multimno_aggregation/multimno_aggregation/#components.execution.multimno_aggregation.multimno_aggregation.MultiMNOAggregation.filter_dataframe","title":"<code>filter_dataframe(df)</code>","text":"<p>Filtering function that takes the partitions of the dataframe specified via configuration file</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>original DataFrame</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>season</code> value in configuration file is not one of allowed values</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>filtered DataFrame</p> Source code in <code>multimno/components/execution/multimno_aggregation/multimno_aggregation.py</code> <pre><code>def filter_dataframe(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Filtering function that takes the partitions of the dataframe specified via configuration file\n\n    Args:\n        df (DataFrame): original DataFrame\n\n    Raises:\n        ValueError: if `season` value in configuration file is not one of allowed values\n\n    Returns:\n        DataFrame: filtered DataFrame\n    \"\"\"\n    # TODO: move config reading and validation to __init__ (?)\n    if self.current_component_id == PresentPopulationEstimation.COMPONENT_ID:\n        zoning_dataset = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"zoning_dataset_id\")\n        levels = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"hierarchical_levels\")\n        levels = list(int(x.strip()) for x in levels.split(\",\"))\n\n        start_date = dt.datetime.strptime(\n            self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"start_date\"), \"%Y-%m-%d\"\n        )\n        end_date = dt.datetime.strptime(\n            self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"end_date\"), \"%Y-%m-%d\"\n        )\n\n        df = (\n            df.where(F.col(ColNames.dataset_id) == F.lit(zoning_dataset))\n            .where(F.col(ColNames.level).isin(levels))\n            .where(F.make_date(ColNames.year, ColNames.month, ColNames.day).between(start_date, end_date))\n        )\n\n    if self.current_component_id == UsualEnvironmentAggregation.COMPONENT_ID:\n        zoning_dataset = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"zoning_dataset_id\")\n        levels = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"hierarchical_levels\")\n        levels = list(int(x.strip()) for x in levels.split(\",\"))\n        labels = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"labels\")\n        labels = list(x.strip() for x in labels.split(\",\"))\n\n        start_date = dt.datetime.strptime(\n            self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"start_month\"), \"%Y-%m\"\n        )\n        end_date = dt.datetime.strptime(\n            self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"end_month\"), \"%Y-%m\"\n        )\n        end_date = end_date + dt.timedelta(days=cal.monthrange(end_date.year, end_date.month)[1] - 1)\n        season = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"season\")\n        if season not in SEASONS:\n            raise ValueError(f\"Unknown season {season} -- valid values are {SEASONS}\")\n\n        df = (\n            df.where(F.col(ColNames.dataset_id) == zoning_dataset)\n            .where(F.col(ColNames.level).isin(levels))\n            .where(F.col(ColNames.label).isin(labels))\n            .where(F.col(ColNames.start_date) == start_date)\n            .where(F.col(ColNames.end_date) == end_date)\n            .where(F.col(ColNames.season) == season)\n        )\n\n    if self.current_component_id == InternalMigration.COMPONENT_ID:\n        zoning_dataset = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"zoning_dataset_id\")\n        levels = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"hierarchical_levels\")\n        levels = list(int(x.strip()) for x in levels.split(\",\"))\n\n        start_date_prev = dt.datetime.strptime(\n            self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"start_month_previous\"), \"%Y-%m\"\n        )\n        end_date_prev = dt.datetime.strptime(\n            self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"end_month_previous\"), \"%Y-%m\"\n        )\n        end_date_prev = end_date_prev + dt.timedelta(\n            days=cal.monthrange(end_date_prev.year, end_date_prev.month)[1] - 1\n        )\n        season_prev = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"season_previous\")\n        if season_prev not in SEASONS:\n            raise ValueError(f\"Unknown season {season_prev} -- valid values are {SEASONS}\")\n\n        start_date_new = dt.datetime.strptime(\n            self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"start_month_new\"), \"%Y-%m\"\n        )\n        end_date_new = dt.datetime.strptime(\n            self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"end_month_new\"), \"%Y-%m\"\n        )\n        end_date_new = end_date_new + dt.timedelta(\n            days=cal.monthrange(end_date_new.year, end_date_new.month)[1] - 1\n        )\n        season_new = self.config.get(f\"{self.COMPONENT_ID}.{self.current_component_id}\", \"season_new\")\n        if season_new not in SEASONS:\n            raise ValueError(f\"Unknown season {season_new} -- valid values are {SEASONS}\")\n\n        df = (\n            df.where(F.col(ColNames.dataset_id) == zoning_dataset)\n            .where(F.col(ColNames.level).isin(levels))\n            .where(F.col(ColNames.start_date_previous) == start_date_prev)\n            .where(F.col(ColNames.end_date_previous) == end_date_prev)\n            .where(F.col(ColNames.season_previous) == season_prev)\n            .where(F.col(ColNames.start_date_new) == start_date_new)\n            .where(F.col(ColNames.end_date_new) == end_date_new)\n            .where(F.col(ColNames.season_new) == season_new)\n        )\n    return df\n    return df\n</code></pre>"},{"location":"reference/components/execution/multimno_aggregation/multimno_aggregation/#components.execution.multimno_aggregation.multimno_aggregation.MultiMNOAggregation.filter_out_obfuscated_records","title":"<code>filter_out_obfuscated_records(dfs, target_column)</code>  <code>staticmethod</code>","text":"<p>Filter out obfuscated records, which are records flagged by the k-anonimity process to have values lower than k, for some value k. Specific treatment for these obfuscated values is not yet defined, so we simply remove them in this version of the code.</p> <p>Parameters:</p> Name Type Description Default <code>dfs</code> <code>list[DataFrame]</code> <p>list of single MNO dataframes with possibly obfuscated values</p> required <code>target_column</code> <code>str</code> <p>name of the target column containig the value of the indicator</p> required <p>Returns:</p> Type Description <code>list[DataFrame]</code> <p>list[DataFrame]: list of single MNO dataframes without obfuscated values</p> Source code in <code>multimno/components/execution/multimno_aggregation/multimno_aggregation.py</code> <pre><code>@staticmethod\ndef filter_out_obfuscated_records(dfs: list[DataFrame], target_column: str) -&gt; list[DataFrame]:\n    \"\"\"Filter out obfuscated records, which are records flagged by the k-anonimity process to have values lower\n    than k, for some value k. Specific treatment for these obfuscated values is not yet defined, so we simply\n    remove them in this version of the code.\n\n    Args:\n        dfs (list[DataFrame]): list of single MNO dataframes with possibly obfuscated values\n        target_column (str): name of the target column containig the value of the indicator\n\n    Returns:\n        list[DataFrame]: list of single MNO dataframes without obfuscated values\n    \"\"\"\n    output_dfs = [df.where(F.col(target_column) &gt; F.lit(0)) for df in dfs]\n    return output_dfs\n</code></pre>"},{"location":"reference/components/execution/network_cleaning/","title":"network_cleaning","text":""},{"location":"reference/components/execution/network_cleaning/network_cleaning/","title":"network_cleaning","text":"<p>Module that cleans raw MNO Network Topology data.</p>"},{"location":"reference/components/execution/network_cleaning/network_cleaning/#components.execution.network_cleaning.network_cleaning.NetworkCleaning","title":"<code>NetworkCleaning</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class that cleans MNO Network Topology Data (based on physical properties of the cell)</p> Source code in <code>multimno/components/execution/network_cleaning/network_cleaning.py</code> <pre><code>class NetworkCleaning(Component):\n    \"\"\"\n    Class that cleans MNO Network Topology Data (based on physical properties of the cell)\n    \"\"\"\n\n    COMPONENT_ID = \"NetworkCleaning\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n        self.latitude_max = self.config.getfloat(self.COMPONENT_ID, \"latitude_max\")\n        self.latitude_min = self.config.getfloat(self.COMPONENT_ID, \"latitude_min\")\n        self.longitude_max = self.config.getfloat(self.COMPONENT_ID, \"longitude_max\")\n        self.longitude_min = self.config.getfloat(self.COMPONENT_ID, \"longitude_min\")\n\n        # list of possible technologies\n        self.tech = self.config.get(self.COMPONENT_ID, \"technology_options\").strip().replace(\" \", \"\").split(\",\")\n\n        # list of possible cell types\n        self.cell_type_options = (\n            self.config.get(self.COMPONENT_ID, \"cell_type_options\").strip().replace(\" \", \"\").split(\",\")\n        )\n\n        self.data_period_start = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n        self.data_period_end = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n\n        # List of datetime.dates for which to perform cleaning of the raw network input data.\n        # Notice that date_period_end is included\n        self.data_period_dates = [\n            self.data_period_start + datetime.timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n        # Timestamp format that the function pyspark.sql.functions.to_timestamp expects.\n        # Format must follow guidelines in https://spark.apache.org/docs/3.4.2/sql-ref-datetime-pattern.html\n        self.valid_date_timestamp_format = self.config.get(self.COMPONENT_ID, \"valid_date_timestamp_format\")\n\n        self.frequent_error_criterion = self.config.get(self.COMPONENT_ID, \"frequent_error_criterion\")\n        if self.frequent_error_criterion not in (\"absolute\", \"percentage\"):\n            raise ValueError(\n                \"unexpected value in frequent_error_criterion: expected `absolute` or `percentage`, got\",\n                self.frequent_error_criterion,\n            )\n\n        if self.frequent_error_criterion == \"absolute\":\n            self.top_k_errors = self.config.getint(self.COMPONENT_ID, \"top_k_errors\")\n        else:  # percentage\n            self.top_k_errors = self.config.getfloat(self.COMPONENT_ID, \"top_k_errors\")\n\n        self.timestamp = datetime.datetime.now()\n        self.current_date: datetime.date = None\n        self.cells_df: DataFrame = None\n        self.accdf: DataFrame = None\n\n    def initalize_data_objects(self):\n        input_bronze_network_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"network_data_bronze\")\n        output_silver_network_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"network_data_silver\")\n        output_silver_network_syntactic_quality_metrics_by_column = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"network_syntactic_quality_metrics_by_column\"\n        )\n        output_silver_network_top_errors_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"network_top_frequent_errors\")\n        output_silver_network_row_error_metrics_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"network_row_error_metrics\"\n        )\n\n        bronze_network = BronzeNetworkDataObject(self.spark, input_bronze_network_path)\n        silver_network = SilverNetworkDataObject(self.spark, output_silver_network_path)\n\n        silver_network_quality_metrics_by_column = SilverNetworkDataQualityMetricsByColumn(\n            self.spark,\n            output_silver_network_syntactic_quality_metrics_by_column,\n        )\n\n        silver_network_row_error_metrics = SilverNetworkRowErrorMetrics(\n            self.spark,\n            output_silver_network_row_error_metrics_path,\n        )\n\n        silver_network_top_errors = SilverNetworkDataTopFrequentErrors(\n            self.spark,\n            output_silver_network_top_errors_path,\n        )\n\n        self.input_data_objects = {bronze_network.ID: bronze_network}\n        self.output_data_objects = {\n            silver_network.ID: silver_network,\n            silver_network_quality_metrics_by_column.ID: silver_network_quality_metrics_by_column,\n            silver_network_top_errors.ID: silver_network_top_errors,\n            silver_network_row_error_metrics.ID: silver_network_row_error_metrics,\n        }\n\n    def transform(self):\n        # Raw/Bronze Network Topology DF\n        self.cells_df = self.input_data_objects[BronzeNetworkDataObject.ID].df\n\n        # Read only desired dates, specified via config\n        self.cells_df = self.cells_df.filter(\n            F.make_date(ColNames.year, ColNames.month, ColNames.day) == F.lit(self.current_date)\n        )\n\n        # List that will contain all the columns created to keep track of every kind of error\n        auxiliar_columns = []\n\n        # Columns for which we will check for null values\n        # Notice that currently, valid_date_end can have null values by definition as long as for the current date the tower is\n        # still operational. Thus, it is not taken into account for the deletion of rows/records\n        check_for_null_columns = [\n            ColNames.cell_id,\n            ColNames.valid_date_start,\n            ColNames.valid_date_end,  # should not be counted for discarding rows!!\n            ColNames.latitude,\n            ColNames.longitude,\n            ColNames.altitude,\n            ColNames.antenna_height,\n            ColNames.directionality,\n            ColNames.elevation_angle,\n            ColNames.horizontal_beam_width,\n            ColNames.vertical_beam_width,\n            ColNames.power,\n            ColNames.range,\n            ColNames.frequency,\n            ColNames.technology,\n            ColNames.cell_type,\n        ]\n\n        # Add auxiliar columns to track instances where a row has a null value\n        # Note that currently valid_date_end has a permited null value, as well as\n        # azimith_angle when directionality is 0\n        self.cells_df = self.cells_df.withColumns(\n            {f\"{col}_{NetworkErrorType.NULL_VALUE}\": F.col(col).isNull() for col in check_for_null_columns}\n        ).withColumn(\n            f\"{ColNames.azimuth_angle}_{NetworkErrorType.NULL_VALUE}\",\n            F.when(F.col(ColNames.directionality) == F.lit(1), F.col(ColNames.azimuth_angle).isNull()).otherwise(\n                F.lit(False)\n            ),\n        )\n\n        auxiliar_columns.extend([f\"{col}_{NetworkErrorType.NULL_VALUE}\" for col in check_for_null_columns])\n        auxiliar_columns.append(f\"{ColNames.azimuth_angle}_{NetworkErrorType.NULL_VALUE}\")\n\n        # Now, we try to parse the valid_date_start and valid_date_end columns, from a string to a timestamp\n        self.cells_df = (\n            self.cells_df.withColumn(\n                f\"{ColNames.valid_date_start}_parsed\",\n                F.to_timestamp(F.col(ColNames.valid_date_start), self.valid_date_timestamp_format),\n            )\n            .withColumn(\n                f\"{ColNames.valid_date_end}_parsed\",\n                F.to_timestamp(F.col(ColNames.valid_date_end), self.valid_date_timestamp_format),\n            )\n            # Check when parsing failed, excluding the cases where the field was null to begi with\n            .withColumn(\n                f\"{ColNames.valid_date_start}_{NetworkErrorType.CANNOT_PARSE}\",\n                F.when(\n                    F.col(ColNames.valid_date_start).isNotNull()\n                    &amp; F.col(f\"{ColNames.valid_date_start}_parsed\").isNull(),\n                    F.lit(True),\n                ).otherwise(F.lit(False)),\n            )\n            .withColumn(\n                f\"{ColNames.valid_date_end}_{NetworkErrorType.CANNOT_PARSE}\",\n                F.when(\n                    F.col(ColNames.valid_date_end).isNotNull() &amp; F.col(f\"{ColNames.valid_date_end}_parsed\").isNull(),\n                    F.lit(True),\n                ).otherwise(F.lit(False)),\n            )\n        )\n\n        auxiliar_columns.extend(\n            [\n                f\"{ColNames.valid_date_start}_{NetworkErrorType.CANNOT_PARSE}\",\n                f\"{ColNames.valid_date_end}_{NetworkErrorType.CANNOT_PARSE}\",\n            ]\n        )\n\n        # Now, we check for incoherent dates (valid_date_end is earlier in time than valid_date_start)\n        self.cells_df = self.cells_df.withColumn(\n            f\"dates_{NetworkErrorType.OUT_OF_RANGE}\",\n            F.when(\n                F.col(f\"{ColNames.valid_date_start}_parsed\").isNotNull()\n                &amp; F.col(f\"{ColNames.valid_date_end}_parsed\").isNotNull(),\n                F.col(f\"{ColNames.valid_date_start}_parsed\") &gt; F.col(f\"{ColNames.valid_date_end}_parsed\"),\n            ).otherwise(F.lit(False)),\n        )\n\n        auxiliar_columns.append(f\"dates_{NetworkErrorType.OUT_OF_RANGE}\")\n\n        # Now we check for invalid values that are outside of the range defined for the data object.\n\n        # TODO: correct check for CGI in cell ids\n        do_cgi_check = self.config.getboolean(self.COMPONENT_ID, \"do_cell_cgi_check\", fallback=False)\n        if do_cgi_check:\n            cgi_condition = (F.length(F.col(ColNames.cell_id)) != F.lit(14)) &amp; (\n                F.length(F.col(ColNames.cell_id)) != F.lit(15)\n            )\n        else:\n            cgi_condition = F.lit(False)\n        self.cells_df = self.cells_df.withColumn(\n            f\"{ColNames.cell_id}_{NetworkErrorType.OUT_OF_RANGE}\",\n            cgi_condition,\n        )\n\n        # TODO: cover case where bounding box crosses the -180/180 longitude\n        self.cells_df = (\n            self.cells_df.withColumn(\n                f\"{ColNames.latitude}_{NetworkErrorType.OUT_OF_RANGE}\",\n                (F.col(ColNames.latitude) &gt; F.lit(self.latitude_max))\n                | (F.lit(ColNames.latitude) &lt; F.lit(self.latitude_min)),\n            )\n            .withColumn(\n                f\"{ColNames.longitude}_{NetworkErrorType.OUT_OF_RANGE}\",\n                (F.col(ColNames.longitude) &gt; F.lit(self.longitude_max))\n                | (F.lit(ColNames.longitude) &lt; F.lit(self.longitude_min)),\n            )\n            # altitude: must only be float, no checks\n            # antenna height: must be positive\n            .withColumn(\n                f\"{ColNames.antenna_height}_{NetworkErrorType.OUT_OF_RANGE}\",\n                F.col(ColNames.antenna_height) &lt;= F.lit(0),\n            )\n            # directionality: 0 or 1\n            .withColumn(\n                f\"{ColNames.directionality}_{NetworkErrorType.OUT_OF_RANGE}\",\n                (F.col(ColNames.directionality) != F.lit(0)) &amp; (F.col(ColNames.directionality) != F.lit(1)),\n            )\n            .withColumn(\n                f\"{ColNames.azimuth_angle}_{NetworkErrorType.OUT_OF_RANGE}\",\n                F.when(\n                    (F.col(ColNames.directionality) == F.lit(1)),  # &amp; F.col(ColNames.azimuth_angle).isNotNull(),\n                    (F.col(ColNames.azimuth_angle) &lt; F.lit(0)) | (F.col(ColNames.azimuth_angle) &gt; F.lit(360)),\n                ).otherwise(F.lit(False)),\n            )\n            .withColumn(\n                f\"{ColNames.elevation_angle}_{NetworkErrorType.OUT_OF_RANGE}\",\n                (F.col(ColNames.elevation_angle) &lt; F.lit(-90)) | (F.col(ColNames.elevation_angle) &gt; F.lit(90)),\n            )\n            .withColumn(\n                f\"{ColNames.horizontal_beam_width}_{NetworkErrorType.OUT_OF_RANGE}\",\n                (F.col(ColNames.horizontal_beam_width) &lt; F.lit(0))\n                | (F.col(ColNames.horizontal_beam_width) &gt; F.lit(360)),\n            )\n            .withColumn(\n                f\"{ColNames.vertical_beam_width}_{NetworkErrorType.OUT_OF_RANGE}\",\n                (F.col(ColNames.vertical_beam_width) &lt; F.lit(0)) | (F.col(ColNames.vertical_beam_width) &gt; F.lit(360)),\n            )\n            .withColumn(\n                f\"{ColNames.power}_{NetworkErrorType.OUT_OF_RANGE}\",\n                F.col(ColNames.power) &lt; F.lit(0),\n            )\n            .withColumn(\n                f\"{ColNames.range}_{NetworkErrorType.OUT_OF_RANGE}\",\n                F.col(ColNames.range) &lt; F.lit(0),\n            )\n            .withColumn(\n                f\"{ColNames.frequency}_{NetworkErrorType.OUT_OF_RANGE}\",\n                F.col(ColNames.frequency) &lt; F.lit(0),\n            )\n            .withColumn(\n                f\"{ColNames.technology}_{NetworkErrorType.OUT_OF_RANGE}\",\n                ~F.col(ColNames.technology).isin(self.tech),\n            )\n            .withColumn(\n                f\"{ColNames.cell_type}_{NetworkErrorType.OUT_OF_RANGE}\",\n                ~F.col(ColNames.cell_type).isin(self.cell_type_options),\n            )\n        )\n\n        # Null values will appear for the above checks when the raw data was null. Thus, for these columns\n        # we change null for False by using the .fillna() method\n        self.cells_df = self.cells_df.fillna(\n            False,\n            [\n                f\"{ColNames.cell_id}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.latitude}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.longitude}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.antenna_height}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.directionality}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.azimuth_angle}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.elevation_angle}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.horizontal_beam_width}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.vertical_beam_width}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.power}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.range}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.frequency}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.technology}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.cell_type}_{NetworkErrorType.OUT_OF_RANGE}\",\n            ],\n        )\n\n        auxiliar_columns.extend(\n            [\n                f\"{ColNames.cell_id}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.latitude}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.longitude}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.antenna_height}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.directionality}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.azimuth_angle}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.elevation_angle}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.horizontal_beam_width}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.vertical_beam_width}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.power}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.range}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.frequency}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.technology}_{NetworkErrorType.OUT_OF_RANGE}\",\n                f\"{ColNames.cell_type}_{NetworkErrorType.OUT_OF_RANGE}\",\n            ]\n        )\n\n        # Auxiliar dict, relating the DO columns with its auxiliar columns\n        column_groups = dict()\n        for col in self.output_data_objects[\"SilverNetworkDO\"].SCHEMA.names:\n            # for each auxiliar column\n            column_groups[col] = []\n            for cc in auxiliar_columns:\n                # if it is related to the DO's column:\n                if col == \"_\".join(cc.split(\"_\")[:-1]):\n                    # Ignore nulls for valid date end\n                    if cc == f\"{ColNames.valid_date_end}_{NetworkErrorType.NULL_VALUE}\":\n                        continue\n                    column_groups[col].append(F.col(cc))\n\n        # For each column, create an abstract conditional whenever a value of that column has ANY type of error.\n        # Example: latitude can have two types of error: a) being null, or b) being out of range.\n        # The coniditonal for this column is then&gt; (isNull(latitude) OR isOutOfRange(latitude))\n        column_conditions = {\n            col: reduce(lambda a, b: a | b, column_groups[col]) for col in column_groups if len(column_groups[col]) &gt; 0\n        }\n\n        # Negate the conditionals above to get those records without ANY type of error\n        field_without_errors = {col: ~column_conditions[col] for col in column_conditions}\n\n        auxiliar_columns.extend([f\"{col}_{NetworkErrorType.NO_ERROR}\" for col in field_without_errors])\n\n        # Abstract conditional ,indicating those records that do not have any type of error in any\n        # mandatory column, i.e. all accepted values.\n        mandatory_columns = [\n            ColNames.cell_id,\n            ColNames.latitude,\n            ColNames.longitude,\n            ColNames.directionality,\n            ColNames.azimuth_angle,\n        ]\n\n        # Rows to be preserved are those without errors in their mandatory fields\n        preserve_row = reduce(lambda a, b: a &amp; b, [field_without_errors[col] for col in mandatory_columns])\n\n        # Rows to be deleted\n\n        # Rows with any type of error\n        any_error_row = reduce(lambda a, b: a | b, column_conditions.values())\n\n        self.cells_df = self.cells_df.withColumns(\n            {f\"{col}_{NetworkErrorType.NO_ERROR}\": field_without_errors[col] for col in field_without_errors}\n        )\n\n        self.cells_df = self.cells_df.withColumn(\"to_preserve\", preserve_row)\n\n        self.cells_df.cache()\n\n        rows_to_be_deleted = (\n            self.cells_df.select((~preserve_row).cast(ByteType()).alias(\"to_be_deleted\")).withColumn(\n                \"to_be_deleted\", F.sum(\"to_be_deleted\")\n            )\n        ).collect()[0][\"to_be_deleted\"]\n\n        rows_with_any_error = (\n            self.cells_df.select((any_error_row).cast(ByteType()).alias(\"row_with_some_error\")).withColumn(\n                \"row_with_some_error\", F.sum(\"row_with_some_error\")\n            )\n        ).collect()[0][\"row_with_some_error\"]\n\n        row_error_metrics = []\n        row_error_metrics.append(\n            Row(\n                **{\n                    ColNames.result_timestamp: self.timestamp,\n                    ColNames.variable: \"rows_deleted\",\n                    ColNames.value: rows_to_be_deleted,\n                    ColNames.year: self.current_date.year,\n                    ColNames.month: self.current_date.month,\n                    ColNames.day: self.current_date.day,\n                }\n            )\n        )\n\n        row_error_metrics.append(\n            Row(\n                **{\n                    ColNames.result_timestamp: self.timestamp,\n                    ColNames.variable: \"rows_with_some_error\",\n                    ColNames.value: rows_with_any_error,\n                    ColNames.year: self.current_date.year,\n                    ColNames.month: self.current_date.month,\n                    ColNames.day: self.current_date.day,\n                }\n            )\n        )\n\n        row_error_df = self.spark.createDataFrame(row_error_metrics, schema=SilverNetworkRowErrorMetrics.SCHEMA)\n\n        self.output_data_objects[SilverNetworkRowErrorMetrics.ID].df = row_error_df\n\n        # Collect the number of True values in each auxiliar column, that counts the number of each error type, or\n        # any error, in each of the columns of the data object.\n        # TODO: possible improvement if pyspark.sql.GroupedData.pivot can be used instead.\n        metrics = (\n            self.cells_df.withColumns({col: F.col(col).cast(IntegerType()) for col in auxiliar_columns})\n            .groupBy([F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)])\n            .agg({col: \"sum\" for col in auxiliar_columns})\n            .withColumnsRenamed({f\"sum({col})\": col for col in auxiliar_columns})\n            .collect()\n        )\n\n        # Extract the collected values and reformat them into the shape of the metrics DO dataframe.\n        metrics_long_format = []\n\n        for row in metrics:\n            row = row.asDict()\n            year = row[ColNames.year]\n            month = row[ColNames.month]\n            day = row[ColNames.day]\n            date = datetime.date(year=year, month=month, day=day)\n            row_dict = dict()\n\n            for col in row:\n                if col in [ColNames.year, ColNames.month, ColNames.day]:\n                    continue\n\n                row_dict = {\n                    ColNames.field_name: \"_\".join(col.split(\"_\")[:-1]),\n                    ColNames.type_code: int(col.split(\"_\")[-1]),\n                    ColNames.value: row[col],\n                    ColNames.date: date,\n                    ColNames.year: year,\n                    ColNames.month: month,\n                    ColNames.day: day,\n                }\n\n                metrics_long_format.append(Row(**row_dict))\n\n        # Initial records (before cleaning)\n        initial_records = (\n            self.cells_df.groupBy([F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)]).count().collect()\n        )\n\n        for row in initial_records:\n            row = row.asDict()\n            year = row[ColNames.year]\n            month = row[ColNames.month]\n            day = row[ColNames.day]\n            date = datetime.date(year=year, month=month, day=day)\n\n            row_dict = {\n                ColNames.field_name: None,\n                ColNames.type_code: NetworkErrorType.INITIAL_ROWS,\n                ColNames.value: row[\"count\"],\n                ColNames.date: date,\n                ColNames.year: year,\n                ColNames.month: month,\n                ColNames.day: day,\n            }\n\n            metrics_long_format.append(Row(**row_dict))\n\n        # Final records (after cleaning)\n        final_records = (\n            self.cells_df.withColumn(\"to_preserve\", F.col(\"to_preserve\").cast(IntegerType()))\n            .groupBy([F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)])\n            .sum(\"to_preserve\")\n            .withColumnRenamed(\"sum(to_preserve)\", \"count\")\n            .collect()\n        )\n        for row in final_records:\n            row = row.asDict()\n            year = row[ColNames.year]\n            month = row[ColNames.month]\n            day = row[ColNames.day]\n            date = datetime.date(year=year, month=month, day=day)\n\n            row_dict = {\n                ColNames.field_name: None,\n                ColNames.type_code: NetworkErrorType.FINAL_ROWS,\n                ColNames.value: row[\"count\"],\n                ColNames.date: date,\n                ColNames.year: year,\n                ColNames.month: month,\n                ColNames.day: day,\n            }\n\n            metrics_long_format.append(Row(**row_dict))\n\n        # Final result\n        metrics_df = self.spark.createDataFrame(\n            metrics_long_format,\n            schema=StructType(\n                [\n                    StructField(ColNames.field_name, StringType(), nullable=True),\n                    StructField(ColNames.type_code, IntegerType(), nullable=False),\n                    StructField(ColNames.value, IntegerType(), nullable=False),\n                    StructField(ColNames.date, DateType(), nullable=False),\n                    StructField(ColNames.year, ShortType(), nullable=False),\n                    StructField(ColNames.month, ByteType(), nullable=False),\n                    StructField(ColNames.day, ByteType(), nullable=False),\n                ]\n            ),\n        )\n        metrics_df = metrics_df.withColumn(ColNames.result_timestamp, F.lit(self.timestamp).cast(TimestampType()))\n\n        self.output_data_objects[SilverNetworkDataQualityMetricsByColumn.ID].df = metrics_df\n\n        silver_cells_df = self.cells_df.filter(F.col(\"to_preserve\"))\n        # Prepare the clean, silver network data by imputing null values in invalid optional fields\n        for auxcol in auxiliar_columns:\n            # do not impute fields without error or that are already null\n            if int(auxcol.split(\"_\")[-1]) not in (NetworkErrorType.NO_ERROR, NetworkErrorType.NULL_VALUE):\n                split_col = auxcol.split(\"_\")\n                variable = \"_\".join(split_col[:-1])\n\n                if variable == \"dates\":\n                    silver_cells_df = silver_cells_df.withColumn(\n                        ColNames.valid_date_start,\n                        F.when(F.col(auxcol), F.lit(None)).otherwise(F.col(ColNames.valid_date_start)),\n                    ).withColumn(\n                        ColNames.valid_date_end,\n                        F.when(F.col(auxcol), F.lit(None)).otherwise(F.col(ColNames.valid_date_end)),\n                    )\n                    continue\n\n                silver_cells_df = silver_cells_df.withColumn(\n                    variable, F.when(F.col(auxcol), F.lit(None)).otherwise(F.col(variable))\n                )\n\n        silver_cells_df = (\n            silver_cells_df.withColumn(ColNames.valid_date_start, F.col(f\"{ColNames.valid_date_start}_parsed\"))\n            .withColumn(ColNames.valid_date_end, F.col(f\"{ColNames.valid_date_end}_parsed\"))\n            .select(SilverNetworkDataObject.SCHEMA.names)\n        )\n\n        self.output_data_objects[SilverNetworkDataObject.ID].df = silver_cells_df\n\n        # Top Frequent Error Metrics\n        error_counts_df = []\n        for field_name, cols in column_groups.items():\n            if len(cols) == 0:\n                continue\n\n            # Get name of the column and its error code\n            col_name = cols[0]._jc.toString()\n            type_code_column = F.when(F.col(col_name), F.lit(int(col_name.split(\"_\")[-1])))\n\n            # if len(cols) == 1, the loop is not entered\n            for i in range(1, len(cols)):\n                col_name = cols[i]._jc.toString()\n                type_code_column = type_code_column.when(F.col(col_name), F.lit(int(col_name.split(\"_\")[-1])))\n            type_code_column = type_code_column.otherwise(None)\n\n            error_counts_df.append(\n                self.cells_df.filter(~field_without_errors[field_name])  # rows with errors in this field\n                .select(field_name, *cols)  # select field and its aux columns\n                .withColumn(ColNames.type_code, type_code_column)  # new column with error code\n                .groupBy(field_name, ColNames.type_code)\n                .count()  # count frequency of each particular error value\n                .withColumnsRenamed(\n                    {\n                        \"count\": ColNames.error_count,\n                        field_name: ColNames.error_value,\n                    }\n                )\n                .withColumn(ColNames.error_count, F.col(ColNames.error_count).cast(IntegerType()))\n                .withColumn(  # cast values as strings\n                    ColNames.error_value, F.col(ColNames.error_value).cast(StringType())\n                )\n                .withColumn(ColNames.field_name, F.lit(field_name))\n            )\n\n        # Join all error count dataframes\n        errors_df = reduce(lambda x, y: DataFrame.union(x, y), error_counts_df)\n\n        errors_df.cache()\n\n        total_errors = errors_df.select(F.sum(ColNames.error_count).alias(ColNames.error_count)).collect()[0][\n            ColNames.error_count\n        ]\n\n        if total_errors is None:\n            self.output_data_objects[SilverNetworkDataTopFrequentErrors.ID].df = self.spark.createDataFrame(\n                [], schema=SilverNetworkDataTopFrequentErrors.SCHEMA\n            )\n            return\n\n        window = (\n            Window()\n            .orderBy(F.col(ColNames.error_count).desc())\n            .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n        )\n\n        self.accdf = errors_df.withColumn(\n            ColNames.accumulated_percentage,\n            (F.lit(100 / total_errors) * F.sum(F.col(ColNames.error_count)).over(window)).cast(FloatType()),\n        ).withColumn(\"id\", F.row_number().over(window))\n\n        if self.frequent_error_criterion == \"absolute\":\n            self.accdf = self.accdf.filter(F.col(\"id\") &lt;= F.lit(self.top_k_errors)).drop(\"id\")\n        else:  # percentage\n            self.accdf.cache()\n            prev_id = (\n                self.accdf.filter(F.col(ColNames.accumulated_percentage) &lt;= F.lit(self.top_k_errors)).select(\n                    F.max(\"id\")\n                )\n            ).collect()[0][\"max(id)\"]\n\n            if prev_id is None:\n                prev_id = 0\n\n            self.accdf = self.accdf.filter(F.col(\"id\") &lt;= F.lit(prev_id + 1)).drop(\"id\")\n\n        self.accdf = (\n            self.accdf.withColumn(ColNames.result_timestamp, F.lit(self.timestamp).cast(TimestampType()))\n            .withColumn(ColNames.year, F.lit(self.current_date.year).cast(ShortType()))\n            .withColumn(ColNames.month, F.lit(self.current_date.month).cast(ByteType()))\n            .withColumn(ColNames.day, F.lit(self.current_date.day).cast(ByteType()))\n            .select(SilverNetworkDataTopFrequentErrors.SCHEMA.fieldNames())\n        )\n\n        self.output_data_objects[SilverNetworkDataTopFrequentErrors.ID].df = self.accdf\n\n    @get_execution_stats\n    def execute(self):\n        self.read()\n        for date in self.data_period_dates:\n            self.logger.info(f\"Processing {date}...\")\n            self.current_date = date\n            self.accdf = None\n            self.transform()\n            self.write()\n            self.cells_df.unpersist()\n            if self.accdf is not None:\n                self.accdf.unpersist()\n            else:\n                self.logger.info(f\"No errors found for {date} -- no error frequency metrics generated\")\n            self.logger.info(f\"... {date} finished\")\n\n    def write(self):\n        \"\"\"\n        Method that performs the write operation of the output data objects.\n        \"\"\"\n        for data_object in self.output_data_objects.values():\n            self.logger.info(f\"Writing {data_object.ID}...\")\n            data_object.write()\n            self.logger.info(\"... finished\")\n\n        for data_object in self.output_data_objects.values():\n            data_object.df.unpersist()\n</code></pre>"},{"location":"reference/components/execution/network_cleaning/network_cleaning/#components.execution.network_cleaning.network_cleaning.NetworkCleaning.write","title":"<code>write()</code>","text":"<p>Method that performs the write operation of the output data objects.</p> Source code in <code>multimno/components/execution/network_cleaning/network_cleaning.py</code> <pre><code>def write(self):\n    \"\"\"\n    Method that performs the write operation of the output data objects.\n    \"\"\"\n    for data_object in self.output_data_objects.values():\n        self.logger.info(f\"Writing {data_object.ID}...\")\n        data_object.write()\n        self.logger.info(\"... finished\")\n\n    for data_object in self.output_data_objects.values():\n        data_object.df.unpersist()\n</code></pre>"},{"location":"reference/components/execution/present_population/","title":"present_population","text":""},{"location":"reference/components/execution/present_population/present_population_estimation/","title":"present_population_estimation","text":"<p>Module for estimating the present population of a geographical area at a given time.</p>"},{"location":"reference/components/execution/present_population/present_population_estimation/#components.execution.present_population.present_population_estimation.PresentPopulationEstimation","title":"<code>PresentPopulationEstimation</code>","text":"<p>               Bases: <code>Component</code></p> <p>This component calculates the estimated actual population (number of people spatially present) for a specified spatial area (country, municipality, grid).</p> <p>NOTE: In the current variant 1 of implementation, this module implements only the counting of one MNO's users instead of extrapolating to the entire population.</p> Source code in <code>multimno/components/execution/present_population/present_population_estimation.py</code> <pre><code>class PresentPopulationEstimation(Component):\n    \"\"\"This component calculates the estimated actual population (number of people spatially present)\n    for a specified spatial area (country, municipality, grid).\n\n    NOTE: In the current variant 1 of implementation, this module implements only the counting of one\n    MNO's users instead of extrapolating to the entire population.\n    \"\"\"\n\n    COMPONENT_ID = \"PresentPopulationEstimation\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        # Maximum allowed time difference for an event to be included in a time point.\n        self.tolerance_period_s = self.config.getint(self.COMPONENT_ID, \"tolerance_period_s\")\n\n        # Time boundaries for result calculation.\n        self.data_period_start = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d %H:%M:%S\"\n        )\n        self.data_period_end = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d  %H:%M:%S\"\n        )\n\n        # Time gap (time distance in seconds between time points).\n        self.time_point_gap_s = timedelta(seconds=self.config.getint(self.COMPONENT_ID, \"time_point_gap_s\"))\n\n        # Maximum number of iterations for the Bayesian process.\n        self.max_iterations = self.config.getint(self.COMPONENT_ID, \"max_iterations\")\n\n        # Minimum difference threshold between prior and posterior to continue iterating the Bayesian process.\n        # Compares sum of absolute differences of each row.\n        self.min_difference_threshold = self.config.getfloat(self.COMPONENT_ID, \"min_difference_threshold\")\n\n        self.event_error_flags_to_include = self.config.geteval(self.COMPONENT_ID, \"event_error_flags_to_include\")\n        self.time_point = None\n        self.partition_number = self.config.getint(self.COMPONENT_ID, \"partition_number\")\n\n    def initalize_data_objects(self):\n        input_silver_event_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"event_data_silver_flagged\")\n        clear_destination_directory = self.config.getboolean(self.COMPONENT_ID, \"clear_destination_directory\")\n        input_silver_cell_connection_prob_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"cell_connection_probabilities_data_silver\"\n        )\n        input_silver_grid_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"grid_data_silver\")\n        input_silver_event = SilverEventFlaggedDataObject(self.spark, input_silver_event_path)\n        input_silver_cell_connection_prob = SilverCellConnectionProbabilitiesDataObject(\n            self.spark, input_silver_cell_connection_prob_path\n        )\n        input_silver_grid = SilverGridDataObject(self.spark, input_silver_grid_path)\n        self.input_data_objects = {\n            SilverEventFlaggedDataObject.ID: input_silver_event,\n            SilverCellConnectionProbabilitiesDataObject.ID: input_silver_cell_connection_prob,\n            SilverGridDataObject.ID: input_silver_grid,\n        }\n\n        # Output\n        # Output data object depends on whether results are aggregated per grid or per zone.\n        silver_present_population_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"present_population_silver\")\n        output_present_population = SilverPresentPopulationDataObject(\n            self.spark,\n            silver_present_population_path,\n        )\n        self.output_data_objects = {SilverPresentPopulationDataObject.ID: output_present_population}\n\n        if clear_destination_directory:\n            delete_file_or_folder(self.spark, silver_present_population_path)\n\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(\"STARTING: Present Population Estimation\")\n\n        self.read()\n\n        # Generate desired time points.\n        time_points = generate_time_points(self.data_period_start, self.data_period_end, self.time_point_gap_s)\n\n        # Processing logic: handle time points independently one at a time. Write results after each time point.\n        for time_point in time_points:\n            self.logger.info(f\"Present Population: Starting time point {time_point}\")\n            self.time_point = time_point\n            self.transform()\n            self.write()\n            # Cleanup\n            self.spark.catalog.clearCache()\n            tmp_pp_path = self.config.get(GENERAL_CONFIG_KEY, \"tmp_present_population_path\")\n            delete_file_or_folder(self.spark, tmp_pp_path)\n            self.logger.info(f\"Present Population: Finished time point {time_point}\")\n        # TODO: optimizing when to write.\n        # As time points are independent, it seems reasonable to write each out separately.\n        # Currently we have hard-coded write mode \"overwrite\", which does not allow for this,\n        # so DO write and pre-write deletion needs handling first.\n        self.logger.info(\"FINISHED: Present Population Estimation\")\n\n    def transform(self):\n        time_point = self.time_point\n        # Filter event data to dates within allowed time bounds.\n        events_df = self.input_data_objects[SilverEventFlaggedDataObject.ID].df\n        events_df = events_df.filter(F.col(ColNames.error_flag).isin(self.event_error_flags_to_include))\n\n        # Apply date-level filtering to omit events from dates unrelated to this time point.\n        events_df = select_where_dates_include_time_point_window(time_point, self.tolerance_period_s, events_df)\n\n        # Number of devices connected to each cell, taking only their event closest to the time_point\n        count_per_cell_df = self.calculate_devices_per_cell(events_df, time_point)\n\n        cell_conn_prob_df = self.get_cell_connection_probabilities(time_point)\n\n        # calculate population estimates per grid tile.\n        population_per_grid_df = self.calculate_population_per_grid(count_per_cell_df, cell_conn_prob_df)\n\n        # Prepare the results.\n        population_per_grid_df = (\n            population_per_grid_df.withColumn(ColNames.timestamp, F.lit(time_point))\n            .withColumn(ColNames.year, F.lit(time_point.year))\n            .withColumn(ColNames.month, F.lit(time_point.month))\n            .withColumn(ColNames.day, F.lit(time_point.day))\n        )\n        # Set results data object\n        population_per_grid_df = apply_schema_casting(population_per_grid_df, SilverPresentPopulationDataObject.SCHEMA)\n        population_per_grid_df = population_per_grid_df.coalesce(self.partition_number)\n\n        self.output_data_objects[SilverPresentPopulationDataObject.ID].df = population_per_grid_df\n\n    def get_cell_connection_probabilities(self, time_point: datetime) -&gt; DataFrame:\n        \"\"\"\n        Filter the cell connection probabilities of the dates needed for the time_point provided.\n        Args:\n            time_point (datetime.datetime): timestamp of time point\n\n        Returns:\n            DataFrame: (grid_id, cell_id, cell_connection_probability) dataframe\n        \"\"\"\n        time_bound_lower = time_point - timedelta(seconds=self.tolerance_period_s)\n        time_bound_upper = time_point + timedelta(seconds=self.tolerance_period_s)\n\n        cell_conn_prob_df = (\n            self.input_data_objects[SilverCellConnectionProbabilitiesDataObject.ID]\n            .df.select(\n                ColNames.year,\n                ColNames.month,\n                ColNames.day,\n                ColNames.cell_id,\n                ColNames.grid_id,\n                ColNames.cell_connection_probability,\n            )\n            .filter(\n                F.make_date(ColNames.year, ColNames.month, ColNames.day).between(\n                    lowerBound=F.lit(time_bound_lower.date()), upperBound=F.lit(time_bound_upper.date())\n                )\n            )\n        )\n\n        return cell_conn_prob_df\n\n    def calculate_devices_per_cell(\n        self,\n        events_df: DataFrame,\n        time_point: datetime,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Calculates the number of unique users/devices per cell for one time point based on the events inside the\n        interval around the time_point. If a device has multiple events inside the interval, the one closest to the\n        time_point is selected. In case of a tie, the earliest event is chosen.\n\n        Args:\n            events_df (DataFrame): Event data. For each user, expected to contain all of that user's events that can be\n                included in this time point.\n            time_point (datetime): The timestamp for which the population counts are calculated for.\n\n        Returns:\n            DataFrame: Count of devices per cell\n        \"\"\"\n        # Filter to include only events within the time window of the time point.\n        time_bound_lower = time_point - timedelta(seconds=self.tolerance_period_s)\n        time_bound_upper = time_point + timedelta(seconds=self.tolerance_period_s)\n        events_df = events_df.where(\n            (time_bound_lower &lt;= F.col(ColNames.timestamp)) &amp; (F.col(ColNames.timestamp) &lt;= time_bound_upper)\n        )\n\n        # For each user, order events by time distance from time point. If tied, prefer earlier timestamp.\n        window = Window.partitionBy(ColNames.user_id).orderBy(\n            F.abs(F.col(ColNames.timestamp) - time_point).asc(),\n            F.col(ColNames.timestamp),\n        )\n\n        events_df = (\n            events_df.withColumn(\"rank\", F.row_number().over(window))\n            .filter(F.col(\"rank\") == F.lit(1))  # event closes to the time_point\n            .drop(\"rank\")\n        )\n\n        counts_df = events_df.groupBy(ColNames.year, ColNames.month, ColNames.day, ColNames.cell_id).agg(\n            F.count(ColNames.user_id).alias(ColNames.device_count)\n        )\n\n        return counts_df\n\n    def calculate_population_per_grid(self, devices_per_cell_df: DataFrame, cell_conn_prob_df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Calculates population estimates for each grid tile Using an iterative Bayesian process.\n\n        Args:\n            devices_per_cell_df (DataFrame): (cell_id, device_count) dataframe\n            cell_conn_prob_df (DataFrame): (grid_id, cell_id, cell_connection_probability) dataframe\n        Returns:\n            DataFrame: (grid_id, population) dataframe\n        \"\"\"\n        devices_per_cell_df.cache()\n\n        # First, calculate total number of devices and grid tiles to initialise the prior\n        total_devices = devices_per_cell_df.select(F.sum(ColNames.device_count).alias(\"total_devices\")).collect()[0][\n            \"total_devices\"\n        ]\n\n        if total_devices is None or total_devices == 0:\n            total_devices = 0\n            # TODO: do not enter iterations, as everything will be zero!\n\n        grid_df = self.input_data_objects[SilverGridDataObject.ID].df.select(ColNames.grid_id)\n\n        # TODO: total number of grid tiles, or total number of grid tiles covered by the cells our events refer to?\n        total_tiles = grid_df.count()\n\n        # Initial prior value of population per tile\n        initial_prior_value = float(total_devices / total_tiles)\n\n        # Create master dataframe\n        # Fields: year, month, day, cell_id, grid_id, cell_connection_probability, device_count(in the cell)\n        master_df = cell_conn_prob_df.join(\n            devices_per_cell_df, on=[ColNames.year, ColNames.month, ColNames.day, ColNames.cell_id]\n        )\n\n        # Add prior\n        master_df = master_df.withColumn(ColNames.population, F.lit(initial_prior_value))\n\n        # Persist static information\n        master_df.cache()\n\n        niter = 0\n        diff = sys.float_info.max\n\n        normalisation_window = Window().partitionBy(ColNames.year, ColNames.month, ColNames.day, ColNames.cell_id)\n\n        # First iteration\n        # Calculate population using bayes\n        pop_df = (\n            master_df.withColumn(\n                ColNames.population, F.col(ColNames.population) * F.col(ColNames.cell_connection_probability)\n            )\n            .withColumn(\n                ColNames.population,\n                (\n                    F.col(ColNames.device_count)\n                    * F.col(ColNames.population)\n                    / F.sum(ColNames.population).over(normalisation_window)\n                ),\n            )\n            .groupby(ColNames.grid_id)\n            .agg(F.sum(ColNames.population).alias(ColNames.population))\n        )\n\n        # Calculate difference with initial prior\n        diff_df = pop_df.select(\n            F.sum(F.abs(F.col(ColNames.population) - F.lit(initial_prior_value))).alias(\"difference\")\n        )\n\n        # Cache the population dataframe before getting the difference\n        pop_df.cache()\n\n        diff = diff_df.collect()[0][\"difference\"]\n\n        # Persist the population dataframe in disk\n        tmp_pp_path = self.config.get(GENERAL_CONFIG_KEY, \"tmp_present_population_path\")\n        pop_df.write.parquet(tmp_pp_path, mode=\"overwrite\")\n\n        # unpersist in ram\n        pop_df.unpersist()\n\n        # If the difference is None, set it to 0\n        if diff is None:\n            diff = 0\n        # Increment the number of iterations and finish first iteration\n        niter += 1\n        self.logger.info(f\"Finished iteration {niter}, diff {diff} vs threshold {self.min_difference_threshold}\")\n\n        if diff &lt; self.min_difference_threshold:\n            return pop_df.withColumn(ColNames.population, F.col(ColNames.population).cast(FloatType()))\n\n        while niter &lt; self.max_iterations:\n            # Load the population dataframe from disk\n            pop_df = self.spark.read.parquet(tmp_pp_path)\n            # pop_df.cache()\n\n            # Calculate the new population dataframe using the cached master dataframe\n            new_pop_df = (\n                master_df.drop(ColNames.population)\n                .join(pop_df, on=ColNames.grid_id)\n                .withColumn(\n                    ColNames.population, F.col(ColNames.population) * F.col(ColNames.cell_connection_probability)\n                )\n                .withColumn(\n                    ColNames.population,\n                    (\n                        F.col(ColNames.device_count)\n                        * F.col(ColNames.population)\n                        / F.sum(ColNames.population).over(normalisation_window)\n                    ),\n                )\n                .groupby(ColNames.grid_id)\n                .agg(F.sum(ColNames.population).alias(ColNames.population))\n            )\n\n            # Cache the new population dataframe\n            new_pop_df.cache()\n\n            # Calculate the difference between the new population dataframe and the previous one\n            diff_df = (\n                new_pop_df.withColumnRenamed(ColNames.population, \"new_population\")\n                .join(pop_df, on=ColNames.grid_id)\n                .select(F.sum(F.abs(F.col(ColNames.population) - F.col(\"new_population\"))).alias(\"difference\"))\n            )\n\n            # Get the difference\n            diff = diff_df.collect()[0][\"difference\"]\n            if diff is None:\n                diff = 0\n\n            niter += 1\n            self.logger.info(f\"Finished iteration {niter}, diff {diff} vs threshold {self.min_difference_threshold}\")\n\n            if diff &lt; self.min_difference_threshold:\n                break\n\n            # Overwrite in disk the new population dataframe &amp; Unpersist the new population dataframe\n            new_pop_df.write.parquet(tmp_pp_path, mode=\"overwrite\")\n            new_pop_df.unpersist()\n            # pop_df = new_pop_df\n\n        if diff &lt; self.min_difference_threshold:\n            self.logger.info(f\"Algorithm convergence for tolerance {self.min_difference_threshold}!\")\n        else:\n            self.logger.info(f\"Stopped iterations after reaching max iterations {self.max_iterations}\")\n\n        # Cleanup\n        master_df.unpersist()\n\n        # At the end of the iteration, we have our population estimation over the grid tiles\n        return new_pop_df.withColumn(ColNames.population, F.col(ColNames.population).cast(FloatType()))\n</code></pre>"},{"location":"reference/components/execution/present_population/present_population_estimation/#components.execution.present_population.present_population_estimation.PresentPopulationEstimation.calculate_devices_per_cell","title":"<code>calculate_devices_per_cell(events_df, time_point)</code>","text":"<p>Calculates the number of unique users/devices per cell for one time point based on the events inside the interval around the time_point. If a device has multiple events inside the interval, the one closest to the time_point is selected. In case of a tie, the earliest event is chosen.</p> <p>Parameters:</p> Name Type Description Default <code>events_df</code> <code>DataFrame</code> <p>Event data. For each user, expected to contain all of that user's events that can be included in this time point.</p> required <code>time_point</code> <code>datetime</code> <p>The timestamp for which the population counts are calculated for.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Count of devices per cell</p> Source code in <code>multimno/components/execution/present_population/present_population_estimation.py</code> <pre><code>def calculate_devices_per_cell(\n    self,\n    events_df: DataFrame,\n    time_point: datetime,\n) -&gt; DataFrame:\n    \"\"\"\n    Calculates the number of unique users/devices per cell for one time point based on the events inside the\n    interval around the time_point. If a device has multiple events inside the interval, the one closest to the\n    time_point is selected. In case of a tie, the earliest event is chosen.\n\n    Args:\n        events_df (DataFrame): Event data. For each user, expected to contain all of that user's events that can be\n            included in this time point.\n        time_point (datetime): The timestamp for which the population counts are calculated for.\n\n    Returns:\n        DataFrame: Count of devices per cell\n    \"\"\"\n    # Filter to include only events within the time window of the time point.\n    time_bound_lower = time_point - timedelta(seconds=self.tolerance_period_s)\n    time_bound_upper = time_point + timedelta(seconds=self.tolerance_period_s)\n    events_df = events_df.where(\n        (time_bound_lower &lt;= F.col(ColNames.timestamp)) &amp; (F.col(ColNames.timestamp) &lt;= time_bound_upper)\n    )\n\n    # For each user, order events by time distance from time point. If tied, prefer earlier timestamp.\n    window = Window.partitionBy(ColNames.user_id).orderBy(\n        F.abs(F.col(ColNames.timestamp) - time_point).asc(),\n        F.col(ColNames.timestamp),\n    )\n\n    events_df = (\n        events_df.withColumn(\"rank\", F.row_number().over(window))\n        .filter(F.col(\"rank\") == F.lit(1))  # event closes to the time_point\n        .drop(\"rank\")\n    )\n\n    counts_df = events_df.groupBy(ColNames.year, ColNames.month, ColNames.day, ColNames.cell_id).agg(\n        F.count(ColNames.user_id).alias(ColNames.device_count)\n    )\n\n    return counts_df\n</code></pre>"},{"location":"reference/components/execution/present_population/present_population_estimation/#components.execution.present_population.present_population_estimation.PresentPopulationEstimation.calculate_population_per_grid","title":"<code>calculate_population_per_grid(devices_per_cell_df, cell_conn_prob_df)</code>","text":"<p>Calculates population estimates for each grid tile Using an iterative Bayesian process.</p> <p>Parameters:</p> Name Type Description Default <code>devices_per_cell_df</code> <code>DataFrame</code> <p>(cell_id, device_count) dataframe</p> required <code>cell_conn_prob_df</code> <code>DataFrame</code> <p>(grid_id, cell_id, cell_connection_probability) dataframe</p> required <p>Returns:     DataFrame: (grid_id, population) dataframe</p> Source code in <code>multimno/components/execution/present_population/present_population_estimation.py</code> <pre><code>def calculate_population_per_grid(self, devices_per_cell_df: DataFrame, cell_conn_prob_df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Calculates population estimates for each grid tile Using an iterative Bayesian process.\n\n    Args:\n        devices_per_cell_df (DataFrame): (cell_id, device_count) dataframe\n        cell_conn_prob_df (DataFrame): (grid_id, cell_id, cell_connection_probability) dataframe\n    Returns:\n        DataFrame: (grid_id, population) dataframe\n    \"\"\"\n    devices_per_cell_df.cache()\n\n    # First, calculate total number of devices and grid tiles to initialise the prior\n    total_devices = devices_per_cell_df.select(F.sum(ColNames.device_count).alias(\"total_devices\")).collect()[0][\n        \"total_devices\"\n    ]\n\n    if total_devices is None or total_devices == 0:\n        total_devices = 0\n        # TODO: do not enter iterations, as everything will be zero!\n\n    grid_df = self.input_data_objects[SilverGridDataObject.ID].df.select(ColNames.grid_id)\n\n    # TODO: total number of grid tiles, or total number of grid tiles covered by the cells our events refer to?\n    total_tiles = grid_df.count()\n\n    # Initial prior value of population per tile\n    initial_prior_value = float(total_devices / total_tiles)\n\n    # Create master dataframe\n    # Fields: year, month, day, cell_id, grid_id, cell_connection_probability, device_count(in the cell)\n    master_df = cell_conn_prob_df.join(\n        devices_per_cell_df, on=[ColNames.year, ColNames.month, ColNames.day, ColNames.cell_id]\n    )\n\n    # Add prior\n    master_df = master_df.withColumn(ColNames.population, F.lit(initial_prior_value))\n\n    # Persist static information\n    master_df.cache()\n\n    niter = 0\n    diff = sys.float_info.max\n\n    normalisation_window = Window().partitionBy(ColNames.year, ColNames.month, ColNames.day, ColNames.cell_id)\n\n    # First iteration\n    # Calculate population using bayes\n    pop_df = (\n        master_df.withColumn(\n            ColNames.population, F.col(ColNames.population) * F.col(ColNames.cell_connection_probability)\n        )\n        .withColumn(\n            ColNames.population,\n            (\n                F.col(ColNames.device_count)\n                * F.col(ColNames.population)\n                / F.sum(ColNames.population).over(normalisation_window)\n            ),\n        )\n        .groupby(ColNames.grid_id)\n        .agg(F.sum(ColNames.population).alias(ColNames.population))\n    )\n\n    # Calculate difference with initial prior\n    diff_df = pop_df.select(\n        F.sum(F.abs(F.col(ColNames.population) - F.lit(initial_prior_value))).alias(\"difference\")\n    )\n\n    # Cache the population dataframe before getting the difference\n    pop_df.cache()\n\n    diff = diff_df.collect()[0][\"difference\"]\n\n    # Persist the population dataframe in disk\n    tmp_pp_path = self.config.get(GENERAL_CONFIG_KEY, \"tmp_present_population_path\")\n    pop_df.write.parquet(tmp_pp_path, mode=\"overwrite\")\n\n    # unpersist in ram\n    pop_df.unpersist()\n\n    # If the difference is None, set it to 0\n    if diff is None:\n        diff = 0\n    # Increment the number of iterations and finish first iteration\n    niter += 1\n    self.logger.info(f\"Finished iteration {niter}, diff {diff} vs threshold {self.min_difference_threshold}\")\n\n    if diff &lt; self.min_difference_threshold:\n        return pop_df.withColumn(ColNames.population, F.col(ColNames.population).cast(FloatType()))\n\n    while niter &lt; self.max_iterations:\n        # Load the population dataframe from disk\n        pop_df = self.spark.read.parquet(tmp_pp_path)\n        # pop_df.cache()\n\n        # Calculate the new population dataframe using the cached master dataframe\n        new_pop_df = (\n            master_df.drop(ColNames.population)\n            .join(pop_df, on=ColNames.grid_id)\n            .withColumn(\n                ColNames.population, F.col(ColNames.population) * F.col(ColNames.cell_connection_probability)\n            )\n            .withColumn(\n                ColNames.population,\n                (\n                    F.col(ColNames.device_count)\n                    * F.col(ColNames.population)\n                    / F.sum(ColNames.population).over(normalisation_window)\n                ),\n            )\n            .groupby(ColNames.grid_id)\n            .agg(F.sum(ColNames.population).alias(ColNames.population))\n        )\n\n        # Cache the new population dataframe\n        new_pop_df.cache()\n\n        # Calculate the difference between the new population dataframe and the previous one\n        diff_df = (\n            new_pop_df.withColumnRenamed(ColNames.population, \"new_population\")\n            .join(pop_df, on=ColNames.grid_id)\n            .select(F.sum(F.abs(F.col(ColNames.population) - F.col(\"new_population\"))).alias(\"difference\"))\n        )\n\n        # Get the difference\n        diff = diff_df.collect()[0][\"difference\"]\n        if diff is None:\n            diff = 0\n\n        niter += 1\n        self.logger.info(f\"Finished iteration {niter}, diff {diff} vs threshold {self.min_difference_threshold}\")\n\n        if diff &lt; self.min_difference_threshold:\n            break\n\n        # Overwrite in disk the new population dataframe &amp; Unpersist the new population dataframe\n        new_pop_df.write.parquet(tmp_pp_path, mode=\"overwrite\")\n        new_pop_df.unpersist()\n        # pop_df = new_pop_df\n\n    if diff &lt; self.min_difference_threshold:\n        self.logger.info(f\"Algorithm convergence for tolerance {self.min_difference_threshold}!\")\n    else:\n        self.logger.info(f\"Stopped iterations after reaching max iterations {self.max_iterations}\")\n\n    # Cleanup\n    master_df.unpersist()\n\n    # At the end of the iteration, we have our population estimation over the grid tiles\n    return new_pop_df.withColumn(ColNames.population, F.col(ColNames.population).cast(FloatType()))\n</code></pre>"},{"location":"reference/components/execution/present_population/present_population_estimation/#components.execution.present_population.present_population_estimation.PresentPopulationEstimation.get_cell_connection_probabilities","title":"<code>get_cell_connection_probabilities(time_point)</code>","text":"<p>Filter the cell connection probabilities of the dates needed for the time_point provided. Args:     time_point (datetime.datetime): timestamp of time point</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>(grid_id, cell_id, cell_connection_probability) dataframe</p> Source code in <code>multimno/components/execution/present_population/present_population_estimation.py</code> <pre><code>def get_cell_connection_probabilities(self, time_point: datetime) -&gt; DataFrame:\n    \"\"\"\n    Filter the cell connection probabilities of the dates needed for the time_point provided.\n    Args:\n        time_point (datetime.datetime): timestamp of time point\n\n    Returns:\n        DataFrame: (grid_id, cell_id, cell_connection_probability) dataframe\n    \"\"\"\n    time_bound_lower = time_point - timedelta(seconds=self.tolerance_period_s)\n    time_bound_upper = time_point + timedelta(seconds=self.tolerance_period_s)\n\n    cell_conn_prob_df = (\n        self.input_data_objects[SilverCellConnectionProbabilitiesDataObject.ID]\n        .df.select(\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            ColNames.cell_id,\n            ColNames.grid_id,\n            ColNames.cell_connection_probability,\n        )\n        .filter(\n            F.make_date(ColNames.year, ColNames.month, ColNames.day).between(\n                lowerBound=F.lit(time_bound_lower.date()), upperBound=F.lit(time_bound_upper.date())\n            )\n        )\n    )\n\n    return cell_conn_prob_df\n</code></pre>"},{"location":"reference/components/execution/present_population/present_population_estimation/#components.execution.present_population.present_population_estimation.generate_time_points","title":"<code>generate_time_points(period_start, period_end, time_point_gap_s)</code>","text":"<p>Generates time points within the specified period with the specified spacing.</p> <p>Parameters:</p> Name Type Description Default <code>period_start</code> <code>datetime</code> <p>Start timestamp of generation.</p> required <code>period_end</code> <code>datetime</code> <p>End timestamp of generation.</p> required <code>time_point_gap_s</code> <code>timedelta</code> <p>Time delta object defining the space between consectuive time points.</p> required <p>Returns:</p> Type Description <code>List[datetime]</code> <p>[datetime]: List of time point timestamps.</p> Source code in <code>multimno/components/execution/present_population/present_population_estimation.py</code> <pre><code>def generate_time_points(period_start: datetime, period_end: datetime, time_point_gap_s: timedelta) -&gt; List[datetime]:\n    \"\"\"\n    Generates time points within the specified period with the specified spacing.\n\n    Args:\n        period_start (datetime): Start timestamp of generation.\n        period_end (datetime): End timestamp of generation.\n        time_point_gap_s (timedelta): Time delta object defining the space between consectuive time points.\n\n    Returns:\n        [datetime]: List of time point timestamps.\n    \"\"\"\n    # TODO this might be reusable across components.\n    time_points = []\n    one_time_point = period_start\n    while one_time_point &lt;= period_end:\n        time_points.append(one_time_point)\n        one_time_point = one_time_point + time_point_gap_s\n    return time_points\n</code></pre>"},{"location":"reference/components/execution/present_population/present_population_estimation/#components.execution.present_population.present_population_estimation.select_where_dates_include_time_point_window","title":"<code>select_where_dates_include_time_point_window(time_point, tolerance_period_s, df)</code>","text":"<p>Applies filtering to the DataFrame to omit rows from dates which are outside time point boundaries. The purpose is to leverage our Parquet partitioning schema (partitioned by year, month, day) and use predicate pushdown to avoid reading event data from irrelevant days.</p> <p>Parameters:</p> Name Type Description Default <code>time_point</code> <code>datetime</code> <p>Fixed timestamp to calculate results for.</p> required <code>tolerance_period_s</code> <code>int</code> <p>Time window size. Time in seconds before and after the time point</p> required <code>df</code> <code>DataFrame</code> <p>DataFrame of event data storage partitioned by year, month, day.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>df including only data from dates which include some part of the time point's window.</p> Source code in <code>multimno/components/execution/present_population/present_population_estimation.py</code> <pre><code>def select_where_dates_include_time_point_window(\n    time_point: datetime, tolerance_period_s: int, df: DataFrame\n) -&gt; DataFrame:\n    \"\"\"\n    Applies filtering to the DataFrame to omit rows from dates which are outside time point boundaries.\n    The purpose is to leverage our Parquet partitioning schema (partitioned by year, month, day) and\n    use predicate pushdown to avoid reading event data from irrelevant days.\n\n    Args:\n        time_point (datetime): Fixed timestamp to calculate results for.\n        tolerance_period_s (int): Time window size. Time in seconds before and after the time point\n        within which the event data is included.\n        df (DataFrame): DataFrame of event data storage partitioned by year, month, day.\n\n    Returns:\n        DataFrame: df including only data from dates which include some part of the time point's window.\n    \"\"\"\n    time_bound_lower = time_point - timedelta(seconds=tolerance_period_s)\n    date_lower = time_bound_lower.date()\n    time_bound_upper = time_point + timedelta(seconds=tolerance_period_s)\n    date_upper = time_bound_upper.date()\n    return df.where(\n        (F.make_date(ColNames.year, ColNames.month, ColNames.day) &gt;= date_lower)\n        &amp; (F.make_date(ColNames.year, ColNames.month, ColNames.day) &lt;= date_upper)\n    )\n</code></pre>"},{"location":"reference/components/execution/spatial_aggregation/","title":"spatial_aggregation","text":""},{"location":"reference/components/execution/spatial_aggregation/spatial_aggregation/","title":"spatial_aggregation","text":"<p>This module is responsible for aggregation of the gridded indicators to geographical zones of interest.</p>"},{"location":"reference/components/execution/spatial_aggregation/spatial_aggregation/#components.execution.spatial_aggregation.spatial_aggregation.SpatialAggregation","title":"<code>SpatialAggregation</code>","text":"<p>               Bases: <code>Component</code></p> <p>This class is responsible for spatial aggregation of the gridded indicators to geographical zones of interest.</p> Source code in <code>multimno/components/execution/spatial_aggregation/spatial_aggregation.py</code> <pre><code>class SpatialAggregation(Component):\n    \"\"\"\n    This class is responsible for spatial aggregation of the gridded indicators to geographical zones of interest.\n    \"\"\"\n\n    COMPONENT_ID = \"SpatialAggregation\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.zoning_dataset_id = None\n        self.current_level = None\n        self.input_aggregation_do = None\n\n    def initalize_data_objects(self):\n\n        # inputs\n        self.input_data_objects_by_target = {}\n        self.aggegation_targets = []\n        if self.config.getboolean(SpatialAggregation.COMPONENT_ID, \"present_population_execution\"):\n            self.aggegation_targets.append(\"PresentPopulation\")\n\n        if self.config.getboolean(SpatialAggregation.COMPONENT_ID, \"usual_environment_execution\"):\n            self.aggegation_targets.append(\"UsualEnvironment\")\n\n        if len(self.aggegation_targets) == 0:\n            raise ValueError(\"No aggregation targets specified\")\n\n        # prepare input data objects to aggregate\n\n        inputs = {}\n\n        for target in self.aggegation_targets:\n            self.input_data_objects_by_target[target] = {}  # initial value for later\n            inputs[target] = {}\n            input_aggregation_do_params = CLASS_MAPPING[target][\"input\"]\n            inputs[target][input_aggregation_do_params[1]] = input_aggregation_do_params[0]\n\n            # if dataset is not a reserved one, include grid-to-zone map data object as input\n            if (\n                not self.config.get(f\"{SpatialAggregation.COMPONENT_ID}.{target}\", \"zoning_dataset_id\")\n                in ReservedDatasetIDs()\n            ):\n                inputs[target][\"geozones_grid_map_data_silver\"] = SilverGeozonesGridMapDataObject\n\n        for target in self.aggegation_targets:\n            for key, value in inputs[target].items():\n                path = self.config.get(CONFIG_SILVER_PATHS_KEY, key)\n                if check_if_data_path_exists(self.spark, path):\n                    self.input_data_objects_by_target[target][value.ID] = value(self.spark, path)\n                else:\n                    self.logger.warning(f\"Expected path {path} to exist but it does not\")\n                    raise ValueError(f\"Invalid path for {value.ID}: {path}\")\n\n        # prepare output data objects\n\n        self.output_data_objects = {}\n        for target in self.aggegation_targets:\n            output_do_params = CLASS_MAPPING[target][\"output\"]\n            output_do_path = self.config.get(CONFIG_SILVER_PATHS_KEY, output_do_params[1])\n\n            if self.config.getboolean(f\"{SpatialAggregation.COMPONENT_ID}.{target}\", \"clear_destination_directory\"):\n                delete_file_or_folder(self.spark, output_do_path)\n            self.output_data_objects[output_do_params[0].ID] = output_do_params[0](self.spark, output_do_path)\n            self.output_data_objects[output_do_params[0].ID].df = self.spark.createDataFrame(\n                self.spark.sparkContext.emptyRDD(), output_do_params[0].SCHEMA\n            )\n\n    def filter_dataframe(self, aggregation_target: str) -&gt; DataFrame:\n        \"\"\"Filtering function that takes the partitions of the dataframe specified via configuration file\n\n        Args:\n            df (DataFrame): original DataFrame\n\n        Raises:\n            ValueError: if `season` value in configuration file is not one of allowed values\n\n        Returns:\n            DataFrame: filtered DataFrame\n        \"\"\"\n\n        current_input_sdf = self.input_data_objects[self.input_aggregation_do.ID].df\n        if aggregation_target == \"PresentPopulation\":\n            start_date = dt.datetime.strptime(\n                self.config.get(f\"{self.COMPONENT_ID}.{aggregation_target}\", \"start_date\"), \"%Y-%m-%d\"\n            )\n            end_date = dt.datetime.strptime(\n                self.config.get(f\"{self.COMPONENT_ID}.{aggregation_target}\", \"end_date\"), \"%Y-%m-%d\"\n            )\n\n            current_input_sdf = current_input_sdf.where(\n                F.make_date(ColNames.year, ColNames.month, ColNames.day).between(start_date, end_date)\n            )\n\n        if aggregation_target == \"UsualEnvironment\":\n            labels = self.config.get(f\"{self.COMPONENT_ID}.{aggregation_target}\", \"labels\")\n            labels = list(x.strip() for x in labels.split(\",\"))\n\n            start_date = dt.datetime.strptime(\n                self.config.get(f\"{self.COMPONENT_ID}.{aggregation_target}\", \"start_month\"), \"%Y-%m\"\n            )\n            end_date = dt.datetime.strptime(\n                self.config.get(f\"{self.COMPONENT_ID}.{aggregation_target}\", \"end_month\"), \"%Y-%m\"\n            )\n            end_date = end_date + dt.timedelta(days=cal.monthrange(end_date.year, end_date.month)[1] - 1)\n            season = self.config.get(f\"{self.COMPONENT_ID}.{aggregation_target}\", \"season\")\n            if season not in SEASONS:\n                raise ValueError(f\"Unknown season {season} -- valid values are {SEASONS}\")\n\n            current_input_sdf = (\n                current_input_sdf.where(F.col(ColNames.label).isin(labels))\n                .where(F.col(ColNames.start_date) == start_date)\n                .where(F.col(ColNames.end_date) == end_date)\n                .where(F.col(ColNames.season) == season)\n            )\n\n        return current_input_sdf\n\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n\n        for target in self.aggegation_targets:\n            self.logger.info(f\"Starting spatial aggregation for {target} ...\")\n            self.input_data_objects = self.input_data_objects_by_target[target]  # input DOs for this target\n            self.input_aggregation_do = self.input_data_objects[CLASS_MAPPING[target][\"input\"][0].ID]\n            self.output_aggregation_do = self.output_data_objects[CLASS_MAPPING[target][\"output\"][0].ID]\n\n            self.zoning_dataset_id = self.config.get(f\"{self.COMPONENT_ID}.{target}\", \"zoning_dataset_id\")\n\n            if self.zoning_dataset_id == ReservedDatasetIDs.INSPIRE_100m:\n                self.logger.info(\n                    f\"No spatial aggregation needed for zoning_dataset_id {self.zoning_dataset_id} -- skipping {target}\"\n                )\n                continue\n\n            if self.zoning_dataset_id == ReservedDatasetIDs.INSPIRE_1km:\n                self.logger.info(\n                    f\"{target}: zoning_dataset_id is {self.zoning_dataset_id} -- forcing hierarchical levels to `[1]`\"\n                )\n                levels = [1]\n            else:\n                levels = self.config.get(f\"{self.COMPONENT_ID}.{target}\", \"hierarchical_levels\")\n                levels = list(int(x.strip()) for x in levels.split(\",\"))\n\n            self.read()\n            self.current_input_sdf = self.filter_dataframe(target)\n            # iterate over each hierarchichal level of zoning dataset\n            for level in levels:\n                self.logger.info(f\"Starting aggregation for level {level} ...\")\n                self.current_level = level\n                self.transform()\n                self.write()\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}...\")\n\n        if self.zoning_dataset_id == ReservedDatasetIDs.INSPIRE_1km:\n            aggregated_sdf = self.aggregate_to_coarser_grid(\n                self.spark,\n                self.current_input_sdf,\n                self.zoning_dataset_id,\n                self.current_level,\n                self.output_aggregation_do,\n            )\n        else:\n            current_zoning_sdf = self.input_data_objects[SilverGeozonesGridMapDataObject.ID].df\n            current_zoning_sdf = current_zoning_sdf.filter(\n                current_zoning_sdf[ColNames.dataset_id].isin(self.zoning_dataset_id)\n            ).select(ColNames.grid_id, ColNames.hierarchical_id, ColNames.zone_id, ColNames.dataset_id)\n\n            # do aggregation\n            aggregated_sdf = self.aggregate_to_zone(\n                self.current_input_sdf, current_zoning_sdf, self.current_level, self.output_aggregation_do\n            )\n\n        aggregated_sdf = utils.apply_schema_casting(aggregated_sdf, self.output_aggregation_do.SCHEMA)\n\n        self.output_data_objects[self.output_aggregation_do.ID].df = aggregated_sdf\n\n    @staticmethod\n    def aggregate_to_coarser_grid(\n        spark: SparkSession,\n        sdf_to_aggregate: DataFrame,\n        zoning_dataset_id: str,\n        hierarchical_level: int,\n        output_do: DataObject,\n    ) -&gt; DataFrame:\n        \"\"\"This method aggregates the data from the 100m reference grid to a coarser grid.\n\n        Args:\n            spark (SparkSession): sparkSession\n            sdf_to_aggregate (DataFrame): input data to aggregate\n            zoning_dataset_id (str): zoning dataset ID -- currently only accepted value is `ReservedDatasetIds.INSPIRE_1km`\n            hierarchical_level (int): hierarchical level that the output data will have in its `level` column\n            output_do (DataObject): output data object class\n\n        Raises:\n            NotImplementedError: whenever a zoning_dataset_id different to `ReservedDatasetIds.INSPIRE_1km` is passed\n\n        Returns:\n            sdf_to_aggregate: DataFrame - aggregated data\n        \"\"\"\n\n        if zoning_dataset_id == ReservedDatasetIDs.INSPIRE_1km:\n            coarser_resolution = 1000\n        else:\n            raise NotImplementedError(\n                f\"Unexpected Reserved dataset id -- implemented only for {ReservedDatasetIDs.INSPIRE_1km}\"\n            )\n\n        grid_gen = InspireGridGenerator(spark=spark, resolution=100)\n\n        # Replace 100m grid_id for 1km grid_id\n        sdf_to_aggregate = grid_gen.get_parent_grid_ids(\n            sdf_to_aggregate, resolution=coarser_resolution, parent_col_name=ColNames.grid_id\n        )\n\n        # Transform integer ID representation to INSPIRE representation, store in zone_id\n        sdf_to_aggregate = grid_gen.convert_internal_id_to_inspire_specs(\n            sdf=sdf_to_aggregate, resolution=1000, grid_id_col=ColNames.grid_id\n        ).withColumnRenamed(ColNames.grid_id, ColNames.zone_id)\n\n        # Add metadata column\n        sdf_to_aggregate = sdf_to_aggregate.withColumn(ColNames.dataset_id, F.lit(zoning_dataset_id))\n        sdf_to_aggregate = sdf_to_aggregate.withColumn(ColNames.level, F.lit(hierarchical_level))\n\n        # potentially different aggregation functions can be used\n        agg_expressions = [F.sum(F.col(col)).alias(col) for col in output_do.VALUE_COLUMNS]\n        aggregated_sdf = sdf_to_aggregate.groupBy(*output_do.AGGREGATION_COLUMNS).agg(*agg_expressions)\n\n        return aggregated_sdf\n\n    @staticmethod\n    def aggregate_to_zone(\n        sdf_to_aggregate: DataFrame, zone_to_grid_map_sdf: DataFrame, hierarchy_level: int, output_do: DataObject\n    ) -&gt; DataFrame:\n        \"\"\"\n        This method aggregates the input data to the desired hierarchical zone level\n\n        args:\n            sdf_to_aggregate: DataFrame - input data to aggregate\n            zone_to_grid_map_sdf: DataFrame - mapping of grid tiles to zones\n            hierarchy_level: int - desired hierarchical zone level\n            output_do: DataObject - output data object\n\n        returns:\n            sdf_to_aggregate: DataFrame - aggregated data\n        \"\"\"\n        # Override zone_id with the desired hierarchical zone level.\n        zone_to_grid_map_sdf = zone_to_grid_map_sdf.withColumn(\n            ColNames.zone_id,\n            F.element_at(F.split(F.col(ColNames.hierarchical_id), pattern=\"\\\\|\"), hierarchy_level),\n        )\n        zone_to_grid_map_sdf = zone_to_grid_map_sdf.withColumn(ColNames.level, F.lit(hierarchy_level))\n\n        sdf_to_aggregate = sdf_to_aggregate.join(zone_to_grid_map_sdf, on=ColNames.grid_id)\n\n        # potentially different aggregation functions can be used\n        agg_expressions = [F.sum(F.col(col)).alias(col) for col in output_do.VALUE_COLUMNS]\n        aggregated_sdf = sdf_to_aggregate.groupBy(*output_do.AGGREGATION_COLUMNS).agg(*agg_expressions)\n\n        return aggregated_sdf\n</code></pre>"},{"location":"reference/components/execution/spatial_aggregation/spatial_aggregation/#components.execution.spatial_aggregation.spatial_aggregation.SpatialAggregation.aggregate_to_coarser_grid","title":"<code>aggregate_to_coarser_grid(spark, sdf_to_aggregate, zoning_dataset_id, hierarchical_level, output_do)</code>  <code>staticmethod</code>","text":"<p>This method aggregates the data from the 100m reference grid to a coarser grid.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>sparkSession</p> required <code>sdf_to_aggregate</code> <code>DataFrame</code> <p>input data to aggregate</p> required <code>zoning_dataset_id</code> <code>str</code> <p>zoning dataset ID -- currently only accepted value is <code>ReservedDatasetIds.INSPIRE_1km</code></p> required <code>hierarchical_level</code> <code>int</code> <p>hierarchical level that the output data will have in its <code>level</code> column</p> required <code>output_do</code> <code>DataObject</code> <p>output data object class</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>whenever a zoning_dataset_id different to <code>ReservedDatasetIds.INSPIRE_1km</code> is passed</p> <p>Returns:</p> Name Type Description <code>sdf_to_aggregate</code> <code>DataFrame</code> <p>DataFrame - aggregated data</p> Source code in <code>multimno/components/execution/spatial_aggregation/spatial_aggregation.py</code> <pre><code>@staticmethod\ndef aggregate_to_coarser_grid(\n    spark: SparkSession,\n    sdf_to_aggregate: DataFrame,\n    zoning_dataset_id: str,\n    hierarchical_level: int,\n    output_do: DataObject,\n) -&gt; DataFrame:\n    \"\"\"This method aggregates the data from the 100m reference grid to a coarser grid.\n\n    Args:\n        spark (SparkSession): sparkSession\n        sdf_to_aggregate (DataFrame): input data to aggregate\n        zoning_dataset_id (str): zoning dataset ID -- currently only accepted value is `ReservedDatasetIds.INSPIRE_1km`\n        hierarchical_level (int): hierarchical level that the output data will have in its `level` column\n        output_do (DataObject): output data object class\n\n    Raises:\n        NotImplementedError: whenever a zoning_dataset_id different to `ReservedDatasetIds.INSPIRE_1km` is passed\n\n    Returns:\n        sdf_to_aggregate: DataFrame - aggregated data\n    \"\"\"\n\n    if zoning_dataset_id == ReservedDatasetIDs.INSPIRE_1km:\n        coarser_resolution = 1000\n    else:\n        raise NotImplementedError(\n            f\"Unexpected Reserved dataset id -- implemented only for {ReservedDatasetIDs.INSPIRE_1km}\"\n        )\n\n    grid_gen = InspireGridGenerator(spark=spark, resolution=100)\n\n    # Replace 100m grid_id for 1km grid_id\n    sdf_to_aggregate = grid_gen.get_parent_grid_ids(\n        sdf_to_aggregate, resolution=coarser_resolution, parent_col_name=ColNames.grid_id\n    )\n\n    # Transform integer ID representation to INSPIRE representation, store in zone_id\n    sdf_to_aggregate = grid_gen.convert_internal_id_to_inspire_specs(\n        sdf=sdf_to_aggregate, resolution=1000, grid_id_col=ColNames.grid_id\n    ).withColumnRenamed(ColNames.grid_id, ColNames.zone_id)\n\n    # Add metadata column\n    sdf_to_aggregate = sdf_to_aggregate.withColumn(ColNames.dataset_id, F.lit(zoning_dataset_id))\n    sdf_to_aggregate = sdf_to_aggregate.withColumn(ColNames.level, F.lit(hierarchical_level))\n\n    # potentially different aggregation functions can be used\n    agg_expressions = [F.sum(F.col(col)).alias(col) for col in output_do.VALUE_COLUMNS]\n    aggregated_sdf = sdf_to_aggregate.groupBy(*output_do.AGGREGATION_COLUMNS).agg(*agg_expressions)\n\n    return aggregated_sdf\n</code></pre>"},{"location":"reference/components/execution/spatial_aggregation/spatial_aggregation/#components.execution.spatial_aggregation.spatial_aggregation.SpatialAggregation.aggregate_to_zone","title":"<code>aggregate_to_zone(sdf_to_aggregate, zone_to_grid_map_sdf, hierarchy_level, output_do)</code>  <code>staticmethod</code>","text":"<p>This method aggregates the input data to the desired hierarchical zone level</p> <p>Parameters:</p> Name Type Description Default <code>sdf_to_aggregate</code> <code>DataFrame</code> <p>DataFrame - input data to aggregate</p> required <code>zone_to_grid_map_sdf</code> <code>DataFrame</code> <p>DataFrame - mapping of grid tiles to zones</p> required <code>hierarchy_level</code> <code>int</code> <p>int - desired hierarchical zone level</p> required <code>output_do</code> <code>DataObject</code> <p>DataObject - output data object</p> required <p>Returns:</p> Name Type Description <code>sdf_to_aggregate</code> <code>DataFrame</code> <p>DataFrame - aggregated data</p> Source code in <code>multimno/components/execution/spatial_aggregation/spatial_aggregation.py</code> <pre><code>@staticmethod\ndef aggregate_to_zone(\n    sdf_to_aggregate: DataFrame, zone_to_grid_map_sdf: DataFrame, hierarchy_level: int, output_do: DataObject\n) -&gt; DataFrame:\n    \"\"\"\n    This method aggregates the input data to the desired hierarchical zone level\n\n    args:\n        sdf_to_aggregate: DataFrame - input data to aggregate\n        zone_to_grid_map_sdf: DataFrame - mapping of grid tiles to zones\n        hierarchy_level: int - desired hierarchical zone level\n        output_do: DataObject - output data object\n\n    returns:\n        sdf_to_aggregate: DataFrame - aggregated data\n    \"\"\"\n    # Override zone_id with the desired hierarchical zone level.\n    zone_to_grid_map_sdf = zone_to_grid_map_sdf.withColumn(\n        ColNames.zone_id,\n        F.element_at(F.split(F.col(ColNames.hierarchical_id), pattern=\"\\\\|\"), hierarchy_level),\n    )\n    zone_to_grid_map_sdf = zone_to_grid_map_sdf.withColumn(ColNames.level, F.lit(hierarchy_level))\n\n    sdf_to_aggregate = sdf_to_aggregate.join(zone_to_grid_map_sdf, on=ColNames.grid_id)\n\n    # potentially different aggregation functions can be used\n    agg_expressions = [F.sum(F.col(col)).alias(col) for col in output_do.VALUE_COLUMNS]\n    aggregated_sdf = sdf_to_aggregate.groupBy(*output_do.AGGREGATION_COLUMNS).agg(*agg_expressions)\n\n    return aggregated_sdf\n</code></pre>"},{"location":"reference/components/execution/spatial_aggregation/spatial_aggregation/#components.execution.spatial_aggregation.spatial_aggregation.SpatialAggregation.filter_dataframe","title":"<code>filter_dataframe(aggregation_target)</code>","text":"<p>Filtering function that takes the partitions of the dataframe specified via configuration file</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>original DataFrame</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>season</code> value in configuration file is not one of allowed values</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>filtered DataFrame</p> Source code in <code>multimno/components/execution/spatial_aggregation/spatial_aggregation.py</code> <pre><code>def filter_dataframe(self, aggregation_target: str) -&gt; DataFrame:\n    \"\"\"Filtering function that takes the partitions of the dataframe specified via configuration file\n\n    Args:\n        df (DataFrame): original DataFrame\n\n    Raises:\n        ValueError: if `season` value in configuration file is not one of allowed values\n\n    Returns:\n        DataFrame: filtered DataFrame\n    \"\"\"\n\n    current_input_sdf = self.input_data_objects[self.input_aggregation_do.ID].df\n    if aggregation_target == \"PresentPopulation\":\n        start_date = dt.datetime.strptime(\n            self.config.get(f\"{self.COMPONENT_ID}.{aggregation_target}\", \"start_date\"), \"%Y-%m-%d\"\n        )\n        end_date = dt.datetime.strptime(\n            self.config.get(f\"{self.COMPONENT_ID}.{aggregation_target}\", \"end_date\"), \"%Y-%m-%d\"\n        )\n\n        current_input_sdf = current_input_sdf.where(\n            F.make_date(ColNames.year, ColNames.month, ColNames.day).between(start_date, end_date)\n        )\n\n    if aggregation_target == \"UsualEnvironment\":\n        labels = self.config.get(f\"{self.COMPONENT_ID}.{aggregation_target}\", \"labels\")\n        labels = list(x.strip() for x in labels.split(\",\"))\n\n        start_date = dt.datetime.strptime(\n            self.config.get(f\"{self.COMPONENT_ID}.{aggregation_target}\", \"start_month\"), \"%Y-%m\"\n        )\n        end_date = dt.datetime.strptime(\n            self.config.get(f\"{self.COMPONENT_ID}.{aggregation_target}\", \"end_month\"), \"%Y-%m\"\n        )\n        end_date = end_date + dt.timedelta(days=cal.monthrange(end_date.year, end_date.month)[1] - 1)\n        season = self.config.get(f\"{self.COMPONENT_ID}.{aggregation_target}\", \"season\")\n        if season not in SEASONS:\n            raise ValueError(f\"Unknown season {season} -- valid values are {SEASONS}\")\n\n        current_input_sdf = (\n            current_input_sdf.where(F.col(ColNames.label).isin(labels))\n            .where(F.col(ColNames.start_date) == start_date)\n            .where(F.col(ColNames.end_date) == end_date)\n            .where(F.col(ColNames.season) == season)\n        )\n\n    return current_input_sdf\n</code></pre>"},{"location":"reference/components/execution/time_segments/","title":"time_segments","text":""},{"location":"reference/components/execution/time_segments/continuous_time_segmentation/","title":"continuous_time_segmentation","text":"<p>Module that implements the Continuous Time Segmentations functionality</p>"},{"location":"reference/components/execution/time_segments/continuous_time_segmentation/#components.execution.time_segments.continuous_time_segmentation.ContinuousTimeSegmentation","title":"<code>ContinuousTimeSegmentation</code>","text":"<p>               Bases: <code>Component</code></p> <p>A class to aggregate events into time segments.</p> Source code in <code>multimno/components/execution/time_segments/continuous_time_segmentation.py</code> <pre><code>class ContinuousTimeSegmentation(Component):\n    \"\"\"\n    A class to aggregate events into time segments.\n    \"\"\"\n\n    COMPONENT_ID = \"ContinuousTimeSegmentation\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.data_period_start = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n        self.data_period_end = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n\n        self.min_time_stay = timedelta(seconds=self.config.getint(self.COMPONENT_ID, \"min_time_stay_s\"))\n\n        self.max_time_missing_stay = timedelta(seconds=self.config.getint(self.COMPONENT_ID, \"max_time_missing_stay_s\"))\n\n        self.max_time_missing_move = timedelta(seconds=self.config.getint(self.COMPONENT_ID, \"max_time_missing_move_s\"))\n\n        self.pad_time = timedelta(seconds=self.config.getint(self.COMPONENT_ID, \"pad_time_s\"))\n\n        self.event_error_flags_to_include = self.config.geteval(self.COMPONENT_ID, \"event_error_flags_to_include\")\n        # this is for UDF\n        self.segmentation_return_schema = StructType(\n            [\n                StructField(ColNames.start_timestamp, TimestampType()),\n                StructField(ColNames.end_timestamp, TimestampType()),\n                StructField(ColNames.cells, ArrayType(StringType())),\n                StructField(ColNames.state, StringType()),\n                StructField(ColNames.is_last, BooleanType()),\n                StructField(ColNames.time_segment_id, StringType()),\n                StructField(ColNames.user_id, StringType()),\n                StructField(ColNames.mcc, ShortType()),\n                StructField(ColNames.mnc, StringType()),\n                StructField(ColNames.plmn, IntegerType()),\n                StructField(ColNames.user_id_modulo, IntegerType()),\n            ]\n        )\n\n        self.data_period_dates = [\n            self.data_period_start + timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n    def initalize_data_objects(self):\n\n        # Input\n        self.input_data_objects = {}\n        self.is_first_run = self.config.getboolean(self.COMPONENT_ID, \"is_first_run\")\n\n        inputs = {\n            \"event_data_silver_flagged\": SilverEventFlaggedDataObject,\n            \"cell_intersection_groups_data_silver\": SilverCellIntersectionGroupsDataObject,\n        }\n        if not self.is_first_run:\n            inputs[\"time_segments_silver\"] = SilverTimeSegmentsDataObject\n\n        for key, value in inputs.items():\n            path = self.config.get(CONFIG_SILVER_PATHS_KEY, key)\n            if check_if_data_path_exists(self.spark, path):\n                self.input_data_objects[value.ID] = value(self.spark, path)\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")\n                raise ValueError(f\"Invalid path for {value.ID}: {path}\")\n\n        # Output\n        self.output_data_objects = {}\n        self.output_silver_time_segments_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"time_segments_silver\")\n        self.output_data_objects[SilverTimeSegmentsDataObject.ID] = SilverTimeSegmentsDataObject(\n            self.spark,\n            self.output_silver_time_segments_path,\n        )\n\n        # Output clearing\n        clear_destination_directory = self.config.getboolean(\n            self.COMPONENT_ID, \"clear_destination_directory\", fallback=False\n        )\n        if clear_destination_directory:\n            delete_file_or_folder(self.spark, self.output_silver_time_segments_path)\n        # TODO add optional date-limited deletion when not first run,\n        # but consider that segments get generated for an additional one day before the starting date\n\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n\n        # If segements was already calculated and this is continuation of the previous run\n        # we need to get the last time segment for each user.\n        # If this is the first run, we will create an empty dataframe\n        if self.is_first_run:\n            self.intital_time_segment = self.spark.createDataFrame([], SilverTimeSegmentsDataObject.SCHEMA)\n        else:\n            previous_date = self.data_period_start - timedelta(days=1)\n            self.intital_time_segment = self.input_data_objects[SilverTimeSegmentsDataObject.ID].df.filter(\n                (F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(previous_date))\n                &amp; (F.col(ColNames.is_last) == True)\n            )\n\n            # this is needed to join the last time segement with the events of the current date\n            self.intital_time_segment = self.intital_time_segment.withColumn(\n                ColNames.start_timestamp, F.date_add(F.col(ColNames.start_timestamp), 1)\n            ).withColumns(\n                {\n                    ColNames.year: F.year(ColNames.start_timestamp).cast(\"smallint\"),\n                    ColNames.month: F.month(ColNames.start_timestamp).cast(\"tinyint\"),\n                    ColNames.day: F.dayofmonth(ColNames.start_timestamp).cast(\"tinyint\"),\n                }\n            )\n\n            self.intital_time_segment = self.intital_time_segment.withColumn(\n                ColNames.user_id, F.hex(F.col(ColNames.user_id))\n            )\n\n        # for every date in the data period, get the events and the intersection groups\n        # for that date and calculate the time segments\n        for current_date in self.data_period_dates:\n\n            self.logger.info(f\"Processing events for {current_date.strftime('%Y-%m-%d')}\")\n\n            self.current_date = current_date\n            self.current_input_events_sdf = self.input_data_objects[SilverEventFlaggedDataObject.ID].df.filter(\n                (F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(current_date))\n                &amp; (F.col(ColNames.error_flag).isin(self.event_error_flags_to_include))\n            )\n\n            self.current_interesection_groups_sdf = (\n                self.input_data_objects[SilverCellIntersectionGroupsDataObject.ID]\n                .df.filter(\n                    (\n                        F.make_date(\n                            F.col(ColNames.year),\n                            F.col(ColNames.month),\n                            F.col(ColNames.day),\n                        )\n                        == F.lit(current_date)\n                    )\n                )\n                .select(ColNames.cell_id, ColNames.overlapping_cell_ids, ColNames.year, ColNames.month, ColNames.day)\n            )\n\n            self.transform()\n            self.write()\n            self.current_segments_sdf.unpersist()\n\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n\n        current_events = self.current_input_events_sdf\n        last_time_segment = self.intital_time_segment\n\n        # TODO: This conversion is needed for Pandas serialisation/deserialisation,\n        # to remove it when user_id will be stored as string, not as binary\n        current_events = current_events.withColumn(ColNames.user_id, F.hex(F.col(ColNames.user_id)))\n        # Conversion to string is needed for easier intersection groups lookup in the aggregation function\n        intersections_groups_df = self.current_interesection_groups_sdf\n\n        # Add overlapping_cell_ids list to each event\n        current_events = (\n            current_events.alias(\"df1\")\n            .join(\n                intersections_groups_df.alias(\"df2\"),\n                on=[ColNames.year, ColNames.month, ColNames.day, ColNames.cell_id],\n                how=\"left\",\n            )\n            .select(\n                f\"df1.{ColNames.user_id}\",\n                f\"df1.{ColNames.timestamp}\",\n                f\"df1.{ColNames.mcc}\",\n                f\"df1.{ColNames.mnc}\",\n                f\"df1.{ColNames.plmn}\",\n                f\"df1.{ColNames.cell_id}\",\n                f\"df1.{ColNames.year}\",\n                f\"df1.{ColNames.month}\",\n                f\"df1.{ColNames.day}\",\n                f\"df1.{ColNames.user_id_modulo}\",\n                ColNames.overlapping_cell_ids,\n            )\n        )\n        # TODO add first event(s?) from next date to current events to handle last segment of date\n\n        # Partial function to pass the current date and other parameters to the aggregation function\n        aggregate_stays_partial = partial(\n            self.aggregate_stays,\n            current_date=self.current_date,\n            min_time_stay=self.min_time_stay,\n            max_time_missing_stay=self.max_time_missing_stay,\n            max_time_missing_move=self.max_time_missing_move,\n            pad_time=self.pad_time,\n            # intersections_set=intersections_set,\n        )\n\n        groupby_cols = [\n            ColNames.user_id\n        ]  # + self.input_data_objects[SilverEventFlaggedDataObject.ID].PARTITION_COLUMNS\n\n        # TODO This filtration should be removed after bugfixes since it is not necessary\n        #   if previous modules are run correctly, since each valid event should have a valid cell.\n        current_events = current_events.where(F.col(ColNames.overlapping_cell_ids).isNotNull())\n\n        # Using cogroup to join the current events with the last time segment.\n        # Handy to avoid joining last segments to every row of the current events\n        # Also helps to detect missing events for the user for the last day or for the current day\n        # TODO: To test this approach with large datasets, might not be feasible\n        current_segments_sdf = (\n            current_events.groupby(*groupby_cols)\n            .cogroup(last_time_segment.groupby(*groupby_cols))\n            .applyInPandas(aggregate_stays_partial, self.segmentation_return_schema)\n        )\n\n        current_segments_sdf = current_segments_sdf.withColumns(\n            {\n                ColNames.year: F.year(ColNames.start_timestamp).cast(\"smallint\"),\n                ColNames.month: F.month(ColNames.start_timestamp).cast(\"tinyint\"),\n                ColNames.day: F.dayofmonth(ColNames.start_timestamp).cast(\"tinyint\"),\n            }\n        )\n        current_segments_sdf = current_segments_sdf.cache()\n        self.current_segments_sdf = current_segments_sdf\n\n        # Need to keep last segment for the next date iteration\n        # Have to add one day to time columns to be able to cogroup with the next day\n        last_segments = current_segments_sdf.filter(F.col(ColNames.is_last) == True)\n        last_segments = last_segments.withColumn(\n            ColNames.start_timestamp, F.date_add(F.col(ColNames.start_timestamp), 1)\n        )\n\n        last_segments = last_segments.withColumns(\n            {\n                ColNames.year: F.year(ColNames.start_timestamp).cast(\"smallint\"),\n                ColNames.month: F.month(ColNames.start_timestamp).cast(\"tinyint\"),\n                ColNames.day: F.dayofmonth(ColNames.start_timestamp).cast(\"tinyint\"),\n            }\n        )\n        self.intital_time_segment.unpersist()\n        self.intital_time_segment = last_segments.cache()\n        last_segments.count()\n\n        # TODO: This conversion is needed to get back to binary after Pandas serialisation/deserialisation,\n        # to remove it when user_id will be stored as string, not as binary\n        current_segments_sdf = current_segments_sdf.withColumn(ColNames.user_id, F.unhex(F.col(ColNames.user_id)))\n\n        current_segments_sdf = current_segments_sdf.select(\n            *[field.name for field in SilverTimeSegmentsDataObject.SCHEMA.fields]\n        )\n        columns = {\n            field.name: F.col(field.name).cast(field.dataType) for field in SilverTimeSegmentsDataObject.SCHEMA.fields\n        }\n        current_segments_sdf = current_segments_sdf.withColumns(columns)\n\n        current_segments_sdf = current_segments_sdf.repartition(\n            ColNames.year, ColNames.month, ColNames.day, ColNames.user_id_modulo\n        ).sortWithinPartitions(ColNames.user_id, ColNames.start_timestamp)\n\n        self.output_data_objects[SilverTimeSegmentsDataObject.ID].df = current_segments_sdf\n\n    @staticmethod\n    def aggregate_stays(\n        pdf: pdDataFrame,\n        last_segments_pdf: pdDataFrame,\n        current_date: date,\n        min_time_stay: timedelta,\n        max_time_missing_stay: timedelta,\n        max_time_missing_move: timedelta,\n        pad_time: timedelta,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Aggregates events into Time Segments for a given user.\n\n        This method processes a Pandas DataFrame of user events, and aggregates them into time segments based on\n        certain conditions. It handles the first event separately, then iterates over the remaining events. For each\n        event, it checks if there's an intersection of an event cell and the current time segment cells and if the time\n        gap is within anacceptable range. Depending on the state of the current time segment and the result of\n        the intersection check, it either updates the current time segment or creates a new one.\n\n        Input user event data is expected to be sorted by timestamp.\n\n        Parameters:\n        pdf (pdDataFrame): The input Pandas DataFrame containing user events to be processed.\n        last_segments_pdf (pdDataFrame): A Pandas DataFrame containing the last segments.\n        current_date (date): The current date.\n        min_time_stay (timedelta): The minimum time to consider a segment as a 'stay'.\n        max_time_missing_stay (timedelta): The maximum time gap to consider continuation of a 'stay'.\n        max_time_missing_move (timedelta): The maximum time gap to consider continuation of a 'move'.\n        pad_time (timedelta): The padding time to have between 'unknown' segment.\n        groups_sdf (DataFrame): A PySpark DataFrame containing the groups.\n\n        Returns:\n        DataFrame: A DataFrame containing the aggregated time segments.\n        \"\"\"\n        segments = []\n        is_first_ts = True\n\n        current_date_start = datetime.combine(current_date, time(0, 0, 0))\n        current_date_end = datetime.combine(current_date, time(23, 59, 59))\n        previous_date_start = current_date_start - timedelta(days=1)\n        previous_date_end = current_date_end - timedelta(days=1)\n\n        pdf = pdf.sort_values(by=[ColNames.timestamp])\n\n        # The user has current date events, previous time segment, or both. This code is not executed if both are missing.\n        # If previous time segment exists and events do exist, set it as current time segment.\n        #   Then process events to generate segments.\n        # If previous time segment exists and events do not exist, create whole-day \"unknown\" time segment.\n        #   This is the only segment for the date.\n        # If prevous time segment does not exist and events do exist, generate previous day whole-day \"unknown\" time segment using user info from events, set it as current time segment.\n        #   Then process events to generate segments.\n\n        if pdf.empty and not last_segments_pdf.empty:\n            # If a previous segments exists but this date has no data, then\n            # generate current date \"unknown\" segment using user data from previous date segment and add it to segments list.\n            # Do not generate any other segments for the current date.\n            user_id, user_mod, mcc, mnc, plmn = ContinuousTimeSegmentation.get_user_info_from_pdf(last_segments_pdf)\n            current_date_only_ts = ContinuousTimeSegmentation.create_time_segment(\n                current_date_start, current_date_end, [], \"unknown\", user_id\n            )\n            current_date_only_ts[ColNames.is_last] = True\n            segments.append(current_date_only_ts)\n        else:  # there exist event records of this date\n            if last_segments_pdf.empty:\n                # There is no existing previous date segment, so this is the first date this user has data.\n                # Generate previous date \"unknown\" segment, add it to segments list, set it as current segment.\n                user_id, user_mod, mcc, mnc, plmn = ContinuousTimeSegmentation.get_user_info_from_pdf(pdf)\n                current_ts = ContinuousTimeSegmentation.create_time_segment(\n                    previous_date_start, previous_date_end, [], \"unknown\", user_id\n                )\n                current_ts[ColNames.is_last] = True\n                segments.append(current_ts)\n            else:\n                # Get existing previous day segment, set as current segment.\n                user_id, user_mod, mcc, mnc, plmn = ContinuousTimeSegmentation.get_user_info_from_pdf(pdf)\n                current_ts = ContinuousTimeSegmentation.get_previous_date_last_segment(last_segments_pdf)\n            # Iterate over events in chronological order to generate segments.\n            for event in pdf.itertuples(index=False):\n                next_ts = {}\n                ts_to_add = []\n                event_timestamp = event.timestamp\n                event_cell = event.cell_id\n                overlapping_cell_ids = event.overlapping_cell_ids\n\n                # Add event's own cell_id to overlapping_cell_ids\n                current_event_cell_overlapping_cell_ids = overlapping_cell_ids.tolist() + [event_cell]\n\n                # For the first time segment to start, look at the previous day's last segment and\n                # create a new time segment with the same state from the day start until the first event.\n                if is_first_ts:\n                    next_ts = ContinuousTimeSegmentation.handle_first_segment(\n                        current_ts=current_ts,\n                        event_timestamp=event_timestamp,\n                        event_cell=event_cell,\n                        current_date_start=current_date_start,\n                        max_time_missing_stay=max_time_missing_stay,\n                        max_time_missing_move=max_time_missing_move,\n                        pad_time=pad_time,\n                        user_id=user_id,\n                    )\n                    current_ts = next_ts\n                    is_first_ts = False\n\n                # Determine if this event intersects with the current time segment.\n                is_intersected = ContinuousTimeSegmentation.check_intersection(\n                    current_ts[ColNames.cells],\n                    current_event_cell_overlapping_cell_ids,\n                )\n\n                if current_ts[ColNames.state] == \"unknown\":\n                    # If the current state is 'unknown' (from the previous day),\n                    # create a new 'undetermined' time segment with pad_time adjustment\n                    next_ts = ContinuousTimeSegmentation.create_time_segment(\n                        current_ts[ColNames.end_timestamp],\n                        event_timestamp,\n                        [event_cell],\n                        \"undetermined\",\n                        user_id,\n                    )\n                    ts_to_add = [current_ts]\n                    current_ts = next_ts\n\n                elif is_intersected and (event_timestamp - current_ts[ColNames.end_timestamp]) &lt;= max_time_missing_stay:\n                    # If there's an intersection, check if we should update the current_ts or create a new one\n                    if current_ts[ColNames.state] in [\"undetermined\", \"stay\"]:\n                        # If the current state is 'undetermined' or 'stay' and the time gap is within\n                        # the acceptable range for a stay update the current time segment with the new cell\n                        # and the new end timestamp and set state to stay\n                        current_ts[ColNames.end_timestamp] = event_timestamp\n                        current_ts[ColNames.cells] = list(set(current_ts[ColNames.cells] + [event_cell]))\n                        if current_ts[ColNames.end_timestamp] - current_ts[ColNames.start_timestamp] &gt; min_time_stay:\n                            current_ts[ColNames.state] = \"stay\"\n                    elif current_ts[ColNames.state] == \"move\":\n                        # If the current state is 'move' and the time gap is within the acceptable range\n                        # create new time segment with state 'undetermined' after 'move' segment\n                        next_ts = ContinuousTimeSegmentation.create_time_segment(\n                            current_ts[ColNames.end_timestamp],\n                            event_timestamp,\n                            [event_cell],\n                            \"undetermined\",\n                            user_id,\n                        )\n                        # if time gap is big enough to assume that it is a stay, change the state to 'stay'\n                        if next_ts[ColNames.end_timestamp] - next_ts[ColNames.start_timestamp] &gt; min_time_stay:\n                            next_ts[ColNames.state] = \"stay\"\n                        ts_to_add = [current_ts]\n                        current_ts = next_ts\n\n                elif (\n                    not is_intersected and event_timestamp - current_ts[ColNames.end_timestamp] &lt;= max_time_missing_move\n                ):\n                    # If there is no intersection and the time gap is within the acceptable range for a move, create\n                    # two 'move' segments, each covering half the duration of the time gap\n                    mid_point = (\n                        current_ts[ColNames.end_timestamp] + (event_timestamp - current_ts[ColNames.end_timestamp]) / 2\n                    )\n\n                    next_ts_1 = ContinuousTimeSegmentation.create_time_segment(\n                        current_ts[ColNames.end_timestamp],\n                        mid_point,\n                        current_ts[ColNames.cells],\n                        \"move\",\n                        user_id,\n                    )\n\n                    next_ts_2 = ContinuousTimeSegmentation.create_time_segment(\n                        mid_point,\n                        event_timestamp,\n                        [event_cell],\n                        \"move\",\n                        user_id,\n                    )\n\n                    ts_to_add = [current_ts, next_ts_1]\n                    current_ts = next_ts_2\n\n                else:\n                    # If the time gap is too big, create 'unknown' segment for missing time with pad_time adjustment\n                    # create new time segment with state 'undetermined' with pad_time adjustment\n                    current_ts[ColNames.end_timestamp] = current_ts[ColNames.end_timestamp] + pad_time\n\n                    next_ts_1 = ContinuousTimeSegmentation.create_time_segment(\n                        current_ts[ColNames.end_timestamp],\n                        event_timestamp - pad_time,\n                        [],\n                        \"unknown\",\n                        user_id,\n                    )\n\n                    next_ts_2 = ContinuousTimeSegmentation.create_time_segment(\n                        event_timestamp - pad_time,\n                        event_timestamp,\n                        [event_cell],\n                        \"undetermined\",\n                        user_id,\n                    )\n\n                    ts_to_add = [current_ts, next_ts_1]\n                    current_ts = next_ts_2\n\n                segments.extend(ts_to_add)\n            # For the last ongoing segment, set is_last to true and add it as the last segment of the day.\n            current_ts[ColNames.is_last] = True\n            segments.append(current_ts)\n        # TODO: NOT IMPLEMENTED.\n        # Currently there is no methodological description on how to handle the time from the last event to the end of the day.\n        # May need to create and extra time segment that covers the duration from the last event-based time segment until the end of the date.\n\n        # Prepare return columns\n        segments_df = pd.DataFrame(segments)\n        segments_df[ColNames.user_id] = user_id\n        segments_df[ColNames.mcc] = mcc\n        segments_df[ColNames.mnc] = mnc\n        segments_df[ColNames.plmn] = plmn\n        segments_df[ColNames.user_id_modulo] = user_mod\n\n        return segments_df\n\n    @staticmethod\n    def handle_first_segment(\n        current_ts: Dict,\n        event_timestamp: datetime,\n        event_cell: int,\n        current_date_start: datetime,\n        max_time_missing_stay: timedelta,\n        max_time_missing_move: timedelta,\n        pad_time: timedelta,\n        user_id: str,\n    ) -&gt; dict:\n        \"\"\"\n        Handles the first segment for a user for a date based on a previous date last segment.\n\n        This method takes the last time segment of previous date and the timestamp of the first\n            event in the current date.\n        It checks the state of the current time segment and the time difference between the end of the current\n        time segment and the first event in the next date.\n\n        If the state is 'undetermined' or 'stay' and the time difference is within the maximum missing stay time,\n        or if the state is 'move' and the time difference is within the maximum missing move time, it creates a new\n        time segment with the same cells, state. The start timestamp\n        of the new time segment is the start of the current date, and the end timestamp\n            is the timestamp of the first event.\n\n        If neither of these conditions are met, it creates a new time segment with\n            an empty list of cells, state 'unknown'.\n        The start timestamp of the new time segment is the start of the next date,\n            and the end timestamp is the timestamp of the first event minus the padding time.\n\n        Parameters:\n        current_ts (Dict): The last time segment from previous date.\n        event_timestamp (datetime): The timestamp of the first event in the current date.\n        event_cell (int): The cell of the first event in the current date.\n        current_date_start (datetime): The start of the current date.\n        max_time_missing_stay (timedelta): The maximum time gap to consider continuation of a 'stay'.\n        max_time_missing_move (timedelta): The maximum time gap to consider continuation of a 'move'.\n        pad_time (timedelta): The padding time to have between 'unknown' segment.\n\n        Returns:\n        dict: The new first time segment for the current date.\n        \"\"\"\n        # TODO Verify if logic is consistent with non-first segments\n        if (\n            current_ts[ColNames.state] in [\"undetermined\", \"stay\"]\n            and event_timestamp - current_ts[ColNames.end_timestamp] &lt;= max_time_missing_stay\n        ):\n            next_ts = ContinuousTimeSegmentation.create_time_segment(\n                current_date_start,\n                event_timestamp,\n                list(set(current_ts[ColNames.cells] + [event_cell])),\n                current_ts[ColNames.state],\n                user_id,\n            )\n        elif (\n            current_ts[ColNames.state] == \"move\"\n            and event_timestamp - current_ts[ColNames.end_timestamp] &lt;= max_time_missing_move\n        ):\n            next_ts = ContinuousTimeSegmentation.create_time_segment(\n                current_date_start,\n                event_timestamp,\n                list(set(current_ts[ColNames.cells] + [event_cell])),\n                current_ts[ColNames.state],\n                user_id,\n            )\n        else:\n            pad_time = timedelta(seconds=0) if event_timestamp - current_date_start &lt; pad_time else pad_time\n            next_ts = ContinuousTimeSegmentation.create_time_segment(\n                current_date_start,\n                event_timestamp - pad_time,\n                [],\n                \"unknown\",\n                user_id,\n            )\n\n        return next_ts\n\n    @staticmethod\n    def create_time_segment(\n        start_timestamp: datetime,\n        end_timestamp: datetime,\n        cells: List[str],\n        state: str,\n        user_id: str,\n    ) -&gt; Dict:\n        \"\"\"\n        Creates a new time segment.\n\n        It creates a new time segment with these values, incrementing the segment ID by 1\n        if a previous segment ID is provided, or setting it to 1 if not.\n\n        Parameters:\n        start_timestamp (datetime): The start timestamp of the time segment.\n        end_timestamp (datetime): The end timestamp of the time segment.\n        cells (List[str]): The cells of the time segment.\n        state (str): The state of the time segment.\n        previous_segment_id (Optional[int]): The ID of the previous time segment, if any.\n\n        Returns:\n        Dict: The new time segment.\n        \"\"\"\n        segment_id_string = f\"{user_id}{start_timestamp}\"\n        return {\n            ColNames.time_segment_id: hashlib.md5(segment_id_string.encode()).hexdigest(),\n            ColNames.start_timestamp: start_timestamp,\n            ColNames.end_timestamp: end_timestamp,\n            ColNames.cells: cells,\n            ColNames.state: state,\n            ColNames.is_last: False,\n        }\n\n    @staticmethod\n    def get_user_info_from_pdf(pdf: pdDataFrame) -&gt; Tuple[str, int, str]:\n        \"\"\"\n        Gets user_id, user_id_modulo, mcc, mnc and plmn from Pandas DataFrame containing columns with the corresponding names.\n        Values from the first row of the dataframe are used.\n\n        Args:\n            pdf (pdDataFrame): Pandas DataFrame\n\n        Returns:\n            Tuple[str, int, str]: user_id, user_id_modulo, mcc, mnc, plmn\n        \"\"\"\n        user_id = pdf[ColNames.user_id][0]\n        user_id_mod = pdf[ColNames.user_id_modulo][0]\n        mcc = pdf[ColNames.mcc][0]\n        mnc = pdf[ColNames.mnc][0]\n        plmn = pdf[ColNames.plmn][0]\n        return user_id, user_id_mod, mcc, mnc, plmn\n\n    @staticmethod\n    def get_previous_date_last_segment(last_segments_pdf: pdDataFrame) -&gt; Dict:\n        \"\"\"\n        Gets last segment from the previous date. The input DataFrame is expected to contain only one row to be retrieved.\n\n        Args:\n            last_segments_pdf (pdDataFrame): Pandas DataFrame containing the user's previous date's final segment.\n\n        Returns:\n            Dict: dict of segment\n        \"\"\"\n        return last_segments_pdf.iloc[0][\n            [\n                ColNames.time_segment_id,\n                ColNames.start_timestamp,\n                ColNames.end_timestamp,\n                ColNames.cells,\n                ColNames.state,\n            ]\n        ].to_dict()\n\n    @staticmethod\n    def check_intersection(\n        previous_ts_cells: List[str],\n        current_event_overlapping_cell_ids: List[str],\n    ) -&gt; bool:\n        \"\"\"\n        Checks if there is an intersection between the existing time segment and the current event.\n\n        This method takes two lists of cells, one for the cells included in the existing time segment and the other for\n        the overlapping cell ids of the current event's cell.\n        The time segment intersects with the event if each of the time segment's cells are included in the event's overlapping cell ids list.\n\n        A segment with no cells cannot intersect and returns False.\n\n        Parameters:\n        previous_ts_cells (List[str]): The cells of the existing time segment.\n        current_event_overlapping_cell_ids (List[str]): Cells the current event's cell overlaps with, including itself.\n\n        Returns:\n        bool: True if there is an intersection, False otherwise.\n        \"\"\"\n        if len(previous_ts_cells) == 0:\n            is_intersected = False\n        else:\n            is_intersected = set(previous_ts_cells).issubset(set(current_event_overlapping_cell_ids))\n        return is_intersected\n</code></pre>"},{"location":"reference/components/execution/time_segments/continuous_time_segmentation/#components.execution.time_segments.continuous_time_segmentation.ContinuousTimeSegmentation.aggregate_stays","title":"<code>aggregate_stays(pdf, last_segments_pdf, current_date, min_time_stay, max_time_missing_stay, max_time_missing_move, pad_time)</code>  <code>staticmethod</code>","text":"<p>Aggregates events into Time Segments for a given user.</p> <p>This method processes a Pandas DataFrame of user events, and aggregates them into time segments based on certain conditions. It handles the first event separately, then iterates over the remaining events. For each event, it checks if there's an intersection of an event cell and the current time segment cells and if the time gap is within anacceptable range. Depending on the state of the current time segment and the result of the intersection check, it either updates the current time segment or creates a new one.</p> <p>Input user event data is expected to be sorted by timestamp.</p> <p>Parameters: pdf (pdDataFrame): The input Pandas DataFrame containing user events to be processed. last_segments_pdf (pdDataFrame): A Pandas DataFrame containing the last segments. current_date (date): The current date. min_time_stay (timedelta): The minimum time to consider a segment as a 'stay'. max_time_missing_stay (timedelta): The maximum time gap to consider continuation of a 'stay'. max_time_missing_move (timedelta): The maximum time gap to consider continuation of a 'move'. pad_time (timedelta): The padding time to have between 'unknown' segment. groups_sdf (DataFrame): A PySpark DataFrame containing the groups.</p> <p>Returns: DataFrame: A DataFrame containing the aggregated time segments.</p> Source code in <code>multimno/components/execution/time_segments/continuous_time_segmentation.py</code> <pre><code>@staticmethod\ndef aggregate_stays(\n    pdf: pdDataFrame,\n    last_segments_pdf: pdDataFrame,\n    current_date: date,\n    min_time_stay: timedelta,\n    max_time_missing_stay: timedelta,\n    max_time_missing_move: timedelta,\n    pad_time: timedelta,\n) -&gt; DataFrame:\n    \"\"\"\n    Aggregates events into Time Segments for a given user.\n\n    This method processes a Pandas DataFrame of user events, and aggregates them into time segments based on\n    certain conditions. It handles the first event separately, then iterates over the remaining events. For each\n    event, it checks if there's an intersection of an event cell and the current time segment cells and if the time\n    gap is within anacceptable range. Depending on the state of the current time segment and the result of\n    the intersection check, it either updates the current time segment or creates a new one.\n\n    Input user event data is expected to be sorted by timestamp.\n\n    Parameters:\n    pdf (pdDataFrame): The input Pandas DataFrame containing user events to be processed.\n    last_segments_pdf (pdDataFrame): A Pandas DataFrame containing the last segments.\n    current_date (date): The current date.\n    min_time_stay (timedelta): The minimum time to consider a segment as a 'stay'.\n    max_time_missing_stay (timedelta): The maximum time gap to consider continuation of a 'stay'.\n    max_time_missing_move (timedelta): The maximum time gap to consider continuation of a 'move'.\n    pad_time (timedelta): The padding time to have between 'unknown' segment.\n    groups_sdf (DataFrame): A PySpark DataFrame containing the groups.\n\n    Returns:\n    DataFrame: A DataFrame containing the aggregated time segments.\n    \"\"\"\n    segments = []\n    is_first_ts = True\n\n    current_date_start = datetime.combine(current_date, time(0, 0, 0))\n    current_date_end = datetime.combine(current_date, time(23, 59, 59))\n    previous_date_start = current_date_start - timedelta(days=1)\n    previous_date_end = current_date_end - timedelta(days=1)\n\n    pdf = pdf.sort_values(by=[ColNames.timestamp])\n\n    # The user has current date events, previous time segment, or both. This code is not executed if both are missing.\n    # If previous time segment exists and events do exist, set it as current time segment.\n    #   Then process events to generate segments.\n    # If previous time segment exists and events do not exist, create whole-day \"unknown\" time segment.\n    #   This is the only segment for the date.\n    # If prevous time segment does not exist and events do exist, generate previous day whole-day \"unknown\" time segment using user info from events, set it as current time segment.\n    #   Then process events to generate segments.\n\n    if pdf.empty and not last_segments_pdf.empty:\n        # If a previous segments exists but this date has no data, then\n        # generate current date \"unknown\" segment using user data from previous date segment and add it to segments list.\n        # Do not generate any other segments for the current date.\n        user_id, user_mod, mcc, mnc, plmn = ContinuousTimeSegmentation.get_user_info_from_pdf(last_segments_pdf)\n        current_date_only_ts = ContinuousTimeSegmentation.create_time_segment(\n            current_date_start, current_date_end, [], \"unknown\", user_id\n        )\n        current_date_only_ts[ColNames.is_last] = True\n        segments.append(current_date_only_ts)\n    else:  # there exist event records of this date\n        if last_segments_pdf.empty:\n            # There is no existing previous date segment, so this is the first date this user has data.\n            # Generate previous date \"unknown\" segment, add it to segments list, set it as current segment.\n            user_id, user_mod, mcc, mnc, plmn = ContinuousTimeSegmentation.get_user_info_from_pdf(pdf)\n            current_ts = ContinuousTimeSegmentation.create_time_segment(\n                previous_date_start, previous_date_end, [], \"unknown\", user_id\n            )\n            current_ts[ColNames.is_last] = True\n            segments.append(current_ts)\n        else:\n            # Get existing previous day segment, set as current segment.\n            user_id, user_mod, mcc, mnc, plmn = ContinuousTimeSegmentation.get_user_info_from_pdf(pdf)\n            current_ts = ContinuousTimeSegmentation.get_previous_date_last_segment(last_segments_pdf)\n        # Iterate over events in chronological order to generate segments.\n        for event in pdf.itertuples(index=False):\n            next_ts = {}\n            ts_to_add = []\n            event_timestamp = event.timestamp\n            event_cell = event.cell_id\n            overlapping_cell_ids = event.overlapping_cell_ids\n\n            # Add event's own cell_id to overlapping_cell_ids\n            current_event_cell_overlapping_cell_ids = overlapping_cell_ids.tolist() + [event_cell]\n\n            # For the first time segment to start, look at the previous day's last segment and\n            # create a new time segment with the same state from the day start until the first event.\n            if is_first_ts:\n                next_ts = ContinuousTimeSegmentation.handle_first_segment(\n                    current_ts=current_ts,\n                    event_timestamp=event_timestamp,\n                    event_cell=event_cell,\n                    current_date_start=current_date_start,\n                    max_time_missing_stay=max_time_missing_stay,\n                    max_time_missing_move=max_time_missing_move,\n                    pad_time=pad_time,\n                    user_id=user_id,\n                )\n                current_ts = next_ts\n                is_first_ts = False\n\n            # Determine if this event intersects with the current time segment.\n            is_intersected = ContinuousTimeSegmentation.check_intersection(\n                current_ts[ColNames.cells],\n                current_event_cell_overlapping_cell_ids,\n            )\n\n            if current_ts[ColNames.state] == \"unknown\":\n                # If the current state is 'unknown' (from the previous day),\n                # create a new 'undetermined' time segment with pad_time adjustment\n                next_ts = ContinuousTimeSegmentation.create_time_segment(\n                    current_ts[ColNames.end_timestamp],\n                    event_timestamp,\n                    [event_cell],\n                    \"undetermined\",\n                    user_id,\n                )\n                ts_to_add = [current_ts]\n                current_ts = next_ts\n\n            elif is_intersected and (event_timestamp - current_ts[ColNames.end_timestamp]) &lt;= max_time_missing_stay:\n                # If there's an intersection, check if we should update the current_ts or create a new one\n                if current_ts[ColNames.state] in [\"undetermined\", \"stay\"]:\n                    # If the current state is 'undetermined' or 'stay' and the time gap is within\n                    # the acceptable range for a stay update the current time segment with the new cell\n                    # and the new end timestamp and set state to stay\n                    current_ts[ColNames.end_timestamp] = event_timestamp\n                    current_ts[ColNames.cells] = list(set(current_ts[ColNames.cells] + [event_cell]))\n                    if current_ts[ColNames.end_timestamp] - current_ts[ColNames.start_timestamp] &gt; min_time_stay:\n                        current_ts[ColNames.state] = \"stay\"\n                elif current_ts[ColNames.state] == \"move\":\n                    # If the current state is 'move' and the time gap is within the acceptable range\n                    # create new time segment with state 'undetermined' after 'move' segment\n                    next_ts = ContinuousTimeSegmentation.create_time_segment(\n                        current_ts[ColNames.end_timestamp],\n                        event_timestamp,\n                        [event_cell],\n                        \"undetermined\",\n                        user_id,\n                    )\n                    # if time gap is big enough to assume that it is a stay, change the state to 'stay'\n                    if next_ts[ColNames.end_timestamp] - next_ts[ColNames.start_timestamp] &gt; min_time_stay:\n                        next_ts[ColNames.state] = \"stay\"\n                    ts_to_add = [current_ts]\n                    current_ts = next_ts\n\n            elif (\n                not is_intersected and event_timestamp - current_ts[ColNames.end_timestamp] &lt;= max_time_missing_move\n            ):\n                # If there is no intersection and the time gap is within the acceptable range for a move, create\n                # two 'move' segments, each covering half the duration of the time gap\n                mid_point = (\n                    current_ts[ColNames.end_timestamp] + (event_timestamp - current_ts[ColNames.end_timestamp]) / 2\n                )\n\n                next_ts_1 = ContinuousTimeSegmentation.create_time_segment(\n                    current_ts[ColNames.end_timestamp],\n                    mid_point,\n                    current_ts[ColNames.cells],\n                    \"move\",\n                    user_id,\n                )\n\n                next_ts_2 = ContinuousTimeSegmentation.create_time_segment(\n                    mid_point,\n                    event_timestamp,\n                    [event_cell],\n                    \"move\",\n                    user_id,\n                )\n\n                ts_to_add = [current_ts, next_ts_1]\n                current_ts = next_ts_2\n\n            else:\n                # If the time gap is too big, create 'unknown' segment for missing time with pad_time adjustment\n                # create new time segment with state 'undetermined' with pad_time adjustment\n                current_ts[ColNames.end_timestamp] = current_ts[ColNames.end_timestamp] + pad_time\n\n                next_ts_1 = ContinuousTimeSegmentation.create_time_segment(\n                    current_ts[ColNames.end_timestamp],\n                    event_timestamp - pad_time,\n                    [],\n                    \"unknown\",\n                    user_id,\n                )\n\n                next_ts_2 = ContinuousTimeSegmentation.create_time_segment(\n                    event_timestamp - pad_time,\n                    event_timestamp,\n                    [event_cell],\n                    \"undetermined\",\n                    user_id,\n                )\n\n                ts_to_add = [current_ts, next_ts_1]\n                current_ts = next_ts_2\n\n            segments.extend(ts_to_add)\n        # For the last ongoing segment, set is_last to true and add it as the last segment of the day.\n        current_ts[ColNames.is_last] = True\n        segments.append(current_ts)\n    # TODO: NOT IMPLEMENTED.\n    # Currently there is no methodological description on how to handle the time from the last event to the end of the day.\n    # May need to create and extra time segment that covers the duration from the last event-based time segment until the end of the date.\n\n    # Prepare return columns\n    segments_df = pd.DataFrame(segments)\n    segments_df[ColNames.user_id] = user_id\n    segments_df[ColNames.mcc] = mcc\n    segments_df[ColNames.mnc] = mnc\n    segments_df[ColNames.plmn] = plmn\n    segments_df[ColNames.user_id_modulo] = user_mod\n\n    return segments_df\n</code></pre>"},{"location":"reference/components/execution/time_segments/continuous_time_segmentation/#components.execution.time_segments.continuous_time_segmentation.ContinuousTimeSegmentation.check_intersection","title":"<code>check_intersection(previous_ts_cells, current_event_overlapping_cell_ids)</code>  <code>staticmethod</code>","text":"<p>Checks if there is an intersection between the existing time segment and the current event.</p> <p>This method takes two lists of cells, one for the cells included in the existing time segment and the other for the overlapping cell ids of the current event's cell. The time segment intersects with the event if each of the time segment's cells are included in the event's overlapping cell ids list.</p> <p>A segment with no cells cannot intersect and returns False.</p> <p>Parameters: previous_ts_cells (List[str]): The cells of the existing time segment. current_event_overlapping_cell_ids (List[str]): Cells the current event's cell overlaps with, including itself.</p> <p>Returns: bool: True if there is an intersection, False otherwise.</p> Source code in <code>multimno/components/execution/time_segments/continuous_time_segmentation.py</code> <pre><code>@staticmethod\ndef check_intersection(\n    previous_ts_cells: List[str],\n    current_event_overlapping_cell_ids: List[str],\n) -&gt; bool:\n    \"\"\"\n    Checks if there is an intersection between the existing time segment and the current event.\n\n    This method takes two lists of cells, one for the cells included in the existing time segment and the other for\n    the overlapping cell ids of the current event's cell.\n    The time segment intersects with the event if each of the time segment's cells are included in the event's overlapping cell ids list.\n\n    A segment with no cells cannot intersect and returns False.\n\n    Parameters:\n    previous_ts_cells (List[str]): The cells of the existing time segment.\n    current_event_overlapping_cell_ids (List[str]): Cells the current event's cell overlaps with, including itself.\n\n    Returns:\n    bool: True if there is an intersection, False otherwise.\n    \"\"\"\n    if len(previous_ts_cells) == 0:\n        is_intersected = False\n    else:\n        is_intersected = set(previous_ts_cells).issubset(set(current_event_overlapping_cell_ids))\n    return is_intersected\n</code></pre>"},{"location":"reference/components/execution/time_segments/continuous_time_segmentation/#components.execution.time_segments.continuous_time_segmentation.ContinuousTimeSegmentation.create_time_segment","title":"<code>create_time_segment(start_timestamp, end_timestamp, cells, state, user_id)</code>  <code>staticmethod</code>","text":"<p>Creates a new time segment.</p> <p>It creates a new time segment with these values, incrementing the segment ID by 1 if a previous segment ID is provided, or setting it to 1 if not.</p> <p>Parameters: start_timestamp (datetime): The start timestamp of the time segment. end_timestamp (datetime): The end timestamp of the time segment. cells (List[str]): The cells of the time segment. state (str): The state of the time segment. previous_segment_id (Optional[int]): The ID of the previous time segment, if any.</p> <p>Returns: Dict: The new time segment.</p> Source code in <code>multimno/components/execution/time_segments/continuous_time_segmentation.py</code> <pre><code>@staticmethod\ndef create_time_segment(\n    start_timestamp: datetime,\n    end_timestamp: datetime,\n    cells: List[str],\n    state: str,\n    user_id: str,\n) -&gt; Dict:\n    \"\"\"\n    Creates a new time segment.\n\n    It creates a new time segment with these values, incrementing the segment ID by 1\n    if a previous segment ID is provided, or setting it to 1 if not.\n\n    Parameters:\n    start_timestamp (datetime): The start timestamp of the time segment.\n    end_timestamp (datetime): The end timestamp of the time segment.\n    cells (List[str]): The cells of the time segment.\n    state (str): The state of the time segment.\n    previous_segment_id (Optional[int]): The ID of the previous time segment, if any.\n\n    Returns:\n    Dict: The new time segment.\n    \"\"\"\n    segment_id_string = f\"{user_id}{start_timestamp}\"\n    return {\n        ColNames.time_segment_id: hashlib.md5(segment_id_string.encode()).hexdigest(),\n        ColNames.start_timestamp: start_timestamp,\n        ColNames.end_timestamp: end_timestamp,\n        ColNames.cells: cells,\n        ColNames.state: state,\n        ColNames.is_last: False,\n    }\n</code></pre>"},{"location":"reference/components/execution/time_segments/continuous_time_segmentation/#components.execution.time_segments.continuous_time_segmentation.ContinuousTimeSegmentation.get_previous_date_last_segment","title":"<code>get_previous_date_last_segment(last_segments_pdf)</code>  <code>staticmethod</code>","text":"<p>Gets last segment from the previous date. The input DataFrame is expected to contain only one row to be retrieved.</p> <p>Parameters:</p> Name Type Description Default <code>last_segments_pdf</code> <code>DataFrame</code> <p>Pandas DataFrame containing the user's previous date's final segment.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>dict of segment</p> Source code in <code>multimno/components/execution/time_segments/continuous_time_segmentation.py</code> <pre><code>@staticmethod\ndef get_previous_date_last_segment(last_segments_pdf: pdDataFrame) -&gt; Dict:\n    \"\"\"\n    Gets last segment from the previous date. The input DataFrame is expected to contain only one row to be retrieved.\n\n    Args:\n        last_segments_pdf (pdDataFrame): Pandas DataFrame containing the user's previous date's final segment.\n\n    Returns:\n        Dict: dict of segment\n    \"\"\"\n    return last_segments_pdf.iloc[0][\n        [\n            ColNames.time_segment_id,\n            ColNames.start_timestamp,\n            ColNames.end_timestamp,\n            ColNames.cells,\n            ColNames.state,\n        ]\n    ].to_dict()\n</code></pre>"},{"location":"reference/components/execution/time_segments/continuous_time_segmentation/#components.execution.time_segments.continuous_time_segmentation.ContinuousTimeSegmentation.get_user_info_from_pdf","title":"<code>get_user_info_from_pdf(pdf)</code>  <code>staticmethod</code>","text":"<p>Gets user_id, user_id_modulo, mcc, mnc and plmn from Pandas DataFrame containing columns with the corresponding names. Values from the first row of the dataframe are used.</p> <p>Parameters:</p> Name Type Description Default <code>pdf</code> <code>DataFrame</code> <p>Pandas DataFrame</p> required <p>Returns:</p> Type Description <code>Tuple[str, int, str]</code> <p>Tuple[str, int, str]: user_id, user_id_modulo, mcc, mnc, plmn</p> Source code in <code>multimno/components/execution/time_segments/continuous_time_segmentation.py</code> <pre><code>@staticmethod\ndef get_user_info_from_pdf(pdf: pdDataFrame) -&gt; Tuple[str, int, str]:\n    \"\"\"\n    Gets user_id, user_id_modulo, mcc, mnc and plmn from Pandas DataFrame containing columns with the corresponding names.\n    Values from the first row of the dataframe are used.\n\n    Args:\n        pdf (pdDataFrame): Pandas DataFrame\n\n    Returns:\n        Tuple[str, int, str]: user_id, user_id_modulo, mcc, mnc, plmn\n    \"\"\"\n    user_id = pdf[ColNames.user_id][0]\n    user_id_mod = pdf[ColNames.user_id_modulo][0]\n    mcc = pdf[ColNames.mcc][0]\n    mnc = pdf[ColNames.mnc][0]\n    plmn = pdf[ColNames.plmn][0]\n    return user_id, user_id_mod, mcc, mnc, plmn\n</code></pre>"},{"location":"reference/components/execution/time_segments/continuous_time_segmentation/#components.execution.time_segments.continuous_time_segmentation.ContinuousTimeSegmentation.handle_first_segment","title":"<code>handle_first_segment(current_ts, event_timestamp, event_cell, current_date_start, max_time_missing_stay, max_time_missing_move, pad_time, user_id)</code>  <code>staticmethod</code>","text":"<p>Handles the first segment for a user for a date based on a previous date last segment.</p> <p>This method takes the last time segment of previous date and the timestamp of the first     event in the current date. It checks the state of the current time segment and the time difference between the end of the current time segment and the first event in the next date.</p> <p>If the state is 'undetermined' or 'stay' and the time difference is within the maximum missing stay time, or if the state is 'move' and the time difference is within the maximum missing move time, it creates a new time segment with the same cells, state. The start timestamp of the new time segment is the start of the current date, and the end timestamp     is the timestamp of the first event.</p> <p>If neither of these conditions are met, it creates a new time segment with     an empty list of cells, state 'unknown'. The start timestamp of the new time segment is the start of the next date,     and the end timestamp is the timestamp of the first event minus the padding time.</p> <p>Parameters: current_ts (Dict): The last time segment from previous date. event_timestamp (datetime): The timestamp of the first event in the current date. event_cell (int): The cell of the first event in the current date. current_date_start (datetime): The start of the current date. max_time_missing_stay (timedelta): The maximum time gap to consider continuation of a 'stay'. max_time_missing_move (timedelta): The maximum time gap to consider continuation of a 'move'. pad_time (timedelta): The padding time to have between 'unknown' segment.</p> <p>Returns: dict: The new first time segment for the current date.</p> Source code in <code>multimno/components/execution/time_segments/continuous_time_segmentation.py</code> <pre><code>@staticmethod\ndef handle_first_segment(\n    current_ts: Dict,\n    event_timestamp: datetime,\n    event_cell: int,\n    current_date_start: datetime,\n    max_time_missing_stay: timedelta,\n    max_time_missing_move: timedelta,\n    pad_time: timedelta,\n    user_id: str,\n) -&gt; dict:\n    \"\"\"\n    Handles the first segment for a user for a date based on a previous date last segment.\n\n    This method takes the last time segment of previous date and the timestamp of the first\n        event in the current date.\n    It checks the state of the current time segment and the time difference between the end of the current\n    time segment and the first event in the next date.\n\n    If the state is 'undetermined' or 'stay' and the time difference is within the maximum missing stay time,\n    or if the state is 'move' and the time difference is within the maximum missing move time, it creates a new\n    time segment with the same cells, state. The start timestamp\n    of the new time segment is the start of the current date, and the end timestamp\n        is the timestamp of the first event.\n\n    If neither of these conditions are met, it creates a new time segment with\n        an empty list of cells, state 'unknown'.\n    The start timestamp of the new time segment is the start of the next date,\n        and the end timestamp is the timestamp of the first event minus the padding time.\n\n    Parameters:\n    current_ts (Dict): The last time segment from previous date.\n    event_timestamp (datetime): The timestamp of the first event in the current date.\n    event_cell (int): The cell of the first event in the current date.\n    current_date_start (datetime): The start of the current date.\n    max_time_missing_stay (timedelta): The maximum time gap to consider continuation of a 'stay'.\n    max_time_missing_move (timedelta): The maximum time gap to consider continuation of a 'move'.\n    pad_time (timedelta): The padding time to have between 'unknown' segment.\n\n    Returns:\n    dict: The new first time segment for the current date.\n    \"\"\"\n    # TODO Verify if logic is consistent with non-first segments\n    if (\n        current_ts[ColNames.state] in [\"undetermined\", \"stay\"]\n        and event_timestamp - current_ts[ColNames.end_timestamp] &lt;= max_time_missing_stay\n    ):\n        next_ts = ContinuousTimeSegmentation.create_time_segment(\n            current_date_start,\n            event_timestamp,\n            list(set(current_ts[ColNames.cells] + [event_cell])),\n            current_ts[ColNames.state],\n            user_id,\n        )\n    elif (\n        current_ts[ColNames.state] == \"move\"\n        and event_timestamp - current_ts[ColNames.end_timestamp] &lt;= max_time_missing_move\n    ):\n        next_ts = ContinuousTimeSegmentation.create_time_segment(\n            current_date_start,\n            event_timestamp,\n            list(set(current_ts[ColNames.cells] + [event_cell])),\n            current_ts[ColNames.state],\n            user_id,\n        )\n    else:\n        pad_time = timedelta(seconds=0) if event_timestamp - current_date_start &lt; pad_time else pad_time\n        next_ts = ContinuousTimeSegmentation.create_time_segment(\n            current_date_start,\n            event_timestamp - pad_time,\n            [],\n            \"unknown\",\n            user_id,\n        )\n\n    return next_ts\n</code></pre>"},{"location":"reference/components/execution/tourism_stays_estimation/","title":"tourism_stays_estimation","text":""},{"location":"reference/components/execution/tourism_stays_estimation/tourism_stays_estimation/","title":"tourism_stays_estimation","text":"<p>Module that implements the Tourism Stays Estimation functionality.</p>"},{"location":"reference/components/execution/tourism_stays_estimation/tourism_stays_estimation/#components.execution.tourism_stays_estimation.tourism_stays_estimation.TourismStaysEstimation","title":"<code>TourismStaysEstimation</code>","text":"<p>               Bases: <code>Component</code></p> <p>A class to calculate geozones for inbound time segments.</p> Source code in <code>multimno/components/execution/tourism_stays_estimation/tourism_stays_estimation.py</code> <pre><code>class TourismStaysEstimation(Component):\n    \"\"\"\n    A class to calculate geozones for inbound time segments.\n    \"\"\"\n\n    COMPONENT_ID = \"TourismStaysEstimation\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.data_period_start = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n        self.data_period_end = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n\n        self.data_period_dates = [\n            self.data_period_start + timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n        self.segment_states_to_include = [\"stay\"]\n        self.local_mcc = self.config.geteval(self.COMPONENT_ID, \"local_mcc\")\n        self.zoning_dataset_id = self.config.geteval(self.COMPONENT_ID, \"zoning_dataset_id\")\n        self.min_duration_segment_m = self.config.geteval(self.COMPONENT_ID, \"min_duration_segment_m\")\n        self.functional_midnight_h = self.config.geteval(self.COMPONENT_ID, \"functional_midnight_h\")\n        self.min_duration_segment_night_m = self.config.geteval(self.COMPONENT_ID, \"min_duration_segment_night_m\")\n\n    def initalize_data_objects(self):\n\n        self.filter_ue_segments = self.config.getboolean(self.COMPONENT_ID, \"filter_ue_segments\")\n\n        # Input\n        self.input_data_objects = {}\n\n        inputs = {\n            \"time_segments_silver\": SilverTimeSegmentsDataObject,\n            \"cell_connection_probabilities_data_silver\": SilverCellConnectionProbabilitiesDataObject,\n            \"geozones_grid_map_data_silver\": SilverGeozonesGridMapDataObject,\n        }\n\n        # If releveant, add usual environment labels data object to filter all devices that have a usual environment label\n        if self.filter_ue_segments:\n            inputs[\"usual_environment_labels_data_silver\"] = SilverUsualEnvironmentLabelsDataObject\n\n        for key, value in inputs.items():\n            path = self.config.get(CONFIG_SILVER_PATHS_KEY, key)\n            if check_if_data_path_exists(self.spark, path):\n                self.input_data_objects[value.ID] = value(self.spark, path)\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")\n                raise ValueError(f\"Invalid path for {value.ID}: {path}\")\n\n        # Output\n        self.output_data_objects = {}\n        self.output_daily_silver_tourism_stays_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"tourism_stays_estimation_silver\"\n        )\n        self.output_data_objects[SilverTourismStaysDataObject.ID] = SilverTourismStaysDataObject(\n            self.spark,\n            self.output_daily_silver_tourism_stays_path,\n        )\n\n        # Output clearing\n        clear_destination_directory = self.config.getboolean(self.COMPONENT_ID, \"clear_destination_directory\")\n        if clear_destination_directory:\n            delete_file_or_folder(self.spark, self.output_daily_silver_tourism_stays_path)\n\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n\n        # for every date in the data period, process segments and map them to zone ids\n        for current_date in self.data_period_dates:\n            self.logger.info(f\"Processing events for {current_date.strftime('%Y-%m-%d')}\")\n\n            # get list of inbound roamers from usual environment labels\n            if self.filter_ue_segments:\n                inbound_residents_df = (\n                    self.input_data_objects[SilverUsualEnvironmentLabelsDataObject.ID]\n                    .df.filter(\n                        ((F.col(ColNames.label) == \"ue\"))\n                        &amp; (F.col(ColNames.start_date) &lt;= current_date)\n                        &amp; (F.col(ColNames.end_date) &gt;= current_date)\n                    )\n                    .select(ColNames.user_id)\n                    .distinct()\n                )\n\n            self.current_date = current_date\n            self.current_time_segments_df = self.input_data_objects[SilverTimeSegmentsDataObject.ID].df.filter(\n                (F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(current_date))\n                &amp; (F.col(ColNames.state).isin(self.segment_states_to_include))\n                &amp; (F.col(ColNames.mcc) != self.local_mcc)\n                &amp; (\n                    (F.col(ColNames.end_timestamp).cast(\"long\") - F.col(ColNames.start_timestamp).cast(\"long\")) / 60.0\n                    &gt;= self.min_duration_segment_m\n                )\n            )\n\n            # filter out segments that are inbound residents\n            if self.filter_ue_segments:\n                self.current_time_segments_df = self.current_time_segments_df.join(\n                    inbound_residents_df, on=ColNames.user_id, how=\"left_anti\"\n                )\n\n            self.current_cell_connection_probabilities_df = self.input_data_objects[\n                SilverCellConnectionProbabilitiesDataObject.ID\n            ].df.filter(\n                (\n                    F.make_date(\n                        F.col(ColNames.year),\n                        F.col(ColNames.month),\n                        F.col(ColNames.day),\n                    )\n                    == F.lit(current_date)\n                )\n            )\n\n            self.current_geozones_grid_mapping_df = self.input_data_objects[\n                SilverGeozonesGridMapDataObject.ID\n            ].df.filter(F.col(ColNames.dataset_id) == self.zoning_dataset_id)\n            self.transform()\n            self.write()\n\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n\n        current_time_segments_df = self.current_time_segments_df\n        current_cell_connection_probabilities_df = self.current_cell_connection_probabilities_df\n        current_geozones_grid_mapping_df = self.current_geozones_grid_mapping_df\n\n        # TODO cache current_time_segments_df?\n        # Extract unique cells arrays among segments.\n        unique_cells_df = current_time_segments_df.select(F.col(ColNames.cells)).distinct()\n\n        # For each unique cells array, join to cell-grid dataframe to get grids with posterior probabilities.\n        # Normalize grid probabilities (divide probabilities by cell count) to sum to 1.\n        cells_arr_grid_prob_df = (\n            unique_cells_df.withColumn(ColNames.cell_id, F.explode(F.col(ColNames.cells)))\n            .alias(\"df1\")\n            .join(\n                current_cell_connection_probabilities_df.alias(\"df2\"),\n                on=ColNames.cell_id,\n                how=\"inner\",\n            )\n            .select(\n                \"df1.*\",\n                f\"df2.{ColNames.grid_id}\",\n                (F.col(f\"df2.{ColNames.posterior_probability}\") / F.size(f\"df1.{ColNames.cells}\")).alias(\"grid_weight\"),\n            )\n        )\n\n        # For each unique cells array, map to the lowest level of zone hierarchy and calculate zone weight as sum of grid weights in that zone.\n        cells_arr_zone_df = (\n            cells_arr_grid_prob_df.alias(\"df1\")\n            .join(current_geozones_grid_mapping_df, on=ColNames.grid_id, how=\"inner\")\n            .groupBy(ColNames.cells, ColNames.hierarchical_id)\n            .agg(\n                F.sum(F.col(\"grid_weight\")).cast(\"float\").alias(ColNames.zone_weight),\n            )\n        )\n\n        # Join unique cells array with zone mappings back to the original segments.\n        segments_with_zone_weights_df = (\n            current_time_segments_df.alias(\"df1\")\n            .join(cells_arr_zone_df.alias(\"df2\"), on=ColNames.cells, how=\"inner\")\n            .select(\n                \"df1.*\",\n                ColNames.hierarchical_id,\n                ColNames.zone_weight,\n            )\n        )\n\n        # Repartittion and sort\n        segments_with_zone_weights_df = segments_with_zone_weights_df.repartition(\n            *SilverTourismStaysDataObject.PARTITION_COLUMNS\n        ).sortWithinPartitions(ColNames.start_timestamp, ColNames.hierarchical_id)\n\n        # Mark segments that contain the functional midnight hour and are sufficiently long as overnight segments.\n        segments_with_zone_weights_df = segments_with_zone_weights_df.withColumn(\n            ColNames.is_overnight,\n            F.when(\n                (F.hour(F.col(f\"df1.{ColNames.start_timestamp}\")) &lt;= self.functional_midnight_h)\n                &amp; (F.hour(F.col(f\"df1.{ColNames.end_timestamp}\")) &gt; self.functional_midnight_h)\n                &amp; (\n                    ((F.col(ColNames.end_timestamp).cast(\"long\") - F.col(ColNames.start_timestamp).cast(\"long\")) / 60.0)\n                    &gt;= self.min_duration_segment_night_m\n                ),\n                F.lit(True),\n            ).otherwise(F.lit(False)),\n        )\n\n        # Keep one row per segment, aggregating zone ids and weights.\n        segments_with_zone_weights_df = segments_with_zone_weights_df.groupBy(\n            ColNames.user_id,\n            ColNames.time_segment_id,\n            ColNames.start_timestamp,\n            ColNames.end_timestamp,\n            ColNames.is_overnight,\n            ColNames.mcc,\n            ColNames.mnc,\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            ColNames.user_id_modulo,\n        ).agg(\n            F.collect_list(ColNames.hierarchical_id).alias(ColNames.zone_ids_list),\n            F.collect_list(ColNames.zone_weight).alias(ColNames.zone_weights_list),\n        )\n\n        segments_with_zone_weights_df = apply_schema_casting(\n            segments_with_zone_weights_df, SilverTourismStaysDataObject.SCHEMA\n        )\n\n        # Prepare output\n        self.output_data_objects[SilverTourismStaysDataObject.ID].df = segments_with_zone_weights_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_aggregation/","title":"usual_environment_aggregation","text":""},{"location":"reference/components/execution/usual_environment_aggregation/usual_environment_aggregation/","title":"usual_environment_aggregation","text":"<p>This module is responsible for usual environment and location labels aggregation to reference grid</p>"},{"location":"reference/components/execution/usual_environment_aggregation/usual_environment_aggregation/#components.execution.usual_environment_aggregation.usual_environment_aggregation.UsualEnvironmentAggregation","title":"<code>UsualEnvironmentAggregation</code>","text":"<p>               Bases: <code>Component</code></p> <p>A class to aggregate devices usual environment and location labels to reference grid.</p> Source code in <code>multimno/components/execution/usual_environment_aggregation/usual_environment_aggregation.py</code> <pre><code>class UsualEnvironmentAggregation(Component):\n    \"\"\"\n    A class to aggregate devices usual environment and location labels to reference grid.\n    \"\"\"\n\n    COMPONENT_ID = \"UsualEnvironmentAggregation\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        # Months that compose the long-term period, at least one\n        start_month = self.config.get(self.COMPONENT_ID, \"start_month\")\n        try:\n            self.start_date = dt.datetime.strptime(start_month, \"%Y-%m\").date()\n        except ValueError as e:\n            self.logger.error(f\"Could not parse start_month = `{start_month}`. Expected format: YYYY-MM\")\n            raise e\n\n        end_month = self.config.get(self.COMPONENT_ID, \"end_month\")\n        try:\n            self.end_date = dt.datetime.strptime(end_month, \"%Y-%m\").date()\n        except ValueError as e:\n            self.logger.error(f\"Could not parse end_month = `{end_month}`. Expected format: YYYY-MM\")\n            raise e\n        self.end_date = self.end_date.replace(day=cal.monthrange(self.end_date.year, self.end_date.month)[1])\n\n        if self.end_date &lt; self.start_date:\n            raise ValueError(f\"End month `{end_month}` should not be an earlier than start month `{start_month}`\")\n\n        self.uniform_tile_weights = self.config.getboolean(\n            UsualEnvironmentAggregation.COMPONENT_ID, \"uniform_tile_weights\"\n        )\n\n        self.season = self.config.get(self.COMPONENT_ID, \"season\")\n        if self.season not in SEASONS:\n            error_msg = f\"season: expected one of: {', '.join(SEASONS)} - found: {self.season}\"\n            self.logger.error(error_msg)\n            raise ValueError(error_msg)\n\n    def initalize_data_objects(self):\n\n        # inputs\n        self.clear_destination_directory = self.config.getboolean(\n            UsualEnvironmentAggregation.COMPONENT_ID, \"clear_destination_directory\"\n        )\n\n        self.input_data_objects = {}\n        # Check uniform for getting the grid or the enriched grid data\n        uniform_tile_weights = self.config.getboolean(UsualEnvironmentAggregation.COMPONENT_ID, \"uniform_tile_weights\")\n\n        if uniform_tile_weights:\n            inputs = {\n                \"grid_data_silver\": SilverGridDataObject,\n                \"usual_environment_labels_data_silver\": SilverUsualEnvironmentLabelsDataObject,\n            }\n        else:\n            inputs = {\n                \"enriched_grid_data_silver\": SilverEnrichedGridDataObject,\n                \"usual_environment_labels_data_silver\": SilverUsualEnvironmentLabelsDataObject,\n            }\n\n        for key, value in inputs.items():\n            path = self.config.get(CONFIG_SILVER_PATHS_KEY, key)\n            if check_if_data_path_exists(self.spark, path):\n                self.input_data_objects[value.ID] = value(self.spark, path)\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")\n                raise ValueError(f\"Invalid path for {value.ID}: {path}\")\n\n        # outputs\n\n        output_do_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"aggregated_usual_environments_silver\")\n\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, output_do_path)\n\n        self.output_data_objects = {}\n        self.output_data_objects[SilverAggregatedUsualEnvironmentsDataObject.ID] = (\n            SilverAggregatedUsualEnvironmentsDataObject(\n                self.spark, output_do_path, [ColNames.start_date, ColNames.end_date, ColNames.season]\n            )\n        )\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}...\")\n\n        # prepare grid with tile weights\n        if self.uniform_tile_weights:\n            grid_sdf = self.input_data_objects[SilverGridDataObject.ID].df\n        else:\n            grid_sdf = self.input_data_objects[SilverEnrichedGridDataObject.ID].df\n            grid_sdf = grid_sdf.select(\n                ColNames.grid_id,\n                ColNames.prior_probability,\n            )\n\n        grid_sdf = self.assign_tile_weights(grid_sdf, self.uniform_tile_weights)\n\n        # prepare usual environment labels\n        ue_labels_sdf = self.input_data_objects[SilverUsualEnvironmentLabelsDataObject.ID].df\n        ue_labels_sdf = ue_labels_sdf.filter(\n            (F.col(ColNames.start_date) == F.lit(self.start_date))\n            &amp; (F.col(ColNames.end_date) == F.lit(self.end_date))\n            &amp; (F.col(ColNames.season) == F.lit(self.season))\n        )\n        ue_labels_sdf = ue_labels_sdf.select(\n            ColNames.user_id, ColNames.grid_id, ColNames.label, ColNames.user_id_modulo\n        )\n        # aggregate usual environments\n        aggregated_ue_sdf = self.aggregate_usual_environments(ue_labels_sdf, grid_sdf)\n\n        # aggreagate location labels\n        aggregated_home_labels_sdf = self.aggregate_location_labels(ue_labels_sdf, grid_sdf, \"home\")\n\n        aggregated_work_labels_sdf = self.aggregate_location_labels(ue_labels_sdf, grid_sdf, \"work\")\n\n        # union all aggregated results\n        aggregated_results_sdf = reduce(\n            lambda df1, df2: df1.union(df2), [aggregated_ue_sdf, aggregated_home_labels_sdf, aggregated_work_labels_sdf]\n        )\n\n        # Cast column types to DO schema, add missing columns manually\n        aggregated_results_sdf = (\n            aggregated_results_sdf.withColumn(ColNames.start_date, F.lit(self.start_date))\n            .withColumn(ColNames.end_date, F.lit(self.end_date))\n            .withColumn(ColNames.season, F.lit(self.season))\n        )\n\n        aggregated_results_sdf = utils.apply_schema_casting(\n            aggregated_results_sdf, SilverAggregatedUsualEnvironmentsDataObject.SCHEMA\n        )\n\n        aggregated_results_sdf = aggregated_results_sdf.repartition(\n            *SilverAggregatedUsualEnvironmentsDataObject.PARTITION_COLUMNS\n        )\n\n        self.output_data_objects[SilverAggregatedUsualEnvironmentsDataObject.ID].df = aggregated_results_sdf\n\n    def assign_tile_weights(self, grid_sdf: DataFrame, uniform_tile_weights: bool) -&gt; DataFrame:\n        \"\"\"\n        Assigns weights to each tile in a DataFrame based on the specified weighting strategy.\n\n        This method updates the input DataFrame by adding a new column that contains the weight of each tile.\n        The weighting strategy is determined by the `uniform_tile_weights` parameter. If `uniform_tile_weights` is True,\n        all tiles are assigned a uniform weight of 1.0. Otherwise, the tile weights are set to the values derrived from landuse\n        information in an input grid data object.\n\n        Parameters:\n        - grid_sdf (DataFrame): The input grid DataFrame.\n        - uniform_tile_weights (bool): A flag indicating whether to assign uniform weights to all tiles (True)\n        or to use the values from grid data object column as weights (False).\n\n        Returns:\n        - DataFrame: The updated DataFrame with a new column containing the weights of each tile.\n        \"\"\"\n        if uniform_tile_weights:\n            grid_sdf = grid_sdf.withColumn(ColNames.tile_weight, F.lit(1.0))\n        else:\n            grid_sdf = grid_sdf.withColumn(ColNames.tile_weight, F.col(ColNames.prior_probability))\n\n        return grid_sdf\n\n    def get_device_tile_weights(self, ue_labels_sdf: DataFrame, grid_sdf: DataFrame, label: str = \"ue\") -&gt; DataFrame:\n        \"\"\"\n        Calculates and assigns weights to each device UE tiles based on the tile weights in the grid DataFrame.\n\n        This method performs a join operation between ue labels DataFrame and a grid DataFrame based on grid IDs.\n        It then calculates the weight of each device tile as the ratio of the tile's weight to the sum of all tile weights\n        for the same user for a given label.\n\n        If a label is specified, the method first filters the UE labels DataFrame to include only rows with the matching label\n        before proceeding with the join and weight calculation.\n\n        If a label is not specified, the method uses all tiles of a device to calculate the weights.\n\n        Parameters:\n        - ue_labels_sdf (DataFrame): The DataFrame containing usual environment and location labels for each device.\n        - grid_sdf (DataFrame): The DataFrame containing grid information, including grid IDs and tile weights.\n        - label (str, optional): A specific label to filter the UE labels DataFrame.\n\n        Returns:\n        - DataFrame: A DataFrame containing the calculated weights for each device tile.\n        \"\"\"\n\n        if label == \"ue\":\n            ue_labels_sdf = ue_labels_sdf.filter(F.col(ColNames.ue_label_rule) != F.lit(\"ue_na\"))\n\n            # as the same tile can be labeled into multiple label categories (e.g. home and work), we need to remove duplicates for ue weights\n            ue_labels_sdf = ue_labels_sdf.dropDuplicates([ColNames.user_id, ColNames.grid_id, ColNames.user_id_modulo])\n        else:\n            ue_labels_sdf = ue_labels_sdf.filter(\n                (F.col(ColNames.label) == F.lit(label)) &amp; (F.col(ColNames.location_label_rule) != F.lit(\"loc_na\"))\n            )\n\n        ue_labels_sdf = ue_labels_sdf.join(grid_sdf, on=ColNames.grid_id, how=\"inner\")\n\n        window_spec = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id)\n\n        ue_labels_sdf = ue_labels_sdf.withColumn(\n            ColNames.device_tile_weight, F.col(ColNames.tile_weight) / F.sum(ColNames.tile_weight).over(window_spec)\n        )\n\n        return ue_labels_sdf\n\n    def aggregate_usual_environments(self, ue_labels_sdf: DataFrame, grid_sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Aggregates usual environment by grid ID and calculates the sum of weighted device count.\n\n        This method first calculates device tile weights for usual environment tiles.\n        It then aggregates these weights by grid ID to compute the total weighted device count for each grid.\n        Finally, it assigns a label \"ue\" to all aggregated entries to indicate their association with usual environments.\n\n        Parameters:\n        - ue_labels_sdf (DataFrame): The DataFrame containing ue labels.\n        - grid_sdf (DataFrame): The DataFrame containing grid information with tile weights.\n\n        Returns:\n        - DataFrame: A DataFrame with sum of weighted device count per grid tile.\n        \"\"\"\n        ue_with_device_weights_sdf = self.get_device_tile_weights(ue_labels_sdf, grid_sdf)\n\n        aggregated_ue_sdf = ue_with_device_weights_sdf.groupBy(ColNames.grid_id).agg(\n            F.sum(ColNames.device_tile_weight).alias(ColNames.weighted_device_count)\n        )\n\n        aggregated_ue_sdf = aggregated_ue_sdf.withColumn(ColNames.label, F.lit(\"ue\"))\n\n        return aggregated_ue_sdf\n\n    def aggregate_location_labels(self, ue_labels_sdf: DataFrame, grid_sdf: DataFrame, label: str) -&gt; DataFrame:\n        \"\"\"\n        Aggregates location labels by grid ID and calculates the sum of weighted device count.\n\n        This method first calculates device tile weights for location label tiles.\n        It then aggregates these weights by grid ID to compute the total weighted device count for each grid for each label.\n        Finally, it assigns corresponding label name to all aggregated entries to indicate their association with usual environments.\n\n        Parameters:\n        - ue_labels_sdf (DataFrame): The DataFrame containing ue labels.\n        - grid_sdf (DataFrame): The DataFrame containing grid information with tile weights.\n\n        Returns:\n        - DataFrame: A DataFrame with sum of weighted device count per grid tile.\n        \"\"\"\n\n        loc_label_with_device_weights_sdf = self.get_device_tile_weights(ue_labels_sdf, grid_sdf, label)\n\n        aggregated_labels_sdf = loc_label_with_device_weights_sdf.groupBy(ColNames.grid_id).agg(\n            F.sum(ColNames.device_tile_weight).alias(ColNames.weighted_device_count)\n        )\n\n        aggregated_labels_sdf = aggregated_labels_sdf.withColumn(ColNames.label, F.lit(label))\n\n        return aggregated_labels_sdf\n</code></pre>"},{"location":"reference/components/execution/usual_environment_aggregation/usual_environment_aggregation/#components.execution.usual_environment_aggregation.usual_environment_aggregation.UsualEnvironmentAggregation.aggregate_location_labels","title":"<code>aggregate_location_labels(ue_labels_sdf, grid_sdf, label)</code>","text":"<p>Aggregates location labels by grid ID and calculates the sum of weighted device count.</p> <p>This method first calculates device tile weights for location label tiles. It then aggregates these weights by grid ID to compute the total weighted device count for each grid for each label. Finally, it assigns corresponding label name to all aggregated entries to indicate their association with usual environments.</p> <p>Parameters: - ue_labels_sdf (DataFrame): The DataFrame containing ue labels. - grid_sdf (DataFrame): The DataFrame containing grid information with tile weights.</p> <p>Returns: - DataFrame: A DataFrame with sum of weighted device count per grid tile.</p> Source code in <code>multimno/components/execution/usual_environment_aggregation/usual_environment_aggregation.py</code> <pre><code>def aggregate_location_labels(self, ue_labels_sdf: DataFrame, grid_sdf: DataFrame, label: str) -&gt; DataFrame:\n    \"\"\"\n    Aggregates location labels by grid ID and calculates the sum of weighted device count.\n\n    This method first calculates device tile weights for location label tiles.\n    It then aggregates these weights by grid ID to compute the total weighted device count for each grid for each label.\n    Finally, it assigns corresponding label name to all aggregated entries to indicate their association with usual environments.\n\n    Parameters:\n    - ue_labels_sdf (DataFrame): The DataFrame containing ue labels.\n    - grid_sdf (DataFrame): The DataFrame containing grid information with tile weights.\n\n    Returns:\n    - DataFrame: A DataFrame with sum of weighted device count per grid tile.\n    \"\"\"\n\n    loc_label_with_device_weights_sdf = self.get_device_tile_weights(ue_labels_sdf, grid_sdf, label)\n\n    aggregated_labels_sdf = loc_label_with_device_weights_sdf.groupBy(ColNames.grid_id).agg(\n        F.sum(ColNames.device_tile_weight).alias(ColNames.weighted_device_count)\n    )\n\n    aggregated_labels_sdf = aggregated_labels_sdf.withColumn(ColNames.label, F.lit(label))\n\n    return aggregated_labels_sdf\n</code></pre>"},{"location":"reference/components/execution/usual_environment_aggregation/usual_environment_aggregation/#components.execution.usual_environment_aggregation.usual_environment_aggregation.UsualEnvironmentAggregation.aggregate_usual_environments","title":"<code>aggregate_usual_environments(ue_labels_sdf, grid_sdf)</code>","text":"<p>Aggregates usual environment by grid ID and calculates the sum of weighted device count.</p> <p>This method first calculates device tile weights for usual environment tiles. It then aggregates these weights by grid ID to compute the total weighted device count for each grid. Finally, it assigns a label \"ue\" to all aggregated entries to indicate their association with usual environments.</p> <p>Parameters: - ue_labels_sdf (DataFrame): The DataFrame containing ue labels. - grid_sdf (DataFrame): The DataFrame containing grid information with tile weights.</p> <p>Returns: - DataFrame: A DataFrame with sum of weighted device count per grid tile.</p> Source code in <code>multimno/components/execution/usual_environment_aggregation/usual_environment_aggregation.py</code> <pre><code>def aggregate_usual_environments(self, ue_labels_sdf: DataFrame, grid_sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Aggregates usual environment by grid ID and calculates the sum of weighted device count.\n\n    This method first calculates device tile weights for usual environment tiles.\n    It then aggregates these weights by grid ID to compute the total weighted device count for each grid.\n    Finally, it assigns a label \"ue\" to all aggregated entries to indicate their association with usual environments.\n\n    Parameters:\n    - ue_labels_sdf (DataFrame): The DataFrame containing ue labels.\n    - grid_sdf (DataFrame): The DataFrame containing grid information with tile weights.\n\n    Returns:\n    - DataFrame: A DataFrame with sum of weighted device count per grid tile.\n    \"\"\"\n    ue_with_device_weights_sdf = self.get_device_tile_weights(ue_labels_sdf, grid_sdf)\n\n    aggregated_ue_sdf = ue_with_device_weights_sdf.groupBy(ColNames.grid_id).agg(\n        F.sum(ColNames.device_tile_weight).alias(ColNames.weighted_device_count)\n    )\n\n    aggregated_ue_sdf = aggregated_ue_sdf.withColumn(ColNames.label, F.lit(\"ue\"))\n\n    return aggregated_ue_sdf\n</code></pre>"},{"location":"reference/components/execution/usual_environment_aggregation/usual_environment_aggregation/#components.execution.usual_environment_aggregation.usual_environment_aggregation.UsualEnvironmentAggregation.assign_tile_weights","title":"<code>assign_tile_weights(grid_sdf, uniform_tile_weights)</code>","text":"<p>Assigns weights to each tile in a DataFrame based on the specified weighting strategy.</p> <p>This method updates the input DataFrame by adding a new column that contains the weight of each tile. The weighting strategy is determined by the <code>uniform_tile_weights</code> parameter. If <code>uniform_tile_weights</code> is True, all tiles are assigned a uniform weight of 1.0. Otherwise, the tile weights are set to the values derrived from landuse information in an input grid data object.</p> <p>Parameters: - grid_sdf (DataFrame): The input grid DataFrame. - uniform_tile_weights (bool): A flag indicating whether to assign uniform weights to all tiles (True) or to use the values from grid data object column as weights (False).</p> <p>Returns: - DataFrame: The updated DataFrame with a new column containing the weights of each tile.</p> Source code in <code>multimno/components/execution/usual_environment_aggregation/usual_environment_aggregation.py</code> <pre><code>def assign_tile_weights(self, grid_sdf: DataFrame, uniform_tile_weights: bool) -&gt; DataFrame:\n    \"\"\"\n    Assigns weights to each tile in a DataFrame based on the specified weighting strategy.\n\n    This method updates the input DataFrame by adding a new column that contains the weight of each tile.\n    The weighting strategy is determined by the `uniform_tile_weights` parameter. If `uniform_tile_weights` is True,\n    all tiles are assigned a uniform weight of 1.0. Otherwise, the tile weights are set to the values derrived from landuse\n    information in an input grid data object.\n\n    Parameters:\n    - grid_sdf (DataFrame): The input grid DataFrame.\n    - uniform_tile_weights (bool): A flag indicating whether to assign uniform weights to all tiles (True)\n    or to use the values from grid data object column as weights (False).\n\n    Returns:\n    - DataFrame: The updated DataFrame with a new column containing the weights of each tile.\n    \"\"\"\n    if uniform_tile_weights:\n        grid_sdf = grid_sdf.withColumn(ColNames.tile_weight, F.lit(1.0))\n    else:\n        grid_sdf = grid_sdf.withColumn(ColNames.tile_weight, F.col(ColNames.prior_probability))\n\n    return grid_sdf\n</code></pre>"},{"location":"reference/components/execution/usual_environment_aggregation/usual_environment_aggregation/#components.execution.usual_environment_aggregation.usual_environment_aggregation.UsualEnvironmentAggregation.get_device_tile_weights","title":"<code>get_device_tile_weights(ue_labels_sdf, grid_sdf, label='ue')</code>","text":"<p>Calculates and assigns weights to each device UE tiles based on the tile weights in the grid DataFrame.</p> <p>This method performs a join operation between ue labels DataFrame and a grid DataFrame based on grid IDs. It then calculates the weight of each device tile as the ratio of the tile's weight to the sum of all tile weights for the same user for a given label.</p> <p>If a label is specified, the method first filters the UE labels DataFrame to include only rows with the matching label before proceeding with the join and weight calculation.</p> <p>If a label is not specified, the method uses all tiles of a device to calculate the weights.</p> <p>Parameters: - ue_labels_sdf (DataFrame): The DataFrame containing usual environment and location labels for each device. - grid_sdf (DataFrame): The DataFrame containing grid information, including grid IDs and tile weights. - label (str, optional): A specific label to filter the UE labels DataFrame.</p> <p>Returns: - DataFrame: A DataFrame containing the calculated weights for each device tile.</p> Source code in <code>multimno/components/execution/usual_environment_aggregation/usual_environment_aggregation.py</code> <pre><code>def get_device_tile_weights(self, ue_labels_sdf: DataFrame, grid_sdf: DataFrame, label: str = \"ue\") -&gt; DataFrame:\n    \"\"\"\n    Calculates and assigns weights to each device UE tiles based on the tile weights in the grid DataFrame.\n\n    This method performs a join operation between ue labels DataFrame and a grid DataFrame based on grid IDs.\n    It then calculates the weight of each device tile as the ratio of the tile's weight to the sum of all tile weights\n    for the same user for a given label.\n\n    If a label is specified, the method first filters the UE labels DataFrame to include only rows with the matching label\n    before proceeding with the join and weight calculation.\n\n    If a label is not specified, the method uses all tiles of a device to calculate the weights.\n\n    Parameters:\n    - ue_labels_sdf (DataFrame): The DataFrame containing usual environment and location labels for each device.\n    - grid_sdf (DataFrame): The DataFrame containing grid information, including grid IDs and tile weights.\n    - label (str, optional): A specific label to filter the UE labels DataFrame.\n\n    Returns:\n    - DataFrame: A DataFrame containing the calculated weights for each device tile.\n    \"\"\"\n\n    if label == \"ue\":\n        ue_labels_sdf = ue_labels_sdf.filter(F.col(ColNames.ue_label_rule) != F.lit(\"ue_na\"))\n\n        # as the same tile can be labeled into multiple label categories (e.g. home and work), we need to remove duplicates for ue weights\n        ue_labels_sdf = ue_labels_sdf.dropDuplicates([ColNames.user_id, ColNames.grid_id, ColNames.user_id_modulo])\n    else:\n        ue_labels_sdf = ue_labels_sdf.filter(\n            (F.col(ColNames.label) == F.lit(label)) &amp; (F.col(ColNames.location_label_rule) != F.lit(\"loc_na\"))\n        )\n\n    ue_labels_sdf = ue_labels_sdf.join(grid_sdf, on=ColNames.grid_id, how=\"inner\")\n\n    window_spec = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id)\n\n    ue_labels_sdf = ue_labels_sdf.withColumn(\n        ColNames.device_tile_weight, F.col(ColNames.tile_weight) / F.sum(ColNames.tile_weight).over(window_spec)\n    )\n\n    return ue_labels_sdf\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/","title":"usual_environment_labeling","text":""},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/","title":"usual_environment_labeling","text":"<p>Module that implements the Usual Environment Labeling functionality</p>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling","title":"<code>UsualEnvironmentLabeling</code>","text":"<p>               Bases: <code>Component</code></p> <p>A class to calculate the grid tiles that conform the usual environment, home and work locations of each user.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>class UsualEnvironmentLabeling(Component):\n    \"\"\"\n    A class to calculate the grid tiles that conform the usual environment, home and work locations of each user.\n    \"\"\"\n\n    COMPONENT_ID = \"UsualEnvironmentLabeling\"\n    LABEL_TO_LABELNAMES = {\"ue\": \"no_label\", \"home\": \"home\", \"work\": \"work\"}\n    LABEL_TO_SHORT_LABELNAMES = {\"ue\": \"ue\", \"home\": \"h\", \"work\": \"w\"}\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.gap_ps_thresholds = {\n            \"ue\": self.config.getfloat(self.COMPONENT_ID, \"ue_gap_ps_threshold\"),\n            \"home\": self.config.getint(self.COMPONENT_ID, \"gap_ps_threshold\"),\n            \"work\": self.config.getint(self.COMPONENT_ID, \"gap_ps_threshold\"),\n        }\n\n        self.gap_ps_threshold_is_absolute = {\n            \"ue\": False,\n            \"home\": True,\n            \"work\": True,\n        }  # TODO:\n\n        self.ps_thresholds = {\n            \"ue\": self.config.getfloat(self.COMPONENT_ID, \"ue_ps_threshold\"),\n            \"home\": self.config.getfloat(self.COMPONENT_ID, \"home_ps_threshold\"),\n            \"work\": self.config.getfloat(self.COMPONENT_ID, \"work_ps_threshold\"),\n        }\n\n        self.freq_thresholds = {\n            \"ue\": self.config.getfloat(self.COMPONENT_ID, \"ue_ndays_threshold\"),\n            \"home\": self.config.getfloat(self.COMPONENT_ID, \"home_ndays_threshold\"),\n            \"work\": self.config.getfloat(self.COMPONENT_ID, \"work_ndays_threshold\"),\n        }\n\n        self.ps_threshold_for_rare_devices = self.config.getfloat(self.COMPONENT_ID, \"total_ps_threshold\")\n        self.freq_threshold_for_discontinuous_devices = self.config.getfloat(self.COMPONENT_ID, \"total_ndays_threshold\")\n\n        self.day_and_interval_type_combinations = {\n            \"ue\": [(\"all\", \"all\"), (\"all\", \"night_time\"), (\"workdays\", \"working_hours\")],\n            \"home\": [(\"all\", \"all\"), (\"all\", \"night_time\")],\n            \"work\": [(\"workdays\", \"working_hours\")],\n        }\n\n        self.season = self.config.get(self.COMPONENT_ID, \"season\")\n        if self.season not in SEASONS:\n            error_msg = f\"season: expected one of: {', '.join(SEASONS)} - found: {self.season}\"\n            self.logger.error(error_msg)\n            raise ValueError(error_msg)\n\n        # Months that compose the long-term period, at least one\n        start_month = self.config.get(self.COMPONENT_ID, \"start_month\")\n        try:\n            self.start_date = dt.datetime.strptime(start_month, \"%Y-%m\").date()\n        except ValueError as e:\n            self.logger.error(f\"Could not parse start_month = `{start_month}`. Expected format: YYYY-MM\")\n            raise e\n\n        end_month = self.config.get(self.COMPONENT_ID, \"end_month\")\n        try:\n            self.end_date = dt.datetime.strptime(end_month, \"%Y-%m\").date()\n        except ValueError as e:\n            self.logger.error(f\"Could not parse end_month = `{end_month}`. Expected format: YYYY-MM\")\n            raise e\n        self.end_date = self.end_date.replace(day=cal.monthrange(self.end_date.year, self.end_date.month)[1])\n\n        if self.end_date &lt; self.start_date:\n            raise ValueError(f\"End month `{end_month}` should not be earlier than start month `{start_month}`\")\n\n        self.ltps_df: DataFrame = None\n        self.rare_devices_count = 0\n        self.discontinuous_devices_count = 0\n        self.disaggregate_to_100m_grid = self.config.getboolean(\n            self.COMPONENT_ID, \"disaggregate_to_100m_grid\", fallback=False\n        )\n        self.grid_gen = InspireGridGenerator(self.spark)\n\n    def initalize_data_objects(self):\n        # Load paths from configuration file:\n        input_ltps_silver_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"longterm_permanence_score_data_silver\")\n        output_uelabels_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"usual_environment_labels_data_silver\")\n        output_quality_metrics_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"usual_environment_labeling_quality_metrics_data_silver\"\n        )\n\n        # Initialise input and output data objects:\n        silver_ltps = SilverLongtermPermanenceScoreDataObject(self.spark, input_ltps_silver_path)\n        ue_labels = SilverUsualEnvironmentLabelsDataObject(self.spark, output_uelabels_path)\n        ue_quality_metrics = SilverUsualEnvironmentLabelingQualityMetricsDataObject(\n            self.spark, output_quality_metrics_path\n        )\n\n        # Store data objects in the corresponding attributes:\n        self.input_data_objects = {silver_ltps.ID: silver_ltps}\n        self.output_data_objects = {ue_labels.ID: ue_labels, ue_quality_metrics.ID: ue_quality_metrics}\n\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        # read input data object:\n        self.read()\n        full_ltps_df = self.input_data_objects[SilverLongtermPermanenceScoreDataObject.ID].df\n        # filtering to obtain the main dataset which this method will work with:\n        self.ltps_df = self.filter_ltps_by_target_dates(full_ltps_df, self.start_date, self.end_date, self.season)\n        # assert that all the needed day type and interval times are available in the main dataset:\n        self.check_needed_day_and_interval_types(self.ltps_df, self.day_and_interval_type_combinations)\n        partition_chunks = self._get_partition_chunks()\n        for i, partition_chunk in enumerate(partition_chunks):\n            self.logger.info(f\"Processing partition chunk: {i}\")\n            self.logger.debug(f\"Partition chunk: {partition_chunk}\")\n            self.partition_chunk = partition_chunk\n            # main transformations of this method:\n            self.transform()\n            # write output data objects:\n            self.write()\n            self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    @staticmethod\n    def filter_ltps_by_target_dates(\n        full_ltps_df: DataFrame, start_date: dt.date, end_date: dt.date, season: str\n    ) -&gt; DataFrame:\n        \"\"\"\n        Keep only the rows of the input SilverLongtermPermanenceScoreDataObject in which the start\n        date, end date and season match the ones specified for the processing of this method.\n\n        Args:\n            full_ltps_df (DataFrame): full dataset.\n            start_date (dt.date): specified target start date for the execution of the method.\n            end_date (dt.date): specified target end date for the execution of the method.\n            season (str): specified target season for the execution of the method.\n\n        Returns:\n            DataFrame: filtered dataset.\n        \"\"\"\n        filtered_ltps_df = full_ltps_df.filter(\n            (F.col(ColNames.start_date) == start_date)\n            &amp; (F.col(ColNames.end_date) == end_date)\n            &amp; (F.col(ColNames.season) == season)\n        ).select(\n            ColNames.user_id,\n            ColNames.grid_id,\n            ColNames.lps,\n            ColNames.total_frequency,\n            ColNames.day_type,\n            ColNames.time_interval,\n            ColNames.user_id_modulo,\n            ColNames.id_type,\n        )\n        return filtered_ltps_df\n\n    @staticmethod\n    def check_needed_day_and_interval_types(\n        ltps_df: DataFrame, day_and_interval_type_combinations: Dict[str, List[Tuple[str, str]]]\n    ):\n        \"\"\"\n        Method that checks if the needed combinations of day type and interval type are available\n        in the provided dataset.\n\n        Args:\n            ltps_df (DataFrame): provided Long-Term Permanence Score dataset\n            day_and_interval_type_combinations (Dict[str,List[Tuple[str,str]]]): day type and interval type\n                combinations that are needed for the execution of the method.\n\n        Raises:\n            FileNotFoundError: If there is no data for one or more of the needed combinations of day type\n                and interval type.\n        \"\"\"\n        # Build set of all needed day-interval type combinations:\n        all_day_and_interval_type_combinations = {\n            comb for el in day_and_interval_type_combinations.values() for comb in el\n        }\n\n        # Assert that these combinations appear at least once in the input Long-Term Permanence\n        # Score data object:\n        for day_type, time_interval in all_day_and_interval_type_combinations:\n\n            filtered_ltps_df = ltps_df.filter(\n                (F.col(ColNames.day_type) == day_type) &amp; (F.col(ColNames.time_interval) == time_interval)\n            )\n\n            data_exists = filtered_ltps_df.count() &gt; 0\n            if not data_exists:\n                raise FileNotFoundError(\n                    \"No Long-term Permanence Score data has been found for \"\n                    f\"day_type `{day_type}` and time_interval `{time_interval}`\"\n                )\n\n    @staticmethod\n    def find_rarely_observed_devices(total_observations_df: DataFrame, total_device_ps_threshold: int) -&gt; DataFrame:\n        \"\"\"\n        Find devices (user ids) which match the condition for being considered \"rarely observed\".\n        This condition consists in having a total device observation of: lps &gt; total_ps_threshold.\n\n        Args:\n            total_observations_df (DataFrame): Long-Term Permanence Score dataset, filtered by the\n                corresponding start and end dates, and filtered by id_type == 'device_observation'.\n            total_device_ps_threshold (int): ps threshold.\n\n        Returns:\n            DataFrame: two-column dataframe with the ids of the devices to discard and the corresponding user modulos.\n        \"\"\"\n        rarely_observed_user_ids = total_observations_df.filter(F.col(ColNames.lps) &lt; total_device_ps_threshold).select(\n            ColNames.user_id_modulo, ColNames.user_id\n        )\n        return rarely_observed_user_ids\n\n    @staticmethod\n    def find_discontinuously_observed_devices(\n        total_observations_df: DataFrame, total_device_freq_threshold: int\n    ) -&gt; DataFrame:\n        \"\"\"\n        Find devices (user ids) which match the condition for being considered \"discontinuously observed\".\n        This condition consists in having a total device observation of: freq &gt; total_freq_threshold.\n\n        Args:\n            total_observations_df (DataFrame): Long-Term Permanence Score dataset, filtered by the\n                corresponding start and end dates, and filtered by id_type == 'device_observation'.\n            total_device_freq_threshold (int): ps threshold.\n\n        Returns:\n            DataFrame: two-column dataframe with the ids of the devices to discard and the corresponding user modulos.\n        \"\"\"\n        discontinuously_observed_user_ids = total_observations_df.filter(\n            F.col(ColNames.total_frequency) &lt; total_device_freq_threshold\n        ).select(ColNames.user_id_modulo, ColNames.user_id)\n        return discontinuously_observed_user_ids\n\n    @staticmethod\n    def discard_user_ids(ltps_df: DataFrame, user_ids: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Given a list of user ids, discard them from a Long-Term Permanence Score dataset.\n\n        Args:\n            ltps_df (DataFrame): Long-Term Permanence Score dataset.\n            user_ids (DataFrame): one-column dataframe with the ids of the devices to discard.\n\n        Returns:\n            DataFrame: filtered Long-Term Permanence Score dataset.\n        \"\"\"\n        ltps_df = ltps_df.join(user_ids, on=[ColNames.user_id_modulo, ColNames.user_id], how=\"left_anti\")\n        return ltps_df\n\n    def discard_devices(self, ltps_df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Given a Long-Term Permanence Score dataset (filtered for the corresponding start and end dates),\n        discard the rows corresponding to some devices.\n\n        There are 2 type of devices to discard:\n            - rarely observed devices, based on LPS (long-term permanence score).\n            - discontinuously observed devices, based on frequency.\n\n        The user ids that are classified in any of these 2 groups are discarded from the Long-Term\n        Permanence Score dataset, and the number of discarded users of each kind is saved to the\n        corresponding attributes.\n\n        Args:\n            ltps_df (DataFrame): Long-Term Permanence Score dataset (filtered for the corresponding\n                start and end dates).\n\n        Returns:\n            DataFrame: Long-Term Permanence Score dataset (filtered for the corresponding start and\n                end dates), without the rows associated to rarely or discontinuously observed\n                devices.\n        \"\"\"\n        # Initial filter of ltps dataset to keep total device observation values:\n        total_observations_df = ltps_df.filter(\n            (F.col(ColNames.id_type) == \"device_observation\")\n            &amp; (F.col(ColNames.day_type) == \"all\")\n            &amp; (F.col(ColNames.time_interval) == \"all\")\n        )\n\n        # Rarely observed:\n        rarely_observed_user_ids = self.find_rarely_observed_devices(\n            total_observations_df, self.ps_threshold_for_rare_devices\n        )\n        self.rare_devices_count = rarely_observed_user_ids.count()\n\n        # Discontinuously observed:\n        discontinuously_observed_user_ids = self.find_discontinuously_observed_devices(\n            total_observations_df, self.freq_threshold_for_discontinuous_devices\n        )\n        self.discontinuous_devices_count = discontinuously_observed_user_ids.count()\n\n        # All user ids to discard:\n        discardable_user_ids = rarely_observed_user_ids.union(discontinuously_observed_user_ids)\n\n        # Filter dataset:\n        filtered_ltps_df = self.discard_user_ids(ltps_df, discardable_user_ids)\n\n        return filtered_ltps_df\n\n    @staticmethod\n    def add_abs_ps_threshold(\n        ltps_df: DataFrame, window: Window, gap_ps_threshold: Union[int, float], threshold_is_absolute: bool\n    ) -&gt; DataFrame:\n        \"\"\"\n        Add \"abs_ps_threshold\" field to the ltps dataset.\n\n        If \"threshold_is_absolute\" is True, then just assign \"abs_ps_threshold\" = \"gap_ps_threshold\"\n        value to all registers.\n\n        If threshold_is_absolute is False, then reach the maximum value of lps for a grid tile for\n        each device (ps_max) and calculate \"abs_ps_threshold\" = \"gap_ps_threshold\" * \"ps_max\".\n\n        Args:\n            ltps_df (DataFrame): Long-Term Permanence Score dataset.\n            window (Window): window, partitioned by user id and ordered by lps.\n            gap_ps_threshold (int|float): absolute/relative lps threshold.\n            threshold_is_absolute (bool): indicates if gap_ps_threshold is a relative/absolute value.\n\n        Returns:\n            DataFrame: Long-Term Permanence Score dataset with the additional \"abs_ps_threshold\"\n                column.\n        \"\"\"\n        if threshold_is_absolute is True:\n            ltps_df = ltps_df.withColumn(\"abs_ps_threshold\", F.lit(gap_ps_threshold))\n        else:\n            ltps_df = (\n                ltps_df.withColumn(\"ps_max\", F.first(ColNames.lps).over(window))\n                .withColumn(\"abs_ps_threshold\", F.col(\"ps_max\") * F.lit(gap_ps_threshold / 100))\n                .drop(\"ps_max\")\n            )\n        return ltps_df\n\n    def cut_tiles_at_gap(self, ltps_df: DataFrame, gap_ps_threshold: float, threshold_is_absolute: bool) -&gt; DataFrame:\n        \"\"\"\n        Preprocessing function. Given a Long-Term Permanence Metrics dataset, for each user:\n        - sort the grid tiles of the user by LPS\n        - find a high difference (gap) in the LPS values between one grid tile and the next one,\n          where what is a high difference is defined through the gap_ps_threshold argument.\n        - for each agent, filter out all tiles after this high difference: the remaining tiles are\n          the \"pre-selected tiles\", which are the output of this function.\n\n        Args:\n            ltps_df (DataFrame): Long-Term Permanence Metrics dataset, for a specific day type and\n                time interval combination.\n            gap_ps_threshold (int/float): absolute/relative lps threshold.\n            threshold_is_absolute (bool): indicates if gap_ps_threshold is a relative/absolute value.\n\n        Returns:\n            DataFrame: dataset with the pre-selected tiles.\n        \"\"\"\n        ltps_df = ltps_df.filter(F.col(ColNames.id_type) == \"grid\")\n\n        window = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id).orderBy(F.desc(ColNames.lps))\n        cumulative_window = window.rowsBetween(Window.unboundedPreceding, Window.currentRow)\n\n        ltps_df = self.add_abs_ps_threshold(ltps_df, window, gap_ps_threshold, threshold_is_absolute)\n\n        pre_selected_tiles_df = (\n            ltps_df.withColumn(\"previous_lps\", F.lag(ColNames.lps, 1).over(window))\n            .withColumn(\"lps_difference\", F.col(\"previous_lps\") - F.col(ColNames.lps))\n            .fillna({\"lps_difference\": 0})\n            .withColumn(\"high_difference\", F.when(F.col(\"lps_difference\") &gt;= F.col(\"abs_ps_threshold\"), 1).otherwise(0))\n            .withColumn(\"cumulative_condition\", F.sum(F.col(\"high_difference\")).over(cumulative_window))\n            .filter(F.col(\"cumulative_condition\") == F.lit(0))\n            .drop(\"lps_difference\", \"previous_lps\", \"high_difference\", \"cumulative_condition\", \"abs_ps_threshold\")\n        )\n\n        return pre_selected_tiles_df\n\n    @staticmethod\n    def calculate_device_abs_ps_threshold(\n        ltps_df_i: DataFrame, target_rows_ltps_df: DataFrame, perc_ps_threshold: float\n    ) -&gt; DataFrame:\n        \"\"\"\n        Calculate the total assigned long-term permanence score for each device (total_device_ps) for a given day type\n        and time interval and add this information to an additional column of the provided dataset. Then, based on\n        this column, generate the absolute ps threshold to consider for each device by applying the corresponding\n        configured percentage (perc_ps_threshold).\n\n        Args:\n            ltps_df_i (DataFrame): Long-Term Permanence Metrics dataset, for a specific day type and time interval\n                combination.\n            target_rows_ltps_df (DataFrame): Long-Term Permanence Metrics dataset, for a specific day type and time\n                interval combination, with one 'id_type' == 'grid'.\n            perc_ps_threshold (float): specified ps threshold (in percentage).\n\n        Returns:\n            DataFrame: Long-Term Permanence Metrics dataset, for a specific day type and time interval combination,\n                with one 'id_type' == 'grid', with an additional column named \"abs_ps_threshold\".\n        \"\"\"\n        device_total_ps_df = (\n            ltps_df_i.filter(F.col(ColNames.id_type) == \"device_observation\")\n            .withColumnRenamed(ColNames.lps, \"total_device_ps\")\n            .select(ColNames.user_id_modulo, ColNames.user_id, \"total_device_ps\")\n        )\n\n        target_rows_ltps_df = (\n            target_rows_ltps_df.join(device_total_ps_df, on=[ColNames.user_id_modulo, ColNames.user_id])\n            .withColumn(\"abs_ps_threshold\", F.col(\"total_device_ps\") * F.lit(perc_ps_threshold / 100))\n            .drop(\"total_device_ps\")\n        )\n\n        return target_rows_ltps_df\n\n    @staticmethod\n    def filter_by_ps(ltps_df: DataFrame) -&gt; Tuple[DataFrame, DataFrame]:\n        \"\"\"\n        Apply the long-term permanence score filter (ps) to a Long-Term Permanence Metrics dataset. Filter rows for\n        which 'lps' &gt; abs_ps_threshold.\n\n        Args:\n            ltps_df (DataFrame): Long-Term Permanence Metrics dataset, for a specific day type and time interval\n                combination.\n\n        Returns:\n            DataFrame: Long-Term Permanence Metrics dataset, for a specific day type and time interval combination,\n                keeping only the rows that DO pass the ps filter.\n            DataFrame: Long-Term Permanence Metrics dataset, for a specific day type and time interval combination,\n                keeping only the rows that DO NOT pass the ps filter.\n        \"\"\"\n        common_df = ltps_df.withColumn(\n            \"selected_flag\", F.when(F.col(ColNames.lps) &gt;= F.col(\"abs_ps_threshold\"), True).otherwise(False)\n        )\n\n        selected_tiles_df = common_df.filter(F.col(\"selected_flag\")).drop(\"abs_ps_threshold\", \"selected_flag\")\n        not_selected_tiles_df = common_df.filter(~F.col(\"selected_flag\")).drop(\"abs_ps_threshold\", \"selected_flag\")\n\n        return selected_tiles_df, not_selected_tiles_df\n\n    @staticmethod\n    def calculate_device_abs_ndays_threshold(\n        ltps_df_i: DataFrame, target_rows_ltps_df: DataFrame, perc_ndays_threshold: float\n    ) -&gt; DataFrame:\n        \"\"\"\n        Calculate the total frequency (observed number of days) for each device (total_device_ndays) for a given day\n        type and time interval and add this information to an additional column of the provided dataset. Then, based on\n        this column, generate the absolute ndays threshold to consider for each device by applying the corresponding\n        configured percentage (perc_ndays_threshold).\n\n        Args:\n            ltps_df_i (DataFrame): Long-Term Permanence Metrics dataset, for a specific day type and time interval\n                combination.\n            target_rows_ltps_df (DataFrame): Long-Term Permanence Metrics dataset, for a specific day type and time\n                interval combination, with one 'id_type' == 'grid'.\n            perc_ndays_threshold (float): specified ndays threshold (in percentage).\n\n        Returns:\n            DataFrame: Long-Term Permanence Metrics dataset, for a specific day type and time interval combination,\n                with one 'id_type' == 'grid', with an additional column named \"abs_ndays_threshold\".\n        \"\"\"\n        device_total_ndays_df = (\n            ltps_df_i.filter(F.col(ColNames.id_type) == \"device_observation\")\n            .withColumnRenamed(ColNames.total_frequency, \"total_device_ndays\")\n            .select(ColNames.user_id_modulo, ColNames.user_id, \"total_device_ndays\")\n        )\n\n        target_rows_ltps_df = (\n            target_rows_ltps_df.join(device_total_ndays_df, on=[ColNames.user_id_modulo, ColNames.user_id])\n            .withColumn(\"abs_ndays_threshold\", F.col(\"total_device_ndays\") * F.lit(perc_ndays_threshold / 100))\n            .drop(\"total_device_ndays\")\n        )\n\n        return target_rows_ltps_df\n\n    @staticmethod\n    def filter_by_ndays(ltps_df: DataFrame) -&gt; Tuple[DataFrame, DataFrame]:\n        \"\"\"\n        Apply the frequency filter (number of days) to a Long-Term Permanence Metrics dataset. Filter rows\n        for which 'total_frequency' &gt; abs_ndays_threshold.\n\n        Args:\n            ltps_df (DataFrame): Long-Term Permanence Metrics dataset, for a specific day type and time interval\n                combination.\n\n        Returns:\n            DataFrame: Long-Term Permanence Metrics dataset, for a specific day type and time interval combination,\n                keeping only the rows that DO pass the ndays filter.\n            DataFrame: Long-Term Permanence Metrics dataset, for a specific day type and time interval combination,\n                keeping only the rows that DO NOT pass the ndays filter.\n        \"\"\"\n        common_df = ltps_df.withColumn(\n            \"selected_flag\",\n            F.when(F.col(ColNames.total_frequency) &gt;= F.col(\"abs_ndays_threshold\"), True).otherwise(False),\n        )\n\n        selected_tiles_df = common_df.filter(F.col(\"selected_flag\")).drop(\"abs_ndays_threshold\", \"selected_flag\")\n        not_selected_tiles_df = common_df.filter(~F.col(\"selected_flag\")).drop(\"abs_ndays_threshold\", \"selected_flag\")\n\n        return selected_tiles_df, not_selected_tiles_df\n\n    def format_selected_tiles(self, selected_tiles_df: DataFrame, label_type: str, n_rule: int) -&gt; DataFrame:\n        \"\"\"\n        Add the \"label\", \"ue_label_rule\" and \"location_label_rule\" columns to the given selected tiles dataframe:\n        - the \"label\" column is assigned directly depending on the label type being computed by mapping with the\n          provided dictionary: LABEL_TO_LABELNAMES[label_type].\n        - the \"ue_label_rule\" is equal to \"ue_na\" if the label type being computed is different from \"ue\", or is\n          equal to \"{ue}_{N}\" if the label type being computed is \"ue\", where:\n            - {N} = n_rule\n            - {ue} = LABEL_TO_SHORT_LABELNAMES[label_type].\n        - the \"location_label_rule\" is equal to \"loc_na\" if the label type being computed is \"ue\", or is equal to\n          \"{loc}_{N}\" if the label type being computed is NOT \"ue\", where:\n            - {N} = n_rule\n            - {loc} = LABEL_TO_SHORT_LABELNAMES[label_type].\n\n        Args:\n            selected_tiles_df (DataFrame): selected tiles dataset.\n            label_type (str): label type to compute: 'ue', 'home' or 'work'.\n            n_rule (int): number of the current rule in order to generate the label_rule column values.\n\n        Returns:\n            DataFrame: selected tiles dataset, with the additional columns: \"label\", \"ue_label_rule\" and\n                \"location_label_rule\".\n        \"\"\"\n        short_labelname = self.LABEL_TO_SHORT_LABELNAMES[label_type]\n\n        if label_type == \"ue\":\n            ue_rule_txt = f\"{short_labelname}_{n_rule}\"\n            selected_tiles_df = selected_tiles_df.withColumn(ColNames.ue_label_rule, F.lit(ue_rule_txt))\n        else:\n            labelname = self.LABEL_TO_LABELNAMES[label_type]\n            selected_tiles_df = selected_tiles_df.withColumn(ColNames.label, F.lit(labelname))\n            loc_rule_txt = f\"{short_labelname}_{n_rule}\"\n            selected_tiles_df = selected_tiles_df.withColumn(ColNames.location_label_rule, F.lit(loc_rule_txt))\n\n        return selected_tiles_df\n\n    def format_not_selected_tiles(self, not_selected_tiles_df: DataFrame, label_type: str) -&gt; DataFrame:\n        \"\"\"\n        Add the \"label\", \"ue_label_rule\" and \"location_label_rule\" columns to the not selected tiles dataframe:\n        - the \"label\" column is assigned directly depending on the label type being computed by mapping with the\n          provided dictionary: LABEL_TO_LABELNAMES[label_type].\n        - the \"ue_label_rule\" is equal to \"ue_na\".\n        - the \"location_label_rule\" is equal to \"loc_na\".\n\n        Args:\n            not_selected_tiles_df (DataFrame): not selected tiles dataset.\n            label_type (str): label type to compute: 'ue', 'home' or 'work'.\n\n        Returns:\n            DataFrame: not selected tiles dataset, with the additional columns: \"label\", \"ue_label_rule\" and\n                \"location_label_rule\".\n        \"\"\"\n        if label_type == \"ue\":\n            not_selected_tiles_df = not_selected_tiles_df.withColumn(ColNames.ue_label_rule, F.lit(\"ue_na\"))\n        else:\n            labelname = self.LABEL_TO_LABELNAMES[label_type]\n            not_selected_tiles_df = not_selected_tiles_df.withColumn(ColNames.label, F.lit(labelname))\n            not_selected_tiles_df = not_selected_tiles_df.withColumn(ColNames.location_label_rule, F.lit(\"loc_na\"))\n\n        return not_selected_tiles_df\n\n    def compute_generic_labeling(self, label_type: str, apply_ndays_filter: bool) -&gt; DataFrame:\n        \"\"\"\n        Generate the labeled tiles dataset for the specified label type. This function is generic and works for all\n        label types.\n\n        Args:\n            label_type (str): label type to compute: 'ue', 'home' or 'work'.\n            apply_ndays_filter (bool): Indicates if the final ndays frequency filter shall be applied in the computation\n                of the specified label type.\n\n        Returns:\n            DataFrame: labeled tiles dataset for the specified label type.\n        \"\"\"\n        ps_threshold = self.ps_thresholds[label_type]\n        gap_ps_threshold = self.gap_ps_thresholds[label_type]\n        gap_ps_threshold_is_absolute = self.gap_ps_threshold_is_absolute[label_type]\n        ndays_threshold = self.freq_thresholds[label_type]\n        day_and_interval_combinations = self.day_and_interval_type_combinations[label_type]\n\n        selected_tiles_dfs_list = []\n\n        ##### 1st STAGE #####\n        for i, (day_type, time_interval) in enumerate(day_and_interval_combinations):\n\n            # filter ltps df for the current day type and time interval combination:\n            ltps_df_i = self.ltps_df.filter(\n                (F.col(ColNames.day_type) == day_type) &amp; (F.col(ColNames.time_interval) == time_interval)\n            )\n\n            # if first day_type, time_interval combination:\n            if i == 0:\n                # cut tiles at gap to generate preselected tiles\n                tiles_before_gap_df = self.cut_tiles_at_gap(ltps_df_i, gap_ps_threshold, gap_ps_threshold_is_absolute)\n                preselected_tiles_df = tiles_before_gap_df\n\n            # rest of day_type, time_interval combinations:\n            else:\n                # reach not selected tiles from previous iteration to generate preselected tiles\n                preselected_tiles_df = ltps_df_i.join(\n                    not_selected_tiles_df.select(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id),\n                    on=[ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id],\n                )\n\n            # add abs_ps_threshold column to dataframe:\n            preselected_tiles_df = self.calculate_device_abs_ps_threshold(ltps_df_i, preselected_tiles_df, ps_threshold)\n\n            # apply relative lps filter to obtain selected tiles and not selected tiles:\n            selected_tiles_df, not_selected_tiles_df = self.filter_by_ps(preselected_tiles_df)\n\n            # format current selected tiles df and add to list:\n            n_rule = 1 if i == 0 else 2\n            selected_tiles_df = self.format_selected_tiles(selected_tiles_df, label_type, n_rule)\n            selected_tiles_dfs_list.append(selected_tiles_df)\n\n        ##### 2nd STAGE #####\n        if apply_ndays_filter is True:\n\n            # filter ltps df for the first day type and time interval combination:\n            first_day_type, first_time_interval = day_and_interval_combinations[0]\n            ltps_df_i = self.ltps_df.filter(\n                (F.col(ColNames.day_type) == first_day_type) &amp; (F.col(ColNames.time_interval) == first_time_interval)\n            )\n\n            # add abs_ndays_threshold column to dataframe:\n            preselected_tiles_df = self.calculate_device_abs_ndays_threshold(\n                ltps_df_i, not_selected_tiles_df, ndays_threshold\n            )\n\n            # apply relative lps filter to obtain selected tiles and not selected tiles:\n            selected_tiles_df, not_selected_tiles_df = self.filter_by_ndays(preselected_tiles_df)\n\n            # format current selected tiles df and add to list:\n            n_rule = 2 if len(day_and_interval_combinations) == 1 else 3\n            selected_tiles_df = self.format_selected_tiles(selected_tiles_df, label_type, n_rule)\n            selected_tiles_dfs_list.append(selected_tiles_df)\n\n        ##### Concatenate to generate final dataset for this label type #####\n        # format not selected tiles df and add to list\n        not_selected_tiles_df = self.format_not_selected_tiles(not_selected_tiles_df, label_type)\n        selected_tiles_dfs_list.append(not_selected_tiles_df)\n\n        # concatenate all selected and not selected\n        labeled_tiles_df = reduce(DataFrame.unionAll, selected_tiles_dfs_list)\n\n        # select columns for output\n        base_columns = [\n            ColNames.user_id,\n            ColNames.grid_id,\n            ColNames.user_id_modulo,\n        ]\n        if label_type == \"ue\":\n            columns = base_columns + [ColNames.ue_label_rule]\n        else:\n            columns = base_columns + [ColNames.label, ColNames.location_label_rule]\n\n        labeled_tiles_df = labeled_tiles_df.select(columns)\n\n        return labeled_tiles_df\n\n    def compute_ue_labeling(self) -&gt; DataFrame:\n        \"\"\"\n        Generate dataset with the tiles that conform the UE (Usual Environment) of each device.\n\n        Returns:\n            DataFrame: UE tiles dataset.\n        \"\"\"\n        ue_tiles_df = self.compute_generic_labeling(label_type=\"ue\", apply_ndays_filter=False)\n        return ue_tiles_df\n\n    def compute_home_labeling(self) -&gt; DataFrame:\n        \"\"\"\n        Generate dataset with the tiles that conform the home location of each device.\n\n        Returns:\n            DataFrame: home tiles dataset.\n        \"\"\"\n        home_tiles_df = self.compute_generic_labeling(label_type=\"home\", apply_ndays_filter=True)\n        return home_tiles_df\n\n    def compute_work_labeling(self) -&gt; DataFrame:\n        \"\"\"\n        Generate dataset with the tiles that conform the work location of each device.\n\n        Returns:\n            DataFrame: work tiles dataset.\n        \"\"\"\n        work_tiles_df = self.compute_generic_labeling(label_type=\"work\", apply_ndays_filter=True)\n        return work_tiles_df\n\n    def concatenate_labeled_tiles(\n        self, ue_tiles_df: DataFrame, home_tiles_df: DataFrame, work_tiles_df: DataFrame\n    ) -&gt; DataFrame:\n        \"\"\"\n        Concatenate UE tiles, home tiles and work tiles datasets into a unique dataset.\n\n        Args:\n            ue_tiles_df (DataFrame): UE tiles dataset.\n            home_tiles_df (DataFrame): home tiles dataset.\n            work_tiles_df (DataFrame): work tiles dataset.\n\n        Returns:\n            DataFrame: Usual Environment Labels dataframe.\n        \"\"\"\n        # just concatenate home and work tiles:\n        loc_tiles_df = home_tiles_df.union(work_tiles_df)\n\n        labels_df = (\n            loc_tiles_df.join(\n                ue_tiles_df,\n                on=[ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id],\n                how=\"outer\",\n            )\n            .select(\n                F.col(ColNames.user_id_modulo),\n                F.col(ColNames.user_id),\n                F.col(ColNames.grid_id),\n                F.col(ColNames.label),\n                F.col(ColNames.ue_label_rule),\n                F.col(ColNames.location_label_rule),\n            )\n            .fillna(\n                {ColNames.label: \"no_label\", ColNames.ue_label_rule: \"ue_na\", ColNames.location_label_rule: \"loc_na\"}\n            )\n        )\n\n        labels_df = (\n            labels_df.withColumn(ColNames.season, F.lit(self.season))\n            .withColumn(ColNames.start_date, F.lit(self.start_date))\n            .withColumn(ColNames.end_date, F.lit(self.end_date))\n        )\n\n        return labels_df\n\n    @staticmethod\n    def get_rule_count(df: DataFrame, col_to_value: Dict[str, str]) -&gt; int:\n        \"\"\"\n        Sums the count column values of the given dataframe for the corresponding filter (col_to_value).\n\n        Args:\n            df (DataFrame): input dataframe.\n            col_to_value (Dict[str, str]): filter to apply.\n\n        Returns:\n            int: count column sum.\n        \"\"\"\n        conditions = (F.col(colname) == colvalue for colname, colvalue in col_to_value.items())\n        combined_condition = reduce(lambda x, y: x &amp; y, conditions)\n        count_value = df.filter(combined_condition).agg(F.sum(\"count\")).collect()[0][0]\n        if count_value is None:\n            count_value = 0\n        return count_value\n\n    def generate_quality_metrics(self, ue_labels_df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Build usual environment labeling quality metrics dataframe.\n\n        Args:\n            ue_labels_df (DataFrame): usual environment labels dataframe.\n\n        Returns:\n            DataFrame: quality metrics dataframe.\n        \"\"\"\n        grouped_df = ue_labels_df.groupby(ColNames.label, ColNames.ue_label_rule, ColNames.location_label_rule).count()\n        grouped_df.cache()\n\n        ue_1_rule_count = self.get_rule_count(grouped_df, {ColNames.ue_label_rule: \"ue_1\"})\n        ue_2_rule_count = self.get_rule_count(grouped_df, {ColNames.ue_label_rule: \"ue_2\"})\n        ue_3_rule_count = self.get_rule_count(grouped_df, {ColNames.ue_label_rule: \"ue_3\"})\n        h_1_rule_count = self.get_rule_count(grouped_df, {ColNames.location_label_rule: \"h_1\"})\n        h_2_rule_count = self.get_rule_count(grouped_df, {ColNames.location_label_rule: \"h_2\"})\n        h_3_rule_count = self.get_rule_count(grouped_df, {ColNames.location_label_rule: \"h_3\"})\n        w_1_rule_count = self.get_rule_count(grouped_df, {ColNames.location_label_rule: \"w_1\"})\n        w_2_rule_count = self.get_rule_count(grouped_df, {ColNames.location_label_rule: \"w_2\"})\n        w_3_rule_count = self.get_rule_count(grouped_df, {ColNames.location_label_rule: \"w_3\"})\n        ue_na_rule_count = self.get_rule_count(grouped_df, {ColNames.ue_label_rule: \"ue_na\"})\n        loc_na_rule_count = self.get_rule_count(grouped_df, {ColNames.location_label_rule: \"loc_na\"})\n        h_non_ue_count = self.get_rule_count(grouped_df, {ColNames.label: \"home\", ColNames.ue_label_rule: \"ue_na\"})\n        w_non_ue_count = self.get_rule_count(grouped_df, {ColNames.label: \"work\", ColNames.ue_label_rule: \"ue_na\"})\n\n        grouped_df.unpersist()\n\n        data = [\n            (\"device_filter_1_rule\", self.rare_devices_count, self.start_date, self.end_date, self.season),\n            (\"device_filter_2_rule\", self.discontinuous_devices_count, self.start_date, self.end_date, self.season),\n            (\"ue_1_rule\", ue_1_rule_count, self.start_date, self.end_date, self.season),\n            (\"ue_2_rule\", ue_2_rule_count, self.start_date, self.end_date, self.season),\n            (\"ue_3_rule\", ue_3_rule_count, self.start_date, self.end_date, self.season),\n            (\"h_1_rule\", h_1_rule_count, self.start_date, self.end_date, self.season),\n            (\"h_2_rule\", h_2_rule_count, self.start_date, self.end_date, self.season),\n            (\"h_3_rule\", h_3_rule_count, self.start_date, self.end_date, self.season),\n            (\"w_1_rule\", w_1_rule_count, self.start_date, self.end_date, self.season),\n            (\"w_2_rule\", w_2_rule_count, self.start_date, self.end_date, self.season),\n            (\"w_3_rule\", w_3_rule_count, self.start_date, self.end_date, self.season),\n            (\"ue_na_rule\", ue_na_rule_count, self.start_date, self.end_date, self.season),\n            (\"loc_na_rule\", loc_na_rule_count, self.start_date, self.end_date, self.season),\n            (\"h_non_ue\", h_non_ue_count, self.start_date, self.end_date, self.season),\n            (\"w_non_ue\", w_non_ue_count, self.start_date, self.end_date, self.season),\n        ]\n\n        # Create DataFrame\n        schema = SilverUsualEnvironmentLabelingQualityMetricsDataObject.SCHEMA\n        quality_metrics_df = self.spark.createDataFrame(data, schema)\n\n        return quality_metrics_df\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n\n        if self.partition_chunk is not None:\n            self.ltps_df = self.ltps_df.filter(F.col(ColNames.user_id_modulo).isin(self.partition_chunk))\n\n        # discard devices that will not be analysed ('rarely observed' or 'discontinuously observed'):\n        self.ltps_df = self.discard_devices(self.ltps_df)\n\n        # calculate tiles that belong to the ue (usual environment) of each device:\n        ue_tiles_df = self.compute_ue_labeling()\n\n        # calculate tiles that belong to the home location of each device:\n        home_tiles_df = self.compute_home_labeling()\n\n        # calculate tiles that belong to the work location of each device:\n        work_tiles_df = self.compute_work_labeling()\n\n        # join ue tiles, home tiles and work tiles datasets into a ue labels dataset:\n        labeled_tiles_df = self.concatenate_labeled_tiles(ue_tiles_df, home_tiles_df, work_tiles_df)\n\n        # generate labeling quality metrics dataset:\n        labeling_quality_metrics_df = self.generate_quality_metrics(labeled_tiles_df)\n\n        # Repartition\n        if self.disaggregate_to_100m_grid:\n            self.logger.info(\"Dissagregating 200m to 100m grid\")\n            labeled_tiles_df = self.grid_gen.get_children_grid_ids(labeled_tiles_df, 200, 100)\n        labeled_tiles_df = labeled_tiles_df.repartition(*SilverUsualEnvironmentLabelsDataObject.PARTITION_COLUMNS)\n        labeling_quality_metrics_df = labeling_quality_metrics_df.repartition(\n            *SilverUsualEnvironmentLabelingQualityMetricsDataObject.PARTITION_COLUMNS\n        )\n\n        # save objects to output data dict:\n        self.output_data_objects[SilverUsualEnvironmentLabelsDataObject.ID].df = labeled_tiles_df\n        self.output_data_objects[SilverUsualEnvironmentLabelingQualityMetricsDataObject.ID].df = (\n            labeling_quality_metrics_df\n        )\n\n    def _get_partition_chunks(self) -&gt; List[List[int]]:\n        \"\"\"\n        Method that returns the partition chunks for the current date.\n\n        Returns:\n            List[List[int, int]]: list of partition chunks. If the partition_chunk_size is not defined in the config or\n                the number of partitions is less than the desired chunk size, it will return a list with a single None element.\n        \"\"\"\n        # Get partitions desired\n        partition_chunk_size = self.config.getint(self.COMPONENT_ID, \"partition_chunk_size\", fallback=None)\n        number_of_partitions = self.config.getint(self.COMPONENT_ID, \"number_of_partitions\", fallback=None)\n\n        if partition_chunk_size is None or number_of_partitions is None or partition_chunk_size &lt;= 0:\n            return [None]\n\n        if number_of_partitions &lt;= partition_chunk_size:\n            self.logger.warning(\n                f\"Available Partition number ({number_of_partitions}) is \"\n                f\"less than the desired chunk size ({partition_chunk_size}). \"\n                f\"Using all partitions.\"\n            )\n            return [None]\n        partition_chunks = [\n            list(range(i, min(i + partition_chunk_size, number_of_partitions)))\n            for i in range(0, number_of_partitions, partition_chunk_size)\n        ]\n        # NOTE: Generate chunks if partition_values were read for each day\n        # getting exactly the amount of partitions for that day\n\n        # partition_chunks = [\n        #     partition_values[i : i + partition_chunk_size]\n        #     for i in range(0, partition_values_size, partition_chunk_size)\n        # ]\n\n        return partition_chunks\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling._get_partition_chunks","title":"<code>_get_partition_chunks()</code>","text":"<p>Method that returns the partition chunks for the current date.</p> <p>Returns:</p> Type Description <code>List[List[int]]</code> <p>List[List[int, int]]: list of partition chunks. If the partition_chunk_size is not defined in the config or the number of partitions is less than the desired chunk size, it will return a list with a single None element.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>def _get_partition_chunks(self) -&gt; List[List[int]]:\n    \"\"\"\n    Method that returns the partition chunks for the current date.\n\n    Returns:\n        List[List[int, int]]: list of partition chunks. If the partition_chunk_size is not defined in the config or\n            the number of partitions is less than the desired chunk size, it will return a list with a single None element.\n    \"\"\"\n    # Get partitions desired\n    partition_chunk_size = self.config.getint(self.COMPONENT_ID, \"partition_chunk_size\", fallback=None)\n    number_of_partitions = self.config.getint(self.COMPONENT_ID, \"number_of_partitions\", fallback=None)\n\n    if partition_chunk_size is None or number_of_partitions is None or partition_chunk_size &lt;= 0:\n        return [None]\n\n    if number_of_partitions &lt;= partition_chunk_size:\n        self.logger.warning(\n            f\"Available Partition number ({number_of_partitions}) is \"\n            f\"less than the desired chunk size ({partition_chunk_size}). \"\n            f\"Using all partitions.\"\n        )\n        return [None]\n    partition_chunks = [\n        list(range(i, min(i + partition_chunk_size, number_of_partitions)))\n        for i in range(0, number_of_partitions, partition_chunk_size)\n    ]\n    # NOTE: Generate chunks if partition_values were read for each day\n    # getting exactly the amount of partitions for that day\n\n    # partition_chunks = [\n    #     partition_values[i : i + partition_chunk_size]\n    #     for i in range(0, partition_values_size, partition_chunk_size)\n    # ]\n\n    return partition_chunks\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.add_abs_ps_threshold","title":"<code>add_abs_ps_threshold(ltps_df, window, gap_ps_threshold, threshold_is_absolute)</code>  <code>staticmethod</code>","text":"<p>Add \"abs_ps_threshold\" field to the ltps dataset.</p> <p>If \"threshold_is_absolute\" is True, then just assign \"abs_ps_threshold\" = \"gap_ps_threshold\" value to all registers.</p> <p>If threshold_is_absolute is False, then reach the maximum value of lps for a grid tile for each device (ps_max) and calculate \"abs_ps_threshold\" = \"gap_ps_threshold\" * \"ps_max\".</p> <p>Parameters:</p> Name Type Description Default <code>ltps_df</code> <code>DataFrame</code> <p>Long-Term Permanence Score dataset.</p> required <code>window</code> <code>Window</code> <p>window, partitioned by user id and ordered by lps.</p> required <code>gap_ps_threshold</code> <code>int | float</code> <p>absolute/relative lps threshold.</p> required <code>threshold_is_absolute</code> <code>bool</code> <p>indicates if gap_ps_threshold is a relative/absolute value.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Long-Term Permanence Score dataset with the additional \"abs_ps_threshold\" column.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>@staticmethod\ndef add_abs_ps_threshold(\n    ltps_df: DataFrame, window: Window, gap_ps_threshold: Union[int, float], threshold_is_absolute: bool\n) -&gt; DataFrame:\n    \"\"\"\n    Add \"abs_ps_threshold\" field to the ltps dataset.\n\n    If \"threshold_is_absolute\" is True, then just assign \"abs_ps_threshold\" = \"gap_ps_threshold\"\n    value to all registers.\n\n    If threshold_is_absolute is False, then reach the maximum value of lps for a grid tile for\n    each device (ps_max) and calculate \"abs_ps_threshold\" = \"gap_ps_threshold\" * \"ps_max\".\n\n    Args:\n        ltps_df (DataFrame): Long-Term Permanence Score dataset.\n        window (Window): window, partitioned by user id and ordered by lps.\n        gap_ps_threshold (int|float): absolute/relative lps threshold.\n        threshold_is_absolute (bool): indicates if gap_ps_threshold is a relative/absolute value.\n\n    Returns:\n        DataFrame: Long-Term Permanence Score dataset with the additional \"abs_ps_threshold\"\n            column.\n    \"\"\"\n    if threshold_is_absolute is True:\n        ltps_df = ltps_df.withColumn(\"abs_ps_threshold\", F.lit(gap_ps_threshold))\n    else:\n        ltps_df = (\n            ltps_df.withColumn(\"ps_max\", F.first(ColNames.lps).over(window))\n            .withColumn(\"abs_ps_threshold\", F.col(\"ps_max\") * F.lit(gap_ps_threshold / 100))\n            .drop(\"ps_max\")\n        )\n    return ltps_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.calculate_device_abs_ndays_threshold","title":"<code>calculate_device_abs_ndays_threshold(ltps_df_i, target_rows_ltps_df, perc_ndays_threshold)</code>  <code>staticmethod</code>","text":"<p>Calculate the total frequency (observed number of days) for each device (total_device_ndays) for a given day type and time interval and add this information to an additional column of the provided dataset. Then, based on this column, generate the absolute ndays threshold to consider for each device by applying the corresponding configured percentage (perc_ndays_threshold).</p> <p>Parameters:</p> Name Type Description Default <code>ltps_df_i</code> <code>DataFrame</code> <p>Long-Term Permanence Metrics dataset, for a specific day type and time interval combination.</p> required <code>target_rows_ltps_df</code> <code>DataFrame</code> <p>Long-Term Permanence Metrics dataset, for a specific day type and time interval combination, with one 'id_type' == 'grid'.</p> required <code>perc_ndays_threshold</code> <code>float</code> <p>specified ndays threshold (in percentage).</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Long-Term Permanence Metrics dataset, for a specific day type and time interval combination, with one 'id_type' == 'grid', with an additional column named \"abs_ndays_threshold\".</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>@staticmethod\ndef calculate_device_abs_ndays_threshold(\n    ltps_df_i: DataFrame, target_rows_ltps_df: DataFrame, perc_ndays_threshold: float\n) -&gt; DataFrame:\n    \"\"\"\n    Calculate the total frequency (observed number of days) for each device (total_device_ndays) for a given day\n    type and time interval and add this information to an additional column of the provided dataset. Then, based on\n    this column, generate the absolute ndays threshold to consider for each device by applying the corresponding\n    configured percentage (perc_ndays_threshold).\n\n    Args:\n        ltps_df_i (DataFrame): Long-Term Permanence Metrics dataset, for a specific day type and time interval\n            combination.\n        target_rows_ltps_df (DataFrame): Long-Term Permanence Metrics dataset, for a specific day type and time\n            interval combination, with one 'id_type' == 'grid'.\n        perc_ndays_threshold (float): specified ndays threshold (in percentage).\n\n    Returns:\n        DataFrame: Long-Term Permanence Metrics dataset, for a specific day type and time interval combination,\n            with one 'id_type' == 'grid', with an additional column named \"abs_ndays_threshold\".\n    \"\"\"\n    device_total_ndays_df = (\n        ltps_df_i.filter(F.col(ColNames.id_type) == \"device_observation\")\n        .withColumnRenamed(ColNames.total_frequency, \"total_device_ndays\")\n        .select(ColNames.user_id_modulo, ColNames.user_id, \"total_device_ndays\")\n    )\n\n    target_rows_ltps_df = (\n        target_rows_ltps_df.join(device_total_ndays_df, on=[ColNames.user_id_modulo, ColNames.user_id])\n        .withColumn(\"abs_ndays_threshold\", F.col(\"total_device_ndays\") * F.lit(perc_ndays_threshold / 100))\n        .drop(\"total_device_ndays\")\n    )\n\n    return target_rows_ltps_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.calculate_device_abs_ps_threshold","title":"<code>calculate_device_abs_ps_threshold(ltps_df_i, target_rows_ltps_df, perc_ps_threshold)</code>  <code>staticmethod</code>","text":"<p>Calculate the total assigned long-term permanence score for each device (total_device_ps) for a given day type and time interval and add this information to an additional column of the provided dataset. Then, based on this column, generate the absolute ps threshold to consider for each device by applying the corresponding configured percentage (perc_ps_threshold).</p> <p>Parameters:</p> Name Type Description Default <code>ltps_df_i</code> <code>DataFrame</code> <p>Long-Term Permanence Metrics dataset, for a specific day type and time interval combination.</p> required <code>target_rows_ltps_df</code> <code>DataFrame</code> <p>Long-Term Permanence Metrics dataset, for a specific day type and time interval combination, with one 'id_type' == 'grid'.</p> required <code>perc_ps_threshold</code> <code>float</code> <p>specified ps threshold (in percentage).</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Long-Term Permanence Metrics dataset, for a specific day type and time interval combination, with one 'id_type' == 'grid', with an additional column named \"abs_ps_threshold\".</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>@staticmethod\ndef calculate_device_abs_ps_threshold(\n    ltps_df_i: DataFrame, target_rows_ltps_df: DataFrame, perc_ps_threshold: float\n) -&gt; DataFrame:\n    \"\"\"\n    Calculate the total assigned long-term permanence score for each device (total_device_ps) for a given day type\n    and time interval and add this information to an additional column of the provided dataset. Then, based on\n    this column, generate the absolute ps threshold to consider for each device by applying the corresponding\n    configured percentage (perc_ps_threshold).\n\n    Args:\n        ltps_df_i (DataFrame): Long-Term Permanence Metrics dataset, for a specific day type and time interval\n            combination.\n        target_rows_ltps_df (DataFrame): Long-Term Permanence Metrics dataset, for a specific day type and time\n            interval combination, with one 'id_type' == 'grid'.\n        perc_ps_threshold (float): specified ps threshold (in percentage).\n\n    Returns:\n        DataFrame: Long-Term Permanence Metrics dataset, for a specific day type and time interval combination,\n            with one 'id_type' == 'grid', with an additional column named \"abs_ps_threshold\".\n    \"\"\"\n    device_total_ps_df = (\n        ltps_df_i.filter(F.col(ColNames.id_type) == \"device_observation\")\n        .withColumnRenamed(ColNames.lps, \"total_device_ps\")\n        .select(ColNames.user_id_modulo, ColNames.user_id, \"total_device_ps\")\n    )\n\n    target_rows_ltps_df = (\n        target_rows_ltps_df.join(device_total_ps_df, on=[ColNames.user_id_modulo, ColNames.user_id])\n        .withColumn(\"abs_ps_threshold\", F.col(\"total_device_ps\") * F.lit(perc_ps_threshold / 100))\n        .drop(\"total_device_ps\")\n    )\n\n    return target_rows_ltps_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.check_needed_day_and_interval_types","title":"<code>check_needed_day_and_interval_types(ltps_df, day_and_interval_type_combinations)</code>  <code>staticmethod</code>","text":"<p>Method that checks if the needed combinations of day type and interval type are available in the provided dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ltps_df</code> <code>DataFrame</code> <p>provided Long-Term Permanence Score dataset</p> required <code>day_and_interval_type_combinations</code> <code>Dict[str, List[Tuple[str, str]]]</code> <p>day type and interval type combinations that are needed for the execution of the method.</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If there is no data for one or more of the needed combinations of day type and interval type.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>@staticmethod\ndef check_needed_day_and_interval_types(\n    ltps_df: DataFrame, day_and_interval_type_combinations: Dict[str, List[Tuple[str, str]]]\n):\n    \"\"\"\n    Method that checks if the needed combinations of day type and interval type are available\n    in the provided dataset.\n\n    Args:\n        ltps_df (DataFrame): provided Long-Term Permanence Score dataset\n        day_and_interval_type_combinations (Dict[str,List[Tuple[str,str]]]): day type and interval type\n            combinations that are needed for the execution of the method.\n\n    Raises:\n        FileNotFoundError: If there is no data for one or more of the needed combinations of day type\n            and interval type.\n    \"\"\"\n    # Build set of all needed day-interval type combinations:\n    all_day_and_interval_type_combinations = {\n        comb for el in day_and_interval_type_combinations.values() for comb in el\n    }\n\n    # Assert that these combinations appear at least once in the input Long-Term Permanence\n    # Score data object:\n    for day_type, time_interval in all_day_and_interval_type_combinations:\n\n        filtered_ltps_df = ltps_df.filter(\n            (F.col(ColNames.day_type) == day_type) &amp; (F.col(ColNames.time_interval) == time_interval)\n        )\n\n        data_exists = filtered_ltps_df.count() &gt; 0\n        if not data_exists:\n            raise FileNotFoundError(\n                \"No Long-term Permanence Score data has been found for \"\n                f\"day_type `{day_type}` and time_interval `{time_interval}`\"\n            )\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.compute_generic_labeling","title":"<code>compute_generic_labeling(label_type, apply_ndays_filter)</code>","text":"<p>Generate the labeled tiles dataset for the specified label type. This function is generic and works for all label types.</p> <p>Parameters:</p> Name Type Description Default <code>label_type</code> <code>str</code> <p>label type to compute: 'ue', 'home' or 'work'.</p> required <code>apply_ndays_filter</code> <code>bool</code> <p>Indicates if the final ndays frequency filter shall be applied in the computation of the specified label type.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>labeled tiles dataset for the specified label type.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>def compute_generic_labeling(self, label_type: str, apply_ndays_filter: bool) -&gt; DataFrame:\n    \"\"\"\n    Generate the labeled tiles dataset for the specified label type. This function is generic and works for all\n    label types.\n\n    Args:\n        label_type (str): label type to compute: 'ue', 'home' or 'work'.\n        apply_ndays_filter (bool): Indicates if the final ndays frequency filter shall be applied in the computation\n            of the specified label type.\n\n    Returns:\n        DataFrame: labeled tiles dataset for the specified label type.\n    \"\"\"\n    ps_threshold = self.ps_thresholds[label_type]\n    gap_ps_threshold = self.gap_ps_thresholds[label_type]\n    gap_ps_threshold_is_absolute = self.gap_ps_threshold_is_absolute[label_type]\n    ndays_threshold = self.freq_thresholds[label_type]\n    day_and_interval_combinations = self.day_and_interval_type_combinations[label_type]\n\n    selected_tiles_dfs_list = []\n\n    ##### 1st STAGE #####\n    for i, (day_type, time_interval) in enumerate(day_and_interval_combinations):\n\n        # filter ltps df for the current day type and time interval combination:\n        ltps_df_i = self.ltps_df.filter(\n            (F.col(ColNames.day_type) == day_type) &amp; (F.col(ColNames.time_interval) == time_interval)\n        )\n\n        # if first day_type, time_interval combination:\n        if i == 0:\n            # cut tiles at gap to generate preselected tiles\n            tiles_before_gap_df = self.cut_tiles_at_gap(ltps_df_i, gap_ps_threshold, gap_ps_threshold_is_absolute)\n            preselected_tiles_df = tiles_before_gap_df\n\n        # rest of day_type, time_interval combinations:\n        else:\n            # reach not selected tiles from previous iteration to generate preselected tiles\n            preselected_tiles_df = ltps_df_i.join(\n                not_selected_tiles_df.select(ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id),\n                on=[ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id],\n            )\n\n        # add abs_ps_threshold column to dataframe:\n        preselected_tiles_df = self.calculate_device_abs_ps_threshold(ltps_df_i, preselected_tiles_df, ps_threshold)\n\n        # apply relative lps filter to obtain selected tiles and not selected tiles:\n        selected_tiles_df, not_selected_tiles_df = self.filter_by_ps(preselected_tiles_df)\n\n        # format current selected tiles df and add to list:\n        n_rule = 1 if i == 0 else 2\n        selected_tiles_df = self.format_selected_tiles(selected_tiles_df, label_type, n_rule)\n        selected_tiles_dfs_list.append(selected_tiles_df)\n\n    ##### 2nd STAGE #####\n    if apply_ndays_filter is True:\n\n        # filter ltps df for the first day type and time interval combination:\n        first_day_type, first_time_interval = day_and_interval_combinations[0]\n        ltps_df_i = self.ltps_df.filter(\n            (F.col(ColNames.day_type) == first_day_type) &amp; (F.col(ColNames.time_interval) == first_time_interval)\n        )\n\n        # add abs_ndays_threshold column to dataframe:\n        preselected_tiles_df = self.calculate_device_abs_ndays_threshold(\n            ltps_df_i, not_selected_tiles_df, ndays_threshold\n        )\n\n        # apply relative lps filter to obtain selected tiles and not selected tiles:\n        selected_tiles_df, not_selected_tiles_df = self.filter_by_ndays(preselected_tiles_df)\n\n        # format current selected tiles df and add to list:\n        n_rule = 2 if len(day_and_interval_combinations) == 1 else 3\n        selected_tiles_df = self.format_selected_tiles(selected_tiles_df, label_type, n_rule)\n        selected_tiles_dfs_list.append(selected_tiles_df)\n\n    ##### Concatenate to generate final dataset for this label type #####\n    # format not selected tiles df and add to list\n    not_selected_tiles_df = self.format_not_selected_tiles(not_selected_tiles_df, label_type)\n    selected_tiles_dfs_list.append(not_selected_tiles_df)\n\n    # concatenate all selected and not selected\n    labeled_tiles_df = reduce(DataFrame.unionAll, selected_tiles_dfs_list)\n\n    # select columns for output\n    base_columns = [\n        ColNames.user_id,\n        ColNames.grid_id,\n        ColNames.user_id_modulo,\n    ]\n    if label_type == \"ue\":\n        columns = base_columns + [ColNames.ue_label_rule]\n    else:\n        columns = base_columns + [ColNames.label, ColNames.location_label_rule]\n\n    labeled_tiles_df = labeled_tiles_df.select(columns)\n\n    return labeled_tiles_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.compute_home_labeling","title":"<code>compute_home_labeling()</code>","text":"<p>Generate dataset with the tiles that conform the home location of each device.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>home tiles dataset.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>def compute_home_labeling(self) -&gt; DataFrame:\n    \"\"\"\n    Generate dataset with the tiles that conform the home location of each device.\n\n    Returns:\n        DataFrame: home tiles dataset.\n    \"\"\"\n    home_tiles_df = self.compute_generic_labeling(label_type=\"home\", apply_ndays_filter=True)\n    return home_tiles_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.compute_ue_labeling","title":"<code>compute_ue_labeling()</code>","text":"<p>Generate dataset with the tiles that conform the UE (Usual Environment) of each device.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>UE tiles dataset.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>def compute_ue_labeling(self) -&gt; DataFrame:\n    \"\"\"\n    Generate dataset with the tiles that conform the UE (Usual Environment) of each device.\n\n    Returns:\n        DataFrame: UE tiles dataset.\n    \"\"\"\n    ue_tiles_df = self.compute_generic_labeling(label_type=\"ue\", apply_ndays_filter=False)\n    return ue_tiles_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.compute_work_labeling","title":"<code>compute_work_labeling()</code>","text":"<p>Generate dataset with the tiles that conform the work location of each device.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>work tiles dataset.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>def compute_work_labeling(self) -&gt; DataFrame:\n    \"\"\"\n    Generate dataset with the tiles that conform the work location of each device.\n\n    Returns:\n        DataFrame: work tiles dataset.\n    \"\"\"\n    work_tiles_df = self.compute_generic_labeling(label_type=\"work\", apply_ndays_filter=True)\n    return work_tiles_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.concatenate_labeled_tiles","title":"<code>concatenate_labeled_tiles(ue_tiles_df, home_tiles_df, work_tiles_df)</code>","text":"<p>Concatenate UE tiles, home tiles and work tiles datasets into a unique dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ue_tiles_df</code> <code>DataFrame</code> <p>UE tiles dataset.</p> required <code>home_tiles_df</code> <code>DataFrame</code> <p>home tiles dataset.</p> required <code>work_tiles_df</code> <code>DataFrame</code> <p>work tiles dataset.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Usual Environment Labels dataframe.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>def concatenate_labeled_tiles(\n    self, ue_tiles_df: DataFrame, home_tiles_df: DataFrame, work_tiles_df: DataFrame\n) -&gt; DataFrame:\n    \"\"\"\n    Concatenate UE tiles, home tiles and work tiles datasets into a unique dataset.\n\n    Args:\n        ue_tiles_df (DataFrame): UE tiles dataset.\n        home_tiles_df (DataFrame): home tiles dataset.\n        work_tiles_df (DataFrame): work tiles dataset.\n\n    Returns:\n        DataFrame: Usual Environment Labels dataframe.\n    \"\"\"\n    # just concatenate home and work tiles:\n    loc_tiles_df = home_tiles_df.union(work_tiles_df)\n\n    labels_df = (\n        loc_tiles_df.join(\n            ue_tiles_df,\n            on=[ColNames.user_id_modulo, ColNames.user_id, ColNames.grid_id],\n            how=\"outer\",\n        )\n        .select(\n            F.col(ColNames.user_id_modulo),\n            F.col(ColNames.user_id),\n            F.col(ColNames.grid_id),\n            F.col(ColNames.label),\n            F.col(ColNames.ue_label_rule),\n            F.col(ColNames.location_label_rule),\n        )\n        .fillna(\n            {ColNames.label: \"no_label\", ColNames.ue_label_rule: \"ue_na\", ColNames.location_label_rule: \"loc_na\"}\n        )\n    )\n\n    labels_df = (\n        labels_df.withColumn(ColNames.season, F.lit(self.season))\n        .withColumn(ColNames.start_date, F.lit(self.start_date))\n        .withColumn(ColNames.end_date, F.lit(self.end_date))\n    )\n\n    return labels_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.cut_tiles_at_gap","title":"<code>cut_tiles_at_gap(ltps_df, gap_ps_threshold, threshold_is_absolute)</code>","text":"<p>Preprocessing function. Given a Long-Term Permanence Metrics dataset, for each user: - sort the grid tiles of the user by LPS - find a high difference (gap) in the LPS values between one grid tile and the next one,   where what is a high difference is defined through the gap_ps_threshold argument. - for each agent, filter out all tiles after this high difference: the remaining tiles are   the \"pre-selected tiles\", which are the output of this function.</p> <p>Parameters:</p> Name Type Description Default <code>ltps_df</code> <code>DataFrame</code> <p>Long-Term Permanence Metrics dataset, for a specific day type and time interval combination.</p> required <code>gap_ps_threshold</code> <code>int / float</code> <p>absolute/relative lps threshold.</p> required <code>threshold_is_absolute</code> <code>bool</code> <p>indicates if gap_ps_threshold is a relative/absolute value.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>dataset with the pre-selected tiles.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>def cut_tiles_at_gap(self, ltps_df: DataFrame, gap_ps_threshold: float, threshold_is_absolute: bool) -&gt; DataFrame:\n    \"\"\"\n    Preprocessing function. Given a Long-Term Permanence Metrics dataset, for each user:\n    - sort the grid tiles of the user by LPS\n    - find a high difference (gap) in the LPS values between one grid tile and the next one,\n      where what is a high difference is defined through the gap_ps_threshold argument.\n    - for each agent, filter out all tiles after this high difference: the remaining tiles are\n      the \"pre-selected tiles\", which are the output of this function.\n\n    Args:\n        ltps_df (DataFrame): Long-Term Permanence Metrics dataset, for a specific day type and\n            time interval combination.\n        gap_ps_threshold (int/float): absolute/relative lps threshold.\n        threshold_is_absolute (bool): indicates if gap_ps_threshold is a relative/absolute value.\n\n    Returns:\n        DataFrame: dataset with the pre-selected tiles.\n    \"\"\"\n    ltps_df = ltps_df.filter(F.col(ColNames.id_type) == \"grid\")\n\n    window = Window.partitionBy(ColNames.user_id_modulo, ColNames.user_id).orderBy(F.desc(ColNames.lps))\n    cumulative_window = window.rowsBetween(Window.unboundedPreceding, Window.currentRow)\n\n    ltps_df = self.add_abs_ps_threshold(ltps_df, window, gap_ps_threshold, threshold_is_absolute)\n\n    pre_selected_tiles_df = (\n        ltps_df.withColumn(\"previous_lps\", F.lag(ColNames.lps, 1).over(window))\n        .withColumn(\"lps_difference\", F.col(\"previous_lps\") - F.col(ColNames.lps))\n        .fillna({\"lps_difference\": 0})\n        .withColumn(\"high_difference\", F.when(F.col(\"lps_difference\") &gt;= F.col(\"abs_ps_threshold\"), 1).otherwise(0))\n        .withColumn(\"cumulative_condition\", F.sum(F.col(\"high_difference\")).over(cumulative_window))\n        .filter(F.col(\"cumulative_condition\") == F.lit(0))\n        .drop(\"lps_difference\", \"previous_lps\", \"high_difference\", \"cumulative_condition\", \"abs_ps_threshold\")\n    )\n\n    return pre_selected_tiles_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.discard_devices","title":"<code>discard_devices(ltps_df)</code>","text":"<p>Given a Long-Term Permanence Score dataset (filtered for the corresponding start and end dates), discard the rows corresponding to some devices.</p> There are 2 type of devices to discard <ul> <li>rarely observed devices, based on LPS (long-term permanence score).</li> <li>discontinuously observed devices, based on frequency.</li> </ul> <p>The user ids that are classified in any of these 2 groups are discarded from the Long-Term Permanence Score dataset, and the number of discarded users of each kind is saved to the corresponding attributes.</p> <p>Parameters:</p> Name Type Description Default <code>ltps_df</code> <code>DataFrame</code> <p>Long-Term Permanence Score dataset (filtered for the corresponding start and end dates).</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Long-Term Permanence Score dataset (filtered for the corresponding start and end dates), without the rows associated to rarely or discontinuously observed devices.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>def discard_devices(self, ltps_df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Given a Long-Term Permanence Score dataset (filtered for the corresponding start and end dates),\n    discard the rows corresponding to some devices.\n\n    There are 2 type of devices to discard:\n        - rarely observed devices, based on LPS (long-term permanence score).\n        - discontinuously observed devices, based on frequency.\n\n    The user ids that are classified in any of these 2 groups are discarded from the Long-Term\n    Permanence Score dataset, and the number of discarded users of each kind is saved to the\n    corresponding attributes.\n\n    Args:\n        ltps_df (DataFrame): Long-Term Permanence Score dataset (filtered for the corresponding\n            start and end dates).\n\n    Returns:\n        DataFrame: Long-Term Permanence Score dataset (filtered for the corresponding start and\n            end dates), without the rows associated to rarely or discontinuously observed\n            devices.\n    \"\"\"\n    # Initial filter of ltps dataset to keep total device observation values:\n    total_observations_df = ltps_df.filter(\n        (F.col(ColNames.id_type) == \"device_observation\")\n        &amp; (F.col(ColNames.day_type) == \"all\")\n        &amp; (F.col(ColNames.time_interval) == \"all\")\n    )\n\n    # Rarely observed:\n    rarely_observed_user_ids = self.find_rarely_observed_devices(\n        total_observations_df, self.ps_threshold_for_rare_devices\n    )\n    self.rare_devices_count = rarely_observed_user_ids.count()\n\n    # Discontinuously observed:\n    discontinuously_observed_user_ids = self.find_discontinuously_observed_devices(\n        total_observations_df, self.freq_threshold_for_discontinuous_devices\n    )\n    self.discontinuous_devices_count = discontinuously_observed_user_ids.count()\n\n    # All user ids to discard:\n    discardable_user_ids = rarely_observed_user_ids.union(discontinuously_observed_user_ids)\n\n    # Filter dataset:\n    filtered_ltps_df = self.discard_user_ids(ltps_df, discardable_user_ids)\n\n    return filtered_ltps_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.discard_user_ids","title":"<code>discard_user_ids(ltps_df, user_ids)</code>  <code>staticmethod</code>","text":"<p>Given a list of user ids, discard them from a Long-Term Permanence Score dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ltps_df</code> <code>DataFrame</code> <p>Long-Term Permanence Score dataset.</p> required <code>user_ids</code> <code>DataFrame</code> <p>one-column dataframe with the ids of the devices to discard.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>filtered Long-Term Permanence Score dataset.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>@staticmethod\ndef discard_user_ids(ltps_df: DataFrame, user_ids: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Given a list of user ids, discard them from a Long-Term Permanence Score dataset.\n\n    Args:\n        ltps_df (DataFrame): Long-Term Permanence Score dataset.\n        user_ids (DataFrame): one-column dataframe with the ids of the devices to discard.\n\n    Returns:\n        DataFrame: filtered Long-Term Permanence Score dataset.\n    \"\"\"\n    ltps_df = ltps_df.join(user_ids, on=[ColNames.user_id_modulo, ColNames.user_id], how=\"left_anti\")\n    return ltps_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.filter_by_ndays","title":"<code>filter_by_ndays(ltps_df)</code>  <code>staticmethod</code>","text":"<p>Apply the frequency filter (number of days) to a Long-Term Permanence Metrics dataset. Filter rows for which 'total_frequency' &gt; abs_ndays_threshold.</p> <p>Parameters:</p> Name Type Description Default <code>ltps_df</code> <code>DataFrame</code> <p>Long-Term Permanence Metrics dataset, for a specific day type and time interval combination.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Long-Term Permanence Metrics dataset, for a specific day type and time interval combination, keeping only the rows that DO pass the ndays filter.</p> <code>DataFrame</code> <code>DataFrame</code> <p>Long-Term Permanence Metrics dataset, for a specific day type and time interval combination, keeping only the rows that DO NOT pass the ndays filter.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>@staticmethod\ndef filter_by_ndays(ltps_df: DataFrame) -&gt; Tuple[DataFrame, DataFrame]:\n    \"\"\"\n    Apply the frequency filter (number of days) to a Long-Term Permanence Metrics dataset. Filter rows\n    for which 'total_frequency' &gt; abs_ndays_threshold.\n\n    Args:\n        ltps_df (DataFrame): Long-Term Permanence Metrics dataset, for a specific day type and time interval\n            combination.\n\n    Returns:\n        DataFrame: Long-Term Permanence Metrics dataset, for a specific day type and time interval combination,\n            keeping only the rows that DO pass the ndays filter.\n        DataFrame: Long-Term Permanence Metrics dataset, for a specific day type and time interval combination,\n            keeping only the rows that DO NOT pass the ndays filter.\n    \"\"\"\n    common_df = ltps_df.withColumn(\n        \"selected_flag\",\n        F.when(F.col(ColNames.total_frequency) &gt;= F.col(\"abs_ndays_threshold\"), True).otherwise(False),\n    )\n\n    selected_tiles_df = common_df.filter(F.col(\"selected_flag\")).drop(\"abs_ndays_threshold\", \"selected_flag\")\n    not_selected_tiles_df = common_df.filter(~F.col(\"selected_flag\")).drop(\"abs_ndays_threshold\", \"selected_flag\")\n\n    return selected_tiles_df, not_selected_tiles_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.filter_by_ps","title":"<code>filter_by_ps(ltps_df)</code>  <code>staticmethod</code>","text":"<p>Apply the long-term permanence score filter (ps) to a Long-Term Permanence Metrics dataset. Filter rows for which 'lps' &gt; abs_ps_threshold.</p> <p>Parameters:</p> Name Type Description Default <code>ltps_df</code> <code>DataFrame</code> <p>Long-Term Permanence Metrics dataset, for a specific day type and time interval combination.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Long-Term Permanence Metrics dataset, for a specific day type and time interval combination, keeping only the rows that DO pass the ps filter.</p> <code>DataFrame</code> <code>DataFrame</code> <p>Long-Term Permanence Metrics dataset, for a specific day type and time interval combination, keeping only the rows that DO NOT pass the ps filter.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>@staticmethod\ndef filter_by_ps(ltps_df: DataFrame) -&gt; Tuple[DataFrame, DataFrame]:\n    \"\"\"\n    Apply the long-term permanence score filter (ps) to a Long-Term Permanence Metrics dataset. Filter rows for\n    which 'lps' &gt; abs_ps_threshold.\n\n    Args:\n        ltps_df (DataFrame): Long-Term Permanence Metrics dataset, for a specific day type and time interval\n            combination.\n\n    Returns:\n        DataFrame: Long-Term Permanence Metrics dataset, for a specific day type and time interval combination,\n            keeping only the rows that DO pass the ps filter.\n        DataFrame: Long-Term Permanence Metrics dataset, for a specific day type and time interval combination,\n            keeping only the rows that DO NOT pass the ps filter.\n    \"\"\"\n    common_df = ltps_df.withColumn(\n        \"selected_flag\", F.when(F.col(ColNames.lps) &gt;= F.col(\"abs_ps_threshold\"), True).otherwise(False)\n    )\n\n    selected_tiles_df = common_df.filter(F.col(\"selected_flag\")).drop(\"abs_ps_threshold\", \"selected_flag\")\n    not_selected_tiles_df = common_df.filter(~F.col(\"selected_flag\")).drop(\"abs_ps_threshold\", \"selected_flag\")\n\n    return selected_tiles_df, not_selected_tiles_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.filter_ltps_by_target_dates","title":"<code>filter_ltps_by_target_dates(full_ltps_df, start_date, end_date, season)</code>  <code>staticmethod</code>","text":"<p>Keep only the rows of the input SilverLongtermPermanenceScoreDataObject in which the start date, end date and season match the ones specified for the processing of this method.</p> <p>Parameters:</p> Name Type Description Default <code>full_ltps_df</code> <code>DataFrame</code> <p>full dataset.</p> required <code>start_date</code> <code>date</code> <p>specified target start date for the execution of the method.</p> required <code>end_date</code> <code>date</code> <p>specified target end date for the execution of the method.</p> required <code>season</code> <code>str</code> <p>specified target season for the execution of the method.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>filtered dataset.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>@staticmethod\ndef filter_ltps_by_target_dates(\n    full_ltps_df: DataFrame, start_date: dt.date, end_date: dt.date, season: str\n) -&gt; DataFrame:\n    \"\"\"\n    Keep only the rows of the input SilverLongtermPermanenceScoreDataObject in which the start\n    date, end date and season match the ones specified for the processing of this method.\n\n    Args:\n        full_ltps_df (DataFrame): full dataset.\n        start_date (dt.date): specified target start date for the execution of the method.\n        end_date (dt.date): specified target end date for the execution of the method.\n        season (str): specified target season for the execution of the method.\n\n    Returns:\n        DataFrame: filtered dataset.\n    \"\"\"\n    filtered_ltps_df = full_ltps_df.filter(\n        (F.col(ColNames.start_date) == start_date)\n        &amp; (F.col(ColNames.end_date) == end_date)\n        &amp; (F.col(ColNames.season) == season)\n    ).select(\n        ColNames.user_id,\n        ColNames.grid_id,\n        ColNames.lps,\n        ColNames.total_frequency,\n        ColNames.day_type,\n        ColNames.time_interval,\n        ColNames.user_id_modulo,\n        ColNames.id_type,\n    )\n    return filtered_ltps_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.find_discontinuously_observed_devices","title":"<code>find_discontinuously_observed_devices(total_observations_df, total_device_freq_threshold)</code>  <code>staticmethod</code>","text":"<p>Find devices (user ids) which match the condition for being considered \"discontinuously observed\". This condition consists in having a total device observation of: freq &gt; total_freq_threshold.</p> <p>Parameters:</p> Name Type Description Default <code>total_observations_df</code> <code>DataFrame</code> <p>Long-Term Permanence Score dataset, filtered by the corresponding start and end dates, and filtered by id_type == 'device_observation'.</p> required <code>total_device_freq_threshold</code> <code>int</code> <p>ps threshold.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>two-column dataframe with the ids of the devices to discard and the corresponding user modulos.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>@staticmethod\ndef find_discontinuously_observed_devices(\n    total_observations_df: DataFrame, total_device_freq_threshold: int\n) -&gt; DataFrame:\n    \"\"\"\n    Find devices (user ids) which match the condition for being considered \"discontinuously observed\".\n    This condition consists in having a total device observation of: freq &gt; total_freq_threshold.\n\n    Args:\n        total_observations_df (DataFrame): Long-Term Permanence Score dataset, filtered by the\n            corresponding start and end dates, and filtered by id_type == 'device_observation'.\n        total_device_freq_threshold (int): ps threshold.\n\n    Returns:\n        DataFrame: two-column dataframe with the ids of the devices to discard and the corresponding user modulos.\n    \"\"\"\n    discontinuously_observed_user_ids = total_observations_df.filter(\n        F.col(ColNames.total_frequency) &lt; total_device_freq_threshold\n    ).select(ColNames.user_id_modulo, ColNames.user_id)\n    return discontinuously_observed_user_ids\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.find_rarely_observed_devices","title":"<code>find_rarely_observed_devices(total_observations_df, total_device_ps_threshold)</code>  <code>staticmethod</code>","text":"<p>Find devices (user ids) which match the condition for being considered \"rarely observed\". This condition consists in having a total device observation of: lps &gt; total_ps_threshold.</p> <p>Parameters:</p> Name Type Description Default <code>total_observations_df</code> <code>DataFrame</code> <p>Long-Term Permanence Score dataset, filtered by the corresponding start and end dates, and filtered by id_type == 'device_observation'.</p> required <code>total_device_ps_threshold</code> <code>int</code> <p>ps threshold.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>two-column dataframe with the ids of the devices to discard and the corresponding user modulos.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>@staticmethod\ndef find_rarely_observed_devices(total_observations_df: DataFrame, total_device_ps_threshold: int) -&gt; DataFrame:\n    \"\"\"\n    Find devices (user ids) which match the condition for being considered \"rarely observed\".\n    This condition consists in having a total device observation of: lps &gt; total_ps_threshold.\n\n    Args:\n        total_observations_df (DataFrame): Long-Term Permanence Score dataset, filtered by the\n            corresponding start and end dates, and filtered by id_type == 'device_observation'.\n        total_device_ps_threshold (int): ps threshold.\n\n    Returns:\n        DataFrame: two-column dataframe with the ids of the devices to discard and the corresponding user modulos.\n    \"\"\"\n    rarely_observed_user_ids = total_observations_df.filter(F.col(ColNames.lps) &lt; total_device_ps_threshold).select(\n        ColNames.user_id_modulo, ColNames.user_id\n    )\n    return rarely_observed_user_ids\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.format_not_selected_tiles","title":"<code>format_not_selected_tiles(not_selected_tiles_df, label_type)</code>","text":"<p>Add the \"label\", \"ue_label_rule\" and \"location_label_rule\" columns to the not selected tiles dataframe: - the \"label\" column is assigned directly depending on the label type being computed by mapping with the   provided dictionary: LABEL_TO_LABELNAMES[label_type]. - the \"ue_label_rule\" is equal to \"ue_na\". - the \"location_label_rule\" is equal to \"loc_na\".</p> <p>Parameters:</p> Name Type Description Default <code>not_selected_tiles_df</code> <code>DataFrame</code> <p>not selected tiles dataset.</p> required <code>label_type</code> <code>str</code> <p>label type to compute: 'ue', 'home' or 'work'.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>not selected tiles dataset, with the additional columns: \"label\", \"ue_label_rule\" and \"location_label_rule\".</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>def format_not_selected_tiles(self, not_selected_tiles_df: DataFrame, label_type: str) -&gt; DataFrame:\n    \"\"\"\n    Add the \"label\", \"ue_label_rule\" and \"location_label_rule\" columns to the not selected tiles dataframe:\n    - the \"label\" column is assigned directly depending on the label type being computed by mapping with the\n      provided dictionary: LABEL_TO_LABELNAMES[label_type].\n    - the \"ue_label_rule\" is equal to \"ue_na\".\n    - the \"location_label_rule\" is equal to \"loc_na\".\n\n    Args:\n        not_selected_tiles_df (DataFrame): not selected tiles dataset.\n        label_type (str): label type to compute: 'ue', 'home' or 'work'.\n\n    Returns:\n        DataFrame: not selected tiles dataset, with the additional columns: \"label\", \"ue_label_rule\" and\n            \"location_label_rule\".\n    \"\"\"\n    if label_type == \"ue\":\n        not_selected_tiles_df = not_selected_tiles_df.withColumn(ColNames.ue_label_rule, F.lit(\"ue_na\"))\n    else:\n        labelname = self.LABEL_TO_LABELNAMES[label_type]\n        not_selected_tiles_df = not_selected_tiles_df.withColumn(ColNames.label, F.lit(labelname))\n        not_selected_tiles_df = not_selected_tiles_df.withColumn(ColNames.location_label_rule, F.lit(\"loc_na\"))\n\n    return not_selected_tiles_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.format_selected_tiles","title":"<code>format_selected_tiles(selected_tiles_df, label_type, n_rule)</code>","text":"<p>Add the \"label\", \"ue_label_rule\" and \"location_label_rule\" columns to the given selected tiles dataframe: - the \"label\" column is assigned directly depending on the label type being computed by mapping with the   provided dictionary: LABEL_TO_LABELNAMES[label_type]. - the \"ue_label_rule\" is equal to \"ue_na\" if the label type being computed is different from \"ue\", or is   equal to \"{ue}{N}\" if the label type being computed is \"ue\", where:     - {N} = n_rule     - {ue} = LABEL_TO_SHORT_LABELNAMES[label_type]. - the \"location_label_rule\" is equal to \"loc_na\" if the label type being computed is \"ue\", or is equal to   \"{loc}{N}\" if the label type being computed is NOT \"ue\", where:     - {N} = n_rule     - {loc} = LABEL_TO_SHORT_LABELNAMES[label_type].</p> <p>Parameters:</p> Name Type Description Default <code>selected_tiles_df</code> <code>DataFrame</code> <p>selected tiles dataset.</p> required <code>label_type</code> <code>str</code> <p>label type to compute: 'ue', 'home' or 'work'.</p> required <code>n_rule</code> <code>int</code> <p>number of the current rule in order to generate the label_rule column values.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>selected tiles dataset, with the additional columns: \"label\", \"ue_label_rule\" and \"location_label_rule\".</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>def format_selected_tiles(self, selected_tiles_df: DataFrame, label_type: str, n_rule: int) -&gt; DataFrame:\n    \"\"\"\n    Add the \"label\", \"ue_label_rule\" and \"location_label_rule\" columns to the given selected tiles dataframe:\n    - the \"label\" column is assigned directly depending on the label type being computed by mapping with the\n      provided dictionary: LABEL_TO_LABELNAMES[label_type].\n    - the \"ue_label_rule\" is equal to \"ue_na\" if the label type being computed is different from \"ue\", or is\n      equal to \"{ue}_{N}\" if the label type being computed is \"ue\", where:\n        - {N} = n_rule\n        - {ue} = LABEL_TO_SHORT_LABELNAMES[label_type].\n    - the \"location_label_rule\" is equal to \"loc_na\" if the label type being computed is \"ue\", or is equal to\n      \"{loc}_{N}\" if the label type being computed is NOT \"ue\", where:\n        - {N} = n_rule\n        - {loc} = LABEL_TO_SHORT_LABELNAMES[label_type].\n\n    Args:\n        selected_tiles_df (DataFrame): selected tiles dataset.\n        label_type (str): label type to compute: 'ue', 'home' or 'work'.\n        n_rule (int): number of the current rule in order to generate the label_rule column values.\n\n    Returns:\n        DataFrame: selected tiles dataset, with the additional columns: \"label\", \"ue_label_rule\" and\n            \"location_label_rule\".\n    \"\"\"\n    short_labelname = self.LABEL_TO_SHORT_LABELNAMES[label_type]\n\n    if label_type == \"ue\":\n        ue_rule_txt = f\"{short_labelname}_{n_rule}\"\n        selected_tiles_df = selected_tiles_df.withColumn(ColNames.ue_label_rule, F.lit(ue_rule_txt))\n    else:\n        labelname = self.LABEL_TO_LABELNAMES[label_type]\n        selected_tiles_df = selected_tiles_df.withColumn(ColNames.label, F.lit(labelname))\n        loc_rule_txt = f\"{short_labelname}_{n_rule}\"\n        selected_tiles_df = selected_tiles_df.withColumn(ColNames.location_label_rule, F.lit(loc_rule_txt))\n\n    return selected_tiles_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.generate_quality_metrics","title":"<code>generate_quality_metrics(ue_labels_df)</code>","text":"<p>Build usual environment labeling quality metrics dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>ue_labels_df</code> <code>DataFrame</code> <p>usual environment labels dataframe.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>quality metrics dataframe.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>def generate_quality_metrics(self, ue_labels_df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Build usual environment labeling quality metrics dataframe.\n\n    Args:\n        ue_labels_df (DataFrame): usual environment labels dataframe.\n\n    Returns:\n        DataFrame: quality metrics dataframe.\n    \"\"\"\n    grouped_df = ue_labels_df.groupby(ColNames.label, ColNames.ue_label_rule, ColNames.location_label_rule).count()\n    grouped_df.cache()\n\n    ue_1_rule_count = self.get_rule_count(grouped_df, {ColNames.ue_label_rule: \"ue_1\"})\n    ue_2_rule_count = self.get_rule_count(grouped_df, {ColNames.ue_label_rule: \"ue_2\"})\n    ue_3_rule_count = self.get_rule_count(grouped_df, {ColNames.ue_label_rule: \"ue_3\"})\n    h_1_rule_count = self.get_rule_count(grouped_df, {ColNames.location_label_rule: \"h_1\"})\n    h_2_rule_count = self.get_rule_count(grouped_df, {ColNames.location_label_rule: \"h_2\"})\n    h_3_rule_count = self.get_rule_count(grouped_df, {ColNames.location_label_rule: \"h_3\"})\n    w_1_rule_count = self.get_rule_count(grouped_df, {ColNames.location_label_rule: \"w_1\"})\n    w_2_rule_count = self.get_rule_count(grouped_df, {ColNames.location_label_rule: \"w_2\"})\n    w_3_rule_count = self.get_rule_count(grouped_df, {ColNames.location_label_rule: \"w_3\"})\n    ue_na_rule_count = self.get_rule_count(grouped_df, {ColNames.ue_label_rule: \"ue_na\"})\n    loc_na_rule_count = self.get_rule_count(grouped_df, {ColNames.location_label_rule: \"loc_na\"})\n    h_non_ue_count = self.get_rule_count(grouped_df, {ColNames.label: \"home\", ColNames.ue_label_rule: \"ue_na\"})\n    w_non_ue_count = self.get_rule_count(grouped_df, {ColNames.label: \"work\", ColNames.ue_label_rule: \"ue_na\"})\n\n    grouped_df.unpersist()\n\n    data = [\n        (\"device_filter_1_rule\", self.rare_devices_count, self.start_date, self.end_date, self.season),\n        (\"device_filter_2_rule\", self.discontinuous_devices_count, self.start_date, self.end_date, self.season),\n        (\"ue_1_rule\", ue_1_rule_count, self.start_date, self.end_date, self.season),\n        (\"ue_2_rule\", ue_2_rule_count, self.start_date, self.end_date, self.season),\n        (\"ue_3_rule\", ue_3_rule_count, self.start_date, self.end_date, self.season),\n        (\"h_1_rule\", h_1_rule_count, self.start_date, self.end_date, self.season),\n        (\"h_2_rule\", h_2_rule_count, self.start_date, self.end_date, self.season),\n        (\"h_3_rule\", h_3_rule_count, self.start_date, self.end_date, self.season),\n        (\"w_1_rule\", w_1_rule_count, self.start_date, self.end_date, self.season),\n        (\"w_2_rule\", w_2_rule_count, self.start_date, self.end_date, self.season),\n        (\"w_3_rule\", w_3_rule_count, self.start_date, self.end_date, self.season),\n        (\"ue_na_rule\", ue_na_rule_count, self.start_date, self.end_date, self.season),\n        (\"loc_na_rule\", loc_na_rule_count, self.start_date, self.end_date, self.season),\n        (\"h_non_ue\", h_non_ue_count, self.start_date, self.end_date, self.season),\n        (\"w_non_ue\", w_non_ue_count, self.start_date, self.end_date, self.season),\n    ]\n\n    # Create DataFrame\n    schema = SilverUsualEnvironmentLabelingQualityMetricsDataObject.SCHEMA\n    quality_metrics_df = self.spark.createDataFrame(data, schema)\n\n    return quality_metrics_df\n</code></pre>"},{"location":"reference/components/execution/usual_environment_labeling/usual_environment_labeling/#components.execution.usual_environment_labeling.usual_environment_labeling.UsualEnvironmentLabeling.get_rule_count","title":"<code>get_rule_count(df, col_to_value)</code>  <code>staticmethod</code>","text":"<p>Sums the count column values of the given dataframe for the corresponding filter (col_to_value).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>input dataframe.</p> required <code>col_to_value</code> <code>Dict[str, str]</code> <p>filter to apply.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>count column sum.</p> Source code in <code>multimno/components/execution/usual_environment_labeling/usual_environment_labeling.py</code> <pre><code>@staticmethod\ndef get_rule_count(df: DataFrame, col_to_value: Dict[str, str]) -&gt; int:\n    \"\"\"\n    Sums the count column values of the given dataframe for the corresponding filter (col_to_value).\n\n    Args:\n        df (DataFrame): input dataframe.\n        col_to_value (Dict[str, str]): filter to apply.\n\n    Returns:\n        int: count column sum.\n    \"\"\"\n    conditions = (F.col(colname) == colvalue for colname, colvalue in col_to_value.items())\n    combined_condition = reduce(lambda x, y: x &amp; y, conditions)\n    count_value = df.filter(combined_condition).agg(F.sum(\"count\")).collect()[0][0]\n    if count_value is None:\n        count_value = 0\n    return count_value\n</code></pre>"},{"location":"reference/components/ingestion/","title":"ingestion","text":""},{"location":"reference/components/ingestion/data_filtering/","title":"data_filtering","text":""},{"location":"reference/components/ingestion/data_filtering/data_filtering/","title":"data_filtering","text":"<p>Module that cleans RAW MNO Event and Network Topology data.</p>"},{"location":"reference/components/ingestion/data_filtering/data_filtering/#components.ingestion.data_filtering.data_filtering.DataFiltering","title":"<code>DataFiltering</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class that filters MNO Event and network topology data</p> Source code in <code>multimno/components/ingestion/data_filtering/data_filtering.py</code> <pre><code>class DataFiltering(Component):\n    \"\"\"\n    Class that filters MNO Event and network topology data\n    \"\"\"\n\n    COMPONENT_ID = \"DataFiltering\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.data_period_start = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n        self.data_period_end = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n\n        self.data_period_dates = [\n            self.data_period_start + timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n        self.do_spatial_filtering = self.config.getboolean(\n            self.COMPONENT_ID,\n            \"do_spatial_filtering\",\n        )\n\n        self.spatial_filtering_variant = self.config.getint(\n            self.COMPONENT_ID,\n            \"spatial_filtering_variant\",\n        )\n\n        self.extent = self.config.geteval(self.COMPONENT_ID, \"extent\")\n\n        self.do_device_sampling = self.config.getboolean(\n            self.COMPONENT_ID,\n            \"do_device_sampling\",\n        )\n\n        self.sample_size = self.config.getfloat(\n            self.COMPONENT_ID,\n            \"sample_size\",\n        )\n\n        self.reference_polygon = self.config.get(self.COMPONENT_ID, \"reference_polygon\")\n\n        self.current_date = None\n\n        self.repartition_num = self.config.getint(self.COMPONENT_ID, \"repartition_num\")\n        self.sample_seed = self.config.getint(self.COMPONENT_ID, \"sample_seed\")\n        self.devices_subset = None\n\n    def initalize_data_objects(self):\n        # Input\n        self.input_data_objects = {}\n\n        inputs = {\n            \"event_data_bronze\": BronzeEventDataObject,\n            \"network_data_bronze\": BronzeNetworkDataObject,\n        }\n        self.spatial_filtering_mask = self.config.get(self.COMPONENT_ID, \"spatial_filtering_mask\")\n        for key, value in inputs.items():\n            path = self.config.get(CONFIG_BRONZE_PATHS_KEY, key)\n            if check_if_data_path_exists(self.spark, path):\n                self.input_data_objects[value.ID] = value(self.spark, path)\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")\n                raise ValueError(f\"Invalid path for {value.ID}: {path}\")\n\n        if self.spatial_filtering_mask == \"polygon\":\n            self.input_data_objects[BronzeCountriesDataObject.ID] = BronzeCountriesDataObject(\n                self.spark,\n                self.config.get(CONFIG_BRONZE_PATHS_KEY, \"countries_data_bronze\"),\n            )\n\n        # Output\n        bronze_dir = self.config.get(CONFIG_PATHS_KEY, \"bronze_dir\")\n        bronze_dir_sample = self.config.get(CONFIG_PATHS_KEY, \"bronze_dir_sample\")\n        self.clear_destination_directory = self.config.getboolean(self.COMPONENT_ID, \"clear_destination_directory\")\n        self.output_data_objects = {}\n\n        outputs = {\n            \"event_data_bronze\": BronzeEventDataObject,\n            \"network_data_bronze\": BronzeNetworkDataObject,\n        }\n\n        for key, value in outputs.items():\n            path = self.config.get(CONFIG_BRONZE_PATHS_KEY, key)\n            path = path.replace(bronze_dir, bronze_dir_sample)\n            if self.clear_destination_directory:\n                delete_file_or_folder(self.spark, path)\n            self.output_data_objects[value.ID] = value(self.spark, path)\n\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        if self.do_device_sampling:\n            self.logger.info(f\"Sampling devices for data period with sample size {self.sample_size}\")\n            self.input_data_objects[BronzeEventDataObject.ID].read()\n            events_sdf = self.input_data_objects[BronzeEventDataObject.ID].df\n            # Get users in first day\n            events_sdf = events_sdf.filter(\n                F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n                == F.lit(self.data_period_dates[0])\n            )\n            self.devices_subset = self.sample_devices(events_sdf, self.sample_size, self.sample_seed)\n            self.devices_subset.cache()\n            self.logger.info(f\"Devices sampled: {self.devices_subset.count()}\")\n        for current_date in self.data_period_dates:\n            self.current_date = current_date\n            self.logger.info(f\"Processing {current_date}\")\n            self.read()\n            self.transform()\n            self.write()\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n        events_sdf = self.input_data_objects[BronzeEventDataObject.ID].df.filter(\n            F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(self.current_date)\n        )\n        cells_sdf = self.input_data_objects[BronzeNetworkDataObject.ID].df.filter(\n            F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(self.current_date)\n        )\n        if self.spatial_filtering_mask == \"polygon\":\n            polygons = self.input_data_objects[BronzeCountriesDataObject.ID].df.filter(\n                F.col(ColNames.iso2) == self.reference_polygon\n            )\n            polygons = utils.project_to_crs(polygons, 3035, 4326)\n\n        if self.do_spatial_filtering:\n            if self.spatial_filtering_variant == 1:\n                if self.spatial_filtering_mask == \"polygon\":\n                    events_sdf, cells_sdf = self.filter_by_spatial_area(events_sdf, cells_sdf, polygons)\n                elif self.spatial_filtering_mask == \"extent\":\n                    events_sdf, cells_sdf = self.filter_by_spatial_area(events_sdf, cells_sdf, bbox=self.extent)\n            elif self.spatial_filtering_variant == 2:\n                if self.spatial_filtering_mask == \"polygon\":\n                    events_sdf, cells_sdf = self.filter_by_devices_in_spatial_area(events_sdf, cells_sdf, polygons)\n                elif self.spatial_filtering_mask == \"extent\":\n                    events_sdf, cells_sdf = self.filter_by_devices_in_spatial_area(\n                        events_sdf, cells_sdf, bbox=self.extent\n                    )\n\n        if self.do_device_sampling:\n            events_sdf = self.sample_events(events_sdf, self.devices_subset)\n\n        # --- Export actions ---\n        # Schema casting\n        events_sdf = utils.apply_schema_casting(events_sdf, BronzeEventDataObject.SCHEMA)\n        cells_sdf = utils.apply_schema_casting(cells_sdf, BronzeNetworkDataObject.SCHEMA)\n\n        # Partition output data (Avoid generating thousands of small files)\n        events_sdf = events_sdf.repartition(self.repartition_num)\n        cells_sdf = cells_sdf.repartition(self.repartition_num)\n\n        self.output_data_objects[BronzeNetworkDataObject.ID].df = cells_sdf\n        self.output_data_objects[BronzeEventDataObject.ID].df = events_sdf\n\n    @staticmethod\n    def filter_cells_bbox(cells_sdf: DataFrame, bbox: List) -&gt; DataFrame:\n        \"\"\"\n        Filters cells DataFrame based on bounding box.\n        \"\"\"\n        bbox_filter = (\n            (F.col(ColNames.longitude) &gt;= bbox[0])\n            &amp; (F.col(ColNames.longitude) &lt;= bbox[2])\n            &amp; (F.col(ColNames.latitude) &gt;= bbox[1])\n            &amp; (F.col(ColNames.latitude) &lt;= bbox[3])\n        )\n        return cells_sdf.filter(bbox_filter)\n\n    @staticmethod\n    def filter_cells_polygon(cells_sdf: DataFrame, polygons_sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Filters cells DataFrame based on polygon.\n        \"\"\"\n        cells_sdf = cells_sdf.withColumn(\n            ColNames.geometry, STC.ST_Point(ColNames.longitude, ColNames.latitude)\n        ).withColumn(ColNames.geometry, STF.ST_SetSRID(ColNames.geometry, 4326))\n\n        cells_sdf = cells_sdf.join(\n            polygons_sdf, STP.ST_Intersects(cells_sdf[ColNames.geometry], polygons_sdf[ColNames.geometry]), \"inner\"\n        ).drop(polygons_sdf[ColNames.geometry])\n\n        return cells_sdf\n\n    @staticmethod\n    def filter_by_spatial_area(\n        events_sdf: DataFrame, cells_sdf: DataFrame, polygons_sdf: DataFrame = None, bbox: List = None\n    ) -&gt; DataFrame:\n        \"\"\"\n        Filters events DataFrame based on spatial area.\n        \"\"\"\n        if polygons_sdf is not None:\n            cells_sdf = DataFiltering.filter_cells_polygon(cells_sdf, polygons_sdf)\n        elif bbox is not None:\n            cells_sdf = DataFiltering.filter_cells_bbox(cells_sdf, bbox)\n\n        events_sdf = events_sdf.join(cells_sdf.select(ColNames.cell_id), on=ColNames.cell_id, how=\"inner\")\n\n        return events_sdf, cells_sdf\n\n    @staticmethod\n    def filter_by_devices_in_spatial_area(\n        events_sdf: DataFrame, cells_sdf: DataFrame, polygons_sdf: DataFrame = None, bbox: List = None\n    ) -&gt; DataFrame:\n        \"\"\"\n        Filters events DataFrame based on devices in bounding box.\n        \"\"\"\n        # Get cells in the bounding box\n        if polygons_sdf is not None:\n            cells_in_area_sdf = DataFiltering.filter_cells_polygon(cells_sdf, polygons_sdf)\n        elif bbox is not None:\n            cells_in_area_sdf = DataFiltering.filter_cells_bbox(cells_sdf, bbox)\n\n        # Filter events based on cell_id being in the cells from the bounding box\n        events_in_bbox_sdf = events_sdf.join(\n            cells_in_area_sdf.select(ColNames.cell_id), on=ColNames.cell_id, how=\"inner\"\n        )\n\n        # Get distinct devices in the filtered events\n        devices_in_bbox_sdf = events_in_bbox_sdf.select(ColNames.user_id).distinct()\n\n        # Filter original events by the devices in the bounding box\n        events_sdf = events_sdf.join(devices_in_bbox_sdf, on=ColNames.user_id, how=\"inner\")\n\n        # Filter cells based on the distinct cell IDs in the filtered events\n        all_cells_sdf = events_sdf.select(ColNames.cell_id).distinct()\n\n        cells_sdf = cells_sdf.join(all_cells_sdf, on=ColNames.cell_id, how=\"inner\")\n\n        return events_sdf, cells_sdf\n\n    @staticmethod\n    def sample_devices(events_sdf: DataFrame, sample_size: float, sample_seed: int) -&gt; DataFrame:\n\n        devices_sdf = events_sdf.select(ColNames.user_id).distinct()\n        devices_sdf = devices_sdf.sample(False, sample_size, sample_seed)\n\n        return devices_sdf\n\n    @staticmethod\n    def sample_events(events_sdf: DataFrame, devices_sdf: DataFrame) -&gt; DataFrame:\n\n        events_sdf = events_sdf.join(devices_sdf, on=ColNames.user_id, how=\"inner\")\n\n        return events_sdf\n</code></pre>"},{"location":"reference/components/ingestion/data_filtering/data_filtering/#components.ingestion.data_filtering.data_filtering.DataFiltering.filter_by_devices_in_spatial_area","title":"<code>filter_by_devices_in_spatial_area(events_sdf, cells_sdf, polygons_sdf=None, bbox=None)</code>  <code>staticmethod</code>","text":"<p>Filters events DataFrame based on devices in bounding box.</p> Source code in <code>multimno/components/ingestion/data_filtering/data_filtering.py</code> <pre><code>@staticmethod\ndef filter_by_devices_in_spatial_area(\n    events_sdf: DataFrame, cells_sdf: DataFrame, polygons_sdf: DataFrame = None, bbox: List = None\n) -&gt; DataFrame:\n    \"\"\"\n    Filters events DataFrame based on devices in bounding box.\n    \"\"\"\n    # Get cells in the bounding box\n    if polygons_sdf is not None:\n        cells_in_area_sdf = DataFiltering.filter_cells_polygon(cells_sdf, polygons_sdf)\n    elif bbox is not None:\n        cells_in_area_sdf = DataFiltering.filter_cells_bbox(cells_sdf, bbox)\n\n    # Filter events based on cell_id being in the cells from the bounding box\n    events_in_bbox_sdf = events_sdf.join(\n        cells_in_area_sdf.select(ColNames.cell_id), on=ColNames.cell_id, how=\"inner\"\n    )\n\n    # Get distinct devices in the filtered events\n    devices_in_bbox_sdf = events_in_bbox_sdf.select(ColNames.user_id).distinct()\n\n    # Filter original events by the devices in the bounding box\n    events_sdf = events_sdf.join(devices_in_bbox_sdf, on=ColNames.user_id, how=\"inner\")\n\n    # Filter cells based on the distinct cell IDs in the filtered events\n    all_cells_sdf = events_sdf.select(ColNames.cell_id).distinct()\n\n    cells_sdf = cells_sdf.join(all_cells_sdf, on=ColNames.cell_id, how=\"inner\")\n\n    return events_sdf, cells_sdf\n</code></pre>"},{"location":"reference/components/ingestion/data_filtering/data_filtering/#components.ingestion.data_filtering.data_filtering.DataFiltering.filter_by_spatial_area","title":"<code>filter_by_spatial_area(events_sdf, cells_sdf, polygons_sdf=None, bbox=None)</code>  <code>staticmethod</code>","text":"<p>Filters events DataFrame based on spatial area.</p> Source code in <code>multimno/components/ingestion/data_filtering/data_filtering.py</code> <pre><code>@staticmethod\ndef filter_by_spatial_area(\n    events_sdf: DataFrame, cells_sdf: DataFrame, polygons_sdf: DataFrame = None, bbox: List = None\n) -&gt; DataFrame:\n    \"\"\"\n    Filters events DataFrame based on spatial area.\n    \"\"\"\n    if polygons_sdf is not None:\n        cells_sdf = DataFiltering.filter_cells_polygon(cells_sdf, polygons_sdf)\n    elif bbox is not None:\n        cells_sdf = DataFiltering.filter_cells_bbox(cells_sdf, bbox)\n\n    events_sdf = events_sdf.join(cells_sdf.select(ColNames.cell_id), on=ColNames.cell_id, how=\"inner\")\n\n    return events_sdf, cells_sdf\n</code></pre>"},{"location":"reference/components/ingestion/data_filtering/data_filtering/#components.ingestion.data_filtering.data_filtering.DataFiltering.filter_cells_bbox","title":"<code>filter_cells_bbox(cells_sdf, bbox)</code>  <code>staticmethod</code>","text":"<p>Filters cells DataFrame based on bounding box.</p> Source code in <code>multimno/components/ingestion/data_filtering/data_filtering.py</code> <pre><code>@staticmethod\ndef filter_cells_bbox(cells_sdf: DataFrame, bbox: List) -&gt; DataFrame:\n    \"\"\"\n    Filters cells DataFrame based on bounding box.\n    \"\"\"\n    bbox_filter = (\n        (F.col(ColNames.longitude) &gt;= bbox[0])\n        &amp; (F.col(ColNames.longitude) &lt;= bbox[2])\n        &amp; (F.col(ColNames.latitude) &gt;= bbox[1])\n        &amp; (F.col(ColNames.latitude) &lt;= bbox[3])\n    )\n    return cells_sdf.filter(bbox_filter)\n</code></pre>"},{"location":"reference/components/ingestion/data_filtering/data_filtering/#components.ingestion.data_filtering.data_filtering.DataFiltering.filter_cells_polygon","title":"<code>filter_cells_polygon(cells_sdf, polygons_sdf)</code>  <code>staticmethod</code>","text":"<p>Filters cells DataFrame based on polygon.</p> Source code in <code>multimno/components/ingestion/data_filtering/data_filtering.py</code> <pre><code>@staticmethod\ndef filter_cells_polygon(cells_sdf: DataFrame, polygons_sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Filters cells DataFrame based on polygon.\n    \"\"\"\n    cells_sdf = cells_sdf.withColumn(\n        ColNames.geometry, STC.ST_Point(ColNames.longitude, ColNames.latitude)\n    ).withColumn(ColNames.geometry, STF.ST_SetSRID(ColNames.geometry, 4326))\n\n    cells_sdf = cells_sdf.join(\n        polygons_sdf, STP.ST_Intersects(cells_sdf[ColNames.geometry], polygons_sdf[ColNames.geometry]), \"inner\"\n    ).drop(polygons_sdf[ColNames.geometry])\n\n    return cells_sdf\n</code></pre>"},{"location":"reference/components/ingestion/grid_generation/","title":"grid_generation","text":""},{"location":"reference/components/ingestion/grid_generation/inspire_grid_generation/","title":"inspire_grid_generation","text":"<p>This module contains the InspireGridGeneration class which is responsible for generating the INSPIRE grid  and enrich it with elevation and landuse data.</p>"},{"location":"reference/components/ingestion/grid_generation/inspire_grid_generation/#components.ingestion.grid_generation.inspire_grid_generation.InspireGridGeneration","title":"<code>InspireGridGeneration</code>","text":"<p>               Bases: <code>Component</code></p> <p>This class is responsible for generating the INSPIRE grid for given extent or polygon and enrich it with elevation and landuse data.</p> Source code in <code>multimno/components/ingestion/grid_generation/inspire_grid_generation.py</code> <pre><code>class InspireGridGeneration(Component):\n    \"\"\"\n    This class is responsible for generating the INSPIRE grid for given extent or polygon\n    and enrich it with elevation and landuse data.\n    \"\"\"\n\n    COMPONENT_ID = \"InspireGridGeneration\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.grid_extent = self.config.geteval(InspireGridGeneration.COMPONENT_ID, \"extent\")\n        self.grid_partition_size = self.config.get(InspireGridGeneration.COMPONENT_ID, \"grid_generation_partition_size\")\n\n        self.grid_quadkey_level = self.config.getint(\n            InspireGridGeneration.COMPONENT_ID,\n            \"grid_processing_partition_quadkey_level\",\n        )\n\n        self.reference_country = self.config.get(InspireGridGeneration.COMPONENT_ID, \"reference_country\")\n\n        self.country_buffer = self.config.get(InspireGridGeneration.COMPONENT_ID, \"country_buffer\")\n\n        # TODO: For now set to default 100, but can be dynamic from config in future\n        self.grid_resolution = 100\n\n        self.grid_generator = InspireGridGenerator(\n            self.spark,\n            self.grid_resolution,\n            ColNames.geometry,\n            ColNames.grid_id,\n            self.grid_partition_size,\n        )\n\n        self.quadkey_udf = F.udf(utils.latlon_to_quadkey)\n\n    def initalize_data_objects(self):\n\n        self.clear_destination_directory = self.config.getboolean(\n            InspireGridGeneration.COMPONENT_ID, \"clear_destination_directory\"\n        )\n\n        self.grid_mask = self.config.get(InspireGridGeneration.COMPONENT_ID, \"grid_mask\")\n\n        if self.grid_mask == \"polygon\":\n            # inputs\n            self.input_data_objects = {}\n            self.input_data_objects[BronzeCountriesDataObject.ID] = BronzeCountriesDataObject(\n                self.spark,\n                self.config.get(CONFIG_BRONZE_PATHS_KEY, \"countries_data_bronze\"),\n            )\n\n        # outputs\n\n        grid_do_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"grid_data_silver\")\n\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, grid_do_path)\n\n        self.output_data_objects = {}\n        self.output_data_objects[SilverGridDataObject.ID] = SilverGridDataObject(\n            self.spark, grid_do_path, [ColNames.quadkey]\n        )\n\n    @get_execution_stats\n    def execute(self):\n\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n\n        if self.grid_mask == \"polygon\":\n            self.read()\n\n            countries = self.get_country_mask(self.reference_country, self.country_buffer)\n            ids = [row[\"temp_id\"] for row in countries.select(\"temp_id\").collect()]\n\n            self.logger.info(f\"Processing {len(ids)} parts of the country\")\n            processed_parts = 0\n            for id in ids:\n                self.current_country_part = countries.filter(F.col(\"temp_id\") == id)\n                self.transform()\n                self.write()\n                processed_parts += 1\n                self.logger.info(f\"Finished processing {processed_parts} parts\")\n        else:\n            self.transform()\n            self.write()\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}...\")\n\n        if self.grid_mask == \"polygon\":\n            grid_sdf = self.grid_generator.cover_polygon_with_grid_centroids(self.current_country_part)\n        else:\n            grid_sdf = self.grid_generator.cover_extent_with_grid_centroids(self.grid_extent)\n        grid_sdf = utils.assign_quadkey(grid_sdf, 3035, self.grid_quadkey_level)\n\n        grid_sdf = grid_sdf.orderBy(ColNames.quadkey)\n        grid_sdf = grid_sdf.repartition(ColNames.quadkey)\n\n        grid_sdf = utils.apply_schema_casting(grid_sdf, SilverGridDataObject.SCHEMA)\n\n        self.output_data_objects[SilverGridDataObject.ID].df = grid_sdf\n\n    def get_country_mask(self, reference_country, country_buffer):\n\n        countries = self.input_data_objects[BronzeCountriesDataObject.ID].df\n        countries = (\n            countries.filter(F.col(ColNames.iso2) == reference_country)\n            .withColumn(\n                ColNames.geometry,\n                STF.ST_Buffer(ColNames.geometry, F.lit(country_buffer)),\n            )\n            .groupBy()\n            .agg(STA.ST_Union_Aggr(ColNames.geometry).alias(ColNames.geometry))\n            .withColumn(ColNames.geometry, F.explode(STF.ST_Dump(ColNames.geometry)))\n            .withColumn(\"temp_id\", F.monotonically_increasing_id())\n        )\n\n        countries = utils.project_to_crs(countries, 3035, 4326)\n\n        return countries\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/","title":"spatial_data_ingestion","text":""},{"location":"reference/components/ingestion/spatial_data_ingestion/gisco_data_ingestion/","title":"gisco_data_ingestion","text":""},{"location":"reference/components/ingestion/spatial_data_ingestion/gisco_data_ingestion/#components.ingestion.spatial_data_ingestion.gisco_data_ingestion.GiscoDataIngestion","title":"<code>GiscoDataIngestion</code>","text":"<p>               Bases: <code>Component</code></p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/gisco_data_ingestion.py</code> <pre><code>class GiscoDataIngestion(Component):\n    \"\"\" \"\"\"\n\n    COMPONENT_ID = \"GiscoDataIngestion\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n    def initalize_data_objects(self):\n\n        base_url = self.config.get(GiscoDataIngestion.COMPONENT_ID, \"base_url\")\n\n        self.get_countries = self.config.getboolean(GiscoDataIngestion.COMPONENT_ID, \"get_countries\")\n\n        self.get_nuts = self.config.getboolean(GiscoDataIngestion.COMPONENT_ID, \"get_nuts\")\n\n        self.default_crs = self.config.getint(GiscoDataIngestion.COMPONENT_ID, \"default_crs\")\n\n        self.input_data_objects = {}\n        self.output_data_objects = {}\n        if self.get_countries:\n\n            countries_resolution = self.config.get(GiscoDataIngestion.COMPONENT_ID, \"countries_resolution\")\n            countries_year = self.config.get(GiscoDataIngestion.COMPONENT_ID, \"countries_year\")\n\n            countries_url = (\n                f\"{base_url}/countries/geojson/CNTR_RG_{countries_resolution}M_{countries_year}_4326.geojson\"\n            )\n\n            self.input_data_objects[\"countries\"] = LandingHttpGeoJsonDataObject(self.spark, countries_url, 240, 3)\n\n            countries_do_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"countries_data_bronze\")\n\n            self.output_data_objects[BronzeCountriesDataObject.ID] = BronzeCountriesDataObject(\n                self.spark,\n                countries_do_path,\n                [],\n                self.default_crs,\n            )\n\n        if self.get_nuts:\n\n            nuts_resolution = self.config.get(GiscoDataIngestion.COMPONENT_ID, \"nuts_resolution\")\n            self.nuts_year = self.config.get(GiscoDataIngestion.COMPONENT_ID, \"nuts_year\")\n            nuts_levels = self.config.get(GiscoDataIngestion.COMPONENT_ID, \"nuts_levels\")\n            self.nuts_levels = nuts_levels.split(\",\")\n\n            self.reference_country = self.config.get(GiscoDataIngestion.COMPONENT_ID, \"reference_country\")\n            for level in self.nuts_levels:\n                nuts_url = (\n                    f\"{base_url}/nuts/geojson/NUTS_RG_{nuts_resolution}M_{self.nuts_year}_4326_LEVL_{level}.geojson\"\n                )\n                self.input_data_objects[f\"nuts_{level}\"] = LandingHttpGeoJsonDataObject(self.spark, nuts_url, 300, 5)\n\n            geographic_zones_do_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"geographic_zones_data_bronze\")\n\n            self.output_data_objects[BronzeGeographicZonesDataObject.ID] = BronzeGeographicZonesDataObject(\n                self.spark,\n                geographic_zones_do_path,\n                [ColNames.dataset_id],\n                self.default_crs,\n            )\n\n        self.clear_destination_directory = self.config.getboolean(\n            GiscoDataIngestion.COMPONENT_ID, \"clear_destination_directory\"\n        )\n\n        if self.clear_destination_directory:\n            for do in self.output_data_objects.values():\n                self.logger.info(f\"Clearing {do.default_path}\")\n                delete_file_or_folder(self.spark, do.default_path)\n\n    def read(self):\n        # need to read the data from the input data objects separately\n        pass\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n\n        if self.get_countries:\n\n            countries = self.get_countries_data()\n            countries = utils.apply_schema_casting(countries, BronzeCountriesDataObject.SCHEMA)\n            self.output_data_objects[BronzeCountriesDataObject.ID].df = countries\n\n        if self.get_nuts:\n            self.output_data_objects[BronzeGeographicZonesDataObject.ID].df = self.spark.createDataFrame(\n                [], BronzeGeographicZonesDataObject.SCHEMA\n            )\n            for level in self.nuts_levels:\n\n                nuts = self.get_nuts_data(level)\n                nuts = nuts.withColumn(ColNames.dataset_id, F.lit(\"nuts\"))\n                nuts = utils.apply_schema_casting(nuts, BronzeGeographicZonesDataObject.SCHEMA)\n                self.output_data_objects[BronzeGeographicZonesDataObject.ID].df = self.output_data_objects[\n                    BronzeGeographicZonesDataObject.ID\n                ].df.union(nuts)\n\n    def get_countries_data(self) -&gt; DataFrame:\n        \"\"\"\n        Retrieves and processes country data.\n\n        This method reads country data from the GISCO portal\n        and processes the data by renaming columns, exploding the 'geometry' column,\n        and projecting the data to a default CRS.\n        The processed data is returned as a DataFrame.\n\n        Returns:\n            DataFrame: A DataFrame containing processed country data.\n                    The DataFrame has columns for the ISO 2 country code, country name, and geometry.\n        \"\"\"\n\n        self.input_data_objects[\"countries\"].read()\n        self.logger.info(f\"got countries data\")\n        countries_sdf = self.input_data_objects[\"countries\"].df\n        countries_sdf = countries_sdf.withColumnRenamed(\"CNTR_ID\", ColNames.iso2).withColumnRenamed(\n            \"NAME_ENGL\", ColNames.name\n        )\n\n        countries_sdf = countries_sdf.withColumn(\"geometry\", F.explode(STF.ST_Dump(\"geometry\")))\n\n        countries_sdf = utils.project_to_crs(countries_sdf, 4326, self.default_crs)\n        return countries_sdf\n\n    def get_nuts_data(self, level: str) -&gt; DataFrame:\n        \"\"\"\n        Retrieves and processes NUTS (Nomenclature of Territorial Units for Statistics) data for a specific level.\n\n        This method reads NUTS data from GISCO portal and processes the data by\n        renaming columns, filtering by reference country, and projecting the data to a default CRS.\n        It also adds a 'parent_id' column that contains the ID of the parent NUTS unit for each NUTS unit,\n        and adds columns for the year, month, and day with fixed values.\n        The processed data is returned as a DataFrame.\n\n        Args:\n            level (str): The NUTS level for which to retrieve and process data.\n\n        Returns:\n            DataFrame: A DataFrame containing processed NUTS data.\n                    The DataFrame has columns for the NUTS ID, name, ISO 2 country code, level,\n                    geometry, parent ID, year, month, and day.\n        \"\"\"\n\n        self.input_data_objects[f\"nuts_{level}\"].read()\n        self.logger.info(f\"got NUTS data for level {level}\")\n        nuts_sdf = self.input_data_objects[f\"nuts_{level}\"].df\n        nuts_sdf = nuts_sdf.drop(\"id\")\n        nuts_sdf = nuts_sdf.filter(F.col(\"CNTR_CODE\") == self.reference_country)\n\n        nuts_sdf = (\n            nuts_sdf.withColumnRenamed(\"NUTS_ID\", ColNames.zone_id)\n            .withColumnRenamed(\"NAME_LATN\", ColNames.name)\n            .withColumnRenamed(\"CNTR_CODE\", ColNames.iso2)\n            .withColumnRenamed(\"LEVL_CODE\", ColNames.level)\n        )\n\n        nuts_sdf = nuts_sdf.select(\n            ColNames.zone_id,\n            ColNames.name,\n            ColNames.iso2,\n            ColNames.level,\n            ColNames.geometry,\n        )\n\n        if level == 0:\n            nuts_sdf = nuts_sdf.withColumn(\"parent_id\", F.lit(None))\n        else:\n            nuts_sdf = nuts_sdf.withColumn(\"parent_id\", F.expr(\"substring(zone_id, 1, length(zone_id)-1)\"))\n\n        nuts_sdf = utils.project_to_crs(nuts_sdf, 4326, self.default_crs)\n\n        nuts_sdf = nuts_sdf.withColumns(\n            {\n                ColNames.year: F.lit(self.nuts_year),\n                ColNames.month: F.lit(1),\n                ColNames.day: F.lit(1),\n            }\n        )\n        return nuts_sdf\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/gisco_data_ingestion/#components.ingestion.spatial_data_ingestion.gisco_data_ingestion.GiscoDataIngestion.get_countries_data","title":"<code>get_countries_data()</code>","text":"<p>Retrieves and processes country data.</p> <p>This method reads country data from the GISCO portal and processes the data by renaming columns, exploding the 'geometry' column, and projecting the data to a default CRS. The processed data is returned as a DataFrame.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing processed country data.     The DataFrame has columns for the ISO 2 country code, country name, and geometry.</p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/gisco_data_ingestion.py</code> <pre><code>def get_countries_data(self) -&gt; DataFrame:\n    \"\"\"\n    Retrieves and processes country data.\n\n    This method reads country data from the GISCO portal\n    and processes the data by renaming columns, exploding the 'geometry' column,\n    and projecting the data to a default CRS.\n    The processed data is returned as a DataFrame.\n\n    Returns:\n        DataFrame: A DataFrame containing processed country data.\n                The DataFrame has columns for the ISO 2 country code, country name, and geometry.\n    \"\"\"\n\n    self.input_data_objects[\"countries\"].read()\n    self.logger.info(f\"got countries data\")\n    countries_sdf = self.input_data_objects[\"countries\"].df\n    countries_sdf = countries_sdf.withColumnRenamed(\"CNTR_ID\", ColNames.iso2).withColumnRenamed(\n        \"NAME_ENGL\", ColNames.name\n    )\n\n    countries_sdf = countries_sdf.withColumn(\"geometry\", F.explode(STF.ST_Dump(\"geometry\")))\n\n    countries_sdf = utils.project_to_crs(countries_sdf, 4326, self.default_crs)\n    return countries_sdf\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/gisco_data_ingestion/#components.ingestion.spatial_data_ingestion.gisco_data_ingestion.GiscoDataIngestion.get_nuts_data","title":"<code>get_nuts_data(level)</code>","text":"<p>Retrieves and processes NUTS (Nomenclature of Territorial Units for Statistics) data for a specific level.</p> <p>This method reads NUTS data from GISCO portal and processes the data by renaming columns, filtering by reference country, and projecting the data to a default CRS. It also adds a 'parent_id' column that contains the ID of the parent NUTS unit for each NUTS unit, and adds columns for the year, month, and day with fixed values. The processed data is returned as a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>str</code> <p>The NUTS level for which to retrieve and process data.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing processed NUTS data.     The DataFrame has columns for the NUTS ID, name, ISO 2 country code, level,     geometry, parent ID, year, month, and day.</p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/gisco_data_ingestion.py</code> <pre><code>def get_nuts_data(self, level: str) -&gt; DataFrame:\n    \"\"\"\n    Retrieves and processes NUTS (Nomenclature of Territorial Units for Statistics) data for a specific level.\n\n    This method reads NUTS data from GISCO portal and processes the data by\n    renaming columns, filtering by reference country, and projecting the data to a default CRS.\n    It also adds a 'parent_id' column that contains the ID of the parent NUTS unit for each NUTS unit,\n    and adds columns for the year, month, and day with fixed values.\n    The processed data is returned as a DataFrame.\n\n    Args:\n        level (str): The NUTS level for which to retrieve and process data.\n\n    Returns:\n        DataFrame: A DataFrame containing processed NUTS data.\n                The DataFrame has columns for the NUTS ID, name, ISO 2 country code, level,\n                geometry, parent ID, year, month, and day.\n    \"\"\"\n\n    self.input_data_objects[f\"nuts_{level}\"].read()\n    self.logger.info(f\"got NUTS data for level {level}\")\n    nuts_sdf = self.input_data_objects[f\"nuts_{level}\"].df\n    nuts_sdf = nuts_sdf.drop(\"id\")\n    nuts_sdf = nuts_sdf.filter(F.col(\"CNTR_CODE\") == self.reference_country)\n\n    nuts_sdf = (\n        nuts_sdf.withColumnRenamed(\"NUTS_ID\", ColNames.zone_id)\n        .withColumnRenamed(\"NAME_LATN\", ColNames.name)\n        .withColumnRenamed(\"CNTR_CODE\", ColNames.iso2)\n        .withColumnRenamed(\"LEVL_CODE\", ColNames.level)\n    )\n\n    nuts_sdf = nuts_sdf.select(\n        ColNames.zone_id,\n        ColNames.name,\n        ColNames.iso2,\n        ColNames.level,\n        ColNames.geometry,\n    )\n\n    if level == 0:\n        nuts_sdf = nuts_sdf.withColumn(\"parent_id\", F.lit(None))\n    else:\n        nuts_sdf = nuts_sdf.withColumn(\"parent_id\", F.expr(\"substring(zone_id, 1, length(zone_id)-1)\"))\n\n    nuts_sdf = utils.project_to_crs(nuts_sdf, 4326, self.default_crs)\n\n    nuts_sdf = nuts_sdf.withColumns(\n        {\n            ColNames.year: F.lit(self.nuts_year),\n            ColNames.month: F.lit(1),\n            ColNames.day: F.lit(1),\n        }\n    )\n    return nuts_sdf\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/overture_data_ingestion/","title":"overture_data_ingestion","text":""},{"location":"reference/components/ingestion/spatial_data_ingestion/overture_data_ingestion/#components.ingestion.spatial_data_ingestion.overture_data_ingestion.OvertureDataIngestion","title":"<code>OvertureDataIngestion</code>","text":"<p>               Bases: <code>Component</code></p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/overture_data_ingestion.py</code> <pre><code>class OvertureDataIngestion(Component):\n    \"\"\" \"\"\"\n\n    COMPONENT_ID = \"OvertureDataIngestion\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.transportation_reclass_map = self.config.geteval(\n            OvertureDataIngestion.COMPONENT_ID, \"transportation_reclass_map\"\n        )\n        self.landuse_reclass_map = self.config.geteval(\n            OvertureDataIngestion.COMPONENT_ID, \"landuse_landcover_reclass_map\"\n        )\n        self.bulding_reclass_map = self.config.geteval(OvertureDataIngestion.COMPONENT_ID, \"buildings_reclass_map\")\n\n        self.landuse_filter_subtypes = self.config.geteval(\n            OvertureDataIngestion.COMPONENT_ID, \"landuse_filter_subtypes\"\n        )\n        self.landcover_filter_subtypes = self.config.geteval(\n            OvertureDataIngestion.COMPONENT_ID, \"landcover_filter_subtypes\"\n        )\n\n        self.transportation_filter_subtypes = self.config.geteval(\n            OvertureDataIngestion.COMPONENT_ID, \"transportation_filter_subtypes\"\n        )\n\n        self.extent = self.config.geteval(OvertureDataIngestion.COMPONENT_ID, \"extent\")\n\n        self.spatial_repartition_size_rows = self.config.getint(\n            OvertureDataIngestion.COMPONENT_ID, \"spatial_repartition_size_rows\"\n        )\n\n        self.min_partition_quadkey_level = self.config.getint(\n            OvertureDataIngestion.COMPONENT_ID, \"min_partition_quadkey_level\"\n        )\n\n        self.max_partition_quadkey_level = self.config.getint(\n            OvertureDataIngestion.COMPONENT_ID, \"max_partition_quadkey_level\"\n        )\n\n        self.extraction_quadkey_level = self.config.getint(\n            OvertureDataIngestion.COMPONENT_ID, \"extraction_quadkey_level\"\n        )\n\n        self.use_buildings = self.config.getboolean(OvertureDataIngestion.COMPONENT_ID, \"use_buildings\")\n\n        self.quadkey_udf = F.udf(utils.latlon_to_quadkey)\n\n    def initalize_data_objects(self):\n\n        overture_url = self.config.get(OvertureDataIngestion.COMPONENT_ID, \"overture_url\")\n\n        transportation_url = overture_url + \"/theme=transportation/type=segment\"\n\n        buildings_url = overture_url + \"/theme=buildings/type=building\"\n\n        landcover_url = overture_url + \"/theme=base/type=land\"\n        landuse_url = overture_url + \"/theme=base/type=land_use\"\n        water_url = overture_url + \"/theme=base/type=water\"\n\n        self.input_data_objects = {}\n        self.input_data_objects[\"transportation\"] = LandingGeoParquetDataObject(self.spark, transportation_url, [])\n        self.input_data_objects[\"buildings\"] = LandingGeoParquetDataObject(self.spark, buildings_url, [])\n        self.input_data_objects[\"landcover\"] = LandingGeoParquetDataObject(self.spark, landcover_url, [])\n        self.input_data_objects[\"landuse\"] = LandingGeoParquetDataObject(self.spark, landuse_url, [])\n        self.input_data_objects[\"water\"] = LandingGeoParquetDataObject(self.spark, water_url, [])\n\n        self.clear_destination_directory = self.config.getboolean(\n            OvertureDataIngestion.COMPONENT_ID, \"clear_destination_directory\"\n        )\n\n        transportation_do_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"transportation_data_bronze\")\n\n        landuse_do_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"landuse_data_bronze\")\n\n        self.output_data_objects = {}\n\n        self.output_data_objects[BronzeTransportationDataObject.ID] = BronzeTransportationDataObject(\n            self.spark,\n            transportation_do_path,\n            [ColNames.year, ColNames.month, ColNames.day, ColNames.quadkey],\n        )\n        self.output_data_objects[BronzeLanduseDataObject.ID] = BronzeLanduseDataObject(\n            self.spark,\n            landuse_do_path,\n            [ColNames.year, ColNames.month, ColNames.day, ColNames.quadkey],\n        )\n\n        if self.clear_destination_directory:\n            for do in self.output_data_objects.values():\n                self.logger.info(f\"Clearing {do.default_path}\")\n                delete_file_or_folder(self.spark, do.default_path)\n\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n\n        quadkeys = utils.get_quadkeys_for_bbox(self.extent, self.extraction_quadkey_level)\n\n        self.logger.info(f\"Extraction will be done in {len(quadkeys)} parts.\")\n        for quadkey in quadkeys:\n            self.logger.info(f\"Processing quadkey {quadkey}\")\n            self.current_extent = utils.quadkey_to_extent(quadkey)\n            self.current_quadkey = quadkey\n            self.transform()\n            self.write()\n            self.spark.catalog.clearCache()\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n\n        # process transportaion\n        transportation_sdf = self.get_transportation_data()\n        transportation_sdf = transportation_sdf.withColumns(\n            {\n                ColNames.year: F.year(F.current_date()),\n                ColNames.month: F.month(F.current_date()),\n                ColNames.day: F.day(F.current_date()),\n            }\n        )\n        transportation_sdf = self.apply_schema_casting(transportation_sdf, BronzeTransportationDataObject.SCHEMA)\n\n        self.output_data_objects[BronzeTransportationDataObject.ID].df = transportation_sdf\n\n        # process landuse\n        landuse_cols_to_select = [\"subtype\", \"geometry\"]\n\n        landcover_sdf = self.get_raw_data_for_landuse(\n            \"landcover\",\n            landuse_cols_to_select,\n            self.landcover_filter_subtypes,\n            persist=True,\n        )\n\n        landuse_sdf = self.get_raw_data_for_landuse(\n            \"landuse\",\n            landuse_cols_to_select,\n            self.landuse_filter_subtypes,\n            persist=True,\n        )\n\n        # combine landcover and landuse\n        landcover_sdf = utils.cut_polygons_with_mask_polygons(landcover_sdf, landuse_sdf, landuse_cols_to_select)\n\n        landcover_sdf = landcover_sdf.persist(StorageLevel.MEMORY_AND_DISK)\n        landcover_sdf.count()\n\n        landuse_sdf = utils.cut_polygons_with_mask_polygons(landuse_sdf, landcover_sdf, landuse_cols_to_select)\n\n        landuse_sdf = landuse_sdf.persist(StorageLevel.MEMORY_AND_DISK)\n        landuse_sdf.count()\n\n        landuse_sdf = utils.cut_polygons_with_mask_polygons(landuse_sdf, landuse_sdf, landuse_cols_to_select, True)\n\n        landuse_sdf = landuse_sdf.persist(StorageLevel.MEMORY_AND_DISK)\n        landuse_sdf.count()\n\n        landuse_landcover_sdf = landcover_sdf.union(landuse_sdf)\n\n        # blow up too big landuse polygons\n        # TODO: asses feasibility of this\n        # TODO: make vertices number parameter\n        # TODO: introduce cut by qaudkeys\n        # landuse_landcover_sdf = landuse_landcover_sdf.withColumn(ColNames.geometry, STF.ST_SubDivideExplode(ColNames.geometry, 1000))\n\n        landuse_landcover_sdf = landuse_landcover_sdf.persist(StorageLevel.MEMORY_AND_DISK)\n        landuse_landcover_sdf.count()\n        self.logger.info(f\"merged landuse and landcover\")\n\n        water_sdf = self.get_raw_data_for_landuse(\"water\", landuse_cols_to_select, persist=True)\n        water_sdf = water_sdf.withColumn(\"subtype\", F.lit(\"water\"))\n\n        # combine landuse with water\n        landuse_landcover_sdf = utils.cut_polygons_with_mask_polygons(\n            landuse_landcover_sdf, water_sdf, landuse_cols_to_select\n        )\n        landuse_cover_water_sdf = landuse_landcover_sdf.union(water_sdf)\n\n        # reclassify to config categories\n        landuse_cover_water_sdf = self.reclassify(\n            landuse_cover_water_sdf,\n            self.landuse_reclass_map,\n            \"subtype\",\n            ColNames.category,\n            \"open_area\",\n        )\n\n        landuse_cover_water_sdf = utils.fix_geometry(landuse_cover_water_sdf, 3)\n\n        landuse_cover_water_sdf = landuse_cover_water_sdf.persist(StorageLevel.MEMORY_AND_DISK)\n        landuse_cover_water_sdf.count()\n        self.logger.info(f\"merged landuse and water\")\n\n        # add buildings data to full landuse\n        if self.use_buildings:\n\n            buildings_sdf = self.get_raw_data_for_landuse(\"buildings\", [\"class\", \"geometry\"], persist=True)\n\n            buildings_sdf = self.reclassify(\n                buildings_sdf,\n                self.bulding_reclass_map,\n                \"class\",\n                ColNames.category,\n                \"other_builtup\",\n            )\n\n            buildings_sdf = self.merge_buildings_by_quadkey(buildings_sdf, 3035, 16)\n            buildings_sdf = buildings_sdf.drop(\"quadkey\")\n            buildings_sdf = buildings_sdf.persist(StorageLevel.MEMORY_AND_DISK)\n            buildings_sdf.count()\n            self.logger.info(f\"merged buildings\")\n\n            # combine landuse with buildings\n            landuse_cover_water_sdf = utils.cut_polygons_with_mask_polygons(\n                landuse_cover_water_sdf,\n                buildings_sdf,\n                [ColNames.category, ColNames.geometry],\n            )\n\n            landuse_cover_water_sdf = utils.fix_geometry(landuse_cover_water_sdf, 3, ColNames.geometry)\n\n            landuse_cover_water_sdf = landuse_cover_water_sdf.union(buildings_sdf)\n\n        landuse_cover_water_sdf = utils.assign_quadkey(landuse_cover_water_sdf, 3035, self.max_partition_quadkey_level)\n\n        # TODO: Figure out if this optimization is ever needed and how to implement it properly\n        # landuse_cover_water_sdf = utils.coarsen_quadkey_to_partition_size(landuse_cover_water_sdf,\n        #                                                                   self.spatial_repartition_size_rows,\n        #                                                                   self.min_partition_quadkey_level)\n\n        landuse_cover_water_sdf = landuse_cover_water_sdf.withColumns(\n            {\n                ColNames.year: F.year(F.current_date()),\n                ColNames.month: F.month(F.current_date()),\n                ColNames.day: F.day(F.current_date()),\n            }\n        )\n        landuse_cover_water_sdf = landuse_cover_water_sdf.orderBy(\"quadkey\")\n        landuse_cover_water_sdf = landuse_cover_water_sdf.repartition(\"quadkey\")\n\n        landuse_cover_water_sdf = self.apply_schema_casting(landuse_cover_water_sdf, BronzeLanduseDataObject.SCHEMA)\n\n        self.output_data_objects[BronzeLanduseDataObject.ID].df = landuse_cover_water_sdf\n\n    def get_transportation_data(self) -&gt; DataFrame:\n        \"\"\"\n        Processes and returns Overture Maps transportation data.\n\n        This function filters input data objects based on the transportation class and specified subtypes,\n        reclassifies the transportation data based on a predefined map, and selects specific columns for further processing.\n        It then fixes the line geometry, assigns quadkeys, projects the data to a specific CRS,\n        orders the data by quadkey, and repartitions the data based on quadkey.\n\n        Returns:\n            DataFrame: A DataFrame containing the processed transportation data.\n            The DataFrame includes a category column, a geometry column, and a quadkey column.\n        \"\"\"\n        # function implementation...\n\n        transportation_sdf = self.filter_input_data_objects(\n            \"transportation\",\n            [\"class\", \"geometry\"],\n            \"class\",\n            self.transportation_filter_subtypes,\n        )\n        transportation_sdf = self.reclassify(\n            transportation_sdf,\n            self.transportation_reclass_map,\n            \"class\",\n            ColNames.category,\n        )\n        transportation_sdf = transportation_sdf.select(ColNames.category, ColNames.geometry).drop(\"subtype\")\n\n        # transportation_sdf.persist(StorageLevel.MEMORY_AND_DISK)\n        # transportation_sdf.count()\n        # self.logger.info(f\"got transportation\")\n        transportation_sdf = utils.project_to_crs(transportation_sdf, 4326, 3035)\n        transportation_sdf = utils.fix_geometry(transportation_sdf, 2)\n        transportation_sdf = utils.assign_quadkey(transportation_sdf, 3035, self.max_partition_quadkey_level)\n\n        # TODO: Figure out how to implement this\n        # transportation_sdf = utils.coarsen_quadkey_to_partition_size(transportation_sdf, self.spatial_repartition_size_rows, self.min_partition_quadkey_level)\n        transportation_sdf = transportation_sdf.orderBy(\"quadkey\")\n        transportation_sdf = transportation_sdf.repartition(\"quadkey\")\n\n        return transportation_sdf\n\n    def get_raw_data_for_landuse(\n        self,\n        data_type: str,\n        cols_to_select,\n        filter_types: Optional[List[str]] = None,\n        persist: bool = True,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Retrieves and processes Overture Maps raw data for a specific land use type.\n\n        This function filters input data objects based on the specified data type and optional filter types,\n        fixes the polygon geometry, and projects the data to a specific CRS.\n        If the persist parameter is set to True, the resulting DataFrame is persisted in memory and disk.\n\n        Args:\n            data_type (str): The type of land use data to retrieve.\n            filter_types (list, optional): A list of subtypes to filter the data by. Each subtype is a string.\n                If None, no filtering is performed. Defaults to None.\n            persist (bool, optional): Whether to persist the resulting DataFrame in memory and disk. Defaults to True.\n\n        Returns:\n            DataFrame: A DataFrame containing the processed land use data.\n            The DataFrame includes a subtype column and a geometry column.\n        \"\"\"\n        # function implementation...\n\n        sdf = self.filter_input_data_objects(data_type, cols_to_select, \"subtype\", filter_types)\n\n        sdf = utils.project_to_crs(sdf, 4326, 3035)\n        sdf = utils.fix_geometry(sdf, 3, ColNames.geometry)\n\n        if persist:\n            sdf = sdf.persist(StorageLevel.MEMORY_AND_DISK)\n            sdf.count()\n            self.logger.info(f\"got {data_type}\")\n\n        return sdf\n\n    def merge_buildings_by_quadkey(self, sdf: DataFrame, crs: int, quadkey_level: int = 16) -&gt; DataFrame:\n        \"\"\"\n        Merges building polygons within each quadkey.\n\n        This function assigns a quadkey to each building polygon in the input DataFrame,\n        then groups the DataFrame by quadkey and building category, and merges the polygons within each group.\n\n        Args:\n            sdf (DataFrame): A DataFrame containing the building polygons.\n            crs (int): The coordinate reference system of the input geometries.\n            quadkey_level (int, optional): The zoom level to use when assigning quadkeys. Defaults to 16.\n\n        Returns:\n            DataFrame: A DataFrame containing the merged building polygons.\n        \"\"\"\n\n        sdf = utils.assign_quadkey(sdf, crs, quadkey_level)\n\n        # TODO: test more if this would make any difference\n\n        # sdf = utils.coarsen_quadkey_to_partition_size(\n        #     sdf, self.spatial_repartition_size_rows, 10\n        # )\n\n        # sdf = sdf.withColumn(\"quadkey_merge\", F.col(\"quadkey\").substr(1, self.max_partition_quadkey_level))\n        # sdf = sdf.repartition(\"quadkey_merge\").drop(\"quadkey_merge\")\n\n        sdf = sdf.groupBy(\"quadkey\", \"category\").agg(STA.ST_Union_Aggr(ColNames.geometry).alias(ColNames.geometry))\n\n        return sdf\n\n    def filter_input_data_objects(\n        self,\n        data_type: str,\n        required_columns: List[str],\n        category_col: str,\n        subtypes: Optional[List[str]] = None,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Filters and processes input Overture Maps data based on the specified data type and columns.\n\n        This function selects the required columns from the input data objects,\n        filters the data to the current processing iteration extent, and cuts the data to the general extent.\n        If the data type is \"landcover\", \"landuse\", or \"transportation\", it further filters the data by the specified subtypes.\n        If the data type is not \"transportation\", it filters out invalid polygons.\n\n        Args:\n            data_type (str): The type of data to filter and process. \"landcover\", \"landuse\", \"transportation\", \"buildings\", or \"water\".\n            required_columns (list): A list of column names to select from the data. Each column name is a string.\n            category_col (str): The name of the category column to filter by when the data type is \"landcover\", \"landuse\", or \"transportation\".\n            subtypes (list, optional): A list of subtypes to filter the data by when the data type is \"landcover\", \"landuse\", or \"transportation\".\n                Each subtype is a string. If None, no subtype filtering is performed. Defaults to None.\n\n        Returns:\n            DataFrame: A DataFrame containing the filtered and processed data.\n        \"\"\"\n        # function implementation...\n\n        do_sdf = self.input_data_objects[data_type].df.select(*required_columns)\n        do_sdf = self.filter_data_to_extent(do_sdf, self.extent)\n        do_sdf = utils.cut_geodata_to_extent(do_sdf, self.current_extent, 4326)\n        do_sdf = self.filter_to_quadkey(do_sdf, self.current_quadkey, self.extraction_quadkey_level)\n        if data_type in [\"landcover\", \"landuse\", \"transportation\"]:\n            do_sdf = do_sdf.filter(F.col(category_col).isin(subtypes))\n\n        if data_type not in [\"transportation\"]:\n            do_sdf = self.filter_polygons(do_sdf)\n\n        return do_sdf\n\n    @staticmethod\n    def filter_to_quadkey(sdf: DataFrame, current_quadkey: str, quadkey_level: int) -&gt; DataFrame:\n        \"\"\"\n        Filters a DataFrame to include only rows with polygon geometries.\n\n        Args:\n            sdf (DataFrame): A DataFrame that includes a geometry column.\n\n        Returns:\n            DataFrame: A DataFrame containing only the rows from the input DataFrame where the geometry is a polygon or multipolygon.\n        \"\"\"\n\n        sdf = utils.assign_quadkey(sdf, 4326, quadkey_level)\n\n        return sdf.filter(F.col(\"quadkey\") == F.lit(current_quadkey)).drop(\"quadkey\")\n\n    @staticmethod\n    def filter_polygons(sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Filters a DataFrame to include only rows with polygon geometries.\n\n        Args:\n            sdf (DataFrame): A DataFrame that includes a geometry column.\n\n        Returns:\n            DataFrame: A DataFrame containing only the rows from the input DataFrame where the geometry is a polygon or multipolygon.\n        \"\"\"\n        return sdf.filter(STF.ST_GeometryType(F.col(ColNames.geometry)).like(\"%Polygon%\"))\n\n    @staticmethod\n    def apply_schema_casting(sdf: DataFrame, schema: StructType) -&gt; DataFrame:\n        \"\"\"\n        This function takes a DataFrame and a schema, and applies the schema to the DataFrame.\n        It selects the columns in the DataFrame that are in the schema, and casts each column to the type specified in the schema.\n\n        Args:\n            sdf (DataFrame): The DataFrame to apply the schema to.\n            schema (StructType): The schema to apply to the DataFrame.\n\n        Returns:\n            DataFrame: A new DataFrame that includes the same rows as the input DataFrame,\n                but with the columns cast to the types specified in the schema.\n        \"\"\"\n\n        sdf = sdf.select(*[F.col(field.name) for field in schema.fields])\n        for field in schema.fields:\n            sdf = sdf.withColumn(field.name, F.col(field.name).cast(field.dataType))\n\n        return sdf\n\n    @staticmethod\n    def filter_data_to_extent(sdf: DataFrame, extent: Tuple[float, float, float, float]) -&gt; DataFrame:\n        \"\"\"\n        Filters an Overture Maps DataFrame to include only rows within a specified extent.\n\n        Args:\n            sdf (DataFrame): The DataFrame to filter.\n            extent (tuple): A tuple representing the extent. The tuple contains four elements:\n                (west, south, east, north), which are the western, southern, eastern, and northern bounds of the WGS84 extent.\n\n        Returns:\n            DataFrame: A DataFrame containing only the rows from the input DataFrame where the bbox is within the extent.\n        \"\"\"\n\n        sdf = sdf.filter(\n            ((F.col(\"bbox\")[\"xmin\"]).between(F.lit(extent[0]), F.lit(extent[2])))\n            &amp; ((F.col(\"bbox\")[\"ymin\"]).between(F.lit(extent[1]), F.lit(extent[3])))\n        )\n        return sdf\n\n    @staticmethod\n    def reclassify(\n        sdf: DataFrame,\n        reclass_map: Dict[str, List[str]],\n        class_column: str,\n        reclass_column: str,\n        default_reclass: str = \"unknown\",\n    ) -&gt; DataFrame:\n        \"\"\"\n        Reclassifies a column in a DataFrame based on a reclassification map.\n\n        This function takes a DataFrame, a reclassification map, and the names of a class column and a reclassification column.\n        It creates a new column in the DataFrame by reclassifying the values in the class column based on the reclassification map.\n        If a value in the class column is not in the reclassification map, it is classified as the default reclassification.\n\n        Args:\n            sdf (DataFrame): The DataFrame to reclassify.\n            reclass_map (dict): The reclassification map. The keys are the reclassified classes, and the values are lists of original classes.\n            class_column (str): The name of the column in the DataFrame to reclassify.\n            reclass_column (str): The name of the new column to create with the reclassified classes.\n            default_reclass (str, optional): The class to assign to values in the class column that are not in the map. Defaults to \"unknown\".\n\n        Returns:\n            DataFrame: A DataFrame containing the same rows as the input DataFrame, but with the class column replaced by the reclassification column.\n        \"\"\"\n        # function implementation...\n\n        keys = list(reclass_map.keys())\n        reclass_expr = F.when(F.col(class_column).isin(reclass_map[keys[0]]), keys[0])\n\n        for key in keys[1:]:\n            reclass_expr = reclass_expr.when(F.col(class_column).isin(reclass_map[key]), key)\n\n        reclass_expr = reclass_expr.otherwise(default_reclass)\n\n        sdf = sdf.withColumn(reclass_column, reclass_expr)\n\n        return sdf.select(ColNames.category, ColNames.geometry).drop(class_column)\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/overture_data_ingestion/#components.ingestion.spatial_data_ingestion.overture_data_ingestion.OvertureDataIngestion.apply_schema_casting","title":"<code>apply_schema_casting(sdf, schema)</code>  <code>staticmethod</code>","text":"<p>This function takes a DataFrame and a schema, and applies the schema to the DataFrame. It selects the columns in the DataFrame that are in the schema, and casts each column to the type specified in the schema.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The DataFrame to apply the schema to.</p> required <code>schema</code> <code>StructType</code> <p>The schema to apply to the DataFrame.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame that includes the same rows as the input DataFrame, but with the columns cast to the types specified in the schema.</p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/overture_data_ingestion.py</code> <pre><code>@staticmethod\ndef apply_schema_casting(sdf: DataFrame, schema: StructType) -&gt; DataFrame:\n    \"\"\"\n    This function takes a DataFrame and a schema, and applies the schema to the DataFrame.\n    It selects the columns in the DataFrame that are in the schema, and casts each column to the type specified in the schema.\n\n    Args:\n        sdf (DataFrame): The DataFrame to apply the schema to.\n        schema (StructType): The schema to apply to the DataFrame.\n\n    Returns:\n        DataFrame: A new DataFrame that includes the same rows as the input DataFrame,\n            but with the columns cast to the types specified in the schema.\n    \"\"\"\n\n    sdf = sdf.select(*[F.col(field.name) for field in schema.fields])\n    for field in schema.fields:\n        sdf = sdf.withColumn(field.name, F.col(field.name).cast(field.dataType))\n\n    return sdf\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/overture_data_ingestion/#components.ingestion.spatial_data_ingestion.overture_data_ingestion.OvertureDataIngestion.filter_data_to_extent","title":"<code>filter_data_to_extent(sdf, extent)</code>  <code>staticmethod</code>","text":"<p>Filters an Overture Maps DataFrame to include only rows within a specified extent.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The DataFrame to filter.</p> required <code>extent</code> <code>tuple</code> <p>A tuple representing the extent. The tuple contains four elements: (west, south, east, north), which are the western, southern, eastern, and northern bounds of the WGS84 extent.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing only the rows from the input DataFrame where the bbox is within the extent.</p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/overture_data_ingestion.py</code> <pre><code>@staticmethod\ndef filter_data_to_extent(sdf: DataFrame, extent: Tuple[float, float, float, float]) -&gt; DataFrame:\n    \"\"\"\n    Filters an Overture Maps DataFrame to include only rows within a specified extent.\n\n    Args:\n        sdf (DataFrame): The DataFrame to filter.\n        extent (tuple): A tuple representing the extent. The tuple contains four elements:\n            (west, south, east, north), which are the western, southern, eastern, and northern bounds of the WGS84 extent.\n\n    Returns:\n        DataFrame: A DataFrame containing only the rows from the input DataFrame where the bbox is within the extent.\n    \"\"\"\n\n    sdf = sdf.filter(\n        ((F.col(\"bbox\")[\"xmin\"]).between(F.lit(extent[0]), F.lit(extent[2])))\n        &amp; ((F.col(\"bbox\")[\"ymin\"]).between(F.lit(extent[1]), F.lit(extent[3])))\n    )\n    return sdf\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/overture_data_ingestion/#components.ingestion.spatial_data_ingestion.overture_data_ingestion.OvertureDataIngestion.filter_input_data_objects","title":"<code>filter_input_data_objects(data_type, required_columns, category_col, subtypes=None)</code>","text":"<p>Filters and processes input Overture Maps data based on the specified data type and columns.</p> <p>This function selects the required columns from the input data objects, filters the data to the current processing iteration extent, and cuts the data to the general extent. If the data type is \"landcover\", \"landuse\", or \"transportation\", it further filters the data by the specified subtypes. If the data type is not \"transportation\", it filters out invalid polygons.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>str</code> <p>The type of data to filter and process. \"landcover\", \"landuse\", \"transportation\", \"buildings\", or \"water\".</p> required <code>required_columns</code> <code>list</code> <p>A list of column names to select from the data. Each column name is a string.</p> required <code>category_col</code> <code>str</code> <p>The name of the category column to filter by when the data type is \"landcover\", \"landuse\", or \"transportation\".</p> required <code>subtypes</code> <code>list</code> <p>A list of subtypes to filter the data by when the data type is \"landcover\", \"landuse\", or \"transportation\". Each subtype is a string. If None, no subtype filtering is performed. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing the filtered and processed data.</p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/overture_data_ingestion.py</code> <pre><code>def filter_input_data_objects(\n    self,\n    data_type: str,\n    required_columns: List[str],\n    category_col: str,\n    subtypes: Optional[List[str]] = None,\n) -&gt; DataFrame:\n    \"\"\"\n    Filters and processes input Overture Maps data based on the specified data type and columns.\n\n    This function selects the required columns from the input data objects,\n    filters the data to the current processing iteration extent, and cuts the data to the general extent.\n    If the data type is \"landcover\", \"landuse\", or \"transportation\", it further filters the data by the specified subtypes.\n    If the data type is not \"transportation\", it filters out invalid polygons.\n\n    Args:\n        data_type (str): The type of data to filter and process. \"landcover\", \"landuse\", \"transportation\", \"buildings\", or \"water\".\n        required_columns (list): A list of column names to select from the data. Each column name is a string.\n        category_col (str): The name of the category column to filter by when the data type is \"landcover\", \"landuse\", or \"transportation\".\n        subtypes (list, optional): A list of subtypes to filter the data by when the data type is \"landcover\", \"landuse\", or \"transportation\".\n            Each subtype is a string. If None, no subtype filtering is performed. Defaults to None.\n\n    Returns:\n        DataFrame: A DataFrame containing the filtered and processed data.\n    \"\"\"\n    # function implementation...\n\n    do_sdf = self.input_data_objects[data_type].df.select(*required_columns)\n    do_sdf = self.filter_data_to_extent(do_sdf, self.extent)\n    do_sdf = utils.cut_geodata_to_extent(do_sdf, self.current_extent, 4326)\n    do_sdf = self.filter_to_quadkey(do_sdf, self.current_quadkey, self.extraction_quadkey_level)\n    if data_type in [\"landcover\", \"landuse\", \"transportation\"]:\n        do_sdf = do_sdf.filter(F.col(category_col).isin(subtypes))\n\n    if data_type not in [\"transportation\"]:\n        do_sdf = self.filter_polygons(do_sdf)\n\n    return do_sdf\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/overture_data_ingestion/#components.ingestion.spatial_data_ingestion.overture_data_ingestion.OvertureDataIngestion.filter_polygons","title":"<code>filter_polygons(sdf)</code>  <code>staticmethod</code>","text":"<p>Filters a DataFrame to include only rows with polygon geometries.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>A DataFrame that includes a geometry column.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing only the rows from the input DataFrame where the geometry is a polygon or multipolygon.</p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/overture_data_ingestion.py</code> <pre><code>@staticmethod\ndef filter_polygons(sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Filters a DataFrame to include only rows with polygon geometries.\n\n    Args:\n        sdf (DataFrame): A DataFrame that includes a geometry column.\n\n    Returns:\n        DataFrame: A DataFrame containing only the rows from the input DataFrame where the geometry is a polygon or multipolygon.\n    \"\"\"\n    return sdf.filter(STF.ST_GeometryType(F.col(ColNames.geometry)).like(\"%Polygon%\"))\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/overture_data_ingestion/#components.ingestion.spatial_data_ingestion.overture_data_ingestion.OvertureDataIngestion.filter_to_quadkey","title":"<code>filter_to_quadkey(sdf, current_quadkey, quadkey_level)</code>  <code>staticmethod</code>","text":"<p>Filters a DataFrame to include only rows with polygon geometries.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>A DataFrame that includes a geometry column.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing only the rows from the input DataFrame where the geometry is a polygon or multipolygon.</p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/overture_data_ingestion.py</code> <pre><code>@staticmethod\ndef filter_to_quadkey(sdf: DataFrame, current_quadkey: str, quadkey_level: int) -&gt; DataFrame:\n    \"\"\"\n    Filters a DataFrame to include only rows with polygon geometries.\n\n    Args:\n        sdf (DataFrame): A DataFrame that includes a geometry column.\n\n    Returns:\n        DataFrame: A DataFrame containing only the rows from the input DataFrame where the geometry is a polygon or multipolygon.\n    \"\"\"\n\n    sdf = utils.assign_quadkey(sdf, 4326, quadkey_level)\n\n    return sdf.filter(F.col(\"quadkey\") == F.lit(current_quadkey)).drop(\"quadkey\")\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/overture_data_ingestion/#components.ingestion.spatial_data_ingestion.overture_data_ingestion.OvertureDataIngestion.get_raw_data_for_landuse","title":"<code>get_raw_data_for_landuse(data_type, cols_to_select, filter_types=None, persist=True)</code>","text":"<p>Retrieves and processes Overture Maps raw data for a specific land use type.</p> <p>This function filters input data objects based on the specified data type and optional filter types, fixes the polygon geometry, and projects the data to a specific CRS. If the persist parameter is set to True, the resulting DataFrame is persisted in memory and disk.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>str</code> <p>The type of land use data to retrieve.</p> required <code>filter_types</code> <code>list</code> <p>A list of subtypes to filter the data by. Each subtype is a string. If None, no filtering is performed. Defaults to None.</p> <code>None</code> <code>persist</code> <code>bool</code> <p>Whether to persist the resulting DataFrame in memory and disk. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing the processed land use data.</p> <code>DataFrame</code> <p>The DataFrame includes a subtype column and a geometry column.</p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/overture_data_ingestion.py</code> <pre><code>def get_raw_data_for_landuse(\n    self,\n    data_type: str,\n    cols_to_select,\n    filter_types: Optional[List[str]] = None,\n    persist: bool = True,\n) -&gt; DataFrame:\n    \"\"\"\n    Retrieves and processes Overture Maps raw data for a specific land use type.\n\n    This function filters input data objects based on the specified data type and optional filter types,\n    fixes the polygon geometry, and projects the data to a specific CRS.\n    If the persist parameter is set to True, the resulting DataFrame is persisted in memory and disk.\n\n    Args:\n        data_type (str): The type of land use data to retrieve.\n        filter_types (list, optional): A list of subtypes to filter the data by. Each subtype is a string.\n            If None, no filtering is performed. Defaults to None.\n        persist (bool, optional): Whether to persist the resulting DataFrame in memory and disk. Defaults to True.\n\n    Returns:\n        DataFrame: A DataFrame containing the processed land use data.\n        The DataFrame includes a subtype column and a geometry column.\n    \"\"\"\n    # function implementation...\n\n    sdf = self.filter_input_data_objects(data_type, cols_to_select, \"subtype\", filter_types)\n\n    sdf = utils.project_to_crs(sdf, 4326, 3035)\n    sdf = utils.fix_geometry(sdf, 3, ColNames.geometry)\n\n    if persist:\n        sdf = sdf.persist(StorageLevel.MEMORY_AND_DISK)\n        sdf.count()\n        self.logger.info(f\"got {data_type}\")\n\n    return sdf\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/overture_data_ingestion/#components.ingestion.spatial_data_ingestion.overture_data_ingestion.OvertureDataIngestion.get_transportation_data","title":"<code>get_transportation_data()</code>","text":"<p>Processes and returns Overture Maps transportation data.</p> <p>This function filters input data objects based on the transportation class and specified subtypes, reclassifies the transportation data based on a predefined map, and selects specific columns for further processing. It then fixes the line geometry, assigns quadkeys, projects the data to a specific CRS, orders the data by quadkey, and repartitions the data based on quadkey.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing the processed transportation data.</p> <code>DataFrame</code> <p>The DataFrame includes a category column, a geometry column, and a quadkey column.</p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/overture_data_ingestion.py</code> <pre><code>def get_transportation_data(self) -&gt; DataFrame:\n    \"\"\"\n    Processes and returns Overture Maps transportation data.\n\n    This function filters input data objects based on the transportation class and specified subtypes,\n    reclassifies the transportation data based on a predefined map, and selects specific columns for further processing.\n    It then fixes the line geometry, assigns quadkeys, projects the data to a specific CRS,\n    orders the data by quadkey, and repartitions the data based on quadkey.\n\n    Returns:\n        DataFrame: A DataFrame containing the processed transportation data.\n        The DataFrame includes a category column, a geometry column, and a quadkey column.\n    \"\"\"\n    # function implementation...\n\n    transportation_sdf = self.filter_input_data_objects(\n        \"transportation\",\n        [\"class\", \"geometry\"],\n        \"class\",\n        self.transportation_filter_subtypes,\n    )\n    transportation_sdf = self.reclassify(\n        transportation_sdf,\n        self.transportation_reclass_map,\n        \"class\",\n        ColNames.category,\n    )\n    transportation_sdf = transportation_sdf.select(ColNames.category, ColNames.geometry).drop(\"subtype\")\n\n    # transportation_sdf.persist(StorageLevel.MEMORY_AND_DISK)\n    # transportation_sdf.count()\n    # self.logger.info(f\"got transportation\")\n    transportation_sdf = utils.project_to_crs(transportation_sdf, 4326, 3035)\n    transportation_sdf = utils.fix_geometry(transportation_sdf, 2)\n    transportation_sdf = utils.assign_quadkey(transportation_sdf, 3035, self.max_partition_quadkey_level)\n\n    # TODO: Figure out how to implement this\n    # transportation_sdf = utils.coarsen_quadkey_to_partition_size(transportation_sdf, self.spatial_repartition_size_rows, self.min_partition_quadkey_level)\n    transportation_sdf = transportation_sdf.orderBy(\"quadkey\")\n    transportation_sdf = transportation_sdf.repartition(\"quadkey\")\n\n    return transportation_sdf\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/overture_data_ingestion/#components.ingestion.spatial_data_ingestion.overture_data_ingestion.OvertureDataIngestion.merge_buildings_by_quadkey","title":"<code>merge_buildings_by_quadkey(sdf, crs, quadkey_level=16)</code>","text":"<p>Merges building polygons within each quadkey.</p> <p>This function assigns a quadkey to each building polygon in the input DataFrame, then groups the DataFrame by quadkey and building category, and merges the polygons within each group.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>A DataFrame containing the building polygons.</p> required <code>crs</code> <code>int</code> <p>The coordinate reference system of the input geometries.</p> required <code>quadkey_level</code> <code>int</code> <p>The zoom level to use when assigning quadkeys. Defaults to 16.</p> <code>16</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing the merged building polygons.</p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/overture_data_ingestion.py</code> <pre><code>def merge_buildings_by_quadkey(self, sdf: DataFrame, crs: int, quadkey_level: int = 16) -&gt; DataFrame:\n    \"\"\"\n    Merges building polygons within each quadkey.\n\n    This function assigns a quadkey to each building polygon in the input DataFrame,\n    then groups the DataFrame by quadkey and building category, and merges the polygons within each group.\n\n    Args:\n        sdf (DataFrame): A DataFrame containing the building polygons.\n        crs (int): The coordinate reference system of the input geometries.\n        quadkey_level (int, optional): The zoom level to use when assigning quadkeys. Defaults to 16.\n\n    Returns:\n        DataFrame: A DataFrame containing the merged building polygons.\n    \"\"\"\n\n    sdf = utils.assign_quadkey(sdf, crs, quadkey_level)\n\n    # TODO: test more if this would make any difference\n\n    # sdf = utils.coarsen_quadkey_to_partition_size(\n    #     sdf, self.spatial_repartition_size_rows, 10\n    # )\n\n    # sdf = sdf.withColumn(\"quadkey_merge\", F.col(\"quadkey\").substr(1, self.max_partition_quadkey_level))\n    # sdf = sdf.repartition(\"quadkey_merge\").drop(\"quadkey_merge\")\n\n    sdf = sdf.groupBy(\"quadkey\", \"category\").agg(STA.ST_Union_Aggr(ColNames.geometry).alias(ColNames.geometry))\n\n    return sdf\n</code></pre>"},{"location":"reference/components/ingestion/spatial_data_ingestion/overture_data_ingestion/#components.ingestion.spatial_data_ingestion.overture_data_ingestion.OvertureDataIngestion.reclassify","title":"<code>reclassify(sdf, reclass_map, class_column, reclass_column, default_reclass='unknown')</code>  <code>staticmethod</code>","text":"<p>Reclassifies a column in a DataFrame based on a reclassification map.</p> <p>This function takes a DataFrame, a reclassification map, and the names of a class column and a reclassification column. It creates a new column in the DataFrame by reclassifying the values in the class column based on the reclassification map. If a value in the class column is not in the reclassification map, it is classified as the default reclassification.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The DataFrame to reclassify.</p> required <code>reclass_map</code> <code>dict</code> <p>The reclassification map. The keys are the reclassified classes, and the values are lists of original classes.</p> required <code>class_column</code> <code>str</code> <p>The name of the column in the DataFrame to reclassify.</p> required <code>reclass_column</code> <code>str</code> <p>The name of the new column to create with the reclassified classes.</p> required <code>default_reclass</code> <code>str</code> <p>The class to assign to values in the class column that are not in the map. Defaults to \"unknown\".</p> <code>'unknown'</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing the same rows as the input DataFrame, but with the class column replaced by the reclassification column.</p> Source code in <code>multimno/components/ingestion/spatial_data_ingestion/overture_data_ingestion.py</code> <pre><code>@staticmethod\ndef reclassify(\n    sdf: DataFrame,\n    reclass_map: Dict[str, List[str]],\n    class_column: str,\n    reclass_column: str,\n    default_reclass: str = \"unknown\",\n) -&gt; DataFrame:\n    \"\"\"\n    Reclassifies a column in a DataFrame based on a reclassification map.\n\n    This function takes a DataFrame, a reclassification map, and the names of a class column and a reclassification column.\n    It creates a new column in the DataFrame by reclassifying the values in the class column based on the reclassification map.\n    If a value in the class column is not in the reclassification map, it is classified as the default reclassification.\n\n    Args:\n        sdf (DataFrame): The DataFrame to reclassify.\n        reclass_map (dict): The reclassification map. The keys are the reclassified classes, and the values are lists of original classes.\n        class_column (str): The name of the column in the DataFrame to reclassify.\n        reclass_column (str): The name of the new column to create with the reclassified classes.\n        default_reclass (str, optional): The class to assign to values in the class column that are not in the map. Defaults to \"unknown\".\n\n    Returns:\n        DataFrame: A DataFrame containing the same rows as the input DataFrame, but with the class column replaced by the reclassification column.\n    \"\"\"\n    # function implementation...\n\n    keys = list(reclass_map.keys())\n    reclass_expr = F.when(F.col(class_column).isin(reclass_map[keys[0]]), keys[0])\n\n    for key in keys[1:]:\n        reclass_expr = reclass_expr.when(F.col(class_column).isin(reclass_map[key]), key)\n\n    reclass_expr = reclass_expr.otherwise(default_reclass)\n\n    sdf = sdf.withColumn(reclass_column, reclass_expr)\n\n    return sdf.select(ColNames.category, ColNames.geometry).drop(class_column)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/","title":"synthetic","text":""},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/","title":"synthetic_diaries","text":"<p>Module that generates a MNO synthetic network.</p>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries","title":"<code>SyntheticDiaries</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class that generates the synthetic activity-trip diaries data. It inherits from the Component abstract class.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>class SyntheticDiaries(Component):\n    \"\"\"\n    Class that generates the synthetic activity-trip diaries data.\n    It inherits from the Component abstract class.\n    \"\"\"\n\n    COMPONENT_ID = \"SyntheticDiaries\"\n\n    def __init__(self, general_config_path: str, component_config_path: str):\n        # keep super class init method:\n        super().__init__(general_config_path=general_config_path, component_config_path=component_config_path)\n\n        # and additionally:\n        # self.n_partitions = self.config.getint(self.COMPONENT_ID, \"n_partitions\")\n\n        self.number_of_users = self.config.getint(self.COMPONENT_ID, \"number_of_users\")\n\n        self.date_format = self.config.get(self.COMPONENT_ID, \"date_format\")\n        self.initial_date = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"initial_date\"), self.date_format\n        ).date()\n        self.number_of_dates = self.config.getint(self.COMPONENT_ID, \"number_of_dates\")\n        self.date_range = [(self.initial_date + datetime.timedelta(days=d)) for d in range(self.number_of_dates)]\n\n        self.longitude_min = self.config.getfloat(self.COMPONENT_ID, \"longitude_min\")\n        self.longitude_max = self.config.getfloat(self.COMPONENT_ID, \"longitude_max\")\n        self.latitude_min = self.config.getfloat(self.COMPONENT_ID, \"latitude_min\")\n        self.latitude_max = self.config.getfloat(self.COMPONENT_ID, \"latitude_max\")\n\n        self.home_work_distance_min = self.config.getfloat(self.COMPONENT_ID, \"home_work_distance_min\")\n        self.home_work_distance_max = self.config.getfloat(self.COMPONENT_ID, \"home_work_distance_max\")\n        self.other_distance_min = self.config.getfloat(self.COMPONENT_ID, \"other_distance_min\")\n        self.other_distance_max = self.config.getfloat(self.COMPONENT_ID, \"other_distance_max\")\n\n        self.home_duration_min = self.config.getfloat(self.COMPONENT_ID, \"home_duration_min\")\n        self.home_duration_max = self.config.getfloat(self.COMPONENT_ID, \"home_duration_max\")\n        self.work_duration_min = self.config.getfloat(self.COMPONENT_ID, \"work_duration_min\")\n        self.work_duration_max = self.config.getfloat(self.COMPONENT_ID, \"work_duration_max\")\n        self.other_duration_min = self.config.getfloat(self.COMPONENT_ID, \"other_duration_min\")\n        self.other_duration_max = self.config.getfloat(self.COMPONENT_ID, \"other_duration_max\")\n\n        self.displacement_speed = self.config.getfloat(self.COMPONENT_ID, \"displacement_speed\")\n\n        self.stay_sequence_superset = self.config.get(self.COMPONENT_ID, \"stay_sequence_superset\").split(\",\")\n        self.stay_sequence_probabilities = [\n            float(w)\n            for w in self.config.get(self.COMPONENT_ID, \"stay_sequence_probabilities\").split(\n                \",\"\n            )  # TODO: cambiar por stay_sequence\n        ]\n        assert len(self.stay_sequence_superset) == len(self.stay_sequence_probabilities)\n\n    def initalize_data_objects(self):\n        output_synthetic_diaries_data_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"diaries_data_bronze\")\n        bronze_synthetic_diaries = BronzeSyntheticDiariesDataObject(self.spark, output_synthetic_diaries_data_path)\n        self.output_data_objects = {BronzeSyntheticDiariesDataObject.ID: bronze_synthetic_diaries}\n\n    def read(self):\n        pass  # No input datasets are used in this component\n\n    def transform(self):\n        spark = self.spark\n        activities_df = spark.createDataFrame(self.generate_activities())\n        activities_df = calc_hashed_user_id(activities_df)\n        columns = {\n            field.name: F.col(field.name).cast(field.dataType)\n            for field in BronzeSyntheticDiariesDataObject.SCHEMA.fields\n        }\n        activities_df = activities_df.withColumns(columns)\n        self.output_data_objects[BronzeSyntheticDiariesDataObject.ID].df = activities_df\n\n    def haversine(self, lon1: float, lat1: float, lon2: float, lat2: float) -&gt; float:\n        \"\"\"\n        Calculate the haversine distance in meters between two points.\n\n        Args:\n            lon1 (float): longitude of first point, in decimal degrees.\n            lat1 (float): latitude of first point, in decimal degrees.\n            lon2 (float): longitude of second point, in decimal degrees.\n            lat2 (float): latitude of second point, in decimal degrees.\n\n        Returns:\n            float: distance between both points, in meters.\n        \"\"\"\n        r = 6_371_000  # Radius of earth in meters.\n\n        # convert decimal degrees to radians\n        lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n\n        # haversine formula\n        dlon = lon2 - lon1\n        dlat = lat2 - lat1\n        a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n        c = 2 * asin(sqrt(a))\n        return c * r\n\n    def random_seed_number_generator(\n        self, base_seed: int, agent_id: int = None, date: datetime.date = None, i: int = None\n    ) -&gt; int:\n        \"\"\"\n        Generate random seed integer based on provided arguments.\n\n        Args:\n            base_seed (int): base integer for operations.\n            agent_id (int, optional): agent identifier. Defaults to None.\n            date (datetime.date, optional): date. Defaults to None.\n            i (int, optional): position integer. Defaults to None.\n\n        Returns:\n            int: generated random seed integer.\n        \"\"\"\n        seed = base_seed\n        if agent_id is not None:\n            seed += int(agent_id) * 100\n        if date is not None:\n            start_datetime = datetime.datetime.combine(date, datetime.time(0))\n            seed += int(start_datetime.timestamp())\n        if i is not None:\n            seed += i\n        return seed\n\n    def calculate_trip_time(self, o_location: Tuple[float, float], d_location: Tuple[float, float]) -&gt; float:\n        \"\"\"\n        Calculate trip time given an origin location and a destination\n        location, according to the specified trip speed.\n\n        Args:\n            o_location (Tuple[float,float]): lon, lat of 1st point,\n                in decimal degrees.\n            d_location (Tuple[float,float]): lon, lat of 2nd point,\n                in decimal degrees.\n\n        Returns:\n            float: trip time, in seconds.\n        \"\"\"\n        trip_distance = self.haversine(o_location[0], o_location[1], d_location[0], d_location[1])  # m\n        trip_speed = self.displacement_speed  # m/s\n        trip_time = trip_distance / trip_speed  # s\n        return trip_time\n\n    def calculate_trip_final_time(\n        self,\n        origin_location: Tuple[float, float],\n        destin_location: Tuple[float, float],\n        origin_timestamp: datetime.datetime,\n    ) -&gt; datetime.datetime:\n        \"\"\"\n        Calculate end time of a trip given an origin time, an origin location,\n        a destination location and a speed.\n\n        Args:\n            origin_location (Tuple[float,float]): lon, lat of 1st point,\n                in decimal degrees.\n            destin_location (Tuple[float,float]): lon, lat of 2nd point,\n                in decimal degrees.\n            origin_timestamp (datetime.datetime): start time of trip.\n\n        Returns:\n            datetime.datetime: end time of trip.\n        \"\"\"\n\n        trip_time = self.calculate_trip_time(origin_location, destin_location)  # s\n        return origin_timestamp + datetime.timedelta(seconds=trip_time)\n\n    def generate_stay_location(\n        self,\n        stay_type: str,\n        home_location: Tuple[float, float],\n        work_location: Tuple[float, float],\n        previous_location: Tuple[float, float],\n        user_id: int,\n        date: datetime.date,\n        i: int,\n    ) -&gt; Tuple[float, float]:\n        \"\"\"\n        Generate a random activity location within the bounding box limits based\n        on the activity type and previous activity locations.\n\n        Args:\n            stay_type (str): type of stay (\"home\", \"work\" or \"other\").\n            home_location (Tuple[float,float]): coordinates of home location.\n            work_location (Tuple[float,float]): coordinates of work location.\n            previous_location (Tuple[float,float]): coordinates of previous\n                activity location.\n            user_id (int): agent identifier, used for random seed generation.\n            date (datetime.date): date, used for random seed generation.\n            i (int): activity position, used for random seed generation.\n\n        Returns:\n            Tuple[float,float]: randomly generated activity location coordinates.\n        \"\"\"\n        if stay_type == \"home\":\n            location = home_location\n        elif stay_type == \"work\":\n            location = work_location\n        else:\n            location = self.generate_other_location(user_id, date, i, home_location, previous_location)\n        return location\n\n    def create_agent_activities_min_duration(\n        self,\n        user_id: int,\n        agent_stay_type_sequence: List[str],\n        home_location: Tuple[float, float],\n        work_location: Tuple[float, float],\n        date: datetime.date,\n        start_of_date: datetime.datetime,\n        end_of_date: datetime.datetime,\n    ) -&gt; List[Row]:\n        \"\"\"\n        Generate activities of the minimum duration following the specified agent\n        activity sequence for this agent and date.\n\n        Args:\n            user_id (int): agent identifier.\n            agent_stay_type_sequence (List[str]): list of generated stay types,\n                each represented by a string indicating the stay type.\n            home_location (Tuple[float,float]): coordinates of home location.\n            work_location (Tuple[float,float]): coordinates of work location.\n            date (datetime.date): date for activity sequence generation, used for\n                timestamps and random seed generation.\n            start_of_date (datetime.datetime): timestamp of current date at 00:00:00.\n            end_of_date (datetime.datetime): timestamp of current date at 23:59:59.\n\n        Returns:\n            List[Row]: list of generated activities and trips, each represented by a\n                spark row object with all its information.\n        \"\"\"\n        date_activities = []\n        previous_location = None\n        for i, stay_type in enumerate(agent_stay_type_sequence):\n            # activity location:\n            location = self.generate_stay_location(\n                stay_type, home_location, work_location, previous_location, user_id, date, i\n            )\n            # previous move (unless first stay)\n            if i != 0:\n                # move timestamps:\n                trip_initial_timestamp = stay_final_timestamp\n                trip_final_timestamp = self.calculate_trip_final_time(\n                    previous_location, location, trip_initial_timestamp\n                )\n                # add move:\n                date_activities.append(\n                    Row(\n                        user_id=user_id,\n                        activity_type=\"move\",\n                        stay_type=\"move\",\n                        longitude=float(\"nan\"),\n                        latitude=float(\"nan\"),\n                        initial_timestamp=trip_initial_timestamp,\n                        final_timestamp=trip_final_timestamp,\n                        year=date.year,\n                        month=date.month,\n                        day=date.day,\n                    )\n                )\n            # stay timestamps:\n            stay_initial_timestamp = start_of_date if i == 0 else trip_final_timestamp\n            stay_duration = self.generate_min_stay_duration(stay_type)\n            stay_final_timestamp = stay_initial_timestamp + datetime.timedelta(hours=stay_duration)\n\n            # add stay:\n            date_activities.append(\n                Row(\n                    user_id=user_id,\n                    activity_type=\"stay\",\n                    stay_type=stay_type,\n                    longitude=location[0],\n                    latitude=location[1],\n                    initial_timestamp=stay_initial_timestamp,\n                    final_timestamp=stay_final_timestamp,\n                    year=date.year,\n                    month=date.month,\n                    day=date.day,\n                )\n            )\n\n            previous_location = location\n\n        # after the iterations:\n        if not date_activities:  # 0 stays\n            condition_for_full_home = True\n        elif stay_final_timestamp &gt; end_of_date:  # too many stays\n            condition_for_full_home = True\n        else:\n            condition_for_full_home = False\n\n        if condition_for_full_home:  # simple \"only home\" diary\n            return [\n                Row(\n                    user_id=user_id,\n                    activity_type=\"stay\",\n                    stay_type=\"home\",\n                    longitude=home_location[0],\n                    latitude=home_location[1],\n                    initial_timestamp=start_of_date,\n                    final_timestamp=end_of_date,\n                    year=date.year,\n                    month=date.month,\n                    day=date.day,\n                )\n            ]\n        else:\n            return date_activities  # actual generated diary\n\n    def update_spark_row(self, row: Row, column_name: str, new_value: Any) -&gt; Row:\n        \"\"\"\n        Return an updated spark row object, changing the value of a column.\n\n        Args:\n            row (Row): input spark row.\n            column_name (str): name of column to modify.\n            new_value (Any): new value to assign.\n\n        Returns:\n            Row: modified spark row\n        \"\"\"\n        return Row(**{**row.asDict(), **{column_name: new_value}})\n\n    def adjust_activity_times(\n        self,\n        date_activities: List[Row],\n        remaining_time: float,\n        user_id: int,\n        date: datetime.date,\n        start_of_date: datetime.datetime,\n        end_of_date: datetime.datetime,\n    ):\n        \"\"\"\n        Modifies the \"date_activities\" list, changing the initial and\n        final timestamps of both stays and moves probablilistically in order to\n        generate stay durations different from the minimum and adjust the\n        durations of the activities to the 24h of the day.\n\n        Args:\n            date_activities (List[Row]): list of generated activities (stays and\n                moves) of the agent for the specified date. Each activity/trip is a\n                spark row object.\n            user_id (int): agent identifier.\n            date (datetime.date): date for activity sequence generation, used for\n                timestamps and random seed generation.\n            start_of_date (datetime.datetime): timestamp of current date at 00:00:00.\n            end_of_date (datetime.datetime): timestamp of current date at 23:59:59.\n        \"\"\"\n        current_timestamp = start_of_date\n        for i, activity_row in enumerate(date_activities):\n            if activity_row.activity_type == \"stay\":  # stay:\n                stay_type = activity_row.stay_type\n                old_stay_duration = (\n                    activity_row.final_timestamp - activity_row.initial_timestamp\n                ).total_seconds() / 3600.0\n                new_initial_timestamp = current_timestamp\n                if i == len(date_activities) - 1:\n                    new_final_timestamp = end_of_date\n                    remaining_time = 0.0\n                else:\n                    new_stay_duration = self.generate_stay_duration(user_id, date, i, stay_type, remaining_time)\n                    new_duration_td = datetime.timedelta(seconds=new_stay_duration * 3600.0)\n                    new_final_timestamp = new_initial_timestamp + new_duration_td\n                    remaining_time -= new_stay_duration - old_stay_duration\n            else:  # move:\n                old_move_duration = activity_row.final_timestamp - activity_row.initial_timestamp\n                new_initial_timestamp = current_timestamp\n                new_final_timestamp = new_initial_timestamp + old_move_duration\n\n            # common for all activities (stays and moves):\n            activity_row = self.update_spark_row(activity_row, \"initial_timestamp\", new_initial_timestamp)\n            activity_row = self.update_spark_row(activity_row, \"final_timestamp\", new_final_timestamp)\n            date_activities[i] = activity_row\n            current_timestamp = new_final_timestamp\n\n    def add_agent_date_activities(\n        self,\n        activities: List[Row],\n        user_id: int,\n        agent_stay_type_sequence: List[str],\n        home_location: Tuple[float, float],\n        work_location: Tuple[float, float],\n        date: datetime.date,\n        start_of_date: datetime.datetime,\n        end_of_date: datetime.datetime,\n    ):\n        \"\"\"\n        For a specific date and user, generate a sequence of activities probabilistically\n        according to the specified activity superset and the activity probabilities.\n        Firstly, assign to each of these activities the minimum duration considered for\n        that activity type. Trip times are based on Pythagorean distance and a specified\n        average speed.\n        If the sum of all minimum duration of the activities and the duration of the trips\n        is higher than the 24h of the day, then assign just one \"home\" activity to the\n        agent from 00:00:00 to 23:59:59.\n        Else, there will be a remaining time. E.g., the diary of an agent, after adding\n        up all trip durations and minimum activity durations may end at 20:34:57. There is\n        a remaining time to complete the full diary (23:59:59 - 20:34:57).\n        Adjust activity times probabilistically according to the maximum activity duration\n        and this remaining time, making the diary end at exactly 23:59:59.\n\n        Args:\n            activities (List[Row]): list of generated activities (stays and moves) for\n                the agent for all of the specified dates. Each activity is a spark\n                row object.\n            user_id (int): agent identifier.\n            agent_stay_type_sequence (List[str]): list of generated stay types,\n                each represented by a string indicating the stay type.\n            home_location (Tuple[float,float]): coordinates of home location.\n            work_location (Tuple[float,float]): coordinates of work location.\n            date (datetime.date): date for activity sequence generation, used for\n                timestamps and random seed generation.\n            start_of_date (datetime.datetime): timestamp of current date at 00:00:00.\n            end_of_date (datetime.datetime): timestamp of current date at 23:59:59.\n        \"\"\"\n        date_activities = self.create_agent_activities_min_duration(\n            user_id, agent_stay_type_sequence, home_location, work_location, date, start_of_date, end_of_date\n        )\n        remaining_time = (end_of_date - date_activities[-1].final_timestamp).total_seconds() / 3600.0\n\n        if remaining_time != 0:\n            self.adjust_activity_times(\n                date_activities,\n                remaining_time,\n                user_id,\n                date,\n                start_of_date,\n                end_of_date,\n            )\n        activities += date_activities\n\n    def add_date_activities(self, date: datetime.date, activities: List[Row]):\n        \"\"\"\n        Generate activity (stays and moves) rows for a specific date according to\n        parameters.\n\n        Args:\n            date (datetime.date): date for activity sequence generation, used for\n                timestamps and random seed generation.\n            activities (List[Row]): list of generated activities (stays and moves) for\n                the agent for all of the specified dates. Each activity is a spark\n                row object.\n        \"\"\"\n        # Start of date, end of date: datetime object generation\n        start_of_date = datetime.datetime.combine(date, datetime.time(0, 0, 0))\n        end_of_date = datetime.datetime.combine(date, datetime.time(23, 59, 59))\n        for user_id in range(self.number_of_users):\n            # generate user information:\n            agent_stay_type_sequence = self.generate_stay_type_sequence(user_id, date)\n            home_location = self.generate_home_location(user_id)\n            work_location = self.generate_work_location(user_id, home_location)\n            self.add_agent_date_activities(\n                activities,\n                user_id,\n                agent_stay_type_sequence,\n                home_location,\n                work_location,\n                date,\n                start_of_date,\n                end_of_date,\n            )\n\n    def generate_activities(self) -&gt; List[Row]:\n        \"\"\"\n        Generate activity and trip rows according to parameters.\n\n        Returns:\n            List[Row]: list of generated activities and trips for the agent for all\n                of the specified dates. Each activity/trip is a spark row object.\n        \"\"\"\n        activities = []\n        for date in self.date_range:\n            self.add_date_activities(date, activities)\n        return activities\n\n    def generate_lonlat_at_distance(self, lon1: float, lat1: float, d: float, seed: int) -&gt; Tuple[float, float]:\n        \"\"\"\n        Given a point (lon, lat) and a distance, in meters, calculate a new random\n        point that is exactly at the specified distance of the provided lon, lat.\n\n        Args:\n            lon1 (float): longitude of point, specified in decimal degrees.\n            lat1 (float): latitude of point, specified in decimal degrees.\n            d (float): distance, in meters.\n            seed (int): random seed integer.\n\n        Returns:\n            Tuple[float, float]: coordinates of randomly generated point.\n        \"\"\"\n        r = 6_371_000  # Radius of earth in meters.\n\n        d_x = Random(seed).uniform(0, d)\n        d_y = sqrt(d**2 - d_x**2)\n\n        # firstly, convert lat to radians for later\n        lat1_radians = lat1 * pi / 180.0\n\n        # how many meters correspond to one degree of latitude?\n        deg_to_meters = r * pi / 180  # aprox. 111111 meters\n        # thus, the northwards displacement, in degrees of latitude is:\n        north_delta = d_y / deg_to_meters\n\n        # but one degree of longitude does not always correspond to the\n        # same distance... depends on the latitude at where you are!\n        parallel_radius = abs(r * cos(lat1_radians))\n        deg_to_meters = parallel_radius * pi / 180  # variable\n        # thus, the eastwards displacement, in degrees of longitude is:\n        east_delta = d_x / deg_to_meters\n\n        final_lon = lon1 + east_delta * Random(seed).choice([-1, 1])\n        final_lat = lat1 + north_delta * Random(seed).choice([-1, 1])\n\n        return (final_lon, final_lat)\n\n    def generate_home_location(self, agent_id: int) -&gt; Tuple[float, float]:\n        \"\"\"\n        Generate random home location based on bounding box limits.\n\n        Args:\n            agent_id (int): identifier of agent, used for random seed generation.\n\n        Returns:\n            Tuple[float,float]: coordinates of generated home location.\n        \"\"\"\n        seed_lon = self.random_seed_number_generator(1, agent_id)\n        seed_lat = self.random_seed_number_generator(2, agent_id)\n        hlon = Random(seed_lon).uniform(self.longitude_min, self.longitude_max)\n        hlat = Random(seed_lat).uniform(self.latitude_min, self.latitude_max)\n        return (hlon, hlat)\n\n    def generate_work_location(\n        self, agent_id: int, home_location: Tuple[float, float], seed: int = 4\n    ) -&gt; Tuple[float, float]:\n        \"\"\"\n        Generate random work location based on home location and maximum distance to\n        home. If the work location falls outside of bounding box limits, try again.\n\n        Args:\n            agent_id (int): identifier of agent, used for random seed generation.\n            home_location (Tuple[float,float]): coordinates of home location.\n            seed (int, optional): random seed integer. Defaults to 4.\n\n        Returns:\n            Tuple[float,float]: coordinates of generated work location.\n        \"\"\"\n        seed_distance = self.random_seed_number_generator(seed - 1, agent_id)\n        random_distance = Random(seed_distance).uniform(self.home_work_distance_min, self.home_work_distance_max)\n        hlon, hlat = home_location\n        seed_coords = self.random_seed_number_generator(seed, agent_id)\n        wlon, wlat = self.generate_lonlat_at_distance(hlon, hlat, random_distance, seed_coords)\n\n        if not (self.longitude_min &lt; wlon &lt; self.longitude_max) or not (\n            self.latitude_min &lt; wlat &lt; self.latitude_max\n        ):  # outside limits\n            seed += 1\n            wlon, wlat = self.generate_work_location(agent_id, home_location, seed=seed)\n\n        return (wlon, wlat)\n\n    def generate_other_location(\n        self,\n        agent_id: int,\n        date: datetime.date,\n        activity_number: int,\n        home_location: Tuple[float, float],\n        previous_location: Tuple[float, float],\n        seed: int = 6,\n    ) -&gt; Tuple[float, float]:\n        \"\"\"\n        Generate other activity location based on previous location and maximum distance\n        to previous location. If there is no previous location (this is the first\n        activity of the day), then the home location is considered as previous location.\n        If the location falls outside of bounding box limits, try again.\n\n        Args:\n            agent_id (int): identifier of agent, used for random seed generation.\n            date (datetime.date): date, used for random seed generation.\n            activity_number (int): act position, used for random seed generation.\n            home_location (Tuple[float,float]): coordinates of home location.\n            previous_location (Tuple[float,float]): coordinates of previous location.\n            seed (int, optional): random seed integer. Defaults to 6.\n\n        Returns:\n            Tuple[float,float]: coordinates of generated location.\n        \"\"\"\n        seed_distance = self.random_seed_number_generator(seed - 1, agent_id)\n        random_distance = Random(seed_distance).uniform(self.other_distance_min, self.other_distance_max)\n        if previous_location is None:\n            plon, plat = home_location\n        else:\n            plon, plat = previous_location\n\n        seed_coords = self.random_seed_number_generator(seed, agent_id)\n        olon, olat = self.generate_lonlat_at_distance(plon, plat, random_distance, seed_coords)\n        if not (self.longitude_min &lt; olon &lt; self.longitude_max) or not (\n            self.latitude_min &lt; olat &lt; self.latitude_max\n        ):  # outside limits\n            seed += 1\n            olon, olat = self.generate_other_location(\n                agent_id, date, activity_number, home_location, previous_location, seed=seed\n            )\n\n        return (olon, olat)\n\n    def generate_stay_duration(\n        self, agent_id: int, date: datetime.date, i: int, stay_type: str, remaining_time: float\n    ) -&gt; float:\n        \"\"\"\n        Generate stay duration probabilistically based on activity type\n        abd remaining time.\n\n        Args:\n            agent_id (int): identifier of agent, used for random seed generation.\n            date (datetime.date): date, used for random seed generation.\n            i (int): activity position, used for random seed generation.\n            stay_type (str): type of stay. Shall be \"home\", \"work\" or \"other\".\n            remaining_time (float): same units as durations.\n\n        Returns:\n            float: generated activity duration.\n        \"\"\"\n        if stay_type == \"home\":\n            min_duration = self.home_duration_min\n            max_duration = self.home_duration_max\n        elif stay_type == \"work\":\n            min_duration = self.work_duration_min\n            max_duration = self.work_duration_max\n        elif stay_type == \"other\":\n            min_duration = self.other_duration_min\n            max_duration = self.other_duration_max\n        else:\n            raise ValueError\n        seed = self.random_seed_number_generator(7, agent_id, date, i)\n        max_value = min(max_duration, min_duration + remaining_time)\n        return Random(seed).uniform(min_duration, max_value)\n\n    def generate_min_stay_duration(self, stay_type: str) -&gt; float:\n        \"\"\"\n        Generate minimum stay duration based on stay type specifications.\n\n        Args:\n            stay_type (str): type of stay. Shall be \"home\", \"work\" or \"other\".\n\n        Returns:\n            float: minimum stay duration.\n        \"\"\"\n        if stay_type == \"home\":\n            return self.home_duration_min\n        elif stay_type == \"work\":\n            return self.work_duration_min\n        elif stay_type == \"other\":\n            return self.other_duration_min\n        else:\n            raise ValueError\n\n    def remove_consecutive_stay_types(self, stay_sequence_list: List[str], stay_types_to_group: Set[str]) -&gt; List[str]:\n        \"\"\"\n        Generate new list replacing consecutive stays of the same type by\n        a unique stay as long as the stay type is contained in the\n        \"stay_types_to_group\" list.\n\n        Args:\n            stay_sequence_list (List[str]): input stay type list.\n            stay_types_to_group (Set[str]): stay types to group.\n\n        Returns:\n            List[str]: output stay sequence list.\n        \"\"\"\n        new_stay_sequence_list = []\n        previous_stay = None\n        for stay in stay_sequence_list:\n            if stay == previous_stay and stay in stay_types_to_group:\n                pass\n            else:\n                new_stay_sequence_list.append(stay)\n            previous_stay = stay\n        return new_stay_sequence_list\n\n    def generate_stay_type_sequence(self, agent_id: int, date: datetime.date) -&gt; List[str]:\n        \"\"\"\n        Generate the sequence of stay types for an agent for a specific date\n        probabilistically based on the superset sequence and specified\n        probabilities.\n        Replace 'home'-'home' and 'work'-'work' sequences by just 'home' or\n        'work'.\n\n        Args:\n            agent_id (int): identifier of agent, used for random seed generation.\n            date (datetime.date): date for activity sequence generation, used for\n                random seed generation.\n\n        Returns:\n            List[str]: list of generated stay types, each represented by a string\n                indicating the stay type (e.g. \"home\", \"work\", \"other\").\n        \"\"\"\n        stay_type_sequence = []\n        for i, stay_type in enumerate(self.stay_sequence_superset):\n            stay_weight = self.stay_sequence_probabilities[i]\n            seed = self.random_seed_number_generator(0, agent_id, date, i)\n            if Random(seed).random() &lt; stay_weight:\n                stay_type_sequence.append(stay_type)\n        stay_type_sequence = self.remove_consecutive_stay_types(stay_type_sequence, {\"home\", \"work\"})\n        return stay_type_sequence\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.add_agent_date_activities","title":"<code>add_agent_date_activities(activities, user_id, agent_stay_type_sequence, home_location, work_location, date, start_of_date, end_of_date)</code>","text":"<p>For a specific date and user, generate a sequence of activities probabilistically according to the specified activity superset and the activity probabilities. Firstly, assign to each of these activities the minimum duration considered for that activity type. Trip times are based on Pythagorean distance and a specified average speed. If the sum of all minimum duration of the activities and the duration of the trips is higher than the 24h of the day, then assign just one \"home\" activity to the agent from 00:00:00 to 23:59:59. Else, there will be a remaining time. E.g., the diary of an agent, after adding up all trip durations and minimum activity durations may end at 20:34:57. There is a remaining time to complete the full diary (23:59:59 - 20:34:57). Adjust activity times probabilistically according to the maximum activity duration and this remaining time, making the diary end at exactly 23:59:59.</p> <p>Parameters:</p> Name Type Description Default <code>activities</code> <code>List[Row]</code> <p>list of generated activities (stays and moves) for the agent for all of the specified dates. Each activity is a spark row object.</p> required <code>user_id</code> <code>int</code> <p>agent identifier.</p> required <code>agent_stay_type_sequence</code> <code>List[str]</code> <p>list of generated stay types, each represented by a string indicating the stay type.</p> required <code>home_location</code> <code>Tuple[float, float]</code> <p>coordinates of home location.</p> required <code>work_location</code> <code>Tuple[float, float]</code> <p>coordinates of work location.</p> required <code>date</code> <code>date</code> <p>date for activity sequence generation, used for timestamps and random seed generation.</p> required <code>start_of_date</code> <code>datetime</code> <p>timestamp of current date at 00:00:00.</p> required <code>end_of_date</code> <code>datetime</code> <p>timestamp of current date at 23:59:59.</p> required Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def add_agent_date_activities(\n    self,\n    activities: List[Row],\n    user_id: int,\n    agent_stay_type_sequence: List[str],\n    home_location: Tuple[float, float],\n    work_location: Tuple[float, float],\n    date: datetime.date,\n    start_of_date: datetime.datetime,\n    end_of_date: datetime.datetime,\n):\n    \"\"\"\n    For a specific date and user, generate a sequence of activities probabilistically\n    according to the specified activity superset and the activity probabilities.\n    Firstly, assign to each of these activities the minimum duration considered for\n    that activity type. Trip times are based on Pythagorean distance and a specified\n    average speed.\n    If the sum of all minimum duration of the activities and the duration of the trips\n    is higher than the 24h of the day, then assign just one \"home\" activity to the\n    agent from 00:00:00 to 23:59:59.\n    Else, there will be a remaining time. E.g., the diary of an agent, after adding\n    up all trip durations and minimum activity durations may end at 20:34:57. There is\n    a remaining time to complete the full diary (23:59:59 - 20:34:57).\n    Adjust activity times probabilistically according to the maximum activity duration\n    and this remaining time, making the diary end at exactly 23:59:59.\n\n    Args:\n        activities (List[Row]): list of generated activities (stays and moves) for\n            the agent for all of the specified dates. Each activity is a spark\n            row object.\n        user_id (int): agent identifier.\n        agent_stay_type_sequence (List[str]): list of generated stay types,\n            each represented by a string indicating the stay type.\n        home_location (Tuple[float,float]): coordinates of home location.\n        work_location (Tuple[float,float]): coordinates of work location.\n        date (datetime.date): date for activity sequence generation, used for\n            timestamps and random seed generation.\n        start_of_date (datetime.datetime): timestamp of current date at 00:00:00.\n        end_of_date (datetime.datetime): timestamp of current date at 23:59:59.\n    \"\"\"\n    date_activities = self.create_agent_activities_min_duration(\n        user_id, agent_stay_type_sequence, home_location, work_location, date, start_of_date, end_of_date\n    )\n    remaining_time = (end_of_date - date_activities[-1].final_timestamp).total_seconds() / 3600.0\n\n    if remaining_time != 0:\n        self.adjust_activity_times(\n            date_activities,\n            remaining_time,\n            user_id,\n            date,\n            start_of_date,\n            end_of_date,\n        )\n    activities += date_activities\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.add_date_activities","title":"<code>add_date_activities(date, activities)</code>","text":"<p>Generate activity (stays and moves) rows for a specific date according to parameters.</p> <p>Parameters:</p> Name Type Description Default <code>date</code> <code>date</code> <p>date for activity sequence generation, used for timestamps and random seed generation.</p> required <code>activities</code> <code>List[Row]</code> <p>list of generated activities (stays and moves) for the agent for all of the specified dates. Each activity is a spark row object.</p> required Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def add_date_activities(self, date: datetime.date, activities: List[Row]):\n    \"\"\"\n    Generate activity (stays and moves) rows for a specific date according to\n    parameters.\n\n    Args:\n        date (datetime.date): date for activity sequence generation, used for\n            timestamps and random seed generation.\n        activities (List[Row]): list of generated activities (stays and moves) for\n            the agent for all of the specified dates. Each activity is a spark\n            row object.\n    \"\"\"\n    # Start of date, end of date: datetime object generation\n    start_of_date = datetime.datetime.combine(date, datetime.time(0, 0, 0))\n    end_of_date = datetime.datetime.combine(date, datetime.time(23, 59, 59))\n    for user_id in range(self.number_of_users):\n        # generate user information:\n        agent_stay_type_sequence = self.generate_stay_type_sequence(user_id, date)\n        home_location = self.generate_home_location(user_id)\n        work_location = self.generate_work_location(user_id, home_location)\n        self.add_agent_date_activities(\n            activities,\n            user_id,\n            agent_stay_type_sequence,\n            home_location,\n            work_location,\n            date,\n            start_of_date,\n            end_of_date,\n        )\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.adjust_activity_times","title":"<code>adjust_activity_times(date_activities, remaining_time, user_id, date, start_of_date, end_of_date)</code>","text":"<p>Modifies the \"date_activities\" list, changing the initial and final timestamps of both stays and moves probablilistically in order to generate stay durations different from the minimum and adjust the durations of the activities to the 24h of the day.</p> <p>Parameters:</p> Name Type Description Default <code>date_activities</code> <code>List[Row]</code> <p>list of generated activities (stays and moves) of the agent for the specified date. Each activity/trip is a spark row object.</p> required <code>user_id</code> <code>int</code> <p>agent identifier.</p> required <code>date</code> <code>date</code> <p>date for activity sequence generation, used for timestamps and random seed generation.</p> required <code>start_of_date</code> <code>datetime</code> <p>timestamp of current date at 00:00:00.</p> required <code>end_of_date</code> <code>datetime</code> <p>timestamp of current date at 23:59:59.</p> required Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def adjust_activity_times(\n    self,\n    date_activities: List[Row],\n    remaining_time: float,\n    user_id: int,\n    date: datetime.date,\n    start_of_date: datetime.datetime,\n    end_of_date: datetime.datetime,\n):\n    \"\"\"\n    Modifies the \"date_activities\" list, changing the initial and\n    final timestamps of both stays and moves probablilistically in order to\n    generate stay durations different from the minimum and adjust the\n    durations of the activities to the 24h of the day.\n\n    Args:\n        date_activities (List[Row]): list of generated activities (stays and\n            moves) of the agent for the specified date. Each activity/trip is a\n            spark row object.\n        user_id (int): agent identifier.\n        date (datetime.date): date for activity sequence generation, used for\n            timestamps and random seed generation.\n        start_of_date (datetime.datetime): timestamp of current date at 00:00:00.\n        end_of_date (datetime.datetime): timestamp of current date at 23:59:59.\n    \"\"\"\n    current_timestamp = start_of_date\n    for i, activity_row in enumerate(date_activities):\n        if activity_row.activity_type == \"stay\":  # stay:\n            stay_type = activity_row.stay_type\n            old_stay_duration = (\n                activity_row.final_timestamp - activity_row.initial_timestamp\n            ).total_seconds() / 3600.0\n            new_initial_timestamp = current_timestamp\n            if i == len(date_activities) - 1:\n                new_final_timestamp = end_of_date\n                remaining_time = 0.0\n            else:\n                new_stay_duration = self.generate_stay_duration(user_id, date, i, stay_type, remaining_time)\n                new_duration_td = datetime.timedelta(seconds=new_stay_duration * 3600.0)\n                new_final_timestamp = new_initial_timestamp + new_duration_td\n                remaining_time -= new_stay_duration - old_stay_duration\n        else:  # move:\n            old_move_duration = activity_row.final_timestamp - activity_row.initial_timestamp\n            new_initial_timestamp = current_timestamp\n            new_final_timestamp = new_initial_timestamp + old_move_duration\n\n        # common for all activities (stays and moves):\n        activity_row = self.update_spark_row(activity_row, \"initial_timestamp\", new_initial_timestamp)\n        activity_row = self.update_spark_row(activity_row, \"final_timestamp\", new_final_timestamp)\n        date_activities[i] = activity_row\n        current_timestamp = new_final_timestamp\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.calculate_trip_final_time","title":"<code>calculate_trip_final_time(origin_location, destin_location, origin_timestamp)</code>","text":"<p>Calculate end time of a trip given an origin time, an origin location, a destination location and a speed.</p> <p>Parameters:</p> Name Type Description Default <code>origin_location</code> <code>Tuple[float, float]</code> <p>lon, lat of 1st point, in decimal degrees.</p> required <code>destin_location</code> <code>Tuple[float, float]</code> <p>lon, lat of 2nd point, in decimal degrees.</p> required <code>origin_timestamp</code> <code>datetime</code> <p>start time of trip.</p> required <p>Returns:</p> Type Description <code>datetime</code> <p>datetime.datetime: end time of trip.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def calculate_trip_final_time(\n    self,\n    origin_location: Tuple[float, float],\n    destin_location: Tuple[float, float],\n    origin_timestamp: datetime.datetime,\n) -&gt; datetime.datetime:\n    \"\"\"\n    Calculate end time of a trip given an origin time, an origin location,\n    a destination location and a speed.\n\n    Args:\n        origin_location (Tuple[float,float]): lon, lat of 1st point,\n            in decimal degrees.\n        destin_location (Tuple[float,float]): lon, lat of 2nd point,\n            in decimal degrees.\n        origin_timestamp (datetime.datetime): start time of trip.\n\n    Returns:\n        datetime.datetime: end time of trip.\n    \"\"\"\n\n    trip_time = self.calculate_trip_time(origin_location, destin_location)  # s\n    return origin_timestamp + datetime.timedelta(seconds=trip_time)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.calculate_trip_time","title":"<code>calculate_trip_time(o_location, d_location)</code>","text":"<p>Calculate trip time given an origin location and a destination location, according to the specified trip speed.</p> <p>Parameters:</p> Name Type Description Default <code>o_location</code> <code>Tuple[float, float]</code> <p>lon, lat of 1st point, in decimal degrees.</p> required <code>d_location</code> <code>Tuple[float, float]</code> <p>lon, lat of 2nd point, in decimal degrees.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>trip time, in seconds.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def calculate_trip_time(self, o_location: Tuple[float, float], d_location: Tuple[float, float]) -&gt; float:\n    \"\"\"\n    Calculate trip time given an origin location and a destination\n    location, according to the specified trip speed.\n\n    Args:\n        o_location (Tuple[float,float]): lon, lat of 1st point,\n            in decimal degrees.\n        d_location (Tuple[float,float]): lon, lat of 2nd point,\n            in decimal degrees.\n\n    Returns:\n        float: trip time, in seconds.\n    \"\"\"\n    trip_distance = self.haversine(o_location[0], o_location[1], d_location[0], d_location[1])  # m\n    trip_speed = self.displacement_speed  # m/s\n    trip_time = trip_distance / trip_speed  # s\n    return trip_time\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.create_agent_activities_min_duration","title":"<code>create_agent_activities_min_duration(user_id, agent_stay_type_sequence, home_location, work_location, date, start_of_date, end_of_date)</code>","text":"<p>Generate activities of the minimum duration following the specified agent activity sequence for this agent and date.</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>int</code> <p>agent identifier.</p> required <code>agent_stay_type_sequence</code> <code>List[str]</code> <p>list of generated stay types, each represented by a string indicating the stay type.</p> required <code>home_location</code> <code>Tuple[float, float]</code> <p>coordinates of home location.</p> required <code>work_location</code> <code>Tuple[float, float]</code> <p>coordinates of work location.</p> required <code>date</code> <code>date</code> <p>date for activity sequence generation, used for timestamps and random seed generation.</p> required <code>start_of_date</code> <code>datetime</code> <p>timestamp of current date at 00:00:00.</p> required <code>end_of_date</code> <code>datetime</code> <p>timestamp of current date at 23:59:59.</p> required <p>Returns:</p> Type Description <code>List[Row]</code> <p>List[Row]: list of generated activities and trips, each represented by a spark row object with all its information.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def create_agent_activities_min_duration(\n    self,\n    user_id: int,\n    agent_stay_type_sequence: List[str],\n    home_location: Tuple[float, float],\n    work_location: Tuple[float, float],\n    date: datetime.date,\n    start_of_date: datetime.datetime,\n    end_of_date: datetime.datetime,\n) -&gt; List[Row]:\n    \"\"\"\n    Generate activities of the minimum duration following the specified agent\n    activity sequence for this agent and date.\n\n    Args:\n        user_id (int): agent identifier.\n        agent_stay_type_sequence (List[str]): list of generated stay types,\n            each represented by a string indicating the stay type.\n        home_location (Tuple[float,float]): coordinates of home location.\n        work_location (Tuple[float,float]): coordinates of work location.\n        date (datetime.date): date for activity sequence generation, used for\n            timestamps and random seed generation.\n        start_of_date (datetime.datetime): timestamp of current date at 00:00:00.\n        end_of_date (datetime.datetime): timestamp of current date at 23:59:59.\n\n    Returns:\n        List[Row]: list of generated activities and trips, each represented by a\n            spark row object with all its information.\n    \"\"\"\n    date_activities = []\n    previous_location = None\n    for i, stay_type in enumerate(agent_stay_type_sequence):\n        # activity location:\n        location = self.generate_stay_location(\n            stay_type, home_location, work_location, previous_location, user_id, date, i\n        )\n        # previous move (unless first stay)\n        if i != 0:\n            # move timestamps:\n            trip_initial_timestamp = stay_final_timestamp\n            trip_final_timestamp = self.calculate_trip_final_time(\n                previous_location, location, trip_initial_timestamp\n            )\n            # add move:\n            date_activities.append(\n                Row(\n                    user_id=user_id,\n                    activity_type=\"move\",\n                    stay_type=\"move\",\n                    longitude=float(\"nan\"),\n                    latitude=float(\"nan\"),\n                    initial_timestamp=trip_initial_timestamp,\n                    final_timestamp=trip_final_timestamp,\n                    year=date.year,\n                    month=date.month,\n                    day=date.day,\n                )\n            )\n        # stay timestamps:\n        stay_initial_timestamp = start_of_date if i == 0 else trip_final_timestamp\n        stay_duration = self.generate_min_stay_duration(stay_type)\n        stay_final_timestamp = stay_initial_timestamp + datetime.timedelta(hours=stay_duration)\n\n        # add stay:\n        date_activities.append(\n            Row(\n                user_id=user_id,\n                activity_type=\"stay\",\n                stay_type=stay_type,\n                longitude=location[0],\n                latitude=location[1],\n                initial_timestamp=stay_initial_timestamp,\n                final_timestamp=stay_final_timestamp,\n                year=date.year,\n                month=date.month,\n                day=date.day,\n            )\n        )\n\n        previous_location = location\n\n    # after the iterations:\n    if not date_activities:  # 0 stays\n        condition_for_full_home = True\n    elif stay_final_timestamp &gt; end_of_date:  # too many stays\n        condition_for_full_home = True\n    else:\n        condition_for_full_home = False\n\n    if condition_for_full_home:  # simple \"only home\" diary\n        return [\n            Row(\n                user_id=user_id,\n                activity_type=\"stay\",\n                stay_type=\"home\",\n                longitude=home_location[0],\n                latitude=home_location[1],\n                initial_timestamp=start_of_date,\n                final_timestamp=end_of_date,\n                year=date.year,\n                month=date.month,\n                day=date.day,\n            )\n        ]\n    else:\n        return date_activities  # actual generated diary\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_activities","title":"<code>generate_activities()</code>","text":"<p>Generate activity and trip rows according to parameters.</p> <p>Returns:</p> Type Description <code>List[Row]</code> <p>List[Row]: list of generated activities and trips for the agent for all of the specified dates. Each activity/trip is a spark row object.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_activities(self) -&gt; List[Row]:\n    \"\"\"\n    Generate activity and trip rows according to parameters.\n\n    Returns:\n        List[Row]: list of generated activities and trips for the agent for all\n            of the specified dates. Each activity/trip is a spark row object.\n    \"\"\"\n    activities = []\n    for date in self.date_range:\n        self.add_date_activities(date, activities)\n    return activities\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_home_location","title":"<code>generate_home_location(agent_id)</code>","text":"<p>Generate random home location based on bounding box limits.</p> <p>Parameters:</p> Name Type Description Default <code>agent_id</code> <code>int</code> <p>identifier of agent, used for random seed generation.</p> required <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>Tuple[float,float]: coordinates of generated home location.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_home_location(self, agent_id: int) -&gt; Tuple[float, float]:\n    \"\"\"\n    Generate random home location based on bounding box limits.\n\n    Args:\n        agent_id (int): identifier of agent, used for random seed generation.\n\n    Returns:\n        Tuple[float,float]: coordinates of generated home location.\n    \"\"\"\n    seed_lon = self.random_seed_number_generator(1, agent_id)\n    seed_lat = self.random_seed_number_generator(2, agent_id)\n    hlon = Random(seed_lon).uniform(self.longitude_min, self.longitude_max)\n    hlat = Random(seed_lat).uniform(self.latitude_min, self.latitude_max)\n    return (hlon, hlat)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_lonlat_at_distance","title":"<code>generate_lonlat_at_distance(lon1, lat1, d, seed)</code>","text":"<p>Given a point (lon, lat) and a distance, in meters, calculate a new random point that is exactly at the specified distance of the provided lon, lat.</p> <p>Parameters:</p> Name Type Description Default <code>lon1</code> <code>float</code> <p>longitude of point, specified in decimal degrees.</p> required <code>lat1</code> <code>float</code> <p>latitude of point, specified in decimal degrees.</p> required <code>d</code> <code>float</code> <p>distance, in meters.</p> required <code>seed</code> <code>int</code> <p>random seed integer.</p> required <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>Tuple[float, float]: coordinates of randomly generated point.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_lonlat_at_distance(self, lon1: float, lat1: float, d: float, seed: int) -&gt; Tuple[float, float]:\n    \"\"\"\n    Given a point (lon, lat) and a distance, in meters, calculate a new random\n    point that is exactly at the specified distance of the provided lon, lat.\n\n    Args:\n        lon1 (float): longitude of point, specified in decimal degrees.\n        lat1 (float): latitude of point, specified in decimal degrees.\n        d (float): distance, in meters.\n        seed (int): random seed integer.\n\n    Returns:\n        Tuple[float, float]: coordinates of randomly generated point.\n    \"\"\"\n    r = 6_371_000  # Radius of earth in meters.\n\n    d_x = Random(seed).uniform(0, d)\n    d_y = sqrt(d**2 - d_x**2)\n\n    # firstly, convert lat to radians for later\n    lat1_radians = lat1 * pi / 180.0\n\n    # how many meters correspond to one degree of latitude?\n    deg_to_meters = r * pi / 180  # aprox. 111111 meters\n    # thus, the northwards displacement, in degrees of latitude is:\n    north_delta = d_y / deg_to_meters\n\n    # but one degree of longitude does not always correspond to the\n    # same distance... depends on the latitude at where you are!\n    parallel_radius = abs(r * cos(lat1_radians))\n    deg_to_meters = parallel_radius * pi / 180  # variable\n    # thus, the eastwards displacement, in degrees of longitude is:\n    east_delta = d_x / deg_to_meters\n\n    final_lon = lon1 + east_delta * Random(seed).choice([-1, 1])\n    final_lat = lat1 + north_delta * Random(seed).choice([-1, 1])\n\n    return (final_lon, final_lat)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_min_stay_duration","title":"<code>generate_min_stay_duration(stay_type)</code>","text":"<p>Generate minimum stay duration based on stay type specifications.</p> <p>Parameters:</p> Name Type Description Default <code>stay_type</code> <code>str</code> <p>type of stay. Shall be \"home\", \"work\" or \"other\".</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>minimum stay duration.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_min_stay_duration(self, stay_type: str) -&gt; float:\n    \"\"\"\n    Generate minimum stay duration based on stay type specifications.\n\n    Args:\n        stay_type (str): type of stay. Shall be \"home\", \"work\" or \"other\".\n\n    Returns:\n        float: minimum stay duration.\n    \"\"\"\n    if stay_type == \"home\":\n        return self.home_duration_min\n    elif stay_type == \"work\":\n        return self.work_duration_min\n    elif stay_type == \"other\":\n        return self.other_duration_min\n    else:\n        raise ValueError\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_other_location","title":"<code>generate_other_location(agent_id, date, activity_number, home_location, previous_location, seed=6)</code>","text":"<p>Generate other activity location based on previous location and maximum distance to previous location. If there is no previous location (this is the first activity of the day), then the home location is considered as previous location. If the location falls outside of bounding box limits, try again.</p> <p>Parameters:</p> Name Type Description Default <code>agent_id</code> <code>int</code> <p>identifier of agent, used for random seed generation.</p> required <code>date</code> <code>date</code> <p>date, used for random seed generation.</p> required <code>activity_number</code> <code>int</code> <p>act position, used for random seed generation.</p> required <code>home_location</code> <code>Tuple[float, float]</code> <p>coordinates of home location.</p> required <code>previous_location</code> <code>Tuple[float, float]</code> <p>coordinates of previous location.</p> required <code>seed</code> <code>int</code> <p>random seed integer. Defaults to 6.</p> <code>6</code> <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>Tuple[float,float]: coordinates of generated location.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_other_location(\n    self,\n    agent_id: int,\n    date: datetime.date,\n    activity_number: int,\n    home_location: Tuple[float, float],\n    previous_location: Tuple[float, float],\n    seed: int = 6,\n) -&gt; Tuple[float, float]:\n    \"\"\"\n    Generate other activity location based on previous location and maximum distance\n    to previous location. If there is no previous location (this is the first\n    activity of the day), then the home location is considered as previous location.\n    If the location falls outside of bounding box limits, try again.\n\n    Args:\n        agent_id (int): identifier of agent, used for random seed generation.\n        date (datetime.date): date, used for random seed generation.\n        activity_number (int): act position, used for random seed generation.\n        home_location (Tuple[float,float]): coordinates of home location.\n        previous_location (Tuple[float,float]): coordinates of previous location.\n        seed (int, optional): random seed integer. Defaults to 6.\n\n    Returns:\n        Tuple[float,float]: coordinates of generated location.\n    \"\"\"\n    seed_distance = self.random_seed_number_generator(seed - 1, agent_id)\n    random_distance = Random(seed_distance).uniform(self.other_distance_min, self.other_distance_max)\n    if previous_location is None:\n        plon, plat = home_location\n    else:\n        plon, plat = previous_location\n\n    seed_coords = self.random_seed_number_generator(seed, agent_id)\n    olon, olat = self.generate_lonlat_at_distance(plon, plat, random_distance, seed_coords)\n    if not (self.longitude_min &lt; olon &lt; self.longitude_max) or not (\n        self.latitude_min &lt; olat &lt; self.latitude_max\n    ):  # outside limits\n        seed += 1\n        olon, olat = self.generate_other_location(\n            agent_id, date, activity_number, home_location, previous_location, seed=seed\n        )\n\n    return (olon, olat)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_stay_duration","title":"<code>generate_stay_duration(agent_id, date, i, stay_type, remaining_time)</code>","text":"<p>Generate stay duration probabilistically based on activity type abd remaining time.</p> <p>Parameters:</p> Name Type Description Default <code>agent_id</code> <code>int</code> <p>identifier of agent, used for random seed generation.</p> required <code>date</code> <code>date</code> <p>date, used for random seed generation.</p> required <code>i</code> <code>int</code> <p>activity position, used for random seed generation.</p> required <code>stay_type</code> <code>str</code> <p>type of stay. Shall be \"home\", \"work\" or \"other\".</p> required <code>remaining_time</code> <code>float</code> <p>same units as durations.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>generated activity duration.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_stay_duration(\n    self, agent_id: int, date: datetime.date, i: int, stay_type: str, remaining_time: float\n) -&gt; float:\n    \"\"\"\n    Generate stay duration probabilistically based on activity type\n    abd remaining time.\n\n    Args:\n        agent_id (int): identifier of agent, used for random seed generation.\n        date (datetime.date): date, used for random seed generation.\n        i (int): activity position, used for random seed generation.\n        stay_type (str): type of stay. Shall be \"home\", \"work\" or \"other\".\n        remaining_time (float): same units as durations.\n\n    Returns:\n        float: generated activity duration.\n    \"\"\"\n    if stay_type == \"home\":\n        min_duration = self.home_duration_min\n        max_duration = self.home_duration_max\n    elif stay_type == \"work\":\n        min_duration = self.work_duration_min\n        max_duration = self.work_duration_max\n    elif stay_type == \"other\":\n        min_duration = self.other_duration_min\n        max_duration = self.other_duration_max\n    else:\n        raise ValueError\n    seed = self.random_seed_number_generator(7, agent_id, date, i)\n    max_value = min(max_duration, min_duration + remaining_time)\n    return Random(seed).uniform(min_duration, max_value)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_stay_location","title":"<code>generate_stay_location(stay_type, home_location, work_location, previous_location, user_id, date, i)</code>","text":"<p>Generate a random activity location within the bounding box limits based on the activity type and previous activity locations.</p> <p>Parameters:</p> Name Type Description Default <code>stay_type</code> <code>str</code> <p>type of stay (\"home\", \"work\" or \"other\").</p> required <code>home_location</code> <code>Tuple[float, float]</code> <p>coordinates of home location.</p> required <code>work_location</code> <code>Tuple[float, float]</code> <p>coordinates of work location.</p> required <code>previous_location</code> <code>Tuple[float, float]</code> <p>coordinates of previous activity location.</p> required <code>user_id</code> <code>int</code> <p>agent identifier, used for random seed generation.</p> required <code>date</code> <code>date</code> <p>date, used for random seed generation.</p> required <code>i</code> <code>int</code> <p>activity position, used for random seed generation.</p> required <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>Tuple[float,float]: randomly generated activity location coordinates.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_stay_location(\n    self,\n    stay_type: str,\n    home_location: Tuple[float, float],\n    work_location: Tuple[float, float],\n    previous_location: Tuple[float, float],\n    user_id: int,\n    date: datetime.date,\n    i: int,\n) -&gt; Tuple[float, float]:\n    \"\"\"\n    Generate a random activity location within the bounding box limits based\n    on the activity type and previous activity locations.\n\n    Args:\n        stay_type (str): type of stay (\"home\", \"work\" or \"other\").\n        home_location (Tuple[float,float]): coordinates of home location.\n        work_location (Tuple[float,float]): coordinates of work location.\n        previous_location (Tuple[float,float]): coordinates of previous\n            activity location.\n        user_id (int): agent identifier, used for random seed generation.\n        date (datetime.date): date, used for random seed generation.\n        i (int): activity position, used for random seed generation.\n\n    Returns:\n        Tuple[float,float]: randomly generated activity location coordinates.\n    \"\"\"\n    if stay_type == \"home\":\n        location = home_location\n    elif stay_type == \"work\":\n        location = work_location\n    else:\n        location = self.generate_other_location(user_id, date, i, home_location, previous_location)\n    return location\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_stay_type_sequence","title":"<code>generate_stay_type_sequence(agent_id, date)</code>","text":"<p>Generate the sequence of stay types for an agent for a specific date probabilistically based on the superset sequence and specified probabilities. Replace 'home'-'home' and 'work'-'work' sequences by just 'home' or 'work'.</p> <p>Parameters:</p> Name Type Description Default <code>agent_id</code> <code>int</code> <p>identifier of agent, used for random seed generation.</p> required <code>date</code> <code>date</code> <p>date for activity sequence generation, used for random seed generation.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: list of generated stay types, each represented by a string indicating the stay type (e.g. \"home\", \"work\", \"other\").</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_stay_type_sequence(self, agent_id: int, date: datetime.date) -&gt; List[str]:\n    \"\"\"\n    Generate the sequence of stay types for an agent for a specific date\n    probabilistically based on the superset sequence and specified\n    probabilities.\n    Replace 'home'-'home' and 'work'-'work' sequences by just 'home' or\n    'work'.\n\n    Args:\n        agent_id (int): identifier of agent, used for random seed generation.\n        date (datetime.date): date for activity sequence generation, used for\n            random seed generation.\n\n    Returns:\n        List[str]: list of generated stay types, each represented by a string\n            indicating the stay type (e.g. \"home\", \"work\", \"other\").\n    \"\"\"\n    stay_type_sequence = []\n    for i, stay_type in enumerate(self.stay_sequence_superset):\n        stay_weight = self.stay_sequence_probabilities[i]\n        seed = self.random_seed_number_generator(0, agent_id, date, i)\n        if Random(seed).random() &lt; stay_weight:\n            stay_type_sequence.append(stay_type)\n    stay_type_sequence = self.remove_consecutive_stay_types(stay_type_sequence, {\"home\", \"work\"})\n    return stay_type_sequence\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.generate_work_location","title":"<code>generate_work_location(agent_id, home_location, seed=4)</code>","text":"<p>Generate random work location based on home location and maximum distance to home. If the work location falls outside of bounding box limits, try again.</p> <p>Parameters:</p> Name Type Description Default <code>agent_id</code> <code>int</code> <p>identifier of agent, used for random seed generation.</p> required <code>home_location</code> <code>Tuple[float, float]</code> <p>coordinates of home location.</p> required <code>seed</code> <code>int</code> <p>random seed integer. Defaults to 4.</p> <code>4</code> <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>Tuple[float,float]: coordinates of generated work location.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def generate_work_location(\n    self, agent_id: int, home_location: Tuple[float, float], seed: int = 4\n) -&gt; Tuple[float, float]:\n    \"\"\"\n    Generate random work location based on home location and maximum distance to\n    home. If the work location falls outside of bounding box limits, try again.\n\n    Args:\n        agent_id (int): identifier of agent, used for random seed generation.\n        home_location (Tuple[float,float]): coordinates of home location.\n        seed (int, optional): random seed integer. Defaults to 4.\n\n    Returns:\n        Tuple[float,float]: coordinates of generated work location.\n    \"\"\"\n    seed_distance = self.random_seed_number_generator(seed - 1, agent_id)\n    random_distance = Random(seed_distance).uniform(self.home_work_distance_min, self.home_work_distance_max)\n    hlon, hlat = home_location\n    seed_coords = self.random_seed_number_generator(seed, agent_id)\n    wlon, wlat = self.generate_lonlat_at_distance(hlon, hlat, random_distance, seed_coords)\n\n    if not (self.longitude_min &lt; wlon &lt; self.longitude_max) or not (\n        self.latitude_min &lt; wlat &lt; self.latitude_max\n    ):  # outside limits\n        seed += 1\n        wlon, wlat = self.generate_work_location(agent_id, home_location, seed=seed)\n\n    return (wlon, wlat)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.haversine","title":"<code>haversine(lon1, lat1, lon2, lat2)</code>","text":"<p>Calculate the haversine distance in meters between two points.</p> <p>Parameters:</p> Name Type Description Default <code>lon1</code> <code>float</code> <p>longitude of first point, in decimal degrees.</p> required <code>lat1</code> <code>float</code> <p>latitude of first point, in decimal degrees.</p> required <code>lon2</code> <code>float</code> <p>longitude of second point, in decimal degrees.</p> required <code>lat2</code> <code>float</code> <p>latitude of second point, in decimal degrees.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>distance between both points, in meters.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def haversine(self, lon1: float, lat1: float, lon2: float, lat2: float) -&gt; float:\n    \"\"\"\n    Calculate the haversine distance in meters between two points.\n\n    Args:\n        lon1 (float): longitude of first point, in decimal degrees.\n        lat1 (float): latitude of first point, in decimal degrees.\n        lon2 (float): longitude of second point, in decimal degrees.\n        lat2 (float): latitude of second point, in decimal degrees.\n\n    Returns:\n        float: distance between both points, in meters.\n    \"\"\"\n    r = 6_371_000  # Radius of earth in meters.\n\n    # convert decimal degrees to radians\n    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n\n    # haversine formula\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n    a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n    c = 2 * asin(sqrt(a))\n    return c * r\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.random_seed_number_generator","title":"<code>random_seed_number_generator(base_seed, agent_id=None, date=None, i=None)</code>","text":"<p>Generate random seed integer based on provided arguments.</p> <p>Parameters:</p> Name Type Description Default <code>base_seed</code> <code>int</code> <p>base integer for operations.</p> required <code>agent_id</code> <code>int</code> <p>agent identifier. Defaults to None.</p> <code>None</code> <code>date</code> <code>date</code> <p>date. Defaults to None.</p> <code>None</code> <code>i</code> <code>int</code> <p>position integer. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>generated random seed integer.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def random_seed_number_generator(\n    self, base_seed: int, agent_id: int = None, date: datetime.date = None, i: int = None\n) -&gt; int:\n    \"\"\"\n    Generate random seed integer based on provided arguments.\n\n    Args:\n        base_seed (int): base integer for operations.\n        agent_id (int, optional): agent identifier. Defaults to None.\n        date (datetime.date, optional): date. Defaults to None.\n        i (int, optional): position integer. Defaults to None.\n\n    Returns:\n        int: generated random seed integer.\n    \"\"\"\n    seed = base_seed\n    if agent_id is not None:\n        seed += int(agent_id) * 100\n    if date is not None:\n        start_datetime = datetime.datetime.combine(date, datetime.time(0))\n        seed += int(start_datetime.timestamp())\n    if i is not None:\n        seed += i\n    return seed\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.remove_consecutive_stay_types","title":"<code>remove_consecutive_stay_types(stay_sequence_list, stay_types_to_group)</code>","text":"<p>Generate new list replacing consecutive stays of the same type by a unique stay as long as the stay type is contained in the \"stay_types_to_group\" list.</p> <p>Parameters:</p> Name Type Description Default <code>stay_sequence_list</code> <code>List[str]</code> <p>input stay type list.</p> required <code>stay_types_to_group</code> <code>Set[str]</code> <p>stay types to group.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: output stay sequence list.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def remove_consecutive_stay_types(self, stay_sequence_list: List[str], stay_types_to_group: Set[str]) -&gt; List[str]:\n    \"\"\"\n    Generate new list replacing consecutive stays of the same type by\n    a unique stay as long as the stay type is contained in the\n    \"stay_types_to_group\" list.\n\n    Args:\n        stay_sequence_list (List[str]): input stay type list.\n        stay_types_to_group (Set[str]): stay types to group.\n\n    Returns:\n        List[str]: output stay sequence list.\n    \"\"\"\n    new_stay_sequence_list = []\n    previous_stay = None\n    for stay in stay_sequence_list:\n        if stay == previous_stay and stay in stay_types_to_group:\n            pass\n        else:\n            new_stay_sequence_list.append(stay)\n        previous_stay = stay\n    return new_stay_sequence_list\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_diaries/#components.ingestion.synthetic.synthetic_diaries.SyntheticDiaries.update_spark_row","title":"<code>update_spark_row(row, column_name, new_value)</code>","text":"<p>Return an updated spark row object, changing the value of a column.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Row</code> <p>input spark row.</p> required <code>column_name</code> <code>str</code> <p>name of column to modify.</p> required <code>new_value</code> <code>Any</code> <p>new value to assign.</p> required <p>Returns:</p> Name Type Description <code>Row</code> <code>Row</code> <p>modified spark row</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_diaries.py</code> <pre><code>def update_spark_row(self, row: Row, column_name: str, new_value: Any) -&gt; Row:\n    \"\"\"\n    Return an updated spark row object, changing the value of a column.\n\n    Args:\n        row (Row): input spark row.\n        column_name (str): name of column to modify.\n        new_value (Any): new value to assign.\n\n    Returns:\n        Row: modified spark row\n    \"\"\"\n    return Row(**{**row.asDict(), **{column_name: new_value}})\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/","title":"synthetic_events","text":"<p>This module contains the SyntheticEvents class, which is responsible for generating the synthetic event data.</p>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents","title":"<code>SyntheticEvents</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class that generates the event synthetic data. It inherits from the Component abstract class.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>class SyntheticEvents(Component):\n    \"\"\"\n    Class that generates the event synthetic data. It inherits from the Component abstract class.\n    \"\"\"\n\n    COMPONENT_ID = \"SyntheticEvents\"\n\n    def __init__(self, general_config_path: str, component_config_path: str):\n        super().__init__(\n            general_config_path=general_config_path,\n            component_config_path=component_config_path,\n        )\n\n        self.seed = self.config.getint(self.COMPONENT_ID, \"seed\")\n        self.event_freq_stays = self.config.getint(self.COMPONENT_ID, \"event_freq_stays\")\n        self.event_freq_moves = self.config.getint(self.COMPONENT_ID, \"event_freq_moves\")\n        self.closest_cell_distance_max = self.config.getint(self.COMPONENT_ID, \"closest_cell_distance_max\")\n        self.closest_cell_distance_max_for_errors = self.config.getint(\n            self.COMPONENT_ID, \"closest_cell_distance_max_for_errors\"\n        )\n        self.error_location_probability = self.config.getfloat(self.COMPONENT_ID, \"error_location_probability\")\n        self.error_location_distance_min = self.config.getint(self.COMPONENT_ID, \"error_location_distance_min\")\n        self.error_location_distance_max = self.config.getint(self.COMPONENT_ID, \"error_location_distance_max\")\n        self.cartesian_crs = self.config.getint(self.COMPONENT_ID, \"cartesian_crs\")\n        self.error_cell_id_probability = self.config.getfloat(self.COMPONENT_ID, \"error_cell_id_probability\")\n        self.maximum_number_of_cells_for_event = self.config.getfloat(\n            self.COMPONENT_ID, \"maximum_number_of_cells_for_event\"\n        )\n\n        self.mcc = self.config.getint(self.COMPONENT_ID, \"mcc\")\n        self.mnc = self.config.get(self.COMPONENT_ID, \"mnc\")\n\n        # Parameters for synthetic event errors generation (these are not locational errors)\n\n        self.do_event_error_generation = self.config.getboolean(self.COMPONENT_ID, \"do_event_error_generation\")\n        self.column_is_null_probability = self.config.getfloat(self.COMPONENT_ID, \"column_is_null_probability\")\n        self.null_row_prob = self.config.getfloat(self.COMPONENT_ID, \"null_row_probability\")\n        self.data_type_error_prob = self.config.getfloat(self.COMPONENT_ID, \"data_type_error_probability\")\n        self.out_of_bounds_prob = self.config.getfloat(self.COMPONENT_ID, \"out_of_bounds_probability\")\n        self.same_location_duplicate_prob = self.config.getfloat(\n            self.COMPONENT_ID, \"same_location_duplicates_probability\"\n        )\n        self.different_location_duplicate_prob = self.config.getfloat(\n            self.COMPONENT_ID, \"different_location_duplicates_probability\"\n        )\n\n        self.mandatory_columns = [i.name for i in BronzeEventDataObject.SCHEMA]\n        self.error_generation_allowed_columns = set(self.mandatory_columns) - set(\n            [ColNames.year, ColNames.month, ColNames.day]\n        )\n\n        self.order_output_by_timestamp = self.config.getboolean(self.COMPONENT_ID, \"order_output_by_timestamp\")\n\n        self.data_period_start = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_start\"), \"%Y-%m-%d\"\n        ).date()\n        self.data_period_end = datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"data_period_end\"), \"%Y-%m-%d\"\n        ).date()\n\n        self.data_period_dates = [\n            self.data_period_start + timedelta(days=i)\n            for i in range((self.data_period_end - self.data_period_start).days + 1)\n        ]\n\n    def initalize_data_objects(self):\n\n        pop_diares_input_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"diaries_data_bronze\")\n        pop_diaries_bronze_event = BronzeSyntheticDiariesDataObject(self.spark, pop_diares_input_path)\n\n        # Input for cell attributes\n        network_data_input_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"network_data_bronze\")\n\n        cell_locations_bronze = BronzeNetworkDataObject(self.spark, network_data_input_path)\n\n        self.input_data_objects = {\n            BronzeSyntheticDiariesDataObject.ID: pop_diaries_bronze_event,\n            BronzeNetworkDataObject.ID: cell_locations_bronze,\n        }\n\n        output_records_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"event_data_bronze\")\n        bronze_event = BronzeEventDataObject(self.spark, output_records_path)\n\n        self.output_data_objects = {BronzeEventDataObject.ID: bronze_event}\n\n    @get_execution_stats\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n\n        for current_date in self.data_period_dates:\n\n            self.logger.info(f\"Processing diaries for {current_date.strftime('%Y-%m-%d')}\")\n\n            self.current_diaries = self.input_data_objects[BronzeSyntheticDiariesDataObject.ID].df.filter(\n                (F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day)) == F.lit(current_date))\n            )\n\n            self.current_cells = (\n                self.input_data_objects[BronzeNetworkDataObject.ID]\n                .df.filter(\n                    (\n                        F.make_date(F.col(ColNames.year), F.col(ColNames.month), F.col(ColNames.day))\n                        == F.lit(current_date)\n                    )\n                )\n                .select(\n                    ColNames.cell_id,\n                    ColNames.latitude,\n                    ColNames.longitude,\n                    ColNames.year,\n                    ColNames.month,\n                    ColNames.day,\n                )\n            )\n\n            self.transform()\n            self.write()\n            self.spark.catalog.clearCache()\n\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n\n        pop_diaries_df = self.current_diaries\n        cells_df = self.current_cells\n\n        # Filtering stays to get the lat and lon of movement starting and end point\n        stays_df = pop_diaries_df.filter(F.col(ColNames.activity_type) == \"stay\")\n\n        move_events_df = self.generate_event_timestamps_for_moves(\n            stays_df, self.event_freq_moves, self.cartesian_crs, self.seed\n        )\n        move_events_with_locations_df = self.generate_locations_for_moves(move_events_df, self.cartesian_crs)\n\n        stay_events_df = self.generate_event_timestamps_for_stays(\n            stays_df, self.event_freq_stays, self.cartesian_crs, self.seed\n        )\n\n        generated_stays_and_moves = stay_events_df.union(move_events_with_locations_df)\n\n        # Add geometry column to cells\n        cells_df = cells_df.withColumn(\n            \"cell_geometry\",\n            STF.ST_Transform(\n                STC.ST_Point(F.col(ColNames.longitude), F.col(ColNames.latitude)),\n                F.lit(\"EPSG:4326\"),\n                F.lit(f\"EPSG:{self.cartesian_crs}\"),\n            ),\n        ).select(ColNames.cell_id, \"cell_geometry\")\n\n        # 1) From the clean records, sample records for location errors\n        sampled_records = generated_stays_and_moves.sample(self.error_location_probability, self.seed)\n        generated_stays_and_moves = generated_stays_and_moves.subtract(sampled_records)\n        generated_stays_and_moves = generated_stays_and_moves.withColumn(\n            ColNames.loc_error, F.lit(None).cast(FloatType())\n        )\n\n        records_with_location_errors = self.generate_location_errors(\n            sampled_records,\n            self.error_location_distance_max,\n            self.error_location_distance_min,\n            self.closest_cell_distance_max_for_errors,\n            self.cartesian_crs,\n            self.seed,\n        )\n\n        # 2) From the clean records, sample records for erroneous cell id creation\n        sampled_records = generated_stays_and_moves.sample(self.error_cell_id_probability, self.seed)\n        records_with_cell_id_errors = self.generate_records_with_non_existant_cell_ids(\n            sampled_records, cells_df, self.seed\n        )\n\n        generated_stays_and_moves = generated_stays_and_moves.subtract(sampled_records)\n        generated_stays_and_moves = generated_stays_and_moves.withColumn(\n            \"closest_cell_distance_max\", F.lit(self.closest_cell_distance_max)\n        )\n\n        # Label error rows so that these would be ignored in syntactic error generation\n        records_with_location_errors = records_with_location_errors.withColumn(\"is_modified\", F.lit(True))\n        records_with_cell_id_errors = records_with_cell_id_errors.withColumn(\"is_modified\", F.lit(True))\n        generated_stays_and_moves = generated_stays_and_moves.withColumn(\"is_modified\", F.lit(False))\n\n        # 3) Link a cell id to each location\n\n        records_sdf = generated_stays_and_moves.union(records_with_location_errors)\n\n        records_sdf = self.add_cell_ids_to_locations(\n            records_sdf,\n            cells_df,\n            self.maximum_number_of_cells_for_event,\n            self.seed,\n        )\n\n        # 4) Continuing with the combined dataframe\n\n        records_sdf = records_sdf.union(records_with_cell_id_errors)\n\n        records_sdf = records_sdf.dropDuplicates([ColNames.user_id, ColNames.timestamp])\n\n        records_sdf = records_sdf.withColumn(\n            \"generated_geometry\",\n            STF.ST_Transform(\n                \"generated_geometry\",\n                F.lit(f\"EPSG:{self.cartesian_crs}\"),\n                F.lit(\"EPSG:4326\"),\n            ),\n        )\n\n        records_sdf = (\n            records_sdf.withColumn(ColNames.longitude, STF.ST_X(F.col(\"generated_geometry\")))\n            .withColumn(ColNames.latitude, STF.ST_Y(F.col(\"generated_geometry\")))\n            .drop(\"generated_geometry\")\n        )\n\n        # TODO: add rows with PLMN\n        # MCC and loc_error are added to the records\n        records_sdf = records_sdf.withColumn(ColNames.mcc, F.lit(self.mcc).cast(IntegerType()))\n\n        records_sdf = records_sdf.withColumn(ColNames.mnc, F.lit(self.mnc))\n\n        records_sdf = records_sdf.withColumn(ColNames.plmn, F.lit(None).cast(IntegerType()))\n\n        records_sdf = records_sdf.withColumn(ColNames.year, F.year(F.col(ColNames.timestamp)))\n        records_sdf = records_sdf.withColumn(ColNames.month, F.month(F.col(ColNames.timestamp)))\n        records_sdf = records_sdf.withColumn(ColNames.day, F.dayofmonth(F.col(ColNames.timestamp)))\n\n        # Generate errors\n\n        if self.do_event_error_generation:\n            records_sdf = self.generate_errors(synth_df_raw=records_sdf)\n\n        # Select bronze schema columns\n        columns = {field.name: F.col(field.name).cast(field.dataType) for field in BronzeEventDataObject.SCHEMA.fields}\n        records_sdf = records_sdf.withColumns(columns)\n\n        if self.order_output_by_timestamp:\n            records_sdf = records_sdf.orderBy(ColNames.timestamp)\n\n        self.output_data_objects[BronzeEventDataObject.ID].df = records_sdf\n\n    @staticmethod\n    def generate_event_timestamps_for_moves(\n        stays_sdf: DataFrame, event_freq_moves: int, cartesian_crs: int, seed: int\n    ) -&gt; DataFrame:\n        \"\"\"\n        Generates a DataFrame of event timestamps for movements between stays.\n\n        For each stay in the input DataFrame, this method generates a random number of timestamps equal to the time\n            difference between the end of the current stay and the start of the next stay, divided by the event\n            frequency for moves.\n\n        Args:\n            stays_sdf (pyspark.sql.DataFrame): A DataFrame of stays.\n            event_freq_moves (int): The frequency of events for movements.\n            cartesian_crs (int): The EPSG code of the Cartesian coordinate reference system to use for the geometries.\n            seed (int): The seed that determines the randomness of timestamp generation, and subsequent lat/lon generation.\n\n        Returns:\n            pyspark.sql.DataFrame: A DataFrame of event timestamps for movements.\n        \"\"\"\n\n        # Since the rows with activity_type = movement don't have any locations in the population diaries,\n        # we select the stay points and start generating timestamps in between the start and end of the stay\n\n        stays_sdf = stays_sdf.withColumn(\n            ColNames.geometry,\n            STF.ST_Transform(\n                STC.ST_Point(F.col(ColNames.longitude), F.col(ColNames.latitude)),\n                F.lit(\"EPSG:4326\"),\n                F.lit(f\"EPSG:{cartesian_crs}\"),\n            ),\n        )\n\n        # Define the window specification\u00a0\u2022\n        window_spec = Window.partitionBy(ColNames.year, ColNames.month, ColNames.day, ColNames.user_id).orderBy(\n            ColNames.initial_timestamp\n        )\n\n        # Add columns for next stay's geometry and start timestamp using the lead function\n        stays_sdf = stays_sdf.withColumn(\n            \"next_stay_initial_timestamp\",\n            F.lead(ColNames.initial_timestamp, 1).over(window_spec),\n        )\n\n        stays_sdf = stays_sdf.withColumn(\"next_stay_geometry\", F.lead(ColNames.geometry, 1).over(window_spec))\n\n        stays_sdf = stays_sdf.withColumn(\n            \"time_diff_seconds\",\n            F.unix_timestamp(\"next_stay_initial_timestamp\") - F.unix_timestamp(ColNames.final_timestamp),\n        )\n\n        # Calculate how many timestamps fit in the interval for the given frequency\n        stays_sdf = stays_sdf.withColumn(\n            \"timestamps_count\",\n            (F.col(\"time_diff_seconds\") / event_freq_moves).cast(\"integer\"),\n        )\n\n        expr_str = f\"transform(sequence(1, timestamps_count), x -&gt; rand({seed}))\"\n\n        stays_sdf = stays_sdf.withColumn(\n            \"random_fraction_on_line\",\n            F.expr(expr_str),\n        ).withColumn(\"random_fraction_on_line\", F.explode(F.col(\"random_fraction_on_line\")))\n\n        # Generate timestamps\n        stays_sdf = stays_sdf.withColumn(\n            \"offset_seconds\",\n            F.col(\"random_fraction_on_line\") * F.col(\"time_diff_seconds\"),\n        )\n        stays_sdf = stays_sdf.withColumn(\n            ColNames.timestamp,\n            F.from_unixtime(F.unix_timestamp(ColNames.final_timestamp) + F.col(\"offset_seconds\")),\n        )\n\n        moves_sdf = stays_sdf.withColumn(ColNames.activity_type, F.lit(\"move\"))\n\n        # Keep only necessary columns\n        moves_sdf = moves_sdf.select(\n            ColNames.user_id,\n            ColNames.timestamp,\n            ColNames.geometry,\n            \"next_stay_geometry\",\n            \"random_fraction_on_line\",\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n        )\n\n        return moves_sdf\n\n    @staticmethod\n    def generate_event_timestamps_for_stays(\n        stays_df: DataFrame, event_freq_stays: int, cartesian_crs: int, seed: int\n    ) -&gt; DataFrame:\n        \"\"\"\n        Generates a DataFrame of event timestamps for stays based on the event frequency for stays.\n\n        For each stay in the input DataFrame, this method calculates\n        the time difference between the initial and final timestamps of the stay.\n        It then generates a number of timestamps equal to this time difference divided by the event\n        frequency for stays. Each timestamp is associated with the location of the stay.\n\n        Args:\n            stays_df (pyspark.sql.DataFrame): A DataFrame of stays.\n            event_freq_stays (int): The frequency of events for stays.\n            cartesian_crs (int): The EPSG code of the Cartesian coordinate reference system to use for the geometries.\n            seed (int): The seed for generating timestamps randomly.\n        Returns:\n            pyspark.sql.DataFrame: A DataFrame of event timestamps for stays.\n        \"\"\"\n\n        stays_df = stays_df.withColumn(\n            \"time_diff_seconds\",\n            F.unix_timestamp(F.col(ColNames.final_timestamp)) - F.unix_timestamp(F.col(ColNames.initial_timestamp)),\n        )\n\n        stays_df = stays_df.withColumn(\n            \"timestamps_count\",\n            (F.col(\"time_diff_seconds\") / event_freq_stays).cast(\"integer\"),\n        )\n\n        expr_str = f\"transform(sequence(1, timestamps_count), x -&gt; rand({seed}))\"\n        stays_df = stays_df.withColumn(\n            \"random_fraction_between_timestamps\",\n            F.expr(expr_str),\n        )\n\n        stays_df = stays_df.withColumn(\n            \"random_fraction_between_timestamps\",\n            F.explode(F.col(\"random_fraction_between_timestamps\")),\n        )\n        stays_df = stays_df.withColumn(\n            \"offset_seconds\",\n            F.col(\"random_fraction_between_timestamps\") * F.col(\"time_diff_seconds\"),\n        )\n        stays_df = stays_df.withColumn(\n            ColNames.timestamp,\n            F.from_unixtime(F.unix_timestamp(ColNames.initial_timestamp) + F.col(\"offset_seconds\")),\n        )\n\n        stays_df = stays_df.withColumn(\n            \"generated_geometry\",\n            STF.ST_Transform(\n                STC.ST_Point(F.col(ColNames.longitude), F.col(ColNames.latitude)),\n                F.lit(\"EPSG:4326\"),\n                F.lit(f\"EPSG:{cartesian_crs}\"),\n            ),\n        )\n\n        stays_df = stays_df.select(\n            ColNames.user_id,\n            ColNames.timestamp,\n            \"generated_geometry\",\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n        )\n\n        return stays_df\n\n    @staticmethod\n    def generate_locations_for_moves(event_timestamps_df: DataFrame, cartesian_crs: int) -&gt; DataFrame:\n        \"\"\"\n        Generates locations for moves based on the event timestamps dataframe.\n        Returns a dataframe, where for each move in the event timestamps dataframe\n        a geometry column is added, representing the location of the move.\n\n        Performs interpolation along the line between the starting move point (previous stay point)\n        and next move point (next stay point) using the randomly generated values in the column random_fraction_on_line.\n\n        Args:\n            event_timestamps_df (pyspark.sql.DataFrame): The event timestamps dataframe.\n\n        Returns:\n            pyspark.sql.DataFrame: The event timestamps dataframe with the added geometry column.\n        \"\"\"\n\n        moves_with_geometry = event_timestamps_df.withColumn(\n            \"line\",\n            STF.ST_MakeLine(F.col(ColNames.geometry), F.col(\"next_stay_geometry\")),\n        )\n        moves_with_geometry = moves_with_geometry.withColumn(\n            \"generated_geometry\",\n            STF.ST_SetSRID(\n                STF.ST_LineInterpolatePoint(F.col(\"line\"), F.col(\"random_fraction_on_line\")),\n                cartesian_crs,\n            ),\n        )\n\n        moves_with_geometry = moves_with_geometry.select(\n            ColNames.user_id,\n            ColNames.timestamp,\n            \"generated_geometry\",\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n        )\n\n        return moves_with_geometry\n\n    @staticmethod\n    def add_cell_ids_to_locations(\n        events_with_locations_df: DataFrame,\n        cells_df: DataFrame,\n        max_n_of_cells: int,\n        seed: int,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Links cell IDs to locations in the events DataFrame.\n\n        This method performs a spatial join between the events and cells DataFrames to add cell IDs to each event.\n        It first creates a buffer around each event location and finds cells that intersect with this buffer.\n        It then calculates the distance from each event location to the cell and ranks the cells based on this distance.\n        It keeps only the top 'max_n_of_cells' closest cells for each event.\n\n        The method also adds a random index to each event-cell pair and filters to keep only one pair per event,\n        randomly selecting one of the closest cells for each event.\n\n        Args:\n            events_with_locations_df (pyspark.sql.DataFrame): A DataFrame of events\n            cells_df (pyspark.sql.DataFrame): A DataFrame of cells\n            max_n_of_cells (int): The maximum number of closest cells to consider for each event.\n\n        Returns:\n            pyspark.sql.DataFrame: A DataFrame of events with cell IDs added.\n        \"\"\"\n        events_with_cells_sdf = events_with_locations_df.join(\n            cells_df[[ColNames.cell_id, \"cell_geometry\"]],\n            (\n                STP.ST_Intersects(\n                    STF.ST_Buffer(\n                        events_with_locations_df[\"generated_geometry\"],\n                        F.col(\"closest_cell_distance_max\"),\n                    ),\n                    cells_df[\"cell_geometry\"],\n                )\n            ),\n        ).withColumn(\n            \"distance_to_cell\",\n            STF.ST_Distance(\n                events_with_locations_df[\"generated_geometry\"],\n                cells_df[\"cell_geometry\"],\n            ),\n        )\n\n        # Selection of different cell ids for a given timestamp is random\n        window_spec = Window.partitionBy(\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            ColNames.user_id,\n            ColNames.timestamp,\n        ).orderBy(F.col(\"distance_to_cell\"))\n\n        events_with_cells_sdf = events_with_cells_sdf.withColumn(\n            \"closest_cells_index\", F.row_number().over(window_spec)\n        ).filter(F.col(\"closest_cells_index\") &lt;= max_n_of_cells)\n\n        window_spec_random = Window.partitionBy(\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            ColNames.user_id,\n            ColNames.timestamp,\n        ).orderBy(F.rand(seed=seed))\n        events_with_cells_sdf = events_with_cells_sdf.withColumn(\n            \"random_cell_index\", F.row_number().over(window_spec_random)\n        ).filter(F.col(\"random_cell_index\") == 1)\n\n        records_sdf = events_with_cells_sdf.select(\n            ColNames.user_id,\n            ColNames.timestamp,\n            \"generated_geometry\",\n            ColNames.cell_id,\n            ColNames.loc_error,\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            \"is_modified\",\n        )\n\n        return records_sdf\n\n    @staticmethod\n    def generate_location_errors(\n        records_sdf: DataFrame,\n        error_location_distance_max: float,\n        error_location_distance_min: float,\n        closest_cell_distance_max: float,\n        cartesian_crs: int,\n        seed: int,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Generates location errors for x and y coordinates of each record in the DataFrame.\n\n        This method adds a random location error to the x and y coordinates of each record in the input DataFrame.\n        The location error is a random value between error_location_distance_min and error_location_distance_max,\n        and is added or subtracted from the x and y coordinates based on a random sign.\n\n        Args:\n            records_sdf (DataFrame): A DataFrame of records\n            error_location_distance_max (float): The maximum location error distance.\n            error_location_distance_min (float): The minimum location error distance.\n            closest_cell_distance_max (float): The maximum distance to the closest cell.\n            cartesian_crs (int): The EPSG code of the Cartesian coordinate reference system to use for the geometries.\n            seed (int): The seed for the random number generator.\n\n        Returns:\n            DataFrame: A DataFrame of records with location errors added to the x and y coordinates.\n        \"\"\"\n\n        errors_df = (\n            records_sdf.withColumn(\"y\", STF.ST_Y(F.col(\"generated_geometry\")))\n            .withColumn(\"x\", STF.ST_X(F.col(\"generated_geometry\")))\n            .drop(\"generated_geometry\")\n        )\n\n        errors_df = errors_df.withColumn(\n            ColNames.loc_error,\n            (F.rand(seed=seed) * (error_location_distance_max - error_location_distance_min))\n            + error_location_distance_min,\n        )\n\n        errors_df = (\n            errors_df.withColumn(\"sign\", F.when(F.rand(seed=seed) &gt; 0.5, 1).otherwise(-1))\n            .withColumn(\"new_x\", F.col(\"x\") + (F.col(ColNames.loc_error) * F.col(\"sign\")))\n            .withColumn(\"new_y\", F.col(\"y\") + (F.col(ColNames.loc_error) * F.col(\"sign\")))\n        )\n\n        errors_df = errors_df.withColumn(\n            \"generated_geometry\",\n            STF.ST_SetSRID(STC.ST_Point(F.col(\"new_x\"), F.col(\"new_y\")), cartesian_crs),\n        ).drop(\"new_x\", \"new_y\")\n\n        errors_df = errors_df.withColumn(\"closest_cell_distance_max\", F.lit(closest_cell_distance_max))\n\n        errors_df = errors_df.select(\n            ColNames.user_id,\n            ColNames.timestamp,\n            \"generated_geometry\",\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n            ColNames.loc_error,\n            \"closest_cell_distance_max\",\n        )\n\n        return errors_df\n\n    @staticmethod\n    def generate_records_with_non_existant_cell_ids(records_sdf: DataFrame, cells_sdf: DataFrame, seed) -&gt; DataFrame:\n        \"\"\"\n        Adds the cell_id column so that it will contain cell_ids that\n        are not present in the cells_df dataframe, yet follow the format of a cell id.\n\n        Args:\n            records_sdf (DataFrame): generated records\n            cells_sdf (DataFrame): cells dataframe\n\n            DataFrame: records with cell ids that are not present in the cells_df dataframe\n        \"\"\"\n\n        # Generates random cell ids for cells_df, and selects those\n        # Join to records is implemented with a monotonically increasing id\n        # So to limit that, this number of all unique cells is used\n        # TODO check how to make this more optimal\n\n        n_of_actual_cell_ids_as_base = cells_sdf.count()\n\n        cells_df_with_random_cell_ids = cells_sdf[[ColNames.cell_id]].withColumn(\n            \"random_cell_id\",\n            (F.rand(seed=seed) * (999_999_999_999_999 - 10_000_000_000_000) + 10_000_000_000_000).cast(\"bigint\"),\n        )\n\n        # Do an  left anti join to ensure that now generated cell ids are not among actual cell ids\n\n        cells_df_inner_joined = cells_df_with_random_cell_ids[[\"random_cell_id\"]].join(\n            cells_sdf[[ColNames.cell_id]],\n            on=F.col(\"random_cell_id\") == F.col(ColNames.cell_id),\n            how=\"leftanti\",\n        )\n\n        cells_df_inner_joined = cells_df_inner_joined.withColumn(\"row_number\", (F.monotonically_increasing_id()))\n        records_sdf = records_sdf.withColumn(\n            \"row_number\",\n            (F.monotonically_increasing_id() % n_of_actual_cell_ids_as_base) + 1,\n        )\n\n        cells_df_inner_joined = cells_df_inner_joined.select(\n            \"row_number\", F.col(\"random_cell_id\").alias(ColNames.cell_id)\n        )\n\n        records_with_random_cell_id = records_sdf.join(cells_df_inner_joined, on=\"row_number\", how=\"left\").drop(\n            \"row_number\"\n        )\n\n        records_with_random_cell_id = records_with_random_cell_id.select(\n            ColNames.user_id,\n            ColNames.timestamp,\n            \"generated_geometry\",\n            ColNames.cell_id,\n            ColNames.loc_error,\n            ColNames.year,\n            ColNames.month,\n            ColNames.day,\n        )\n\n        return records_with_random_cell_id\n\n    # @staticmethod\n    def generate_nulls_in_mandatory_fields(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Generates null values in some fields of some rows based on configuration parameters.\n\n        Args:\n            df (pyspark.sql.DataFrame): clean synthetic data\n\n        Returns:\n            pyspark.sql.DataFrame: synthetic records dataframe with nulls in some columns of some rows\n        \"\"\"\n\n        # Two probability parameters from config apply:\n        # First one sets how many rows (as fraction of all rows) are selected for possible value nulling.\n        # Second one sets the likelyhood for each column to be set to null.\n        # If 1.0, then all mandatory columns of each selected row will be nulled.\n\n        if self.null_row_prob == 0.0:\n            # TODO logging\n            return df\n\n        # Split input dataframe to unchanged and changed portions\n        df = df.cache()\n        error_row_prob = self.null_row_prob\n        unchanged_row_prob = 1.0 - error_row_prob\n        unchanged_rows_df, error_rows_df = df.randomSplit([unchanged_row_prob, error_row_prob], seed=self.seed + 1)\n\n        columns_for_null_selection = list(self.error_generation_allowed_columns)\n        columns_for_null_selection.sort()\n\n        random.seed(self.seed)\n        columns_to_set_as_null = random.sample(\n            columns_for_null_selection,\n            int(round(self.column_is_null_probability * len(columns_for_null_selection), 0)),\n        )\n\n        for column in columns_to_set_as_null:\n            error_rows_df = error_rows_df.withColumn(column, F.lit(None))\n\n        error_rows_df = error_rows_df.withColumn(\"is_modified\", F.lit(True))\n\n        # Re-combine unchanged and changed rows of the dataframe.\n        return unchanged_rows_df.union(error_rows_df)\n\n    # @staticmethod\n    def generate_out_of_bounds_dates(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Transforms the timestamp column values to be out of bound of the selected period,\n        based on probabilities from configuration.\n        Only rows with non-null timestamp values can become altered here.\n\n        Args:\n            df (pyspark.sql.DataFrame): Dataframe of clean synthetic events.\n\n        Returns:\n            pyspark.sql.DataFrame: Data where some timestamp column values are out of bounds as per config.\n        \"\"\"\n\n        if self.out_of_bounds_prob == 0.0:\n            # TODO logging\n            return df\n\n        # Calculate approximate span in months from config parameters.\n        # The parameters should approximately cover the range of \"clean\" dates, so that the erroneous values can be generated outside the range.\n\n        # TODO\n        # This now uses the whole input data to set the bounds\n        ending_timestamp = df.agg(F.max(ColNames.timestamp)).collect()[0][0]\n        starting_timestamp = df.agg(F.min(ColNames.timestamp)).collect()[0][0]\n        events_span_in_months = max(1, (pd.Timestamp(ending_timestamp) - pd.Timestamp(starting_timestamp)).days / 30)\n\n        # Split rows by null/non-null timestamp.\n        df = df.cache()\n        null_timestamp_df = df.where(F.col(ColNames.timestamp).isNull())\n        nonnull_timestamp_df = df.where(F.col(ColNames.timestamp).isNotNull())\n        df.unpersist()\n\n        # From non-null timestamp rows, select a subset for adding errors to, depending on config parameter.\n        nonnull_timestamp_df = nonnull_timestamp_df.cache()\n        error_row_prob = self.out_of_bounds_prob\n        unchanged_row_prob = 1.0 - error_row_prob\n        unchanged_rows_df, error_rows_df = nonnull_timestamp_df.randomSplit(\n            [unchanged_row_prob, error_row_prob], seed=self.seed + 3\n        )\n        # Combine null timestamp rows and not-modified non-null timestamp rows.\n        unchanged_rows_df = unchanged_rows_df.union(null_timestamp_df)\n\n        # Add months offset to error rows to make their timestamp values become outside expected range.\n        months_to_add_col = (F.lit(2) + F.rand(seed=self.seed)) * F.lit(events_span_in_months)\n        modified_date_col = F.add_months(F.col(ColNames.timestamp), months_to_add_col)\n        time_col = F.date_format(F.col(ColNames.timestamp), \" HH:mm:ss\")\n        error_rows_df = error_rows_df.withColumn(ColNames.timestamp, F.concat(modified_date_col, time_col))\n        error_rows_df = error_rows_df.withColumn(\"is_modified\", F.lit(True))\n\n        # Combine changed and unchanged rows dataframes.\n        return unchanged_rows_df.union(error_rows_df)\n\n    # @staticmethod\n    def generate_erroneous_type_values(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Generates errors for sampled rows. Errors are custom defined, for instance a random string, or corrupt timestamp.\n        Does not cast the columns to a different type.\n\n        Args:\n            df (pyspark.sql.DataFrame): dataframe that may have out of bound and null records.\n\n        Returns:\n            pyspark.sql.DataFrame: dataframe with erroneous rows, and possibly, with nulls and out of bound records.\n        \"\"\"\n\n        if self.data_type_error_prob == 0:\n            # TODO logging\n            return df\n\n        # Split dataframe by whether the row has been modified during the error-adding process already.\n        # Already errored rows do not get further changes.\n        df = df.cache()\n        previously_modified_rows_df = df.where(F.col(\"is_modified\"))\n        unmodified_rows_df = df.where(~(F.col(\"is_modified\")))\n        df.unpersist()\n\n        # From unmodified rows, select a subset for adding errors to, depending on config parameter.\n        unmodified_rows_df = unmodified_rows_df.cache()\n        error_row_prob = self.data_type_error_prob\n        unchanged_row_prob = 1.0 - error_row_prob\n        unchanged_rows_df, error_rows_df = unmodified_rows_df.randomSplit(\n            [unchanged_row_prob, error_row_prob], seed=self.seed + 5\n        )\n        # Gather rows that are not modified in this step (combine previously-modified rows and not-selected unmodified rows).\n        unchanged_rows_df = unchanged_rows_df.union(previously_modified_rows_df)\n\n        # Iterate over mandatory columns to mutate the value, depending on column data type.\n        for struct_schema in BronzeEventDataObject.SCHEMA:\n            if struct_schema.name not in self.error_generation_allowed_columns:\n                continue\n\n            column = struct_schema.name\n            col_dtype = struct_schema.dataType\n\n            if col_dtype in [BinaryType()]:\n                # md5 is a smaller hash,\n                to_value = F.unhex(F.md5(F.base64(F.col(column)).cast(StringType())))\n\n            if col_dtype in [FloatType(), IntegerType()]:\n                # changes mcc, lat, lon\n                to_value = (F.col(column) + ((F.rand(seed=self.seed) + F.lit(180)) * 10000)).cast(\"int\")\n\n            if column == ColNames.timestamp and col_dtype == StringType():\n                # Timezone difference manipulation may be performed here, if cleaning module were to support it.\n                # statically one timezone difference\n                # timezone_to = random.randint(0, 12)\n                to_value = F.concat(\n                    F.substring(F.col(column), 1, 10),\n                    F.lit(\"T\"),\n                    F.substring(F.col(column), 12, 9),\n                    # TODO: Temporary remove of timezone addition as cleaning\n                    # module does not support it\n                    # F.lit(f\"+0{timezone_to}:00\")\n                )\n\n            if column == ColNames.cell_id and col_dtype == StringType():\n                random.seed(self.seed)\n                random_string = \"\".join(random.choice(string.ascii_uppercase + string.digits) for _ in range(6)) + \"_\"\n                to_value = F.concat(F.lit(random_string), (F.rand(seed=self.seed) * 100).cast(\"int\"))\n\n            error_rows_df = error_rows_df.withColumn(column, to_value)\n        error_rows_df = error_rows_df.withColumn(\"is_modified\", F.lit(True))\n        return unchanged_rows_df.union(error_rows_df)\n\n    def generate_same_location_duplicates(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Selects a subset of the previously generated data (syntactically clean data) and creates same location duplicates.\n        If input has odd number of rows, one row is discarded.\n\n        Args:\n            df (pyspark.sql.DataFrame): Dataframe of clean synthetic events.\n\n        Returns:\n            pyspark.sql.DataFrame: Dataframe with same location duplicates.\n\n        \"\"\"\n\n        if self.same_location_duplicate_prob == 0:\n            return df\n\n        # Split dataframe by whether the row has been modified during the error-adding process already.\n        # Already errored rows do not get further changes.\n        df = df.cache()\n        previously_modified_rows_df = df.where(F.col(\"is_modified\"))\n        unmodified_rows_df = df.where(~(F.col(\"is_modified\")))\n        df.unpersist()\n\n        # From unmodified rows, select a subset for adding errors to, depending on config parameter.\n        unmodified_rows_df = unmodified_rows_df.cache()\n        error_row_prob = self.same_location_duplicate_prob\n        unchanged_row_prob = 1.0 - error_row_prob\n\n        unchanged_rows_df, error_rows_df = unmodified_rows_df.randomSplit(\n            [unchanged_row_prob, error_row_prob], seed=self.seed + 7\n        )\n\n        # Gather rows that are not modified in this step (combine previously-modified rows and not-selected unmodified rows).\n        unchanged_rows_df = unchanged_rows_df.union(previously_modified_rows_df)\n\n        # Select all even rows and duplicate these\n        error_rows_df = error_rows_df.withColumn(\n            \"user_row_num\",\n            F.row_number().over(Window.partitionBy(ColNames.user_id).orderBy(ColNames.timestamp)),\n        )\n\n        even_rows = error_rows_df.where(F.col(\"user_row_num\") % 2 == 0).drop(\"user_row_num\")\n\n        error_rows_df = even_rows.union(even_rows)\n        error_rows_df = error_rows_df.withColumn(\"is_modified\", F.lit(True))\n\n        return unchanged_rows_df.union(error_rows_df)\n\n    def generate_different_location_duplicates(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Selects a subset of the previously generated data (syntactically clean data) and creates different location duplicates.\n        If input has odd number of rows, one row is discarded.\n\n        Args:\n            df (pyspark.sql.DataFrame): Dataframe of clean synthetic events.\n\n        Returns:\n            pyspark.sql.DataFrame: Dataframe with same different location duplicates.\n\n        \"\"\"\n\n        if self.different_location_duplicate_prob == 0:\n            return df\n\n        # Split dataframe by whether the row has been modified during the error-adding process already.\n        # Already errored rows do not get further changes.\n        df = df.cache()\n        previously_modified_rows_df = df.where(F.col(\"is_modified\"))\n        unmodified_rows_df = df.where(~(F.col(\"is_modified\")))\n        df.unpersist()\n\n        # From unmodified rows, select a subset for adding errors to, depending on config parameter.\n        unmodified_rows_df = unmodified_rows_df.cache()\n        error_row_prob = self.different_location_duplicate_prob\n        unchanged_row_prob = 1.0 - error_row_prob\n        unchanged_rows_df, error_rows_df = unmodified_rows_df.randomSplit(\n            [unchanged_row_prob, error_row_prob], seed=self.seed + 9\n        )\n\n        # Gather rows that are not modified in this step (combine previously-modified rows and not-selected unmodified rows).\n        unchanged_rows_df = unchanged_rows_df.union(previously_modified_rows_df)\n\n        # Select all even rows and modify one set of these to offset the location\n\n        error_rows_df = error_rows_df.withColumn(\n            \"user_row_num\",\n            F.row_number().over(Window.partitionBy(ColNames.user_id).orderBy(ColNames.timestamp)),\n        )\n\n        even_rows = error_rows_df.where(F.col(\"user_row_num\") % 2 == 0).drop(\"user_row_num\")\n\n        # No change to cell id here, as that would additionnally need to check that the modified cell id is actual in the diaries\n        modified_rows = even_rows.withColumn(ColNames.latitude, F.col(ColNames.latitude) * F.lit(0.95)).withColumn(\n            ColNames.longitude, F.col(ColNames.longitude) * F.lit(0.95)\n        )\n\n        error_rows_df = even_rows.union(modified_rows)\n        error_rows_df = error_rows_df.withColumn(\"is_modified\", F.lit(True))\n\n        return unchanged_rows_df.union(error_rows_df)\n\n    def generate_errors(self, synth_df_raw: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Inputs a dataframe that contains synthetic records based on diaries.\n        These records include locational errors, etc. This function only selects the clean generated records from previous steps.\n        Generates errors for those clean records.\n        Calls all error generation functions.\n\n        Args:\n            synth_df_raw (pyspark.sql.DataFrame): Data of raw and clean synthetic events.\n\n        Returns:\n            pyspark.sql.DataFrame: Dataframe, with erroneous records, according to probabilities defined in the configuration.\n        \"\"\"\n\n        synth_df_raw = synth_df_raw.where(~F.col(\"is_modified\"))\n\n        synth_df = synth_df_raw.cache()\n\n        synth_df = self.generate_nulls_in_mandatory_fields(synth_df)\n        synth_df = self.generate_out_of_bounds_dates(synth_df)\n        synth_df = self.generate_erroneous_type_values(synth_df)\n        synth_df = self.generate_same_location_duplicates(synth_df)\n        synth_df = self.generate_different_location_duplicates(synth_df)\n\n        return synth_df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.add_cell_ids_to_locations","title":"<code>add_cell_ids_to_locations(events_with_locations_df, cells_df, max_n_of_cells, seed)</code>  <code>staticmethod</code>","text":"<p>Links cell IDs to locations in the events DataFrame.</p> <p>This method performs a spatial join between the events and cells DataFrames to add cell IDs to each event. It first creates a buffer around each event location and finds cells that intersect with this buffer. It then calculates the distance from each event location to the cell and ranks the cells based on this distance. It keeps only the top 'max_n_of_cells' closest cells for each event.</p> <p>The method also adds a random index to each event-cell pair and filters to keep only one pair per event, randomly selecting one of the closest cells for each event.</p> <p>Parameters:</p> Name Type Description Default <code>events_with_locations_df</code> <code>DataFrame</code> <p>A DataFrame of events</p> required <code>cells_df</code> <code>DataFrame</code> <p>A DataFrame of cells</p> required <code>max_n_of_cells</code> <code>int</code> <p>The maximum number of closest cells to consider for each event.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: A DataFrame of events with cell IDs added.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>@staticmethod\ndef add_cell_ids_to_locations(\n    events_with_locations_df: DataFrame,\n    cells_df: DataFrame,\n    max_n_of_cells: int,\n    seed: int,\n) -&gt; DataFrame:\n    \"\"\"\n    Links cell IDs to locations in the events DataFrame.\n\n    This method performs a spatial join between the events and cells DataFrames to add cell IDs to each event.\n    It first creates a buffer around each event location and finds cells that intersect with this buffer.\n    It then calculates the distance from each event location to the cell and ranks the cells based on this distance.\n    It keeps only the top 'max_n_of_cells' closest cells for each event.\n\n    The method also adds a random index to each event-cell pair and filters to keep only one pair per event,\n    randomly selecting one of the closest cells for each event.\n\n    Args:\n        events_with_locations_df (pyspark.sql.DataFrame): A DataFrame of events\n        cells_df (pyspark.sql.DataFrame): A DataFrame of cells\n        max_n_of_cells (int): The maximum number of closest cells to consider for each event.\n\n    Returns:\n        pyspark.sql.DataFrame: A DataFrame of events with cell IDs added.\n    \"\"\"\n    events_with_cells_sdf = events_with_locations_df.join(\n        cells_df[[ColNames.cell_id, \"cell_geometry\"]],\n        (\n            STP.ST_Intersects(\n                STF.ST_Buffer(\n                    events_with_locations_df[\"generated_geometry\"],\n                    F.col(\"closest_cell_distance_max\"),\n                ),\n                cells_df[\"cell_geometry\"],\n            )\n        ),\n    ).withColumn(\n        \"distance_to_cell\",\n        STF.ST_Distance(\n            events_with_locations_df[\"generated_geometry\"],\n            cells_df[\"cell_geometry\"],\n        ),\n    )\n\n    # Selection of different cell ids for a given timestamp is random\n    window_spec = Window.partitionBy(\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n        ColNames.user_id,\n        ColNames.timestamp,\n    ).orderBy(F.col(\"distance_to_cell\"))\n\n    events_with_cells_sdf = events_with_cells_sdf.withColumn(\n        \"closest_cells_index\", F.row_number().over(window_spec)\n    ).filter(F.col(\"closest_cells_index\") &lt;= max_n_of_cells)\n\n    window_spec_random = Window.partitionBy(\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n        ColNames.user_id,\n        ColNames.timestamp,\n    ).orderBy(F.rand(seed=seed))\n    events_with_cells_sdf = events_with_cells_sdf.withColumn(\n        \"random_cell_index\", F.row_number().over(window_spec_random)\n    ).filter(F.col(\"random_cell_index\") == 1)\n\n    records_sdf = events_with_cells_sdf.select(\n        ColNames.user_id,\n        ColNames.timestamp,\n        \"generated_geometry\",\n        ColNames.cell_id,\n        ColNames.loc_error,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n        \"is_modified\",\n    )\n\n    return records_sdf\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_different_location_duplicates","title":"<code>generate_different_location_duplicates(df)</code>","text":"<p>Selects a subset of the previously generated data (syntactically clean data) and creates different location duplicates. If input has odd number of rows, one row is discarded.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe of clean synthetic events.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: Dataframe with same different location duplicates.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>def generate_different_location_duplicates(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Selects a subset of the previously generated data (syntactically clean data) and creates different location duplicates.\n    If input has odd number of rows, one row is discarded.\n\n    Args:\n        df (pyspark.sql.DataFrame): Dataframe of clean synthetic events.\n\n    Returns:\n        pyspark.sql.DataFrame: Dataframe with same different location duplicates.\n\n    \"\"\"\n\n    if self.different_location_duplicate_prob == 0:\n        return df\n\n    # Split dataframe by whether the row has been modified during the error-adding process already.\n    # Already errored rows do not get further changes.\n    df = df.cache()\n    previously_modified_rows_df = df.where(F.col(\"is_modified\"))\n    unmodified_rows_df = df.where(~(F.col(\"is_modified\")))\n    df.unpersist()\n\n    # From unmodified rows, select a subset for adding errors to, depending on config parameter.\n    unmodified_rows_df = unmodified_rows_df.cache()\n    error_row_prob = self.different_location_duplicate_prob\n    unchanged_row_prob = 1.0 - error_row_prob\n    unchanged_rows_df, error_rows_df = unmodified_rows_df.randomSplit(\n        [unchanged_row_prob, error_row_prob], seed=self.seed + 9\n    )\n\n    # Gather rows that are not modified in this step (combine previously-modified rows and not-selected unmodified rows).\n    unchanged_rows_df = unchanged_rows_df.union(previously_modified_rows_df)\n\n    # Select all even rows and modify one set of these to offset the location\n\n    error_rows_df = error_rows_df.withColumn(\n        \"user_row_num\",\n        F.row_number().over(Window.partitionBy(ColNames.user_id).orderBy(ColNames.timestamp)),\n    )\n\n    even_rows = error_rows_df.where(F.col(\"user_row_num\") % 2 == 0).drop(\"user_row_num\")\n\n    # No change to cell id here, as that would additionnally need to check that the modified cell id is actual in the diaries\n    modified_rows = even_rows.withColumn(ColNames.latitude, F.col(ColNames.latitude) * F.lit(0.95)).withColumn(\n        ColNames.longitude, F.col(ColNames.longitude) * F.lit(0.95)\n    )\n\n    error_rows_df = even_rows.union(modified_rows)\n    error_rows_df = error_rows_df.withColumn(\"is_modified\", F.lit(True))\n\n    return unchanged_rows_df.union(error_rows_df)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_erroneous_type_values","title":"<code>generate_erroneous_type_values(df)</code>","text":"<p>Generates errors for sampled rows. Errors are custom defined, for instance a random string, or corrupt timestamp. Does not cast the columns to a different type.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe that may have out of bound and null records.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: dataframe with erroneous rows, and possibly, with nulls and out of bound records.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>def generate_erroneous_type_values(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Generates errors for sampled rows. Errors are custom defined, for instance a random string, or corrupt timestamp.\n    Does not cast the columns to a different type.\n\n    Args:\n        df (pyspark.sql.DataFrame): dataframe that may have out of bound and null records.\n\n    Returns:\n        pyspark.sql.DataFrame: dataframe with erroneous rows, and possibly, with nulls and out of bound records.\n    \"\"\"\n\n    if self.data_type_error_prob == 0:\n        # TODO logging\n        return df\n\n    # Split dataframe by whether the row has been modified during the error-adding process already.\n    # Already errored rows do not get further changes.\n    df = df.cache()\n    previously_modified_rows_df = df.where(F.col(\"is_modified\"))\n    unmodified_rows_df = df.where(~(F.col(\"is_modified\")))\n    df.unpersist()\n\n    # From unmodified rows, select a subset for adding errors to, depending on config parameter.\n    unmodified_rows_df = unmodified_rows_df.cache()\n    error_row_prob = self.data_type_error_prob\n    unchanged_row_prob = 1.0 - error_row_prob\n    unchanged_rows_df, error_rows_df = unmodified_rows_df.randomSplit(\n        [unchanged_row_prob, error_row_prob], seed=self.seed + 5\n    )\n    # Gather rows that are not modified in this step (combine previously-modified rows and not-selected unmodified rows).\n    unchanged_rows_df = unchanged_rows_df.union(previously_modified_rows_df)\n\n    # Iterate over mandatory columns to mutate the value, depending on column data type.\n    for struct_schema in BronzeEventDataObject.SCHEMA:\n        if struct_schema.name not in self.error_generation_allowed_columns:\n            continue\n\n        column = struct_schema.name\n        col_dtype = struct_schema.dataType\n\n        if col_dtype in [BinaryType()]:\n            # md5 is a smaller hash,\n            to_value = F.unhex(F.md5(F.base64(F.col(column)).cast(StringType())))\n\n        if col_dtype in [FloatType(), IntegerType()]:\n            # changes mcc, lat, lon\n            to_value = (F.col(column) + ((F.rand(seed=self.seed) + F.lit(180)) * 10000)).cast(\"int\")\n\n        if column == ColNames.timestamp and col_dtype == StringType():\n            # Timezone difference manipulation may be performed here, if cleaning module were to support it.\n            # statically one timezone difference\n            # timezone_to = random.randint(0, 12)\n            to_value = F.concat(\n                F.substring(F.col(column), 1, 10),\n                F.lit(\"T\"),\n                F.substring(F.col(column), 12, 9),\n                # TODO: Temporary remove of timezone addition as cleaning\n                # module does not support it\n                # F.lit(f\"+0{timezone_to}:00\")\n            )\n\n        if column == ColNames.cell_id and col_dtype == StringType():\n            random.seed(self.seed)\n            random_string = \"\".join(random.choice(string.ascii_uppercase + string.digits) for _ in range(6)) + \"_\"\n            to_value = F.concat(F.lit(random_string), (F.rand(seed=self.seed) * 100).cast(\"int\"))\n\n        error_rows_df = error_rows_df.withColumn(column, to_value)\n    error_rows_df = error_rows_df.withColumn(\"is_modified\", F.lit(True))\n    return unchanged_rows_df.union(error_rows_df)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_errors","title":"<code>generate_errors(synth_df_raw)</code>","text":"<p>Inputs a dataframe that contains synthetic records based on diaries. These records include locational errors, etc. This function only selects the clean generated records from previous steps. Generates errors for those clean records. Calls all error generation functions.</p> <p>Parameters:</p> Name Type Description Default <code>synth_df_raw</code> <code>DataFrame</code> <p>Data of raw and clean synthetic events.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: Dataframe, with erroneous records, according to probabilities defined in the configuration.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>def generate_errors(self, synth_df_raw: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Inputs a dataframe that contains synthetic records based on diaries.\n    These records include locational errors, etc. This function only selects the clean generated records from previous steps.\n    Generates errors for those clean records.\n    Calls all error generation functions.\n\n    Args:\n        synth_df_raw (pyspark.sql.DataFrame): Data of raw and clean synthetic events.\n\n    Returns:\n        pyspark.sql.DataFrame: Dataframe, with erroneous records, according to probabilities defined in the configuration.\n    \"\"\"\n\n    synth_df_raw = synth_df_raw.where(~F.col(\"is_modified\"))\n\n    synth_df = synth_df_raw.cache()\n\n    synth_df = self.generate_nulls_in_mandatory_fields(synth_df)\n    synth_df = self.generate_out_of_bounds_dates(synth_df)\n    synth_df = self.generate_erroneous_type_values(synth_df)\n    synth_df = self.generate_same_location_duplicates(synth_df)\n    synth_df = self.generate_different_location_duplicates(synth_df)\n\n    return synth_df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_event_timestamps_for_moves","title":"<code>generate_event_timestamps_for_moves(stays_sdf, event_freq_moves, cartesian_crs, seed)</code>  <code>staticmethod</code>","text":"<p>Generates a DataFrame of event timestamps for movements between stays.</p> <p>For each stay in the input DataFrame, this method generates a random number of timestamps equal to the time     difference between the end of the current stay and the start of the next stay, divided by the event     frequency for moves.</p> <p>Parameters:</p> Name Type Description Default <code>stays_sdf</code> <code>DataFrame</code> <p>A DataFrame of stays.</p> required <code>event_freq_moves</code> <code>int</code> <p>The frequency of events for movements.</p> required <code>cartesian_crs</code> <code>int</code> <p>The EPSG code of the Cartesian coordinate reference system to use for the geometries.</p> required <code>seed</code> <code>int</code> <p>The seed that determines the randomness of timestamp generation, and subsequent lat/lon generation.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: A DataFrame of event timestamps for movements.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>@staticmethod\ndef generate_event_timestamps_for_moves(\n    stays_sdf: DataFrame, event_freq_moves: int, cartesian_crs: int, seed: int\n) -&gt; DataFrame:\n    \"\"\"\n    Generates a DataFrame of event timestamps for movements between stays.\n\n    For each stay in the input DataFrame, this method generates a random number of timestamps equal to the time\n        difference between the end of the current stay and the start of the next stay, divided by the event\n        frequency for moves.\n\n    Args:\n        stays_sdf (pyspark.sql.DataFrame): A DataFrame of stays.\n        event_freq_moves (int): The frequency of events for movements.\n        cartesian_crs (int): The EPSG code of the Cartesian coordinate reference system to use for the geometries.\n        seed (int): The seed that determines the randomness of timestamp generation, and subsequent lat/lon generation.\n\n    Returns:\n        pyspark.sql.DataFrame: A DataFrame of event timestamps for movements.\n    \"\"\"\n\n    # Since the rows with activity_type = movement don't have any locations in the population diaries,\n    # we select the stay points and start generating timestamps in between the start and end of the stay\n\n    stays_sdf = stays_sdf.withColumn(\n        ColNames.geometry,\n        STF.ST_Transform(\n            STC.ST_Point(F.col(ColNames.longitude), F.col(ColNames.latitude)),\n            F.lit(\"EPSG:4326\"),\n            F.lit(f\"EPSG:{cartesian_crs}\"),\n        ),\n    )\n\n    # Define the window specification\u00a0\u2022\n    window_spec = Window.partitionBy(ColNames.year, ColNames.month, ColNames.day, ColNames.user_id).orderBy(\n        ColNames.initial_timestamp\n    )\n\n    # Add columns for next stay's geometry and start timestamp using the lead function\n    stays_sdf = stays_sdf.withColumn(\n        \"next_stay_initial_timestamp\",\n        F.lead(ColNames.initial_timestamp, 1).over(window_spec),\n    )\n\n    stays_sdf = stays_sdf.withColumn(\"next_stay_geometry\", F.lead(ColNames.geometry, 1).over(window_spec))\n\n    stays_sdf = stays_sdf.withColumn(\n        \"time_diff_seconds\",\n        F.unix_timestamp(\"next_stay_initial_timestamp\") - F.unix_timestamp(ColNames.final_timestamp),\n    )\n\n    # Calculate how many timestamps fit in the interval for the given frequency\n    stays_sdf = stays_sdf.withColumn(\n        \"timestamps_count\",\n        (F.col(\"time_diff_seconds\") / event_freq_moves).cast(\"integer\"),\n    )\n\n    expr_str = f\"transform(sequence(1, timestamps_count), x -&gt; rand({seed}))\"\n\n    stays_sdf = stays_sdf.withColumn(\n        \"random_fraction_on_line\",\n        F.expr(expr_str),\n    ).withColumn(\"random_fraction_on_line\", F.explode(F.col(\"random_fraction_on_line\")))\n\n    # Generate timestamps\n    stays_sdf = stays_sdf.withColumn(\n        \"offset_seconds\",\n        F.col(\"random_fraction_on_line\") * F.col(\"time_diff_seconds\"),\n    )\n    stays_sdf = stays_sdf.withColumn(\n        ColNames.timestamp,\n        F.from_unixtime(F.unix_timestamp(ColNames.final_timestamp) + F.col(\"offset_seconds\")),\n    )\n\n    moves_sdf = stays_sdf.withColumn(ColNames.activity_type, F.lit(\"move\"))\n\n    # Keep only necessary columns\n    moves_sdf = moves_sdf.select(\n        ColNames.user_id,\n        ColNames.timestamp,\n        ColNames.geometry,\n        \"next_stay_geometry\",\n        \"random_fraction_on_line\",\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    )\n\n    return moves_sdf\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_event_timestamps_for_stays","title":"<code>generate_event_timestamps_for_stays(stays_df, event_freq_stays, cartesian_crs, seed)</code>  <code>staticmethod</code>","text":"<p>Generates a DataFrame of event timestamps for stays based on the event frequency for stays.</p> <p>For each stay in the input DataFrame, this method calculates the time difference between the initial and final timestamps of the stay. It then generates a number of timestamps equal to this time difference divided by the event frequency for stays. Each timestamp is associated with the location of the stay.</p> <p>Parameters:</p> Name Type Description Default <code>stays_df</code> <code>DataFrame</code> <p>A DataFrame of stays.</p> required <code>event_freq_stays</code> <code>int</code> <p>The frequency of events for stays.</p> required <code>cartesian_crs</code> <code>int</code> <p>The EPSG code of the Cartesian coordinate reference system to use for the geometries.</p> required <code>seed</code> <code>int</code> <p>The seed for generating timestamps randomly.</p> required <p>Returns:     pyspark.sql.DataFrame: A DataFrame of event timestamps for stays.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>@staticmethod\ndef generate_event_timestamps_for_stays(\n    stays_df: DataFrame, event_freq_stays: int, cartesian_crs: int, seed: int\n) -&gt; DataFrame:\n    \"\"\"\n    Generates a DataFrame of event timestamps for stays based on the event frequency for stays.\n\n    For each stay in the input DataFrame, this method calculates\n    the time difference between the initial and final timestamps of the stay.\n    It then generates a number of timestamps equal to this time difference divided by the event\n    frequency for stays. Each timestamp is associated with the location of the stay.\n\n    Args:\n        stays_df (pyspark.sql.DataFrame): A DataFrame of stays.\n        event_freq_stays (int): The frequency of events for stays.\n        cartesian_crs (int): The EPSG code of the Cartesian coordinate reference system to use for the geometries.\n        seed (int): The seed for generating timestamps randomly.\n    Returns:\n        pyspark.sql.DataFrame: A DataFrame of event timestamps for stays.\n    \"\"\"\n\n    stays_df = stays_df.withColumn(\n        \"time_diff_seconds\",\n        F.unix_timestamp(F.col(ColNames.final_timestamp)) - F.unix_timestamp(F.col(ColNames.initial_timestamp)),\n    )\n\n    stays_df = stays_df.withColumn(\n        \"timestamps_count\",\n        (F.col(\"time_diff_seconds\") / event_freq_stays).cast(\"integer\"),\n    )\n\n    expr_str = f\"transform(sequence(1, timestamps_count), x -&gt; rand({seed}))\"\n    stays_df = stays_df.withColumn(\n        \"random_fraction_between_timestamps\",\n        F.expr(expr_str),\n    )\n\n    stays_df = stays_df.withColumn(\n        \"random_fraction_between_timestamps\",\n        F.explode(F.col(\"random_fraction_between_timestamps\")),\n    )\n    stays_df = stays_df.withColumn(\n        \"offset_seconds\",\n        F.col(\"random_fraction_between_timestamps\") * F.col(\"time_diff_seconds\"),\n    )\n    stays_df = stays_df.withColumn(\n        ColNames.timestamp,\n        F.from_unixtime(F.unix_timestamp(ColNames.initial_timestamp) + F.col(\"offset_seconds\")),\n    )\n\n    stays_df = stays_df.withColumn(\n        \"generated_geometry\",\n        STF.ST_Transform(\n            STC.ST_Point(F.col(ColNames.longitude), F.col(ColNames.latitude)),\n            F.lit(\"EPSG:4326\"),\n            F.lit(f\"EPSG:{cartesian_crs}\"),\n        ),\n    )\n\n    stays_df = stays_df.select(\n        ColNames.user_id,\n        ColNames.timestamp,\n        \"generated_geometry\",\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    )\n\n    return stays_df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_location_errors","title":"<code>generate_location_errors(records_sdf, error_location_distance_max, error_location_distance_min, closest_cell_distance_max, cartesian_crs, seed)</code>  <code>staticmethod</code>","text":"<p>Generates location errors for x and y coordinates of each record in the DataFrame.</p> <p>This method adds a random location error to the x and y coordinates of each record in the input DataFrame. The location error is a random value between error_location_distance_min and error_location_distance_max, and is added or subtracted from the x and y coordinates based on a random sign.</p> <p>Parameters:</p> Name Type Description Default <code>records_sdf</code> <code>DataFrame</code> <p>A DataFrame of records</p> required <code>error_location_distance_max</code> <code>float</code> <p>The maximum location error distance.</p> required <code>error_location_distance_min</code> <code>float</code> <p>The minimum location error distance.</p> required <code>closest_cell_distance_max</code> <code>float</code> <p>The maximum distance to the closest cell.</p> required <code>cartesian_crs</code> <code>int</code> <p>The EPSG code of the Cartesian coordinate reference system to use for the geometries.</p> required <code>seed</code> <code>int</code> <p>The seed for the random number generator.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame of records with location errors added to the x and y coordinates.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>@staticmethod\ndef generate_location_errors(\n    records_sdf: DataFrame,\n    error_location_distance_max: float,\n    error_location_distance_min: float,\n    closest_cell_distance_max: float,\n    cartesian_crs: int,\n    seed: int,\n) -&gt; DataFrame:\n    \"\"\"\n    Generates location errors for x and y coordinates of each record in the DataFrame.\n\n    This method adds a random location error to the x and y coordinates of each record in the input DataFrame.\n    The location error is a random value between error_location_distance_min and error_location_distance_max,\n    and is added or subtracted from the x and y coordinates based on a random sign.\n\n    Args:\n        records_sdf (DataFrame): A DataFrame of records\n        error_location_distance_max (float): The maximum location error distance.\n        error_location_distance_min (float): The minimum location error distance.\n        closest_cell_distance_max (float): The maximum distance to the closest cell.\n        cartesian_crs (int): The EPSG code of the Cartesian coordinate reference system to use for the geometries.\n        seed (int): The seed for the random number generator.\n\n    Returns:\n        DataFrame: A DataFrame of records with location errors added to the x and y coordinates.\n    \"\"\"\n\n    errors_df = (\n        records_sdf.withColumn(\"y\", STF.ST_Y(F.col(\"generated_geometry\")))\n        .withColumn(\"x\", STF.ST_X(F.col(\"generated_geometry\")))\n        .drop(\"generated_geometry\")\n    )\n\n    errors_df = errors_df.withColumn(\n        ColNames.loc_error,\n        (F.rand(seed=seed) * (error_location_distance_max - error_location_distance_min))\n        + error_location_distance_min,\n    )\n\n    errors_df = (\n        errors_df.withColumn(\"sign\", F.when(F.rand(seed=seed) &gt; 0.5, 1).otherwise(-1))\n        .withColumn(\"new_x\", F.col(\"x\") + (F.col(ColNames.loc_error) * F.col(\"sign\")))\n        .withColumn(\"new_y\", F.col(\"y\") + (F.col(ColNames.loc_error) * F.col(\"sign\")))\n    )\n\n    errors_df = errors_df.withColumn(\n        \"generated_geometry\",\n        STF.ST_SetSRID(STC.ST_Point(F.col(\"new_x\"), F.col(\"new_y\")), cartesian_crs),\n    ).drop(\"new_x\", \"new_y\")\n\n    errors_df = errors_df.withColumn(\"closest_cell_distance_max\", F.lit(closest_cell_distance_max))\n\n    errors_df = errors_df.select(\n        ColNames.user_id,\n        ColNames.timestamp,\n        \"generated_geometry\",\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n        ColNames.loc_error,\n        \"closest_cell_distance_max\",\n    )\n\n    return errors_df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_locations_for_moves","title":"<code>generate_locations_for_moves(event_timestamps_df, cartesian_crs)</code>  <code>staticmethod</code>","text":"<p>Generates locations for moves based on the event timestamps dataframe. Returns a dataframe, where for each move in the event timestamps dataframe a geometry column is added, representing the location of the move.</p> <p>Performs interpolation along the line between the starting move point (previous stay point) and next move point (next stay point) using the randomly generated values in the column random_fraction_on_line.</p> <p>Parameters:</p> Name Type Description Default <code>event_timestamps_df</code> <code>DataFrame</code> <p>The event timestamps dataframe.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: The event timestamps dataframe with the added geometry column.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>@staticmethod\ndef generate_locations_for_moves(event_timestamps_df: DataFrame, cartesian_crs: int) -&gt; DataFrame:\n    \"\"\"\n    Generates locations for moves based on the event timestamps dataframe.\n    Returns a dataframe, where for each move in the event timestamps dataframe\n    a geometry column is added, representing the location of the move.\n\n    Performs interpolation along the line between the starting move point (previous stay point)\n    and next move point (next stay point) using the randomly generated values in the column random_fraction_on_line.\n\n    Args:\n        event_timestamps_df (pyspark.sql.DataFrame): The event timestamps dataframe.\n\n    Returns:\n        pyspark.sql.DataFrame: The event timestamps dataframe with the added geometry column.\n    \"\"\"\n\n    moves_with_geometry = event_timestamps_df.withColumn(\n        \"line\",\n        STF.ST_MakeLine(F.col(ColNames.geometry), F.col(\"next_stay_geometry\")),\n    )\n    moves_with_geometry = moves_with_geometry.withColumn(\n        \"generated_geometry\",\n        STF.ST_SetSRID(\n            STF.ST_LineInterpolatePoint(F.col(\"line\"), F.col(\"random_fraction_on_line\")),\n            cartesian_crs,\n        ),\n    )\n\n    moves_with_geometry = moves_with_geometry.select(\n        ColNames.user_id,\n        ColNames.timestamp,\n        \"generated_geometry\",\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    )\n\n    return moves_with_geometry\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_nulls_in_mandatory_fields","title":"<code>generate_nulls_in_mandatory_fields(df)</code>","text":"<p>Generates null values in some fields of some rows based on configuration parameters.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>clean synthetic data</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: synthetic records dataframe with nulls in some columns of some rows</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>def generate_nulls_in_mandatory_fields(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Generates null values in some fields of some rows based on configuration parameters.\n\n    Args:\n        df (pyspark.sql.DataFrame): clean synthetic data\n\n    Returns:\n        pyspark.sql.DataFrame: synthetic records dataframe with nulls in some columns of some rows\n    \"\"\"\n\n    # Two probability parameters from config apply:\n    # First one sets how many rows (as fraction of all rows) are selected for possible value nulling.\n    # Second one sets the likelyhood for each column to be set to null.\n    # If 1.0, then all mandatory columns of each selected row will be nulled.\n\n    if self.null_row_prob == 0.0:\n        # TODO logging\n        return df\n\n    # Split input dataframe to unchanged and changed portions\n    df = df.cache()\n    error_row_prob = self.null_row_prob\n    unchanged_row_prob = 1.0 - error_row_prob\n    unchanged_rows_df, error_rows_df = df.randomSplit([unchanged_row_prob, error_row_prob], seed=self.seed + 1)\n\n    columns_for_null_selection = list(self.error_generation_allowed_columns)\n    columns_for_null_selection.sort()\n\n    random.seed(self.seed)\n    columns_to_set_as_null = random.sample(\n        columns_for_null_selection,\n        int(round(self.column_is_null_probability * len(columns_for_null_selection), 0)),\n    )\n\n    for column in columns_to_set_as_null:\n        error_rows_df = error_rows_df.withColumn(column, F.lit(None))\n\n    error_rows_df = error_rows_df.withColumn(\"is_modified\", F.lit(True))\n\n    # Re-combine unchanged and changed rows of the dataframe.\n    return unchanged_rows_df.union(error_rows_df)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_out_of_bounds_dates","title":"<code>generate_out_of_bounds_dates(df)</code>","text":"<p>Transforms the timestamp column values to be out of bound of the selected period, based on probabilities from configuration. Only rows with non-null timestamp values can become altered here.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe of clean synthetic events.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: Data where some timestamp column values are out of bounds as per config.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>def generate_out_of_bounds_dates(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms the timestamp column values to be out of bound of the selected period,\n    based on probabilities from configuration.\n    Only rows with non-null timestamp values can become altered here.\n\n    Args:\n        df (pyspark.sql.DataFrame): Dataframe of clean synthetic events.\n\n    Returns:\n        pyspark.sql.DataFrame: Data where some timestamp column values are out of bounds as per config.\n    \"\"\"\n\n    if self.out_of_bounds_prob == 0.0:\n        # TODO logging\n        return df\n\n    # Calculate approximate span in months from config parameters.\n    # The parameters should approximately cover the range of \"clean\" dates, so that the erroneous values can be generated outside the range.\n\n    # TODO\n    # This now uses the whole input data to set the bounds\n    ending_timestamp = df.agg(F.max(ColNames.timestamp)).collect()[0][0]\n    starting_timestamp = df.agg(F.min(ColNames.timestamp)).collect()[0][0]\n    events_span_in_months = max(1, (pd.Timestamp(ending_timestamp) - pd.Timestamp(starting_timestamp)).days / 30)\n\n    # Split rows by null/non-null timestamp.\n    df = df.cache()\n    null_timestamp_df = df.where(F.col(ColNames.timestamp).isNull())\n    nonnull_timestamp_df = df.where(F.col(ColNames.timestamp).isNotNull())\n    df.unpersist()\n\n    # From non-null timestamp rows, select a subset for adding errors to, depending on config parameter.\n    nonnull_timestamp_df = nonnull_timestamp_df.cache()\n    error_row_prob = self.out_of_bounds_prob\n    unchanged_row_prob = 1.0 - error_row_prob\n    unchanged_rows_df, error_rows_df = nonnull_timestamp_df.randomSplit(\n        [unchanged_row_prob, error_row_prob], seed=self.seed + 3\n    )\n    # Combine null timestamp rows and not-modified non-null timestamp rows.\n    unchanged_rows_df = unchanged_rows_df.union(null_timestamp_df)\n\n    # Add months offset to error rows to make their timestamp values become outside expected range.\n    months_to_add_col = (F.lit(2) + F.rand(seed=self.seed)) * F.lit(events_span_in_months)\n    modified_date_col = F.add_months(F.col(ColNames.timestamp), months_to_add_col)\n    time_col = F.date_format(F.col(ColNames.timestamp), \" HH:mm:ss\")\n    error_rows_df = error_rows_df.withColumn(ColNames.timestamp, F.concat(modified_date_col, time_col))\n    error_rows_df = error_rows_df.withColumn(\"is_modified\", F.lit(True))\n\n    # Combine changed and unchanged rows dataframes.\n    return unchanged_rows_df.union(error_rows_df)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_records_with_non_existant_cell_ids","title":"<code>generate_records_with_non_existant_cell_ids(records_sdf, cells_sdf, seed)</code>  <code>staticmethod</code>","text":"<p>Adds the cell_id column so that it will contain cell_ids that are not present in the cells_df dataframe, yet follow the format of a cell id.</p> <p>Parameters:</p> Name Type Description Default <code>records_sdf</code> <code>DataFrame</code> <p>generated records</p> required <code>cells_sdf</code> <code>DataFrame</code> <p>cells dataframe</p> required <code>DataFrame</code> <p>records with cell ids that are not present in the cells_df dataframe</p> required Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>@staticmethod\ndef generate_records_with_non_existant_cell_ids(records_sdf: DataFrame, cells_sdf: DataFrame, seed) -&gt; DataFrame:\n    \"\"\"\n    Adds the cell_id column so that it will contain cell_ids that\n    are not present in the cells_df dataframe, yet follow the format of a cell id.\n\n    Args:\n        records_sdf (DataFrame): generated records\n        cells_sdf (DataFrame): cells dataframe\n\n        DataFrame: records with cell ids that are not present in the cells_df dataframe\n    \"\"\"\n\n    # Generates random cell ids for cells_df, and selects those\n    # Join to records is implemented with a monotonically increasing id\n    # So to limit that, this number of all unique cells is used\n    # TODO check how to make this more optimal\n\n    n_of_actual_cell_ids_as_base = cells_sdf.count()\n\n    cells_df_with_random_cell_ids = cells_sdf[[ColNames.cell_id]].withColumn(\n        \"random_cell_id\",\n        (F.rand(seed=seed) * (999_999_999_999_999 - 10_000_000_000_000) + 10_000_000_000_000).cast(\"bigint\"),\n    )\n\n    # Do an  left anti join to ensure that now generated cell ids are not among actual cell ids\n\n    cells_df_inner_joined = cells_df_with_random_cell_ids[[\"random_cell_id\"]].join(\n        cells_sdf[[ColNames.cell_id]],\n        on=F.col(\"random_cell_id\") == F.col(ColNames.cell_id),\n        how=\"leftanti\",\n    )\n\n    cells_df_inner_joined = cells_df_inner_joined.withColumn(\"row_number\", (F.monotonically_increasing_id()))\n    records_sdf = records_sdf.withColumn(\n        \"row_number\",\n        (F.monotonically_increasing_id() % n_of_actual_cell_ids_as_base) + 1,\n    )\n\n    cells_df_inner_joined = cells_df_inner_joined.select(\n        \"row_number\", F.col(\"random_cell_id\").alias(ColNames.cell_id)\n    )\n\n    records_with_random_cell_id = records_sdf.join(cells_df_inner_joined, on=\"row_number\", how=\"left\").drop(\n        \"row_number\"\n    )\n\n    records_with_random_cell_id = records_with_random_cell_id.select(\n        ColNames.user_id,\n        ColNames.timestamp,\n        \"generated_geometry\",\n        ColNames.cell_id,\n        ColNames.loc_error,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    )\n\n    return records_with_random_cell_id\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events/#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_same_location_duplicates","title":"<code>generate_same_location_duplicates(df)</code>","text":"<p>Selects a subset of the previously generated data (syntactically clean data) and creates same location duplicates. If input has odd number of rows, one row is discarded.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe of clean synthetic events.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: Dataframe with same location duplicates.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>def generate_same_location_duplicates(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Selects a subset of the previously generated data (syntactically clean data) and creates same location duplicates.\n    If input has odd number of rows, one row is discarded.\n\n    Args:\n        df (pyspark.sql.DataFrame): Dataframe of clean synthetic events.\n\n    Returns:\n        pyspark.sql.DataFrame: Dataframe with same location duplicates.\n\n    \"\"\"\n\n    if self.same_location_duplicate_prob == 0:\n        return df\n\n    # Split dataframe by whether the row has been modified during the error-adding process already.\n    # Already errored rows do not get further changes.\n    df = df.cache()\n    previously_modified_rows_df = df.where(F.col(\"is_modified\"))\n    unmodified_rows_df = df.where(~(F.col(\"is_modified\")))\n    df.unpersist()\n\n    # From unmodified rows, select a subset for adding errors to, depending on config parameter.\n    unmodified_rows_df = unmodified_rows_df.cache()\n    error_row_prob = self.same_location_duplicate_prob\n    unchanged_row_prob = 1.0 - error_row_prob\n\n    unchanged_rows_df, error_rows_df = unmodified_rows_df.randomSplit(\n        [unchanged_row_prob, error_row_prob], seed=self.seed + 7\n    )\n\n    # Gather rows that are not modified in this step (combine previously-modified rows and not-selected unmodified rows).\n    unchanged_rows_df = unchanged_rows_df.union(previously_modified_rows_df)\n\n    # Select all even rows and duplicate these\n    error_rows_df = error_rows_df.withColumn(\n        \"user_row_num\",\n        F.row_number().over(Window.partitionBy(ColNames.user_id).orderBy(ColNames.timestamp)),\n    )\n\n    even_rows = error_rows_df.where(F.col(\"user_row_num\") % 2 == 0).drop(\"user_row_num\")\n\n    error_rows_df = even_rows.union(even_rows)\n    error_rows_df = error_rows_df.withColumn(\"is_modified\", F.lit(True))\n\n    return unchanged_rows_df.union(error_rows_df)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/","title":"synthetic_network","text":"<p>Module that generates a MNO synthetic network.</p>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.CellIDGenerator","title":"<code>CellIDGenerator</code>","text":"<p>Abstract class for cell ID generation.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>class CellIDGenerator(metaclass=ABCMeta):\n    \"\"\"\n    Abstract class for cell ID generation.\n    \"\"\"\n\n    def __init__(self, rng: Union[int, Random]) -&gt; None:\n        \"\"\"Cell ID Generator constructor\n\n        Args:\n            rng (Union[int, Random]): either an integer to act as a seed for RNG, or an instantiated Random object.\n        \"\"\"\n        if isinstance(rng, int):\n            self.rng = Random(rng)\n        elif isinstance(rng, Random):\n            self.rng = rng\n\n    @abstractmethod\n    def generate_cell_ids(self, n_cells: int) -&gt; List[str]:\n        \"\"\"Method that generates random cell IDs.\n\n        Args:\n            n_cells (int): number of cell IDs to generate.\n\n        Returns:\n            List[str]: list of cell IDs.\n        \"\"\"\n        return None\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.CellIDGenerator.__init__","title":"<code>__init__(rng)</code>","text":"<p>Cell ID Generator constructor</p> <p>Parameters:</p> Name Type Description Default <code>rng</code> <code>Union[int, Random]</code> <p>either an integer to act as a seed for RNG, or an instantiated Random object.</p> required Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>def __init__(self, rng: Union[int, Random]) -&gt; None:\n    \"\"\"Cell ID Generator constructor\n\n    Args:\n        rng (Union[int, Random]): either an integer to act as a seed for RNG, or an instantiated Random object.\n    \"\"\"\n    if isinstance(rng, int):\n        self.rng = Random(rng)\n    elif isinstance(rng, Random):\n        self.rng = rng\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.CellIDGenerator.generate_cell_ids","title":"<code>generate_cell_ids(n_cells)</code>  <code>abstractmethod</code>","text":"<p>Method that generates random cell IDs.</p> <p>Parameters:</p> Name Type Description Default <code>n_cells</code> <code>int</code> <p>number of cell IDs to generate.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: list of cell IDs.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>@abstractmethod\ndef generate_cell_ids(self, n_cells: int) -&gt; List[str]:\n    \"\"\"Method that generates random cell IDs.\n\n    Args:\n        n_cells (int): number of cell IDs to generate.\n\n    Returns:\n        List[str]: list of cell IDs.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.CellIDGeneratorBuilder","title":"<code>CellIDGeneratorBuilder</code>","text":"<p>Type/method of cell ID generation enumeration class.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>class CellIDGeneratorBuilder:\n    \"\"\"\n    Type/method of cell ID generation enumeration class.\n    \"\"\"\n\n    RANDOM_CELL_ID = \"random_cell_id\"\n\n    CONSTRUCTORS = {RANDOM_CELL_ID: RandomCellIDGenerator}\n\n    @staticmethod\n    def build(constructor_key: str, rng: Union[int, Random]) -&gt; CellIDGenerator:\n        \"\"\"\n        Method that builds a CellIDGenerator.\n\n        Args:\n            constructor_key (str): Key of the constructor\n            rng (Union[int, Random]): either an integer to act as a seed for RNG, or an instantiated Random object.\n\n        Raises:\n            ValueError: If the given constructor_key is not supported\n\n        Returns:\n            CellIDGenerator: Class that generates random cell_id's\n        \"\"\"\n        try:\n            constructor = CellIDGeneratorBuilder.CONSTRUCTORS[constructor_key]\n        except KeyError as e:\n            raise ValueError(f\"Random cell ID generator: {constructor_key} is not supported\") from e\n\n        return constructor(rng)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.CellIDGeneratorBuilder.build","title":"<code>build(constructor_key, rng)</code>  <code>staticmethod</code>","text":"<p>Method that builds a CellIDGenerator.</p> <p>Parameters:</p> Name Type Description Default <code>constructor_key</code> <code>str</code> <p>Key of the constructor</p> required <code>rng</code> <code>Union[int, Random]</code> <p>either an integer to act as a seed for RNG, or an instantiated Random object.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the given constructor_key is not supported</p> <p>Returns:</p> Name Type Description <code>CellIDGenerator</code> <code>CellIDGenerator</code> <p>Class that generates random cell_id's</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>@staticmethod\ndef build(constructor_key: str, rng: Union[int, Random]) -&gt; CellIDGenerator:\n    \"\"\"\n    Method that builds a CellIDGenerator.\n\n    Args:\n        constructor_key (str): Key of the constructor\n        rng (Union[int, Random]): either an integer to act as a seed for RNG, or an instantiated Random object.\n\n    Raises:\n        ValueError: If the given constructor_key is not supported\n\n    Returns:\n        CellIDGenerator: Class that generates random cell_id's\n    \"\"\"\n    try:\n        constructor = CellIDGeneratorBuilder.CONSTRUCTORS[constructor_key]\n    except KeyError as e:\n        raise ValueError(f\"Random cell ID generator: {constructor_key} is not supported\") from e\n\n    return constructor(rng)\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.RandomCellIDGenerator","title":"<code>RandomCellIDGenerator</code>","text":"<p>               Bases: <code>CellIDGenerator</code></p> <p>Class that generates completely random cell IDs. Inherits from the AbstractCellIDGenerator class.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>class RandomCellIDGenerator(CellIDGenerator):\n    \"\"\"\n    Class that generates completely random cell IDs. Inherits from the AbstractCellIDGenerator class.\n    \"\"\"\n\n    def generate_cell_ids(self, n_cells: int) -&gt; List[str]:\n        \"\"\"Generate UNIQUE random cell IDs with no logic behind it, i.e. not following CGI/eCGI standards.\n        The resuling cell IDs are 14- or 15-digit strings.\n\n        Args:\n            n_cells (int): number of cell IDs to generate.\n\n        Returns:\n            List[str]: list of cell IDs.\n        \"\"\"\n        return list(map(str, self.rng.sample(range(10_000_000_000_000, 999_999_999_999_999), n_cells)))\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.RandomCellIDGenerator.generate_cell_ids","title":"<code>generate_cell_ids(n_cells)</code>","text":"<p>Generate UNIQUE random cell IDs with no logic behind it, i.e. not following CGI/eCGI standards. The resuling cell IDs are 14- or 15-digit strings.</p> <p>Parameters:</p> Name Type Description Default <code>n_cells</code> <code>int</code> <p>number of cell IDs to generate.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: list of cell IDs.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>def generate_cell_ids(self, n_cells: int) -&gt; List[str]:\n    \"\"\"Generate UNIQUE random cell IDs with no logic behind it, i.e. not following CGI/eCGI standards.\n    The resuling cell IDs are 14- or 15-digit strings.\n\n    Args:\n        n_cells (int): number of cell IDs to generate.\n\n    Returns:\n        List[str]: list of cell IDs.\n    \"\"\"\n    return list(map(str, self.rng.sample(range(10_000_000_000_000, 999_999_999_999_999), n_cells)))\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.SyntheticNetwork","title":"<code>SyntheticNetwork</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class that generates the synthetic network topology data. It inherits from the Component abstract class.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>class SyntheticNetwork(Component):\n    \"\"\"\n    Class that generates the synthetic network topology data. It inherits from the Component abstract class.\n    \"\"\"\n\n    COMPONENT_ID = \"SyntheticNetwork\"\n\n    def __init__(self, general_config_path: str, component_config_path: str):\n        super().__init__(general_config_path=general_config_path, component_config_path=component_config_path)\n        self.seed = self.config.getint(self.COMPONENT_ID, \"seed\")\n        self.rng = Random(self.seed)\n        self.n_cells = self.config.getint(self.COMPONENT_ID, \"n_cells\")\n        self.cell_type_options = (\n            self.config.get(self.COMPONENT_ID, \"cell_type_options\").strip().replace(\" \", \"\").split(\",\")\n        )\n        self.tech = [\"5G\", \"LTE\", \"UMTS\", \"GSM\"]\n        self.latitude_min = self.config.getfloat(self.COMPONENT_ID, \"latitude_min\")\n        self.latitude_max = self.config.getfloat(self.COMPONENT_ID, \"latitude_max\")\n        self.longitude_min = self.config.getfloat(self.COMPONENT_ID, \"longitude_min\")\n        self.longitude_max = self.config.getfloat(self.COMPONENT_ID, \"longitude_max\")\n        self.altitude_min = self.config.getfloat(self.COMPONENT_ID, \"altitude_min\")\n        self.altitude_max = self.config.getfloat(self.COMPONENT_ID, \"altitude_max\")\n        self.antenna_height_max = self.config.getfloat(self.COMPONENT_ID, \"antenna_height_max\")\n        self.power_min = self.config.getfloat(self.COMPONENT_ID, \"power_min\")\n        self.power_max = self.config.getfloat(self.COMPONENT_ID, \"power_max\")\n        self.range_min = self.config.getfloat(self.COMPONENT_ID, \"range_min\")\n        self.range_max = self.config.getfloat(self.COMPONENT_ID, \"range_max\")\n        self.frequency_min = self.config.getfloat(self.COMPONENT_ID, \"frequency_min\")\n        self.frequency_max = self.config.getfloat(self.COMPONENT_ID, \"frequency_max\")\n\n        self.timestamp_format = self.config.get(self.COMPONENT_ID, \"timestamp_format\")\n        self.earliest_valid_date_start = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"earliest_valid_date_start\"), self.timestamp_format\n        )\n        self.latest_valid_date_end = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"latest_valid_date_end\"), self.timestamp_format\n        )\n\n        self.date_format = self.config.get(self.COMPONENT_ID, \"date_format\")\n\n        self.starting_date = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"starting_date\"), self.date_format\n        ).date()\n        self.ending_date = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"ending_date\"), self.date_format\n        ).date()\n\n        self.date_range = [\n            (self.starting_date + datetime.timedelta(days=dd))\n            for dd in range((self.ending_date - self.starting_date).days + 1)\n        ]\n\n        # Cell generation object\n        cell_id_generation_type = self.config.get(self.COMPONENT_ID, \"cell_id_generation_type\")\n\n        self.cell_id_generator = CellIDGeneratorBuilder.build(cell_id_generation_type, self.seed)\n\n        self.no_optional_fields_probability = self.config.getfloat(self.COMPONENT_ID, \"no_optional_fields_probability\")\n        self.mandatory_null_probability = self.config.getfloat(self.COMPONENT_ID, \"mandatory_null_probability\")\n        self.out_of_bounds_values_probability = self.config.getfloat(\n            self.COMPONENT_ID, \"out_of_bounds_values_probability\"\n        )\n        self.erroneous_values_probability = self.config.getfloat(self.COMPONENT_ID, \"erroneous_values_probability\")\n\n    def initalize_data_objects(self):\n        output_network_data_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"network_data_bronze\")\n        bronze_network = BronzeNetworkDataObject(self.spark, output_network_data_path)\n        self.output_data_objects = {bronze_network.ID: bronze_network}\n\n    def read(self):\n        pass  # No input datasets are used in this component\n\n    def transform(self):\n        spark = self.spark\n\n        # Create Spark DataFrame with all valid cells and all optional fields\n\n        cells_df = spark.createDataFrame(self.clean_cells_generator(), schema=BronzeNetworkDataObject.SCHEMA)\n\n        # With certain probability, set ALL optional fields of a row to null.\n        # Fixing F.rand(seed=self.seed) will generate the same random column for every column, so\n        # it could be optimized to be generated only once\n        # If random optional fields should be set to zero (not all at the same time), use seed = self.seed + i(col_name)\n\n        for col_name in BronzeNetworkDataObject.OPTIONAL_COLUMNS:\n            cells_df = cells_df.withColumn(\n                col_name,\n                F.when(F.rand(seed=self.seed) &lt; self.no_optional_fields_probability, None).otherwise(F.col(col_name)),\n            )\n\n        cells_df = self.generate_errors(cells_df)\n\n        self.output_data_objects[BronzeNetworkDataObject.ID].df = cells_df\n\n    def clean_cells_generator(self):\n        \"\"\"Method that generates valid and fully complete (mandatory and optional) cell physical attibutes.\n\n        An underlying set of cells are created, covering the config-specified date interval.\n        Then, for each cell and date,\n        the valid_date_start and valid_date_end fields, marking the time interval in which the cell is operational, are\n        comparted with the date:\n            a) If  date &lt; valid_date_start, the cell-date row will not appear.\n            b) If valid_date_start &lt;= date &lt; valid_date_end, the cell-date row will appear and\n                the valid_date_end will be null, as the cell was currently operational\n            c) If valid_date_end &lt;= date, the cell-date row will appear and\n                the valid_date_end will NOT be null, marking the past, now known, time interval of operation.\n\n        Yields:\n            (\n                cell_id (str),\n                latitude (float),\n                longitude (float),\n                altitudes (float),\n                antenna_height (float),\n                directionality (int),\n                azimuth_angle (float | None),\n                elevation_angle (float),\n                hor_beam_width (float),\n                ver_beam_width (float),\n                power (float),\n                range (float),\n                frequency (int),\n                technology (str),\n                valid_date_start (str),\n                valid_date_end (str | None)\n                cell_type (str),\n                year (int),\n                month (int),\n                day (int)\n            )\n        \"\"\"\n        # MANDATORY FIELDS\n        cell_ids = self.cell_id_generator.generate_cell_ids(self.n_cells)\n\n        latitudes = [self.rng.uniform(self.latitude_min, self.latitude_max) for _ in range(self.n_cells)]\n        longitudes = [self.rng.uniform(self.longitude_min, self.longitude_max) for _ in range(self.n_cells)]\n\n        # OPTIONAL FIELDS\n        altitudes = [self.rng.uniform(self.altitude_min, self.altitude_max) for _ in range(self.n_cells)]\n\n        # antenna height always positive\n        antenna_heights = [self.rng.uniform(0, self.antenna_height_max) for _ in range(self.n_cells)]\n\n        # Directionality: 0 or 1\n        directionalities = [self.rng.choice([0, 1]) for _ in range(self.n_cells)]\n\n        def random_azimuth_angle(directionality):\n            if directionality == 0:\n                return None\n            else:\n                return self.rng.uniform(0, 360)\n\n        # Azimuth angle: None if directionality is 0, float in [0, 360] if directionality is 1\n        azimuth_angles = [random_azimuth_angle(direc) for direc in directionalities]\n\n        # Eleveation angle: in [-90, 90]\n        elevation_angles = [self.rng.uniform(-90, 90) for _ in range(self.n_cells)]\n\n        # Horizontal/Vertical beam width: float in [0, 360]\n        hor_beam_widths = [self.rng.uniform(0, 360) for _ in range(self.n_cells)]\n        ver_beam_widths = [self.rng.uniform(0, 360) for _ in range(self.n_cells)]\n\n        # Power, float in specified range (unit: watts, W)\n        powers = [self.rng.uniform(self.power_min, self.power_max) for _ in range(self.n_cells)]\n\n        # Range, float in specified range (unit: metres, m)\n        ranges = [self.rng.uniform(self.range_min, self.range_max) for _ in range(self.n_cells)]\n\n        # Frequency: int in specifed range (unit: MHz)\n        frequencies = [self.rng.randint(self.frequency_min, self.frequency_max) for _ in range(self.n_cells)]\n\n        # Technology: str\n        technologies = [self.rng.choice(self.tech) for _ in range(self.n_cells)]\n\n        # # Valid start date, should be in the timestamp interval provided via config file\n        # span_seconds = (self.latest_valid_date_end - self.earliest_valid_date_start).total_seconds()\n\n        # # Start date will be some random nb of seconds after the earliest valid date start\n        # valid_date_start_dts = [\n        #     self.earliest_valid_date_start + datetime.timedelta(seconds=self.rng.uniform(0, span_seconds))\n        #     for _ in range(self.n_cells)\n        # ]\n\n        # # Remaining seconds from the valid date starts to the ending date\n        # remaining_seconds = [\n        #     (self.latest_valid_date_end - start_dt).total_seconds() for start_dt in valid_date_start_dts\n        # ]\n        # # Choose valid date ends ALWAYS after the valid date start\n        # # Minimum of two seconds, as valid date end is excluded from the time window,\n        # # so cell will be valid for at least 1 second\n        # valid_date_end_dts = [\n        #     (start_dt + datetime.timedelta(seconds=self.rng.uniform(2, rem_secs)))\n        #     for (start_dt, rem_secs) in zip(valid_date_start_dts, remaining_seconds)\n        # ]\n\n        valid_date_start_dts = [self.earliest_valid_date_start for _ in range(self.n_cells)]\n        valid_date_end_dts = [self.latest_valid_date_end for _ in range(self.n_cells)]\n\n        def check_operational(_curr_date: datetime.date, _end_datetime: datetime.datetime):\n            if _curr_date &lt; _end_datetime.date():  # still operational, return None\n                return None\n            else:\n                return _end_datetime.strftime(self.timestamp_format)\n\n        # Cell type: str in option list\n        cell_types = [self.rng.choice(self.cell_type_options) for _ in range(self.n_cells)]\n\n        for date in self.date_range:\n            for i in range(self.n_cells):\n                # Assume we do not have info about future cells\n                if valid_date_start_dts[i].date() &lt;= date:\n                    yield (\n                        cell_ids[i],\n                        latitudes[i],\n                        longitudes[i],\n                        altitudes[i],\n                        antenna_heights[i],\n                        directionalities[i],\n                        azimuth_angles[i],\n                        elevation_angles[i],\n                        hor_beam_widths[i],\n                        ver_beam_widths[i],\n                        powers[i],\n                        ranges[i],\n                        frequencies[i],\n                        technologies[i],\n                        valid_date_start_dts[i].strftime(self.timestamp_format),\n                        check_operational(date, valid_date_end_dts[i]),  # null if still operational today\n                        cell_types[i],\n                        date.year,\n                        date.month,\n                        date.day,\n                    )\n\n    def generate_errors(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Function handling the generation of out-of-bounds, null and erroneous values in different columns\n        according to the config-specified probabilities\n\n        Args:\n            df (DataFrame): clean DataFrame\n\n        Returns:\n            DataFrame: DataFrame after the generation of different invalid or null values\n        \"\"\"\n\n        if self.out_of_bounds_values_probability &gt; 0:\n            df = self.generate_out_of_bounds_values(df)\n\n        if self.mandatory_null_probability &gt; 0:\n            df = self.generate_nulls_in_mandatory_columns(df)\n\n        if self.erroneous_values_probability &gt; 0:\n            df = self.generate_erroneous_values(df)\n\n        return df\n\n    def generate_nulls_in_mandatory_columns(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Generates null values in the mandatory fields based on probabilities form config\n\n        Args:\n            df (DataFrame): synthetic dataframe\n\n        Returns:\n            DataFrame: synthetic dataframe with nulls in some mandatory fields\n        \"\"\"\n        # Use different seed for each column\n        for i, col_name in enumerate(BronzeNetworkDataObject.MANDATORY_COLUMNS):\n            df = df.withColumn(\n                col_name,\n                F.when(F.rand(seed=self.seed + i + 1) &lt; self.mandatory_null_probability, None).otherwise(\n                    F.col(col_name)\n                ),\n            )\n\n        return df\n\n    def generate_out_of_bounds_values(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Function that generates out-of-bounds values for the appropriate columns of the data object\n\n        Args:\n            df (DataFrame): cell dataframe with in-bound values\n\n        Returns:\n            DataFrame: cell dataframe with some out-of-bounds values\n        \"\"\"\n        # (2 * F.round(F.rand(seed=self.seed-1)) - 1) -&gt; random column of 1s and -1s\n        df = df.withColumn(\n            ColNames.latitude,\n            F.when(\n                F.rand(seed=self.seed - 1) &lt; self.out_of_bounds_values_probability,\n                F.col(ColNames.latitude) + (2 * F.round(F.rand(seed=self.seed - 1)) - 1) * 90,\n            )\n            .otherwise(F.col(ColNames.latitude))\n            .cast(FloatType()),  # the operation turns it into DoubleType, so cast it to Float\n        )\n\n        df = df.withColumn(\n            ColNames.longitude,\n            F.when(\n                F.rand(seed=self.seed - 2) &lt; self.out_of_bounds_values_probability,\n                F.col(ColNames.longitude) + (2 * F.round(F.rand(seed=self.seed - 2)) - 1) * 180,\n            )\n            .otherwise(F.col(ColNames.longitude))\n            .cast(FloatType()),\n        )\n\n        # antenna height, non positive\n        df = df.withColumn(\n            ColNames.antenna_height,\n            F.when(F.rand(seed=self.seed - 3) &lt; self.out_of_bounds_values_probability, -F.col(ColNames.antenna_height))\n            .otherwise(F.col(ColNames.antenna_height))\n            .cast(FloatType()),\n        )\n\n        # directionality: int different from 0 or 1. Just add a static 5 to the value\n        df = df.withColumn(\n            ColNames.directionality,\n            F.when(\n                F.rand(seed=self.seed - 4) &lt; self.out_of_bounds_values_probability, F.col(ColNames.directionality) + 5\n            )\n            .otherwise(F.col(ColNames.directionality))\n            .cast(IntegerType()),\n        )\n\n        # azimuth angle: outside of [0, 360]\n        df = df.withColumn(\n            ColNames.azimuth_angle,\n            F.when(\n                F.rand(seed=self.seed - 5) &lt; self.out_of_bounds_values_probability,\n                F.col(ColNames.azimuth_angle) + (2 * F.round(F.rand(seed=self.seed - 5)) - 1) * 360,\n            )\n            .otherwise(F.col(ColNames.azimuth_angle))\n            .cast(FloatType()),\n        )\n\n        # elevation_angle: outside of [-90, 90]\n        df = df.withColumn(\n            ColNames.elevation_angle,\n            F.when(\n                F.rand(seed=self.seed - 6) &lt; self.out_of_bounds_values_probability,\n                F.col(ColNames.elevation_angle) + (2 * F.round(F.rand(seed=self.seed - 6)) - 1) * 180,\n            )\n            .otherwise(F.col(ColNames.elevation_angle))\n            .cast(FloatType()),\n        )\n\n        # horizontal_beam_width: outside of [0, 360]\n        df = df.withColumn(\n            ColNames.horizontal_beam_width,\n            F.when(\n                F.rand(seed=self.seed - 7) &lt; self.out_of_bounds_values_probability,\n                F.col(ColNames.horizontal_beam_width) + (2 * F.round(F.rand(seed=self.seed - 7)) - 1) * 360,\n            )\n            .otherwise(F.col(ColNames.horizontal_beam_width))\n            .cast(FloatType()),\n        )\n\n        # vertical_beam_width: outside of [0, 360]\n        df = df.withColumn(\n            ColNames.vertical_beam_width,\n            F.when(\n                F.rand(seed=self.seed - 8) &lt; self.out_of_bounds_values_probability,\n                F.col(ColNames.vertical_beam_width) + (2 * F.round(F.rand(seed=self.seed - 8)) - 1) * 360,\n            )\n            .otherwise(F.col(ColNames.vertical_beam_width))\n            .cast(FloatType()),\n        )\n\n        # power: non positive value\n        df = df.withColumn(\n            ColNames.power,\n            F.when(F.rand(seed=self.seed - 9) &lt; self.out_of_bounds_values_probability, -F.col(ColNames.power))\n            .otherwise(F.col(ColNames.power))\n            .cast(FloatType()),\n        )\n\n        # range: non positive value\n        df = df.withColumn(\n            ColNames.range,\n            F.when(F.rand(seed=self.seed - 10) &lt; self.out_of_bounds_values_probability, -F.col(ColNames.range))\n            .otherwise(F.col(ColNames.range))\n            .cast(FloatType()),\n        )\n\n        # frequency: non positive vallue\n        df = df.withColumn(\n            ColNames.frequency,\n            F.when(F.rand(seed=self.seed - 11) &lt; self.out_of_bounds_values_probability, -F.col(ColNames.frequency))\n            .otherwise(F.col(ColNames.frequency))\n            .cast(IntegerType()),\n        )\n        return df\n\n    def generate_erroneous_values(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"Function that generates erroneous values in the cell_id, valid_date_start and valid_date_end columns\n\n        Args:\n            df (DataFrame): DataFrame before the generation of erroneous values\n\n        Returns:\n            DataFrame: DataFrame with erroneous values\n        \"\"\"\n        # Erroneous cells: for now, a string not of 14 or 15 digits\n        df = df.withColumn(\n            ColNames.cell_id,\n            F.when(\n                F.rand(seed=self.seed * 1000) &lt; self.erroneous_values_probability,\n                (\n                    F.when(\n                        F.rand(seed=self.seed * 2000) &gt; 0.5,\n                        F.concat(\n                            F.col(ColNames.cell_id), F.substring(F.col(ColNames.cell_id), 3, 3)\n                        ),  # 17 or 18 digits\n                    ).otherwise(\n                        F.substring(F.col(ColNames.cell_id), 3, 10)  # 10 digits\n                    )\n                ),\n            )\n            .otherwise(F.col(ColNames.cell_id))\n            .cast(LongType())\n            .cast(StringType()),\n        )\n\n        # Dates\n        df_as_is, df_swap, df_wrong = df.randomSplit(\n            weights=[\n                1 - self.erroneous_values_probability,\n                self.erroneous_values_probability / 2,\n                self.erroneous_values_probability / 2,\n            ],\n            seed=self.seed,\n        )\n        # For some columns, swap valid_date_start and valid_date_end\n        df_swap = df_swap.withColumns(\n            {ColNames.valid_date_start: ColNames.valid_date_end, ColNames.valid_date_end: ColNames.valid_date_start}\n        )\n\n        chars_to_change = [\"0\", \"T\", \":\", \"/\", \"2\"]\n        changed_chars = [\"0\", \"T\", \":\", \"/\", \"2\"]\n        # Now, for dates as well, make the timestamp format incorrect\n\n        self.rng.shuffle(changed_chars)\n        df_wrong = df_wrong.withColumn(\n            ColNames.valid_date_start,\n            F.translate(F.col(ColNames.valid_date_start), \"\".join(chars_to_change), \"\".join(changed_chars)),\n        )\n\n        self.rng.shuffle(changed_chars)\n        df_wrong = df_wrong.withColumn(\n            ColNames.valid_date_end,\n            F.translate(F.col(ColNames.valid_date_end), \"\".join(chars_to_change), \"\".join(changed_chars)),\n        )\n\n        df = df_as_is.union(df_swap).union(df_wrong)\n\n        return df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.SyntheticNetwork.clean_cells_generator","title":"<code>clean_cells_generator()</code>","text":"<p>Method that generates valid and fully complete (mandatory and optional) cell physical attibutes.</p> <p>An underlying set of cells are created, covering the config-specified date interval. Then, for each cell and date, the valid_date_start and valid_date_end fields, marking the time interval in which the cell is operational, are comparted with the date:     a) If  date &lt; valid_date_start, the cell-date row will not appear.     b) If valid_date_start &lt;= date &lt; valid_date_end, the cell-date row will appear and         the valid_date_end will be null, as the cell was currently operational     c) If valid_date_end &lt;= date, the cell-date row will appear and         the valid_date_end will NOT be null, marking the past, now known, time interval of operation.</p> <p>Yields:</p> Type Description <p>( cell_id (str), latitude (float), longitude (float), altitudes (float), antenna_height (float), directionality (int), azimuth_angle (float | None), elevation_angle (float), hor_beam_width (float), ver_beam_width (float), power (float), range (float), frequency (int), technology (str), valid_date_start (str), valid_date_end (str | None) cell_type (str), year (int), month (int), day (int)</p> <p>)</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>def clean_cells_generator(self):\n    \"\"\"Method that generates valid and fully complete (mandatory and optional) cell physical attibutes.\n\n    An underlying set of cells are created, covering the config-specified date interval.\n    Then, for each cell and date,\n    the valid_date_start and valid_date_end fields, marking the time interval in which the cell is operational, are\n    comparted with the date:\n        a) If  date &lt; valid_date_start, the cell-date row will not appear.\n        b) If valid_date_start &lt;= date &lt; valid_date_end, the cell-date row will appear and\n            the valid_date_end will be null, as the cell was currently operational\n        c) If valid_date_end &lt;= date, the cell-date row will appear and\n            the valid_date_end will NOT be null, marking the past, now known, time interval of operation.\n\n    Yields:\n        (\n            cell_id (str),\n            latitude (float),\n            longitude (float),\n            altitudes (float),\n            antenna_height (float),\n            directionality (int),\n            azimuth_angle (float | None),\n            elevation_angle (float),\n            hor_beam_width (float),\n            ver_beam_width (float),\n            power (float),\n            range (float),\n            frequency (int),\n            technology (str),\n            valid_date_start (str),\n            valid_date_end (str | None)\n            cell_type (str),\n            year (int),\n            month (int),\n            day (int)\n        )\n    \"\"\"\n    # MANDATORY FIELDS\n    cell_ids = self.cell_id_generator.generate_cell_ids(self.n_cells)\n\n    latitudes = [self.rng.uniform(self.latitude_min, self.latitude_max) for _ in range(self.n_cells)]\n    longitudes = [self.rng.uniform(self.longitude_min, self.longitude_max) for _ in range(self.n_cells)]\n\n    # OPTIONAL FIELDS\n    altitudes = [self.rng.uniform(self.altitude_min, self.altitude_max) for _ in range(self.n_cells)]\n\n    # antenna height always positive\n    antenna_heights = [self.rng.uniform(0, self.antenna_height_max) for _ in range(self.n_cells)]\n\n    # Directionality: 0 or 1\n    directionalities = [self.rng.choice([0, 1]) for _ in range(self.n_cells)]\n\n    def random_azimuth_angle(directionality):\n        if directionality == 0:\n            return None\n        else:\n            return self.rng.uniform(0, 360)\n\n    # Azimuth angle: None if directionality is 0, float in [0, 360] if directionality is 1\n    azimuth_angles = [random_azimuth_angle(direc) for direc in directionalities]\n\n    # Eleveation angle: in [-90, 90]\n    elevation_angles = [self.rng.uniform(-90, 90) for _ in range(self.n_cells)]\n\n    # Horizontal/Vertical beam width: float in [0, 360]\n    hor_beam_widths = [self.rng.uniform(0, 360) for _ in range(self.n_cells)]\n    ver_beam_widths = [self.rng.uniform(0, 360) for _ in range(self.n_cells)]\n\n    # Power, float in specified range (unit: watts, W)\n    powers = [self.rng.uniform(self.power_min, self.power_max) for _ in range(self.n_cells)]\n\n    # Range, float in specified range (unit: metres, m)\n    ranges = [self.rng.uniform(self.range_min, self.range_max) for _ in range(self.n_cells)]\n\n    # Frequency: int in specifed range (unit: MHz)\n    frequencies = [self.rng.randint(self.frequency_min, self.frequency_max) for _ in range(self.n_cells)]\n\n    # Technology: str\n    technologies = [self.rng.choice(self.tech) for _ in range(self.n_cells)]\n\n    # # Valid start date, should be in the timestamp interval provided via config file\n    # span_seconds = (self.latest_valid_date_end - self.earliest_valid_date_start).total_seconds()\n\n    # # Start date will be some random nb of seconds after the earliest valid date start\n    # valid_date_start_dts = [\n    #     self.earliest_valid_date_start + datetime.timedelta(seconds=self.rng.uniform(0, span_seconds))\n    #     for _ in range(self.n_cells)\n    # ]\n\n    # # Remaining seconds from the valid date starts to the ending date\n    # remaining_seconds = [\n    #     (self.latest_valid_date_end - start_dt).total_seconds() for start_dt in valid_date_start_dts\n    # ]\n    # # Choose valid date ends ALWAYS after the valid date start\n    # # Minimum of two seconds, as valid date end is excluded from the time window,\n    # # so cell will be valid for at least 1 second\n    # valid_date_end_dts = [\n    #     (start_dt + datetime.timedelta(seconds=self.rng.uniform(2, rem_secs)))\n    #     for (start_dt, rem_secs) in zip(valid_date_start_dts, remaining_seconds)\n    # ]\n\n    valid_date_start_dts = [self.earliest_valid_date_start for _ in range(self.n_cells)]\n    valid_date_end_dts = [self.latest_valid_date_end for _ in range(self.n_cells)]\n\n    def check_operational(_curr_date: datetime.date, _end_datetime: datetime.datetime):\n        if _curr_date &lt; _end_datetime.date():  # still operational, return None\n            return None\n        else:\n            return _end_datetime.strftime(self.timestamp_format)\n\n    # Cell type: str in option list\n    cell_types = [self.rng.choice(self.cell_type_options) for _ in range(self.n_cells)]\n\n    for date in self.date_range:\n        for i in range(self.n_cells):\n            # Assume we do not have info about future cells\n            if valid_date_start_dts[i].date() &lt;= date:\n                yield (\n                    cell_ids[i],\n                    latitudes[i],\n                    longitudes[i],\n                    altitudes[i],\n                    antenna_heights[i],\n                    directionalities[i],\n                    azimuth_angles[i],\n                    elevation_angles[i],\n                    hor_beam_widths[i],\n                    ver_beam_widths[i],\n                    powers[i],\n                    ranges[i],\n                    frequencies[i],\n                    technologies[i],\n                    valid_date_start_dts[i].strftime(self.timestamp_format),\n                    check_operational(date, valid_date_end_dts[i]),  # null if still operational today\n                    cell_types[i],\n                    date.year,\n                    date.month,\n                    date.day,\n                )\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.SyntheticNetwork.generate_erroneous_values","title":"<code>generate_erroneous_values(df)</code>","text":"<p>Function that generates erroneous values in the cell_id, valid_date_start and valid_date_end columns</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame before the generation of erroneous values</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with erroneous values</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>def generate_erroneous_values(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Function that generates erroneous values in the cell_id, valid_date_start and valid_date_end columns\n\n    Args:\n        df (DataFrame): DataFrame before the generation of erroneous values\n\n    Returns:\n        DataFrame: DataFrame with erroneous values\n    \"\"\"\n    # Erroneous cells: for now, a string not of 14 or 15 digits\n    df = df.withColumn(\n        ColNames.cell_id,\n        F.when(\n            F.rand(seed=self.seed * 1000) &lt; self.erroneous_values_probability,\n            (\n                F.when(\n                    F.rand(seed=self.seed * 2000) &gt; 0.5,\n                    F.concat(\n                        F.col(ColNames.cell_id), F.substring(F.col(ColNames.cell_id), 3, 3)\n                    ),  # 17 or 18 digits\n                ).otherwise(\n                    F.substring(F.col(ColNames.cell_id), 3, 10)  # 10 digits\n                )\n            ),\n        )\n        .otherwise(F.col(ColNames.cell_id))\n        .cast(LongType())\n        .cast(StringType()),\n    )\n\n    # Dates\n    df_as_is, df_swap, df_wrong = df.randomSplit(\n        weights=[\n            1 - self.erroneous_values_probability,\n            self.erroneous_values_probability / 2,\n            self.erroneous_values_probability / 2,\n        ],\n        seed=self.seed,\n    )\n    # For some columns, swap valid_date_start and valid_date_end\n    df_swap = df_swap.withColumns(\n        {ColNames.valid_date_start: ColNames.valid_date_end, ColNames.valid_date_end: ColNames.valid_date_start}\n    )\n\n    chars_to_change = [\"0\", \"T\", \":\", \"/\", \"2\"]\n    changed_chars = [\"0\", \"T\", \":\", \"/\", \"2\"]\n    # Now, for dates as well, make the timestamp format incorrect\n\n    self.rng.shuffle(changed_chars)\n    df_wrong = df_wrong.withColumn(\n        ColNames.valid_date_start,\n        F.translate(F.col(ColNames.valid_date_start), \"\".join(chars_to_change), \"\".join(changed_chars)),\n    )\n\n    self.rng.shuffle(changed_chars)\n    df_wrong = df_wrong.withColumn(\n        ColNames.valid_date_end,\n        F.translate(F.col(ColNames.valid_date_end), \"\".join(chars_to_change), \"\".join(changed_chars)),\n    )\n\n    df = df_as_is.union(df_swap).union(df_wrong)\n\n    return df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.SyntheticNetwork.generate_errors","title":"<code>generate_errors(df)</code>","text":"<p>Function handling the generation of out-of-bounds, null and erroneous values in different columns according to the config-specified probabilities</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>clean DataFrame</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame after the generation of different invalid or null values</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>def generate_errors(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Function handling the generation of out-of-bounds, null and erroneous values in different columns\n    according to the config-specified probabilities\n\n    Args:\n        df (DataFrame): clean DataFrame\n\n    Returns:\n        DataFrame: DataFrame after the generation of different invalid or null values\n    \"\"\"\n\n    if self.out_of_bounds_values_probability &gt; 0:\n        df = self.generate_out_of_bounds_values(df)\n\n    if self.mandatory_null_probability &gt; 0:\n        df = self.generate_nulls_in_mandatory_columns(df)\n\n    if self.erroneous_values_probability &gt; 0:\n        df = self.generate_erroneous_values(df)\n\n    return df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.SyntheticNetwork.generate_nulls_in_mandatory_columns","title":"<code>generate_nulls_in_mandatory_columns(df)</code>","text":"<p>Generates null values in the mandatory fields based on probabilities form config</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>synthetic dataframe</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>synthetic dataframe with nulls in some mandatory fields</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>def generate_nulls_in_mandatory_columns(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Generates null values in the mandatory fields based on probabilities form config\n\n    Args:\n        df (DataFrame): synthetic dataframe\n\n    Returns:\n        DataFrame: synthetic dataframe with nulls in some mandatory fields\n    \"\"\"\n    # Use different seed for each column\n    for i, col_name in enumerate(BronzeNetworkDataObject.MANDATORY_COLUMNS):\n        df = df.withColumn(\n            col_name,\n            F.when(F.rand(seed=self.seed + i + 1) &lt; self.mandatory_null_probability, None).otherwise(\n                F.col(col_name)\n            ),\n        )\n\n    return df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_network/#components.ingestion.synthetic.synthetic_network.SyntheticNetwork.generate_out_of_bounds_values","title":"<code>generate_out_of_bounds_values(df)</code>","text":"<p>Function that generates out-of-bounds values for the appropriate columns of the data object</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>cell dataframe with in-bound values</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>cell dataframe with some out-of-bounds values</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_network.py</code> <pre><code>def generate_out_of_bounds_values(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"Function that generates out-of-bounds values for the appropriate columns of the data object\n\n    Args:\n        df (DataFrame): cell dataframe with in-bound values\n\n    Returns:\n        DataFrame: cell dataframe with some out-of-bounds values\n    \"\"\"\n    # (2 * F.round(F.rand(seed=self.seed-1)) - 1) -&gt; random column of 1s and -1s\n    df = df.withColumn(\n        ColNames.latitude,\n        F.when(\n            F.rand(seed=self.seed - 1) &lt; self.out_of_bounds_values_probability,\n            F.col(ColNames.latitude) + (2 * F.round(F.rand(seed=self.seed - 1)) - 1) * 90,\n        )\n        .otherwise(F.col(ColNames.latitude))\n        .cast(FloatType()),  # the operation turns it into DoubleType, so cast it to Float\n    )\n\n    df = df.withColumn(\n        ColNames.longitude,\n        F.when(\n            F.rand(seed=self.seed - 2) &lt; self.out_of_bounds_values_probability,\n            F.col(ColNames.longitude) + (2 * F.round(F.rand(seed=self.seed - 2)) - 1) * 180,\n        )\n        .otherwise(F.col(ColNames.longitude))\n        .cast(FloatType()),\n    )\n\n    # antenna height, non positive\n    df = df.withColumn(\n        ColNames.antenna_height,\n        F.when(F.rand(seed=self.seed - 3) &lt; self.out_of_bounds_values_probability, -F.col(ColNames.antenna_height))\n        .otherwise(F.col(ColNames.antenna_height))\n        .cast(FloatType()),\n    )\n\n    # directionality: int different from 0 or 1. Just add a static 5 to the value\n    df = df.withColumn(\n        ColNames.directionality,\n        F.when(\n            F.rand(seed=self.seed - 4) &lt; self.out_of_bounds_values_probability, F.col(ColNames.directionality) + 5\n        )\n        .otherwise(F.col(ColNames.directionality))\n        .cast(IntegerType()),\n    )\n\n    # azimuth angle: outside of [0, 360]\n    df = df.withColumn(\n        ColNames.azimuth_angle,\n        F.when(\n            F.rand(seed=self.seed - 5) &lt; self.out_of_bounds_values_probability,\n            F.col(ColNames.azimuth_angle) + (2 * F.round(F.rand(seed=self.seed - 5)) - 1) * 360,\n        )\n        .otherwise(F.col(ColNames.azimuth_angle))\n        .cast(FloatType()),\n    )\n\n    # elevation_angle: outside of [-90, 90]\n    df = df.withColumn(\n        ColNames.elevation_angle,\n        F.when(\n            F.rand(seed=self.seed - 6) &lt; self.out_of_bounds_values_probability,\n            F.col(ColNames.elevation_angle) + (2 * F.round(F.rand(seed=self.seed - 6)) - 1) * 180,\n        )\n        .otherwise(F.col(ColNames.elevation_angle))\n        .cast(FloatType()),\n    )\n\n    # horizontal_beam_width: outside of [0, 360]\n    df = df.withColumn(\n        ColNames.horizontal_beam_width,\n        F.when(\n            F.rand(seed=self.seed - 7) &lt; self.out_of_bounds_values_probability,\n            F.col(ColNames.horizontal_beam_width) + (2 * F.round(F.rand(seed=self.seed - 7)) - 1) * 360,\n        )\n        .otherwise(F.col(ColNames.horizontal_beam_width))\n        .cast(FloatType()),\n    )\n\n    # vertical_beam_width: outside of [0, 360]\n    df = df.withColumn(\n        ColNames.vertical_beam_width,\n        F.when(\n            F.rand(seed=self.seed - 8) &lt; self.out_of_bounds_values_probability,\n            F.col(ColNames.vertical_beam_width) + (2 * F.round(F.rand(seed=self.seed - 8)) - 1) * 360,\n        )\n        .otherwise(F.col(ColNames.vertical_beam_width))\n        .cast(FloatType()),\n    )\n\n    # power: non positive value\n    df = df.withColumn(\n        ColNames.power,\n        F.when(F.rand(seed=self.seed - 9) &lt; self.out_of_bounds_values_probability, -F.col(ColNames.power))\n        .otherwise(F.col(ColNames.power))\n        .cast(FloatType()),\n    )\n\n    # range: non positive value\n    df = df.withColumn(\n        ColNames.range,\n        F.when(F.rand(seed=self.seed - 10) &lt; self.out_of_bounds_values_probability, -F.col(ColNames.range))\n        .otherwise(F.col(ColNames.range))\n        .cast(FloatType()),\n    )\n\n    # frequency: non positive vallue\n    df = df.withColumn(\n        ColNames.frequency,\n        F.when(F.rand(seed=self.seed - 11) &lt; self.out_of_bounds_values_probability, -F.col(ColNames.frequency))\n        .otherwise(F.col(ColNames.frequency))\n        .cast(IntegerType()),\n    )\n    return df\n</code></pre>"},{"location":"reference/components/quality/","title":"quality","text":""},{"location":"reference/components/quality/event_quality_warnings/","title":"event_quality_warnings","text":""},{"location":"reference/components/quality/event_quality_warnings/event_quality_warnings/","title":"event_quality_warnings","text":""},{"location":"reference/components/quality/event_quality_warnings/event_quality_warnings/#components.quality.event_quality_warnings.event_quality_warnings.EventQualityWarnings","title":"<code>EventQualityWarnings</code>","text":"<p>               Bases: <code>Component</code></p> <p>Component that calculates Quality Warnings from the Quality Metrics generated by the Event Cleaning component</p> Source code in <code>multimno/components/quality/event_quality_warnings/event_quality_warnings.py</code> <pre><code>class EventQualityWarnings(Component):\n    \"\"\"\n    Component that calculates Quality Warnings from the Quality Metrics generated by the Event Cleaning component\n    \"\"\"\n\n    COMPONENT_ID = \"EventQualityWarnings\"\n\n    dict_convert_to_num_days = {\"week\": 7, \"month\": 30}\n    # dict to store info regarding error type\n    # first element - corresponding encoding of ErrorTypes class\n    # second element - naming constants for coresponding measure definitions, conditions, and warning texts\n    dict_error_type_info = {\n        \"missing_value\": [ErrorTypes.NULL_VALUE, \"Missing value rate\"],\n        \"not_right_syntactic_format\": [\n            ErrorTypes.CANNOT_PARSE,\n            \"Wrong type/format rate\",\n        ],\n        \"out_of_admissible_values\": [\n            ErrorTypes.OUT_OF_RANGE,\n            \"Out of range rate\",\n        ],\n        \"no_location\": [ErrorTypes.NO_LOCATION_INFO, \"No location error rate\"],\n        \"no_domain\": [ErrorTypes.NO_MNO_INFO, \"No domain error rate\"],\n        \"out_of_bounding_box\": [\n            ErrorTypes.OUT_OF_RANGE,\n            \"Out of bounding box error rate\",\n        ],\n        \"same_location_duplicate\": [\n            ErrorTypes.DUPLICATED,\n            \"Deduplication same locations rate\",\n        ],\n    }\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        print(self.config)\n        self.lookback_period = self.config.get(EventQualityWarnings.COMPONENT_ID, \"lookback_period\")\n        self.lookback_period_in_days = self.dict_convert_to_num_days[self.lookback_period]\n\n        self.data_period_start = self.config.get(EventQualityWarnings.COMPONENT_ID, \"data_period_start\")\n        self.data_period_end = self.config.get(EventQualityWarnings.COMPONENT_ID, \"data_period_end\")\n\n        self.qw_dfs_log = []\n        self.qw_dfs_plots = []\n\n        # FOR SYNTACTIC QUALITY WARNINGS\n        self.do_size_raw_data_qw = self.config.getboolean(\n            EventQualityWarnings.COMPONENT_ID, \"do_size_raw_data_qw\", fallback=False\n        )\n        self.do_size_clean_data_qw = self.config.getboolean(\n            EventQualityWarnings.COMPONENT_ID, \"do_size_clean_data_qw\", fallback=False\n        )\n\n        self.data_size_tresholds = self.config.geteval(\n            EventQualityWarnings.COMPONENT_ID, \"data_size_tresholds\", fallback=None\n        )\n\n        self.do_error_rate_by_date_qw = self.config.getboolean(\n            EventQualityWarnings.COMPONENT_ID,\n            \"do_error_rate_by_date_qw\",\n            fallback=False,\n        )\n\n        self.do_error_rate_by_date_and_cell_qw = self.config.getboolean(\n            EventQualityWarnings.COMPONENT_ID,\n            \"do_error_rate_by_date_and_cell_qw\",\n            fallback=False,\n        )\n\n        self.do_error_rate_by_date_and_user_qw = self.config.getboolean(\n            EventQualityWarnings.COMPONENT_ID,\n            \"do_error_rate_by_date_and_user_qw\",\n            fallback=False,\n        )\n\n        self.do_error_rate_by_date_and_cell_user_qw = self.config.getboolean(\n            EventQualityWarnings.COMPONENT_ID,\n            \"do_error_rate_by_date_and_cell_user_qw\",\n            fallback=False,\n        )\n\n        self.error_rate_tresholds = self.config.geteval(\n            EventQualityWarnings.COMPONENT_ID, \"error_rate_tresholds\", fallback=None\n        )\n\n        self.error_type_qw_checks = self.config.geteval(\n            EventQualityWarnings.COMPONENT_ID, \"error_type_qw_checks\", fallback=None\n        )\n\n        self.missing_value_thresholds = self.config.geteval(\n            EventQualityWarnings.COMPONENT_ID, \"missing_value_thresholds\", fallback=None\n        )\n\n        self.out_of_admissible_values_thresholds = self.config.geteval(\n            EventQualityWarnings.COMPONENT_ID,\n            \"out_of_admissible_values_thresholds\",\n            fallback=None,\n        )\n\n        self.not_right_syntactic_format_thresholds = self.config.geteval(\n            EventQualityWarnings.COMPONENT_ID,\n            \"not_right_syntactic_format_thresholds\",\n            fallback=None,\n        )\n\n        self.no_location_thresholds = self.config.geteval(\n            EventQualityWarnings.COMPONENT_ID, \"no_location_thresholds\", fallback=None\n        )\n\n        self.no_domain_thresholds = self.config.geteval(\n            EventQualityWarnings.COMPONENT_ID, \"no_domain_thresholds\", fallback=None\n        )\n\n        self.out_of_bounding_box_thresholds = self.config.geteval(\n            EventQualityWarnings.COMPONENT_ID,\n            \"out_of_bounding_box_thresholds\",\n            fallback=None,\n        )\n        # FOR DEDUPLICATION QUALITY WARNINGS\n        self.deduplication_same_location_thresholds = self.config.geteval(\n            EventQualityWarnings.COMPONENT_ID,\n            \"deduplication_same_location_thresholds\",\n            fallback=None,\n        )\n\n    def initalize_data_objects(self):\n        self.input_qm_data_objects = {}\n        self.output_qw_data_objects = {}\n        self.clear_destination_directory = self.config.getboolean(\n            EventQualityWarnings.COMPONENT_ID, \"clear_destination_directory\"\n        )\n\n        self.input_qm_by_column_path_key = self.config.get(\n            EventQualityWarnings.COMPONENT_ID, \"input_qm_by_column_path_key\"\n        )\n        self.input_qm_freq_distr_path_key = self.config.get(\n            EventQualityWarnings.COMPONENT_ID, \"input_qm_freq_distr_path_key\"\n        )\n        self.output_qw_log_table_path_key = self.config.get(\n            EventQualityWarnings.COMPONENT_ID, \"output_qw_log_table_path_key\"\n        )\n        self.output_qw_for_plots_path_key = self.config.get(\n            EventQualityWarnings.COMPONENT_ID,\n            \"output_qw_for_plots_path_key\",\n            fallback=None,\n        )\n\n        self.input_qm_by_column_path = self.config.get(CONFIG_SILVER_PATHS_KEY, self.input_qm_by_column_path_key)\n        if check_if_data_path_exists(self.spark, self.input_qm_by_column_path):\n            self.input_qm_data_objects[SilverEventDataSyntacticQualityMetricsByColumn.ID] = (\n                SilverEventDataSyntacticQualityMetricsByColumn(self.spark, self.input_qm_by_column_path)\n            )\n        else:\n            self.logger.warning(\"Wrong path for Quality Metrics By Column, terminating component execution\")\n            raise ValueError(\"Invalid path for Quality Metrics By Column\")\n\n        self.input_qm_freq_distr_path = self.config.get(CONFIG_SILVER_PATHS_KEY, self.input_qm_freq_distr_path_key)\n        if check_if_data_path_exists(self.spark, self.input_qm_freq_distr_path):\n            self.input_qm_data_objects[SilverEventDataSyntacticQualityMetricsFrequencyDistribution.ID] = (\n                SilverEventDataSyntacticQualityMetricsFrequencyDistribution(self.spark, self.input_qm_freq_distr_path)\n            )\n        else:\n            self.logger.warning(\n                \"Wrong path for Quality Metrics Frequency Distribution, terminating component execution\"\n            )\n            raise ValueError(\"Invalid path for Quality Metrics Frequency Distribution\")\n\n        self.output_qw_log_table_path = self.config.get(CONFIG_SILVER_PATHS_KEY, self.output_qw_log_table_path_key)\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, self.output_qw_log_table_path)\n        check_or_create_data_path(self.spark, self.output_qw_log_table_path)\n        self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsLogTable.ID] = (\n            SilverEventDataSyntacticQualityWarningsLogTable(self.spark, self.output_qw_log_table_path)\n        )\n        # no plots information is intended for EventDeduplicationQualityWarnings\n        if self.output_qw_for_plots_path_key is not None:\n            self.output_qw_for_plots_path = self.config.get(CONFIG_SILVER_PATHS_KEY, self.output_qw_for_plots_path_key)\n            if self.clear_destination_directory:\n                delete_file_or_folder(self.spark, self.output_qw_for_plots_path)\n            check_or_create_data_path(self.spark, self.output_qw_for_plots_path)\n            self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsForPlots.ID] = (\n                SilverEventDataSyntacticQualityWarningsForPlots(self.spark, self.output_qw_for_plots_path)\n            )\n\n    def read(self):\n        self.input_qm_data_objects[SilverEventDataSyntacticQualityMetricsByColumn.ID].read()\n        self.input_qm_data_objects[SilverEventDataSyntacticQualityMetricsFrequencyDistribution.ID].read()\n\n    def write(self):\n        self.save_quality_warnings_log_table(self.qw_dfs_log)\n        if self.output_qw_for_plots_path_key is not None:\n            self.save_quality_warnings_for_plots(self.qw_dfs_plots)\n\n    def execute(self):\n        self.logger.info(f\"Starting {EventQualityWarnings.COMPONENT_ID}...\")\n        self.read()\n        self.transform()  # Transforms the input_df\n        self.write()\n        self.logger.info(f\"Finished {EventQualityWarnings.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {EventQualityWarnings.COMPONENT_ID}\")\n        # Read QA Metrics of EventCleaning Component, the period of intrest is\n        #  [data_period_start-lookback_period_in_days, data_period_end]\n        # Since QualityWarnings are calculated based on prior data\n        # TODO: deal with cases when df_qa_by_column, df_qa_freq_distribution do not have data for\n        # whole defined period\n        # TODO: dynamically define/check the possible research period of QW  based on data period\n        #  of df_qa_by_column and df_qa_freq_distribution\n        # TODO: implement min_period conf param which is minimal amount of days with previous data to\n        #  have in order to calculate QW (the case for first days in reaserch period)\n        sdate = pd.to_datetime(self.data_period_start) - pd.Timedelta(days=self.lookback_period_in_days)\n        edate = pd.to_datetime(self.data_period_end)\n\n        df_qa_by_column = self.input_qm_data_objects[SilverEventDataSyntacticQualityMetricsByColumn.ID].df.where(\n            psf.col(ColNames.date).between(sdate, edate)\n        )\n\n        df_qa_freq_distribution = self.input_qm_data_objects[\n            SilverEventDataSyntacticQualityMetricsFrequencyDistribution.ID\n        ].df.where(psf.col(ColNames.date).between(sdate, edate))\n\n        df_qa_by_column = df_qa_by_column.cache()\n        # TODO: maybe makes sense to first sum init and final freq, cache and parse this aggregation\n        # to further QW functions\n        df_qa_freq_distribution = df_qa_freq_distribution.cache()\n\n        if self.do_size_raw_data_qw:\n            # for raw data size QW compute warnings and also retrive data to plot distribution of initial frequency\n            df_raw_data_qw, df_raw_plots = self.data_size_qw(\n                df_qa_freq_distribution,\n                self.lookback_period_in_days,\n                self.data_size_tresholds[\"SIZE_RAW_DATA_BYDATE_VARIABILITY\"],\n                self.data_size_tresholds[\"SIZE_RAW_DATA_BYDATE_ABS_VALUE_LOWER_LIMIT\"],\n                self.data_size_tresholds[\"SIZE_RAW_DATA_BYDATE_ABS_VALUE_UPPER_LIMIT\"],\n                type_of_data=\"raw\",\n            )\n            self.qw_dfs_log.append(df_raw_data_qw)\n            self.qw_dfs_plots.append(df_raw_plots)\n\n        if self.do_size_clean_data_qw:\n            # for clean data size QW compute warnings and also retrive data to plot distribution of total frequency\n            df_clean_data_qw, df_clean_plots = self.data_size_qw(\n                df_qa_freq_distribution,\n                self.lookback_period_in_days,\n                self.data_size_tresholds[\"SIZE_CLEAN_DATA_BYDATE_VARIABILITY\"],\n                self.data_size_tresholds[\"SIZE_CLEAN_DATA_BYDATE_ABS_VALUE_LOWER_LIMIT\"],\n                self.data_size_tresholds[\"SIZE_CLEAN_DATA_BYDATE_ABS_VALUE_UPPER_LIMIT\"],\n                type_of_data=\"clean\",\n            )\n            self.qw_dfs_log.append(df_clean_data_qw)\n            self.qw_dfs_plots.append(df_clean_plots)\n\n        if self.do_error_rate_by_date_qw:\n            # for error rate by date QW compute warnings and also retrive data to\n            # plot distribution of error rate by date\n            df_error_rate_by_date_qw, df_error_rate_plots = self.error_rate_qw(\n                df_qa_freq_distribution,\n                self.lookback_period_in_days,\n                [ColNames.date],\n                self.error_rate_tresholds[\"TOTAL_ERROR_RATE_BYDATE_OVER_AVERAGE\"],\n                self.error_rate_tresholds[\"TOTAL_ERROR_RATE_BYDATE_VARIABILITY\"],\n                self.error_rate_tresholds[\"TOTAL_ERROR_RATE_BYDATE_ABS_VALUE_UPPER_LIMIT\"],\n                save_data_for_plots=True,\n            )\n\n            self.qw_dfs_log.append(df_error_rate_by_date_qw)\n            self.qw_dfs_plots.append(df_error_rate_plots)\n        # The current aggrement is that for next error rates (more granular ones) do not store any data for plots\n        # although it could be done with save_data_for_plots=True\n        # TODO: should we consider error rate of null user_id or/and null cell_id in QW computation\n        if self.do_error_rate_by_date_and_cell_qw:\n            df_error_rate_by_date_and_cell_qw, _ = self.error_rate_qw(\n                df_qa_freq_distribution,\n                self.lookback_period_in_days,\n                [ColNames.date, ColNames.cell_id],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYCELL_OVER_AVERAGE\"],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYCELL_VARIABILITY\"],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYCELL_ABS_VALUE_UPPER_LIMIT\"],\n            )\n\n            self.qw_dfs_log.append(df_error_rate_by_date_and_cell_qw)\n\n        if self.do_error_rate_by_date_and_user_qw:\n            df_error_rate_by_date_and_user_qw, _ = self.error_rate_qw(\n                df_qa_freq_distribution,\n                self.lookback_period_in_days,\n                [ColNames.date, ColNames.user_id],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYUSER_OVER_AVERAGE\"],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYUSER_VARIABILITY\"],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYUSER_ABS_VALUE_UPPER_LIMIT\"],\n            )\n\n            self.qw_dfs_log.append(df_error_rate_by_date_and_user_qw)\n\n        if self.do_error_rate_by_date_and_cell_user_qw:\n            df_error_rate_by_date_and_cell_user_qw, _ = self.error_rate_qw(\n                df_qa_freq_distribution,\n                self.lookback_period_in_days,\n                [ColNames.date, ColNames.cell_id, ColNames.user_id],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYCELL_USER_OVER_AVERAGE\"],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYCELL_USER_VARIABILITY\"],\n                self.error_rate_tresholds[\"ERROR_RATE_BYDATE_BYCELL_USER_ABS_VALUE_UPPER_LIMIT\"],\n            )\n\n            self.qw_dfs_log.append(df_error_rate_by_date_and_cell_user_qw)\n\n        # Two previous types of QW were using df_qa_freq_distribution only\n        # Now calculate error rate for different error types like missing_value, wrong type\n        # based on two QA metrics - df_qa_by_column and df_qa_freq_distribution\n        # error_type_qw_checks - dict('error_type':[relevant columns])\n        for error_type, field_names in self.error_type_qw_checks.items():\n            if field_names == []:\n                self.logger.info(f\"No field name(s) were specified for error type: {error_type}\")\n            else:\n                # if you have a new error_type and thus new error_type_thresholds entry in config\n                # make sure to add it to class atributes and to this block with elif statement\n                if error_type == \"missing_value\":\n                    error_type_thresholds = self.missing_value_thresholds\n                elif error_type == \"out_of_admissible_values\":\n                    error_type_thresholds = self.out_of_admissible_values_thresholds\n                elif error_type == \"not_right_syntactic_format\":\n                    error_type_thresholds = self.not_right_syntactic_format_thresholds\n                elif error_type == \"no_location\":\n                    error_type_thresholds = self.no_location_thresholds\n                elif error_type == \"no_domain\":\n                    error_type_thresholds = self.no_domain_thresholds\n                elif error_type == \"out_of_bounding_box\":\n                    error_type_thresholds = self.out_of_bounding_box_thresholds\n                elif error_type == \"same_location_duplicate\":\n                    error_type_thresholds = self.deduplication_same_location_thresholds\n                else:\n                    self.logger.warning(\n                        f\"Unexpected error type in error_type_qw_checks config param\"\n                        f\": {error_type}, skipping calculation for this qw\"\n                    )\n                    continue\n\n            for field_name in field_names:\n                if field_name in error_type_thresholds.keys():\n                    error_type_qw, _ = self.error_type_rate_qw(\n                        df_qa_by_column,\n                        df_qa_freq_distribution,\n                        field_name,\n                        error_type,\n                        self.lookback_period_in_days,\n                        *list(error_type_thresholds[field_name].values()),\n                    )\n                    self.qw_dfs_log.append(error_type_qw)\n                else:\n                    self.logger.warning(\n                        f\"No thresholds were specified for field {field_name} of {error_type} error_type\"\n                    )\n\n        self.spark.catalog.clearCache()\n\n    def data_size_qw(\n        self,\n        df_freq_distribution: DataFrame,\n        lookback_period_in_days: int,\n        variablility: Union[int, float],\n        lower_limit: Union[int, float],\n        upper_limit: Union[int, float],\n        type_of_data: str,\n        measure_definition_canva: str = f\"{MeasureDefinitions.size_data}\",\n        cond_warn_variability_canva: str = f\"{Conditions.size_data_variability}-{Warnings.size_data_variability}\",\n        cond_warn_upper_lower_canva: str = f\"{Conditions.size_data_upper_lower}-{Warnings.size_data_upper_lower}\",\n    ) -&gt; Tuple[DataFrame]:\n        \"\"\"\n        A unified function to check both raw and clean data sizes, calculates four types of QWs:\n        LOWER_VARIABILITY - for each row using calculated mean and std compute lower variability limit\n            which is mean - SD*variability, check if  daily_value is lower tan limit\n        UPPER_VARIABILITY - for each row using calculated mean and std compute upper variability limit\n             which is mean + SD*variability, check if  daily_value exceeds limit\n        ABS_LOWER_LIMIT - check if daily_value is lower than absolute number lower_limit\n        ABS_UPPER_LIMIT - check if daily_value exceeds absolute number upper_limit\n        All four QWs depend on thresholds, in case if statement condition is met -&gt; store cond-warn-condition_value\n            information in array column,\n        some rows may have several QWs. In the end array column is exploded and cond-warn-consition_value information\n            is split into three corresponding columns.\n        The function returns almost ready dfs for SilverEventDataSyntacticQualityWarningsLogTable\n             and SilverEventDataSyntacticQualityWarningsForPlots DOs\n\n        Args:\n            df_freq_distribution (DataFrame): df with frequency data\n            lookback_period_in_days (int): lenght of lookback period in days\n            variablility (Union[int, float]): config param, the number of SD to define the upper and lower varibaility\n                limits: mean_size \u00b1 SD*variability, which daily_value should not exceed/be lower\n            lower_limit (Union[int, float]): absolute number which daily_value should not be lower\n            upper_limit (Union[int, float]): absolute number which daily_value can not exceed\n            type_of_data (str): which type of data raw or clean to check for QWs\n            measure_definition_canva (str): canva text to use for measure_definition column (see measure_definition.py)\n            cond_warn_variability_canva (str): canva text to use for lower_upper_variability cases\n                of data_size QWs (see conditions.py and warnings.py)\n            cond_warn_upper_lower_canva (str): canva text to use for lower_upper_limit cases\n                of data_size QWs (see conditions.py and warnings.py)\n\n        Returns:\n            tuple(DataFrame, DataFrame): a tuple, where first df\n                is used for warning log table, and the second df - for plots\n        \"\"\"\n        # based on type_of_data calculate either total daily initial freqeuncy or total daily final frequency\n        if type_of_data == \"raw\":\n            sum_column = ColNames.initial_frequency\n        else:\n            sum_column = ColNames.final_frequency\n        # fill in string canvases\n        measure_definition = measure_definition_canva.format(type_of_data=type_of_data)\n        cond_warn_variability = cond_warn_variability_canva.format(SD=variablility, type_of_data=type_of_data)\n        cond_warn_upper_lower = cond_warn_upper_lower_canva.format(\n            X=lower_limit, Y=upper_limit, type_of_data=type_of_data\n        )\n        # define lookback period\n        window = Window.orderBy(ColNames.date).rowsBetween(-lookback_period_in_days, -1)\n        # prepare data\n        # calculate mean and std over window, and based on them UCL - for UPPER_VARIABILITY check and LCL\n        #   - for LOWER_VARIABILITY check\n        # create empty array cond_warn_condition_value column to store information about qws\n        df_prep = (\n            df_freq_distribution.groupBy(ColNames.date)\n            .agg(psf.sum(sum_column).alias(ColNames.daily_value))\n            .withColumns(\n                {\n                    ColNames.average: psf.avg(ColNames.daily_value).over(window),\n                    \"std_freq\": psf.stddev(ColNames.daily_value).over(window),\n                    ColNames.UCL: psf.col(ColNames.average) + variablility * psf.col(\"std_freq\"),\n                    ColNames.LCL: psf.col(ColNames.average) - variablility * psf.col(\"std_freq\"),\n                    \"cond_warn_condition_value\": psf.array(),\n                }\n            )\n        )\n\n        df_prep = df_prep.cache()\n        # continue with QWs checks\n        # first filter data by period from [data_period_start-lookback_period_in_days, data_period_end]\n        # to [data_period_start, data_period_end]\n        # - a specified research period of QW\n        df_qw = df_prep.filter(psf.col(ColNames.date) &gt;= self.data_period_start)\n        # perform LOWER_VARIABILITY, UPPER_VARIABILITY, ABS_LOWER_LIMIT and ABS_UPPER_LIMIT checks\n        # if condition is met append information about condition-warning_text-condition_value as a string\n        # into array column 'cond_warn_condition_value'\n        df_qw = (\n            df_qw.withColumn(\n                \"cond_warn_condition_value\",\n                psf.when(\n                    (psf.col(ColNames.daily_value) &lt; psf.col(ColNames.LCL)),\n                    psf.array_append(\n                        psf.col(\"cond_warn_condition_value\"),\n                        psf.concat(\n                            psf.lit(f\"{cond_warn_variability}-\"),\n                            psf.col(ColNames.LCL).cast(\"string\"),\n                        ),\n                    ),\n                ).otherwise(psf.col(\"cond_warn_condition_value\")),\n            )\n            .withColumn(\n                \"cond_warn_condition_value\",\n                psf.when(\n                    (psf.col(ColNames.daily_value) &gt; psf.col(ColNames.UCL)),\n                    psf.array_append(\n                        psf.col(\"cond_warn_condition_value\"),\n                        psf.concat(\n                            psf.lit(f\"{cond_warn_variability}-\"),\n                            psf.col(ColNames.UCL).cast(\"string\"),\n                        ),\n                    ),\n                ).otherwise(psf.col(\"cond_warn_condition_value\")),\n            )\n            .withColumn(\n                \"cond_warn_condition_value\",\n                psf.when(\n                    (psf.col(ColNames.daily_value) &lt; psf.lit(lower_limit)),\n                    psf.array_append(\n                        psf.col(\"cond_warn_condition_value\"),\n                        psf.lit(f\"{cond_warn_upper_lower}-{str(lower_limit)}\"),\n                    ),\n                ).otherwise(psf.col(\"cond_warn_condition_value\")),\n            )\n            .withColumn(\n                \"cond_warn_condition_value\",\n                psf.when(\n                    (psf.col(ColNames.daily_value) &gt; psf.lit(upper_limit)),\n                    psf.array_append(\n                        psf.col(\"cond_warn_condition_value\"),\n                        psf.lit(f\"{cond_warn_upper_lower}-{str(upper_limit)}\"),\n                    ),\n                ).otherwise(psf.col(\"cond_warn_condition_value\")),\n            )\n        )\n        # explode array column 'cond_warn_condition_value'\n        df_qw = df_qw.withColumn(\n            \"cond_warn_condition_value\",\n            psf.explode(psf.col(\"cond_warn_condition_value\")),\n        )\n        # add some column constants\n        # split \"cond_warn_condition_value\" column into three: condition, wanring_text, conditon_value to\n        # match SilverEventDataSyntacticQualityWarningsLogTable.SCHEMA\n        # select only needed columns\n        df_qw = df_qw.withColumns(\n            {\n                ColNames.lookback_period: psf.lit(self.lookback_period),\n                ColNames.measure_definition: psf.lit(measure_definition),\n                ColNames.condition: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(0),\n                ColNames.warning_text: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(1),\n                ColNames.condition_value: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(2).cast(\"float\"),\n            }\n        ).select(self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsLogTable.ID].SCHEMA.fieldNames())\n\n        # save data for plots\n        # no filter by date because we need previous data of first days for plots\n        df_plots = df_prep.withColumns(\n            {\n                ColNames.lookback_period: psf.lit(self.lookback_period),\n                ColNames.type_of_qw: psf.lit(f\"{type_of_data}_data_size\"),\n            }\n        ).select(self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsForPlots.ID].SCHEMA.fieldNames())\n\n        return df_qw, df_plots\n\n    def error_rate_qw(\n        self,\n        df_freq_distribution: DataFrame,\n        lookback_period_in_days: int,\n        variables: List[str],\n        error_rate_over_average: Union[int, float],\n        error_rate_upper_variability: Union[int, float],\n        error_rate_upper_limit: Union[int, float],\n        error_rate_measure_definition_canva: str = f\"{MeasureDefinitions.error_rate}\",\n        error_rate_cond_warn_over_average_canva: str = f\"{Conditions.error_rate_over_average}-{Warnings.error_rate_over_average}\",\n        error_rate_cond_warn_upper_variability_canva: str = f\"{Conditions.error_rate_upper_variability}-{Warnings.error_rate_upper_variability}\",\n        error_rate_cond_warn_upper_limit_canva: str = f\"{Conditions.error_rate_upper_limit}-{Warnings.error_rate_upper_limit}\",\n        save_data_for_plots: bool = False,\n    ) -&gt; Tuple[Union[DataFrame, None]]:\n        \"\"\"\n        Prepare data for error rate calculation. First fill in different string canvas,\n            then define window of aggregation, and calculate error_rate over the window on follwoing formula:\n            (Total initial frequency - Total final frequency) / Total initial frequency*100.\n            Parse preprocessed input to self.rate_common_qw function which calculates three types\n                of QWs: OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT\n\n        Args:\n            df_freq_distribution (DataFrame): df with frequency data.\n            lookback_period_in_days (int): number of days prior to date of interest.\n            variables (List[str]): list of column names by which error rate is calculated, kind of granularity level\n            error_rate_over_average (Union[int, float]): config param, specifies the upper limit which a daily value\n                can not exceed its corresponding mean error rate\n            error_rate_upper_variability (Union[int, float]): config param, the number of SD to define the upper\n                varibaility limit: mean_rate + SD*error_rate_upper_variability, which error rate can't exceed\n            error_rate_upper_limit (Union[int, float]): absolute number which error rate can not exceed\n            error_rate_measure_definition_canva (str): canva text to use for measure_definition\n                column (see measure_definition.py)\n            error_rate_cond_warn_over_average_canva (str): canva text to use for over_average cases of\n                error_rate QWs (see conditions.py and warnings.py)\n            error_rate_cond_warn_upper_variability_canva (str): canva text to use for upper_variability cases of\n                error_rate QWs (see conditions.py and warnings.py)\n            error_rate_cond_warn_upper_limit_canva (str): canva text to use for upper_limit cases of error_rate\n                QWs (see conditions.py and warnings.py)\n            save_data_for_plots (bool): boolean, decide whether to store error rate and its corresponding average\n                and upper variability limit for plots, default False\n        Returns:\n            tuple(Union[DataFrame, None]): a tuple, where first df is used for warning log table,\n                and the second df - for plots (could be also None)\n        \"\"\"\n        # fill in all string comnstants with relevant information\n        # based on variables, error_rate_over_average, error_rate_upper_variability, error_rate_upper_limit args\n        error_rate_measure_definition = error_rate_measure_definition_canva.format(variables=\"&amp;\".join(variables))\n\n        error_rate_cond_warn_over_average = error_rate_cond_warn_over_average_canva.format(\n            variables=\"&amp;\".join(variables), X=error_rate_over_average\n        )\n        error_rate_cond_warn_upper_variability = error_rate_cond_warn_upper_variability_canva.format(\n            variables=\"&amp;\".join(variables), SD=error_rate_upper_variability\n        )\n        error_rate_cond_warn_upper_limit = error_rate_cond_warn_upper_limit_canva.format(\n            variables=\"&amp;\".join(variables), X=error_rate_upper_limit\n        )\n        # qws of error rate by date is calculated based on previous days\n        if variables == [ColNames.date]:\n            window = Window.orderBy(ColNames.date).rowsBetween(-lookback_period_in_days, -1)\n        else:\n            # qws of error rate by date and other colum(s) is calculated over all \"data points\" of the same date\n            window = Window.partitionBy(ColNames.date)\n        # calculate error rate, a.k.a daily_value\n        df_qw = (\n            df_freq_distribution.groupBy(*variables)\n            .agg(\n                psf.sum(ColNames.initial_frequency).alias(\"sum_init_freq\"),\n                psf.sum(ColNames.final_frequency).alias(\"sum_final_freq\"),\n            )\n            .withColumn(\n                ColNames.daily_value,\n                (psf.col(\"sum_init_freq\") - psf.col(\"sum_final_freq\")) / psf.col(\"sum_init_freq\") * 100,\n            )\n        )\n        # using self.rate_common_qw funciton calculate three types of QWs\n        #   -&gt; OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT\n        # return a tuple of (df_log, df_plots | None)\n        qw_result = self.rate_common_qw(\n            df_qw,\n            window,\n            error_rate_upper_variability,\n            error_rate_over_average,\n            error_rate_upper_limit,\n            error_rate_measure_definition,\n            error_rate_cond_warn_upper_variability,\n            error_rate_cond_warn_over_average,\n            error_rate_cond_warn_upper_limit,\n            save_data_for_plots,\n        )\n\n        return qw_result\n\n    def error_type_rate_qw(\n        self,\n        df_qa_by_column: DataFrame,\n        df_freq_distribution: DataFrame,\n        field_name: Union[str, None],\n        error_type: str,\n        lookback_period_in_days: int,\n        error_type_rate_over_average: Union[int, float],\n        error_type_rate_upper_variability: Union[int, float],\n        error_type_rate_upper_limit: Union[int, float],\n        error_type_rate_measure_definition_canva: str = f\"{MeasureDefinitions.error_type_rate}\",\n        error_type_rate_cond_warn_over_average_canva: str = f\"{Conditions.error_type_rate_over_average}-{Warnings.error_type_rate_over_average}\",\n        error_type_rate_cond_warn_upper_variability_canva: str = f\"{Conditions.error_type_rate_upper_variability}-{Warnings.error_type_rate_upper_variability}\",\n        error_type_rate_cond_warn_upper_limit_canva: str = f\"{Conditions.error_type_rate_upper_limit}-{Warnings.error_type_rate_upper_limit}\",\n    ) -&gt; Tuple[Union[DataFrame, None]]:\n        \"\"\"\n        Prepare data for error type rate calculation. First fill in different string canvas, then based\n            on field name and error type calculate their corresponding error rate using formula:\n            number of errors of this error_type&amp;field_name combo / Total initial frequency *100 (BY DATE).\n            Parse preprocessed input along with window (which is a lookback period)\n            to self.rate_common_qw function which calculates three types of QWs:\n            OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT\n\n\n        Args:\n            df_qa_by_column (DataFrame): df with qa by column data.\n            df_freq_distribution (DataFrame): df with frequency data.\n            field_name (str | None): config param, the name of column of which to check error_type.\n            error_type (str): config param, the name of error type.\n            lookback_period_in_days (int): number of days prior to date of intrest.\n            error_type_rate_over_average (Union[int, float]): config param, specifies the upper limit over which daily\n                value can not exceed its corresponding mean error rate.\n            error_type_rate_upper_variability (Union[int, float]): config param, the number of SD to define the upper\n                varibaility limit: mean_rate + SD*error_type_rate_upper_variability, which daily value can not exceed\n            error_type_rate_upper_limit (Union[int, float]): absolute number which daily value can not exceed\n            error_type_rate_measure_definition_canva (str): canva text to use for measure_definition\n                column (see measure_definition.py)\n            error_type_rate_cond_warn_over_average_canva (str): canva text to use for over_average cases of\n                error_type_rate QWs (see conditions.py and warnings.py)\n            error_type_rate_cond_warn_upper_variability_canva (str): canva text to use for upper_variability\n                cases of error_type_rate QWs (see conditions.py and warnings.py)\n            error_type_rate_cond_warn_upper_limit_canva (str): canva text to use for upper_limit cases of\n                error_type_rate QWs (see conditions.py and warnings.py)\n        Returns:\n            tuple(Union[DataFrame, None]): a tuple, where first df is used for warning log table,\n                and the second df - for plots, but since save_data_for_plots always False, output=None\n        \"\"\"\n        # based on error_type, error_type_rate_over_average, error_type_rate_upper_variability,\n        #  error_type_rate_upper_limit\n        # fill in string canvases\n        colname_error_type, error_type_qw_name = self.dict_error_type_info[error_type]\n\n        error_type_rate_measure_definition = error_type_rate_measure_definition_canva.format(\n            error_type_name=error_type_qw_name, field_name=str(field_name)\n        )\n\n        error_type_rate_cond_warn_over_average = error_type_rate_cond_warn_over_average_canva.format(\n            error_type_name=error_type_qw_name,\n            field_name=str(field_name),\n            X=error_type_rate_over_average,\n        )\n        error_type_rate_cond_warn_upper_variability = error_type_rate_cond_warn_upper_variability_canva.format(\n            error_type_name=error_type_qw_name,\n            field_name=str(field_name),\n            SD=error_type_rate_upper_variability,\n        )\n        error_type_rate_cond_warn_upper_limit = error_type_rate_cond_warn_upper_limit_canva.format(\n            error_type_name=error_type_qw_name,\n            field_name=str(field_name),\n            X=error_type_rate_upper_limit,\n        )\n        # for error_type that have more then one or applicable columns\n        # filter df_qa_by_column by field_name and error_type\n        if field_name is not None:\n            df_qa_by_column = df_qa_by_column.filter(\n                (psf.col(ColNames.variable) == field_name) &amp; (psf.col(ColNames.type_of_error) == colname_error_type)\n            ).select(ColNames.date, ColNames.value)\n        else:\n            # for error_types which technically do not belong specifically to one of event\n            # columns filter only by error_type (e.g. no_location error_type)\n            df_qa_by_column = df_qa_by_column.filter(psf.col(ColNames.type_of_error) == colname_error_type).select(\n                ColNames.date, ColNames.value\n            )\n        # calculate total daily initial frequency\n        df_freq_distribution = (\n            df_freq_distribution.groupby(ColNames.date)\n            .agg(psf.sum(ColNames.initial_frequency).alias(\"sum_init_freq\"))\n            .select(ColNames.date, \"sum_init_freq\")\n        )\n        # for each date combine two type of information number of errors and total daily initial frequency\n        df_combined = df_qa_by_column.join(df_freq_distribution, on=ColNames.date, how=\"inner\")\n        # for each date calculate error_type_rate, a.k.a daily_value\n        df_temp = df_combined.withColumn(\n            ColNames.daily_value,\n            (psf.col(ColNames.value) / psf.col(\"sum_init_freq\")) * 100,\n        )\n\n        # qws will be caluclated based on previous days\n        window = Window.orderBy(ColNames.date).rowsBetween(-lookback_period_in_days, -1)\n        # using self.rate_common_qw funciton calculate three types of QWs -&gt;\n        # OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT\n        # return a tuple of (df_log, None)\n        qw_result = self.rate_common_qw(\n            df_temp,\n            window,\n            error_type_rate_upper_variability,\n            error_type_rate_over_average,\n            error_type_rate_upper_limit,\n            error_type_rate_measure_definition,\n            error_type_rate_cond_warn_upper_variability,\n            error_type_rate_cond_warn_over_average,\n            error_type_rate_cond_warn_upper_limit,\n        )\n        return qw_result\n\n    def rate_common_qw(\n        self,\n        df_temp: DataFrame,\n        window: Window,\n        rate_upper_variability: Union[int, float],\n        rate_over_average: Union[int, float],\n        rate_upper_limit: Union[int, float],\n        measure_definition: str,\n        cond_warn_upper_variability: str,\n        cond_warn_over_average: str,\n        cond_warn_upper_limit: str,\n        save_data_for_plots: bool = False,\n    ) -&gt; Tuple[Union[DataFrame, None]]:\n        \"\"\"\n        Take input df with \"daily_value\" column, and calculates three types of QWs:\n        OVER_AVERAGE - for each row first based on specified window take mean of values, and then check if\n            daily_value exceeds mean by more than rate_over_average\n        UPPER_VARIABILITY - for each row using already calculated mean compute upper variability limit which is\n            mean + SD*rate_upper_variability, check if  daily_value exceeds it\n        ABS_UPPER_LIMIT - check if daily_value exceeds absolute number rate_upper_limit\n        All three QWs depend on specified thresholds, if daily_value exceeds one of calculated values it will\n            store cond-warn-condition_value information in array column,\n        some rows may have several QWs. In the end array column is exploded and cond-warn-condition_value\n            information is split into three corresponding columns.\n        The function returns almost ready df for SilverEventDataSyntacticQualityWarningsLogTable DO, and based\n            on save_data_for_plots arg returns either almost ready data for plots or None\n\n        Args:\n            df_temp (DataFrame): temprory data that must have daily_value column to\n                be used in further QW calculations\n            window (Window): a window within which perform aggregation\n            rate_upper_variability (int|float): config param, specifies the upper limit over which daily value\n                 can not exceed its corresponding mean error rate\n            rate_over_average (int|float): config param, the number of SD to define the upper varibaility\n                 limit: mean_rate + SD*error_rate_upper_variability, which daily value can not exceed\n            rate_upper_limit (int|float): absolute number which daily value can not exceed\n            measure_definition (str): canva text to use for measure_definition column (see measure_definition.py)\n            cond_warn_over_average (str): canva text to use for over_average cases (see conditions.py and warnings.py)\n            cond_warn_upper_variability (str): canva text to use for\n                upper_variability cases (see conditions.py and warnings.py)\n            cond_warn_upper_limit (str): canva text to use for upper_limit cases (see conditions.py and warnings.py)\n            save_data_for_plots (bool): boolean, decide whether to store daily_value and its corresponding average\n                and upper variability limit for plots. Defaults to False.\n        Returns:\n             tuple(Union[DataFrame, None]): a tuple, where first df is used for\n                warning log table, and the second df - for plots\n        \"\"\"\n        # prepare data\n        # calculate mean and std over window, and based on them UCL - for UPPER_VARIABILITY check\n        # ratio_perc - for OVER_AVERAGE check\n        # create empty array cond_warn_condition_value column to store inromation about qws\n        df_prep = df_temp.withColumns(\n            {\n                ColNames.average: psf.avg(ColNames.daily_value).over(window),\n                \"std_rate\": psf.stddev(ColNames.daily_value).over(window),\n                \"ratio_perc\": (psf.col(ColNames.daily_value) / psf.col(ColNames.average)) * 100,\n                ColNames.UCL: psf.col(ColNames.average) + rate_upper_variability * psf.col(\"std_rate\"),\n                \"cond_warn_condition_value\": psf.array(),\n            }\n        )\n        # if save_data_for_plots=True, add some new columns with constant values\n        # and select only applicable to SilverEventDataSyntacticQualityWarningsForPlots.SCHEMA\n        # else - return None\n        if save_data_for_plots:\n            df_prep = df_prep.cache()\n            df_plots = df_prep.withColumns(\n                {\n                    ColNames.lookback_period: psf.lit(self.lookback_period),\n                    ColNames.type_of_qw: psf.lit(\"error_rate\"),\n                    ColNames.LCL: psf.lit(None).cast(\"float\"),\n                }\n            ).select(\n                self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsForPlots.ID].SCHEMA.fieldNames()\n            )\n        else:\n            df_plots = None\n\n        # continue with QWs checks\n        # first filter temp data by period from [data_period_start-lookback_period_in_days, data_period_end]\n        #  to [data_period_start, data_period_end]\n        # filter is aaplied after plot block because the first days of research period needs previous data to plot\n        df_qw = df_prep.filter(psf.col(ColNames.date) &gt;= self.data_period_start)\n        # perform UPPER_VARIABILITY, OVER_AVERAGE, and ABS_UPPER_LIMIT checks\n        # if condition is met store information about condition-warning_text-condition_value as a string into\n        # array column 'cond_warn_condition_value'\n        df_qw = (\n            df_qw.withColumn(\n                \"cond_warn_condition_value\",\n                psf.when(\n                    psf.col(ColNames.daily_value) &gt; psf.col(ColNames.UCL),\n                    psf.array_append(\n                        psf.col(\"cond_warn_condition_value\"),\n                        psf.concat(\n                            psf.lit(f\"{cond_warn_upper_variability}-\"),\n                            psf.col(ColNames.UCL).cast(\"string\"),\n                        ),\n                    ),\n                ).otherwise(psf.col(\"cond_warn_condition_value\")),\n            )\n            .withColumn(\n                \"cond_warn_condition_value\",\n                psf.when(\n                    psf.col(\"ratio_perc\") &gt; psf.lit(100 + rate_over_average),\n                    psf.array_append(\n                        psf.col(\"cond_warn_condition_value\"),\n                        psf.lit(f\"{cond_warn_over_average}-{str(rate_over_average)}\"),\n                    ),\n                ).otherwise(psf.col(\"cond_warn_condition_value\")),\n            )\n            .withColumn(\n                \"cond_warn_condition_value\",\n                psf.when(\n                    psf.col(ColNames.daily_value) &gt; psf.lit(rate_upper_limit),\n                    psf.array_append(\n                        psf.col(\"cond_warn_condition_value\"),\n                        psf.lit(f\"{cond_warn_upper_limit}-{str(rate_upper_limit)}\"),\n                    ),\n                ).otherwise(psf.col(\"cond_warn_condition_value\")),\n            )\n        )\n        # explode array column 'cond_warn_condition_value'\n        df_qw = df_qw.withColumn(\n            \"cond_warn_condition_value\",\n            psf.explode(psf.col(\"cond_warn_condition_value\")),\n        )\n        # add some column constants\n        # split \"cond_warn_condition_value\" column into three: condition, wanring_text, conditon_value to match\n        # SilverEventDataSyntacticQualityWarningsLogTable.SCHEMA\n        # select only needed columns\n        df_qw = df_qw.withColumns(\n            {\n                ColNames.lookback_period: psf.lit(self.lookback_period),\n                ColNames.measure_definition: psf.lit(measure_definition),\n                ColNames.condition: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(0),\n                ColNames.warning_text: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(1),\n                ColNames.condition_value: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(2).cast(\"float\"),\n            }\n        ).select(self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsLogTable.ID].SCHEMA.fieldNames())\n\n        return (df_qw, df_plots)\n\n    def save_quality_warnings_output(\n        self,\n        dfs_qw: List[Union[DataFrame, None]],\n        output_do: Union[\n            SilverEventDataSyntacticQualityWarningsLogTable, SilverEventDataSyntacticQualityWarningsForPlots\n        ],\n    ):\n        \"\"\"\n        Concatenates all elements in dfs_qw list, adjustes to schema of output_do, and using write\n            method of output_do stores the result\n\n        Args:\n            dfs_qw (list): _description_\n            output_do (SilverEventDataSyntacticQualityWarningsLogTable | \\\n                SilverEventDataSyntacticQualityWarningsForPlots): _description_\n        \"\"\"\n\n        output_do.df = reduce(lambda x, y: x.union(y), dfs_qw)\n\n        output_do.df = self.spark.createDataFrame(output_do.df.rdd, output_do.SCHEMA)\n\n        output_do.write()\n\n    def save_quality_warnings_log_table(self, dfs_qw):\n\n        self.save_quality_warnings_output(\n            dfs_qw,\n            self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsLogTable.ID],\n        )\n\n    def save_quality_warnings_for_plots(self, dfs_qw):\n\n        self.save_quality_warnings_output(\n            dfs_qw,\n            self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsForPlots.ID],\n        )\n</code></pre>"},{"location":"reference/components/quality/event_quality_warnings/event_quality_warnings/#components.quality.event_quality_warnings.event_quality_warnings.EventQualityWarnings.data_size_qw","title":"<code>data_size_qw(df_freq_distribution, lookback_period_in_days, variablility, lower_limit, upper_limit, type_of_data, measure_definition_canva=f'{MeasureDefinitions.size_data}', cond_warn_variability_canva=f'{Conditions.size_data_variability}-{Warnings.size_data_variability}', cond_warn_upper_lower_canva=f'{Conditions.size_data_upper_lower}-{Warnings.size_data_upper_lower}')</code>","text":"<p>A unified function to check both raw and clean data sizes, calculates four types of QWs: LOWER_VARIABILITY - for each row using calculated mean and std compute lower variability limit     which is mean - SDvariability, check if  daily_value is lower tan limit UPPER_VARIABILITY - for each row using calculated mean and std compute upper variability limit      which is mean + SDvariability, check if  daily_value exceeds limit ABS_LOWER_LIMIT - check if daily_value is lower than absolute number lower_limit ABS_UPPER_LIMIT - check if daily_value exceeds absolute number upper_limit All four QWs depend on thresholds, in case if statement condition is met -&gt; store cond-warn-condition_value     information in array column, some rows may have several QWs. In the end array column is exploded and cond-warn-consition_value information     is split into three corresponding columns. The function returns almost ready dfs for SilverEventDataSyntacticQualityWarningsLogTable      and SilverEventDataSyntacticQualityWarningsForPlots DOs</p> <p>Parameters:</p> Name Type Description Default <code>df_freq_distribution</code> <code>DataFrame</code> <p>df with frequency data</p> required <code>lookback_period_in_days</code> <code>int</code> <p>lenght of lookback period in days</p> required <code>variablility</code> <code>Union[int, float]</code> <p>config param, the number of SD to define the upper and lower varibaility limits: mean_size \u00b1 SD*variability, which daily_value should not exceed/be lower</p> required <code>lower_limit</code> <code>Union[int, float]</code> <p>absolute number which daily_value should not be lower</p> required <code>upper_limit</code> <code>Union[int, float]</code> <p>absolute number which daily_value can not exceed</p> required <code>type_of_data</code> <code>str</code> <p>which type of data raw or clean to check for QWs</p> required <code>measure_definition_canva</code> <code>str</code> <p>canva text to use for measure_definition column (see measure_definition.py)</p> <code>f'{size_data}'</code> <code>cond_warn_variability_canva</code> <code>str</code> <p>canva text to use for lower_upper_variability cases of data_size QWs (see conditions.py and warnings.py)</p> <code>f'{size_data_variability}-{size_data_variability}'</code> <code>cond_warn_upper_lower_canva</code> <code>str</code> <p>canva text to use for lower_upper_limit cases of data_size QWs (see conditions.py and warnings.py)</p> <code>f'{size_data_upper_lower}-{size_data_upper_lower}'</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>(DataFrame, DataFrame)</code> <p>a tuple, where first df is used for warning log table, and the second df - for plots</p> Source code in <code>multimno/components/quality/event_quality_warnings/event_quality_warnings.py</code> <pre><code>def data_size_qw(\n    self,\n    df_freq_distribution: DataFrame,\n    lookback_period_in_days: int,\n    variablility: Union[int, float],\n    lower_limit: Union[int, float],\n    upper_limit: Union[int, float],\n    type_of_data: str,\n    measure_definition_canva: str = f\"{MeasureDefinitions.size_data}\",\n    cond_warn_variability_canva: str = f\"{Conditions.size_data_variability}-{Warnings.size_data_variability}\",\n    cond_warn_upper_lower_canva: str = f\"{Conditions.size_data_upper_lower}-{Warnings.size_data_upper_lower}\",\n) -&gt; Tuple[DataFrame]:\n    \"\"\"\n    A unified function to check both raw and clean data sizes, calculates four types of QWs:\n    LOWER_VARIABILITY - for each row using calculated mean and std compute lower variability limit\n        which is mean - SD*variability, check if  daily_value is lower tan limit\n    UPPER_VARIABILITY - for each row using calculated mean and std compute upper variability limit\n         which is mean + SD*variability, check if  daily_value exceeds limit\n    ABS_LOWER_LIMIT - check if daily_value is lower than absolute number lower_limit\n    ABS_UPPER_LIMIT - check if daily_value exceeds absolute number upper_limit\n    All four QWs depend on thresholds, in case if statement condition is met -&gt; store cond-warn-condition_value\n        information in array column,\n    some rows may have several QWs. In the end array column is exploded and cond-warn-consition_value information\n        is split into three corresponding columns.\n    The function returns almost ready dfs for SilverEventDataSyntacticQualityWarningsLogTable\n         and SilverEventDataSyntacticQualityWarningsForPlots DOs\n\n    Args:\n        df_freq_distribution (DataFrame): df with frequency data\n        lookback_period_in_days (int): lenght of lookback period in days\n        variablility (Union[int, float]): config param, the number of SD to define the upper and lower varibaility\n            limits: mean_size \u00b1 SD*variability, which daily_value should not exceed/be lower\n        lower_limit (Union[int, float]): absolute number which daily_value should not be lower\n        upper_limit (Union[int, float]): absolute number which daily_value can not exceed\n        type_of_data (str): which type of data raw or clean to check for QWs\n        measure_definition_canva (str): canva text to use for measure_definition column (see measure_definition.py)\n        cond_warn_variability_canva (str): canva text to use for lower_upper_variability cases\n            of data_size QWs (see conditions.py and warnings.py)\n        cond_warn_upper_lower_canva (str): canva text to use for lower_upper_limit cases\n            of data_size QWs (see conditions.py and warnings.py)\n\n    Returns:\n        tuple(DataFrame, DataFrame): a tuple, where first df\n            is used for warning log table, and the second df - for plots\n    \"\"\"\n    # based on type_of_data calculate either total daily initial freqeuncy or total daily final frequency\n    if type_of_data == \"raw\":\n        sum_column = ColNames.initial_frequency\n    else:\n        sum_column = ColNames.final_frequency\n    # fill in string canvases\n    measure_definition = measure_definition_canva.format(type_of_data=type_of_data)\n    cond_warn_variability = cond_warn_variability_canva.format(SD=variablility, type_of_data=type_of_data)\n    cond_warn_upper_lower = cond_warn_upper_lower_canva.format(\n        X=lower_limit, Y=upper_limit, type_of_data=type_of_data\n    )\n    # define lookback period\n    window = Window.orderBy(ColNames.date).rowsBetween(-lookback_period_in_days, -1)\n    # prepare data\n    # calculate mean and std over window, and based on them UCL - for UPPER_VARIABILITY check and LCL\n    #   - for LOWER_VARIABILITY check\n    # create empty array cond_warn_condition_value column to store information about qws\n    df_prep = (\n        df_freq_distribution.groupBy(ColNames.date)\n        .agg(psf.sum(sum_column).alias(ColNames.daily_value))\n        .withColumns(\n            {\n                ColNames.average: psf.avg(ColNames.daily_value).over(window),\n                \"std_freq\": psf.stddev(ColNames.daily_value).over(window),\n                ColNames.UCL: psf.col(ColNames.average) + variablility * psf.col(\"std_freq\"),\n                ColNames.LCL: psf.col(ColNames.average) - variablility * psf.col(\"std_freq\"),\n                \"cond_warn_condition_value\": psf.array(),\n            }\n        )\n    )\n\n    df_prep = df_prep.cache()\n    # continue with QWs checks\n    # first filter data by period from [data_period_start-lookback_period_in_days, data_period_end]\n    # to [data_period_start, data_period_end]\n    # - a specified research period of QW\n    df_qw = df_prep.filter(psf.col(ColNames.date) &gt;= self.data_period_start)\n    # perform LOWER_VARIABILITY, UPPER_VARIABILITY, ABS_LOWER_LIMIT and ABS_UPPER_LIMIT checks\n    # if condition is met append information about condition-warning_text-condition_value as a string\n    # into array column 'cond_warn_condition_value'\n    df_qw = (\n        df_qw.withColumn(\n            \"cond_warn_condition_value\",\n            psf.when(\n                (psf.col(ColNames.daily_value) &lt; psf.col(ColNames.LCL)),\n                psf.array_append(\n                    psf.col(\"cond_warn_condition_value\"),\n                    psf.concat(\n                        psf.lit(f\"{cond_warn_variability}-\"),\n                        psf.col(ColNames.LCL).cast(\"string\"),\n                    ),\n                ),\n            ).otherwise(psf.col(\"cond_warn_condition_value\")),\n        )\n        .withColumn(\n            \"cond_warn_condition_value\",\n            psf.when(\n                (psf.col(ColNames.daily_value) &gt; psf.col(ColNames.UCL)),\n                psf.array_append(\n                    psf.col(\"cond_warn_condition_value\"),\n                    psf.concat(\n                        psf.lit(f\"{cond_warn_variability}-\"),\n                        psf.col(ColNames.UCL).cast(\"string\"),\n                    ),\n                ),\n            ).otherwise(psf.col(\"cond_warn_condition_value\")),\n        )\n        .withColumn(\n            \"cond_warn_condition_value\",\n            psf.when(\n                (psf.col(ColNames.daily_value) &lt; psf.lit(lower_limit)),\n                psf.array_append(\n                    psf.col(\"cond_warn_condition_value\"),\n                    psf.lit(f\"{cond_warn_upper_lower}-{str(lower_limit)}\"),\n                ),\n            ).otherwise(psf.col(\"cond_warn_condition_value\")),\n        )\n        .withColumn(\n            \"cond_warn_condition_value\",\n            psf.when(\n                (psf.col(ColNames.daily_value) &gt; psf.lit(upper_limit)),\n                psf.array_append(\n                    psf.col(\"cond_warn_condition_value\"),\n                    psf.lit(f\"{cond_warn_upper_lower}-{str(upper_limit)}\"),\n                ),\n            ).otherwise(psf.col(\"cond_warn_condition_value\")),\n        )\n    )\n    # explode array column 'cond_warn_condition_value'\n    df_qw = df_qw.withColumn(\n        \"cond_warn_condition_value\",\n        psf.explode(psf.col(\"cond_warn_condition_value\")),\n    )\n    # add some column constants\n    # split \"cond_warn_condition_value\" column into three: condition, wanring_text, conditon_value to\n    # match SilverEventDataSyntacticQualityWarningsLogTable.SCHEMA\n    # select only needed columns\n    df_qw = df_qw.withColumns(\n        {\n            ColNames.lookback_period: psf.lit(self.lookback_period),\n            ColNames.measure_definition: psf.lit(measure_definition),\n            ColNames.condition: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(0),\n            ColNames.warning_text: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(1),\n            ColNames.condition_value: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(2).cast(\"float\"),\n        }\n    ).select(self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsLogTable.ID].SCHEMA.fieldNames())\n\n    # save data for plots\n    # no filter by date because we need previous data of first days for plots\n    df_plots = df_prep.withColumns(\n        {\n            ColNames.lookback_period: psf.lit(self.lookback_period),\n            ColNames.type_of_qw: psf.lit(f\"{type_of_data}_data_size\"),\n        }\n    ).select(self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsForPlots.ID].SCHEMA.fieldNames())\n\n    return df_qw, df_plots\n</code></pre>"},{"location":"reference/components/quality/event_quality_warnings/event_quality_warnings/#components.quality.event_quality_warnings.event_quality_warnings.EventQualityWarnings.error_rate_qw","title":"<code>error_rate_qw(df_freq_distribution, lookback_period_in_days, variables, error_rate_over_average, error_rate_upper_variability, error_rate_upper_limit, error_rate_measure_definition_canva=f'{MeasureDefinitions.error_rate}', error_rate_cond_warn_over_average_canva=f'{Conditions.error_rate_over_average}-{Warnings.error_rate_over_average}', error_rate_cond_warn_upper_variability_canva=f'{Conditions.error_rate_upper_variability}-{Warnings.error_rate_upper_variability}', error_rate_cond_warn_upper_limit_canva=f'{Conditions.error_rate_upper_limit}-{Warnings.error_rate_upper_limit}', save_data_for_plots=False)</code>","text":"<p>Prepare data for error rate calculation. First fill in different string canvas,     then define window of aggregation, and calculate error_rate over the window on follwoing formula:     (Total initial frequency - Total final frequency) / Total initial frequency*100.     Parse preprocessed input to self.rate_common_qw function which calculates three types         of QWs: OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT</p> <p>Parameters:</p> Name Type Description Default <code>df_freq_distribution</code> <code>DataFrame</code> <p>df with frequency data.</p> required <code>lookback_period_in_days</code> <code>int</code> <p>number of days prior to date of interest.</p> required <code>variables</code> <code>List[str]</code> <p>list of column names by which error rate is calculated, kind of granularity level</p> required <code>error_rate_over_average</code> <code>Union[int, float]</code> <p>config param, specifies the upper limit which a daily value can not exceed its corresponding mean error rate</p> required <code>error_rate_upper_variability</code> <code>Union[int, float]</code> <p>config param, the number of SD to define the upper varibaility limit: mean_rate + SD*error_rate_upper_variability, which error rate can't exceed</p> required <code>error_rate_upper_limit</code> <code>Union[int, float]</code> <p>absolute number which error rate can not exceed</p> required <code>error_rate_measure_definition_canva</code> <code>str</code> <p>canva text to use for measure_definition column (see measure_definition.py)</p> <code>f'{error_rate}'</code> <code>error_rate_cond_warn_over_average_canva</code> <code>str</code> <p>canva text to use for over_average cases of error_rate QWs (see conditions.py and warnings.py)</p> <code>f'{error_rate_over_average}-{error_rate_over_average}'</code> <code>error_rate_cond_warn_upper_variability_canva</code> <code>str</code> <p>canva text to use for upper_variability cases of error_rate QWs (see conditions.py and warnings.py)</p> <code>f'{error_rate_upper_variability}-{error_rate_upper_variability}'</code> <code>error_rate_cond_warn_upper_limit_canva</code> <code>str</code> <p>canva text to use for upper_limit cases of error_rate QWs (see conditions.py and warnings.py)</p> <code>f'{error_rate_upper_limit}-{error_rate_upper_limit}'</code> <code>save_data_for_plots</code> <code>bool</code> <p>boolean, decide whether to store error rate and its corresponding average and upper variability limit for plots, default False</p> <code>False</code> <p>Returns:     tuple(Union[DataFrame, None]): a tuple, where first df is used for warning log table,         and the second df - for plots (could be also None)</p> Source code in <code>multimno/components/quality/event_quality_warnings/event_quality_warnings.py</code> <pre><code>def error_rate_qw(\n    self,\n    df_freq_distribution: DataFrame,\n    lookback_period_in_days: int,\n    variables: List[str],\n    error_rate_over_average: Union[int, float],\n    error_rate_upper_variability: Union[int, float],\n    error_rate_upper_limit: Union[int, float],\n    error_rate_measure_definition_canva: str = f\"{MeasureDefinitions.error_rate}\",\n    error_rate_cond_warn_over_average_canva: str = f\"{Conditions.error_rate_over_average}-{Warnings.error_rate_over_average}\",\n    error_rate_cond_warn_upper_variability_canva: str = f\"{Conditions.error_rate_upper_variability}-{Warnings.error_rate_upper_variability}\",\n    error_rate_cond_warn_upper_limit_canva: str = f\"{Conditions.error_rate_upper_limit}-{Warnings.error_rate_upper_limit}\",\n    save_data_for_plots: bool = False,\n) -&gt; Tuple[Union[DataFrame, None]]:\n    \"\"\"\n    Prepare data for error rate calculation. First fill in different string canvas,\n        then define window of aggregation, and calculate error_rate over the window on follwoing formula:\n        (Total initial frequency - Total final frequency) / Total initial frequency*100.\n        Parse preprocessed input to self.rate_common_qw function which calculates three types\n            of QWs: OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT\n\n    Args:\n        df_freq_distribution (DataFrame): df with frequency data.\n        lookback_period_in_days (int): number of days prior to date of interest.\n        variables (List[str]): list of column names by which error rate is calculated, kind of granularity level\n        error_rate_over_average (Union[int, float]): config param, specifies the upper limit which a daily value\n            can not exceed its corresponding mean error rate\n        error_rate_upper_variability (Union[int, float]): config param, the number of SD to define the upper\n            varibaility limit: mean_rate + SD*error_rate_upper_variability, which error rate can't exceed\n        error_rate_upper_limit (Union[int, float]): absolute number which error rate can not exceed\n        error_rate_measure_definition_canva (str): canva text to use for measure_definition\n            column (see measure_definition.py)\n        error_rate_cond_warn_over_average_canva (str): canva text to use for over_average cases of\n            error_rate QWs (see conditions.py and warnings.py)\n        error_rate_cond_warn_upper_variability_canva (str): canva text to use for upper_variability cases of\n            error_rate QWs (see conditions.py and warnings.py)\n        error_rate_cond_warn_upper_limit_canva (str): canva text to use for upper_limit cases of error_rate\n            QWs (see conditions.py and warnings.py)\n        save_data_for_plots (bool): boolean, decide whether to store error rate and its corresponding average\n            and upper variability limit for plots, default False\n    Returns:\n        tuple(Union[DataFrame, None]): a tuple, where first df is used for warning log table,\n            and the second df - for plots (could be also None)\n    \"\"\"\n    # fill in all string comnstants with relevant information\n    # based on variables, error_rate_over_average, error_rate_upper_variability, error_rate_upper_limit args\n    error_rate_measure_definition = error_rate_measure_definition_canva.format(variables=\"&amp;\".join(variables))\n\n    error_rate_cond_warn_over_average = error_rate_cond_warn_over_average_canva.format(\n        variables=\"&amp;\".join(variables), X=error_rate_over_average\n    )\n    error_rate_cond_warn_upper_variability = error_rate_cond_warn_upper_variability_canva.format(\n        variables=\"&amp;\".join(variables), SD=error_rate_upper_variability\n    )\n    error_rate_cond_warn_upper_limit = error_rate_cond_warn_upper_limit_canva.format(\n        variables=\"&amp;\".join(variables), X=error_rate_upper_limit\n    )\n    # qws of error rate by date is calculated based on previous days\n    if variables == [ColNames.date]:\n        window = Window.orderBy(ColNames.date).rowsBetween(-lookback_period_in_days, -1)\n    else:\n        # qws of error rate by date and other colum(s) is calculated over all \"data points\" of the same date\n        window = Window.partitionBy(ColNames.date)\n    # calculate error rate, a.k.a daily_value\n    df_qw = (\n        df_freq_distribution.groupBy(*variables)\n        .agg(\n            psf.sum(ColNames.initial_frequency).alias(\"sum_init_freq\"),\n            psf.sum(ColNames.final_frequency).alias(\"sum_final_freq\"),\n        )\n        .withColumn(\n            ColNames.daily_value,\n            (psf.col(\"sum_init_freq\") - psf.col(\"sum_final_freq\")) / psf.col(\"sum_init_freq\") * 100,\n        )\n    )\n    # using self.rate_common_qw funciton calculate three types of QWs\n    #   -&gt; OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT\n    # return a tuple of (df_log, df_plots | None)\n    qw_result = self.rate_common_qw(\n        df_qw,\n        window,\n        error_rate_upper_variability,\n        error_rate_over_average,\n        error_rate_upper_limit,\n        error_rate_measure_definition,\n        error_rate_cond_warn_upper_variability,\n        error_rate_cond_warn_over_average,\n        error_rate_cond_warn_upper_limit,\n        save_data_for_plots,\n    )\n\n    return qw_result\n</code></pre>"},{"location":"reference/components/quality/event_quality_warnings/event_quality_warnings/#components.quality.event_quality_warnings.event_quality_warnings.EventQualityWarnings.error_type_rate_qw","title":"<code>error_type_rate_qw(df_qa_by_column, df_freq_distribution, field_name, error_type, lookback_period_in_days, error_type_rate_over_average, error_type_rate_upper_variability, error_type_rate_upper_limit, error_type_rate_measure_definition_canva=f'{MeasureDefinitions.error_type_rate}', error_type_rate_cond_warn_over_average_canva=f'{Conditions.error_type_rate_over_average}-{Warnings.error_type_rate_over_average}', error_type_rate_cond_warn_upper_variability_canva=f'{Conditions.error_type_rate_upper_variability}-{Warnings.error_type_rate_upper_variability}', error_type_rate_cond_warn_upper_limit_canva=f'{Conditions.error_type_rate_upper_limit}-{Warnings.error_type_rate_upper_limit}')</code>","text":"<p>Prepare data for error type rate calculation. First fill in different string canvas, then based     on field name and error type calculate their corresponding error rate using formula:     number of errors of this error_type&amp;field_name combo / Total initial frequency *100 (BY DATE).     Parse preprocessed input along with window (which is a lookback period)     to self.rate_common_qw function which calculates three types of QWs:     OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT</p> <p>Parameters:</p> Name Type Description Default <code>df_qa_by_column</code> <code>DataFrame</code> <p>df with qa by column data.</p> required <code>df_freq_distribution</code> <code>DataFrame</code> <p>df with frequency data.</p> required <code>field_name</code> <code>str | None</code> <p>config param, the name of column of which to check error_type.</p> required <code>error_type</code> <code>str</code> <p>config param, the name of error type.</p> required <code>lookback_period_in_days</code> <code>int</code> <p>number of days prior to date of intrest.</p> required <code>error_type_rate_over_average</code> <code>Union[int, float]</code> <p>config param, specifies the upper limit over which daily value can not exceed its corresponding mean error rate.</p> required <code>error_type_rate_upper_variability</code> <code>Union[int, float]</code> <p>config param, the number of SD to define the upper varibaility limit: mean_rate + SD*error_type_rate_upper_variability, which daily value can not exceed</p> required <code>error_type_rate_upper_limit</code> <code>Union[int, float]</code> <p>absolute number which daily value can not exceed</p> required <code>error_type_rate_measure_definition_canva</code> <code>str</code> <p>canva text to use for measure_definition column (see measure_definition.py)</p> <code>f'{error_type_rate}'</code> <code>error_type_rate_cond_warn_over_average_canva</code> <code>str</code> <p>canva text to use for over_average cases of error_type_rate QWs (see conditions.py and warnings.py)</p> <code>f'{error_type_rate_over_average}-{error_type_rate_over_average}'</code> <code>error_type_rate_cond_warn_upper_variability_canva</code> <code>str</code> <p>canva text to use for upper_variability cases of error_type_rate QWs (see conditions.py and warnings.py)</p> <code>f'{error_type_rate_upper_variability}-{error_type_rate_upper_variability}'</code> <code>error_type_rate_cond_warn_upper_limit_canva</code> <code>str</code> <p>canva text to use for upper_limit cases of error_type_rate QWs (see conditions.py and warnings.py)</p> <code>f'{error_type_rate_upper_limit}-{error_type_rate_upper_limit}'</code> <p>Returns:     tuple(Union[DataFrame, None]): a tuple, where first df is used for warning log table,         and the second df - for plots, but since save_data_for_plots always False, output=None</p> Source code in <code>multimno/components/quality/event_quality_warnings/event_quality_warnings.py</code> <pre><code>def error_type_rate_qw(\n    self,\n    df_qa_by_column: DataFrame,\n    df_freq_distribution: DataFrame,\n    field_name: Union[str, None],\n    error_type: str,\n    lookback_period_in_days: int,\n    error_type_rate_over_average: Union[int, float],\n    error_type_rate_upper_variability: Union[int, float],\n    error_type_rate_upper_limit: Union[int, float],\n    error_type_rate_measure_definition_canva: str = f\"{MeasureDefinitions.error_type_rate}\",\n    error_type_rate_cond_warn_over_average_canva: str = f\"{Conditions.error_type_rate_over_average}-{Warnings.error_type_rate_over_average}\",\n    error_type_rate_cond_warn_upper_variability_canva: str = f\"{Conditions.error_type_rate_upper_variability}-{Warnings.error_type_rate_upper_variability}\",\n    error_type_rate_cond_warn_upper_limit_canva: str = f\"{Conditions.error_type_rate_upper_limit}-{Warnings.error_type_rate_upper_limit}\",\n) -&gt; Tuple[Union[DataFrame, None]]:\n    \"\"\"\n    Prepare data for error type rate calculation. First fill in different string canvas, then based\n        on field name and error type calculate their corresponding error rate using formula:\n        number of errors of this error_type&amp;field_name combo / Total initial frequency *100 (BY DATE).\n        Parse preprocessed input along with window (which is a lookback period)\n        to self.rate_common_qw function which calculates three types of QWs:\n        OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT\n\n\n    Args:\n        df_qa_by_column (DataFrame): df with qa by column data.\n        df_freq_distribution (DataFrame): df with frequency data.\n        field_name (str | None): config param, the name of column of which to check error_type.\n        error_type (str): config param, the name of error type.\n        lookback_period_in_days (int): number of days prior to date of intrest.\n        error_type_rate_over_average (Union[int, float]): config param, specifies the upper limit over which daily\n            value can not exceed its corresponding mean error rate.\n        error_type_rate_upper_variability (Union[int, float]): config param, the number of SD to define the upper\n            varibaility limit: mean_rate + SD*error_type_rate_upper_variability, which daily value can not exceed\n        error_type_rate_upper_limit (Union[int, float]): absolute number which daily value can not exceed\n        error_type_rate_measure_definition_canva (str): canva text to use for measure_definition\n            column (see measure_definition.py)\n        error_type_rate_cond_warn_over_average_canva (str): canva text to use for over_average cases of\n            error_type_rate QWs (see conditions.py and warnings.py)\n        error_type_rate_cond_warn_upper_variability_canva (str): canva text to use for upper_variability\n            cases of error_type_rate QWs (see conditions.py and warnings.py)\n        error_type_rate_cond_warn_upper_limit_canva (str): canva text to use for upper_limit cases of\n            error_type_rate QWs (see conditions.py and warnings.py)\n    Returns:\n        tuple(Union[DataFrame, None]): a tuple, where first df is used for warning log table,\n            and the second df - for plots, but since save_data_for_plots always False, output=None\n    \"\"\"\n    # based on error_type, error_type_rate_over_average, error_type_rate_upper_variability,\n    #  error_type_rate_upper_limit\n    # fill in string canvases\n    colname_error_type, error_type_qw_name = self.dict_error_type_info[error_type]\n\n    error_type_rate_measure_definition = error_type_rate_measure_definition_canva.format(\n        error_type_name=error_type_qw_name, field_name=str(field_name)\n    )\n\n    error_type_rate_cond_warn_over_average = error_type_rate_cond_warn_over_average_canva.format(\n        error_type_name=error_type_qw_name,\n        field_name=str(field_name),\n        X=error_type_rate_over_average,\n    )\n    error_type_rate_cond_warn_upper_variability = error_type_rate_cond_warn_upper_variability_canva.format(\n        error_type_name=error_type_qw_name,\n        field_name=str(field_name),\n        SD=error_type_rate_upper_variability,\n    )\n    error_type_rate_cond_warn_upper_limit = error_type_rate_cond_warn_upper_limit_canva.format(\n        error_type_name=error_type_qw_name,\n        field_name=str(field_name),\n        X=error_type_rate_upper_limit,\n    )\n    # for error_type that have more then one or applicable columns\n    # filter df_qa_by_column by field_name and error_type\n    if field_name is not None:\n        df_qa_by_column = df_qa_by_column.filter(\n            (psf.col(ColNames.variable) == field_name) &amp; (psf.col(ColNames.type_of_error) == colname_error_type)\n        ).select(ColNames.date, ColNames.value)\n    else:\n        # for error_types which technically do not belong specifically to one of event\n        # columns filter only by error_type (e.g. no_location error_type)\n        df_qa_by_column = df_qa_by_column.filter(psf.col(ColNames.type_of_error) == colname_error_type).select(\n            ColNames.date, ColNames.value\n        )\n    # calculate total daily initial frequency\n    df_freq_distribution = (\n        df_freq_distribution.groupby(ColNames.date)\n        .agg(psf.sum(ColNames.initial_frequency).alias(\"sum_init_freq\"))\n        .select(ColNames.date, \"sum_init_freq\")\n    )\n    # for each date combine two type of information number of errors and total daily initial frequency\n    df_combined = df_qa_by_column.join(df_freq_distribution, on=ColNames.date, how=\"inner\")\n    # for each date calculate error_type_rate, a.k.a daily_value\n    df_temp = df_combined.withColumn(\n        ColNames.daily_value,\n        (psf.col(ColNames.value) / psf.col(\"sum_init_freq\")) * 100,\n    )\n\n    # qws will be caluclated based on previous days\n    window = Window.orderBy(ColNames.date).rowsBetween(-lookback_period_in_days, -1)\n    # using self.rate_common_qw funciton calculate three types of QWs -&gt;\n    # OVER_AVERAGE, UPPER_VARIABILITY, and ABS_UPPER_LIMIT\n    # return a tuple of (df_log, None)\n    qw_result = self.rate_common_qw(\n        df_temp,\n        window,\n        error_type_rate_upper_variability,\n        error_type_rate_over_average,\n        error_type_rate_upper_limit,\n        error_type_rate_measure_definition,\n        error_type_rate_cond_warn_upper_variability,\n        error_type_rate_cond_warn_over_average,\n        error_type_rate_cond_warn_upper_limit,\n    )\n    return qw_result\n</code></pre>"},{"location":"reference/components/quality/event_quality_warnings/event_quality_warnings/#components.quality.event_quality_warnings.event_quality_warnings.EventQualityWarnings.rate_common_qw","title":"<code>rate_common_qw(df_temp, window, rate_upper_variability, rate_over_average, rate_upper_limit, measure_definition, cond_warn_upper_variability, cond_warn_over_average, cond_warn_upper_limit, save_data_for_plots=False)</code>","text":"<p>Take input df with \"daily_value\" column, and calculates three types of QWs: OVER_AVERAGE - for each row first based on specified window take mean of values, and then check if     daily_value exceeds mean by more than rate_over_average UPPER_VARIABILITY - for each row using already calculated mean compute upper variability limit which is     mean + SD*rate_upper_variability, check if  daily_value exceeds it ABS_UPPER_LIMIT - check if daily_value exceeds absolute number rate_upper_limit All three QWs depend on specified thresholds, if daily_value exceeds one of calculated values it will     store cond-warn-condition_value information in array column, some rows may have several QWs. In the end array column is exploded and cond-warn-condition_value     information is split into three corresponding columns. The function returns almost ready df for SilverEventDataSyntacticQualityWarningsLogTable DO, and based     on save_data_for_plots arg returns either almost ready data for plots or None</p> <p>Parameters:</p> Name Type Description Default <code>df_temp</code> <code>DataFrame</code> <p>temprory data that must have daily_value column to be used in further QW calculations</p> required <code>window</code> <code>Window</code> <p>a window within which perform aggregation</p> required <code>rate_upper_variability</code> <code>int | float</code> <p>config param, specifies the upper limit over which daily value  can not exceed its corresponding mean error rate</p> required <code>rate_over_average</code> <code>int | float</code> <p>config param, the number of SD to define the upper varibaility  limit: mean_rate + SD*error_rate_upper_variability, which daily value can not exceed</p> required <code>rate_upper_limit</code> <code>int | float</code> <p>absolute number which daily value can not exceed</p> required <code>measure_definition</code> <code>str</code> <p>canva text to use for measure_definition column (see measure_definition.py)</p> required <code>cond_warn_over_average</code> <code>str</code> <p>canva text to use for over_average cases (see conditions.py and warnings.py)</p> required <code>cond_warn_upper_variability</code> <code>str</code> <p>canva text to use for upper_variability cases (see conditions.py and warnings.py)</p> required <code>cond_warn_upper_limit</code> <code>str</code> <p>canva text to use for upper_limit cases (see conditions.py and warnings.py)</p> required <code>save_data_for_plots</code> <code>bool</code> <p>boolean, decide whether to store daily_value and its corresponding average and upper variability limit for plots. Defaults to False.</p> <code>False</code> <p>Returns:      tuple(Union[DataFrame, None]): a tuple, where first df is used for         warning log table, and the second df - for plots</p> Source code in <code>multimno/components/quality/event_quality_warnings/event_quality_warnings.py</code> <pre><code>def rate_common_qw(\n    self,\n    df_temp: DataFrame,\n    window: Window,\n    rate_upper_variability: Union[int, float],\n    rate_over_average: Union[int, float],\n    rate_upper_limit: Union[int, float],\n    measure_definition: str,\n    cond_warn_upper_variability: str,\n    cond_warn_over_average: str,\n    cond_warn_upper_limit: str,\n    save_data_for_plots: bool = False,\n) -&gt; Tuple[Union[DataFrame, None]]:\n    \"\"\"\n    Take input df with \"daily_value\" column, and calculates three types of QWs:\n    OVER_AVERAGE - for each row first based on specified window take mean of values, and then check if\n        daily_value exceeds mean by more than rate_over_average\n    UPPER_VARIABILITY - for each row using already calculated mean compute upper variability limit which is\n        mean + SD*rate_upper_variability, check if  daily_value exceeds it\n    ABS_UPPER_LIMIT - check if daily_value exceeds absolute number rate_upper_limit\n    All three QWs depend on specified thresholds, if daily_value exceeds one of calculated values it will\n        store cond-warn-condition_value information in array column,\n    some rows may have several QWs. In the end array column is exploded and cond-warn-condition_value\n        information is split into three corresponding columns.\n    The function returns almost ready df for SilverEventDataSyntacticQualityWarningsLogTable DO, and based\n        on save_data_for_plots arg returns either almost ready data for plots or None\n\n    Args:\n        df_temp (DataFrame): temprory data that must have daily_value column to\n            be used in further QW calculations\n        window (Window): a window within which perform aggregation\n        rate_upper_variability (int|float): config param, specifies the upper limit over which daily value\n             can not exceed its corresponding mean error rate\n        rate_over_average (int|float): config param, the number of SD to define the upper varibaility\n             limit: mean_rate + SD*error_rate_upper_variability, which daily value can not exceed\n        rate_upper_limit (int|float): absolute number which daily value can not exceed\n        measure_definition (str): canva text to use for measure_definition column (see measure_definition.py)\n        cond_warn_over_average (str): canva text to use for over_average cases (see conditions.py and warnings.py)\n        cond_warn_upper_variability (str): canva text to use for\n            upper_variability cases (see conditions.py and warnings.py)\n        cond_warn_upper_limit (str): canva text to use for upper_limit cases (see conditions.py and warnings.py)\n        save_data_for_plots (bool): boolean, decide whether to store daily_value and its corresponding average\n            and upper variability limit for plots. Defaults to False.\n    Returns:\n         tuple(Union[DataFrame, None]): a tuple, where first df is used for\n            warning log table, and the second df - for plots\n    \"\"\"\n    # prepare data\n    # calculate mean and std over window, and based on them UCL - for UPPER_VARIABILITY check\n    # ratio_perc - for OVER_AVERAGE check\n    # create empty array cond_warn_condition_value column to store inromation about qws\n    df_prep = df_temp.withColumns(\n        {\n            ColNames.average: psf.avg(ColNames.daily_value).over(window),\n            \"std_rate\": psf.stddev(ColNames.daily_value).over(window),\n            \"ratio_perc\": (psf.col(ColNames.daily_value) / psf.col(ColNames.average)) * 100,\n            ColNames.UCL: psf.col(ColNames.average) + rate_upper_variability * psf.col(\"std_rate\"),\n            \"cond_warn_condition_value\": psf.array(),\n        }\n    )\n    # if save_data_for_plots=True, add some new columns with constant values\n    # and select only applicable to SilverEventDataSyntacticQualityWarningsForPlots.SCHEMA\n    # else - return None\n    if save_data_for_plots:\n        df_prep = df_prep.cache()\n        df_plots = df_prep.withColumns(\n            {\n                ColNames.lookback_period: psf.lit(self.lookback_period),\n                ColNames.type_of_qw: psf.lit(\"error_rate\"),\n                ColNames.LCL: psf.lit(None).cast(\"float\"),\n            }\n        ).select(\n            self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsForPlots.ID].SCHEMA.fieldNames()\n        )\n    else:\n        df_plots = None\n\n    # continue with QWs checks\n    # first filter temp data by period from [data_period_start-lookback_period_in_days, data_period_end]\n    #  to [data_period_start, data_period_end]\n    # filter is aaplied after plot block because the first days of research period needs previous data to plot\n    df_qw = df_prep.filter(psf.col(ColNames.date) &gt;= self.data_period_start)\n    # perform UPPER_VARIABILITY, OVER_AVERAGE, and ABS_UPPER_LIMIT checks\n    # if condition is met store information about condition-warning_text-condition_value as a string into\n    # array column 'cond_warn_condition_value'\n    df_qw = (\n        df_qw.withColumn(\n            \"cond_warn_condition_value\",\n            psf.when(\n                psf.col(ColNames.daily_value) &gt; psf.col(ColNames.UCL),\n                psf.array_append(\n                    psf.col(\"cond_warn_condition_value\"),\n                    psf.concat(\n                        psf.lit(f\"{cond_warn_upper_variability}-\"),\n                        psf.col(ColNames.UCL).cast(\"string\"),\n                    ),\n                ),\n            ).otherwise(psf.col(\"cond_warn_condition_value\")),\n        )\n        .withColumn(\n            \"cond_warn_condition_value\",\n            psf.when(\n                psf.col(\"ratio_perc\") &gt; psf.lit(100 + rate_over_average),\n                psf.array_append(\n                    psf.col(\"cond_warn_condition_value\"),\n                    psf.lit(f\"{cond_warn_over_average}-{str(rate_over_average)}\"),\n                ),\n            ).otherwise(psf.col(\"cond_warn_condition_value\")),\n        )\n        .withColumn(\n            \"cond_warn_condition_value\",\n            psf.when(\n                psf.col(ColNames.daily_value) &gt; psf.lit(rate_upper_limit),\n                psf.array_append(\n                    psf.col(\"cond_warn_condition_value\"),\n                    psf.lit(f\"{cond_warn_upper_limit}-{str(rate_upper_limit)}\"),\n                ),\n            ).otherwise(psf.col(\"cond_warn_condition_value\")),\n        )\n    )\n    # explode array column 'cond_warn_condition_value'\n    df_qw = df_qw.withColumn(\n        \"cond_warn_condition_value\",\n        psf.explode(psf.col(\"cond_warn_condition_value\")),\n    )\n    # add some column constants\n    # split \"cond_warn_condition_value\" column into three: condition, wanring_text, conditon_value to match\n    # SilverEventDataSyntacticQualityWarningsLogTable.SCHEMA\n    # select only needed columns\n    df_qw = df_qw.withColumns(\n        {\n            ColNames.lookback_period: psf.lit(self.lookback_period),\n            ColNames.measure_definition: psf.lit(measure_definition),\n            ColNames.condition: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(0),\n            ColNames.warning_text: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(1),\n            ColNames.condition_value: psf.split(psf.col(\"cond_warn_condition_value\"), \"-\").getItem(2).cast(\"float\"),\n        }\n    ).select(self.output_qw_data_objects[SilverEventDataSyntacticQualityWarningsLogTable.ID].SCHEMA.fieldNames())\n\n    return (df_qw, df_plots)\n</code></pre>"},{"location":"reference/components/quality/event_quality_warnings/event_quality_warnings/#components.quality.event_quality_warnings.event_quality_warnings.EventQualityWarnings.save_quality_warnings_output","title":"<code>save_quality_warnings_output(dfs_qw, output_do)</code>","text":"<p>Concatenates all elements in dfs_qw list, adjustes to schema of output_do, and using write     method of output_do stores the result</p> <p>Parameters:</p> Name Type Description Default <code>dfs_qw</code> <code>list</code> <p>description</p> required <code>output_do</code> <code>SilverEventDataSyntacticQualityWarningsLogTable | SilverEventDataSyntacticQualityWarningsForPlots</code> <p>description</p> required Source code in <code>multimno/components/quality/event_quality_warnings/event_quality_warnings.py</code> <pre><code>def save_quality_warnings_output(\n    self,\n    dfs_qw: List[Union[DataFrame, None]],\n    output_do: Union[\n        SilverEventDataSyntacticQualityWarningsLogTable, SilverEventDataSyntacticQualityWarningsForPlots\n    ],\n):\n    \"\"\"\n    Concatenates all elements in dfs_qw list, adjustes to schema of output_do, and using write\n        method of output_do stores the result\n\n    Args:\n        dfs_qw (list): _description_\n        output_do (SilverEventDataSyntacticQualityWarningsLogTable | \\\n            SilverEventDataSyntacticQualityWarningsForPlots): _description_\n    \"\"\"\n\n    output_do.df = reduce(lambda x, y: x.union(y), dfs_qw)\n\n    output_do.df = self.spark.createDataFrame(output_do.df.rdd, output_do.SCHEMA)\n\n    output_do.write()\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/","title":"network_quality_warnings","text":""},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/","title":"network_quality_warnings","text":"<p>Module that generates the quality warnings associated to the syntactic checks/cleaning of the raw MNO Network Topology Data.</p>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings","title":"<code>NetworkQualityWarnings</code>","text":"<p>               Bases: <code>Component</code></p> <p>Class that produces the log tables and data required for plotting associated to Network Topology Data cleaning/syntactic checks.</p> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>class NetworkQualityWarnings(Component):\n    \"\"\"\n    Class that produces the log tables and data required for plotting associated to Network Topology Data\n    cleaning/syntactic checks.\n    \"\"\"\n\n    COMPONENT_ID = \"NetworkQualityWarnings\"\n\n    PERIOD_DURATION = {\"week\": 7, \"month\": 30, \"quarter\": 90}\n\n    TITLE = \"MNO Network Topology Data Quality Warnings\"\n\n    MEASURE_DEFINITION = {\n        \"SIZE_RAW_DATA\": \"Value of the size of the raw data object\",\n        \"SIZE_CLEAN_DATA\": \"Value of the size of the clean data object\",\n        \"TOTAL_ERROR_RATE\": \"Error rate\",\n        \"Missing_value_RATE\": \"Missing rate value of {field_name}\".format,\n        \"Out_of_range_RATE\": \"Out of range rate of {field_name}\".format,\n        \"Parsing_error_RATE\": \"Parsing error rate of {field_name}\".format,\n    }\n\n    ERROR_TYPE = {\n        \"Missing_value_RATE\": NetworkErrorType.NULL_VALUE,\n        \"Out_of_range_RATE\": NetworkErrorType.OUT_OF_RANGE,\n        \"Parsing_error_RATE\": NetworkErrorType.CANNOT_PARSE,\n    }\n\n    CONDITION = {\n        \"Missing_value_RATE\": {\n            \"AVERAGE\": \"Missing value rate of {field_name} is over the previous period average by more than {value} %\".format,\n            \"VARIABILITY\": (\n                \"Missing value rate of {field_name} is over the upper control limit calculated on the basis of average and standard deviation \"\n                + \"of the distribution of the missing value rate of {field_name} in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\"\n            ).format,\n            \"ABS_VALUE_UPPER_LIMIT\": \"Missing value rate of {field_name} is over the value {value}\".format,\n        },\n        \"Out_of_range_RATE\": {\n            \"AVERAGE\": \"Out of range rate of {field_name} is over the previous period average by more than {value} %\".format,\n            \"VARIABILITY\": (\n                \"Out of range rate of {field_name} is over the upper control limit calculated on the basis of average and standard deviation \"\n                + \"of the distribution of the out of range rate of {field_name} in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\"\n            ).format,\n            \"ABS_VALUE_UPPER_LIMIT\": \"Out of range rate of {field_name} is over the value {value} %\".format,\n        },\n        \"Parsing_error_RATE\": {\n            \"AVERAGE\": \"Parsing error rate of {field_name} is over the previous period average by more than {value} %\".format,\n            \"VARIABILITY\": (\n                \"Parsing error rate of {field_name} is over the upper control limit calculated on the basis of average and standard deviation \"\n                + \"of the distribution of the parsing error rate of {field_name} in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\"\n            ).format,\n            \"ABS_VALUE_UPPER_LIMIT\": \"Parsing error rate of {field_name} is over the value {value}\".format,\n        },\n    }\n\n    WARNING_MESSAGE = {\n        \"Missing_value_RATE\": {\n            \"AVERAGE\": \"The missing value rate of {field_name} after the syntactic check procedure is unexpectedly high with respect to the previous period.\".format,\n            \"VARIABILITY\": (\n                \"The missing value rate of {field_name} after the syntactic check procedure is unexpectedly high \"\n                + \"with respect to previous period taking into account its usual variability.\"\n            ).format,\n            \"ABS_VALUE_UPPER_LIMIT\": \"The missing value rate of {field_name} is over the threshold\".format,\n        },\n        \"Out_of_range_RATE\": {\n            \"AVERAGE\": \"The out of range rate of {field_name} after the syntactic check procedure is unexpectedly high with respect to the previous period.\".format,\n            \"VARIABILITY\": (\n                \"The out of range of {field_name} after the syntactic check procedure is unexpectedly high \"\n                + \"with respect to previous period taking into account its usual variability.\"\n            ).format,\n            \"ABS_VALUE_UPPER_LIMIT\": \"The out of range rate of {field_name} is over the threshold\".format,\n        },\n        \"Parsing_error_RATE\": {\n            \"AVERAGE\": \"The parsing error rate of {field_name} after the syntactic check procedure is unexpectedly high with respect to the previous period.\".format,\n            \"VARIABILITY\": (\n                \"The parsing error of {field_name} after the syntactic check procedure is unexpectedly high \"\n                + \"with respect to previous period taking into account its usual variability.\"\n            ).format,\n            \"ABS_VALUE_UPPER_LIMIT\": \"The parsing error rate of {field_name} is over the threshold\".format,\n        },\n    }\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.timestamp = datetime.datetime.now()\n\n        # Read date of study\n        self.date_format = self.config.get(self.COMPONENT_ID, \"date_format\")\n        date_of_study = self.config.get(self.COMPONENT_ID, \"date_of_study\")\n\n        try:\n            self.date_of_study = datetime.datetime.strptime(date_of_study, self.date_format).date()\n        except ValueError as e:\n            self.logger.error(str(e), exc_info=True)\n            raise e\n\n        # Read lookback period\n        self.lookback_period = self.config.get(self.COMPONENT_ID, \"lookback_period\")\n\n        if self.lookback_period not in [\"week\", \"month\", \"quarter\"]:\n            error_msg = (\n                \"Configuration parameter `lookback_period` must be one of `week`, `month`, or `quarter`, \"\n                f\"but {self.lookback_period} was passed\"\n            )\n\n            self.logger.error(error_msg)\n            raise ValueError(error_msg)\n\n        self.lookback_dates = [\n            self.date_of_study - datetime.timedelta(days=d)\n            for d in range(1, self.PERIOD_DURATION[self.lookback_period] + 1)\n        ]\n\n        self.lookback_period_start = min(self.lookback_dates)\n        self.lookback_period_end = max(self.lookback_dates)\n\n        # Read thresholds and use read values instead of default ones when appropriate\n        self.thresholds = self.get_thresholds()\n\n        self.warnings = []\n\n        self.plots_data = dict()\n\n    def get_thresholds(self) -&gt; dict:\n        \"\"\"\n        Method that reads the threshold-related parameters, contained in the config file,\n        and saves them in memory to use them instead of the ones specified by default.\n        Raises:\n            ValueError: non-numerical value that cannot be parsed to float has been used in\n                the config file\n            ValueError: Negative value for a given parameter has been given, when only\n                non-negative values make sense and are thus allowed.\n\n        Returns:\n            thresholds: dict containing the different thresholds used for computing the\n                quality warnings, with the same structure as\n                multimno.core.constants.network_default_thresholds.NETWORK_DEFAULT_THRESHOLDS\n        \"\"\"\n        config_thresholds = self.config.geteval(self.COMPONENT_ID, \"thresholds\")\n        thresholds = NETWORK_DEFAULT_THRESHOLDS\n\n        for error_key in config_thresholds.keys():\n            if error_key not in NETWORK_DEFAULT_THRESHOLDS:\n                self.logger.info(f\"Parameter key {error_key} unknown -- ignored\")\n                continue\n\n            if error_key in [\"SIZE_RAW_DATA\", \"SIZE_CLEAN_DATA\", \"TOTAL_ERROR_RATE\"]:\n                for param_key, val in config_thresholds[error_key].items():\n                    if param_key not in NETWORK_DEFAULT_THRESHOLDS[error_key]:\n                        self.logger.info(f\"Unknown parameter {param_key} under {error_key} -- ignored\")\n                        continue\n\n                    try:\n                        val = float(val)\n                    except ValueError as e:\n                        error_msg = (\n                            f\"Expected numeric value for parameter {param_key} under {error_key} - got {val} instead.\"\n                        )\n                        self.logger.error(error_msg)\n                        raise e\n\n                    if val &lt; 0:\n                        error_msg = f\"Parameter {param_key} under {error_key} must be non-negative - got {val}\"\n                        self.logger.error(error_msg)\n                        raise ValueError(error_msg)\n\n                    thresholds[error_key][param_key] = val\n\n            else:  # then is one of 'Missing_value_RATE', 'Out_of_range_RATE', 'Parsing_error_RATE'\n                for field_name in config_thresholds[error_key]:\n                    if field_name not in NETWORK_DEFAULT_THRESHOLDS[error_key]:\n                        self.logger.info(f\"Unknown field name {field_name} under {error_key} -- ignored\")\n                        continue\n\n                    for param_key, val in config_thresholds[error_key][field_name].items():\n                        if param_key not in NETWORK_DEFAULT_THRESHOLDS[error_key][field_name]:\n                            self.logger.info(\n                                f'Unknown parameter {param_key} under {error_key}[\"{field_name}\"] -- ignored'\n                            )\n                            continue\n\n                        try:\n                            val = float(val)\n                        except ValueError as e:\n                            error_msg = f'Expected numeric value for parameter {param_key} under {error_key}[\"{field_name}\"] - got {val} instead.'\n                            self.logger.error(error_msg)\n                            raise e\n\n                        if val &lt; 0:\n                            error_msg = f'Parameter {param_key} under {error_key}[\"{field_name}\"] must be non-negative - got {val}'\n                            self.logger.error(error_msg)\n                            raise ValueError(error_msg)\n\n                        thresholds[error_key][field_name][param_key] = val\n\n        return thresholds\n\n    def initalize_data_objects(self):\n        input_silver_quality_metrics_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"network_syntactic_quality_metrics_by_column\"\n        )\n        output_silver_log_table_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"network_syntactic_quality_warnings_log_table\"\n        )\n\n        output_silver_line_plot_data_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"network_syntactic_quality_warnings_line_plot_data\"\n        )\n\n        output_silver_pie_plot_data_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"network_syntactic_quality_warnings_pie_plot_data\"\n        )\n\n        silver_quality_metrics = SilverNetworkDataQualityMetricsByColumn(self.spark, input_silver_quality_metrics_path)\n\n        silver_log_table = SilverNetworkDataSyntacticQualityWarningsLogTable(self.spark, output_silver_log_table_path)\n\n        silver_line_plot_data = SilverNetworkSyntacticQualityWarningsLinePlotData(\n            self.spark, output_silver_line_plot_data_path\n        )\n\n        silver_pie_plot_data = SilverNetworkSyntacticQualityWarningsPiePlotData(\n            self.spark, output_silver_pie_plot_data_path\n        )\n\n        self.input_data_objects = {silver_quality_metrics.ID: silver_quality_metrics}\n        self.output_data_objects = {\n            silver_log_table.ID: silver_log_table,\n            silver_line_plot_data.ID: silver_line_plot_data,\n            silver_pie_plot_data.ID: silver_pie_plot_data,\n        }\n\n    def transform(self):\n\n        # Check if both the date of study and the specified lookback period dates are in file\n        self.check_needed_dates()\n\n        lookback_stats, lookback_initial_rows, lookback_final_rows = self.get_lookback_period_statistics()\n\n        today_values = self.get_study_date_values()\n\n        raw_average, raw_UCL, raw_LCL = self.raw_size_warnings(lookback_stats, today_values)\n\n        clean_average, clean_UCL, clean_LCL = self.clean_size_warnings(lookback_stats, today_values)\n\n        error_rate, error_rate_avg, error_rate_UCL = self.error_rate_warnings(\n            lookback_initial_rows, lookback_final_rows, today_values\n        )\n\n        self.all_specific_error_warnings(lookback_stats, today_values)\n\n        self.output_data_objects[SilverNetworkDataSyntacticQualityWarningsLogTable.ID].df = self.spark.createDataFrame(\n            self.warnings, SilverNetworkDataSyntacticQualityWarningsLogTable.SCHEMA\n        )\n\n        lookback_initial_rows[self.date_of_study] = today_values[None][NetworkErrorType.INITIAL_ROWS]\n        lookback_final_rows[self.date_of_study] = today_values[None][NetworkErrorType.FINAL_ROWS]\n\n        self.create_plots_data(\n            lookback_initial_rows=lookback_initial_rows,\n            lookback_final_rows=lookback_final_rows,\n            today_values=today_values,\n            error_rate=error_rate,\n            raw_average=raw_average,\n            clean_average=clean_average,\n            error_rate_avg=error_rate_avg,\n            raw_UCL=raw_UCL,\n            clean_UCL=clean_UCL,\n            error_rate_UCL=error_rate_UCL,\n            raw_LCL=raw_LCL,\n            clean_LCL=clean_LCL,\n        )\n\n    def check_needed_dates(self) -&gt; None:\n        \"\"\"\n        Method that checks if both the date of study and the dates necessary to generate\n        the quality warnings, specified through the lookback_period parameter, are present\n        in the input data.\n        \"\"\"\n\n        # Collect all distinct dates in the input quality metrics within the needed range\n        metrics = self.input_data_objects[SilverNetworkDataQualityMetricsByColumn.ID].df\n        dates = (\n            metrics.filter(\n                F.col(\"date\")\n                # left- and right- inclusive\n                .between(\n                    self.date_of_study - datetime.timedelta(days=self.PERIOD_DURATION[self.lookback_period]),\n                    self.date_of_study,\n                )\n            )\n            .select(F.col(ColNames.date))\n            .distinct()\n            .collect()\n        )\n\n        dates = [row[ColNames.date] for row in dates]\n\n        if self.date_of_study not in dates:\n            raise ValueError(\n                f\"The date of study, {self.date_of_study}, is not present in the input Quality Metrics data\"\n            )\n\n        if set(self.lookback_dates).intersection(set(dates)) != set(self.lookback_dates):\n            error_msg = f\"\"\"\n                The following dates from the lookback period are not present in the\n                input Quality Metrics data:\n                {\n                    sorted(\n                        map(\n                            lambda x: x.strftime(self.date_format),\n                            set(self.lookback_dates).difference(set(dates))\n                        )\n                    )\n                }\"\"\"\n\n            self.logger.error(error_msg)\n            raise ValueError(error_msg)\n\n    def get_lookback_period_statistics(self) -&gt; dict:\n        \"\"\"Method that computes the necessary statistics (average and standard deviation) from the\n        Quality Metrics of the lookback period.\n\n        Returns:\n            statistics (dict): dictionary containing said necessary statistics, with the following\n                structure:\n                {\n                    field_name1: {\n                        type_error1: {\n                            'average': 12.2,\n                            'stddev': 17.5\n                        },\n                        type_error2 : {\n                            'average': 4.5,\n                            'stddev': 10.1\n                        }\n                    },\n                    ...\n                }\n            initial_rows (dict): dictionary indicating the number of rows before syntactic checks for the previous period, {date -&gt; size_raw_data}\n            final_rows (dict): dictionary indicating the number of rows after syntactic checks for the previous period, {date -&gt; size_raw_data}\n        \"\"\"\n        intermediate_df = self.input_data_objects[SilverNetworkDataQualityMetricsByColumn.ID].df.filter(\n            F.col(ColNames.date).between(self.lookback_period_start, self.lookback_period_end)\n        )\n\n        intermediate_df.cache()\n\n        lookback_stats = (\n            intermediate_df.groupBy([ColNames.field_name, ColNames.type_code])\n            .agg(F.mean(F.col(ColNames.value)).alias(\"average\"), F.stddev_samp(F.col(ColNames.value)).alias(\"stddev\"))\n            .collect()\n        )\n\n        error_rate_data = (\n            intermediate_df.select([ColNames.type_code, ColNames.value, ColNames.date])\n            .filter(F.col(ColNames.type_code).isin([NetworkErrorType.INITIAL_ROWS, NetworkErrorType.FINAL_ROWS]))\n            .collect()\n        )\n\n        intermediate_df.unpersist()\n\n        initial_rows = {}\n        final_rows = {}\n\n        for row in error_rate_data:\n            if row[ColNames.type_code] == NetworkErrorType.INITIAL_ROWS:\n                initial_rows[row[ColNames.date]] = row[\"value\"]\n            else:\n                final_rows[row[ColNames.date]] = row[\"value\"]\n\n        statistics = dict()\n        for row in lookback_stats:\n            field_name, type_code, average, stddev = (\n                row[ColNames.field_name],\n                row[ColNames.type_code],\n                row[\"average\"],\n                row[\"stddev\"],\n            )\n            if field_name not in statistics:\n                statistics[field_name] = dict()\n            statistics[field_name][type_code] = {\"average\": average, \"stddev\": stddev}\n\n        return statistics, initial_rows, final_rows\n\n    def get_study_date_values(self) -&gt; dict:\n        \"\"\"Method that reads and returns the quality metrics of the date of study.\n\n        Returns:\n            today_values (dict): dictionary containing said values, with the following\n                structure:\n                {\n                    field_name1: {\n                        type_error1: 123,\n                        type_error2: 23,\n                        type_error3: 0\n                    },\n                    field_name2: {\n                        type_error1: 0,\n                        type_error2: 0,\n                        type_error3: 300\n                    },\n                }\n        \"\"\"\n        today_metrics = (\n            self.input_data_objects[SilverNetworkDataQualityMetricsByColumn.ID]\n            .df.filter(F.col(ColNames.date) == F.lit(self.date_of_study))\n            .select([ColNames.field_name, ColNames.type_code, ColNames.value])\n        ).collect()\n\n        today_values = {}\n\n        for row in today_metrics:\n            field_name, type_code, value = row[ColNames.field_name], row[ColNames.type_code], row[ColNames.value]\n\n            if field_name not in today_values:\n                today_values[field_name] = {}\n\n            today_values[field_name][type_code] = value\n\n        return today_values\n\n    def register_warning(\n        self, measure_definition: str, daily_value: float, condition: str, condition_value: float, warning_text: str\n    ) -&gt; None:\n        \"\"\"Method that abstracts away the creation in the correct format and data type, each of the quality warnings\n        that will be recorded in the log table.\n\n        Args:\n            measure_definition (str): measure that raised the warning (e.g. Error rate)\n            daily_value (float): measure's value in the date of study that raised the warning\n            condition (str): test that was checked in order to raise the warning\n            condition_value (float): value against which the date of study's daily_value was compared\n            warning_text (str): verbose explanation of the condition being satisfied and the warning\n                being raised\n        \"\"\"\n        warning = {\n            ColNames.title: self.TITLE,\n            ColNames.date: self.date_of_study,\n            ColNames.timestamp: self.timestamp,\n            ColNames.measure_definition: measure_definition,\n            ColNames.daily_value: float(daily_value),\n            ColNames.condition: condition,\n            ColNames.lookback_period: self.lookback_period,\n            ColNames.condition_value: float(condition_value),\n            ColNames.warning_text: warning_text,\n            ColNames.year: self.date_of_study.year,\n            ColNames.month: self.date_of_study.month,\n            ColNames.day: self.date_of_study.day,\n        }\n\n        self.warnings.append(Row(**warning))\n\n    def raw_size_warnings(self, lookback_stats, today_values):\n        \"\"\"Method that generates the quality warnings, that will be recorded in the output log table,\n        for the metric regarding the initial number of rows of the raw input network topology data prior\n        to the syntactic check procedure.\n\n        A total of six warnings might be generated:\n            - The study date's number of rows is greater than the average number of rows over the previous period\n                by more than a threshold percentage.\n            - The study date's number of rows is smaller than the average number of rows over the previous period\n                by more than a threshold percentage.\n            - The study date's number of rows is greater than the average number of rows over the previous period\n                by a config-specified number of standard deviations.\n            - The study date's number of rows is smaller than the average number of rows over the previous period\n                by a config-specified number of standard deviations.\n            - The study date's number of rows is greater than a config-specified threshold.\n            - The study date's number of rows is smaller than a config-specified threshold.\n        \"\"\"\n        current_val = today_values[None][NetworkErrorType.INITIAL_ROWS]\n        previous_avg = lookback_stats[None][NetworkErrorType.INITIAL_ROWS][\"average\"]\n        previous_std = lookback_stats[None][NetworkErrorType.INITIAL_ROWS][\"stddev\"]\n\n        measure_definition = self.MEASURE_DEFINITION[\"SIZE_RAW_DATA\"]\n\n        over_average = self.thresholds[\"SIZE_RAW_DATA\"][\"OVER_AVERAGE\"]\n        under_average = self.thresholds[\"SIZE_RAW_DATA\"][\"UNDER_AVERAGE\"]\n        variability = self.thresholds[\"SIZE_RAW_DATA\"][\"VARIABILITY\"]\n        absolute_upper_control_limit = self.thresholds[\"SIZE_RAW_DATA\"][\"ABS_VALUE_UPPER_LIMIT\"]\n        absolute_lower_control_limit = self.thresholds[\"SIZE_RAW_DATA\"][\"ABS_VALUE_LOWER_LIMIT\"]\n\n        if previous_avg == 0:\n            self.logger.warning(\n                f\"\"\"\n                The average of the number of rows in the raw input network topology data is 0\n                for the lookback period especified ({self.lookback_period_start} to {self.lookback_period_end},\n                 both included)\n                \"\"\"\n            )\n        else:\n            pct_difference = 100 * current_val / previous_avg - 100\n            if pct_difference &gt; over_average:\n                self.register_warning(\n                    measure_definition,\n                    pct_difference,\n                    f\"Size is greater than the previous period's average by more than {over_average} %\",\n                    over_average,\n                    \"The number of cells is unexpectedly high with respect to the previous period, please check if there have been issues in the network\",\n                )\n\n            if pct_difference &lt; -under_average:\n                self.register_warning(\n                    measure_definition,\n                    pct_difference,\n                    f\"Size is smaller than the previous period's average by more than {under_average} %\",\n                    under_average,\n                    \"The number of cells is unexpectedly low with respect to the previous period, please check if there have been issues in the network.\",\n                )\n\n        upper_control_limit = previous_avg + variability * previous_std\n        lower_control_limit = previous_avg - variability * previous_std\n\n        if current_val &gt; upper_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                \"Size is out of the upper control limit calculated on the basis of average and standard deviation of the distribution \"\n                f\"of the size in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\",\n                upper_control_limit,\n                \"The number of cells is unexpectedly high with respect to the previous period, taking into account the usual \"\n                \"variability of the cell numbers, please check if there have been issues in the network.\",\n            )\n        if current_val &lt; lower_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                \"Size is out of the lower control limit calculated on the basis of average and standard deviation of the distribution \"\n                f\"of the size in the previous period. Lower control limit = (average - {variability}\u00b7stddev)\",\n                lower_control_limit,\n                \"The number of cells is unexpectedly low with respect to the previous period, taking into account the usual \"\n                \"variability of the cell numbers, please check if there have been issues in the network.\",\n            )\n\n        # Use of UCL and LCL as default values for the absolute limits\n        if absolute_upper_control_limit is None:\n            absolute_upper_control_limit = upper_control_limit\n        if absolute_lower_control_limit is None:\n            absolute_lower_control_limit = lower_control_limit\n\n        if current_val &gt; absolute_upper_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                f\"The size is over the threshold {absolute_upper_control_limit}\",\n                absolute_upper_control_limit,\n                \"The number of cells is over the threshold, please check if there have been changes in the network.\",\n            )\n        if current_val &lt; absolute_lower_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                f\"The size is under the threshold {absolute_lower_control_limit}\",\n                absolute_upper_control_limit,\n                \"The number of cells is under the threshold, please check if there have been changes in the network.\",\n            )\n\n        return previous_avg, upper_control_limit, lower_control_limit\n\n    def clean_size_warnings(self, lookback_stats, today_values):\n        \"\"\"Method that generates quality warnings, that will be recorded in the output log table,\n        for the metric regarding the final number of rows of the raw input network topology data prior\n        to the syntactic check procedure.\n\n        A total of six warnings might be generated:\n            - The study date's number of rows is greater than the average number of rows over the previous period\n                by more than a threshold percentage.\n            - The study date's number of rows is smaller than the average number of rows over the previous period\n                by more than a threshold percentage.\n            - The study date's number of rows is greater than the average number of rows over the previous period\n                by a config-specified number of standard deviations.\n            - The study date's number of rows is smaller than the average number of rows over the previous period\n                by a config-specified number of standard deviations.\n            - The study date's number of rows is greater than a config-specified threshold.\n            - The study date's number of rows is smaller than a config-specified threshold.\n        \"\"\"\n        current_val = today_values[None][NetworkErrorType.FINAL_ROWS]\n        previous_avg = lookback_stats[None][NetworkErrorType.FINAL_ROWS][\"average\"]\n        previous_std = lookback_stats[None][NetworkErrorType.FINAL_ROWS][\"stddev\"]\n\n        measure_definition = self.MEASURE_DEFINITION[\"SIZE_CLEAN_DATA\"]\n\n        over_average = self.thresholds[\"SIZE_CLEAN_DATA\"][\"OVER_AVERAGE\"]\n        under_average = self.thresholds[\"SIZE_CLEAN_DATA\"][\"UNDER_AVERAGE\"]\n        variability = self.thresholds[\"SIZE_CLEAN_DATA\"][\"VARIABILITY\"]\n        absolute_upper_control_limit = self.thresholds[\"SIZE_CLEAN_DATA\"][\"ABS_VALUE_UPPER_LIMIT\"]\n        absolute_lower_control_limit = self.thresholds[\"SIZE_CLEAN_DATA\"][\"ABS_VALUE_LOWER_LIMIT\"]\n\n        if previous_avg == 0:\n            self.logger.warning(\n                f\"\"\"\n                The average of the number of rows in the clean input network topology data after syntactic checks is 0\n                for the lookback period especified ({self.lookback_period_start} to {self.lookback_period_end}, \n                both included)\n                \"\"\"\n            )\n        else:\n            pct_difference = 100 * current_val / previous_avg - 100\n            if pct_difference &gt; over_average:\n                self.register_warning(\n                    measure_definition,\n                    pct_difference,\n                    f\"Size is greater than the previous period's average by {over_average} %\",\n                    over_average,\n                    \"The number of cells after the syntactic checks procedure is unexpectedly high with respect to the previous period.\",\n                )\n\n            if pct_difference &lt; -under_average:\n                self.register_warning(\n                    measure_definition,\n                    pct_difference,\n                    f\"Size is smaller than the previous period's average by {under_average} %\",\n                    under_average,\n                    \"The number of cells after the syntactic checks procedure is unexpectedly low with respect to the previous period.\",\n                )\n\n        upper_control_limit = previous_avg + variability * previous_std\n        lower_control_limit = previous_avg - variability * previous_std\n\n        if current_val &gt; upper_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                \"Size is out of the upper control limit calculated on the basis of average and standard deviation of the distribution \"\n                f\"of the size in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\",\n                upper_control_limit,\n                \"The number of cells is unexpectedly high with respect to the previous period, taking into account the usual \"\n                \"variability of the cell numbers, please check if there have been issues in the network.\",\n            )\n        if current_val &lt; lower_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                \"Size is out of the lower control limit calculated on the basis of average and standard deviation of the distribution \"\n                f\"of the size in the previous period. Lower control limit = (average - {variability}\u00b7stddev)\",\n                lower_control_limit,\n                \"The number of cells is unexpectedly low with respect to the previous period, taking into account the usual \"\n                \"variability of the cell numbers, please check if there have been issues in the network.\",\n            )\n\n        # Use of UCL and LCL as default values for the absolute limits\n        if absolute_upper_control_limit is None:\n            absolute_upper_control_limit = upper_control_limit\n        if absolute_lower_control_limit is None:\n            absolute_lower_control_limit = lower_control_limit\n\n        if current_val &gt; absolute_upper_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                f\"The size is over the threshold {absolute_upper_control_limit}\",\n                absolute_upper_control_limit,\n                \"The number of cells after the syntactic checks procedure is over the threshold.\",\n            )\n        if current_val &lt; absolute_lower_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                f\"The size is under the threshold {absolute_lower_control_limit}\",\n                absolute_upper_control_limit,\n                \"The number of cells after the syntactic checks procedure is under the threshold.\",\n            )\n\n        return previous_avg, upper_control_limit, lower_control_limit\n\n    def error_rate_warnings(self, initial_rows, final_rows, today_values):\n        \"\"\"Method that generates quality warnings, that will be recorded in the output log table,\n        for the metric regarding the error rate observed in the syntactic check procedure.\n\n        A total of three warnings might be generated:\n            - The study date's error rate is greater than the average number of rows over the previous period\n                by more than a threshold percentage.\n            - The study date's error rate is greater than the average number of rows over the previous period\n                by a config-specified number of standard deviations.\n            - The study date's error rate is greater than a config-specified threshold.\n        \"\"\"\n        if len(initial_rows) != len(final_rows):\n            raise ValueError(\n                \"Input Quality Metrics do not have information on the number of rows \"\n                \"before and after syntactic checks on all dates considered!\"\n            )\n\n        error_rate = {\n            date: 100 * (initial_rows[date] - final_rows[date]) / initial_rows[date] for date in initial_rows.keys()\n        }\n\n        current_val = (\n            100\n            * (today_values[None][NetworkErrorType.INITIAL_ROWS] - today_values[None][NetworkErrorType.FINAL_ROWS])\n            / today_values[None][NetworkErrorType.INITIAL_ROWS]\n        )\n        previous_avg = sum(error_rate[date] for date in self.lookback_dates) / len(self.lookback_dates)\n        previous_std = math.sqrt(\n            sum((error_rate[date] - previous_avg) ** 2 for date in self.lookback_dates) / (len(self.lookback_dates) - 1)\n        )\n\n        measure_definition = self.MEASURE_DEFINITION[\"TOTAL_ERROR_RATE\"]\n\n        over_average = self.thresholds[\"TOTAL_ERROR_RATE\"][\"OVER_AVERAGE\"]\n        variability = self.thresholds[\"TOTAL_ERROR_RATE\"][\"VARIABILITY\"]\n        absolute_upper_control_limit = self.thresholds[\"TOTAL_ERROR_RATE\"][\"ABS_VALUE_UPPER_LIMIT\"]\n\n        if previous_avg == 0:\n            self.logger.warning(\n                f\"\"\"\n                The average error rate in the input network topology data 0\n                for the lookback period especified ({self.lookback_period_start} to {self.lookback_period_end}, \n                both included)\n                \"\"\"\n            )\n        else:\n            pct_difference = 100 * current_val / previous_avg - 100\n            if pct_difference &gt; over_average:\n                self.register_warning(\n                    measure_definition,\n                    pct_difference,\n                    f\"Error rate is greater than the previous period's average by more than {over_average} %\",\n                    over_average,\n                    \"The error rate after the syntactic checks procedure is unexpectedly high with respect to the previous period.\",\n                )\n        upper_control_limit = previous_avg + variability * previous_std\n\n        if current_val &gt; upper_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                \"Error rate is over the upper control limit calculated on the basis of average and standard deviation of the distribution \"\n                f\"of the size in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\",\n                upper_control_limit,\n                \"The error rate after the syntactic checks procedure is unexpectedly high with respect to the previous period, taking into account the usual \"\n                \"variability.\",\n            )\n\n        if current_val &gt; absolute_upper_control_limit:\n            self.register_warning(\n                measure_definition,\n                current_val,\n                f\"The error rate is over the value {absolute_upper_control_limit}\",\n                absolute_upper_control_limit,\n                \"The error rate after the syntactic checks procedure is over the threshold.\",\n            )\n        error_rate[self.date_of_study] = current_val\n        return error_rate, previous_avg, upper_control_limit\n\n    def all_specific_error_warnings(self, lookback_stats, today_values):\n        \"\"\"Parent method for the creation of warnings for each type of error rate\n\n        lookback_stats (dict): contains error information of each date of the lookback period\n        today_values (dict): contains error information of the date of study\n        \"\"\"\n        error_rate_types = [\"Missing_value_RATE\", \"Out_of_range_RATE\", \"Parsing_error_RATE\"]\n\n        for error_rate_type in error_rate_types:\n            for field_name in self.thresholds[error_rate_type]:\n                self.specific_error_warnings(error_rate_type, field_name, lookback_stats, today_values)\n\n    def specific_error_warnings(self, error_rate_type, field_name, lookback_stats, today_values):\n        \"\"\"Method that generates the quality warnings, that will be recorded in the output log table,\n        for the metric regarding a specific error type considered in the network syntactic checks.\n\n        A total of three warnings might be generated:\n            - The study date's error rate for this error and this field is greater than the average number of rows over the previous period\n                by more than a threshold percentage.\n            - The study date's error rate for this error and this field is greater than the average number of rows over the previous period\n                by a config-specified number of standard deviations.\n            - The study date's error rate for this error and this field is greater than a config-specified threshold.\n        \"\"\"\n        if error_rate_type not in self.ERROR_TYPE:\n            raise ValueError(f\"Unknown error type for quality warning `{error_rate_type}`\")\n\n        network_error_type = self.ERROR_TYPE[error_rate_type]\n\n        over_average = self.thresholds[error_rate_type][field_name][\"AVERAGE\"]\n        variability = self.thresholds[error_rate_type][field_name][\"VARIABILITY\"]\n        absolute_upper_control_limit = self.thresholds[error_rate_type][field_name][\"ABS_VALUE_UPPER_LIMIT\"]\n\n        if network_error_type == NetworkErrorType.OUT_OF_RANGE and field_name is None:\n            field_name = \"dates\"\n        current_val = today_values[field_name][network_error_type]\n        previous_avg = lookback_stats[field_name][network_error_type][\"average\"]\n        previous_std = lookback_stats[field_name][network_error_type][\"stddev\"]\n\n        if previous_avg == 0:\n            self.logger.warning(\n                f\"\"\"\n                The average {error_rate_type} for field {field_name} in the input network topology data is 0 for the lookback period\n                especified ({self.lookback_period_start} to {self.lookback_period_end}, both included)\n                \"\"\"\n            )\n        else:\n            pct_difference = 100 * current_val / previous_avg - 100\n            if pct_difference &gt; over_average:\n                self.register_warning(\n                    self.MEASURE_DEFINITION[error_rate_type](field_name=field_name),\n                    pct_difference,\n                    self.CONDITION[error_rate_type][\"AVERAGE\"](field_name=field_name, value=over_average),\n                    over_average,\n                    self.WARNING_MESSAGE[error_rate_type][\"AVERAGE\"](field_name=field_name),\n                )\n        upper_control_limit = previous_avg + variability * previous_std\n\n        if current_val &gt; upper_control_limit:\n            self.register_warning(\n                self.MEASURE_DEFINITION[error_rate_type](field_name=field_name),\n                current_val,\n                self.CONDITION[error_rate_type][\"VARIABILITY\"](field_name=field_name, variability=variability),\n                variability,\n                self.WARNING_MESSAGE[error_rate_type][\"VARIABILITY\"](field_name=field_name),\n            )\n\n        if current_val &gt; absolute_upper_control_limit:\n            self.register_warning(\n                self.MEASURE_DEFINITION[error_rate_type](field_name=field_name),\n                current_val,\n                self.CONDITION[error_rate_type][\"ABS_VALUE_UPPER_LIMIT\"](\n                    field_name=field_name, value=absolute_upper_control_limit\n                ),\n                absolute_upper_control_limit,\n                self.WARNING_MESSAGE[error_rate_type][\"ABS_VALUE_UPPER_LIMIT\"](field_name=field_name),\n            )\n\n    def create_plots_data(\n        self,\n        lookback_initial_rows,\n        lookback_final_rows,\n        today_values,\n        error_rate,\n        raw_average,\n        clean_average,\n        error_rate_avg,\n        raw_UCL,\n        clean_UCL,\n        error_rate_UCL,\n        raw_LCL,\n        clean_LCL,\n    ):\n        \"\"\"Method that takes the data needed to generate the required plots, and formats it for\n        easier plotting and saving later on.\n\n        Args:\n            lookback_initial_rows (dict): contains data on the lookback period dates' number of rows before syntactic checks\n            lookback_final_rows (dict): contains data on the lookback period dates' number of rows after syntactic checks\n            today_values (dict): contains data on the date of study error counts and rows before and after the syntactic checks\n            error_rate (dict): cotains data on the error rates for all lookback dates and date of study.\n            raw_average (float): average of rows in the raw data before syntactic checks\n            clean_average (float): average of rows in the clean data after syntactic checks\n            error_rate_avg (float): average of the error rate observed in the syntactic checks\n            raw_UCL (float): upper control limit for the rows in the raw data before syntactic checks\n            clean_UCL (float): upper control limit for the rows in the clean data after syntactic checks\n            error_rate_UCL (float): upper control limit for the error rate\n            raw_LCL (float): lower control limit for the rows in the raw data before syntactic checks\n            clean_LCL (float): lower control limit for the rows in the raw data after syntactic checks\n        \"\"\"\n        # self.plots_data = {\"rows_before_syntactic_check\": [], \"rows_after_syntactic_check\": [], \"error_rate\": []}\n\n        plots_data = {\"line_plot\": [], \"pie_plot\": []}\n\n        for date in sorted(self.lookback_dates) + [self.date_of_study]:\n            plots_data[\"line_plot\"].extend(\n                [\n                    Row(\n                        **{\n                            ColNames.date: date,\n                            ColNames.daily_value: float(lookback_initial_rows[date]),\n                            ColNames.average: float(raw_average),\n                            ColNames.UCL: float(raw_UCL),\n                            ColNames.LCL: float(raw_LCL),\n                            ColNames.variable: \"rows_before_syntactic_check\",\n                            ColNames.year: self.date_of_study.year,\n                            ColNames.month: self.date_of_study.month,\n                            ColNames.day: self.date_of_study.day,\n                            ColNames.timestamp: self.timestamp,\n                        }\n                    )\n                ]\n            )\n\n            plots_data[\"line_plot\"].extend(\n                [\n                    Row(\n                        **{\n                            ColNames.date: date,\n                            ColNames.daily_value: float(lookback_final_rows[date]),\n                            ColNames.average: float(clean_average),\n                            ColNames.UCL: float(clean_UCL),\n                            ColNames.LCL: float(clean_LCL),\n                            ColNames.variable: \"rows_after_syntactic_check\",\n                            ColNames.year: self.date_of_study.year,\n                            ColNames.month: self.date_of_study.month,\n                            ColNames.day: self.date_of_study.day,\n                            ColNames.timestamp: self.timestamp,\n                        }\n                    )\n                ]\n            )\n\n            plots_data[\"line_plot\"].extend(\n                [\n                    Row(\n                        **{\n                            ColNames.date: date,\n                            ColNames.daily_value: float(error_rate[date]),\n                            ColNames.average: float(error_rate_avg),\n                            ColNames.UCL: float(error_rate_UCL),\n                            ColNames.LCL: None,\n                            ColNames.variable: \"error_rate\",\n                            ColNames.year: self.date_of_study.year,\n                            ColNames.month: self.date_of_study.month,\n                            ColNames.day: self.date_of_study.day,\n                            ColNames.timestamp: self.timestamp,\n                        }\n                    )\n                ]\n            )\n\n        # Now, the data for the pie charts\n        # Ugly way to get the relation error_code -&gt; error attribute name\n        error_types = {getattr(NetworkErrorType, att): att for att in dir(NetworkErrorType) if not att.startswith(\"__\")}\n\n        for field_name, error_counts in today_values.items():\n            if field_name in [None, \"dates\"]:\n                continue\n\n            # boolean check if this field had any errors or not\n            field_has_errors = False\n\n            for key in error_counts.keys():\n                if key != NetworkErrorType.NO_ERROR:\n                    if error_counts[key] &gt; 0:\n                        # self.plots_data[field_name].append(\n                        #     field_name, error_types[key], error_counts[key]\n                        # )\n                        field_has_errors = True\n                        plots_data[\"pie_plot\"].append(\n                            Row(\n                                **{\n                                    ColNames.type_code: error_types[key],\n                                    ColNames.value: error_counts[key],\n                                    ColNames.variable: field_name,\n                                    ColNames.year: self.date_of_study.year,\n                                    ColNames.month: self.date_of_study.month,\n                                    ColNames.day: self.date_of_study.day,\n                                    ColNames.timestamp: self.timestamp,\n                                }\n                            )\n                        )\n            if not field_has_errors:\n                self.logger.info(f\"Field `{field_name}` had no errors\")\n        self.output_data_objects[SilverNetworkSyntacticQualityWarningsLinePlotData.ID].df = self.spark.createDataFrame(\n            plots_data[\"line_plot\"], schema=SilverNetworkSyntacticQualityWarningsLinePlotData.SCHEMA\n        )\n\n        self.output_data_objects[SilverNetworkSyntacticQualityWarningsPiePlotData.ID].df = self.spark.createDataFrame(\n            plots_data[\"pie_plot\"], schema=SilverNetworkSyntacticQualityWarningsPiePlotData.SCHEMA\n        )\n\n    def write(self):\n        # Write regular data objects\n        self.logger.info(\"Writing data objects...\")\n        super().write()\n        self.logger.info(\"Data objects written.\")\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.all_specific_error_warnings","title":"<code>all_specific_error_warnings(lookback_stats, today_values)</code>","text":"<p>Parent method for the creation of warnings for each type of error rate</p> <p>lookback_stats (dict): contains error information of each date of the lookback period today_values (dict): contains error information of the date of study</p> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def all_specific_error_warnings(self, lookback_stats, today_values):\n    \"\"\"Parent method for the creation of warnings for each type of error rate\n\n    lookback_stats (dict): contains error information of each date of the lookback period\n    today_values (dict): contains error information of the date of study\n    \"\"\"\n    error_rate_types = [\"Missing_value_RATE\", \"Out_of_range_RATE\", \"Parsing_error_RATE\"]\n\n    for error_rate_type in error_rate_types:\n        for field_name in self.thresholds[error_rate_type]:\n            self.specific_error_warnings(error_rate_type, field_name, lookback_stats, today_values)\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.check_needed_dates","title":"<code>check_needed_dates()</code>","text":"<p>Method that checks if both the date of study and the dates necessary to generate the quality warnings, specified through the lookback_period parameter, are present in the input data.</p> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def check_needed_dates(self) -&gt; None:\n    \"\"\"\n    Method that checks if both the date of study and the dates necessary to generate\n    the quality warnings, specified through the lookback_period parameter, are present\n    in the input data.\n    \"\"\"\n\n    # Collect all distinct dates in the input quality metrics within the needed range\n    metrics = self.input_data_objects[SilverNetworkDataQualityMetricsByColumn.ID].df\n    dates = (\n        metrics.filter(\n            F.col(\"date\")\n            # left- and right- inclusive\n            .between(\n                self.date_of_study - datetime.timedelta(days=self.PERIOD_DURATION[self.lookback_period]),\n                self.date_of_study,\n            )\n        )\n        .select(F.col(ColNames.date))\n        .distinct()\n        .collect()\n    )\n\n    dates = [row[ColNames.date] for row in dates]\n\n    if self.date_of_study not in dates:\n        raise ValueError(\n            f\"The date of study, {self.date_of_study}, is not present in the input Quality Metrics data\"\n        )\n\n    if set(self.lookback_dates).intersection(set(dates)) != set(self.lookback_dates):\n        error_msg = f\"\"\"\n            The following dates from the lookback period are not present in the\n            input Quality Metrics data:\n            {\n                sorted(\n                    map(\n                        lambda x: x.strftime(self.date_format),\n                        set(self.lookback_dates).difference(set(dates))\n                    )\n                )\n            }\"\"\"\n\n        self.logger.error(error_msg)\n        raise ValueError(error_msg)\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.clean_size_warnings","title":"<code>clean_size_warnings(lookback_stats, today_values)</code>","text":"<p>Method that generates quality warnings, that will be recorded in the output log table, for the metric regarding the final number of rows of the raw input network topology data prior to the syntactic check procedure.</p> A total of six warnings might be generated <ul> <li>The study date's number of rows is greater than the average number of rows over the previous period     by more than a threshold percentage.</li> <li>The study date's number of rows is smaller than the average number of rows over the previous period     by more than a threshold percentage.</li> <li>The study date's number of rows is greater than the average number of rows over the previous period     by a config-specified number of standard deviations.</li> <li>The study date's number of rows is smaller than the average number of rows over the previous period     by a config-specified number of standard deviations.</li> <li>The study date's number of rows is greater than a config-specified threshold.</li> <li>The study date's number of rows is smaller than a config-specified threshold.</li> </ul> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def clean_size_warnings(self, lookback_stats, today_values):\n    \"\"\"Method that generates quality warnings, that will be recorded in the output log table,\n    for the metric regarding the final number of rows of the raw input network topology data prior\n    to the syntactic check procedure.\n\n    A total of six warnings might be generated:\n        - The study date's number of rows is greater than the average number of rows over the previous period\n            by more than a threshold percentage.\n        - The study date's number of rows is smaller than the average number of rows over the previous period\n            by more than a threshold percentage.\n        - The study date's number of rows is greater than the average number of rows over the previous period\n            by a config-specified number of standard deviations.\n        - The study date's number of rows is smaller than the average number of rows over the previous period\n            by a config-specified number of standard deviations.\n        - The study date's number of rows is greater than a config-specified threshold.\n        - The study date's number of rows is smaller than a config-specified threshold.\n    \"\"\"\n    current_val = today_values[None][NetworkErrorType.FINAL_ROWS]\n    previous_avg = lookback_stats[None][NetworkErrorType.FINAL_ROWS][\"average\"]\n    previous_std = lookback_stats[None][NetworkErrorType.FINAL_ROWS][\"stddev\"]\n\n    measure_definition = self.MEASURE_DEFINITION[\"SIZE_CLEAN_DATA\"]\n\n    over_average = self.thresholds[\"SIZE_CLEAN_DATA\"][\"OVER_AVERAGE\"]\n    under_average = self.thresholds[\"SIZE_CLEAN_DATA\"][\"UNDER_AVERAGE\"]\n    variability = self.thresholds[\"SIZE_CLEAN_DATA\"][\"VARIABILITY\"]\n    absolute_upper_control_limit = self.thresholds[\"SIZE_CLEAN_DATA\"][\"ABS_VALUE_UPPER_LIMIT\"]\n    absolute_lower_control_limit = self.thresholds[\"SIZE_CLEAN_DATA\"][\"ABS_VALUE_LOWER_LIMIT\"]\n\n    if previous_avg == 0:\n        self.logger.warning(\n            f\"\"\"\n            The average of the number of rows in the clean input network topology data after syntactic checks is 0\n            for the lookback period especified ({self.lookback_period_start} to {self.lookback_period_end}, \n            both included)\n            \"\"\"\n        )\n    else:\n        pct_difference = 100 * current_val / previous_avg - 100\n        if pct_difference &gt; over_average:\n            self.register_warning(\n                measure_definition,\n                pct_difference,\n                f\"Size is greater than the previous period's average by {over_average} %\",\n                over_average,\n                \"The number of cells after the syntactic checks procedure is unexpectedly high with respect to the previous period.\",\n            )\n\n        if pct_difference &lt; -under_average:\n            self.register_warning(\n                measure_definition,\n                pct_difference,\n                f\"Size is smaller than the previous period's average by {under_average} %\",\n                under_average,\n                \"The number of cells after the syntactic checks procedure is unexpectedly low with respect to the previous period.\",\n            )\n\n    upper_control_limit = previous_avg + variability * previous_std\n    lower_control_limit = previous_avg - variability * previous_std\n\n    if current_val &gt; upper_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            \"Size is out of the upper control limit calculated on the basis of average and standard deviation of the distribution \"\n            f\"of the size in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\",\n            upper_control_limit,\n            \"The number of cells is unexpectedly high with respect to the previous period, taking into account the usual \"\n            \"variability of the cell numbers, please check if there have been issues in the network.\",\n        )\n    if current_val &lt; lower_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            \"Size is out of the lower control limit calculated on the basis of average and standard deviation of the distribution \"\n            f\"of the size in the previous period. Lower control limit = (average - {variability}\u00b7stddev)\",\n            lower_control_limit,\n            \"The number of cells is unexpectedly low with respect to the previous period, taking into account the usual \"\n            \"variability of the cell numbers, please check if there have been issues in the network.\",\n        )\n\n    # Use of UCL and LCL as default values for the absolute limits\n    if absolute_upper_control_limit is None:\n        absolute_upper_control_limit = upper_control_limit\n    if absolute_lower_control_limit is None:\n        absolute_lower_control_limit = lower_control_limit\n\n    if current_val &gt; absolute_upper_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            f\"The size is over the threshold {absolute_upper_control_limit}\",\n            absolute_upper_control_limit,\n            \"The number of cells after the syntactic checks procedure is over the threshold.\",\n        )\n    if current_val &lt; absolute_lower_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            f\"The size is under the threshold {absolute_lower_control_limit}\",\n            absolute_upper_control_limit,\n            \"The number of cells after the syntactic checks procedure is under the threshold.\",\n        )\n\n    return previous_avg, upper_control_limit, lower_control_limit\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.create_plots_data","title":"<code>create_plots_data(lookback_initial_rows, lookback_final_rows, today_values, error_rate, raw_average, clean_average, error_rate_avg, raw_UCL, clean_UCL, error_rate_UCL, raw_LCL, clean_LCL)</code>","text":"<p>Method that takes the data needed to generate the required plots, and formats it for easier plotting and saving later on.</p> <p>Parameters:</p> Name Type Description Default <code>lookback_initial_rows</code> <code>dict</code> <p>contains data on the lookback period dates' number of rows before syntactic checks</p> required <code>lookback_final_rows</code> <code>dict</code> <p>contains data on the lookback period dates' number of rows after syntactic checks</p> required <code>today_values</code> <code>dict</code> <p>contains data on the date of study error counts and rows before and after the syntactic checks</p> required <code>error_rate</code> <code>dict</code> <p>cotains data on the error rates for all lookback dates and date of study.</p> required <code>raw_average</code> <code>float</code> <p>average of rows in the raw data before syntactic checks</p> required <code>clean_average</code> <code>float</code> <p>average of rows in the clean data after syntactic checks</p> required <code>error_rate_avg</code> <code>float</code> <p>average of the error rate observed in the syntactic checks</p> required <code>raw_UCL</code> <code>float</code> <p>upper control limit for the rows in the raw data before syntactic checks</p> required <code>clean_UCL</code> <code>float</code> <p>upper control limit for the rows in the clean data after syntactic checks</p> required <code>error_rate_UCL</code> <code>float</code> <p>upper control limit for the error rate</p> required <code>raw_LCL</code> <code>float</code> <p>lower control limit for the rows in the raw data before syntactic checks</p> required <code>clean_LCL</code> <code>float</code> <p>lower control limit for the rows in the raw data after syntactic checks</p> required Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def create_plots_data(\n    self,\n    lookback_initial_rows,\n    lookback_final_rows,\n    today_values,\n    error_rate,\n    raw_average,\n    clean_average,\n    error_rate_avg,\n    raw_UCL,\n    clean_UCL,\n    error_rate_UCL,\n    raw_LCL,\n    clean_LCL,\n):\n    \"\"\"Method that takes the data needed to generate the required plots, and formats it for\n    easier plotting and saving later on.\n\n    Args:\n        lookback_initial_rows (dict): contains data on the lookback period dates' number of rows before syntactic checks\n        lookback_final_rows (dict): contains data on the lookback period dates' number of rows after syntactic checks\n        today_values (dict): contains data on the date of study error counts and rows before and after the syntactic checks\n        error_rate (dict): cotains data on the error rates for all lookback dates and date of study.\n        raw_average (float): average of rows in the raw data before syntactic checks\n        clean_average (float): average of rows in the clean data after syntactic checks\n        error_rate_avg (float): average of the error rate observed in the syntactic checks\n        raw_UCL (float): upper control limit for the rows in the raw data before syntactic checks\n        clean_UCL (float): upper control limit for the rows in the clean data after syntactic checks\n        error_rate_UCL (float): upper control limit for the error rate\n        raw_LCL (float): lower control limit for the rows in the raw data before syntactic checks\n        clean_LCL (float): lower control limit for the rows in the raw data after syntactic checks\n    \"\"\"\n    # self.plots_data = {\"rows_before_syntactic_check\": [], \"rows_after_syntactic_check\": [], \"error_rate\": []}\n\n    plots_data = {\"line_plot\": [], \"pie_plot\": []}\n\n    for date in sorted(self.lookback_dates) + [self.date_of_study]:\n        plots_data[\"line_plot\"].extend(\n            [\n                Row(\n                    **{\n                        ColNames.date: date,\n                        ColNames.daily_value: float(lookback_initial_rows[date]),\n                        ColNames.average: float(raw_average),\n                        ColNames.UCL: float(raw_UCL),\n                        ColNames.LCL: float(raw_LCL),\n                        ColNames.variable: \"rows_before_syntactic_check\",\n                        ColNames.year: self.date_of_study.year,\n                        ColNames.month: self.date_of_study.month,\n                        ColNames.day: self.date_of_study.day,\n                        ColNames.timestamp: self.timestamp,\n                    }\n                )\n            ]\n        )\n\n        plots_data[\"line_plot\"].extend(\n            [\n                Row(\n                    **{\n                        ColNames.date: date,\n                        ColNames.daily_value: float(lookback_final_rows[date]),\n                        ColNames.average: float(clean_average),\n                        ColNames.UCL: float(clean_UCL),\n                        ColNames.LCL: float(clean_LCL),\n                        ColNames.variable: \"rows_after_syntactic_check\",\n                        ColNames.year: self.date_of_study.year,\n                        ColNames.month: self.date_of_study.month,\n                        ColNames.day: self.date_of_study.day,\n                        ColNames.timestamp: self.timestamp,\n                    }\n                )\n            ]\n        )\n\n        plots_data[\"line_plot\"].extend(\n            [\n                Row(\n                    **{\n                        ColNames.date: date,\n                        ColNames.daily_value: float(error_rate[date]),\n                        ColNames.average: float(error_rate_avg),\n                        ColNames.UCL: float(error_rate_UCL),\n                        ColNames.LCL: None,\n                        ColNames.variable: \"error_rate\",\n                        ColNames.year: self.date_of_study.year,\n                        ColNames.month: self.date_of_study.month,\n                        ColNames.day: self.date_of_study.day,\n                        ColNames.timestamp: self.timestamp,\n                    }\n                )\n            ]\n        )\n\n    # Now, the data for the pie charts\n    # Ugly way to get the relation error_code -&gt; error attribute name\n    error_types = {getattr(NetworkErrorType, att): att for att in dir(NetworkErrorType) if not att.startswith(\"__\")}\n\n    for field_name, error_counts in today_values.items():\n        if field_name in [None, \"dates\"]:\n            continue\n\n        # boolean check if this field had any errors or not\n        field_has_errors = False\n\n        for key in error_counts.keys():\n            if key != NetworkErrorType.NO_ERROR:\n                if error_counts[key] &gt; 0:\n                    # self.plots_data[field_name].append(\n                    #     field_name, error_types[key], error_counts[key]\n                    # )\n                    field_has_errors = True\n                    plots_data[\"pie_plot\"].append(\n                        Row(\n                            **{\n                                ColNames.type_code: error_types[key],\n                                ColNames.value: error_counts[key],\n                                ColNames.variable: field_name,\n                                ColNames.year: self.date_of_study.year,\n                                ColNames.month: self.date_of_study.month,\n                                ColNames.day: self.date_of_study.day,\n                                ColNames.timestamp: self.timestamp,\n                            }\n                        )\n                    )\n        if not field_has_errors:\n            self.logger.info(f\"Field `{field_name}` had no errors\")\n    self.output_data_objects[SilverNetworkSyntacticQualityWarningsLinePlotData.ID].df = self.spark.createDataFrame(\n        plots_data[\"line_plot\"], schema=SilverNetworkSyntacticQualityWarningsLinePlotData.SCHEMA\n    )\n\n    self.output_data_objects[SilverNetworkSyntacticQualityWarningsPiePlotData.ID].df = self.spark.createDataFrame(\n        plots_data[\"pie_plot\"], schema=SilverNetworkSyntacticQualityWarningsPiePlotData.SCHEMA\n    )\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.error_rate_warnings","title":"<code>error_rate_warnings(initial_rows, final_rows, today_values)</code>","text":"<p>Method that generates quality warnings, that will be recorded in the output log table, for the metric regarding the error rate observed in the syntactic check procedure.</p> A total of three warnings might be generated <ul> <li>The study date's error rate is greater than the average number of rows over the previous period     by more than a threshold percentage.</li> <li>The study date's error rate is greater than the average number of rows over the previous period     by a config-specified number of standard deviations.</li> <li>The study date's error rate is greater than a config-specified threshold.</li> </ul> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def error_rate_warnings(self, initial_rows, final_rows, today_values):\n    \"\"\"Method that generates quality warnings, that will be recorded in the output log table,\n    for the metric regarding the error rate observed in the syntactic check procedure.\n\n    A total of three warnings might be generated:\n        - The study date's error rate is greater than the average number of rows over the previous period\n            by more than a threshold percentage.\n        - The study date's error rate is greater than the average number of rows over the previous period\n            by a config-specified number of standard deviations.\n        - The study date's error rate is greater than a config-specified threshold.\n    \"\"\"\n    if len(initial_rows) != len(final_rows):\n        raise ValueError(\n            \"Input Quality Metrics do not have information on the number of rows \"\n            \"before and after syntactic checks on all dates considered!\"\n        )\n\n    error_rate = {\n        date: 100 * (initial_rows[date] - final_rows[date]) / initial_rows[date] for date in initial_rows.keys()\n    }\n\n    current_val = (\n        100\n        * (today_values[None][NetworkErrorType.INITIAL_ROWS] - today_values[None][NetworkErrorType.FINAL_ROWS])\n        / today_values[None][NetworkErrorType.INITIAL_ROWS]\n    )\n    previous_avg = sum(error_rate[date] for date in self.lookback_dates) / len(self.lookback_dates)\n    previous_std = math.sqrt(\n        sum((error_rate[date] - previous_avg) ** 2 for date in self.lookback_dates) / (len(self.lookback_dates) - 1)\n    )\n\n    measure_definition = self.MEASURE_DEFINITION[\"TOTAL_ERROR_RATE\"]\n\n    over_average = self.thresholds[\"TOTAL_ERROR_RATE\"][\"OVER_AVERAGE\"]\n    variability = self.thresholds[\"TOTAL_ERROR_RATE\"][\"VARIABILITY\"]\n    absolute_upper_control_limit = self.thresholds[\"TOTAL_ERROR_RATE\"][\"ABS_VALUE_UPPER_LIMIT\"]\n\n    if previous_avg == 0:\n        self.logger.warning(\n            f\"\"\"\n            The average error rate in the input network topology data 0\n            for the lookback period especified ({self.lookback_period_start} to {self.lookback_period_end}, \n            both included)\n            \"\"\"\n        )\n    else:\n        pct_difference = 100 * current_val / previous_avg - 100\n        if pct_difference &gt; over_average:\n            self.register_warning(\n                measure_definition,\n                pct_difference,\n                f\"Error rate is greater than the previous period's average by more than {over_average} %\",\n                over_average,\n                \"The error rate after the syntactic checks procedure is unexpectedly high with respect to the previous period.\",\n            )\n    upper_control_limit = previous_avg + variability * previous_std\n\n    if current_val &gt; upper_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            \"Error rate is over the upper control limit calculated on the basis of average and standard deviation of the distribution \"\n            f\"of the size in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\",\n            upper_control_limit,\n            \"The error rate after the syntactic checks procedure is unexpectedly high with respect to the previous period, taking into account the usual \"\n            \"variability.\",\n        )\n\n    if current_val &gt; absolute_upper_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            f\"The error rate is over the value {absolute_upper_control_limit}\",\n            absolute_upper_control_limit,\n            \"The error rate after the syntactic checks procedure is over the threshold.\",\n        )\n    error_rate[self.date_of_study] = current_val\n    return error_rate, previous_avg, upper_control_limit\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.get_lookback_period_statistics","title":"<code>get_lookback_period_statistics()</code>","text":"<p>Method that computes the necessary statistics (average and standard deviation) from the Quality Metrics of the lookback period.</p> <p>Returns:</p> Name Type Description <code>statistics</code> <code>dict</code> <p>dictionary containing said necessary statistics, with the following structure: {     field_name1: {         type_error1: {             'average': 12.2,             'stddev': 17.5         },         type_error2 : {             'average': 4.5,             'stddev': 10.1         }     },     ... }</p> <code>initial_rows</code> <code>dict</code> <p>dictionary indicating the number of rows before syntactic checks for the previous period, {date -&gt; size_raw_data}</p> <code>final_rows</code> <code>dict</code> <p>dictionary indicating the number of rows after syntactic checks for the previous period, {date -&gt; size_raw_data}</p> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def get_lookback_period_statistics(self) -&gt; dict:\n    \"\"\"Method that computes the necessary statistics (average and standard deviation) from the\n    Quality Metrics of the lookback period.\n\n    Returns:\n        statistics (dict): dictionary containing said necessary statistics, with the following\n            structure:\n            {\n                field_name1: {\n                    type_error1: {\n                        'average': 12.2,\n                        'stddev': 17.5\n                    },\n                    type_error2 : {\n                        'average': 4.5,\n                        'stddev': 10.1\n                    }\n                },\n                ...\n            }\n        initial_rows (dict): dictionary indicating the number of rows before syntactic checks for the previous period, {date -&gt; size_raw_data}\n        final_rows (dict): dictionary indicating the number of rows after syntactic checks for the previous period, {date -&gt; size_raw_data}\n    \"\"\"\n    intermediate_df = self.input_data_objects[SilverNetworkDataQualityMetricsByColumn.ID].df.filter(\n        F.col(ColNames.date).between(self.lookback_period_start, self.lookback_period_end)\n    )\n\n    intermediate_df.cache()\n\n    lookback_stats = (\n        intermediate_df.groupBy([ColNames.field_name, ColNames.type_code])\n        .agg(F.mean(F.col(ColNames.value)).alias(\"average\"), F.stddev_samp(F.col(ColNames.value)).alias(\"stddev\"))\n        .collect()\n    )\n\n    error_rate_data = (\n        intermediate_df.select([ColNames.type_code, ColNames.value, ColNames.date])\n        .filter(F.col(ColNames.type_code).isin([NetworkErrorType.INITIAL_ROWS, NetworkErrorType.FINAL_ROWS]))\n        .collect()\n    )\n\n    intermediate_df.unpersist()\n\n    initial_rows = {}\n    final_rows = {}\n\n    for row in error_rate_data:\n        if row[ColNames.type_code] == NetworkErrorType.INITIAL_ROWS:\n            initial_rows[row[ColNames.date]] = row[\"value\"]\n        else:\n            final_rows[row[ColNames.date]] = row[\"value\"]\n\n    statistics = dict()\n    for row in lookback_stats:\n        field_name, type_code, average, stddev = (\n            row[ColNames.field_name],\n            row[ColNames.type_code],\n            row[\"average\"],\n            row[\"stddev\"],\n        )\n        if field_name not in statistics:\n            statistics[field_name] = dict()\n        statistics[field_name][type_code] = {\"average\": average, \"stddev\": stddev}\n\n    return statistics, initial_rows, final_rows\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.get_study_date_values","title":"<code>get_study_date_values()</code>","text":"<p>Method that reads and returns the quality metrics of the date of study.</p> <p>Returns:</p> Name Type Description <code>today_values</code> <code>dict</code> <p>dictionary containing said values, with the following structure: {     field_name1: {         type_error1: 123,         type_error2: 23,         type_error3: 0     },     field_name2: {         type_error1: 0,         type_error2: 0,         type_error3: 300     }, }</p> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def get_study_date_values(self) -&gt; dict:\n    \"\"\"Method that reads and returns the quality metrics of the date of study.\n\n    Returns:\n        today_values (dict): dictionary containing said values, with the following\n            structure:\n            {\n                field_name1: {\n                    type_error1: 123,\n                    type_error2: 23,\n                    type_error3: 0\n                },\n                field_name2: {\n                    type_error1: 0,\n                    type_error2: 0,\n                    type_error3: 300\n                },\n            }\n    \"\"\"\n    today_metrics = (\n        self.input_data_objects[SilverNetworkDataQualityMetricsByColumn.ID]\n        .df.filter(F.col(ColNames.date) == F.lit(self.date_of_study))\n        .select([ColNames.field_name, ColNames.type_code, ColNames.value])\n    ).collect()\n\n    today_values = {}\n\n    for row in today_metrics:\n        field_name, type_code, value = row[ColNames.field_name], row[ColNames.type_code], row[ColNames.value]\n\n        if field_name not in today_values:\n            today_values[field_name] = {}\n\n        today_values[field_name][type_code] = value\n\n    return today_values\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.get_thresholds","title":"<code>get_thresholds()</code>","text":"<p>Method that reads the threshold-related parameters, contained in the config file, and saves them in memory to use them instead of the ones specified by default. Raises:     ValueError: non-numerical value that cannot be parsed to float has been used in         the config file     ValueError: Negative value for a given parameter has been given, when only         non-negative values make sense and are thus allowed.</p> <p>Returns:</p> Name Type Description <code>thresholds</code> <code>dict</code> <p>dict containing the different thresholds used for computing the quality warnings, with the same structure as multimno.core.constants.network_default_thresholds.NETWORK_DEFAULT_THRESHOLDS</p> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def get_thresholds(self) -&gt; dict:\n    \"\"\"\n    Method that reads the threshold-related parameters, contained in the config file,\n    and saves them in memory to use them instead of the ones specified by default.\n    Raises:\n        ValueError: non-numerical value that cannot be parsed to float has been used in\n            the config file\n        ValueError: Negative value for a given parameter has been given, when only\n            non-negative values make sense and are thus allowed.\n\n    Returns:\n        thresholds: dict containing the different thresholds used for computing the\n            quality warnings, with the same structure as\n            multimno.core.constants.network_default_thresholds.NETWORK_DEFAULT_THRESHOLDS\n    \"\"\"\n    config_thresholds = self.config.geteval(self.COMPONENT_ID, \"thresholds\")\n    thresholds = NETWORK_DEFAULT_THRESHOLDS\n\n    for error_key in config_thresholds.keys():\n        if error_key not in NETWORK_DEFAULT_THRESHOLDS:\n            self.logger.info(f\"Parameter key {error_key} unknown -- ignored\")\n            continue\n\n        if error_key in [\"SIZE_RAW_DATA\", \"SIZE_CLEAN_DATA\", \"TOTAL_ERROR_RATE\"]:\n            for param_key, val in config_thresholds[error_key].items():\n                if param_key not in NETWORK_DEFAULT_THRESHOLDS[error_key]:\n                    self.logger.info(f\"Unknown parameter {param_key} under {error_key} -- ignored\")\n                    continue\n\n                try:\n                    val = float(val)\n                except ValueError as e:\n                    error_msg = (\n                        f\"Expected numeric value for parameter {param_key} under {error_key} - got {val} instead.\"\n                    )\n                    self.logger.error(error_msg)\n                    raise e\n\n                if val &lt; 0:\n                    error_msg = f\"Parameter {param_key} under {error_key} must be non-negative - got {val}\"\n                    self.logger.error(error_msg)\n                    raise ValueError(error_msg)\n\n                thresholds[error_key][param_key] = val\n\n        else:  # then is one of 'Missing_value_RATE', 'Out_of_range_RATE', 'Parsing_error_RATE'\n            for field_name in config_thresholds[error_key]:\n                if field_name not in NETWORK_DEFAULT_THRESHOLDS[error_key]:\n                    self.logger.info(f\"Unknown field name {field_name} under {error_key} -- ignored\")\n                    continue\n\n                for param_key, val in config_thresholds[error_key][field_name].items():\n                    if param_key not in NETWORK_DEFAULT_THRESHOLDS[error_key][field_name]:\n                        self.logger.info(\n                            f'Unknown parameter {param_key} under {error_key}[\"{field_name}\"] -- ignored'\n                        )\n                        continue\n\n                    try:\n                        val = float(val)\n                    except ValueError as e:\n                        error_msg = f'Expected numeric value for parameter {param_key} under {error_key}[\"{field_name}\"] - got {val} instead.'\n                        self.logger.error(error_msg)\n                        raise e\n\n                    if val &lt; 0:\n                        error_msg = f'Parameter {param_key} under {error_key}[\"{field_name}\"] must be non-negative - got {val}'\n                        self.logger.error(error_msg)\n                        raise ValueError(error_msg)\n\n                    thresholds[error_key][field_name][param_key] = val\n\n    return thresholds\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.raw_size_warnings","title":"<code>raw_size_warnings(lookback_stats, today_values)</code>","text":"<p>Method that generates the quality warnings, that will be recorded in the output log table, for the metric regarding the initial number of rows of the raw input network topology data prior to the syntactic check procedure.</p> A total of six warnings might be generated <ul> <li>The study date's number of rows is greater than the average number of rows over the previous period     by more than a threshold percentage.</li> <li>The study date's number of rows is smaller than the average number of rows over the previous period     by more than a threshold percentage.</li> <li>The study date's number of rows is greater than the average number of rows over the previous period     by a config-specified number of standard deviations.</li> <li>The study date's number of rows is smaller than the average number of rows over the previous period     by a config-specified number of standard deviations.</li> <li>The study date's number of rows is greater than a config-specified threshold.</li> <li>The study date's number of rows is smaller than a config-specified threshold.</li> </ul> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def raw_size_warnings(self, lookback_stats, today_values):\n    \"\"\"Method that generates the quality warnings, that will be recorded in the output log table,\n    for the metric regarding the initial number of rows of the raw input network topology data prior\n    to the syntactic check procedure.\n\n    A total of six warnings might be generated:\n        - The study date's number of rows is greater than the average number of rows over the previous period\n            by more than a threshold percentage.\n        - The study date's number of rows is smaller than the average number of rows over the previous period\n            by more than a threshold percentage.\n        - The study date's number of rows is greater than the average number of rows over the previous period\n            by a config-specified number of standard deviations.\n        - The study date's number of rows is smaller than the average number of rows over the previous period\n            by a config-specified number of standard deviations.\n        - The study date's number of rows is greater than a config-specified threshold.\n        - The study date's number of rows is smaller than a config-specified threshold.\n    \"\"\"\n    current_val = today_values[None][NetworkErrorType.INITIAL_ROWS]\n    previous_avg = lookback_stats[None][NetworkErrorType.INITIAL_ROWS][\"average\"]\n    previous_std = lookback_stats[None][NetworkErrorType.INITIAL_ROWS][\"stddev\"]\n\n    measure_definition = self.MEASURE_DEFINITION[\"SIZE_RAW_DATA\"]\n\n    over_average = self.thresholds[\"SIZE_RAW_DATA\"][\"OVER_AVERAGE\"]\n    under_average = self.thresholds[\"SIZE_RAW_DATA\"][\"UNDER_AVERAGE\"]\n    variability = self.thresholds[\"SIZE_RAW_DATA\"][\"VARIABILITY\"]\n    absolute_upper_control_limit = self.thresholds[\"SIZE_RAW_DATA\"][\"ABS_VALUE_UPPER_LIMIT\"]\n    absolute_lower_control_limit = self.thresholds[\"SIZE_RAW_DATA\"][\"ABS_VALUE_LOWER_LIMIT\"]\n\n    if previous_avg == 0:\n        self.logger.warning(\n            f\"\"\"\n            The average of the number of rows in the raw input network topology data is 0\n            for the lookback period especified ({self.lookback_period_start} to {self.lookback_period_end},\n             both included)\n            \"\"\"\n        )\n    else:\n        pct_difference = 100 * current_val / previous_avg - 100\n        if pct_difference &gt; over_average:\n            self.register_warning(\n                measure_definition,\n                pct_difference,\n                f\"Size is greater than the previous period's average by more than {over_average} %\",\n                over_average,\n                \"The number of cells is unexpectedly high with respect to the previous period, please check if there have been issues in the network\",\n            )\n\n        if pct_difference &lt; -under_average:\n            self.register_warning(\n                measure_definition,\n                pct_difference,\n                f\"Size is smaller than the previous period's average by more than {under_average} %\",\n                under_average,\n                \"The number of cells is unexpectedly low with respect to the previous period, please check if there have been issues in the network.\",\n            )\n\n    upper_control_limit = previous_avg + variability * previous_std\n    lower_control_limit = previous_avg - variability * previous_std\n\n    if current_val &gt; upper_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            \"Size is out of the upper control limit calculated on the basis of average and standard deviation of the distribution \"\n            f\"of the size in the previous period. Upper control limit = (average + {variability}\u00b7stddev)\",\n            upper_control_limit,\n            \"The number of cells is unexpectedly high with respect to the previous period, taking into account the usual \"\n            \"variability of the cell numbers, please check if there have been issues in the network.\",\n        )\n    if current_val &lt; lower_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            \"Size is out of the lower control limit calculated on the basis of average and standard deviation of the distribution \"\n            f\"of the size in the previous period. Lower control limit = (average - {variability}\u00b7stddev)\",\n            lower_control_limit,\n            \"The number of cells is unexpectedly low with respect to the previous period, taking into account the usual \"\n            \"variability of the cell numbers, please check if there have been issues in the network.\",\n        )\n\n    # Use of UCL and LCL as default values for the absolute limits\n    if absolute_upper_control_limit is None:\n        absolute_upper_control_limit = upper_control_limit\n    if absolute_lower_control_limit is None:\n        absolute_lower_control_limit = lower_control_limit\n\n    if current_val &gt; absolute_upper_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            f\"The size is over the threshold {absolute_upper_control_limit}\",\n            absolute_upper_control_limit,\n            \"The number of cells is over the threshold, please check if there have been changes in the network.\",\n        )\n    if current_val &lt; absolute_lower_control_limit:\n        self.register_warning(\n            measure_definition,\n            current_val,\n            f\"The size is under the threshold {absolute_lower_control_limit}\",\n            absolute_upper_control_limit,\n            \"The number of cells is under the threshold, please check if there have been changes in the network.\",\n        )\n\n    return previous_avg, upper_control_limit, lower_control_limit\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.register_warning","title":"<code>register_warning(measure_definition, daily_value, condition, condition_value, warning_text)</code>","text":"<p>Method that abstracts away the creation in the correct format and data type, each of the quality warnings that will be recorded in the log table.</p> <p>Parameters:</p> Name Type Description Default <code>measure_definition</code> <code>str</code> <p>measure that raised the warning (e.g. Error rate)</p> required <code>daily_value</code> <code>float</code> <p>measure's value in the date of study that raised the warning</p> required <code>condition</code> <code>str</code> <p>test that was checked in order to raise the warning</p> required <code>condition_value</code> <code>float</code> <p>value against which the date of study's daily_value was compared</p> required <code>warning_text</code> <code>str</code> <p>verbose explanation of the condition being satisfied and the warning being raised</p> required Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def register_warning(\n    self, measure_definition: str, daily_value: float, condition: str, condition_value: float, warning_text: str\n) -&gt; None:\n    \"\"\"Method that abstracts away the creation in the correct format and data type, each of the quality warnings\n    that will be recorded in the log table.\n\n    Args:\n        measure_definition (str): measure that raised the warning (e.g. Error rate)\n        daily_value (float): measure's value in the date of study that raised the warning\n        condition (str): test that was checked in order to raise the warning\n        condition_value (float): value against which the date of study's daily_value was compared\n        warning_text (str): verbose explanation of the condition being satisfied and the warning\n            being raised\n    \"\"\"\n    warning = {\n        ColNames.title: self.TITLE,\n        ColNames.date: self.date_of_study,\n        ColNames.timestamp: self.timestamp,\n        ColNames.measure_definition: measure_definition,\n        ColNames.daily_value: float(daily_value),\n        ColNames.condition: condition,\n        ColNames.lookback_period: self.lookback_period,\n        ColNames.condition_value: float(condition_value),\n        ColNames.warning_text: warning_text,\n        ColNames.year: self.date_of_study.year,\n        ColNames.month: self.date_of_study.month,\n        ColNames.day: self.date_of_study.day,\n    }\n\n    self.warnings.append(Row(**warning))\n</code></pre>"},{"location":"reference/components/quality/network_quality_warnings/network_quality_warnings/#components.quality.network_quality_warnings.network_quality_warnings.NetworkQualityWarnings.specific_error_warnings","title":"<code>specific_error_warnings(error_rate_type, field_name, lookback_stats, today_values)</code>","text":"<p>Method that generates the quality warnings, that will be recorded in the output log table, for the metric regarding a specific error type considered in the network syntactic checks.</p> A total of three warnings might be generated <ul> <li>The study date's error rate for this error and this field is greater than the average number of rows over the previous period     by more than a threshold percentage.</li> <li>The study date's error rate for this error and this field is greater than the average number of rows over the previous period     by a config-specified number of standard deviations.</li> <li>The study date's error rate for this error and this field is greater than a config-specified threshold.</li> </ul> Source code in <code>multimno/components/quality/network_quality_warnings/network_quality_warnings.py</code> <pre><code>def specific_error_warnings(self, error_rate_type, field_name, lookback_stats, today_values):\n    \"\"\"Method that generates the quality warnings, that will be recorded in the output log table,\n    for the metric regarding a specific error type considered in the network syntactic checks.\n\n    A total of three warnings might be generated:\n        - The study date's error rate for this error and this field is greater than the average number of rows over the previous period\n            by more than a threshold percentage.\n        - The study date's error rate for this error and this field is greater than the average number of rows over the previous period\n            by a config-specified number of standard deviations.\n        - The study date's error rate for this error and this field is greater than a config-specified threshold.\n    \"\"\"\n    if error_rate_type not in self.ERROR_TYPE:\n        raise ValueError(f\"Unknown error type for quality warning `{error_rate_type}`\")\n\n    network_error_type = self.ERROR_TYPE[error_rate_type]\n\n    over_average = self.thresholds[error_rate_type][field_name][\"AVERAGE\"]\n    variability = self.thresholds[error_rate_type][field_name][\"VARIABILITY\"]\n    absolute_upper_control_limit = self.thresholds[error_rate_type][field_name][\"ABS_VALUE_UPPER_LIMIT\"]\n\n    if network_error_type == NetworkErrorType.OUT_OF_RANGE and field_name is None:\n        field_name = \"dates\"\n    current_val = today_values[field_name][network_error_type]\n    previous_avg = lookback_stats[field_name][network_error_type][\"average\"]\n    previous_std = lookback_stats[field_name][network_error_type][\"stddev\"]\n\n    if previous_avg == 0:\n        self.logger.warning(\n            f\"\"\"\n            The average {error_rate_type} for field {field_name} in the input network topology data is 0 for the lookback period\n            especified ({self.lookback_period_start} to {self.lookback_period_end}, both included)\n            \"\"\"\n        )\n    else:\n        pct_difference = 100 * current_val / previous_avg - 100\n        if pct_difference &gt; over_average:\n            self.register_warning(\n                self.MEASURE_DEFINITION[error_rate_type](field_name=field_name),\n                pct_difference,\n                self.CONDITION[error_rate_type][\"AVERAGE\"](field_name=field_name, value=over_average),\n                over_average,\n                self.WARNING_MESSAGE[error_rate_type][\"AVERAGE\"](field_name=field_name),\n            )\n    upper_control_limit = previous_avg + variability * previous_std\n\n    if current_val &gt; upper_control_limit:\n        self.register_warning(\n            self.MEASURE_DEFINITION[error_rate_type](field_name=field_name),\n            current_val,\n            self.CONDITION[error_rate_type][\"VARIABILITY\"](field_name=field_name, variability=variability),\n            variability,\n            self.WARNING_MESSAGE[error_rate_type][\"VARIABILITY\"](field_name=field_name),\n        )\n\n    if current_val &gt; absolute_upper_control_limit:\n        self.register_warning(\n            self.MEASURE_DEFINITION[error_rate_type](field_name=field_name),\n            current_val,\n            self.CONDITION[error_rate_type][\"ABS_VALUE_UPPER_LIMIT\"](\n                field_name=field_name, value=absolute_upper_control_limit\n            ),\n            absolute_upper_control_limit,\n            self.WARNING_MESSAGE[error_rate_type][\"ABS_VALUE_UPPER_LIMIT\"](field_name=field_name),\n        )\n</code></pre>"},{"location":"reference/components/quality/semantic_quality_warnings/","title":"semantic_quality_warnings","text":""},{"location":"reference/components/quality/semantic_quality_warnings/semantic_quality_warnings/","title":"semantic_quality_warnings","text":""},{"location":"reference/components/quality/semantic_quality_warnings/semantic_quality_warnings/#components.quality.semantic_quality_warnings.semantic_quality_warnings.SemanticQualityWarnings","title":"<code>SemanticQualityWarnings</code>","text":"<p>               Bases: <code>Component</code></p> Source code in <code>multimno/components/quality/semantic_quality_warnings/semantic_quality_warnings.py</code> <pre><code>class SemanticQualityWarnings(Component):\n    \"\"\" \"\"\"\n\n    COMPONENT_ID = \"SemanticQualityWarnings\"\n\n    MINIMUM_STD_LOOKBACK_DAYS = 3\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.timestamp = datetime.datetime.now()\n\n        # Read date of study\n        self.date_format = self.config.get(self.COMPONENT_ID, \"date_format\")\n        date_of_study = self.config.get(self.COMPONENT_ID, \"date_of_study\")\n\n        try:\n            self.date_of_study = datetime.datetime.strptime(date_of_study, self.date_format).date()\n        except ValueError as e:\n            self.logger.error(str(e), exc_info=True)\n            raise e\n\n        self.thresholds = self.get_thresholds()\n\n        self.warning_long_format = []\n\n    def get_thresholds(self) -&gt; dict:\n        \"\"\"\n        Method that reads the threshold-related parameters, contained in the config file,\n        and saves them in memory to use them instead of the ones specified by default.\n\n        Raises:\n            ValueError: non-numerical value that cannot be parsed to float (or int) has been used in\n                the config file\n            ValueError: Negative value for a given parameter has been given, when only\n                non-negative values make sense and are thus allowed.\n\n        Returns:\n            thresholds: dict containing the different thresholds used for computing the\n                quality warnings, with the same structure as\n                multimno.core.constants.network_default_thresholds.ASEMANTIC_DEFAULT_THRESHOLDS\n        \"\"\"\n        config_thresholds = self.config.geteval(self.COMPONENT_ID, \"thresholds\")\n        thresholds = SEMANTIC_DEFAULT_THRESHOLDS\n\n        for error_key in config_thresholds.keys():\n            if error_key not in SEMANTIC_DEFAULT_THRESHOLDS:\n                self.logger.info(f\"Parameter key {error_key} unknown -- ignored\")\n                continue\n\n            for param_key, val in config_thresholds[error_key].items():\n                if param_key not in SEMANTIC_DEFAULT_THRESHOLDS[error_key]:\n                    self.logger.info(f\"Unknown parameter {param_key} under {error_key} -- ignored\")\n                    continue\n\n                if param_key == \"sd_lookback_days\":\n                    try:\n                        val = int(val)\n                    except ValueError as e:\n                        error_msg = (\n                            f\"Expected integer value for parameter {param_key} under {error_key} - got {val} instead.\"\n                        )\n                        self.logger.error(error_msg)\n                        raise e\n                else:\n                    try:\n                        val = float(val)\n                    except ValueError as e:\n                        error_msg = (\n                            f\"Expected numeric value for parameter {param_key} under {error_key} - got {val} instead.\"\n                        )\n                        self.logger.error(error_msg)\n                        raise e\n\n                if val &lt; 0:\n                    error_msg = f\"Parameter {param_key} under {error_key} must be non-negative - got {val}\"\n                    self.logger.error(error_msg)\n                    raise ValueError(error_msg)\n\n                thresholds[error_key][param_key] = val\n\n        return thresholds\n\n    def initalize_data_objects(self):\n        input_silver_quality_metrics_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"event_device_semantic_quality_metrics\"\n        )\n        output_silver_log_table_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"event_device_semantic_quality_warnings_log_table\"\n        )\n        output_silver_bar_plot_data_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"event_device_semantic_quality_warnings_bar_plot_data\"\n        )\n\n        silver_quality_metrics = SilverEventSemanticQualityMetrics(\n            self.spark,\n            input_silver_quality_metrics_path,\n        )\n\n        silver_log_table = SilverEventSemanticQualityWarningsLogTable(self.spark, output_silver_log_table_path)\n\n        silver_bar_plot_data = SilverEventSemanticQualityWarningsBarPlotData(\n            self.spark, output_silver_bar_plot_data_path\n        )\n\n        self.input_data_objects = {silver_quality_metrics.ID: silver_quality_metrics}\n        self.output_data_objects = {\n            silver_log_table.ID: silver_log_table,\n            silver_bar_plot_data.ID: silver_bar_plot_data,\n        }\n\n    def transform(self):\n        # Pushup filter, select only dates needed\n        # Since currently each QW has a different lookback period, we filter up to the\n        # furthest day in the past needed\n\n        metrics_df = self.input_data_objects[SilverEventSemanticQualityMetrics.ID].df\n\n        furthest_lookback = max(self.thresholds[key][\"sd_lookback_days\"] for key in self.thresholds.keys())\n\n        metrics_df = metrics_df.withColumn(\n            \"date\", F.make_date(year=F.col(ColNames.year), month=F.col(ColNames.month), day=F.col(ColNames.day))\n        ).filter(\n            F.col(\"date\").between(self.date_of_study - datetime.timedelta(days=furthest_lookback), self.date_of_study)\n        )\n\n        # Get all necessary metrics\n        error_counts = metrics_df.select([\"date\", ColNames.type_of_error, ColNames.value]).collect()\n\n        error_counts = [row.asDict() for row in error_counts]\n\n        error_stats = dict()\n        for count in error_counts:\n            date = count[\"date\"]\n            if date not in error_stats:\n                error_stats[date] = dict()\n\n            error_stats[date][count[ColNames.type_of_error]] = count[ColNames.value]\n\n        # If study date not present in the data, throw an exception\n        if self.date_of_study not in error_stats.keys():\n            raise ValueError(\n                f\"The date of study, {self.date_of_study.strftime(self.date_format)}, has no semantic checks metrics!\"\n            )\n\n        for key in error_stats.keys():\n            error_stats[key] = {\"count\": error_stats[key]}\n            error_stats[key][\"total\"] = sum(error_stats[key][\"count\"].values())\n            error_stats[key][\"percentage\"] = {\n                type_of_error: 100 * val / error_stats[key][\"total\"]\n                for type_of_error, val in error_stats[key][\"count\"].items()\n            }\n\n        for error_name in self.thresholds.keys():\n            self.quality_warnings_by_error(error_name, error_stats)\n\n        self.set_output_log_table()\n\n        self.create_plots_data(error_stats)\n\n    def quality_warnings_by_error(self, error_name: str, error_stats: dict):\n        \"\"\"Method that generates the quality warnings that will be recorded in the output log table,\n        for each type of error.\n\n        In the case that the data needed for a specific error's lookback period is not present, only the current date's\n        error percentage is computed and no warning is raised.\n\n        Args:\n            error_name (str): name of the error, an attribute of multimno.core.constants.error_types.SemanticErrorType\n            error_stats (dict): contains different values concerning each type of error, its counts, percentages, etc.\n        \"\"\"\n        # Get the code of the error given its name\n        error_code = getattr(SemanticErrorType, error_name)\n\n        # lookback days for this error\n        lookback_span = self.thresholds[error_name][\"sd_lookback_days\"]\n        lookback_dates = [self.date_of_study - datetime.timedelta(days=dd) for dd in range(1, lookback_span + 1)]\n\n        if not all(lookback_date in error_stats.keys() for lookback_date in lookback_dates):\n            # cannot compute lookback mean and average, so only showing this date's percentages\n            self.register_warning(\n                date=self.date_of_study,\n                error_code=error_code,\n                value=error_stats[self.date_of_study][\"percentage\"][error_code],\n                upper_control_limit=None,\n                display_warning=False,\n            )\n        else:\n            if lookback_span &lt; self.MINIMUM_STD_LOOKBACK_DAYS:\n                upper_control_limit = self.thresholds[error_name][\"min_percentage\"]\n                self.logger.info(\n                    f\"Lookback days for {error_name} lower than {self.MINIMUM_STD_LOOKBACK_DAYS} - using fixed \"\n                    f\"threshold {upper_control_limit}% instead of using average and standard deviation\"\n                )\n            else:\n                previous_avg = sum(error_stats[dd][\"percentage\"][error_code] for dd in lookback_dates) / lookback_span\n                previous_std = math.sqrt(\n                    sum((error_stats[dd][\"percentage\"][error_code] - previous_avg) ** 2 for dd in lookback_dates)\n                    / (lookback_span - 1)\n                )\n\n                upper_control_limit = previous_avg + self.thresholds[error_name][\"min_sd\"] * previous_std\n\n            # Now compare with todays value\n            if error_stats[self.date_of_study][\"percentage\"][error_code] &gt; upper_control_limit:\n                display_warning = True\n            else:\n                display_warning = False\n\n            self.register_warning(\n                date=self.date_of_study,\n                error_code=error_code,\n                value=error_stats[self.date_of_study][\"percentage\"][error_code],\n                upper_control_limit=upper_control_limit,\n                display_warning=display_warning,\n            )\n\n    def register_warning(\n        self, date: datetime.date, error_code: int, value: float, upper_control_limit: float, display_warning: bool\n    ):\n        \"\"\"Method that abstracts away the creation in the correct format and data type, each of the quality\n        warnings that will be recorded in the log table.\n\n        Args:\n            date (datetime.date): study date, for which the warnings are being calculated\n            error_code (int): code of the error\n            value (float): observed percentage of this specific error for the study date\n            upper_control_limit (float): upper control limit, used as threshold for the warning\n            display_warning (bool): whether the warning should be raised or not. It is currently independent of\n                the arguments values, but in theory it should be equal to (value &gt; control_limit)\n        \"\"\"\n        self.warning_long_format.append((date, error_code, value, upper_control_limit, display_warning))\n\n    def set_output_log_table(self):\n        \"\"\"\n        Method that formats the warnings into the expected table format\n        \"\"\"\n        warning_logs = pd.DataFrame(\n            self.warning_long_format, columns=[ColNames.date, ColNames.type_of_error, \"value\", \"UCL\", \"display\"]\n        ).pivot(index=ColNames.date, columns=[ColNames.type_of_error])\n        column_names = []\n        for name, code in warning_logs.columns:\n            if name == \"value\":\n                column_names.append(f\"Error {code}\")\n            elif name == \"UCL\":\n                column_names.append(f\"Error {code} upper control limit\")\n            elif name == \"display\":\n                column_names.append(f\"Error {code} display warning\")\n        warning_logs.columns = column_names\n        warning_logs = warning_logs.assign(execution_id=self.timestamp)\n        warning_logs = warning_logs.reset_index().assign(\n            **{\n                ColNames.year: lambda df_: df_[\"date\"].apply(lambda x: x.year),\n                ColNames.month: lambda df_: df_[\"date\"].apply(lambda x: x.month),\n                ColNames.day: lambda df_: df_[\"date\"].apply(lambda x: x.day),\n            }\n        )\n\n        # Force expected order of columns\n        warning_logs = warning_logs[SilverEventSemanticQualityWarningsLogTable.SCHEMA.names]\n\n        log_table_df = self.spark.createDataFrame(\n            warning_logs, schema=SilverEventSemanticQualityWarningsLogTable.SCHEMA\n        )\n\n        self.output_data_objects[SilverEventSemanticQualityWarningsLogTable.ID].df = log_table_df\n\n    def create_plots_data(self, error_stats):\n        \"\"\"\n        Method that takes the data needed to generate the required plots, and formats it for\n        easier plotting and saving later on.\n        \"\"\"\n        plot_data = []\n\n        def format_error_code(code):\n            if code == SemanticErrorType.NO_ERROR:\n                return \"No Error\"\n\n            return f\"Error {code}\"\n\n        for date in error_stats:\n            for error_code in error_stats[date][\"count\"]:\n                plot_data.append(\n                    Row(\n                        **{\n                            ColNames.date: date,\n                            ColNames.type_of_error: format_error_code(error_code),\n                            ColNames.value: float(error_stats[date][\"count\"][error_code]),\n                            ColNames.variable: \"Number of occurrences\",\n                            ColNames.year: self.date_of_study.year,\n                            ColNames.month: self.date_of_study.month,\n                            ColNames.day: self.date_of_study.day,\n                            ColNames.timestamp: self.timestamp,\n                        }\n                    )\n                )\n            for error_code in error_stats[date][\"percentage\"]:\n                plot_data.append(\n                    Row(\n                        **{\n                            ColNames.date: date,\n                            ColNames.type_of_error: format_error_code(error_code),\n                            ColNames.value: float(error_stats[date][\"percentage\"][error_code]),\n                            ColNames.variable: \"Percentage\",\n                            ColNames.year: self.date_of_study.year,\n                            ColNames.month: self.date_of_study.month,\n                            ColNames.day: self.date_of_study.day,\n                            ColNames.timestamp: self.timestamp,\n                        }\n                    )\n                )\n        self.output_data_objects[SilverEventSemanticQualityWarningsBarPlotData.ID].df = self.spark.createDataFrame(\n            plot_data, schema=SilverEventSemanticQualityWarningsBarPlotData.SCHEMA\n        )\n\n    def write(self):\n        # Write regular data objects\n        self.logger.info(\"Writing data objects...\")\n        super().write()\n        self.logger.info(\"Data objects written.\")\n</code></pre>"},{"location":"reference/components/quality/semantic_quality_warnings/semantic_quality_warnings/#components.quality.semantic_quality_warnings.semantic_quality_warnings.SemanticQualityWarnings.create_plots_data","title":"<code>create_plots_data(error_stats)</code>","text":"<p>Method that takes the data needed to generate the required plots, and formats it for easier plotting and saving later on.</p> Source code in <code>multimno/components/quality/semantic_quality_warnings/semantic_quality_warnings.py</code> <pre><code>def create_plots_data(self, error_stats):\n    \"\"\"\n    Method that takes the data needed to generate the required plots, and formats it for\n    easier plotting and saving later on.\n    \"\"\"\n    plot_data = []\n\n    def format_error_code(code):\n        if code == SemanticErrorType.NO_ERROR:\n            return \"No Error\"\n\n        return f\"Error {code}\"\n\n    for date in error_stats:\n        for error_code in error_stats[date][\"count\"]:\n            plot_data.append(\n                Row(\n                    **{\n                        ColNames.date: date,\n                        ColNames.type_of_error: format_error_code(error_code),\n                        ColNames.value: float(error_stats[date][\"count\"][error_code]),\n                        ColNames.variable: \"Number of occurrences\",\n                        ColNames.year: self.date_of_study.year,\n                        ColNames.month: self.date_of_study.month,\n                        ColNames.day: self.date_of_study.day,\n                        ColNames.timestamp: self.timestamp,\n                    }\n                )\n            )\n        for error_code in error_stats[date][\"percentage\"]:\n            plot_data.append(\n                Row(\n                    **{\n                        ColNames.date: date,\n                        ColNames.type_of_error: format_error_code(error_code),\n                        ColNames.value: float(error_stats[date][\"percentage\"][error_code]),\n                        ColNames.variable: \"Percentage\",\n                        ColNames.year: self.date_of_study.year,\n                        ColNames.month: self.date_of_study.month,\n                        ColNames.day: self.date_of_study.day,\n                        ColNames.timestamp: self.timestamp,\n                    }\n                )\n            )\n    self.output_data_objects[SilverEventSemanticQualityWarningsBarPlotData.ID].df = self.spark.createDataFrame(\n        plot_data, schema=SilverEventSemanticQualityWarningsBarPlotData.SCHEMA\n    )\n</code></pre>"},{"location":"reference/components/quality/semantic_quality_warnings/semantic_quality_warnings/#components.quality.semantic_quality_warnings.semantic_quality_warnings.SemanticQualityWarnings.get_thresholds","title":"<code>get_thresholds()</code>","text":"<p>Method that reads the threshold-related parameters, contained in the config file, and saves them in memory to use them instead of the ones specified by default.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>non-numerical value that cannot be parsed to float (or int) has been used in the config file</p> <code>ValueError</code> <p>Negative value for a given parameter has been given, when only non-negative values make sense and are thus allowed.</p> <p>Returns:</p> Name Type Description <code>thresholds</code> <code>dict</code> <p>dict containing the different thresholds used for computing the quality warnings, with the same structure as multimno.core.constants.network_default_thresholds.ASEMANTIC_DEFAULT_THRESHOLDS</p> Source code in <code>multimno/components/quality/semantic_quality_warnings/semantic_quality_warnings.py</code> <pre><code>def get_thresholds(self) -&gt; dict:\n    \"\"\"\n    Method that reads the threshold-related parameters, contained in the config file,\n    and saves them in memory to use them instead of the ones specified by default.\n\n    Raises:\n        ValueError: non-numerical value that cannot be parsed to float (or int) has been used in\n            the config file\n        ValueError: Negative value for a given parameter has been given, when only\n            non-negative values make sense and are thus allowed.\n\n    Returns:\n        thresholds: dict containing the different thresholds used for computing the\n            quality warnings, with the same structure as\n            multimno.core.constants.network_default_thresholds.ASEMANTIC_DEFAULT_THRESHOLDS\n    \"\"\"\n    config_thresholds = self.config.geteval(self.COMPONENT_ID, \"thresholds\")\n    thresholds = SEMANTIC_DEFAULT_THRESHOLDS\n\n    for error_key in config_thresholds.keys():\n        if error_key not in SEMANTIC_DEFAULT_THRESHOLDS:\n            self.logger.info(f\"Parameter key {error_key} unknown -- ignored\")\n            continue\n\n        for param_key, val in config_thresholds[error_key].items():\n            if param_key not in SEMANTIC_DEFAULT_THRESHOLDS[error_key]:\n                self.logger.info(f\"Unknown parameter {param_key} under {error_key} -- ignored\")\n                continue\n\n            if param_key == \"sd_lookback_days\":\n                try:\n                    val = int(val)\n                except ValueError as e:\n                    error_msg = (\n                        f\"Expected integer value for parameter {param_key} under {error_key} - got {val} instead.\"\n                    )\n                    self.logger.error(error_msg)\n                    raise e\n            else:\n                try:\n                    val = float(val)\n                except ValueError as e:\n                    error_msg = (\n                        f\"Expected numeric value for parameter {param_key} under {error_key} - got {val} instead.\"\n                    )\n                    self.logger.error(error_msg)\n                    raise e\n\n            if val &lt; 0:\n                error_msg = f\"Parameter {param_key} under {error_key} must be non-negative - got {val}\"\n                self.logger.error(error_msg)\n                raise ValueError(error_msg)\n\n            thresholds[error_key][param_key] = val\n\n    return thresholds\n</code></pre>"},{"location":"reference/components/quality/semantic_quality_warnings/semantic_quality_warnings/#components.quality.semantic_quality_warnings.semantic_quality_warnings.SemanticQualityWarnings.quality_warnings_by_error","title":"<code>quality_warnings_by_error(error_name, error_stats)</code>","text":"<p>Method that generates the quality warnings that will be recorded in the output log table, for each type of error.</p> <p>In the case that the data needed for a specific error's lookback period is not present, only the current date's error percentage is computed and no warning is raised.</p> <p>Parameters:</p> Name Type Description Default <code>error_name</code> <code>str</code> <p>name of the error, an attribute of multimno.core.constants.error_types.SemanticErrorType</p> required <code>error_stats</code> <code>dict</code> <p>contains different values concerning each type of error, its counts, percentages, etc.</p> required Source code in <code>multimno/components/quality/semantic_quality_warnings/semantic_quality_warnings.py</code> <pre><code>def quality_warnings_by_error(self, error_name: str, error_stats: dict):\n    \"\"\"Method that generates the quality warnings that will be recorded in the output log table,\n    for each type of error.\n\n    In the case that the data needed for a specific error's lookback period is not present, only the current date's\n    error percentage is computed and no warning is raised.\n\n    Args:\n        error_name (str): name of the error, an attribute of multimno.core.constants.error_types.SemanticErrorType\n        error_stats (dict): contains different values concerning each type of error, its counts, percentages, etc.\n    \"\"\"\n    # Get the code of the error given its name\n    error_code = getattr(SemanticErrorType, error_name)\n\n    # lookback days for this error\n    lookback_span = self.thresholds[error_name][\"sd_lookback_days\"]\n    lookback_dates = [self.date_of_study - datetime.timedelta(days=dd) for dd in range(1, lookback_span + 1)]\n\n    if not all(lookback_date in error_stats.keys() for lookback_date in lookback_dates):\n        # cannot compute lookback mean and average, so only showing this date's percentages\n        self.register_warning(\n            date=self.date_of_study,\n            error_code=error_code,\n            value=error_stats[self.date_of_study][\"percentage\"][error_code],\n            upper_control_limit=None,\n            display_warning=False,\n        )\n    else:\n        if lookback_span &lt; self.MINIMUM_STD_LOOKBACK_DAYS:\n            upper_control_limit = self.thresholds[error_name][\"min_percentage\"]\n            self.logger.info(\n                f\"Lookback days for {error_name} lower than {self.MINIMUM_STD_LOOKBACK_DAYS} - using fixed \"\n                f\"threshold {upper_control_limit}% instead of using average and standard deviation\"\n            )\n        else:\n            previous_avg = sum(error_stats[dd][\"percentage\"][error_code] for dd in lookback_dates) / lookback_span\n            previous_std = math.sqrt(\n                sum((error_stats[dd][\"percentage\"][error_code] - previous_avg) ** 2 for dd in lookback_dates)\n                / (lookback_span - 1)\n            )\n\n            upper_control_limit = previous_avg + self.thresholds[error_name][\"min_sd\"] * previous_std\n\n        # Now compare with todays value\n        if error_stats[self.date_of_study][\"percentage\"][error_code] &gt; upper_control_limit:\n            display_warning = True\n        else:\n            display_warning = False\n\n        self.register_warning(\n            date=self.date_of_study,\n            error_code=error_code,\n            value=error_stats[self.date_of_study][\"percentage\"][error_code],\n            upper_control_limit=upper_control_limit,\n            display_warning=display_warning,\n        )\n</code></pre>"},{"location":"reference/components/quality/semantic_quality_warnings/semantic_quality_warnings/#components.quality.semantic_quality_warnings.semantic_quality_warnings.SemanticQualityWarnings.register_warning","title":"<code>register_warning(date, error_code, value, upper_control_limit, display_warning)</code>","text":"<p>Method that abstracts away the creation in the correct format and data type, each of the quality warnings that will be recorded in the log table.</p> <p>Parameters:</p> Name Type Description Default <code>date</code> <code>date</code> <p>study date, for which the warnings are being calculated</p> required <code>error_code</code> <code>int</code> <p>code of the error</p> required <code>value</code> <code>float</code> <p>observed percentage of this specific error for the study date</p> required <code>upper_control_limit</code> <code>float</code> <p>upper control limit, used as threshold for the warning</p> required <code>display_warning</code> <code>bool</code> <p>whether the warning should be raised or not. It is currently independent of the arguments values, but in theory it should be equal to (value &gt; control_limit)</p> required Source code in <code>multimno/components/quality/semantic_quality_warnings/semantic_quality_warnings.py</code> <pre><code>def register_warning(\n    self, date: datetime.date, error_code: int, value: float, upper_control_limit: float, display_warning: bool\n):\n    \"\"\"Method that abstracts away the creation in the correct format and data type, each of the quality\n    warnings that will be recorded in the log table.\n\n    Args:\n        date (datetime.date): study date, for which the warnings are being calculated\n        error_code (int): code of the error\n        value (float): observed percentage of this specific error for the study date\n        upper_control_limit (float): upper control limit, used as threshold for the warning\n        display_warning (bool): whether the warning should be raised or not. It is currently independent of\n            the arguments values, but in theory it should be equal to (value &gt; control_limit)\n    \"\"\"\n    self.warning_long_format.append((date, error_code, value, upper_control_limit, display_warning))\n</code></pre>"},{"location":"reference/components/quality/semantic_quality_warnings/semantic_quality_warnings/#components.quality.semantic_quality_warnings.semantic_quality_warnings.SemanticQualityWarnings.set_output_log_table","title":"<code>set_output_log_table()</code>","text":"<p>Method that formats the warnings into the expected table format</p> Source code in <code>multimno/components/quality/semantic_quality_warnings/semantic_quality_warnings.py</code> <pre><code>def set_output_log_table(self):\n    \"\"\"\n    Method that formats the warnings into the expected table format\n    \"\"\"\n    warning_logs = pd.DataFrame(\n        self.warning_long_format, columns=[ColNames.date, ColNames.type_of_error, \"value\", \"UCL\", \"display\"]\n    ).pivot(index=ColNames.date, columns=[ColNames.type_of_error])\n    column_names = []\n    for name, code in warning_logs.columns:\n        if name == \"value\":\n            column_names.append(f\"Error {code}\")\n        elif name == \"UCL\":\n            column_names.append(f\"Error {code} upper control limit\")\n        elif name == \"display\":\n            column_names.append(f\"Error {code} display warning\")\n    warning_logs.columns = column_names\n    warning_logs = warning_logs.assign(execution_id=self.timestamp)\n    warning_logs = warning_logs.reset_index().assign(\n        **{\n            ColNames.year: lambda df_: df_[\"date\"].apply(lambda x: x.year),\n            ColNames.month: lambda df_: df_[\"date\"].apply(lambda x: x.month),\n            ColNames.day: lambda df_: df_[\"date\"].apply(lambda x: x.day),\n        }\n    )\n\n    # Force expected order of columns\n    warning_logs = warning_logs[SilverEventSemanticQualityWarningsLogTable.SCHEMA.names]\n\n    log_table_df = self.spark.createDataFrame(\n        warning_logs, schema=SilverEventSemanticQualityWarningsLogTable.SCHEMA\n    )\n\n    self.output_data_objects[SilverEventSemanticQualityWarningsLogTable.ID].df = log_table_df\n</code></pre>"},{"location":"reference/core/","title":"core","text":""},{"location":"reference/core/component/","title":"component","text":"<p>Module that defines the abstract pipeline component class</p>"},{"location":"reference/core/component/#core.component.Component","title":"<code>Component</code>","text":"<p>Class that models a pipeline component.</p> Source code in <code>multimno/core/component.py</code> <pre><code>class Component(metaclass=ABCMeta):\n    \"\"\"\n    Class that models a pipeline component.\n    \"\"\"\n\n    COMPONENT_ID: str = None\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        self.input_data_objects: Dict[str, DataObject] = None\n        self.output_data_objects: Dict[str, DataObject] = None\n\n        # Initialize variables\n        self.config: ConfigParser = parse_configuration(general_config_path, component_config_path)\n        self.logger: Logger = generate_logger(self.config, self.COMPONENT_ID)\n        self.spark: SparkSession = generate_spark_session(self.config)\n        self.initalize_data_objects()\n\n        # Log configuration\n        self.log_config()\n\n    @abstractmethod\n    def initalize_data_objects(self):\n        \"\"\"\n        Method that initializes the data objects associated with the component.\n        \"\"\"\n\n    def read(self):\n        \"\"\"\n        Method that performs the read operation of the input data objects of the component.\n        \"\"\"\n        for data_object in self.input_data_objects.values():\n            data_object.read()\n\n    @abstractmethod\n    def transform(self):\n        \"\"\"\n        Method that performs the data transformations needed to set the dataframes of the output\n         data objects from the input data objects.\n        \"\"\"\n\n    def write(self):\n        \"\"\"\n        Method that performs the write operation of the output data objects.\n        \"\"\"\n        for data_object in self.output_data_objects.values():\n            data_object.write()\n\n    @get_execution_stats\n    def execute(self):\n        \"\"\"\n        Method that performs the read, transform and write methods of the component.\n        \"\"\"\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n        self.transform()\n        self.write()\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def log_config(self):\n        \"\"\"\n        Method that logs all sections and key-value pairs of a ConfigParser object.\n        \"\"\"\n        # Validation\n        if self.config is None or self.logger is None:\n            return\n\n        # Log each section in order\n        for section in self.config.sections():\n            self.logger.info(f\"[{section}]\")\n            for key, value in self.config.items(section):\n                self.logger.info(f\"{key}: {value}\")\n            # Break line for each section\n            self.logger.info(\"\")\n</code></pre>"},{"location":"reference/core/component/#core.component.Component.execute","title":"<code>execute()</code>","text":"<p>Method that performs the read, transform and write methods of the component.</p> Source code in <code>multimno/core/component.py</code> <pre><code>@get_execution_stats\ndef execute(self):\n    \"\"\"\n    Method that performs the read, transform and write methods of the component.\n    \"\"\"\n    self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n    self.read()\n    self.transform()\n    self.write()\n    self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n</code></pre>"},{"location":"reference/core/component/#core.component.Component.initalize_data_objects","title":"<code>initalize_data_objects()</code>  <code>abstractmethod</code>","text":"<p>Method that initializes the data objects associated with the component.</p> Source code in <code>multimno/core/component.py</code> <pre><code>@abstractmethod\ndef initalize_data_objects(self):\n    \"\"\"\n    Method that initializes the data objects associated with the component.\n    \"\"\"\n</code></pre>"},{"location":"reference/core/component/#core.component.Component.log_config","title":"<code>log_config()</code>","text":"<p>Method that logs all sections and key-value pairs of a ConfigParser object.</p> Source code in <code>multimno/core/component.py</code> <pre><code>def log_config(self):\n    \"\"\"\n    Method that logs all sections and key-value pairs of a ConfigParser object.\n    \"\"\"\n    # Validation\n    if self.config is None or self.logger is None:\n        return\n\n    # Log each section in order\n    for section in self.config.sections():\n        self.logger.info(f\"[{section}]\")\n        for key, value in self.config.items(section):\n            self.logger.info(f\"{key}: {value}\")\n        # Break line for each section\n        self.logger.info(\"\")\n</code></pre>"},{"location":"reference/core/component/#core.component.Component.read","title":"<code>read()</code>","text":"<p>Method that performs the read operation of the input data objects of the component.</p> Source code in <code>multimno/core/component.py</code> <pre><code>def read(self):\n    \"\"\"\n    Method that performs the read operation of the input data objects of the component.\n    \"\"\"\n    for data_object in self.input_data_objects.values():\n        data_object.read()\n</code></pre>"},{"location":"reference/core/component/#core.component.Component.transform","title":"<code>transform()</code>  <code>abstractmethod</code>","text":"<p>Method that performs the data transformations needed to set the dataframes of the output  data objects from the input data objects.</p> Source code in <code>multimno/core/component.py</code> <pre><code>@abstractmethod\ndef transform(self):\n    \"\"\"\n    Method that performs the data transformations needed to set the dataframes of the output\n     data objects from the input data objects.\n    \"\"\"\n</code></pre>"},{"location":"reference/core/component/#core.component.Component.write","title":"<code>write()</code>","text":"<p>Method that performs the write operation of the output data objects.</p> Source code in <code>multimno/core/component.py</code> <pre><code>def write(self):\n    \"\"\"\n    Method that performs the write operation of the output data objects.\n    \"\"\"\n    for data_object in self.output_data_objects.values():\n        data_object.write()\n</code></pre>"},{"location":"reference/core/configuration/","title":"configuration","text":"<p>Module that manages the application configuration.</p>"},{"location":"reference/core/configuration/#core.configuration.parse_configuration","title":"<code>parse_configuration(general_config_path, component_config_path='')</code>","text":"<p>Function that parses a list of configurations in a single ConfigParser object. It expects the first element of the list to be the path to general configuration path. It will override values of the general configuration file with component configuration data.</p> <p>Parameters:</p> Name Type Description Default <code>general_config_path</code> <code>list</code> <p>Path to the general configuration file.</p> required <code>component_config_path</code> <code>str</code> <p>Path to the component configuration file.</p> <code>''</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the general configuration path is doesn't exist</p> <p>Returns:</p> Name Type Description <code>config</code> <code>ConfigParser</code> <p>ConfigParser object with the configuration data.</p> Source code in <code>multimno/core/configuration.py</code> <pre><code>def parse_configuration(general_config_path: str, component_config_path: str = \"\") -&gt; ConfigParser:\n    \"\"\"Function that parses a list of configurations in a single ConfigParser object. It expects\n    the first element of the list to be the path to general configuration path. It will override\n    values of the general configuration file with component configuration data.\n\n    Args:\n        general_config_path (list): Path to the general configuration file.\n        component_config_path (str): Path to the component configuration file.\n\n    Raises:\n        FileNotFoundError: If the general configuration path is doesn't exist\n\n    Returns:\n        config: ConfigParser object with the configuration data.\n    \"\"\"\n\n    # Check general configuration file\n    if not os.path.exists(general_config_path):\n        raise FileNotFoundError(f\"General Config file Not found: {general_config_path}\")\n\n    config_paths = [general_config_path, component_config_path]\n\n    converters = {\n        \"list\": lambda val: [i.strip() for i in val.strip().split(\"\\n\")],\n        \"eval\": eval,\n    }\n\n    parser: ConfigParser = ConfigParser(\n        converters=converters, interpolation=ExtendedInterpolation(), inline_comment_prefixes=\"#\"\n    )\n    parser.optionxform = str\n    parser.read(config_paths)\n\n    return parser\n</code></pre>"},{"location":"reference/core/grid/","title":"grid","text":"<p>This module provides functionality for generating a grid based on the INSPIRE grid system specification.</p>"},{"location":"reference/core/grid/#core.grid.GridGenerator","title":"<code>GridGenerator</code>","text":"<p>Abstract class that provides functionality for generating a grid.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>class GridGenerator(metaclass=ABCMeta):\n    \"\"\"\n    Abstract class that provides functionality for generating a grid.\n    \"\"\"\n\n    def __init__(self, spark: SparkSession) -&gt; None:\n        self.spark: SparkSession = spark\n\n    @abstractmethod\n    def cover_extent_with_grid_ids(self, extent: tuple) -&gt; DataFrame:\n        \"\"\"Cover given extent with grid_ids on given resolution.\"\"\"\n\n    @abstractmethod\n    def cover_polygon_with_grid_ids(self, polygon_sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"Cover given polygon with grid_ids on given resolution.\"\"\"\n\n    @abstractmethod\n    def grid_ids_to_centroids(self, sdf: DataFrame, to_crs: int) -&gt; DataFrame:\n        \"\"\"Get geometry centroids from grid_ids with given coordinate system.\"\"\"\n\n    @abstractmethod\n    def grid_ids_to_tiles(self, sdf: DataFrame, to_crs: int) -&gt; DataFrame:\n        \"\"\"Get grid polygons from grid_ids with given coordinate system.\"\"\"\n\n    @abstractmethod\n    def get_parent_grid_ids(self, sdf: DataFrame, resolution: int) -&gt; DataFrame:\n        \"\"\"Get parent grid_id on given resolution.\"\"\"\n\n    @abstractmethod\n    def get_children_grid_ids(self, sdf: DataFrame, resolution: int) -&gt; DataFrame:\n        \"\"\"Get children grid_ids on given resolution.\"\"\"\n</code></pre>"},{"location":"reference/core/grid/#core.grid.GridGenerator.cover_extent_with_grid_ids","title":"<code>cover_extent_with_grid_ids(extent)</code>  <code>abstractmethod</code>","text":"<p>Cover given extent with grid_ids on given resolution.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>@abstractmethod\ndef cover_extent_with_grid_ids(self, extent: tuple) -&gt; DataFrame:\n    \"\"\"Cover given extent with grid_ids on given resolution.\"\"\"\n</code></pre>"},{"location":"reference/core/grid/#core.grid.GridGenerator.cover_polygon_with_grid_ids","title":"<code>cover_polygon_with_grid_ids(polygon_sdf)</code>  <code>abstractmethod</code>","text":"<p>Cover given polygon with grid_ids on given resolution.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>@abstractmethod\ndef cover_polygon_with_grid_ids(self, polygon_sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"Cover given polygon with grid_ids on given resolution.\"\"\"\n</code></pre>"},{"location":"reference/core/grid/#core.grid.GridGenerator.get_children_grid_ids","title":"<code>get_children_grid_ids(sdf, resolution)</code>  <code>abstractmethod</code>","text":"<p>Get children grid_ids on given resolution.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>@abstractmethod\ndef get_children_grid_ids(self, sdf: DataFrame, resolution: int) -&gt; DataFrame:\n    \"\"\"Get children grid_ids on given resolution.\"\"\"\n</code></pre>"},{"location":"reference/core/grid/#core.grid.GridGenerator.get_parent_grid_ids","title":"<code>get_parent_grid_ids(sdf, resolution)</code>  <code>abstractmethod</code>","text":"<p>Get parent grid_id on given resolution.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>@abstractmethod\ndef get_parent_grid_ids(self, sdf: DataFrame, resolution: int) -&gt; DataFrame:\n    \"\"\"Get parent grid_id on given resolution.\"\"\"\n</code></pre>"},{"location":"reference/core/grid/#core.grid.GridGenerator.grid_ids_to_centroids","title":"<code>grid_ids_to_centroids(sdf, to_crs)</code>  <code>abstractmethod</code>","text":"<p>Get geometry centroids from grid_ids with given coordinate system.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>@abstractmethod\ndef grid_ids_to_centroids(self, sdf: DataFrame, to_crs: int) -&gt; DataFrame:\n    \"\"\"Get geometry centroids from grid_ids with given coordinate system.\"\"\"\n</code></pre>"},{"location":"reference/core/grid/#core.grid.GridGenerator.grid_ids_to_tiles","title":"<code>grid_ids_to_tiles(sdf, to_crs)</code>  <code>abstractmethod</code>","text":"<p>Get grid polygons from grid_ids with given coordinate system.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>@abstractmethod\ndef grid_ids_to_tiles(self, sdf: DataFrame, to_crs: int) -&gt; DataFrame:\n    \"\"\"Get grid polygons from grid_ids with given coordinate system.\"\"\"\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator","title":"<code>InspireGridGenerator</code>","text":"<p>               Bases: <code>GridGenerator</code></p> <p>A class used to generate a grid based on the INSPIRE grid system specification.</p> <p>Attributes:</p> Name Type Description <code>GRID_CRS_EPSG_CODE</code> <code>int</code> <p>The EPSG code for the grid's CRS.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>class InspireGridGenerator(GridGenerator):\n    \"\"\"A class used to generate a grid based on the INSPIRE grid system specification.\n\n    Attributes:\n        GRID_CRS_EPSG_CODE (int): The EPSG code for the grid's CRS.\n    \"\"\"\n\n    GRID_CRS_EPSG_CODE = 3035\n    PROJ_COORD_INT_SIZE = 7\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        resolution=100,\n        geometry_col_name: str = \"geometry\",\n        grid_id_col_name: str = \"grid_id\",\n        grid_partition_size: int = 2000,\n    ) -&gt; None:\n        \"\"\"Initializes the InspireGridGenerator with the given parameters.\n\n        Args:\n            spark (SparkSession): The SparkSession to use.\n            resolution (int, optional): The resolution of the grid. Defaults to 100. Has to be divisible by 100.\n            geometry_col_name (str, optional): The name of the geometry column. Defaults to 'geometry'.\n            grid_id_col_name (str, optional): The name of the grid ID column. Defaults to 'grid_id'.\n            grid_partition_size (int, optional): The size of the grid partitions, defined as number of tiles\n                in x and y dimensions of subdivisions of the intital grid. Defaults to 2000.\n\n        Raises:\n            ValueError: If the resolution is not divisible by 100.\n        \"\"\"\n        if resolution % 100 != 0:\n            raise ValueError(\"Resolution must be divisible by 100\")\n\n        super().__init__(spark)\n        self.geometry_col_name = geometry_col_name\n        self.grid_id_col_name = grid_id_col_name\n        self.resolution = resolution\n        self.grid_partition_size = grid_partition_size\n        self.resolution_str = self._format_distance(resolution)\n\n    @staticmethod\n    def _format_distance(value: int) -&gt; str:\n        \"\"\"Formats the given distance value to string.\n\n        Args:\n            value (int): The distance value to format.\n\n        Returns:\n            str: The formatted distance value.\n        \"\"\"\n        if value &lt; 1000:\n            return f\"{value}m\"\n        else:\n            if value % 1000 != 0:\n                raise ValueError(f\"Distance to be formatted not multiple of 1000: {value}\")\n            return f\"{value // 1000}km\"\n\n    def _project_latlon_extent(self, extent: List[float]) -&gt; Union[List[float], List[float]]:\n        \"\"\"Projects the given extent from lat/lon to the grid's CRS.\n\n        Args:\n            extent (List[float]): The extent to project. Order: [lon_min, lat_min, lon_max, lat_max]\n\n        Returns:\n            List[float]: The projected extent.\n        \"\"\"\n        transformer = Transformer.from_crs(\"EPSG:4326\", f\"EPSG:{self.GRID_CRS_EPSG_CODE}\")\n        # EPSG4326: xx -&gt; lat, yy -&gt; lon\n        # EPSG3035: xx -&gt; northing, yy -&gt; easting\n        xx_bottomleft, yy_bottomleft = transformer.transform(extent[1], extent[0])  # bottom-left corner\n        xx_topright, yy_topright = transformer.transform(extent[3], extent[2])  # top-right corner\n        xx_bottomright, yy_bottomright = transformer.transform(extent[1], extent[2])  # bottom-right corner\n        xx_topleft, yy_topleft = transformer.transform(extent[3], extent[0])\n\n        return (\n            [xx_bottomleft, yy_bottomleft, xx_topright, yy_topright],\n            [xx_bottomright, yy_bottomright, xx_topleft, yy_topleft],\n        )\n\n    @staticmethod\n    def _project_bounding_box(extent: List[float], auxiliar_coords: List[float]) -&gt; Tuple[List[float], List[float]]:\n        \"\"\"Returns the bottom-left and top-right coordinates of the rectangular bounding box in the projected CRS\n        that covers the bounding box defined from the bottom-left and top-right corners in lat/lon.\n\n        Args:\n            extent (List[float]): Coordinates in the projected CRS that are the transformation of the minimum and\n                maximum latitude and longitude, in [x_bottomleft, y_bottomleft, x_topright, y_topright] order.\n            auxiliar_coords (List[float]): Auxiliar coordinates in the prohected CRS that are the transformation\n                of the other two cornes of the lat/lon rectangular bounding box, in\n                [x_bottomright, y_bottomright, x_topleft, y_topleft] order\n\n        Returns:\n            List[float]: The projected extent, in [x_bottomleft, y_bottomleft, x_topright, y_topright] order.\n            List[float]: Raster cover bounds, in [x_topleft, y_topleft, x_bottomright, y_bottomright] order.\n        \"\"\"\n        cover_x_bottomleft = min(extent[0], auxiliar_coords[0])  # min lat\n        cover_y_bottomleft = min(extent[1], auxiliar_coords[3])  # min lon\n\n        cover_x_topright = max(extent[2], auxiliar_coords[2])  # max lat\n        cover_y_topright = max(extent[3], auxiliar_coords[1])  # max lon\n\n        cover_x_topleft = max(extent[2], auxiliar_coords[2])  # max lat\n        cover_y_topleft = min(extent[1], auxiliar_coords[3])  # min lon\n\n        cover_x_bottomright = min(extent[0], auxiliar_coords[0])  # min lat\n        cover_y_bottomright = max(extent[3], auxiliar_coords[1])  # max lon\n\n        return (\n            [cover_y_bottomleft, cover_x_bottomleft, cover_y_topright, cover_x_topright],\n            [cover_x_topleft, cover_y_topleft, cover_x_bottomright, cover_y_bottomright],\n        )\n\n    def _snap_extent_to_grid(self, extent: List[float]) -&gt; List[float]:\n        \"\"\"Snaps the given extent to the grid.\n\n        Args:\n            extent (List[float]): The extent to snap.\n\n        Returns:\n            List[float]: The snapped extent.\n        \"\"\"\n        return [round(coord / self.resolution) * self.resolution for coord in extent]\n\n    def _extend_grid_extent(self, extent: List[float], extension_factor: int = 5) -&gt; List[float]:\n        \"\"\"Extends the given extent by the specified factor in all directions.\n\n        Args:\n            extent (List[float]): The extent to extend.\n            extension_factor (int, optional): The factor by which to extend the extent. Defaults to 5.\n\n        Returns:\n            List[float]: The extended extent.\n        \"\"\"\n        extension_size = self.resolution * extension_factor\n        return [\n            extent[0] - extension_size,\n            extent[1] - extension_size,\n            extent[2] + extension_size,\n            extent[3] + extension_size,\n        ]\n\n    def _extend_grid_raster_bounds(self, raster_bounds: List[float], extension_factor: int = 5) -&gt; List[float]:\n        \"\"\"Extends the given extent by the specified factor in all directions.\n\n        Args:.\n            extent (List[float]): Raster cover bounds, in [x_topleft, y_topleft, x_bottomright, y_bottomright] order.\n\n            extension_factor (int, optional): The factor by which to extend the extent. Defaults to 5.\n\n        Returns:\n            List[float]: The extended extent.\n        \"\"\"\n        extension_size = self.resolution * extension_factor\n        return [\n            raster_bounds[0] + extension_size,  # x topleft\n            raster_bounds[1] - extension_size,  # y topleft\n            raster_bounds[2] - extension_size,  # x bottomright\n            raster_bounds[3] + extension_size,  # y bottomright\n        ]\n\n    def _get_grid_height(self, raster_bounds: List[float]) -&gt; int:\n        \"\"\"Calculates the height of the grid for the given extent.\n\n        Args:\n            raster_bounds (List[float]): The raster_bounds for which to calculate the grid height.\n\n        Returns:\n            int: The grid height.\n        \"\"\"\n        return int((raster_bounds[0] - raster_bounds[2]) / self.resolution)\n\n    def _get_grid_width(self, raster_bounds: List[float]) -&gt; int:\n        \"\"\"Calculates the width of the grid for the given extent.\n\n        Args:\n            raster_bounds (List[float]): The raster_bounds for which to calculate the grid width.\n\n        Returns:\n            int: The grid width.\n        \"\"\"\n        return int((raster_bounds[3] - raster_bounds[1]) / self.resolution)\n\n    def _get_grid_blueprint(self, extent: List[float]) -&gt; DataFrame:\n        \"\"\"Generates a blueprint for the grid for the given extent as a raster of grid resolution.\n        Splits initial raster into smaller rasters of size grid_partition_size x grid_partition_size.\n\n        Args:\n            extent (List[float]): The extent for which to generate the grid blueprint.\n\n        Returns:\n            DataFrame: The grid blueprint.\n        \"\"\"\n        extent, auxiliar_coords = self._project_latlon_extent(extent)\n        extent, raster_bounds = self._project_bounding_box(extent, auxiliar_coords)\n\n        extent = self._snap_extent_to_grid(extent)\n        raster_bounds = self._snap_extent_to_grid(raster_bounds)\n\n        extent = self._extend_grid_extent(extent)\n        raster_bounds = self._extend_grid_raster_bounds(raster_bounds)\n\n        grid_height = self._get_grid_height(raster_bounds)\n        grid_width = self._get_grid_width(raster_bounds)\n\n        # ONLY FOR EPSG:3035!!!! which has (northing, easting) order, BUT (Y, X) axis names in its EPSG (not in code)\n        # raster_bounds[1]: easting of the top left corner. \"X\" axis\n        # raster_bounds[0]: northing of the top left corner. \"Y\" axis\n\n        sdf = self.spark.sql(\n            f\"\"\"SELECT RS_MakeEmptyRaster(1, \"B\", {grid_width}, \n                                {grid_height}, \n                                {raster_bounds[1]},\n                                {raster_bounds[0]}, \n                                {self.resolution}, \n                               -{self.resolution}, 0.0, 0.0, {self.GRID_CRS_EPSG_CODE}) as raster\"\"\"\n        )\n\n        sdf = sdf.selectExpr(f\"RS_TileExplode(raster,{self.grid_partition_size}, {self.grid_partition_size})\")\n        return sdf.repartition(sdf.count())\n\n    @staticmethod\n    def _get_polygon_sdf_extent(polygon_sdf: DataFrame) -&gt; List[float]:\n        \"\"\"Gets the extent of the given polygon DataFrame.\n\n        Args:\n            polygon_sdf (DataFrame): The polygon DataFrame.\n\n        Returns:\n            List[float]: The extent of the polygon DataFrame.\n        \"\"\"\n        polygon_sdf = polygon_sdf.withColumn(\"bbox\", STF.ST_Envelope(polygon_sdf[\"geometry\"]))\n        polygon_sdf = (\n            polygon_sdf.withColumn(\"x_min\", STF.ST_XMin(polygon_sdf[\"bbox\"]))\n            .withColumn(\"y_min\", STF.ST_YMin(polygon_sdf[\"bbox\"]))\n            .withColumn(\"x_max\", STF.ST_XMax(polygon_sdf[\"bbox\"]))\n            .withColumn(\"y_max\", STF.ST_YMax(polygon_sdf[\"bbox\"]))\n        )\n\n        return polygon_sdf.select(\"x_min\", \"y_min\", \"x_max\", \"y_max\").collect()[0][0:]\n\n    def _get_grid_intersection_with_mask(self, sdf: DataFrame, polygon_sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"Gets the intersection of the grid with the given mask.\n\n        Args:\n            sdf (DataFrame): The DataFrame representing the grid.\n            polygon_sdf (DataFrame): The DataFrame representing the mask.\n\n        Returns:\n            DataFrame: The DataFrame representing the intersection of the grid with the mask.\n        \"\"\"\n        polygon_sdf = polygon_sdf.withColumn(\n            \"geometry\",\n            STF.ST_Transform(polygon_sdf[\"geometry\"], F.lit(\"EPSG:4326\"), F.lit(f\"EPSG:{self.GRID_CRS_EPSG_CODE}\")),\n        )\n\n        sdf = sdf.join(\n            polygon_sdf, STP.ST_Intersects(sdf[self.geometry_col_name], polygon_sdf[\"geometry\"]), \"inner\"\n        ).drop(polygon_sdf[\"geometry\"])\n\n        return sdf\n\n    def _get_grid_intersection_with_mask(self, sdf: DataFrame, polygon_sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"Covers the given extent with grid centroids.\n\n        Args:\n            extent (List[float]): The extent to cover.\n\n        Returns:\n            DataFrame: The DataFrame representing the grid centroids covering the extent.\n        \"\"\"\n        polygon_sdf = polygon_sdf.withColumn(\n            \"geometry\",\n            STF.ST_Transform(polygon_sdf[\"geometry\"], F.lit(\"EPSG:4326\"), F.lit(f\"EPSG:{self.GRID_CRS_EPSG_CODE}\")),\n        )\n\n        sdf = sdf.join(\n            polygon_sdf, STP.ST_Intersects(sdf[self.geometry_col_name], polygon_sdf[\"geometry\"]), \"inner\"\n        ).drop(polygon_sdf[\"geometry\"])\n\n        return sdf\n\n    def cover_extent_with_grid_centroids(self, extent: List[float]) -&gt; DataFrame:\n        \"\"\"Covers the given polygon with grid centroids.\n\n        Args:\n            extent (List[float]): The extent to cover.\n\n        Returns:\n            DataFrame: The DataFrame representing the grid centroids covering the polygon.\n        \"\"\"\n        sdf = self._get_grid_blueprint(extent)\n\n        sdf = sdf.selectExpr(\"explode(RS_PixelAsCentroids(tile, 1)) as exploded\").selectExpr(\n            f\"exploded.geom as {self.geometry_col_name}\"\n        )\n\n        sdf = sdf.withColumn(\n            self.grid_id_col_name,\n            (\n                (STF.ST_Y(sdf[\"geometry\"]) - (self.resolution / 2)).cast(LongType()) * 10**self.PROJ_COORD_INT_SIZE\n                + (STF.ST_X(sdf[\"geometry\"]) - (self.resolution / 2)).cast(LongType())\n            ).cast(LongType()),\n        )\n\n        return sdf\n\n    def cover_polygon_with_grid_centroids(self, polygon_sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"Covers the given extent with grid IDs.\n\n        Args:\n            polygon_sdf (DataFrame): The DataFrame representing the polygon.\n\n        Returns:\n            DataFrame: The DataFrame representing the grid IDs covering the extent.\n        \"\"\"\n        extent = self._get_polygon_sdf_extent(polygon_sdf)\n\n        sdf = self.cover_extent_with_grid_centroids(extent)\n\n        sdf = self._get_grid_intersection_with_mask(sdf, polygon_sdf)\n\n        return sdf\n\n    def cover_extent_with_grid_ids(self, extent: List[float]) -&gt; DataFrame:\n        \"\"\"Covers the given polygon with grid IDs.\n\n        Args:\n            extent (List[float]): The extent to cover.\n\n        Returns:\n            DataFrame: The DataFrame representing the grid IDs covering the polygon.\n        \"\"\"\n        sdf = self.cover_extent_with_grid_centroids(extent)\n\n        sdf = sdf.drop(self.geometry_col_name)\n\n        return sdf\n\n    def cover_polygon_with_grid_ids(self, polygon_sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"Covers the given polygon with grid IDs.\n\n        Args:\n            polygon_sdf (DataFrame): The DataFrame representing the polygon.\n\n        Returns:\n            DataFrame: The DataFrame representing the grid IDs covering the polygon.\n        \"\"\"\n        extent = self._get_polygon_sdf_extent(polygon_sdf)\n\n        sdf = self.cover_extent_with_grid_centroids(extent)\n\n        sdf = self._get_grid_intersection_with_mask(sdf, polygon_sdf)\n\n        sdf = sdf.drop(\"geometry\")\n\n        return sdf\n\n    def cover_extent_with_grid_tiles(self, extent: List[float]) -&gt; DataFrame:\n        \"\"\"Covers the given extent with grid tiles.\n\n        Args:\n            extent (List[float]): The extent to cover.\n\n        Returns:\n            DataFrame: The DataFrame representing the grid tiles covering the extent.\n        \"\"\"\n        sdf = self._get_grid_blueprint(extent)\n\n        sdf = sdf.selectExpr(\"explode(RS_PixelAsPolygons(tile, 1)) as exploded\").selectExpr(\n            f\"exploded.geom as {self.geometry_col_name}\"\n        )\n\n        sdf = sdf.withColumn(\n            self.grid_id_col_name,\n            (\n                STF.ST_XMin(sdf[\"geometry\"]).cast(LongType()) * 10**self.PROJ_COORD_INT_SIZE\n                + STF.ST_YMin(sdf[\"geometry\"])\n            ).cast(LongType()),\n        )\n\n        return sdf\n\n    def cover_polygon_with_grid_tiles(self, polygon_sdf: DataFrame) -&gt; DataFrame:\n        \"\"\"Covers the given polygon with grid tiles.\n\n        Args:\n            polygon_sdf (DataFrame): The DataFrame representing the polygon.\n\n        Returns:\n            DataFrame: The DataFrame representing the grid tiles covering the polygon.\n        \"\"\"\n        extent = self._get_polygon_sdf_extent(polygon_sdf)\n\n        sdf = self.cover_extent_with_grid_tiles(extent)\n\n        sdf = self._get_grid_intersection_with_mask(sdf, polygon_sdf)\n\n        return sdf\n\n    def convert_internal_id_to_inspire_specs(\n        self, sdf: DataFrame, resolution: int = None, grid_id_col: str = None\n    ) -&gt; DataFrame:\n        \"\"\"\n        Converts the integer grid id column in a DataFrame to the INSPIRE grid string format.\n\n        Args:\n            sdf (DataFrame): Input DataFrame with a column grid_id_col in integer format.\n            resolution (int, optional): The resolution of the grid. If None, it is equal to self.resolution. Defaults\n                to None\n            grid_id_col (str, optional): name of the column containing the integer grid id column. If None, it is\n                equal to self.grid_id_col_name. Defaults to None.\n\n        Returns:\n            DataFrame: A new DataFrame with the grid_id converted to string format.\n        \"\"\"\n        if resolution is None:\n            resolution = self.resolution\n            resolution_str = self.resolution_str\n        else:\n            if resolution % 100 != 0:\n                raise ValueError(f\"Invalid resolution value not divisible by 100: {resolution}\")\n            resolution_str = self._format_distance(resolution)\n\n        if grid_id_col is None:\n            grid_id_col = self.grid_id_col_name\n\n        trailing_zeros = int(log10(resolution))\n\n        sdf = sdf.withColumn(\n            grid_id_col,\n            F.concat(\n                F.lit(resolution_str),\n                F.lit(\"N\"),\n                (F.expr(f\"grid_id DIV {10**(self.PROJ_COORD_INT_SIZE + trailing_zeros)}\")),\n                F.lit(\"E\"),\n                (F.expr(f\"(grid_id % {10**self.PROJ_COORD_INT_SIZE}) DIV {10 ** trailing_zeros}\")),\n            ),\n        )\n\n        return sdf\n\n    def grid_ids_to_centroids(self, sdf: DataFrame, resolution: int = None, to_crs: int = None) -&gt; DataFrame:\n        \"\"\"Converts grid IDs to centroids.\n\n        Args:\n            sdf (DataFrame): The DataFrame containing the grid IDs.\n            to_crs (int, optional): The CRS to which to convert the centroids. If not provided,\n                the centroids will be in default grid crs.\n\n        Returns:\n            DataFrame: The DataFrame containing the centroids.\n        \"\"\"\n        if resolution is None:\n            resolution = self.resolution\n\n        sdf = sdf.withColumn(\n            self.geometry_col_name,\n            STC.ST_Point(\n                F.expr(f\"INT({self.grid_id_col_name} % {10**self.PROJ_COORD_INT_SIZE})\") + resolution / 2,\n                F.expr(f\"INT({self.grid_id_col_name} DIV {10**self.PROJ_COORD_INT_SIZE})\") + resolution / 2,\n            ),\n        )\n\n        sdf = sdf.withColumn(\n            self.geometry_col_name, STF.ST_SetSRID(sdf[self.geometry_col_name], self.GRID_CRS_EPSG_CODE)\n        )\n\n        if to_crs:\n            sdf = sdf.withColumn(\n                self.geometry_col_name, STF.ST_Transform(sdf[self.geometry_col_name], F.lit(f\"EPSG:{to_crs}\"))\n            )\n\n        return sdf\n\n    def grid_ids_to_tiles(self, sdf: DataFrame, resolution: int = None, to_crs: int = None) -&gt; DataFrame:\n        \"\"\"Converts grid IDs in INSPIRE format to tiles.\n\n        Args:\n            sdf (DataFrame): The DataFrame containing the grid IDs.\n            to_crs (int, optional): The CRS to which to convert the tiles. If not provided, the centroids\n                will be in default grid crs.\n        Returns:\n            DataFrame: The DataFrame containing the tiles.\n        \"\"\"\n\n        if resolution is None:\n            resolution = self.resolution\n\n        sdf = sdf.withColumn(\n            self.geometry_col_name,\n            STC.ST_PolygonFromEnvelope(\n                F.expr(f\"{self.grid_id_col_name} % {10**self.PROJ_COORD_INT_SIZE}\"),  # x/easting min\n                F.expr(f\"{self.grid_id_col_name} DIV {10**self.PROJ_COORD_INT_SIZE}\"),  # y/northing min\n                F.expr(f\"{self.grid_id_col_name} % {10**self.PROJ_COORD_INT_SIZE}\") + resolution,  # x/easting max\n                F.expr(f\"{self.grid_id_col_name} DIV {10**self.PROJ_COORD_INT_SIZE}\") + resolution,  # y/northing max\n            ),\n        )\n\n        sdf = sdf.withColumn(\n            self.geometry_col_name, STF.ST_SetSRID(sdf[self.geometry_col_name], self.GRID_CRS_EPSG_CODE)\n        )\n\n        if to_crs:\n            sdf = sdf.withColumn(\n                self.geometry_col_name, STF.ST_Transform(sdf[self.geometry_col_name], F.lit(f\"EPSG:{to_crs}\"))\n            )\n\n        return sdf\n\n    def get_parent_grid_ids(self, sdf: DataFrame, resolution: int, parent_col_name: str = None) -&gt; DataFrame:\n        \"\"\"\n        Coarsens grid IDs to the specified resolution, snapping down to the nearest coarser grid cell.\n\n        Args:\n            sdf (DataFrame): Input Spark DataFrame with a 14-digit grid ID column named 'grid_id'.\n            resolution (int): The desired coarser resolution (e.g., 200).\n            parent_col_name (str, optional): name of the column that will hold the parent grid IDs. If None, it will be\n                named f\"parent_{ColNames.grid_id}\". Defaults to None.\n\n        Returns:\n            DataFrame: DataFrame with coarsened grid IDs in new column.\n        \"\"\"\n        if resolution % 100 != 0:\n            raise ValueError(\"Resolution must be a multiple of 100.\")\n        if parent_col_name is None:\n            parent_col_name = f\"parent_{ColNames.grid_id}\"\n\n        # Extract `northing` and `easting` correctly\n        sdf = sdf.withColumn(\"northing\", F.expr(f\"{ColNames.grid_id} DIV {10**self.PROJ_COORD_INT_SIZE}\")).withColumn(\n            \"easting\", F.col(ColNames.grid_id) % 10**self.PROJ_COORD_INT_SIZE\n        )\n\n        # Snap `northing` and `easting` down to the nearest coarser grid boundary\n        sdf = sdf.withColumn(\"northing_parent\", F.col(\"northing\") - (F.col(\"northing\") % resolution)).withColumn(\n            \"easting_parent\", F.col(\"easting\") - (F.col(\"easting\") % resolution)\n        )\n\n        # Combine the coarsened `northing` and `easting` into `parent_grid_id`\n        sdf = sdf.withColumn(\n            parent_col_name,\n            (F.col(\"northing_parent\").cast(LongType()) * 10**self.PROJ_COORD_INT_SIZE + F.col(\"easting_parent\")),\n        ).drop(\"northing\", \"easting\", \"northing_parent\", \"easting_parent\")\n\n        return sdf\n\n    def get_children_grid_ids(\n        self, sdf: DataFrame, resolution: int, child_resolution: int, child_col_name: str = None\n    ) -&gt; DataFrame:\n        \"\"\"\n        Expands coarsened grid IDs to generate all possible child grid IDs within the specified finer resolution.\n\n        Args:\n            sdf (DataFrame): Input Spark DataFrame with a 14-digit grid ID column named 'grid_id'.\n            resolution (int): The coarser resolution (e.g., 200).\n            child_resolution (int): The finer resolution for child grids (e.g., 100).\n            child_col_name (str, optional): name of the column that will hold the children grid IDs. If None, it will be\n                named f\"child_{ColNames.grid_id}\". Defaults to None.\n\n        Returns:\n            DataFrame: DataFrame with generated child grid IDs in new column.\n        \"\"\"\n        if resolution % 100 != 0 or child_resolution % 100 != 0:\n            raise ValueError(\"Both resolutions must be multiples of 100.\")\n        if child_resolution &gt;= resolution:\n            raise ValueError(\"Child resolution must be finer than the parent resolution.\")\n        if resolution % child_resolution != 0:\n            raise ValueError(\"Parent resolution must be a multiple of the child resolution.\")\n        if child_col_name is None:\n            child_col_name = f\"child_{ColNames.grid_id}\"\n\n        factor = resolution // child_resolution\n\n        # Extract `northing` and `easting` from the parent grid ID\n        sdf = sdf.withColumn(\"northing\", F.expr(f\"{ColNames.grid_id} DIV {10**self.PROJ_COORD_INT_SIZE}\")).withColumn(\n            \"easting\", F.col(ColNames.grid_id) % 10**self.PROJ_COORD_INT_SIZE\n        )\n\n        # Generate all possible offsets for child grids\n        offsets = [(i, j) for i in range(factor) for j in range(factor)]\n        spark = sdf.sparkSession  # Get the current Spark session\n        offsets_df = spark.createDataFrame(offsets, [\"northing_offset\", \"easting_offset\"])\n\n        # Cross join with the offsets to generate all child grid IDs\n        result = (\n            sdf.crossJoin(offsets_df)\n            .withColumn(\"child_northing\", F.expr(f\"northing + northing_offset * {child_resolution}\"))\n            .withColumn(\"child_easting\", F.expr(f\"easting + easting_offset * {child_resolution}\"))\n            .withColumn(\n                child_col_name,\n                (F.col(\"child_northing\").cast(LongType()) * 10**self.PROJ_COORD_INT_SIZE + F.col(\"child_easting\")),\n            )\n            .drop(\"child_northing\", \"child_easting\", \"northing\", \"easting\")\n        )\n\n        return result\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.__init__","title":"<code>__init__(spark, resolution=100, geometry_col_name='geometry', grid_id_col_name='grid_id', grid_partition_size=2000)</code>","text":"<p>Initializes the InspireGridGenerator with the given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>The SparkSession to use.</p> required <code>resolution</code> <code>int</code> <p>The resolution of the grid. Defaults to 100. Has to be divisible by 100.</p> <code>100</code> <code>geometry_col_name</code> <code>str</code> <p>The name of the geometry column. Defaults to 'geometry'.</p> <code>'geometry'</code> <code>grid_id_col_name</code> <code>str</code> <p>The name of the grid ID column. Defaults to 'grid_id'.</p> <code>'grid_id'</code> <code>grid_partition_size</code> <code>int</code> <p>The size of the grid partitions, defined as number of tiles in x and y dimensions of subdivisions of the intital grid. Defaults to 2000.</p> <code>2000</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the resolution is not divisible by 100.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def __init__(\n    self,\n    spark: SparkSession,\n    resolution=100,\n    geometry_col_name: str = \"geometry\",\n    grid_id_col_name: str = \"grid_id\",\n    grid_partition_size: int = 2000,\n) -&gt; None:\n    \"\"\"Initializes the InspireGridGenerator with the given parameters.\n\n    Args:\n        spark (SparkSession): The SparkSession to use.\n        resolution (int, optional): The resolution of the grid. Defaults to 100. Has to be divisible by 100.\n        geometry_col_name (str, optional): The name of the geometry column. Defaults to 'geometry'.\n        grid_id_col_name (str, optional): The name of the grid ID column. Defaults to 'grid_id'.\n        grid_partition_size (int, optional): The size of the grid partitions, defined as number of tiles\n            in x and y dimensions of subdivisions of the intital grid. Defaults to 2000.\n\n    Raises:\n        ValueError: If the resolution is not divisible by 100.\n    \"\"\"\n    if resolution % 100 != 0:\n        raise ValueError(\"Resolution must be divisible by 100\")\n\n    super().__init__(spark)\n    self.geometry_col_name = geometry_col_name\n    self.grid_id_col_name = grid_id_col_name\n    self.resolution = resolution\n    self.grid_partition_size = grid_partition_size\n    self.resolution_str = self._format_distance(resolution)\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator._extend_grid_extent","title":"<code>_extend_grid_extent(extent, extension_factor=5)</code>","text":"<p>Extends the given extent by the specified factor in all directions.</p> <p>Parameters:</p> Name Type Description Default <code>extent</code> <code>List[float]</code> <p>The extent to extend.</p> required <code>extension_factor</code> <code>int</code> <p>The factor by which to extend the extent. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Type Description <code>List[float]</code> <p>List[float]: The extended extent.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def _extend_grid_extent(self, extent: List[float], extension_factor: int = 5) -&gt; List[float]:\n    \"\"\"Extends the given extent by the specified factor in all directions.\n\n    Args:\n        extent (List[float]): The extent to extend.\n        extension_factor (int, optional): The factor by which to extend the extent. Defaults to 5.\n\n    Returns:\n        List[float]: The extended extent.\n    \"\"\"\n    extension_size = self.resolution * extension_factor\n    return [\n        extent[0] - extension_size,\n        extent[1] - extension_size,\n        extent[2] + extension_size,\n        extent[3] + extension_size,\n    ]\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator._extend_grid_raster_bounds","title":"<code>_extend_grid_raster_bounds(raster_bounds, extension_factor=5)</code>","text":"<p>Extends the given extent by the specified factor in all directions.</p> <p>Args:.     extent (List[float]): Raster cover bounds, in [x_topleft, y_topleft, x_bottomright, y_bottomright] order.</p> <pre><code>extension_factor (int, optional): The factor by which to extend the extent. Defaults to 5.\n</code></pre> <p>Returns:</p> Type Description <code>List[float]</code> <p>List[float]: The extended extent.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def _extend_grid_raster_bounds(self, raster_bounds: List[float], extension_factor: int = 5) -&gt; List[float]:\n    \"\"\"Extends the given extent by the specified factor in all directions.\n\n    Args:.\n        extent (List[float]): Raster cover bounds, in [x_topleft, y_topleft, x_bottomright, y_bottomright] order.\n\n        extension_factor (int, optional): The factor by which to extend the extent. Defaults to 5.\n\n    Returns:\n        List[float]: The extended extent.\n    \"\"\"\n    extension_size = self.resolution * extension_factor\n    return [\n        raster_bounds[0] + extension_size,  # x topleft\n        raster_bounds[1] - extension_size,  # y topleft\n        raster_bounds[2] - extension_size,  # x bottomright\n        raster_bounds[3] + extension_size,  # y bottomright\n    ]\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator._format_distance","title":"<code>_format_distance(value)</code>  <code>staticmethod</code>","text":"<p>Formats the given distance value to string.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>int</code> <p>The distance value to format.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The formatted distance value.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>@staticmethod\ndef _format_distance(value: int) -&gt; str:\n    \"\"\"Formats the given distance value to string.\n\n    Args:\n        value (int): The distance value to format.\n\n    Returns:\n        str: The formatted distance value.\n    \"\"\"\n    if value &lt; 1000:\n        return f\"{value}m\"\n    else:\n        if value % 1000 != 0:\n            raise ValueError(f\"Distance to be formatted not multiple of 1000: {value}\")\n        return f\"{value // 1000}km\"\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator._get_grid_blueprint","title":"<code>_get_grid_blueprint(extent)</code>","text":"<p>Generates a blueprint for the grid for the given extent as a raster of grid resolution. Splits initial raster into smaller rasters of size grid_partition_size x grid_partition_size.</p> <p>Parameters:</p> Name Type Description Default <code>extent</code> <code>List[float]</code> <p>The extent for which to generate the grid blueprint.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The grid blueprint.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def _get_grid_blueprint(self, extent: List[float]) -&gt; DataFrame:\n    \"\"\"Generates a blueprint for the grid for the given extent as a raster of grid resolution.\n    Splits initial raster into smaller rasters of size grid_partition_size x grid_partition_size.\n\n    Args:\n        extent (List[float]): The extent for which to generate the grid blueprint.\n\n    Returns:\n        DataFrame: The grid blueprint.\n    \"\"\"\n    extent, auxiliar_coords = self._project_latlon_extent(extent)\n    extent, raster_bounds = self._project_bounding_box(extent, auxiliar_coords)\n\n    extent = self._snap_extent_to_grid(extent)\n    raster_bounds = self._snap_extent_to_grid(raster_bounds)\n\n    extent = self._extend_grid_extent(extent)\n    raster_bounds = self._extend_grid_raster_bounds(raster_bounds)\n\n    grid_height = self._get_grid_height(raster_bounds)\n    grid_width = self._get_grid_width(raster_bounds)\n\n    # ONLY FOR EPSG:3035!!!! which has (northing, easting) order, BUT (Y, X) axis names in its EPSG (not in code)\n    # raster_bounds[1]: easting of the top left corner. \"X\" axis\n    # raster_bounds[0]: northing of the top left corner. \"Y\" axis\n\n    sdf = self.spark.sql(\n        f\"\"\"SELECT RS_MakeEmptyRaster(1, \"B\", {grid_width}, \n                            {grid_height}, \n                            {raster_bounds[1]},\n                            {raster_bounds[0]}, \n                            {self.resolution}, \n                           -{self.resolution}, 0.0, 0.0, {self.GRID_CRS_EPSG_CODE}) as raster\"\"\"\n    )\n\n    sdf = sdf.selectExpr(f\"RS_TileExplode(raster,{self.grid_partition_size}, {self.grid_partition_size})\")\n    return sdf.repartition(sdf.count())\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator._get_grid_height","title":"<code>_get_grid_height(raster_bounds)</code>","text":"<p>Calculates the height of the grid for the given extent.</p> <p>Parameters:</p> Name Type Description Default <code>raster_bounds</code> <code>List[float]</code> <p>The raster_bounds for which to calculate the grid height.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The grid height.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def _get_grid_height(self, raster_bounds: List[float]) -&gt; int:\n    \"\"\"Calculates the height of the grid for the given extent.\n\n    Args:\n        raster_bounds (List[float]): The raster_bounds for which to calculate the grid height.\n\n    Returns:\n        int: The grid height.\n    \"\"\"\n    return int((raster_bounds[0] - raster_bounds[2]) / self.resolution)\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator._get_grid_intersection_with_mask","title":"<code>_get_grid_intersection_with_mask(sdf, polygon_sdf)</code>","text":"<p>Covers the given extent with grid centroids.</p> <p>Parameters:</p> Name Type Description Default <code>extent</code> <code>List[float]</code> <p>The extent to cover.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The DataFrame representing the grid centroids covering the extent.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def _get_grid_intersection_with_mask(self, sdf: DataFrame, polygon_sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"Covers the given extent with grid centroids.\n\n    Args:\n        extent (List[float]): The extent to cover.\n\n    Returns:\n        DataFrame: The DataFrame representing the grid centroids covering the extent.\n    \"\"\"\n    polygon_sdf = polygon_sdf.withColumn(\n        \"geometry\",\n        STF.ST_Transform(polygon_sdf[\"geometry\"], F.lit(\"EPSG:4326\"), F.lit(f\"EPSG:{self.GRID_CRS_EPSG_CODE}\")),\n    )\n\n    sdf = sdf.join(\n        polygon_sdf, STP.ST_Intersects(sdf[self.geometry_col_name], polygon_sdf[\"geometry\"]), \"inner\"\n    ).drop(polygon_sdf[\"geometry\"])\n\n    return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator._get_grid_width","title":"<code>_get_grid_width(raster_bounds)</code>","text":"<p>Calculates the width of the grid for the given extent.</p> <p>Parameters:</p> Name Type Description Default <code>raster_bounds</code> <code>List[float]</code> <p>The raster_bounds for which to calculate the grid width.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The grid width.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def _get_grid_width(self, raster_bounds: List[float]) -&gt; int:\n    \"\"\"Calculates the width of the grid for the given extent.\n\n    Args:\n        raster_bounds (List[float]): The raster_bounds for which to calculate the grid width.\n\n    Returns:\n        int: The grid width.\n    \"\"\"\n    return int((raster_bounds[3] - raster_bounds[1]) / self.resolution)\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator._get_polygon_sdf_extent","title":"<code>_get_polygon_sdf_extent(polygon_sdf)</code>  <code>staticmethod</code>","text":"<p>Gets the extent of the given polygon DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>polygon_sdf</code> <code>DataFrame</code> <p>The polygon DataFrame.</p> required <p>Returns:</p> Type Description <code>List[float]</code> <p>List[float]: The extent of the polygon DataFrame.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>@staticmethod\ndef _get_polygon_sdf_extent(polygon_sdf: DataFrame) -&gt; List[float]:\n    \"\"\"Gets the extent of the given polygon DataFrame.\n\n    Args:\n        polygon_sdf (DataFrame): The polygon DataFrame.\n\n    Returns:\n        List[float]: The extent of the polygon DataFrame.\n    \"\"\"\n    polygon_sdf = polygon_sdf.withColumn(\"bbox\", STF.ST_Envelope(polygon_sdf[\"geometry\"]))\n    polygon_sdf = (\n        polygon_sdf.withColumn(\"x_min\", STF.ST_XMin(polygon_sdf[\"bbox\"]))\n        .withColumn(\"y_min\", STF.ST_YMin(polygon_sdf[\"bbox\"]))\n        .withColumn(\"x_max\", STF.ST_XMax(polygon_sdf[\"bbox\"]))\n        .withColumn(\"y_max\", STF.ST_YMax(polygon_sdf[\"bbox\"]))\n    )\n\n    return polygon_sdf.select(\"x_min\", \"y_min\", \"x_max\", \"y_max\").collect()[0][0:]\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator._project_bounding_box","title":"<code>_project_bounding_box(extent, auxiliar_coords)</code>  <code>staticmethod</code>","text":"<p>Returns the bottom-left and top-right coordinates of the rectangular bounding box in the projected CRS that covers the bounding box defined from the bottom-left and top-right corners in lat/lon.</p> <p>Parameters:</p> Name Type Description Default <code>extent</code> <code>List[float]</code> <p>Coordinates in the projected CRS that are the transformation of the minimum and maximum latitude and longitude, in [x_bottomleft, y_bottomleft, x_topright, y_topright] order.</p> required <code>auxiliar_coords</code> <code>List[float]</code> <p>Auxiliar coordinates in the prohected CRS that are the transformation of the other two cornes of the lat/lon rectangular bounding box, in [x_bottomright, y_bottomright, x_topleft, y_topleft] order</p> required <p>Returns:</p> Type Description <code>List[float]</code> <p>List[float]: The projected extent, in [x_bottomleft, y_bottomleft, x_topright, y_topright] order.</p> <code>List[float]</code> <p>List[float]: Raster cover bounds, in [x_topleft, y_topleft, x_bottomright, y_bottomright] order.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>@staticmethod\ndef _project_bounding_box(extent: List[float], auxiliar_coords: List[float]) -&gt; Tuple[List[float], List[float]]:\n    \"\"\"Returns the bottom-left and top-right coordinates of the rectangular bounding box in the projected CRS\n    that covers the bounding box defined from the bottom-left and top-right corners in lat/lon.\n\n    Args:\n        extent (List[float]): Coordinates in the projected CRS that are the transformation of the minimum and\n            maximum latitude and longitude, in [x_bottomleft, y_bottomleft, x_topright, y_topright] order.\n        auxiliar_coords (List[float]): Auxiliar coordinates in the prohected CRS that are the transformation\n            of the other two cornes of the lat/lon rectangular bounding box, in\n            [x_bottomright, y_bottomright, x_topleft, y_topleft] order\n\n    Returns:\n        List[float]: The projected extent, in [x_bottomleft, y_bottomleft, x_topright, y_topright] order.\n        List[float]: Raster cover bounds, in [x_topleft, y_topleft, x_bottomright, y_bottomright] order.\n    \"\"\"\n    cover_x_bottomleft = min(extent[0], auxiliar_coords[0])  # min lat\n    cover_y_bottomleft = min(extent[1], auxiliar_coords[3])  # min lon\n\n    cover_x_topright = max(extent[2], auxiliar_coords[2])  # max lat\n    cover_y_topright = max(extent[3], auxiliar_coords[1])  # max lon\n\n    cover_x_topleft = max(extent[2], auxiliar_coords[2])  # max lat\n    cover_y_topleft = min(extent[1], auxiliar_coords[3])  # min lon\n\n    cover_x_bottomright = min(extent[0], auxiliar_coords[0])  # min lat\n    cover_y_bottomright = max(extent[3], auxiliar_coords[1])  # max lon\n\n    return (\n        [cover_y_bottomleft, cover_x_bottomleft, cover_y_topright, cover_x_topright],\n        [cover_x_topleft, cover_y_topleft, cover_x_bottomright, cover_y_bottomright],\n    )\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator._project_latlon_extent","title":"<code>_project_latlon_extent(extent)</code>","text":"<p>Projects the given extent from lat/lon to the grid's CRS.</p> <p>Parameters:</p> Name Type Description Default <code>extent</code> <code>List[float]</code> <p>The extent to project. Order: [lon_min, lat_min, lon_max, lat_max]</p> required <p>Returns:</p> Type Description <code>Union[List[float], List[float]]</code> <p>List[float]: The projected extent.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def _project_latlon_extent(self, extent: List[float]) -&gt; Union[List[float], List[float]]:\n    \"\"\"Projects the given extent from lat/lon to the grid's CRS.\n\n    Args:\n        extent (List[float]): The extent to project. Order: [lon_min, lat_min, lon_max, lat_max]\n\n    Returns:\n        List[float]: The projected extent.\n    \"\"\"\n    transformer = Transformer.from_crs(\"EPSG:4326\", f\"EPSG:{self.GRID_CRS_EPSG_CODE}\")\n    # EPSG4326: xx -&gt; lat, yy -&gt; lon\n    # EPSG3035: xx -&gt; northing, yy -&gt; easting\n    xx_bottomleft, yy_bottomleft = transformer.transform(extent[1], extent[0])  # bottom-left corner\n    xx_topright, yy_topright = transformer.transform(extent[3], extent[2])  # top-right corner\n    xx_bottomright, yy_bottomright = transformer.transform(extent[1], extent[2])  # bottom-right corner\n    xx_topleft, yy_topleft = transformer.transform(extent[3], extent[0])\n\n    return (\n        [xx_bottomleft, yy_bottomleft, xx_topright, yy_topright],\n        [xx_bottomright, yy_bottomright, xx_topleft, yy_topleft],\n    )\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator._snap_extent_to_grid","title":"<code>_snap_extent_to_grid(extent)</code>","text":"<p>Snaps the given extent to the grid.</p> <p>Parameters:</p> Name Type Description Default <code>extent</code> <code>List[float]</code> <p>The extent to snap.</p> required <p>Returns:</p> Type Description <code>List[float]</code> <p>List[float]: The snapped extent.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def _snap_extent_to_grid(self, extent: List[float]) -&gt; List[float]:\n    \"\"\"Snaps the given extent to the grid.\n\n    Args:\n        extent (List[float]): The extent to snap.\n\n    Returns:\n        List[float]: The snapped extent.\n    \"\"\"\n    return [round(coord / self.resolution) * self.resolution for coord in extent]\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.convert_internal_id_to_inspire_specs","title":"<code>convert_internal_id_to_inspire_specs(sdf, resolution=None, grid_id_col=None)</code>","text":"<p>Converts the integer grid id column in a DataFrame to the INSPIRE grid string format.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input DataFrame with a column grid_id_col in integer format.</p> required <code>resolution</code> <code>int</code> <p>The resolution of the grid. If None, it is equal to self.resolution. Defaults to None</p> <code>None</code> <code>grid_id_col</code> <code>str</code> <p>name of the column containing the integer grid id column. If None, it is equal to self.grid_id_col_name. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame with the grid_id converted to string format.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def convert_internal_id_to_inspire_specs(\n    self, sdf: DataFrame, resolution: int = None, grid_id_col: str = None\n) -&gt; DataFrame:\n    \"\"\"\n    Converts the integer grid id column in a DataFrame to the INSPIRE grid string format.\n\n    Args:\n        sdf (DataFrame): Input DataFrame with a column grid_id_col in integer format.\n        resolution (int, optional): The resolution of the grid. If None, it is equal to self.resolution. Defaults\n            to None\n        grid_id_col (str, optional): name of the column containing the integer grid id column. If None, it is\n            equal to self.grid_id_col_name. Defaults to None.\n\n    Returns:\n        DataFrame: A new DataFrame with the grid_id converted to string format.\n    \"\"\"\n    if resolution is None:\n        resolution = self.resolution\n        resolution_str = self.resolution_str\n    else:\n        if resolution % 100 != 0:\n            raise ValueError(f\"Invalid resolution value not divisible by 100: {resolution}\")\n        resolution_str = self._format_distance(resolution)\n\n    if grid_id_col is None:\n        grid_id_col = self.grid_id_col_name\n\n    trailing_zeros = int(log10(resolution))\n\n    sdf = sdf.withColumn(\n        grid_id_col,\n        F.concat(\n            F.lit(resolution_str),\n            F.lit(\"N\"),\n            (F.expr(f\"grid_id DIV {10**(self.PROJ_COORD_INT_SIZE + trailing_zeros)}\")),\n            F.lit(\"E\"),\n            (F.expr(f\"(grid_id % {10**self.PROJ_COORD_INT_SIZE}) DIV {10 ** trailing_zeros}\")),\n        ),\n    )\n\n    return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.cover_extent_with_grid_centroids","title":"<code>cover_extent_with_grid_centroids(extent)</code>","text":"<p>Covers the given polygon with grid centroids.</p> <p>Parameters:</p> Name Type Description Default <code>extent</code> <code>List[float]</code> <p>The extent to cover.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The DataFrame representing the grid centroids covering the polygon.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def cover_extent_with_grid_centroids(self, extent: List[float]) -&gt; DataFrame:\n    \"\"\"Covers the given polygon with grid centroids.\n\n    Args:\n        extent (List[float]): The extent to cover.\n\n    Returns:\n        DataFrame: The DataFrame representing the grid centroids covering the polygon.\n    \"\"\"\n    sdf = self._get_grid_blueprint(extent)\n\n    sdf = sdf.selectExpr(\"explode(RS_PixelAsCentroids(tile, 1)) as exploded\").selectExpr(\n        f\"exploded.geom as {self.geometry_col_name}\"\n    )\n\n    sdf = sdf.withColumn(\n        self.grid_id_col_name,\n        (\n            (STF.ST_Y(sdf[\"geometry\"]) - (self.resolution / 2)).cast(LongType()) * 10**self.PROJ_COORD_INT_SIZE\n            + (STF.ST_X(sdf[\"geometry\"]) - (self.resolution / 2)).cast(LongType())\n        ).cast(LongType()),\n    )\n\n    return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.cover_extent_with_grid_ids","title":"<code>cover_extent_with_grid_ids(extent)</code>","text":"<p>Covers the given polygon with grid IDs.</p> <p>Parameters:</p> Name Type Description Default <code>extent</code> <code>List[float]</code> <p>The extent to cover.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The DataFrame representing the grid IDs covering the polygon.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def cover_extent_with_grid_ids(self, extent: List[float]) -&gt; DataFrame:\n    \"\"\"Covers the given polygon with grid IDs.\n\n    Args:\n        extent (List[float]): The extent to cover.\n\n    Returns:\n        DataFrame: The DataFrame representing the grid IDs covering the polygon.\n    \"\"\"\n    sdf = self.cover_extent_with_grid_centroids(extent)\n\n    sdf = sdf.drop(self.geometry_col_name)\n\n    return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.cover_extent_with_grid_tiles","title":"<code>cover_extent_with_grid_tiles(extent)</code>","text":"<p>Covers the given extent with grid tiles.</p> <p>Parameters:</p> Name Type Description Default <code>extent</code> <code>List[float]</code> <p>The extent to cover.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The DataFrame representing the grid tiles covering the extent.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def cover_extent_with_grid_tiles(self, extent: List[float]) -&gt; DataFrame:\n    \"\"\"Covers the given extent with grid tiles.\n\n    Args:\n        extent (List[float]): The extent to cover.\n\n    Returns:\n        DataFrame: The DataFrame representing the grid tiles covering the extent.\n    \"\"\"\n    sdf = self._get_grid_blueprint(extent)\n\n    sdf = sdf.selectExpr(\"explode(RS_PixelAsPolygons(tile, 1)) as exploded\").selectExpr(\n        f\"exploded.geom as {self.geometry_col_name}\"\n    )\n\n    sdf = sdf.withColumn(\n        self.grid_id_col_name,\n        (\n            STF.ST_XMin(sdf[\"geometry\"]).cast(LongType()) * 10**self.PROJ_COORD_INT_SIZE\n            + STF.ST_YMin(sdf[\"geometry\"])\n        ).cast(LongType()),\n    )\n\n    return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.cover_polygon_with_grid_centroids","title":"<code>cover_polygon_with_grid_centroids(polygon_sdf)</code>","text":"<p>Covers the given extent with grid IDs.</p> <p>Parameters:</p> Name Type Description Default <code>polygon_sdf</code> <code>DataFrame</code> <p>The DataFrame representing the polygon.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The DataFrame representing the grid IDs covering the extent.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def cover_polygon_with_grid_centroids(self, polygon_sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"Covers the given extent with grid IDs.\n\n    Args:\n        polygon_sdf (DataFrame): The DataFrame representing the polygon.\n\n    Returns:\n        DataFrame: The DataFrame representing the grid IDs covering the extent.\n    \"\"\"\n    extent = self._get_polygon_sdf_extent(polygon_sdf)\n\n    sdf = self.cover_extent_with_grid_centroids(extent)\n\n    sdf = self._get_grid_intersection_with_mask(sdf, polygon_sdf)\n\n    return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.cover_polygon_with_grid_ids","title":"<code>cover_polygon_with_grid_ids(polygon_sdf)</code>","text":"<p>Covers the given polygon with grid IDs.</p> <p>Parameters:</p> Name Type Description Default <code>polygon_sdf</code> <code>DataFrame</code> <p>The DataFrame representing the polygon.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The DataFrame representing the grid IDs covering the polygon.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def cover_polygon_with_grid_ids(self, polygon_sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"Covers the given polygon with grid IDs.\n\n    Args:\n        polygon_sdf (DataFrame): The DataFrame representing the polygon.\n\n    Returns:\n        DataFrame: The DataFrame representing the grid IDs covering the polygon.\n    \"\"\"\n    extent = self._get_polygon_sdf_extent(polygon_sdf)\n\n    sdf = self.cover_extent_with_grid_centroids(extent)\n\n    sdf = self._get_grid_intersection_with_mask(sdf, polygon_sdf)\n\n    sdf = sdf.drop(\"geometry\")\n\n    return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.cover_polygon_with_grid_tiles","title":"<code>cover_polygon_with_grid_tiles(polygon_sdf)</code>","text":"<p>Covers the given polygon with grid tiles.</p> <p>Parameters:</p> Name Type Description Default <code>polygon_sdf</code> <code>DataFrame</code> <p>The DataFrame representing the polygon.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The DataFrame representing the grid tiles covering the polygon.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def cover_polygon_with_grid_tiles(self, polygon_sdf: DataFrame) -&gt; DataFrame:\n    \"\"\"Covers the given polygon with grid tiles.\n\n    Args:\n        polygon_sdf (DataFrame): The DataFrame representing the polygon.\n\n    Returns:\n        DataFrame: The DataFrame representing the grid tiles covering the polygon.\n    \"\"\"\n    extent = self._get_polygon_sdf_extent(polygon_sdf)\n\n    sdf = self.cover_extent_with_grid_tiles(extent)\n\n    sdf = self._get_grid_intersection_with_mask(sdf, polygon_sdf)\n\n    return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.get_children_grid_ids","title":"<code>get_children_grid_ids(sdf, resolution, child_resolution, child_col_name=None)</code>","text":"<p>Expands coarsened grid IDs to generate all possible child grid IDs within the specified finer resolution.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input Spark DataFrame with a 14-digit grid ID column named 'grid_id'.</p> required <code>resolution</code> <code>int</code> <p>The coarser resolution (e.g., 200).</p> required <code>child_resolution</code> <code>int</code> <p>The finer resolution for child grids (e.g., 100).</p> required <code>child_col_name</code> <code>str</code> <p>name of the column that will hold the children grid IDs. If None, it will be named f\"child_{ColNames.grid_id}\". Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with generated child grid IDs in new column.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def get_children_grid_ids(\n    self, sdf: DataFrame, resolution: int, child_resolution: int, child_col_name: str = None\n) -&gt; DataFrame:\n    \"\"\"\n    Expands coarsened grid IDs to generate all possible child grid IDs within the specified finer resolution.\n\n    Args:\n        sdf (DataFrame): Input Spark DataFrame with a 14-digit grid ID column named 'grid_id'.\n        resolution (int): The coarser resolution (e.g., 200).\n        child_resolution (int): The finer resolution for child grids (e.g., 100).\n        child_col_name (str, optional): name of the column that will hold the children grid IDs. If None, it will be\n            named f\"child_{ColNames.grid_id}\". Defaults to None.\n\n    Returns:\n        DataFrame: DataFrame with generated child grid IDs in new column.\n    \"\"\"\n    if resolution % 100 != 0 or child_resolution % 100 != 0:\n        raise ValueError(\"Both resolutions must be multiples of 100.\")\n    if child_resolution &gt;= resolution:\n        raise ValueError(\"Child resolution must be finer than the parent resolution.\")\n    if resolution % child_resolution != 0:\n        raise ValueError(\"Parent resolution must be a multiple of the child resolution.\")\n    if child_col_name is None:\n        child_col_name = f\"child_{ColNames.grid_id}\"\n\n    factor = resolution // child_resolution\n\n    # Extract `northing` and `easting` from the parent grid ID\n    sdf = sdf.withColumn(\"northing\", F.expr(f\"{ColNames.grid_id} DIV {10**self.PROJ_COORD_INT_SIZE}\")).withColumn(\n        \"easting\", F.col(ColNames.grid_id) % 10**self.PROJ_COORD_INT_SIZE\n    )\n\n    # Generate all possible offsets for child grids\n    offsets = [(i, j) for i in range(factor) for j in range(factor)]\n    spark = sdf.sparkSession  # Get the current Spark session\n    offsets_df = spark.createDataFrame(offsets, [\"northing_offset\", \"easting_offset\"])\n\n    # Cross join with the offsets to generate all child grid IDs\n    result = (\n        sdf.crossJoin(offsets_df)\n        .withColumn(\"child_northing\", F.expr(f\"northing + northing_offset * {child_resolution}\"))\n        .withColumn(\"child_easting\", F.expr(f\"easting + easting_offset * {child_resolution}\"))\n        .withColumn(\n            child_col_name,\n            (F.col(\"child_northing\").cast(LongType()) * 10**self.PROJ_COORD_INT_SIZE + F.col(\"child_easting\")),\n        )\n        .drop(\"child_northing\", \"child_easting\", \"northing\", \"easting\")\n    )\n\n    return result\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.get_parent_grid_ids","title":"<code>get_parent_grid_ids(sdf, resolution, parent_col_name=None)</code>","text":"<p>Coarsens grid IDs to the specified resolution, snapping down to the nearest coarser grid cell.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input Spark DataFrame with a 14-digit grid ID column named 'grid_id'.</p> required <code>resolution</code> <code>int</code> <p>The desired coarser resolution (e.g., 200).</p> required <code>parent_col_name</code> <code>str</code> <p>name of the column that will hold the parent grid IDs. If None, it will be named f\"parent_{ColNames.grid_id}\". Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with coarsened grid IDs in new column.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def get_parent_grid_ids(self, sdf: DataFrame, resolution: int, parent_col_name: str = None) -&gt; DataFrame:\n    \"\"\"\n    Coarsens grid IDs to the specified resolution, snapping down to the nearest coarser grid cell.\n\n    Args:\n        sdf (DataFrame): Input Spark DataFrame with a 14-digit grid ID column named 'grid_id'.\n        resolution (int): The desired coarser resolution (e.g., 200).\n        parent_col_name (str, optional): name of the column that will hold the parent grid IDs. If None, it will be\n            named f\"parent_{ColNames.grid_id}\". Defaults to None.\n\n    Returns:\n        DataFrame: DataFrame with coarsened grid IDs in new column.\n    \"\"\"\n    if resolution % 100 != 0:\n        raise ValueError(\"Resolution must be a multiple of 100.\")\n    if parent_col_name is None:\n        parent_col_name = f\"parent_{ColNames.grid_id}\"\n\n    # Extract `northing` and `easting` correctly\n    sdf = sdf.withColumn(\"northing\", F.expr(f\"{ColNames.grid_id} DIV {10**self.PROJ_COORD_INT_SIZE}\")).withColumn(\n        \"easting\", F.col(ColNames.grid_id) % 10**self.PROJ_COORD_INT_SIZE\n    )\n\n    # Snap `northing` and `easting` down to the nearest coarser grid boundary\n    sdf = sdf.withColumn(\"northing_parent\", F.col(\"northing\") - (F.col(\"northing\") % resolution)).withColumn(\n        \"easting_parent\", F.col(\"easting\") - (F.col(\"easting\") % resolution)\n    )\n\n    # Combine the coarsened `northing` and `easting` into `parent_grid_id`\n    sdf = sdf.withColumn(\n        parent_col_name,\n        (F.col(\"northing_parent\").cast(LongType()) * 10**self.PROJ_COORD_INT_SIZE + F.col(\"easting_parent\")),\n    ).drop(\"northing\", \"easting\", \"northing_parent\", \"easting_parent\")\n\n    return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.grid_ids_to_centroids","title":"<code>grid_ids_to_centroids(sdf, resolution=None, to_crs=None)</code>","text":"<p>Converts grid IDs to centroids.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The DataFrame containing the grid IDs.</p> required <code>to_crs</code> <code>int</code> <p>The CRS to which to convert the centroids. If not provided, the centroids will be in default grid crs.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The DataFrame containing the centroids.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def grid_ids_to_centroids(self, sdf: DataFrame, resolution: int = None, to_crs: int = None) -&gt; DataFrame:\n    \"\"\"Converts grid IDs to centroids.\n\n    Args:\n        sdf (DataFrame): The DataFrame containing the grid IDs.\n        to_crs (int, optional): The CRS to which to convert the centroids. If not provided,\n            the centroids will be in default grid crs.\n\n    Returns:\n        DataFrame: The DataFrame containing the centroids.\n    \"\"\"\n    if resolution is None:\n        resolution = self.resolution\n\n    sdf = sdf.withColumn(\n        self.geometry_col_name,\n        STC.ST_Point(\n            F.expr(f\"INT({self.grid_id_col_name} % {10**self.PROJ_COORD_INT_SIZE})\") + resolution / 2,\n            F.expr(f\"INT({self.grid_id_col_name} DIV {10**self.PROJ_COORD_INT_SIZE})\") + resolution / 2,\n        ),\n    )\n\n    sdf = sdf.withColumn(\n        self.geometry_col_name, STF.ST_SetSRID(sdf[self.geometry_col_name], self.GRID_CRS_EPSG_CODE)\n    )\n\n    if to_crs:\n        sdf = sdf.withColumn(\n            self.geometry_col_name, STF.ST_Transform(sdf[self.geometry_col_name], F.lit(f\"EPSG:{to_crs}\"))\n        )\n\n    return sdf\n</code></pre>"},{"location":"reference/core/grid/#core.grid.InspireGridGenerator.grid_ids_to_tiles","title":"<code>grid_ids_to_tiles(sdf, resolution=None, to_crs=None)</code>","text":"<p>Converts grid IDs in INSPIRE format to tiles.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The DataFrame containing the grid IDs.</p> required <code>to_crs</code> <code>int</code> <p>The CRS to which to convert the tiles. If not provided, the centroids will be in default grid crs.</p> <code>None</code> <p>Returns:     DataFrame: The DataFrame containing the tiles.</p> Source code in <code>multimno/core/grid.py</code> <pre><code>def grid_ids_to_tiles(self, sdf: DataFrame, resolution: int = None, to_crs: int = None) -&gt; DataFrame:\n    \"\"\"Converts grid IDs in INSPIRE format to tiles.\n\n    Args:\n        sdf (DataFrame): The DataFrame containing the grid IDs.\n        to_crs (int, optional): The CRS to which to convert the tiles. If not provided, the centroids\n            will be in default grid crs.\n    Returns:\n        DataFrame: The DataFrame containing the tiles.\n    \"\"\"\n\n    if resolution is None:\n        resolution = self.resolution\n\n    sdf = sdf.withColumn(\n        self.geometry_col_name,\n        STC.ST_PolygonFromEnvelope(\n            F.expr(f\"{self.grid_id_col_name} % {10**self.PROJ_COORD_INT_SIZE}\"),  # x/easting min\n            F.expr(f\"{self.grid_id_col_name} DIV {10**self.PROJ_COORD_INT_SIZE}\"),  # y/northing min\n            F.expr(f\"{self.grid_id_col_name} % {10**self.PROJ_COORD_INT_SIZE}\") + resolution,  # x/easting max\n            F.expr(f\"{self.grid_id_col_name} DIV {10**self.PROJ_COORD_INT_SIZE}\") + resolution,  # y/northing max\n        ),\n    )\n\n    sdf = sdf.withColumn(\n        self.geometry_col_name, STF.ST_SetSRID(sdf[self.geometry_col_name], self.GRID_CRS_EPSG_CODE)\n    )\n\n    if to_crs:\n        sdf = sdf.withColumn(\n            self.geometry_col_name, STF.ST_Transform(sdf[self.geometry_col_name], F.lit(f\"EPSG:{to_crs}\"))\n        )\n\n    return sdf\n</code></pre>"},{"location":"reference/core/io_interface/","title":"io_interface","text":"<p>Module that implements classes for reading data from different data sources into a Spark DataFrames.</p>"},{"location":"reference/core/io_interface/#core.io_interface.CsvInterface","title":"<code>CsvInterface</code>","text":"<p>               Bases: <code>PathInterface</code></p> <p>Class that implements the PathInterface abstract class for reading/writing data from a csv data source.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class CsvInterface(PathInterface):\n    \"\"\"Class that implements the PathInterface abstract class for reading/writing data from a csv data source.\"\"\"\n\n    FILE_FORMAT = \"csv\"\n\n    def read_from_interface(\n        self,\n        spark: SparkSession,\n        path: str,\n        schema: StructType,\n        header: bool = True,\n        sep: str = \",\",\n    ) -&gt; DataFrame:\n        \"\"\"Method that reads data from a csv type data source as a Spark DataFrame.\n\n        Args:\n            spark (SparkSession): Spark session.\n            path (str): Path to the data.\n            schema (StructType, optional): Schema of the data. Defaults to None.\n\n        Returns:\n            df: Spark dataframe.\n        \"\"\"\n        return spark.read.csv(path, schema=schema, header=header, sep=sep)\n\n    def write_from_interface(\n        self,\n        df: DataFrame,\n        path: str,\n        partition_columns: List[str] = None,\n        header: bool = True,\n        sep: str = \",\",\n    ):\n        \"\"\"Method that writes data from a Spark DataFrame to a csv data source.\n\n        Args:\n            df (DataFrame): Spark DataFrame.\n            path (str): Path to the data.\n            partition_columns (List[str], optional): columns used for a partition write.\n        Raises:\n            NotImplementedError: csv files should not be written in this architecture.\n        \"\"\"\n        if partition_columns is None:\n            partition_columns = []\n        df.write.option(\"header\", header).option(\"sep\", sep).mode(\"overwrite\").format(\"csv\").save(path)\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.CsvInterface.read_from_interface","title":"<code>read_from_interface(spark, path, schema, header=True, sep=',')</code>","text":"<p>Method that reads data from a csv type data source as a Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session.</p> required <code>path</code> <code>str</code> <p>Path to the data.</p> required <code>schema</code> <code>StructType</code> <p>Schema of the data. Defaults to None.</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>Spark dataframe.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def read_from_interface(\n    self,\n    spark: SparkSession,\n    path: str,\n    schema: StructType,\n    header: bool = True,\n    sep: str = \",\",\n) -&gt; DataFrame:\n    \"\"\"Method that reads data from a csv type data source as a Spark DataFrame.\n\n    Args:\n        spark (SparkSession): Spark session.\n        path (str): Path to the data.\n        schema (StructType, optional): Schema of the data. Defaults to None.\n\n    Returns:\n        df: Spark dataframe.\n    \"\"\"\n    return spark.read.csv(path, schema=schema, header=header, sep=sep)\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.CsvInterface.write_from_interface","title":"<code>write_from_interface(df, path, partition_columns=None, header=True, sep=',')</code>","text":"<p>Method that writes data from a Spark DataFrame to a csv data source.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Spark DataFrame.</p> required <code>path</code> <code>str</code> <p>Path to the data.</p> required <code>partition_columns</code> <code>List[str]</code> <p>columns used for a partition write.</p> <code>None</code> <p>Raises:     NotImplementedError: csv files should not be written in this architecture.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def write_from_interface(\n    self,\n    df: DataFrame,\n    path: str,\n    partition_columns: List[str] = None,\n    header: bool = True,\n    sep: str = \",\",\n):\n    \"\"\"Method that writes data from a Spark DataFrame to a csv data source.\n\n    Args:\n        df (DataFrame): Spark DataFrame.\n        path (str): Path to the data.\n        partition_columns (List[str], optional): columns used for a partition write.\n    Raises:\n        NotImplementedError: csv files should not be written in this architecture.\n    \"\"\"\n    if partition_columns is None:\n        partition_columns = []\n    df.write.option(\"header\", header).option(\"sep\", sep).mode(\"overwrite\").format(\"csv\").save(path)\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.GeoParquetInterface","title":"<code>GeoParquetInterface</code>","text":"<p>               Bases: <code>PathInterface</code></p> <p>Class that implements the PathInterface abstract class for reading/writing data from a geoparquet data source.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class GeoParquetInterface(PathInterface):\n    \"\"\"Class that implements the PathInterface abstract class for reading/writing data from a geoparquet data source.\"\"\"\n\n    FILE_FORMAT = \"geoparquet\"\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.HttpGeoJsonInterface","title":"<code>HttpGeoJsonInterface</code>","text":"<p>               Bases: <code>IOInterface</code></p> <p>Class that implements the IO interface abstract class for reading GeoJSON data from an HTTP source.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class HttpGeoJsonInterface(IOInterface):\n    \"\"\"Class that implements the IO interface abstract class for reading GeoJSON data from an HTTP source.\"\"\"\n\n    def read_from_interface(self, spark: SparkSession, url: str, timeout: int = 60, max_retries: int = 5) -&gt; DataFrame:\n        \"\"\"Method that reads GeoJSON data from an HTTP source and converts it to a Spark DataFrame.\n\n        Args:\n            url (str): URL of the GeoJSON data.\n            timeout (int): Timeout for the GET request in seconds. Default is 60.\n            max_retries (int): Maximum number of retries for the GET request. Default is 5.\n\n        Returns:\n            df: Spark DataFrame.\n        \"\"\"\n        session = requests.Session()\n        retry = Retry(total=max_retries, backoff_factor=0.1, status_forcelist=[500, 502, 503, 504])\n        adapter = HTTPAdapter(max_retries=retry)\n        session.mount(\"http://\", adapter)\n        session.mount(\"https://\", adapter)\n\n        try:\n            response = session.get(url, timeout=timeout)\n        except requests.exceptions.RequestException as e:\n            print(e)\n            raise Exception(\"Maximum number of retries exceeded.\")\n\n        if response.status_code != 200:\n            raise Exception(\"GET request not successful.\")\n\n        # Read the GeoJSON data into a GeoDataFrame\n        gdf = gpd.read_file(StringIO(response.text))\n\n        # Convert the GeoDataFrame to a Spark DataFrame\n        df = spark.createDataFrame(gdf)\n\n        return df\n\n    def write_from_interface(self, df: DataFrame, url: str, timeout: int = 60, max_retries: int = 5):\n        \"\"\"Method that writes a DataFrame to an HTTP source as GeoJSON data.\n\n        Args:\n            df (DataFrame): DataFrame to write.\n            url (str): URL of the HTTP source.\n            timeout (int): Timeout for the POST request in seconds. Default is 60.\n            max_retries (int): Maximum number of retries for the POST request. Default is 5.\n        \"\"\"\n        raise NotImplementedError(\"This method is not implemented.\")\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.HttpGeoJsonInterface.read_from_interface","title":"<code>read_from_interface(spark, url, timeout=60, max_retries=5)</code>","text":"<p>Method that reads GeoJSON data from an HTTP source and converts it to a Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL of the GeoJSON data.</p> required <code>timeout</code> <code>int</code> <p>Timeout for the GET request in seconds. Default is 60.</p> <code>60</code> <code>max_retries</code> <code>int</code> <p>Maximum number of retries for the GET request. Default is 5.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>Spark DataFrame.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def read_from_interface(self, spark: SparkSession, url: str, timeout: int = 60, max_retries: int = 5) -&gt; DataFrame:\n    \"\"\"Method that reads GeoJSON data from an HTTP source and converts it to a Spark DataFrame.\n\n    Args:\n        url (str): URL of the GeoJSON data.\n        timeout (int): Timeout for the GET request in seconds. Default is 60.\n        max_retries (int): Maximum number of retries for the GET request. Default is 5.\n\n    Returns:\n        df: Spark DataFrame.\n    \"\"\"\n    session = requests.Session()\n    retry = Retry(total=max_retries, backoff_factor=0.1, status_forcelist=[500, 502, 503, 504])\n    adapter = HTTPAdapter(max_retries=retry)\n    session.mount(\"http://\", adapter)\n    session.mount(\"https://\", adapter)\n\n    try:\n        response = session.get(url, timeout=timeout)\n    except requests.exceptions.RequestException as e:\n        print(e)\n        raise Exception(\"Maximum number of retries exceeded.\")\n\n    if response.status_code != 200:\n        raise Exception(\"GET request not successful.\")\n\n    # Read the GeoJSON data into a GeoDataFrame\n    gdf = gpd.read_file(StringIO(response.text))\n\n    # Convert the GeoDataFrame to a Spark DataFrame\n    df = spark.createDataFrame(gdf)\n\n    return df\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.HttpGeoJsonInterface.write_from_interface","title":"<code>write_from_interface(df, url, timeout=60, max_retries=5)</code>","text":"<p>Method that writes a DataFrame to an HTTP source as GeoJSON data.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to write.</p> required <code>url</code> <code>str</code> <p>URL of the HTTP source.</p> required <code>timeout</code> <code>int</code> <p>Timeout for the POST request in seconds. Default is 60.</p> <code>60</code> <code>max_retries</code> <code>int</code> <p>Maximum number of retries for the POST request. Default is 5.</p> <code>5</code> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def write_from_interface(self, df: DataFrame, url: str, timeout: int = 60, max_retries: int = 5):\n    \"\"\"Method that writes a DataFrame to an HTTP source as GeoJSON data.\n\n    Args:\n        df (DataFrame): DataFrame to write.\n        url (str): URL of the HTTP source.\n        timeout (int): Timeout for the POST request in seconds. Default is 60.\n        max_retries (int): Maximum number of retries for the POST request. Default is 5.\n    \"\"\"\n    raise NotImplementedError(\"This method is not implemented.\")\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.IOInterface","title":"<code>IOInterface</code>","text":"<p>Abstract interface that provides functionality for reading and writing data</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class IOInterface(metaclass=ABCMeta):\n    \"\"\"Abstract interface that provides functionality for reading and writing data\"\"\"\n\n    @classmethod\n    def __subclasshook__(cls, subclass: type) -&gt; bool:\n        if cls is IOInterface:\n            attrs: List[str] = []\n            callables: List[str] = [\"read_from_interface\", \"write_from_interface\"]\n            ret: bool = True\n            for attr in attrs:\n                ret = ret and (hasattr(subclass, attr) and isinstance(getattr(subclass, attr), property))\n            for call in callables:\n                ret = ret and (hasattr(subclass, call) and callable(getattr(subclass, call)))\n            return ret\n        else:\n            return NotImplemented\n\n    @abstractmethod\n    def read_from_interface(self, *args, **kwargs) -&gt; DataFrame:\n        pass\n\n    @abstractmethod\n    def write_from_interface(self, df: DataFrame, *args, **kwargs):\n        pass\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.JsonInterface","title":"<code>JsonInterface</code>","text":"<p>               Bases: <code>PathInterface</code></p> <p>Class that implements the PathInterface abstract class for reading/writing data from a json data source.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class JsonInterface(PathInterface):\n    \"\"\"Class that implements the PathInterface abstract class for reading/writing data from a json data source.\"\"\"\n\n    FILE_FORMAT = \"json\"\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.ParquetInterface","title":"<code>ParquetInterface</code>","text":"<p>               Bases: <code>PathInterface</code></p> <p>Class that implements the PathInterface abstract class for reading/writing data from a parquet data source.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class ParquetInterface(PathInterface):\n    \"\"\"Class that implements the PathInterface abstract class for reading/writing data from a parquet data source.\"\"\"\n\n    FILE_FORMAT = \"parquet\"\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.PathInterface","title":"<code>PathInterface</code>","text":"<p>               Bases: <code>IOInterface</code></p> <p>Abstract interface for reading/writing data from a file type data source.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class PathInterface(IOInterface, metaclass=ABCMeta):\n    \"\"\"Abstract interface for reading/writing data from a file type data source.\"\"\"\n\n    FILE_FORMAT = \"\"\n\n    def read_from_interface(self, spark: SparkSession, path: str, schema: StructType = None) -&gt; DataFrame:\n        \"\"\"Method that reads data from a file type data source as a Spark DataFrame.\n\n        Args:\n            spark (SparkSession): Spark session.\n            path (str): Path to the data.\n            schema (StructType, optional): Schema of the data. Defaults to None.\n\n        Returns:\n            df: Spark dataframe.\n        \"\"\"\n        if schema is None:\n            return spark.read.format(self.FILE_FORMAT).load(path)\n        else:\n            return (\n                spark.read.schema(schema).format(self.FILE_FORMAT).load(path)\n            )  # Read schema  # File format  # Load path\n\n    def write_from_interface(\n        self, df: DataFrame, path: str, partition_columns: List[str] = None, mode: str = SPARK_WRITING_MODES.APPEND\n    ):\n        \"\"\"Method that writes data from a Spark DataFrame to a file type data source.\n\n        Args:\n            df (DataFrame): Spark DataFrame.\n            path (str): Path to the data.\n            partition_columns (List[str], optional): columns used for a partition write.\n        \"\"\"\n        # Args check\n        if partition_columns is None:\n            partition_columns = []\n\n        df.write.format(\n            self.FILE_FORMAT,  # File format\n        ).partitionBy(partition_columns).mode(\n            mode\n        ).save(path)\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.PathInterface.read_from_interface","title":"<code>read_from_interface(spark, path, schema=None)</code>","text":"<p>Method that reads data from a file type data source as a Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session.</p> required <code>path</code> <code>str</code> <p>Path to the data.</p> required <code>schema</code> <code>StructType</code> <p>Schema of the data. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>Spark dataframe.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def read_from_interface(self, spark: SparkSession, path: str, schema: StructType = None) -&gt; DataFrame:\n    \"\"\"Method that reads data from a file type data source as a Spark DataFrame.\n\n    Args:\n        spark (SparkSession): Spark session.\n        path (str): Path to the data.\n        schema (StructType, optional): Schema of the data. Defaults to None.\n\n    Returns:\n        df: Spark dataframe.\n    \"\"\"\n    if schema is None:\n        return spark.read.format(self.FILE_FORMAT).load(path)\n    else:\n        return (\n            spark.read.schema(schema).format(self.FILE_FORMAT).load(path)\n        )  # Read schema  # File format  # Load path\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.PathInterface.write_from_interface","title":"<code>write_from_interface(df, path, partition_columns=None, mode=SPARK_WRITING_MODES.APPEND)</code>","text":"<p>Method that writes data from a Spark DataFrame to a file type data source.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Spark DataFrame.</p> required <code>path</code> <code>str</code> <p>Path to the data.</p> required <code>partition_columns</code> <code>List[str]</code> <p>columns used for a partition write.</p> <code>None</code> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def write_from_interface(\n    self, df: DataFrame, path: str, partition_columns: List[str] = None, mode: str = SPARK_WRITING_MODES.APPEND\n):\n    \"\"\"Method that writes data from a Spark DataFrame to a file type data source.\n\n    Args:\n        df (DataFrame): Spark DataFrame.\n        path (str): Path to the data.\n        partition_columns (List[str], optional): columns used for a partition write.\n    \"\"\"\n    # Args check\n    if partition_columns is None:\n        partition_columns = []\n\n    df.write.format(\n        self.FILE_FORMAT,  # File format\n    ).partitionBy(partition_columns).mode(\n        mode\n    ).save(path)\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.ShapefileInterface","title":"<code>ShapefileInterface</code>","text":"<p>               Bases: <code>PathInterface</code></p> <p>Class that implements the PathInterface abstract class for reading/writing data from a ShapeFile data source.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class ShapefileInterface(PathInterface):\n    \"\"\"Class that implements the PathInterface abstract class for reading/writing data from a ShapeFile data source.\"\"\"\n\n    def read_from_interface(self, spark: SparkSession, path: str, schema: StructType = None) -&gt; DataFrame:\n        \"\"\"Method that reads data from a ShapeFile type data source as a Spark DataFrame.\n\n        Args:\n            spark (SparkSession): Spark session.\n            path (str): Path to the data.\n            schema (StructType, optional): Schema of the data. Defaults to None.\n\n        Returns:\n            df: Spark dataframe.\n        \"\"\"\n        df = ShapefileReader.readToGeometryRDD(spark.sparkContext, path)\n        return Adapter.toDf(df, spark)\n\n    def write_from_interface(self, df: DataFrame, path: str, partition_columns: list = None):\n        \"\"\"Method that writes data from a Spark DataFrame to a ShapeFile data source.\n\n        Args:\n            df (DataFrame): Spark DataFrame.\n            path (str): Path to the data.\n            partition_columns (List[str], optional): columns used for a partition write.\n        Raises:\n            NotImplementedError: ShapeFile files should not be written in this architecture.\n        \"\"\"\n        raise NotImplementedError(\"Not implemented as Shapefiles shouldn't be written\")\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.ShapefileInterface.read_from_interface","title":"<code>read_from_interface(spark, path, schema=None)</code>","text":"<p>Method that reads data from a ShapeFile type data source as a Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session.</p> required <code>path</code> <code>str</code> <p>Path to the data.</p> required <code>schema</code> <code>StructType</code> <p>Schema of the data. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>Spark dataframe.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def read_from_interface(self, spark: SparkSession, path: str, schema: StructType = None) -&gt; DataFrame:\n    \"\"\"Method that reads data from a ShapeFile type data source as a Spark DataFrame.\n\n    Args:\n        spark (SparkSession): Spark session.\n        path (str): Path to the data.\n        schema (StructType, optional): Schema of the data. Defaults to None.\n\n    Returns:\n        df: Spark dataframe.\n    \"\"\"\n    df = ShapefileReader.readToGeometryRDD(spark.sparkContext, path)\n    return Adapter.toDf(df, spark)\n</code></pre>"},{"location":"reference/core/io_interface/#core.io_interface.ShapefileInterface.write_from_interface","title":"<code>write_from_interface(df, path, partition_columns=None)</code>","text":"<p>Method that writes data from a Spark DataFrame to a ShapeFile data source.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Spark DataFrame.</p> required <code>path</code> <code>str</code> <p>Path to the data.</p> required <code>partition_columns</code> <code>List[str]</code> <p>columns used for a partition write.</p> <code>None</code> <p>Raises:     NotImplementedError: ShapeFile files should not be written in this architecture.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def write_from_interface(self, df: DataFrame, path: str, partition_columns: list = None):\n    \"\"\"Method that writes data from a Spark DataFrame to a ShapeFile data source.\n\n    Args:\n        df (DataFrame): Spark DataFrame.\n        path (str): Path to the data.\n        partition_columns (List[str], optional): columns used for a partition write.\n    Raises:\n        NotImplementedError: ShapeFile files should not be written in this architecture.\n    \"\"\"\n    raise NotImplementedError(\"Not implemented as Shapefiles shouldn't be written\")\n</code></pre>"},{"location":"reference/core/log/","title":"log","text":"<p>Module that manages the logging functionality.</p>"},{"location":"reference/core/log/#core.log._convert_size_bytes","title":"<code>_convert_size_bytes(size_bytes)</code>","text":"<p>Converts a size in bytes to a human readable string using SI units.</p> Source code in <code>multimno/core/log.py</code> <pre><code>def _convert_size_bytes(size_bytes):\n    \"\"\"\n    Converts a size in bytes to a human readable string using SI units.\n    \"\"\"\n    import math\n    import sys\n\n    if size_bytes == 0:\n        return \"0B\"\n\n    size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n    i = int(math.floor(math.log(size_bytes, 1024)))\n    p = math.pow(1024, i)\n    s = round(size_bytes / p, 2)\n    return \"%s %s\" % (s, size_name[i])\n</code></pre>"},{"location":"reference/core/log/#core.log.generate_logger","title":"<code>generate_logger(config, component_id)</code>","text":"<p>Function that initializes a logger.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ConfigParser</code> <p>Object with the final configuration.</p> required <p>Returns:</p> Type Description <code>Logger</code> <p>Python logging object.</p> Source code in <code>multimno/core/log.py</code> <pre><code>def generate_logger(config: ConfigParser, component_id: str):\n    \"\"\"Function that initializes a logger.\n\n    Args:\n        config (ConfigParser): Object with the final configuration.\n\n    Returns:\n        (logging.Logger): Python logging object.\n    \"\"\"\n\n    notset_level = logging.getLevelName(logging.NOTSET)\n\n    # Parse config\n    console_log_level = config.get(LoggerKeys.LOG_CONFIG_KEY, LoggerKeys.CONSOLE_LOG_LEVEL, fallback=None)\n    file_log_level = config.get(LoggerKeys.LOG_CONFIG_KEY, LoggerKeys.FILE_LOG_LEVEL, fallback=None)\n    console_format = config.get(LoggerKeys.LOG_CONFIG_KEY, LoggerKeys.CONSOLE_FORMAT, fallback=None)\n    file_format = config.get(LoggerKeys.LOG_CONFIG_KEY, LoggerKeys.FILE_FORMAT, fallback=None)\n    datefmt = config.get(LoggerKeys.LOG_CONFIG_KEY, LoggerKeys.DATEFMT, fallback=None)\n    report_path = config.get(LoggerKeys.LOG_CONFIG_KEY, LoggerKeys.REPORT_PATH, fallback=None)\n\n    # Check if logger already exists\n    logger = logging.getLogger(component_id)\n    if len(logger.handlers) &gt; 0:\n        logger.warning(f\"Logger {component_id} already exists.\")\n        return logger\n\n    # Define a console logger\n    if console_log_level is not None and console_log_level != str(notset_level):\n        # Set console handler\n        console_h = logging.StreamHandler()\n        console_h.setLevel(console_log_level)\n        # Set console formatter\n        console_formatter = logging.Formatter(fmt=console_format, datefmt=datefmt)\n        console_h.setFormatter(console_formatter)\n        # Add console handler to logger\n        logger.addHandler(console_h)\n\n    # Define a file logger\n    if file_log_level is not None and file_log_level != str(notset_level):\n        # Verify required fields for file logger\n        if report_path is None:\n            raise ValueError(\"report_path is required to build a file logger.\")\n\n        # Get log path\n        today = datetime.now().strftime(\"%y%m%d_%H%M%S\")\n        log_path = f\"{report_path}/{component_id}/{component_id}_{today}.log\"\n        # Make report path + log dir\n        os.makedirs(os.path.dirname(log_path), exist_ok=True)\n\n        # Set File handler\n        file_h = logging.FileHandler(log_path)\n        file_h.setLevel(file_log_level)\n        # Set file formatter\n        file_formatter = logging.Formatter(fmt=file_format, datefmt=datefmt)\n        file_h.setFormatter(file_formatter)\n        # Add file handler to logger\n        logger.addHandler(file_h)\n\n    # Set logger level\n    logger.setLevel(logging.DEBUG)\n    # Return logger\n    return logger\n</code></pre>"},{"location":"reference/core/settings/","title":"settings","text":"<p>Settings module</p>"},{"location":"reference/core/spark_session/","title":"spark_session","text":"<p>Module that manages the spark session.</p>"},{"location":"reference/core/spark_session/#core.spark_session.SPARK_WRITING_MODES","title":"<code>SPARK_WRITING_MODES</code>","text":"<p>Enum class to define writing modes for spark</p> Source code in <code>multimno/core/spark_session.py</code> <pre><code>class SPARK_WRITING_MODES:\n    \"\"\"Enum class to define writing modes for spark\"\"\"\n\n    OVERWRITE = \"overwrite\"\n    APPEND = \"append\"\n    IGNORE = \"ignore\"\n    ERROR = \"error\"\n</code></pre>"},{"location":"reference/core/spark_session/#core.spark_session.check_if_data_path_exists","title":"<code>check_if_data_path_exists(spark, data_path)</code>","text":"<p>Checks whether data path exists, returns True if it does, False if not</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>active SparkSession</p> required <code>data_path</code> <code>str</code> <p>path to check</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether the passed path exists</p> Source code in <code>multimno/core/spark_session.py</code> <pre><code>def check_if_data_path_exists(spark: SparkSession, data_path: str) -&gt; bool:\n    \"\"\"\n    Checks whether data path exists, returns True if it does, False if not\n\n    Args:\n        spark (SparkSession): active SparkSession\n        data_path (str): path to check\n\n    Returns:\n        bool: Whether the passed path exists\n    \"\"\"\n    conf = spark._jsc.hadoopConfiguration()\n    uri = spark._jvm.java.net.URI.create(data_path)\n    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(uri, conf)\n    return fs.exists(spark._jvm.org.apache.hadoop.fs.Path(data_path))\n</code></pre>"},{"location":"reference/core/spark_session/#core.spark_session.check_or_create_data_path","title":"<code>check_or_create_data_path(spark, data_path)</code>","text":"<p>Create the provided path on a file system. If path already exists, do nothing.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>active SparkSession</p> required <code>data_path</code> <code>str</code> <p>path to check</p> required Source code in <code>multimno/core/spark_session.py</code> <pre><code>def check_or_create_data_path(spark: SparkSession, data_path: str):\n    \"\"\"\n    Create the provided path on a file system. If path already exists, do nothing.\n\n    Args:\n        spark (SparkSession): active SparkSession\n        data_path (str): path to check\n    \"\"\"\n    conf = spark._jsc.hadoopConfiguration()\n    uri = spark._jvm.java.net.URI.create(data_path)\n    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(uri, conf)\n    path = spark._jvm.org.apache.hadoop.fs.Path(data_path)\n    if not fs.exists(path):\n        fs.mkdirs(path)\n</code></pre>"},{"location":"reference/core/spark_session/#core.spark_session.delete_file_or_folder","title":"<code>delete_file_or_folder(spark, data_path)</code>","text":"<p>Deletes file or folder with given path</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Currently active spark session</p> required <code>data_path</code> <code>str</code> <p>Path to remove</p> required Source code in <code>multimno/core/spark_session.py</code> <pre><code>def delete_file_or_folder(spark: SparkSession, data_path: str):\n    \"\"\"\n    Deletes file or folder with given path\n\n    Args:\n        spark (SparkSession): Currently active spark session\n        data_path (str): Path to remove\n    \"\"\"\n    conf = spark._jsc.hadoopConfiguration()\n    uri = spark._jvm.java.net.URI.create(data_path)\n    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(uri, conf)\n    path = spark._jvm.org.apache.hadoop.fs.Path(data_path)\n    fs.delete(path, True)\n</code></pre>"},{"location":"reference/core/spark_session/#core.spark_session.generate_spark_session","title":"<code>generate_spark_session(config)</code>","text":"<p>Function that generates a Spark Sedona session.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ConfigParser</code> <p>Object with the final configuration.</p> required <p>Returns:</p> Name Type Description <code>SparkSession</code> <code>SparkSession</code> <p>Session of spark.</p> Source code in <code>multimno/core/spark_session.py</code> <pre><code>def generate_spark_session(config: ConfigParser) -&gt; SparkSession:\n    \"\"\"Function that generates a Spark Sedona session.\n\n    Args:\n        config (ConfigParser): Object with the final configuration.\n\n    Returns:\n        SparkSession: Session of spark.\n    \"\"\"\n    conf_dict = dict(config[SPARK_CONFIG_KEY])\n    master = conf_dict.pop(\"spark.master\")\n    session_name = conf_dict.pop(\"session_name\")\n\n    builder = SedonaContext.builder().appName(f\"{session_name}\").master(master)\n\n    # Configuration file spark configs\n    for k, v in conf_dict.items():\n        builder = builder.config(k, v)\n\n    ##################\n    # SEDONA\n    ##################\n\n    # Set sedona session\n    spark = SedonaContext.create(builder.getOrCreate())\n    sc = spark.sparkContext\n    sc.setSystemProperty(\"sedona.global.charset\", \"utf8\")\n\n    # Set log\n    sc.setLogLevel(\"ERROR\")\n    log4j = sc._jvm.org.apache.log4j\n    log4j.LogManager.getRootLogger().setLevel(log4j.Level.ERROR)\n\n    return spark\n</code></pre>"},{"location":"reference/core/spark_session/#core.spark_session.list_all_files_helper","title":"<code>list_all_files_helper(path, fs, conf)</code>","text":"<p>This function is used by list_all_files_recursively. This should not be called elsewhere Recursively traverses the file tree from given spot saving all files to a list and returns it.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>py4j.java_gateway.JavaObject: Object from parent function</p> required <code>fs</code> <code>JavaClass</code> <p>Object from parent function</p> required <code>conf</code> <code>JavaObject</code> <p>Object from parent function</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>List[str]</code> <p>List of all files this folder and subdirectories of this folder.</p> Source code in <code>multimno/core/spark_session.py</code> <pre><code>def list_all_files_helper(\n    path: py4j.java_gateway.JavaObject, fs: py4j.java_gateway.JavaClass, conf: py4j.java_gateway.JavaObject\n) -&gt; List[str]:\n    \"\"\"\n    This function is used by list_all_files_recursively. This should not be called elsewhere\n    Recursively traverses the file tree from given spot saving all files to a list and returns it.\n\n    Args:\n        path (str): py4j.java_gateway.JavaObject: Object from parent function\n        fs (py4j.java_gateway.JavaClass): Object from parent function\n        conf (py4j.java_gateway.JavaObject): Object from parent function\n\n    Returns:\n        list: List of all files this folder and subdirectories of this folder.\n    \"\"\"\n    files_list = []\n\n    for f in fs.listStatus(path):\n        if f.isDirectory():\n            files_list.extend(list_all_files_helper(f.getPath(), fs, conf))\n        else:\n            files_list.append(str(f.getPath()))\n\n    return files_list\n</code></pre>"},{"location":"reference/core/spark_session/#core.spark_session.list_all_files_recursively","title":"<code>list_all_files_recursively(spark, data_path)</code>","text":"<p>If path is a file, returns a singleton list with this path. If path is a folder, return a list of all files in this folder and any of its subfolders</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Currently active spark session</p> required <code>data_path</code> <code>str</code> <p>Path to list the files of</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of all files in that folder and its subfolders</p> Source code in <code>multimno/core/spark_session.py</code> <pre><code>def list_all_files_recursively(spark: SparkSession, data_path: str) -&gt; List[str]:\n    \"\"\"\n    If path is a file, returns a singleton list with this path.\n    If path is a folder, return a list of all files in this folder and any of its subfolders\n\n    Args:\n        spark (SparkSession): Currently active spark session\n        data_path (str): Path to list the files of\n\n    Returns:\n        List[str]: A list of all files in that folder and its subfolders\n    \"\"\"\n    conf = spark._jsc.hadoopConfiguration()\n    uri = spark._jvm.java.net.URI.create(data_path)\n    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(uri, conf)\n    path = spark._jvm.org.apache.hadoop.fs.Path(data_path)\n    return list_all_files_helper(path, fs, conf)\n</code></pre>"},{"location":"reference/core/spark_session/#core.spark_session.list_parquet_partition_col_values","title":"<code>list_parquet_partition_col_values(spark, data_path)</code>","text":"<p>Lists all partition column values given a partition parquet folder</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Currently active spark session</p> required <code>data_path</code> <code>str</code> <p>Path of parquet</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>str, List[str]: Name of partition column, List of partition col values</p> Source code in <code>multimno/core/spark_session.py</code> <pre><code>def list_parquet_partition_col_values(spark: SparkSession, data_path: str) -&gt; List[str]:\n    \"\"\"\n    Lists all partition column values given a partition parquet folder\n\n    Args:\n        spark (SparkSession): Currently active spark session\n        data_path (str): Path of parquet\n\n    Returns:\n        str, List[str]: Name of partition column, List of partition col values\n    \"\"\"\n\n    hadoop = spark._jvm.org.apache.hadoop\n    fs = hadoop.fs.FileSystem\n    conf = hadoop.conf.Configuration()\n    path = hadoop.fs.Path(data_path)\n\n    partitions = []\n    for f in fs.get(conf).listStatus(path):\n        if f.isDirectory():\n            partitions.append(str(f.getPath().getName()))\n\n    if len(partitions) == 0:\n        return None, None\n\n    partition_col = partitions[0].split(\"=\")[0]\n\n    partitions = [p.split(\"=\")[1] for p in partitions]\n    return partition_col, sorted(partitions)\n</code></pre>"},{"location":"reference/core/utils/","title":"utils","text":"<p>This module contains utility functions for the multimno package.</p>"},{"location":"reference/core/utils/#core.utils.apply_schema_casting","title":"<code>apply_schema_casting(sdf, schema)</code>","text":"<p>This function takes a DataFrame and a schema, and applies the schema to the DataFrame. It selects the columns in the DataFrame that are in the schema, and casts each column to the type specified in the schema.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The DataFrame to apply the schema to.</p> required <code>schema</code> <code>StructType</code> <p>The schema to apply to the DataFrame.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new DataFrame that includes the same rows as the input DataFrame,</p> <code>DataFrame</code> <p>but with the columns cast to the types specified in the schema.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def apply_schema_casting(sdf: DataFrame, schema: StructType) -&gt; DataFrame:\n    \"\"\"\n    This function takes a DataFrame and a schema, and applies the schema to the DataFrame.\n    It selects the columns in the DataFrame that are in the schema, and casts each column to the type specified in the schema.\n\n    Args:\n        sdf (DataFrame): The DataFrame to apply the schema to.\n        schema (StructType): The schema to apply to the DataFrame.\n\n    Returns:\n        DataFrame: A new DataFrame that includes the same rows as the input DataFrame,\n        but with the columns cast to the types specified in the schema.\n    \"\"\"\n\n    sdf = sdf.select(*[F.col(field.name) for field in schema.fields])\n    for field in schema.fields:\n        sdf = sdf.withColumn(field.name, F.col(field.name).cast(field.dataType))\n\n    return sdf\n</code></pre>"},{"location":"reference/core/utils/#core.utils.assign_quadkey","title":"<code>assign_quadkey(sdf, crs_in, zoom_level)</code>","text":"<p>Assigns a quadkey to each row in a DataFrame based on the centroid of its geometry.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The DataFrame to assign quadkeys to. The DataFrame must contain a geometry column.</p> required <code>crs_in</code> <code>int</code> <p>The CRS of the dataframe to project to 4326 before assigning quadkeys.</p> required <code>zoom_level</code> <code>int</code> <p>The zoom level to use when assigning quadkeys.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing the same rows as the input DataFrame, but with an additional quadkey column.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def assign_quadkey(sdf: DataFrame, crs_in: int, zoom_level: int) -&gt; DataFrame:\n    \"\"\"\n    Assigns a quadkey to each row in a DataFrame based on the centroid of its geometry.\n\n    Args:\n        sdf (DataFrame): The DataFrame to assign quadkeys to. The DataFrame must contain a geometry column.\n        crs_in (int): The CRS of the dataframe to project to 4326 before assigning quadkeys.\n        zoom_level (int): The zoom level to use when assigning quadkeys.\n\n    Returns:\n        DataFrame: A DataFrame containing the same rows as the input DataFrame, but with an additional quadkey column.\n    \"\"\"\n\n    quadkey_udf = F.udf(latlon_to_quadkey, StringType())\n    sdf = sdf.withColumn(\"centroid\", STF.ST_Centroid(ColNames.geometry))\n\n    if crs_in != 4326:\n        sdf = project_to_crs(sdf, crs_in, 4326, \"centroid\")\n\n    sdf = sdf.withColumn(\n        \"quadkey\",\n        quadkey_udf(\n            STF.ST_Y(F.col(\"centroid\")),\n            STF.ST_X(F.col(\"centroid\")),\n            F.lit(zoom_level),\n        ),\n    ).drop(\"centroid\")\n\n    return sdf\n</code></pre>"},{"location":"reference/core/utils/#core.utils.calc_hashed_user_id","title":"<code>calc_hashed_user_id(df, user_column=ColNames.user_id)</code>","text":"<p>Calculates SHA2 hash of user id, takes the first 31 bits and converts them to a non-negative 32-bit integer.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Data of clean synthetic events with a user id column.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: Dataframe, where user_id column is transformered to a hashed value.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def calc_hashed_user_id(df: DataFrame, user_column: str = ColNames.user_id) -&gt; DataFrame:\n    \"\"\"\n    Calculates SHA2 hash of user id, takes the first 31 bits and converts them to a non-negative 32-bit integer.\n\n    Args:\n        df (pyspark.sql.DataFrame): Data of clean synthetic events with a user id column.\n\n    Returns:\n        pyspark.sql.DataFrame: Dataframe, where user_id column is transformered to a hashed value.\n\n    \"\"\"\n\n    df = df.withColumn(user_column, F.unhex(F.sha2(F.col(user_column).cast(\"string\"), 256)))\n    return df\n</code></pre>"},{"location":"reference/core/utils/#core.utils.cut_geodata_to_extent","title":"<code>cut_geodata_to_extent(sdf, extent, target_crs, geometry_column='geometry')</code>","text":"<p>Cuts geometries in a DataFrame to a specified extent.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The DataFrame to filter. The DataFrame must contain a geometry column.</p> required <code>extent</code> <code>tuple</code> <p>A tuple representing the extent. The tuple contains four elements: (west, south, east, north), which are the western, southern, eastern, and northern bounds of the WGS84 extent.</p> required <code>target_crs</code> <code>int</code> <p>The CRS of DataFrame to transform the extent to.</p> required <code>geometry_column</code> <code>str</code> <p>The name of the geometry column. Defaults to \"geometry\".</p> <code>'geometry'</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing the same rows as the input DataFrame, but with the geometries cut to the extent.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def cut_geodata_to_extent(\n    sdf: DataFrame,\n    extent: Tuple[float, float, float, float],\n    target_crs: int,\n    geometry_column: str = \"geometry\",\n) -&gt; DataFrame:\n    \"\"\"\n    Cuts geometries in a DataFrame to a specified extent.\n\n    Args:\n        sdf (DataFrame): The DataFrame to filter. The DataFrame must contain a geometry column.\n        extent (tuple): A tuple representing the extent. The tuple contains four elements:\n            (west, south, east, north), which are the western, southern, eastern, and northern bounds of the WGS84 extent.\n        target_crs (int): The CRS of DataFrame to transform the extent to.\n        geometry_column (str, optional): The name of the geometry column. Defaults to \"geometry\".\n\n    Returns:\n        DataFrame: A DataFrame containing the same rows as the input DataFrame, but with the geometries cut to the extent.\n    \"\"\"\n\n    extent = STC.ST_PolygonFromEnvelope(*extent)\n    if target_crs != 4326:\n        extent = STF.ST_Transform(extent, F.lit(\"EPSG:4326\"), F.lit(f\"EPSG:{target_crs}\"))\n\n    sdf = sdf.withColumn(geometry_column, STF.ST_Intersection(F.col(geometry_column), extent))\n\n    return sdf\n</code></pre>"},{"location":"reference/core/utils/#core.utils.cut_polygons_with_mask_polygons","title":"<code>cut_polygons_with_mask_polygons(input_sdf, mask_sdf, cols_to_keep, self_intersection=False, geometry_column='geometry')</code>","text":"<p>Cuts polygons in the input DataFrame with mask polygons from another DataFrame. This function takes two DataFrames: one with input polygons and another with mask polygons. It cuts the input polygons with the mask polygons, and returns a new DataFrame with the resulting polygons. Both dataframes have to have same coordinate system. Args:     input_sdf (DataFrame): A DataFrame containing the input polygons.     mask_sdf (DataFrame): A DataFrame containing the mask polygons.     cols_to_keep (list): A list of column names to keep from the input DataFrame.     geometry_column (str, optional): The name of the geometry column in the DataFrames.         Defaults to \"geometry\". Returns:     DataFrame: A DataFrame containing the resulting polygons after cutting the input polygons with the mask polygons.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def cut_polygons_with_mask_polygons(\n    input_sdf: DataFrame,\n    mask_sdf: DataFrame,\n    cols_to_keep: List[str],\n    self_intersection=False,\n    geometry_column: str = \"geometry\",\n) -&gt; DataFrame:\n    \"\"\"\n    Cuts polygons in the input DataFrame with mask polygons from another DataFrame.\n    This function takes two DataFrames: one with input polygons and another with mask polygons.\n    It cuts the input polygons with the mask polygons, and returns a new DataFrame with the resulting polygons.\n    Both dataframes have to have same coordinate system.\n    Args:\n        input_sdf (DataFrame): A DataFrame containing the input polygons.\n        mask_sdf (DataFrame): A DataFrame containing the mask polygons.\n        cols_to_keep (list): A list of column names to keep from the input DataFrame.\n        geometry_column (str, optional): The name of the geometry column in the DataFrames.\n            Defaults to \"geometry\".\n    Returns:\n        DataFrame: A DataFrame containing the resulting polygons after cutting the input polygons with the mask polygons.\n    \"\"\"\n    input_sdf = input_sdf.withColumn(\"id\", F.monotonically_increasing_id())\n    cols_to_keep = [f\"a.{col}\" for col in cols_to_keep]\n    if self_intersection:\n        input_sdf = input_sdf.withColumn(\"area\", STF.ST_Area(geometry_column))\n        intersection = input_sdf.alias(\"a\").join(\n            input_sdf.alias(\"b\"),\n            STP.ST_Intersects(\"a.geometry\", \"b.geometry\") &amp; (F.col(\"a.area\") &gt; F.col(\"b.area\")),\n        )\n        input_sdf = input_sdf.drop(\"area\")\n    else:\n        intersection = input_sdf.alias(\"a\").join(\n            mask_sdf.alias(\"b\"),\n            STP.ST_Intersects(\"a.geometry\", \"b.geometry\"),\n        )\n    intersection_cut = intersection.groupby(\"a.id\", *cols_to_keep).agg(\n        STA.ST_Union_Aggr(f\"b.{geometry_column}\").alias(\"cut_geometry\")\n    )\n    intersection_cut = fix_geometry(intersection_cut, 3, \"cut_geometry\")\n    intersection_cut = intersection_cut.withColumn(\n        geometry_column, STF.ST_Difference(f\"a.{geometry_column}\", \"cut_geometry\")\n    ).drop(\"cut_geometry\")\n\n    non_intersection = input_sdf.join(intersection_cut, [\"id\"], \"left_anti\")\n\n    return non_intersection.union(intersection_cut).drop(\"id\")\n</code></pre>"},{"location":"reference/core/utils/#core.utils.filter_geodata_to_extent","title":"<code>filter_geodata_to_extent(sdf, extent, target_crs, geometry_column='geometry')</code>","text":"<p>Filters a DataFrame to include only rows with geometries that intersect a specified extent.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The DataFrame to filter. The DataFrame must contain a geometry column.</p> required <code>extent</code> <code>tuple</code> <p>A tuple representing the extent. The tuple contains four elements: (west, south, east, north), which are the western, southern, eastern, and northern bounds of the WGS84 extent.</p> required <code>target_crs</code> <code>int</code> <p>The CRS of DataFrame to transform the extent to.</p> required <code>geometry_column</code> <code>str</code> <p>The name of the geometry column. Defaults to \"geometry\".</p> <code>'geometry'</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing only the rows from the input DataFrame where the geometry intersects the extent.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def filter_geodata_to_extent(\n    sdf: DataFrame,\n    extent: Tuple[float, float, float, float],\n    target_crs: int,\n    geometry_column: str = \"geometry\",\n) -&gt; DataFrame:\n    \"\"\"\n    Filters a DataFrame to include only rows with geometries that intersect a specified extent.\n\n    Args:\n        sdf (DataFrame): The DataFrame to filter. The DataFrame must contain a geometry column.\n        extent (tuple): A tuple representing the extent. The tuple contains four elements:\n            (west, south, east, north), which are the western, southern, eastern, and northern bounds of the WGS84 extent.\n        target_crs (int): The CRS of DataFrame to transform the extent to.\n        geometry_column (str, optional): The name of the geometry column. Defaults to \"geometry\".\n\n    Returns:\n        DataFrame: A DataFrame containing only the rows from the input DataFrame where the geometry intersects the extent.\n    \"\"\"\n\n    extent = STC.ST_PolygonFromEnvelope(*extent)\n    if target_crs != 4326:\n        extent = STF.ST_Transform(extent, F.lit(\"EPSG:4326\"), F.lit(f\"EPSG:{target_crs}\"))\n\n    sdf = sdf.filter(STP.ST_Intersects(extent, F.col(geometry_column)))\n\n    return sdf\n</code></pre>"},{"location":"reference/core/utils/#core.utils.fix_geometry","title":"<code>fix_geometry(sdf, geometry_type, geometry_column='geometry')</code>","text":"<p>Fixes the geometry of a given type in a DataFrame. This function applies several operations to the geometries in the specified geometry column of the DataFrame: 1. If a geometry is a collection of geometries, extracts only the geometries of the given type. 2. Filters out any geometries of type other than given. 3. Removes any invalid geometries. 4. Removes any empty geometries. Args:     sdf (DataFrame): The DataFrame containing the geometries to check.     geometry_column (str, optional): The name of the column containing the geometries. Defaults to \"geometry\". Returns:     DataFrame: The DataFrame with the fixed polygon geometries.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def fix_geometry(sdf: DataFrame, geometry_type: int, geometry_column: str = \"geometry\") -&gt; DataFrame:\n    \"\"\"\n    Fixes the geometry of a given type in a DataFrame.\n    This function applies several operations to the geometries in the specified geometry column of the DataFrame:\n    1. If a geometry is a collection of geometries, extracts only the geometries of the given type.\n    2. Filters out any geometries of type other than given.\n    3. Removes any invalid geometries.\n    4. Removes any empty geometries.\n    Args:\n        sdf (DataFrame): The DataFrame containing the geometries to check.\n        geometry_column (str, optional): The name of the column containing the geometries. Defaults to \"geometry\".\n    Returns:\n        DataFrame: The DataFrame with the fixed polygon geometries.\n    \"\"\"\n    geometry_name = \"Polygon\" if geometry_type == 3 else (\"Line\" if geometry_type == 2 else \"Point\")\n    if geometry_type == 3:\n        sdf = sdf.withColumn(geometry_column, STF.ST_ReducePrecision(F.col(geometry_column), F.lit(4)))\n    sdf = (\n        sdf.withColumn(\n            geometry_column,\n            F.when(\n                STF.ST_IsCollection(F.col(geometry_column)),\n                STF.ST_CollectionExtract(geometry_column, F.lit(geometry_type)),\n            ).otherwise(F.col(geometry_column)),\n        )\n        .filter(~STF.ST_IsEmpty(F.col(geometry_column)))\n        .filter(STF.ST_GeometryType(F.col(geometry_column)).like(f\"%{geometry_name}%\"))\n        .filter(STF.ST_IsValid(geometry_column))\n    )\n    return sdf\n</code></pre>"},{"location":"reference/core/utils/#core.utils.get_epsg_from_geometry_column","title":"<code>get_epsg_from_geometry_column(df)</code>","text":"<p>Get the EPSG code from the geometry column of a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with a geometry column.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the DataFrame contains multiple EPSG codes.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>EPSG code of the geometry column.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def get_epsg_from_geometry_column(df: DataFrame) -&gt; int:\n    \"\"\"\n    Get the EPSG code from the geometry column of a DataFrame.\n\n    Args:\n        df (DataFrame): DataFrame with a geometry column.\n\n    Raises:\n        ValueError: If the DataFrame contains multiple EPSG codes.\n\n    Returns:\n        int: EPSG code of the geometry column.\n    \"\"\"\n    # Get the EPSG code from the geometry column\n    temp = df.select(STF.ST_SRID(\"geometry\")).distinct().persist()\n    if temp.count() &gt; 1:\n        raise ValueError(\"Dataframe contains multiple EPSG codes\")\n\n    epsg = temp.collect()[0][0]\n    return epsg\n</code></pre>"},{"location":"reference/core/utils/#core.utils.get_quadkeys_for_bbox","title":"<code>get_quadkeys_for_bbox(extent, level_of_detail)</code>","text":"<p>Generates a list of quadkeys for a bounding box at a specific zoom level.</p> <p>This function takes a bounding box defined by its lon min, lat min, lon max, and lat max extents, and a zoom level, and generates a list of quadkeys that cover the bounding box at the specified zoom level. The quadkeys are strings of digits that represent specific tiles in a quadtree-based spatial index.</p> <p>Parameters:</p> Name Type Description Default <code>extent</code> <code>tuple</code> <p>A tuple representing the bounding box. The tuple contains four elements: (west, south, east, north), which are the western, southern, eastern, and northern extents of the bounding box, respectively. Each extent is a float representing a geographic coordinate in degrees.</p> required <code>level_of_detail</code> <code>int</code> <p>The zoom level.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>List[str]</code> <p>A list of quadkeys that cover the bounding box at the specified zoom level. Each quadkey is a string.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def get_quadkeys_for_bbox(extent: Tuple[float, float, float, float], level_of_detail: int) -&gt; List[str]:\n    \"\"\"\n    Generates a list of quadkeys for a bounding box at a specific zoom level.\n\n    This function takes a bounding box defined by its lon min, lat min, lon max, and lat max extents,\n    and a zoom level, and generates a list of quadkeys that cover the bounding box at the specified zoom level.\n    The quadkeys are strings of digits that represent specific tiles in a quadtree-based spatial index.\n\n    Args:\n        extent (tuple): A tuple representing the bounding box. The tuple contains four elements:\n            (west, south, east, north), which are the western, southern, eastern, and northern extents\n            of the bounding box, respectively. Each extent is a float representing a geographic coordinate in degrees.\n        level_of_detail (int): The zoom level.\n\n    Returns:\n        list: A list of quadkeys that cover the bounding box at the specified zoom level. Each quadkey is a string.\n    \"\"\"\n    west, south, east, north = extent\n    min_tile_x, min_tile_y = latlon_to_tilexy(north, west, level_of_detail)\n    max_tile_x, max_tile_y = latlon_to_tilexy(south, east, level_of_detail)\n    quadkeys = []\n    for x in range(min_tile_x, max_tile_x + 1):\n        for y in range(min_tile_y, max_tile_y + 1):\n            quadkeys.append(tilexy_to_quadkey(x, y, level_of_detail))\n    return quadkeys\n</code></pre>"},{"location":"reference/core/utils/#core.utils.latlon_to_quadkey","title":"<code>latlon_to_quadkey(latitude, longitude, level_of_detail)</code>","text":"<p>Converts a geographic coordinate to a quadkey at a specific zoom level.</p> <p>This function takes a latitude and longitude in degrees, and a zoom level, and converts them to a quadkey. The quadkey is a string of digits that represents a specific tile in a quadtree-based spatial index. The conversion process involves first converting the geographic coordinate to tile coordinates, and then converting the tile coordinates to a quadkey.</p> <p>Parameters:</p> Name Type Description Default <code>latitude</code> <code>float</code> <p>The latitude of the geographic coordinate, in degrees.</p> required <code>longitude</code> <code>float</code> <p>The longitude of the geographic coordinate, in degrees.</p> required <code>level_of_detail</code> <code>int</code> <p>The zoom level.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The quadkey representing the geographic coordinate at the specified zoom level.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def latlon_to_quadkey(latitude: float, longitude: float, level_of_detail: int) -&gt; str:\n    \"\"\"\n    Converts a geographic coordinate to a quadkey at a specific zoom level.\n\n    This function takes a latitude and longitude in degrees, and a zoom level, and converts them to a quadkey.\n    The quadkey is a string of digits that represents a specific tile in a quadtree-based spatial index.\n    The conversion process involves first converting the geographic coordinate to tile coordinates,\n    and then converting the tile coordinates to a quadkey.\n\n    Args:\n        latitude (float): The latitude of the geographic coordinate, in degrees.\n        longitude (float): The longitude of the geographic coordinate, in degrees.\n        level_of_detail (int): The zoom level.\n\n    Returns:\n        str: The quadkey representing the geographic coordinate at the specified zoom level.\n    \"\"\"\n    x, y = latlon_to_tilexy(latitude, longitude, level_of_detail)\n    return tilexy_to_quadkey(x, y, level_of_detail)\n</code></pre>"},{"location":"reference/core/utils/#core.utils.latlon_to_tilexy","title":"<code>latlon_to_tilexy(latitude, longitude, level_of_detail)</code>","text":"<p>Converts a geographic coordinate to tile coordinates at a specific zoom level.</p> <p>This function takes a latitude and longitude in degrees, and a zoom level, and converts them to tile coordinates (tile_x, tile_y) at the specified zoom level. The tile coordinates are in the tile system used by Bing Maps, OpenStreetMap, MapBox and other map providers.</p> <p>Parameters:</p> Name Type Description Default <code>latitude</code> <code>float</code> <p>The latitude of the geographic coordinate, in degrees.</p> required <code>longitude</code> <code>float</code> <p>The longitude of the geographic coordinate, in degrees.</p> required <code>level_of_detail</code> <code>int</code> <p>The zoom level.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>int</code> <p>A tuple representing the tile coordinates of the geographic coordinate at the specified</p> <code>int</code> <p>zoom level. The tuple contains two elements: (tile_x, tile_y).</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def latlon_to_tilexy(latitude: float, longitude: float, level_of_detail: int) -&gt; Tuple[int, int]:\n    \"\"\"\n    Converts a geographic coordinate to tile coordinates at a specific zoom level.\n\n    This function takes a latitude and longitude in degrees, and a zoom level, and converts them to\n    tile coordinates (tile_x, tile_y) at the specified zoom level. The tile coordinates are in the\n    tile system used by Bing Maps, OpenStreetMap, MapBox and other map providers.\n\n    Args:\n        latitude (float): The latitude of the geographic coordinate, in degrees.\n        longitude (float): The longitude of the geographic coordinate, in degrees.\n        level_of_detail (int): The zoom level.\n\n    Returns:\n        tuple: A tuple representing the tile coordinates of the geographic coordinate at the specified\n        zoom level. The tuple contains two elements: (tile_x, tile_y).\n    \"\"\"\n    if not -90 &lt;= latitude &lt;= 90:\n        raise ValueError(f\"Latitude must be in the range [-90, 90], got {latitude}\")\n    if not -180 &lt;= longitude &lt;= 180:\n        raise ValueError(f\"Longitude must be in the range [-180, 180], got {longitude}\")\n    latitude = math.radians(latitude)\n    longitude = math.radians(longitude)\n\n    sinLatitude = math.sin(latitude)\n    pixelX = ((longitude + math.pi) / (2 * math.pi)) * 256 * 2**level_of_detail\n    pixelY = (0.5 - math.log((1 + sinLatitude) / (1 - sinLatitude)) / (4 * math.pi)) * 256 * 2**level_of_detail\n    tileX = int(math.floor(pixelX / 256))\n    tileY = int(math.floor(pixelY / 256))\n    return tileX, tileY\n</code></pre>"},{"location":"reference/core/utils/#core.utils.project_to_crs","title":"<code>project_to_crs(sdf, crs_in, crs_out, geometry_column='geometry')</code>","text":"<p>Projects geometry to CRS.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>crs_in</code> <code>int</code> <p>Input CRS.</p> required <code>crs_out</code> <code>int</code> <p>Output CRS.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>DataFrame with geometry projected to cartesian CRS.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def project_to_crs(sdf: DataFrame, crs_in: int, crs_out: int, geometry_column=\"geometry\") -&gt; DataFrame:\n    \"\"\"\n    Projects geometry to CRS.\n\n    Args:\n        sdf (DataFrame): Input DataFrame.\n        crs_in (int): Input CRS.\n        crs_out (int): Output CRS.\n\n    Returns:\n        DataFrame: DataFrame with geometry projected to cartesian CRS.\n    \"\"\"\n    crs_in = f\"EPSG:{crs_in}\"\n    crs_out = f\"EPSG:{crs_out}\"\n\n    sdf = sdf.withColumn(\n        geometry_column,\n        STF.ST_Transform(sdf[geometry_column], F.lit(crs_in), F.lit(crs_out)),\n    )\n    return sdf\n</code></pre>"},{"location":"reference/core/utils/#core.utils.quadkey_to_extent","title":"<code>quadkey_to_extent(quadkey)</code>","text":"<p>Converts a quadkey to a geographic extent (bounding box).</p> <p>This function takes a quadkey and converts it to a geographic extent represented as a tuple of (longitude_min, latitude_min, longitude_max, latitude_max).</p> <p>Parameters:</p> Name Type Description Default <code>quadkey</code> <code>str</code> <p>The quadkey to convert. A quadkey is a string of digits that represents a</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[float, float, float, float]</code> <p>A tuple representing the geographic extent of the quadkey. The tuple contains four elements: (longitude_min, latitude_min, longitude_max, latitude_max).</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def quadkey_to_extent(quadkey: str) -&gt; Tuple[float, float, float, float]:\n    \"\"\"\n    Converts a quadkey to a geographic extent (bounding box).\n\n    This function takes a quadkey and converts it to a geographic extent represented as a tuple of\n    (longitude_min, latitude_min, longitude_max, latitude_max).\n\n    Args:\n        quadkey (str): The quadkey to convert. A quadkey is a string of digits that represents a\n        specific tile in a quadtree-based spatial index.\n\n    Returns:\n        tuple: A tuple representing the geographic extent of the quadkey. The tuple contains four\n            elements: (longitude_min, latitude_min, longitude_max, latitude_max).\n    \"\"\"\n    tile_x, tile_y, zoom_level = quadkey_to_tile(quadkey)\n    n = 2.0**zoom_level\n    lon_min = tile_x / n * 360.0 - 180.0\n    lat_min = math.degrees(math.atan(math.sinh(math.pi * (1 - 2 * (tile_y + 1) / n))))\n    lon_max = (tile_x + 1) / n * 360.0 - 180.0\n    lat_max = math.degrees(math.atan(math.sinh(math.pi * (1 - 2 * tile_y / n))))\n\n    return (lon_min, lat_min, lon_max, lat_max)\n</code></pre>"},{"location":"reference/core/utils/#core.utils.quadkey_to_tile","title":"<code>quadkey_to_tile(quadkey)</code>","text":"<p>Converts a quadkey to tile coordinates and zoom level.</p> <p>This function takes a quadkey and converts it to tile coordinates (tile_x, tile_y) and zoom level. A quadkey is a string of digits that represents a specific tile in a quadtree-based spatial index.</p> <p>Parameters:</p> Name Type Description Default <code>quadkey</code> <code>str</code> <p>The quadkey to convert.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>int</code> <p>A tuple representing the tile coordinates and zoom level of the quadkey. The tuple contains three</p> <code>elements</code> <code>int</code> <p>(tile_x, tile_y, zoom_level).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the quadkey contains an invalid character.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def quadkey_to_tile(quadkey: str) -&gt; Tuple[int, int, int]:\n    \"\"\"\n    Converts a quadkey to tile coordinates and zoom level.\n\n    This function takes a quadkey and converts it to tile coordinates (tile_x, tile_y) and zoom level.\n    A quadkey is a string of digits that represents a specific tile in a quadtree-based spatial index.\n\n    Args:\n        quadkey (str): The quadkey to convert.\n\n    Returns:\n        tuple: A tuple representing the tile coordinates and zoom level of the quadkey. The tuple contains three\n        elements: (tile_x, tile_y, zoom_level).\n\n    Raises:\n        ValueError: If the quadkey contains an invalid character.\n    \"\"\"\n    tile_x = tile_y = 0\n    zoom_level = len(quadkey)\n    for i in range(zoom_level):\n        bit = zoom_level - i - 1\n        mask = 1 &lt;&lt; bit\n        if quadkey[i] == \"0\":\n            pass\n        elif quadkey[i] == \"1\":\n            tile_x |= mask\n        elif quadkey[i] == \"2\":\n            tile_y |= mask\n        elif quadkey[i] == \"3\":\n            tile_x |= mask\n            tile_y |= mask\n        else:\n            raise ValueError(\"Invalid quadkey character.\")\n    return tile_x, tile_y, zoom_level\n</code></pre>"},{"location":"reference/core/utils/#core.utils.spark_to_geopandas","title":"<code>spark_to_geopandas(df, epsg=None)</code>","text":"<p>Convert a Spark DataFrame to a geopandas GeoDataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Spark DataFrame to convert.</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: GeoDataFrame with the same data as the input DataFrame.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def spark_to_geopandas(df: DataFrame, epsg: int = None) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Convert a Spark DataFrame to a geopandas GeoDataFrame.\n\n    Args:\n        df (DataFrame): Spark DataFrame to convert.\n\n    Returns:\n        gpd.GeoDataFrame: GeoDataFrame with the same data as the input DataFrame.\n    \"\"\"\n    # Convert the DataFrame to a GeoDataFrame\n    if epsg is None:\n        epsg = get_epsg_from_geometry_column(df)\n    gdf = gpd.GeoDataFrame(df.toPandas(), crs=f\"EPSG:{epsg}\")\n\n    return gdf\n</code></pre>"},{"location":"reference/core/utils/#core.utils.tilexy_to_quadkey","title":"<code>tilexy_to_quadkey(x, y, level_of_detail)</code>","text":"<p>Converts tile coordinates to a quadkey at a specific zoom level.</p> <p>This function takes tile coordinates (x, y) and a zoom level, and converts them to a quadkey. The quadkey is a string of digits that represents a specific tile in a quadtree-based spatial index. The conversion process involves bitwise operations on the tile coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>int</code> <p>The x-coordinate of the tile.</p> required <code>y</code> <code>int</code> <p>The y-coordinate of the tile.</p> required <code>level_of_detail</code> <code>int</code> <p>The zoom level.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The quadkey representing the tile at the specified zoom level.</p> Source code in <code>multimno/core/utils.py</code> <pre><code>def tilexy_to_quadkey(x: int, y: int, level_of_detail: int) -&gt; str:\n    \"\"\"\n    Converts tile coordinates to a quadkey at a specific zoom level.\n\n    This function takes tile coordinates (x, y) and a zoom level, and converts them to a quadkey.\n    The quadkey is a string of digits that represents a specific tile in a quadtree-based spatial index.\n    The conversion process involves bitwise operations on the tile coordinates.\n\n    Args:\n        x (int): The x-coordinate of the tile.\n        y (int): The y-coordinate of the tile.\n        level_of_detail (int): The zoom level.\n\n    Returns:\n        str: The quadkey representing the tile at the specified zoom level.\n    \"\"\"\n    quadkey = \"\"\n    for i in range(level_of_detail, 0, -1):\n        digit = 0\n        mask = 1 &lt;&lt; (i - 1)\n        if (x &amp; mask) != 0:\n            digit += 1\n        if (y &amp; mask) != 0:\n            digit += 2\n        quadkey += str(digit)\n    return quadkey\n</code></pre>"},{"location":"reference/core/constants/","title":"constants","text":""},{"location":"reference/core/constants/columns/","title":"columns","text":"<p>Reusable internal column names. Useful for referring to the the same column across multiple components.</p>"},{"location":"reference/core/constants/columns/#core.constants.columns.ColNames","title":"<code>ColNames</code>","text":"<p>Class that enumerates all the column names.</p> Source code in <code>multimno/core/constants/columns.py</code> <pre><code>class ColNames:\n    \"\"\"\n    Class that enumerates all the column names.\n    \"\"\"\n\n    user_id = \"user_id\"\n    partition_id = \"partition_id\"\n    timestamp = \"timestamp\"\n    mcc = \"mcc\"\n    mnc = \"mnc\"\n    plmn = \"plmn\"\n    cell_id = \"cell_id\"\n    latitude = \"latitude\"\n    longitude = \"longitude\"\n    error_flag = \"error_flag\"\n    transformation_flag = \"transformation_flag\"\n    affected_field = \"affected_field\"\n    domain = \"domain\"\n    is_last_event = \"is_last_event\"  # 0: initial, 1: final\n    # default values for domain col\n    domestic = 1\n    inbound = 2\n    outbound = 3\n\n    altitude = \"altitude\"\n    antenna_height = \"antenna_height\"\n    directionality = \"directionality\"\n    azimuth_angle = \"azimuth_angle\"\n    elevation_angle = \"elevation_angle\"\n    horizontal_beam_width = \"horizontal_beam_width\"\n    vertical_beam_width = \"vertical_beam_width\"\n    power = \"power\"\n    range = \"range\"\n    frequency = \"frequency\"\n    technology = \"technology\"\n    valid_date_start = \"valid_date_start\"\n    valid_date_end = \"valid_date_end\"\n    cell_type = \"cell_type\"\n\n    loc_error = \"loc_error\"\n    event_id = \"event_id\"\n\n    year = \"year\"\n    month = \"month\"\n    day = \"day\"\n    user_id_modulo = \"user_id_modulo\"\n\n    # for QA by column\n    variable = \"variable\"\n    type_of_error = \"type_of_error\"\n    type_of_transformation = \"type_of_transformation\"\n    value = \"value\"\n    result_timestamp = \"result_timestamp\"\n    data_period_start = \"data_period_start\"\n    data_period_end = \"data_period_end\"\n    field_name = \"field_name\"\n    initial_frequency = \"initial_frequency\"\n    final_frequency = \"final_frequency\"\n    date = \"date\"\n\n    # warnings\n    # log table\n    measure_definition = \"measure_definition\"\n    lookback_period = \"lookback_period\"\n    daily_value = \"daily_value\"\n    condition_value = \"condition_value\"\n    condition = \"condition\"\n    warning_text = \"warning_text\"\n    # for plots\n    type_of_qw = \"type_of_qw\"\n    average = \"average\"\n    UCL = \"UCL\"\n    LCL = \"LCL\"\n    title = \"title\"\n\n    # top frequent errors\n    error_value = \"error_value\"\n    error_count = \"error_count\"\n    accumulated_percentage = \"accumulated_percentage\"\n\n    # for grid generation\n    geometry = \"geometry\"\n    grid_id = \"grid_id\"\n    elevation = \"elevation\"\n    land_use = \"land_use\"\n    type_code = \"type_code\"\n    prior_probability = \"prior_probability\"\n    ple_coefficient = \"environment_ple_coefficient\"\n    quadkey = \"quadkey\"\n\n    # device activity statistics\n    event_cnt = \"event_cnt\"\n    unique_cell_cnt = \"unique_cell_cnt\"\n    unique_location_cnt = \"unique_location_cnt\"\n    sum_distance_m = \"sum_distance_m\"\n    unique_hour_cnt = \"unique_hour_cnt\"\n    mean_time_gap = \"mean_time_gap\"\n    stdev_time_gap = \"stdev_time_gap\"\n\n    # signal\n    signal_strength = \"signal_strength\"\n    distance_to_cell = \"distance_to_cell\"\n    distance_to_cell_3D = \"distance_to_cell_3D\"\n    joined_geometry = \"joined_geometry\"\n    path_loss_exponent = \"path_loss_exponent\"\n    azimuth_signal_strength_back_loss = \"azimuth_signal_strength_back_loss\"\n    elevation_signal_strength_back_loss = \"elevation_signal_strength_back_loss\"\n\n    # for cell footprint\n    signal_dominance = \"signal_dominance\"\n    group_id = \"group_id\"\n    cells = \"cells\"\n    group_size = \"group_size\"\n\n    # Nearby cells and cell overlap\n    overlapping_cell_ids = \"overlapping_cell_ids\"\n    cell_id_a = \"cell_id_a\"\n    cell_id_b = \"cell_id_b\"\n    distance = \"distance\"\n\n    # time segments\n    time_segment_id = \"time_segment_id\"\n    start_timestamp = \"start_timestamp\"\n    end_timestamp = \"end_timestamp\"\n    state = \"state\"\n    is_last = \"is_last\"\n\n    # for cell connection probability\n    cell_connection_probability = \"cell_connection_probability\"\n    posterior_probability = \"posterior_probability\"\n\n    # dps (daily permanence score)\n    dps = \"dps\"\n    stay_duration = \"stay_duration\"\n    time_slot_initial_time = \"time_slot_initial_time\"\n    time_slot_end_time = \"time_slot_end_time\"\n    id_type = \"id_type\"\n\n    # midterm permanence score\n    mps = \"mps\"\n    day_type = \"day_type\"\n    time_interval = \"time_interval\"\n    regularity_mean = \"regularity_mean\"\n    regularity_std = \"regularity_std\"\n\n    # longterm permanence score\n    lps = \"lps\"\n    total_frequency = \"total_frequency\"\n    frequency_mean = \"frequency_mean\"\n    frequency_std = \"frequency_std\"\n    start_date = \"start_date\"\n    end_date = \"end_date\"\n    season = \"season\"\n\n    # diaries\n    stay_type = \"stay_type\"\n    activity_type = \"activity_type\"\n    initial_timestamp = \"initial_timestamp\"\n    final_timestamp = \"final_timestamp\"\n\n    # present population\n    device_count = \"device_count\"\n    population = \"population\"\n\n    # zone to grid mapping\n    zone_id = \"zone_id\"\n    hierarchical_id = \"hierarchical_id\"\n    dataset_id = \"dataset_id\"\n\n    # for spatial data\n    category = \"category\"\n    zone_id = \"zone_id\"\n    level = \"level\"\n    parent_id = \"parent_id\"\n    iso2 = \"iso2\"\n    iso3 = \"iso3\"\n    name = \"name\"\n    dataset_id = \"dataset_id\"\n    hierarchical_id = \"hierarchical_id\"\n\n    # for usual environment labels\n    label = \"label\"\n    ue_label_rule = \"ue_label_rule\"\n    location_label_rule = \"location_label_rule\"\n\n    # for usual environment labeling quality metrics\n    labeling_quality_metric = \"metric\"\n    labeling_quality_count = \"count\"\n\n    # for usual environment aggregation\n    weighted_device_count = \"weighted_device_count\"\n    tile_weight = \"tile_weight\"\n    device_tile_weight = \"device_tile_weight\"\n\n    # for tourism stays\n    zone_weight = \"zone_weight\"\n    is_overnight = \"is_overnight\"\n    zone_weights_list = \"zone_weights_list\"\n    zone_ids_list = \"zone_ids_list\"\n\n    # internal migration\n    previous_zone = \"previous_zone\"\n    new_zone = \"new_zone\"\n    migration = \"migration\"\n    start_date_previous = \"start_date_previous\"\n    end_date_previous = \"end_date_previous\"\n    season_previous = \"season_previous\"\n    start_date_new = \"start_date_new\"\n    end_date_new = \"end_date_new\"\n    season_new = \"season_new\"\n\n    # internal migration quality metrics\n    previous_home_users = \"previous_home_users\"\n    new_home_users = \"new_home_users\"\n    common_home_users = \"common_home_users\"\n</code></pre>"},{"location":"reference/core/constants/conditions/","title":"conditions","text":""},{"location":"reference/core/constants/error_types/","title":"error_types","text":"<p>Transformations Error types module.</p>"},{"location":"reference/core/constants/error_types/#core.constants.error_types.ErrorTypes","title":"<code>ErrorTypes</code>","text":"<p>Class that enumerates the multiple error types of data transformations.</p> Source code in <code>multimno/core/constants/error_types.py</code> <pre><code>class ErrorTypes:\n    \"\"\"\n    Class that enumerates the multiple error types of data transformations.\n    \"\"\"\n\n    NO_ERROR = 0\n    NULL_VALUE = 1\n    OUT_OF_RANGE = 2\n    UNSUPPORTED_TYPE = 3\n    CANNOT_PARSE = 4\n    NO_MNO_INFO = 5\n    NO_LOCATION_INFO = 6\n    DUPLICATED = 7\n\n    # This shows the possible error types that can happen in syntactic event cleaning\n    # This is used for creating the quality metrics data object\n    event_syntactic_cleaning_possible_errors = [0, 1, 2, 3, 4, 5, 6, 7]\n</code></pre>"},{"location":"reference/core/constants/error_types/#core.constants.error_types.NetworkErrorType","title":"<code>NetworkErrorType</code>","text":"<p>Class that enumerates the multiple error types present in network topology data.</p> Source code in <code>multimno/core/constants/error_types.py</code> <pre><code>class NetworkErrorType:\n    \"\"\"\n    Class that enumerates the multiple error types present in network topology data.\n    \"\"\"\n\n    NO_ERROR = 0\n    NULL_VALUE = 1\n    OUT_OF_RANGE = 2\n    UNSUPPORTED_TYPE = 3\n    CANNOT_PARSE = 4\n    INITIAL_ROWS = 100\n    FINAL_ROWS = 101\n</code></pre>"},{"location":"reference/core/constants/error_types/#core.constants.error_types.SemanticErrorType","title":"<code>SemanticErrorType</code>","text":"<p>Class that enumerates the multiple error types associated to event semantic checks.</p> Source code in <code>multimno/core/constants/error_types.py</code> <pre><code>class SemanticErrorType:\n    \"\"\"\n    Class that enumerates the multiple error types associated to event semantic checks.\n    \"\"\"\n\n    NO_ERROR = 0\n    CELL_ID_NON_EXISTENT = 1\n    CELL_ID_NOT_VALID = 2\n    INCORRECT_EVENT_LOCATION = 3\n    SUSPICIOUS_EVENT_LOCATION = 4\n    DIFFERENT_LOCATION_DUPLICATE = 5\n</code></pre>"},{"location":"reference/core/constants/measure_definitions/","title":"measure_definitions","text":""},{"location":"reference/core/constants/network_default_thresholds/","title":"network_default_thresholds","text":"<p>Contains the default threshold values used by the Network Syntactic Quality Warnings</p>"},{"location":"reference/core/constants/period_names/","title":"period_names","text":"<p>List of names of the sub-daily periods/time intervals, sub-monthly periods/day types, and sub-yearly/seasons used in the Permanence Score components.</p>"},{"location":"reference/core/constants/reserved_dataset_ids/","title":"reserved_dataset_ids","text":"<p>Reserved zoning dataset IDs. At this moment they refer to the INSPIRE 100m and 1km grids, which receive a special treatment as their geometries are not needed in order to map the reference grid tiles (the INSPIRE 100m grid) to them.</p>"},{"location":"reference/core/constants/reserved_dataset_ids/#core.constants.reserved_dataset_ids.ReservedDatasetIDs","title":"<code>ReservedDatasetIDs</code>","text":"<p>Class that enumerates reserved dataset IDs.</p> Source code in <code>multimno/core/constants/reserved_dataset_ids.py</code> <pre><code>class ReservedDatasetIDs:\n    \"\"\"\n    Class that enumerates reserved dataset IDs.\n    \"\"\"\n\n    INSPIRE_1km = \"INSPIRE_1km\"\n    INSPIRE_100m = \"INSPIRE_100m\"\n\n    def __contains__(self, value):\n        if self.INSPIRE_100m == value:\n            return True\n        if self.INSPIRE_1km == value:\n            return True\n        return False\n</code></pre>"},{"location":"reference/core/constants/semantic_qw_default_thresholds/","title":"semantic_qw_default_thresholds","text":"<p>Contains the default threshold values used by the Event Device Semantic Quality Warnings</p>"},{"location":"reference/core/constants/spatial/","title":"spatial","text":""},{"location":"reference/core/constants/transformations/","title":"transformations","text":"<p>Data transformations types modukle</p>"},{"location":"reference/core/constants/transformations/#core.constants.transformations.Transformations","title":"<code>Transformations</code>","text":"<p>Class that enumerates the multiple data transformations types.</p> Source code in <code>multimno/core/constants/transformations.py</code> <pre><code>class Transformations:\n    \"\"\"\n    Class that enumerates the multiple data transformations types.\n    \"\"\"\n\n    converted_timestamp = 1\n    other_conversion = 2\n    no_transformation = 9\n\n    event_syntactic_cleaning_possible_transformations = [1, 2, 9]\n</code></pre>"},{"location":"reference/core/constants/warnings/","title":"warnings","text":""},{"location":"reference/core/data_objects/","title":"data_objects","text":""},{"location":"reference/core/data_objects/data_object/","title":"data_object","text":"<p>Module that defines the data object abstract classes</p>"},{"location":"reference/core/data_objects/data_object/#core.data_objects.data_object.DataObject","title":"<code>DataObject</code>","text":"<p>Abstract class that models a DataObject. It defines its data schema including the attributes that compose it.</p> Source code in <code>multimno/core/data_objects/data_object.py</code> <pre><code>class DataObject(metaclass=ABCMeta):\n    \"\"\"\n    Abstract class that models a DataObject. It defines its data schema including the attributes that compose it.\n    \"\"\"\n\n    ID: str = None\n    SCHEMA: StructType = None\n\n    def __init__(self, spark: SparkSession) -&gt; None:\n        self.df: DataFrame = None\n        self.spark: SparkSession = spark\n        self.interface: IOInterface = None\n\n    def read(self, *args, **kwargs):\n        \"\"\"\n        Method that performs the read operation of the data object dataframe through an IOInterface.\n        \"\"\"\n        self.df = self.interface.read_from_interface(*args, **kwargs)\n        return self\n\n    def write(self, *args, **kwargs):\n        \"\"\"\n        Method that performs the write operation of the data object dataframe through an IOInterface.\n        \"\"\"\n        self.interface.write_from_interface(self.df, *args, **kwargs)\n\n    def cast_to_schema(self):\n        columns = {field.name: F.col(field.name).cast(field.dataType) for field in self.SCHEMA.fields}\n        self.df = self.df.withColumns(columns)\n</code></pre>"},{"location":"reference/core/data_objects/data_object/#core.data_objects.data_object.DataObject.read","title":"<code>read(*args, **kwargs)</code>","text":"<p>Method that performs the read operation of the data object dataframe through an IOInterface.</p> Source code in <code>multimno/core/data_objects/data_object.py</code> <pre><code>def read(self, *args, **kwargs):\n    \"\"\"\n    Method that performs the read operation of the data object dataframe through an IOInterface.\n    \"\"\"\n    self.df = self.interface.read_from_interface(*args, **kwargs)\n    return self\n</code></pre>"},{"location":"reference/core/data_objects/data_object/#core.data_objects.data_object.DataObject.write","title":"<code>write(*args, **kwargs)</code>","text":"<p>Method that performs the write operation of the data object dataframe through an IOInterface.</p> Source code in <code>multimno/core/data_objects/data_object.py</code> <pre><code>def write(self, *args, **kwargs):\n    \"\"\"\n    Method that performs the write operation of the data object dataframe through an IOInterface.\n    \"\"\"\n    self.interface.write_from_interface(self.df, *args, **kwargs)\n</code></pre>"},{"location":"reference/core/data_objects/data_object/#core.data_objects.data_object.GeoParquetDataObject","title":"<code>GeoParquetDataObject</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models a DataObject that will use a ParquetInterface for IO operations. It inherits the PathDataObject abstract class.</p> Source code in <code>multimno/core/data_objects/data_object.py</code> <pre><code>class GeoParquetDataObject(PathDataObject):\n    \"\"\"\n    Class that models a DataObject that will use a ParquetInterface for IO operations.\n    It inherits the PathDataObject abstract class.\n    \"\"\"\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        default_path: str,\n        default_partition_columns: List[str] = None,\n        default_mode: str = SPARK_WRITING_MODES.APPEND,\n        default_crs: int = INSPIRE_GRID_EPSG,\n    ) -&gt; None:\n        super().__init__(spark, default_path, default_partition_columns, default_mode)\n        self.interface: PathInterface = GeoParquetInterface()\n        self.default_crs = default_crs\n\n    def read(self):\n        self.df = self.interface.read_from_interface(self.spark, self.default_path, self.SCHEMA)\n        self.df = self.df.withColumn(ColNames.geometry, STF.ST_SetSRID((ColNames.geometry), F.lit(self.default_crs)))\n\n        return self\n</code></pre>"},{"location":"reference/core/data_objects/data_object/#core.data_objects.data_object.ParquetDataObject","title":"<code>ParquetDataObject</code>","text":"<p>               Bases: <code>PathDataObject</code></p> <p>Class that models a DataObject that will use a ParquetInterface for IO operations. It inherits the PathDataObject abstract class.</p> Source code in <code>multimno/core/data_objects/data_object.py</code> <pre><code>class ParquetDataObject(PathDataObject):\n    \"\"\"\n    Class that models a DataObject that will use a ParquetInterface for IO operations.\n    It inherits the PathDataObject abstract class.\n    \"\"\"\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        default_path: str,\n        default_partition_columns: List[str] = None,\n        default_mode: str = SPARK_WRITING_MODES.APPEND,\n    ) -&gt; None:\n        super().__init__(spark, default_path, default_partition_columns, default_mode)\n        self.interface: PathInterface = ParquetInterface()\n</code></pre>"},{"location":"reference/core/data_objects/data_object/#core.data_objects.data_object.PathDataObject","title":"<code>PathDataObject</code>","text":"<p>               Bases: <code>DataObject</code></p> <p>Abstract Class that models DataObjects that will use a PathInterface for IO operations. It inherits the DataObject abstract class.</p> Source code in <code>multimno/core/data_objects/data_object.py</code> <pre><code>class PathDataObject(DataObject, metaclass=ABCMeta):\n    \"\"\"Abstract Class that models DataObjects that will use a PathInterface for IO operations.\n    It inherits the DataObject abstract class.\n    \"\"\"\n\n    ID = ...\n    SCHEMA = ...\n    PARTITION_COLUMNS = ...\n\n    def __init__(\n        self,\n        spark: SparkSession,\n        default_path: str,\n        default_partition_columns: List[str] = None,\n        default_mode: str = SPARK_WRITING_MODES.APPEND,\n    ) -&gt; None:\n        super().__init__(spark)\n        self.interface: PathInterface = None\n        self.default_path: str = default_path\n        if default_partition_columns is None:\n            default_partition_columns = self.PARTITION_COLUMNS\n        self.default_partition_columns: List[str] = default_partition_columns\n        self.default_mode: str = default_mode\n\n    def read(self, *args, path: str = None, **kwargs):\n        if path is None:\n            path = self.default_path\n        self.df = self.interface.read_from_interface(self.spark, path, self.SCHEMA)\n\n        return self\n\n    def write(self, *args, path: str = None, partition_columns: list[str] = None, mode: str = None, **kwargs):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.default_partition_columns\n        if mode is None:\n            mode = self.default_mode\n\n        self.interface.write_from_interface(self.df, path=path, partition_columns=partition_columns, mode=mode)\n\n    def get_size(self) -&gt; int:\n        \"\"\"\n        Returns the size of the data object in bytes.\n        \"\"\"\n        files = self.df.inputFiles()\n\n        if len(files) == 0:\n            return 0\n\n        conf = self.spark._jsc.hadoopConfiguration()\n        # need to get proper URI prefix for the file system\n        uri = self.spark._jvm.java.net.URI.create(files[0])\n        fs = self.spark._jvm.org.apache.hadoop.fs.FileSystem.get(uri, conf)\n        total_size = 0\n\n        for file in files:\n            total_size += fs.getFileStatus(self.spark._jvm.org.apache.hadoop.fs.Path(file)).getLen()\n\n        return total_size\n\n    def get_num_files(self) -&gt; int:\n        \"\"\"\n        Returns the number of files of the data object.\n        \"\"\"\n        return len(self.df.inputFiles())\n\n    def get_top_rows(self, n: int, truncate: int = 20) -&gt; str:\n        \"\"\"\n        Returns string with top n rows. Same as df.show.\n        \"\"\"\n        return self.df._jdf.showString(n, truncate, False)\n</code></pre>"},{"location":"reference/core/data_objects/data_object/#core.data_objects.data_object.PathDataObject.get_num_files","title":"<code>get_num_files()</code>","text":"<p>Returns the number of files of the data object.</p> Source code in <code>multimno/core/data_objects/data_object.py</code> <pre><code>def get_num_files(self) -&gt; int:\n    \"\"\"\n    Returns the number of files of the data object.\n    \"\"\"\n    return len(self.df.inputFiles())\n</code></pre>"},{"location":"reference/core/data_objects/data_object/#core.data_objects.data_object.PathDataObject.get_size","title":"<code>get_size()</code>","text":"<p>Returns the size of the data object in bytes.</p> Source code in <code>multimno/core/data_objects/data_object.py</code> <pre><code>def get_size(self) -&gt; int:\n    \"\"\"\n    Returns the size of the data object in bytes.\n    \"\"\"\n    files = self.df.inputFiles()\n\n    if len(files) == 0:\n        return 0\n\n    conf = self.spark._jsc.hadoopConfiguration()\n    # need to get proper URI prefix for the file system\n    uri = self.spark._jvm.java.net.URI.create(files[0])\n    fs = self.spark._jvm.org.apache.hadoop.fs.FileSystem.get(uri, conf)\n    total_size = 0\n\n    for file in files:\n        total_size += fs.getFileStatus(self.spark._jvm.org.apache.hadoop.fs.Path(file)).getLen()\n\n    return total_size\n</code></pre>"},{"location":"reference/core/data_objects/data_object/#core.data_objects.data_object.PathDataObject.get_top_rows","title":"<code>get_top_rows(n, truncate=20)</code>","text":"<p>Returns string with top n rows. Same as df.show.</p> Source code in <code>multimno/core/data_objects/data_object.py</code> <pre><code>def get_top_rows(self, n: int, truncate: int = 20) -&gt; str:\n    \"\"\"\n    Returns string with top n rows. Same as df.show.\n    \"\"\"\n    return self.df._jdf.showString(n, truncate, False)\n</code></pre>"},{"location":"reference/core/data_objects/bronze/","title":"bronze","text":""},{"location":"reference/core/data_objects/bronze/bronze_admin_units_data_object/","title":"bronze_admin_units_data_object","text":""},{"location":"reference/core/data_objects/bronze/bronze_admin_units_data_object/#core.data_objects.bronze.bronze_admin_units_data_object.BronzeAdminUnitsDataObject","title":"<code>BronzeAdminUnitsDataObject</code>","text":"<p>               Bases: <code>GeoParquetDataObject</code></p> <p>Class that models country polygons spatial data.</p> Source code in <code>multimno/core/data_objects/bronze/bronze_admin_units_data_object.py</code> <pre><code>class BronzeAdminUnitsDataObject(GeoParquetDataObject):\n    \"\"\"\n    Class that models country polygons spatial data.\n    \"\"\"\n\n    ID = \"AdminUnitsDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.zone_id, StringType(), nullable=False),\n            StructField(ColNames.name, StringType(), nullable=True),\n            StructField(ColNames.level, ShortType(), nullable=True),\n            StructField(ColNames.parent_id, StringType(), nullable=True),\n            StructField(ColNames.iso2, StringType(), nullable=False),\n            StructField(ColNames.geometry, GeometryType(), nullable=False),\n            # partition columns\n            StructField(ColNames.dataset_id, StringType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.dataset_id, ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/bronze/bronze_countries_data_object/","title":"bronze_countries_data_object","text":""},{"location":"reference/core/data_objects/bronze/bronze_countries_data_object/#core.data_objects.bronze.bronze_countries_data_object.BronzeCountriesDataObject","title":"<code>BronzeCountriesDataObject</code>","text":"<p>               Bases: <code>GeoParquetDataObject</code></p> <p>Class that models country polygons spatial data.</p> Source code in <code>multimno/core/data_objects/bronze/bronze_countries_data_object.py</code> <pre><code>class BronzeCountriesDataObject(GeoParquetDataObject):\n    \"\"\"\n    Class that models country polygons spatial data.\n    \"\"\"\n\n    ID = \"BronzeCountriesDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.iso2, StringType(), nullable=False),\n            StructField(ColNames.name, StringType(), nullable=False),\n            StructField(ColNames.geometry, GeometryType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = []\n</code></pre>"},{"location":"reference/core/data_objects/bronze/bronze_event_data_object/","title":"bronze_event_data_object","text":"<p>Bronze MNO Event data module</p>"},{"location":"reference/core/data_objects/bronze/bronze_event_data_object/#core.data_objects.bronze.bronze_event_data_object.BronzeEventDataObject","title":"<code>BronzeEventDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the RAW MNO Event data.</p> Source code in <code>multimno/core/data_objects/bronze/bronze_event_data_object.py</code> <pre><code>class BronzeEventDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models the RAW MNO Event data.\n    \"\"\"\n\n    ID = \"BronzeEventDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=True),\n            StructField(ColNames.timestamp, StringType(), nullable=True),\n            StructField(ColNames.mcc, IntegerType(), nullable=True),\n            StructField(ColNames.mnc, StringType(), nullable=True),\n            StructField(ColNames.plmn, IntegerType(), nullable=True),\n            StructField(ColNames.cell_id, StringType(), nullable=True),\n            StructField(ColNames.latitude, FloatType(), nullable=True),\n            StructField(ColNames.longitude, FloatType(), nullable=True),\n            StructField(ColNames.loc_error, FloatType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=True),\n            StructField(ColNames.month, ByteType(), nullable=True),\n            StructField(ColNames.day, ByteType(), nullable=True),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/bronze/bronze_geographic_zones_data_object/","title":"bronze_geographic_zones_data_object","text":""},{"location":"reference/core/data_objects/bronze/bronze_geographic_zones_data_object/#core.data_objects.bronze.bronze_geographic_zones_data_object.BronzeGeographicZonesDataObject","title":"<code>BronzeGeographicZonesDataObject</code>","text":"<p>               Bases: <code>GeoParquetDataObject</code></p> <p>Class that models country polygons spatial data.</p> Source code in <code>multimno/core/data_objects/bronze/bronze_geographic_zones_data_object.py</code> <pre><code>class BronzeGeographicZonesDataObject(GeoParquetDataObject):\n    \"\"\"\n    Class that models country polygons spatial data.\n    \"\"\"\n\n    ID = \"GeographicZonesDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.zone_id, StringType(), nullable=False),\n            StructField(ColNames.name, StringType(), nullable=True),\n            StructField(ColNames.level, ShortType(), nullable=True),\n            StructField(ColNames.parent_id, StringType(), nullable=True),\n            StructField(ColNames.iso2, StringType(), nullable=False),\n            StructField(ColNames.geometry, GeometryType(), nullable=False),\n            # partition columns\n            StructField(ColNames.dataset_id, StringType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.dataset_id, ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/bronze/bronze_holiday_calendar_data_object/","title":"bronze_holiday_calendar_data_object","text":"<p>Bronze Calendar Information Data Object Contains the national holidays of each country</p>"},{"location":"reference/core/data_objects/bronze/bronze_holiday_calendar_data_object/#core.data_objects.bronze.bronze_holiday_calendar_data_object.BronzeHolidayCalendarDataObject","title":"<code>BronzeHolidayCalendarDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the Calendar information regarding national holidays and regular days.</p> Source code in <code>multimno/core/data_objects/bronze/bronze_holiday_calendar_data_object.py</code> <pre><code>class BronzeHolidayCalendarDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models the Calendar information regarding national holidays\n    and regular days.\n    \"\"\"\n\n    ID = \"BronzeHolidayCalendarInfoDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.iso2, StringType(), nullable=False),\n            StructField(ColNames.date, DateType(), nullable=False),\n            StructField(ColNames.name, StringType(), nullable=False),\n        ]\n    )\n    PARTITION_COLUMNS = []\n</code></pre>"},{"location":"reference/core/data_objects/bronze/bronze_landuse_data_object/","title":"bronze_landuse_data_object","text":""},{"location":"reference/core/data_objects/bronze/bronze_landuse_data_object/#core.data_objects.bronze.bronze_landuse_data_object.BronzeLanduseDataObject","title":"<code>BronzeLanduseDataObject</code>","text":"<p>               Bases: <code>GeoParquetDataObject</code></p> <p>Class that models landuse spatial data.</p> Source code in <code>multimno/core/data_objects/bronze/bronze_landuse_data_object.py</code> <pre><code>class BronzeLanduseDataObject(GeoParquetDataObject):\n    \"\"\"\n    Class that models landuse spatial data.\n    \"\"\"\n\n    ID = \"BronzeLanduseDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.category, StringType(), nullable=False),\n            StructField(ColNames.geometry, GeometryType(), nullable=False),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.quadkey, StringType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day, ColNames.quadkey]\n</code></pre>"},{"location":"reference/core/data_objects/bronze/bronze_network_physical_data_object/","title":"bronze_network_physical_data_object","text":"<p>Bronze MNO Network Topology Data module</p> <p>Currently, only considers the \"Cell Locations with Physical Properties\" type</p>"},{"location":"reference/core/data_objects/bronze/bronze_network_physical_data_object/#core.data_objects.bronze.bronze_network_physical_data_object.BronzeNetworkDataObject","title":"<code>BronzeNetworkDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the RAW MNO Network Topology Data, based on the physical properties of the cells.</p> Source code in <code>multimno/core/data_objects/bronze/bronze_network_physical_data_object.py</code> <pre><code>class BronzeNetworkDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models the RAW MNO Network Topology Data, based on the physical\n    properties of the cells.\n    \"\"\"\n\n    ID = \"BronzeNetworkDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.cell_id, StringType(), nullable=True),\n            StructField(ColNames.latitude, FloatType(), nullable=True),\n            StructField(ColNames.longitude, FloatType(), nullable=True),\n            StructField(ColNames.altitude, FloatType(), nullable=True),\n            StructField(ColNames.antenna_height, FloatType(), nullable=True),\n            StructField(ColNames.directionality, IntegerType(), nullable=True),\n            StructField(ColNames.azimuth_angle, FloatType(), nullable=True),\n            StructField(ColNames.elevation_angle, FloatType(), nullable=True),\n            StructField(ColNames.horizontal_beam_width, FloatType(), nullable=True),\n            StructField(ColNames.vertical_beam_width, FloatType(), nullable=True),\n            StructField(ColNames.power, FloatType(), nullable=True),\n            StructField(ColNames.range, FloatType(), nullable=True),\n            StructField(ColNames.frequency, IntegerType(), nullable=True),\n            StructField(ColNames.technology, StringType(), nullable=True),\n            StructField(ColNames.valid_date_start, StringType(), nullable=True),\n            StructField(ColNames.valid_date_end, StringType(), nullable=True),\n            StructField(ColNames.cell_type, StringType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=True),\n            StructField(ColNames.month, ByteType(), nullable=True),\n            StructField(ColNames.day, ByteType(), nullable=True),\n        ]\n    )\n    MANDATORY_COLUMNS = [ColNames.cell_id, ColNames.latitude, ColNames.longitude]\n\n    OPTIONAL_COLUMNS = [\n        ColNames.altitude,\n        ColNames.antenna_height,\n        ColNames.directionality,\n        ColNames.azimuth_angle,\n        ColNames.elevation_angle,\n        ColNames.horizontal_beam_width,\n        ColNames.vertical_beam_width,\n        ColNames.power,\n        ColNames.range,\n        ColNames.frequency,\n        ColNames.technology,\n        ColNames.valid_date_start,\n        ColNames.valid_date_end,\n        ColNames.cell_type,\n    ]\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/bronze/bronze_synthetic_diaries_data_object/","title":"bronze_synthetic_diaries_data_object","text":"<p>Bronze Synthetic Diaries Data module</p>"},{"location":"reference/core/data_objects/bronze/bronze_synthetic_diaries_data_object/#core.data_objects.bronze.bronze_synthetic_diaries_data_object.BronzeSyntheticDiariesDataObject","title":"<code>BronzeSyntheticDiariesDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models synthetically-generated agents activity-trip diaries.</p> <pre><code>    ''''''\n</code></pre> Source code in <code>multimno/core/data_objects/bronze/bronze_synthetic_diaries_data_object.py</code> <pre><code>class BronzeSyntheticDiariesDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models synthetically-generated agents activity-trip diaries.\n\n            ''''''\n\n    \"\"\"\n\n    ID = \"BronzeSyntheticDiariesDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.activity_type, StringType(), nullable=True),\n            StructField(ColNames.stay_type, StringType(), nullable=True),\n            StructField(ColNames.longitude, FloatType(), nullable=True),\n            StructField(ColNames.latitude, FloatType(), nullable=True),\n            StructField(ColNames.initial_timestamp, TimestampType(), nullable=True),\n            StructField(ColNames.final_timestamp, TimestampType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/bronze/bronze_transportation_data_object/","title":"bronze_transportation_data_object","text":""},{"location":"reference/core/data_objects/bronze/bronze_transportation_data_object/#core.data_objects.bronze.bronze_transportation_data_object.BronzeTransportationDataObject","title":"<code>BronzeTransportationDataObject</code>","text":"<p>               Bases: <code>GeoParquetDataObject</code></p> <p>Class that models the transportation network spatial data.</p> Source code in <code>multimno/core/data_objects/bronze/bronze_transportation_data_object.py</code> <pre><code>class BronzeTransportationDataObject(GeoParquetDataObject):\n    \"\"\"\n    Class that models the transportation network spatial data.\n    \"\"\"\n\n    ID = \"BronzeTransportationDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.category, StringType(), nullable=False),\n            StructField(ColNames.geometry, GeometryType(), nullable=False),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.quadkey, StringType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day, ColNames.quadkey]\n</code></pre>"},{"location":"reference/core/data_objects/landing/","title":"landing","text":""},{"location":"reference/core/data_objects/landing/landing_geoparquet_data_object/","title":"landing_geoparquet_data_object","text":""},{"location":"reference/core/data_objects/landing/landing_geoparquet_data_object/#core.data_objects.landing.landing_geoparquet_data_object.LandingGeoParquetDataObject","title":"<code>LandingGeoParquetDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models input geospatial data.</p> Source code in <code>multimno/core/data_objects/landing/landing_geoparquet_data_object.py</code> <pre><code>class LandingGeoParquetDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models input geospatial data.\n    \"\"\"\n\n    ID = \"LandingGeoParquetDO\"\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: List[str] = None) -&gt; None:\n\n        super().__init__(spark, default_path)\n        self.interface: GeoParquetInterface = GeoParquetInterface()\n        self.partition_columns = partition_columns\n\n    def read(self):\n\n        self.df = self.interface.read_from_interface(self.spark, self.default_path, self.SCHEMA)\n\n    def write(self, path: str = None, partition_columns: List[str] = None):\n\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/landing/landing_http_geojson_data_object/","title":"landing_http_geojson_data_object","text":""},{"location":"reference/core/data_objects/landing/landing_http_geojson_data_object/#core.data_objects.landing.landing_http_geojson_data_object.LandingHttpGeoJsonDataObject","title":"<code>LandingHttpGeoJsonDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models input geospatial data in geojson format.</p> Source code in <code>multimno/core/data_objects/landing/landing_http_geojson_data_object.py</code> <pre><code>class LandingHttpGeoJsonDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models input geospatial data in geojson format.\n    \"\"\"\n\n    ID = \"LandingGeoJsonDO\"\n\n    def __init__(self, spark: SparkSession, url: str, timeout: int, max_retries: int) -&gt; None:\n\n        super().__init__(spark, url)\n        self.interface: HttpGeoJsonInterface = HttpGeoJsonInterface()\n        self.default_path = url\n        self.timeout = timeout\n        self.max_retries = max_retries\n\n    def read(self):\n\n        self.df = self.interface.read_from_interface(self.spark, self.default_path, self.timeout, self.max_retries)\n</code></pre>"},{"location":"reference/core/data_objects/silver/","title":"silver","text":""},{"location":"reference/core/data_objects/silver/event_cache_data_object/","title":"event_cache_data_object","text":"<p>Silver MNO Event data module with flags computed in Semantic Checks</p>"},{"location":"reference/core/data_objects/silver/event_cache_data_object/#core.data_objects.silver.event_cache_data_object.EventCacheDataObject","title":"<code>EventCacheDataObject</code>","text":"<p>               Bases: <code>SilverEventFlaggedDataObject</code></p> <p>Class that models the cleaned MNO Event data, with flags computed in the semantic checks module.</p> Source code in <code>multimno/core/data_objects/silver/event_cache_data_object.py</code> <pre><code>class EventCacheDataObject(SilverEventFlaggedDataObject):\n    \"\"\"\n    Class that models the cleaned MNO Event data, with flags computed\n    in the semantic checks module.\n    \"\"\"\n\n    ID = \"EventCacheDO\"\n\n    # SCHEMA and partition columns depend on the semantic event data\n    SCHEMA = StructType(\n        SilverEventFlaggedDataObject.SCHEMA.fields\n        + [StructField(ColNames.is_last_event, BooleanType(), nullable=False)]\n    )\n\n    PARTITION_COLUMNS = SilverEventFlaggedDataObject.PARTITION_COLUMNS + [ColNames.is_last_event]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_aggregated_usual_environments_data_object/","title":"silver_aggregated_usual_environments_data_object","text":"<p>Silver Aggregated Usual Environments data object module</p>"},{"location":"reference/core/data_objects/silver/silver_aggregated_usual_environments_data_object/#core.data_objects.silver.silver_aggregated_usual_environments_data_object.SilverAggregatedUsualEnvironmentsDataObject","title":"<code>SilverAggregatedUsualEnvironmentsDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the Aggregated Usual Environment data object.</p> Source code in <code>multimno/core/data_objects/silver/silver_aggregated_usual_environments_data_object.py</code> <pre><code>class SilverAggregatedUsualEnvironmentsDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models the Aggregated Usual Environment data object.\n    \"\"\"\n\n    ID = \"SilverAggregatedUsualEnvironmentsDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.grid_id, LongType(), nullable=False),\n            StructField(ColNames.weighted_device_count, FloatType(), nullable=False),\n            # partition columns\n            StructField(ColNames.label, StringType(), nullable=False),\n            StructField(ColNames.start_date, DateType(), nullable=False),\n            StructField(ColNames.end_date, DateType(), nullable=False),\n            StructField(ColNames.season, StringType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.label, ColNames.start_date, ColNames.end_date, ColNames.season]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_aggregated_usual_environments_zones_data_object/","title":"silver_aggregated_usual_environments_zones_data_object","text":"<p>Silver usual environments estimatation per zone data object</p>"},{"location":"reference/core/data_objects/silver/silver_aggregated_usual_environments_zones_data_object/#core.data_objects.silver.silver_aggregated_usual_environments_zones_data_object.SilverAggregatedUsualEnvironmentsZonesDataObject","title":"<code>SilverAggregatedUsualEnvironmentsZonesDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Estimation of the population present at a given time at the level of some zoning system.</p> Source code in <code>multimno/core/data_objects/silver/silver_aggregated_usual_environments_zones_data_object.py</code> <pre><code>class SilverAggregatedUsualEnvironmentsZonesDataObject(ParquetDataObject):\n    \"\"\"\n    Estimation of the population present at a given time at the level of some zoning system.\n    \"\"\"\n\n    ID = \"SilverAggregatedUsualEnvironmentsZonesDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.zone_id, StringType(), nullable=False),\n            StructField(ColNames.weighted_device_count, FloatType(), nullable=False),\n            # partition columns\n            StructField(ColNames.dataset_id, StringType(), nullable=False),\n            StructField(ColNames.level, ByteType(), nullable=False),\n            StructField(ColNames.label, StringType(), nullable=False),\n            StructField(ColNames.start_date, DateType(), nullable=False),\n            StructField(ColNames.end_date, DateType(), nullable=False),\n            StructField(ColNames.season, StringType(), nullable=False),\n        ]\n    )\n\n    VALUE_COLUMNS = [ColNames.weighted_device_count]\n\n    AGGREGATION_COLUMNS = [\n        ColNames.zone_id,\n        ColNames.dataset_id,\n        ColNames.label,\n        ColNames.level,\n        ColNames.start_date,\n        ColNames.end_date,\n        ColNames.season,\n    ]\n\n    PARTITION_COLUMNS = [\n        ColNames.dataset_id,\n        ColNames.level,\n        ColNames.label,\n        ColNames.start_date,\n        ColNames.end_date,\n        ColNames.season,\n    ]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_cell_connection_probabilities_data_object/","title":"silver_cell_connection_probabilities_data_object","text":"<p>Cell connection probabilities.</p>"},{"location":"reference/core/data_objects/silver/silver_cell_connection_probabilities_data_object/#core.data_objects.silver.silver_cell_connection_probabilities_data_object.SilverCellConnectionProbabilitiesDataObject","title":"<code>SilverCellConnectionProbabilitiesDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> Source code in <code>multimno/core/data_objects/silver/silver_cell_connection_probabilities_data_object.py</code> <pre><code>class SilverCellConnectionProbabilitiesDataObject(ParquetDataObject):\n    \"\"\" \"\"\"\n\n    ID = \"SilverCellConnectionProbabilitiesDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.cell_id, StringType(), nullable=True),\n            StructField(ColNames.grid_id, LongType(), nullable=True),\n            # StructField(ColNames.valid_date_start, DateType(), nullable=True),\n            # StructField(ColNames.valid_date_end, DateType(), nullable=True),\n            StructField(ColNames.cell_connection_probability, FloatType(), nullable=True),\n            StructField(ColNames.posterior_probability, FloatType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=True),\n            StructField(ColNames.month, ByteType(), nullable=True),\n            StructField(ColNames.day, ByteType(), nullable=True),\n        ]\n    )\n\n    MANDATORY_COLUMNS = [\n        ColNames.cell_id,\n        ColNames.grid_id,\n        ColNames.valid_date_start,\n        ColNames.valid_date_end,\n        ColNames.cell_connection_probability,\n        ColNames.posterior_probability,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    ]\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_cell_distance_data_object/","title":"silver_cell_distance_data_object","text":""},{"location":"reference/core/data_objects/silver/silver_cell_distance_data_object/#core.data_objects.silver.silver_cell_distance_data_object.SilverCellDistanceDataObject","title":"<code>SilverCellDistanceDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Data Object for distances between cells. Identifies for each (cell, date) the distance to another cell.</p> Source code in <code>multimno/core/data_objects/silver/silver_cell_distance_data_object.py</code> <pre><code>class SilverCellDistanceDataObject(ParquetDataObject):\n    \"\"\"\n    Data Object for distances between cells.\n    Identifies for each (cell, date) the distance to another cell.\n    \"\"\"\n\n    ID = \"SilverCellDistanceDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.cell_id_a, StringType(), nullable=True),\n            StructField(ColNames.cell_id_b, StringType(), nullable=True),\n            StructField(ColNames.distance, FloatType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=True),\n            StructField(ColNames.month, ByteType(), nullable=True),\n            StructField(ColNames.day, ByteType(), nullable=True),\n        ]\n    )\n\n    MANDATORY_COLUMNS = [\n        ColNames.cell_id_a,\n        ColNames.cell_id_b,\n        ColNames.distance,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    ]\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_cell_footprint_data_object/","title":"silver_cell_footprint_data_object","text":""},{"location":"reference/core/data_objects/silver/silver_cell_footprint_data_object/#core.data_objects.silver.silver_cell_footprint_data_object.SilverCellFootprintDataObject","title":"<code>SilverCellFootprintDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> Source code in <code>multimno/core/data_objects/silver/silver_cell_footprint_data_object.py</code> <pre><code>class SilverCellFootprintDataObject(ParquetDataObject):\n    \"\"\" \"\"\"\n\n    ID = \"SilverCellFootprintDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.cell_id, StringType(), nullable=True),\n            StructField(ColNames.grid_id, LongType(), nullable=True),\n            # StructField(ColNames.valid_date_start, DateType(), nullable=True),\n            # StructField(ColNames.valid_date_end, DateType(), nullable=True),\n            StructField(ColNames.signal_dominance, FloatType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=True),\n            StructField(ColNames.month, ByteType(), nullable=True),\n            StructField(ColNames.day, ByteType(), nullable=True),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_cell_intersection_groups_data_object/","title":"silver_cell_intersection_groups_data_object","text":""},{"location":"reference/core/data_objects/silver/silver_cell_intersection_groups_data_object/#core.data_objects.silver.silver_cell_intersection_groups_data_object.SilverCellIntersectionGroupsDataObject","title":"<code>SilverCellIntersectionGroupsDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Data Object for cell intersection groups. Identifies for each (cell, date) the list of other cells that are considered as intersecting (having sufficiently overlapping coverage area).</p> Source code in <code>multimno/core/data_objects/silver/silver_cell_intersection_groups_data_object.py</code> <pre><code>class SilverCellIntersectionGroupsDataObject(ParquetDataObject):\n    \"\"\"\n    Data Object for cell intersection groups.\n    Identifies for each (cell, date) the list of other cells\n    that are considered as intersecting (having sufficiently overlapping coverage area).\n    \"\"\"\n\n    ID = \"SilverCellIntersectionGroupsDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.cell_id, StringType(), nullable=True),\n            StructField(ColNames.overlapping_cell_ids, ArrayType(StringType()), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=True),\n            StructField(ColNames.month, ByteType(), nullable=True),\n            StructField(ColNames.day, ByteType(), nullable=True),\n        ]\n    )\n\n    MANDATORY_COLUMNS = [\n        ColNames.cell_id,\n        ColNames.overlapping_cell_ids,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    ]\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_daily_permanence_score_data_object/","title":"silver_daily_permanence_score_data_object","text":"<p>Silver Daily Permanence Score data module</p>"},{"location":"reference/core/data_objects/silver/silver_daily_permanence_score_data_object/#core.data_objects.silver.silver_daily_permanence_score_data_object.SilverDailyPermanenceScoreDataObject","title":"<code>SilverDailyPermanenceScoreDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the Daily Permanence Score data.</p> Source code in <code>multimno/core/data_objects/silver/silver_daily_permanence_score_data_object.py</code> <pre><code>class SilverDailyPermanenceScoreDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models the Daily Permanence Score data.\n    \"\"\"\n\n    ID = \"SilverDailyPermanenceScoreDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.time_slot_initial_time, TimestampType(), nullable=False),\n            StructField(ColNames.time_slot_end_time, TimestampType(), nullable=False),\n            StructField(ColNames.dps, ArrayType(LongType()), nullable=False),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.user_id_modulo, IntegerType(), nullable=False),\n            StructField(ColNames.id_type, StringType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n        ColNames.user_id_modulo,\n        ColNames.id_type,\n    ]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_device_activity_statistics/","title":"silver_device_activity_statistics","text":""},{"location":"reference/core/data_objects/silver/silver_device_activity_statistics/#core.data_objects.silver.silver_device_activity_statistics.SilverDeviceActivityStatistics","title":"<code>SilverDeviceActivityStatistics</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the cleaned MNO Event data.</p> Source code in <code>multimno/core/data_objects/silver/silver_device_activity_statistics.py</code> <pre><code>class SilverDeviceActivityStatistics(ParquetDataObject):\n    \"\"\"\n    Class that models the cleaned MNO Event data.\n    \"\"\"\n\n    ID = \"SilverDeviceActivityStatisticsDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.event_cnt, IntegerType(), nullable=False),\n            StructField(ColNames.unique_cell_cnt, ShortType(), nullable=False),\n            StructField(ColNames.unique_location_cnt, ShortType(), nullable=False),\n            StructField(ColNames.sum_distance_m, IntegerType(), nullable=True),\n            StructField(ColNames.unique_hour_cnt, ByteType(), nullable=False),\n            StructField(ColNames.mean_time_gap, IntegerType(), nullable=True),\n            StructField(ColNames.stdev_time_gap, FloatType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_enriched_grid_data_object/","title":"silver_enriched_grid_data_object","text":""},{"location":"reference/core/data_objects/silver/silver_enriched_grid_data_object/#core.data_objects.silver.silver_enriched_grid_data_object.SilverEnrichedGridDataObject","title":"<code>SilverEnrichedGridDataObject</code>","text":"<p>               Bases: <code>GeoParquetDataObject</code></p> <p>Class that models operational grid.</p> Source code in <code>multimno/core/data_objects/silver/silver_enriched_grid_data_object.py</code> <pre><code>class SilverEnrichedGridDataObject(GeoParquetDataObject):\n    \"\"\"\n    Class that models operational grid.\n    \"\"\"\n\n    ID = \"SilverEnrichedGridDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.geometry, GeometryType(), nullable=False),\n            StructField(ColNames.grid_id, LongType(), nullable=False),\n            StructField(ColNames.elevation, FloatType(), nullable=True),\n            StructField(ColNames.prior_probability, FloatType(), nullable=True),\n            StructField(ColNames.ple_coefficient, FloatType(), nullable=True),\n            # partition columns\n            StructField(ColNames.quadkey, StringType(), nullable=True),\n        ]\n    )\n\n    MANDATORY_COLUMNS = [ColNames.grid_id, ColNames.geometry]\n    OPTIONAL_COLUMNS = [ColNames.elevation, ColNames.ple_coefficient, ColNames.prior_probability]\n\n    PARTITION_COLUMNS = [ColNames.quadkey]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_event_data_object/","title":"silver_event_data_object","text":"<p>Silver MNO Event data module</p>"},{"location":"reference/core/data_objects/silver/silver_event_data_object/#core.data_objects.silver.silver_event_data_object.SilverEventDataObject","title":"<code>SilverEventDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the cleaned MNO Event data.</p> Source code in <code>multimno/core/data_objects/silver/silver_event_data_object.py</code> <pre><code>class SilverEventDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models the cleaned MNO Event data.\n    \"\"\"\n\n    ID = \"SilverEventDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.mcc, IntegerType(), nullable=True),\n            StructField(ColNames.mnc, StringType(), nullable=True),\n            StructField(ColNames.plmn, IntegerType(), nullable=True),\n            StructField(ColNames.cell_id, StringType(), nullable=True),\n            StructField(ColNames.latitude, FloatType(), nullable=True),\n            StructField(ColNames.longitude, FloatType(), nullable=True),\n            StructField(ColNames.loc_error, FloatType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.user_id_modulo, IntegerType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day, ColNames.user_id_modulo]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_metrics_by_column/","title":"silver_event_data_syntactic_quality_metrics_by_column","text":"<p>Silver Event Data quality metrics.</p>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_metrics_by_column/#core.data_objects.silver.silver_event_data_syntactic_quality_metrics_by_column.SilverEventDataSyntacticQualityMetricsByColumn","title":"<code>SilverEventDataSyntacticQualityMetricsByColumn</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the Silver Event Data quality metrics DataObject.</p> Source code in <code>multimno/core/data_objects/silver/silver_event_data_syntactic_quality_metrics_by_column.py</code> <pre><code>class SilverEventDataSyntacticQualityMetricsByColumn(ParquetDataObject):\n    \"\"\"\n    Class that models the Silver Event Data quality metrics DataObject.\n    \"\"\"\n\n    ID = \"SilverEventDataSyntacticQualityMetricsByColumn\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.result_timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.variable, StringType(), nullable=True),\n            StructField(ColNames.type_of_error, ShortType(), nullable=True),\n            StructField(ColNames.value, IntegerType(), nullable=False),\n            # partition columns\n            StructField(ColNames.date, DateType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.date]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_metrics_frequency_distribution/","title":"silver_event_data_syntactic_quality_metrics_frequency_distribution","text":"<p>Silver Event Data deduplication frequency quality metrics.</p>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_metrics_frequency_distribution/#core.data_objects.silver.silver_event_data_syntactic_quality_metrics_frequency_distribution.SilverEventDataSyntacticQualityMetricsFrequencyDistribution","title":"<code>SilverEventDataSyntacticQualityMetricsFrequencyDistribution</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the Silver Event Data syntactic frequency quality metrics DataObject.</p> Source code in <code>multimno/core/data_objects/silver/silver_event_data_syntactic_quality_metrics_frequency_distribution.py</code> <pre><code>class SilverEventDataSyntacticQualityMetricsFrequencyDistribution(ParquetDataObject):\n    \"\"\"\n    Class that models the Silver Event Data syntactic\n    frequency quality metrics DataObject.\n    \"\"\"\n\n    ID = \"SilverEventDataSyntacticQualityMetricsFrequencyDistribution\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.cell_id, StringType(), nullable=True),\n            StructField(ColNames.user_id, BinaryType(), nullable=True),\n            StructField(ColNames.initial_frequency, IntegerType(), nullable=False),\n            StructField(ColNames.final_frequency, IntegerType(), nullable=False),\n            StructField(ColNames.date, DateType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.date]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_warnings_for_plots/","title":"silver_event_data_syntactic_quality_warnings_for_plots","text":"<p>Silver Event Data quality warning for plots table.</p>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_warnings_for_plots/#core.data_objects.silver.silver_event_data_syntactic_quality_warnings_for_plots.SilverEventDataSyntacticQualityWarningsForPlots","title":"<code>SilverEventDataSyntacticQualityWarningsForPlots</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that store data to plot raw, clean data sizez and error rate.</p> Source code in <code>multimno/core/data_objects/silver/silver_event_data_syntactic_quality_warnings_for_plots.py</code> <pre><code>class SilverEventDataSyntacticQualityWarningsForPlots(ParquetDataObject):\n    \"\"\"\n    Class that store data to plot raw, clean data sizez and error rate.\n    \"\"\"\n\n    ID = \"SilverEventDataSyntacticQualityWarningsForPlots\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.type_of_qw, StringType(), nullable=False),\n            StructField(ColNames.lookback_period, StringType(), nullable=False),\n            StructField(ColNames.daily_value, FloatType(), nullable=False),\n            StructField(ColNames.average, FloatType(), nullable=True),\n            StructField(ColNames.LCL, FloatType(), nullable=True),\n            StructField(ColNames.UCL, FloatType(), nullable=True),\n            # partition columns\n            StructField(ColNames.date, DateType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.date]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_warnings_log_table/","title":"silver_event_data_syntactic_quality_warnings_log_table","text":"<p>Silver Event Data Quality Warning log table.</p>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_warnings_log_table/#core.data_objects.silver.silver_event_data_syntactic_quality_warnings_log_table.SilverEventDataSyntacticQualityWarningsLogTable","title":"<code>SilverEventDataSyntacticQualityWarningsLogTable</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that stores information about Event Quallity Warnings</p> Source code in <code>multimno/core/data_objects/silver/silver_event_data_syntactic_quality_warnings_log_table.py</code> <pre><code>class SilverEventDataSyntacticQualityWarningsLogTable(ParquetDataObject):\n    \"\"\"\n    Class that stores information about Event Quallity Warnings\n    \"\"\"\n\n    ID = \"SilverEventDataSyntacticQualityWarningsLogTable\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.measure_definition, StringType(), nullable=False),\n            StructField(ColNames.lookback_period, StringType(), nullable=False),\n            StructField(ColNames.daily_value, FloatType(), nullable=False),\n            StructField(ColNames.condition_value, FloatType(), nullable=True),\n            StructField(ColNames.condition, StringType(), nullable=False),\n            StructField(ColNames.warning_text, StringType(), nullable=False),\n            # partition columns\n            StructField(ColNames.date, DateType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.date]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_event_flagged_data_object/","title":"silver_event_flagged_data_object","text":"<p>Silver MNO Event data module with flags computed in Semantic Checks</p>"},{"location":"reference/core/data_objects/silver/silver_event_flagged_data_object/#core.data_objects.silver.silver_event_flagged_data_object.SilverEventFlaggedDataObject","title":"<code>SilverEventFlaggedDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the cleaned MNO Event data, with flags computed in the semantic checks module.</p> Source code in <code>multimno/core/data_objects/silver/silver_event_flagged_data_object.py</code> <pre><code>class SilverEventFlaggedDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models the cleaned MNO Event data, with flags computed\n    in the semantic checks module.\n    \"\"\"\n\n    ID = \"SilverEventFlaggedDO\"\n\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.mcc, IntegerType(), nullable=True),\n            StructField(ColNames.mnc, StringType(), nullable=True),\n            StructField(ColNames.plmn, IntegerType(), nullable=True),\n            StructField(ColNames.cell_id, StringType(), nullable=True),\n            StructField(ColNames.latitude, FloatType(), nullable=True),\n            StructField(ColNames.longitude, FloatType(), nullable=True),\n            StructField(ColNames.loc_error, FloatType(), nullable=True),\n            StructField(ColNames.error_flag, IntegerType(), nullable=False),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.user_id_modulo, IntegerType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day, ColNames.user_id_modulo]\n\n    def is_data_available(self, date):\n        path = (\n            f\"{self.default_path}/{ColNames.year}={date.year}/{ColNames.month}={date.month}/{ColNames.day}={date.day}\"\n        )\n\n        return check_if_data_path_exists(self.spark, path)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_geozones_grid_map_data_object/","title":"silver_geozones_grid_map_data_object","text":""},{"location":"reference/core/data_objects/silver/silver_geozones_grid_map_data_object/#core.data_objects.silver.silver_geozones_grid_map_data_object.SilverGeozonesGridMapDataObject","title":"<code>SilverGeozonesGridMapDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models geographic and admin zones to grid mapping table.</p> Source code in <code>multimno/core/data_objects/silver/silver_geozones_grid_map_data_object.py</code> <pre><code>class SilverGeozonesGridMapDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models geographic and admin zones to grid mapping table.\n    \"\"\"\n\n    ID = \"SilverGeozonesGridMapDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.grid_id, LongType(), nullable=False),\n            StructField(ColNames.zone_id, StringType(), nullable=False),\n            StructField(ColNames.hierarchical_id, StringType(), nullable=False),\n            # partition columns\n            StructField(ColNames.dataset_id, StringType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            # StructField(ColNames.quadkey, StringType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.dataset_id, ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_grid_data_object/","title":"silver_grid_data_object","text":""},{"location":"reference/core/data_objects/silver/silver_grid_data_object/#core.data_objects.silver.silver_grid_data_object.SilverGridDataObject","title":"<code>SilverGridDataObject</code>","text":"<p>               Bases: <code>GeoParquetDataObject</code></p> <p>Class that models operational grid.</p> Source code in <code>multimno/core/data_objects/silver/silver_grid_data_object.py</code> <pre><code>class SilverGridDataObject(GeoParquetDataObject):\n    \"\"\"\n    Class that models operational grid.\n    \"\"\"\n\n    ID = \"SilverGridDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.geometry, GeometryType(), nullable=False),\n            StructField(ColNames.grid_id, LongType(), nullable=False),\n            # partition columns\n            StructField(ColNames.quadkey, StringType(), nullable=True),\n        ]\n    )\n\n    MANDATORY_COLUMNS = [ColNames.grid_id, ColNames.geometry]\n\n    PARTITION_COLUMNS = [ColNames.quadkey]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_internal_migration_data_object/","title":"silver_internal_migration_data_object","text":"<p>Silver Internal Migration data object module</p>"},{"location":"reference/core/data_objects/silver/silver_internal_migration_data_object/#core.data_objects.silver.silver_internal_migration_data_object.SilverInternalMigrationDataObject","title":"<code>SilverInternalMigrationDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the Internal Migration data object.</p> Source code in <code>multimno/core/data_objects/silver/silver_internal_migration_data_object.py</code> <pre><code>class SilverInternalMigrationDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models the Internal Migration data object.\n    \"\"\"\n\n    ID = \"SilverInternalMigrationDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.previous_zone, StringType(), nullable=False),\n            StructField(ColNames.new_zone, StringType(), nullable=False),\n            StructField(ColNames.migration, FloatType(), nullable=False),\n            # partition columns\n            StructField(ColNames.dataset_id, StringType(), nullable=False),\n            StructField(ColNames.level, ByteType(), nullable=False),\n            StructField(ColNames.start_date_previous, DateType(), nullable=False),\n            StructField(ColNames.end_date_previous, DateType(), nullable=False),\n            StructField(ColNames.season_previous, StringType(), nullable=False),\n            StructField(ColNames.start_date_new, DateType(), nullable=False),\n            StructField(ColNames.end_date_new, DateType(), nullable=False),\n            StructField(ColNames.season_new, StringType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [\n        ColNames.dataset_id,\n        ColNames.level,\n        ColNames.start_date_previous,\n        ColNames.end_date_previous,\n        ColNames.season_previous,\n        ColNames.start_date_new,\n        ColNames.end_date_new,\n        ColNames.season_new,\n    ]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_internal_migration_quality_metrics_data_object/","title":"silver_internal_migration_quality_metrics_data_object","text":"<p>Silver Internal Migration data object module</p>"},{"location":"reference/core/data_objects/silver/silver_internal_migration_quality_metrics_data_object/#core.data_objects.silver.silver_internal_migration_quality_metrics_data_object.SilverInternalMigrationQualityMetricsDataObject","title":"<code>SilverInternalMigrationQualityMetricsDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the Internal Migration data object.</p> Source code in <code>multimno/core/data_objects/silver/silver_internal_migration_quality_metrics_data_object.py</code> <pre><code>class SilverInternalMigrationQualityMetricsDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models the Internal Migration data object.\n    \"\"\"\n\n    ID = \"SilverInternalMigrationQualityMetricsDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.previous_home_users, LongType(), nullable=False),\n            StructField(ColNames.new_home_users, LongType(), nullable=False),\n            StructField(ColNames.common_home_users, LongType(), nullable=False),\n            StructField(ColNames.result_timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.dataset_id, StringType(), nullable=False),\n            StructField(ColNames.level, ByteType(), nullable=False),\n            StructField(ColNames.start_date_previous, DateType(), nullable=False),\n            StructField(ColNames.end_date_previous, DateType(), nullable=False),\n            StructField(ColNames.season_previous, StringType(), nullable=False),\n            StructField(ColNames.start_date_new, DateType(), nullable=False),\n            StructField(ColNames.end_date_new, DateType(), nullable=False),\n            StructField(ColNames.season_new, StringType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [\n        ColNames.dataset_id,\n        ColNames.level,\n        ColNames.start_date_previous,\n        ColNames.end_date_previous,\n        ColNames.season_previous,\n        ColNames.start_date_new,\n        ColNames.end_date_new,\n        ColNames.season_new,\n    ]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_longterm_permanence_score_data_object/","title":"silver_longterm_permanence_score_data_object","text":"<p>Silver Mid Term Permanence Score data object module</p>"},{"location":"reference/core/data_objects/silver/silver_longterm_permanence_score_data_object/#core.data_objects.silver.silver_longterm_permanence_score_data_object.SilverLongtermPermanenceScoreDataObject","title":"<code>SilverLongtermPermanenceScoreDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the Longterm Permanence Score data object.</p> Source code in <code>multimno/core/data_objects/silver/silver_longterm_permanence_score_data_object.py</code> <pre><code>class SilverLongtermPermanenceScoreDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models the Longterm Permanence Score data object.\n    \"\"\"\n\n    ID = \"SilverLongtermPermanenceScoreDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.grid_id, LongType(), nullable=False),\n            StructField(ColNames.lps, IntegerType(), nullable=False),\n            StructField(ColNames.total_frequency, IntegerType(), nullable=False),\n            StructField(ColNames.frequency_mean, FloatType(), nullable=True),\n            StructField(ColNames.frequency_std, FloatType(), nullable=True),\n            StructField(ColNames.regularity_mean, FloatType(), nullable=True),\n            StructField(ColNames.regularity_std, FloatType(), nullable=True),\n            # partition columns\n            StructField(ColNames.season, StringType(), nullable=False),\n            StructField(ColNames.start_date, DateType(), nullable=False),\n            StructField(ColNames.end_date, DateType(), nullable=False),\n            StructField(ColNames.day_type, StringType(), nullable=False),\n            StructField(ColNames.time_interval, StringType(), nullable=False),\n            StructField(ColNames.id_type, StringType(), nullable=False),\n            StructField(ColNames.user_id_modulo, IntegerType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [\n        ColNames.season,\n        ColNames.start_date,\n        ColNames.end_date,\n        ColNames.day_type,\n        ColNames.time_interval,\n        ColNames.id_type,\n        ColNames.user_id_modulo,\n    ]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_midterm_permanence_score_data_object/","title":"silver_midterm_permanence_score_data_object","text":"<p>Silver Mid Term Permanence Score data object module</p>"},{"location":"reference/core/data_objects/silver/silver_midterm_permanence_score_data_object/#core.data_objects.silver.silver_midterm_permanence_score_data_object.SilverMidtermPermanenceScoreDataObject","title":"<code>SilverMidtermPermanenceScoreDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the Midterm Permanence Score data object.</p> Source code in <code>multimno/core/data_objects/silver/silver_midterm_permanence_score_data_object.py</code> <pre><code>class SilverMidtermPermanenceScoreDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models the Midterm Permanence Score data object.\n    \"\"\"\n\n    ID = \"SilverMidtermPermanenceScoreDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.grid_id, LongType(), nullable=False),\n            StructField(ColNames.mps, IntegerType(), nullable=False),\n            StructField(ColNames.frequency, IntegerType(), nullable=False),\n            StructField(ColNames.regularity_mean, FloatType(), nullable=True),\n            StructField(ColNames.regularity_std, FloatType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day_type, StringType(), nullable=False),\n            StructField(ColNames.time_interval, StringType(), nullable=False),\n            StructField(ColNames.id_type, StringType(), nullable=False),\n            StructField(ColNames.user_id_modulo, IntegerType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [\n        ColNames.year,\n        ColNames.month,\n        ColNames.day_type,\n        ColNames.time_interval,\n        ColNames.id_type,\n        ColNames.user_id_modulo,\n    ]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_network_data_object/","title":"silver_network_data_object","text":"<p>Silver MNO Network Topology Data module</p>"},{"location":"reference/core/data_objects/silver/silver_network_data_object/#core.data_objects.silver.silver_network_data_object.SilverNetworkDataObject","title":"<code>SilverNetworkDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the clean MNO Network Topology Data, based on the physical properties of the cells.</p> Source code in <code>multimno/core/data_objects/silver/silver_network_data_object.py</code> <pre><code>class SilverNetworkDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models the clean MNO Network Topology Data, based on the physical\n    properties of the cells.\n    \"\"\"\n\n    ID = \"SilverNetworkDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.cell_id, StringType(), nullable=False),\n            StructField(ColNames.latitude, FloatType(), nullable=False),\n            StructField(ColNames.longitude, FloatType(), nullable=False),\n            StructField(ColNames.altitude, FloatType(), nullable=True),\n            StructField(ColNames.antenna_height, FloatType(), nullable=True),\n            StructField(ColNames.directionality, IntegerType(), nullable=False),\n            StructField(ColNames.azimuth_angle, FloatType(), nullable=True),\n            StructField(ColNames.elevation_angle, FloatType(), nullable=True),\n            StructField(ColNames.horizontal_beam_width, FloatType(), nullable=True),\n            StructField(ColNames.vertical_beam_width, FloatType(), nullable=True),\n            StructField(ColNames.power, FloatType(), nullable=True),\n            StructField(ColNames.range, FloatType(), nullable=True),\n            StructField(ColNames.frequency, IntegerType(), nullable=True),\n            StructField(ColNames.technology, StringType(), nullable=True),\n            StructField(ColNames.valid_date_start, TimestampType(), nullable=True),\n            StructField(ColNames.valid_date_end, TimestampType(), nullable=True),\n            StructField(ColNames.cell_type, StringType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_network_data_syntactic_quality_metrics_by_column/","title":"silver_network_data_syntactic_quality_metrics_by_column","text":"<p>Silver Network topology quality metrics.</p>"},{"location":"reference/core/data_objects/silver/silver_network_data_syntactic_quality_metrics_by_column/#core.data_objects.silver.silver_network_data_syntactic_quality_metrics_by_column.SilverNetworkDataQualityMetricsByColumn","title":"<code>SilverNetworkDataQualityMetricsByColumn</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the Silver Network Topology data quality metrics data object.</p> Source code in <code>multimno/core/data_objects/silver/silver_network_data_syntactic_quality_metrics_by_column.py</code> <pre><code>class SilverNetworkDataQualityMetricsByColumn(ParquetDataObject):\n    \"\"\"\n    Class that models the Silver Network Topology data quality metrics data object.\n    \"\"\"\n\n    ID = \"SilverNetworkDataQualityMetricsByColumn\"\n\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.result_timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.date, DateType(), nullable=False),\n            StructField(ColNames.field_name, StringType(), nullable=True),\n            StructField(ColNames.type_code, IntegerType(), nullable=False),\n            StructField(ColNames.value, IntegerType(), nullable=False),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_network_data_top_frequent_errors_data_object/","title":"silver_network_data_top_frequent_errors_data_object","text":"<p>Silver Network Data Top Frequent Errors.</p>"},{"location":"reference/core/data_objects/silver/silver_network_data_top_frequent_errors_data_object/#core.data_objects.silver.silver_network_data_top_frequent_errors_data_object.SilverNetworkDataTopFrequentErrors","title":"<code>SilverNetworkDataTopFrequentErrors</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the Silver Network Topology Top Frequent Errors data object</p> Source code in <code>multimno/core/data_objects/silver/silver_network_data_top_frequent_errors_data_object.py</code> <pre><code>class SilverNetworkDataTopFrequentErrors(ParquetDataObject):\n    \"\"\"\n    Class that models the Silver Network Topology Top Frequent Errors data object\n    \"\"\"\n\n    ID = \"SilverNetworkDataTopFrequentErrorsDO\"\n\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.result_timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.field_name, StringType(), nullable=False),\n            StructField(ColNames.type_code, IntegerType(), nullable=False),\n            StructField(ColNames.error_value, StringType(), nullable=False),\n            StructField(ColNames.error_count, IntegerType(), nullable=False),\n            StructField(ColNames.accumulated_percentage, FloatType(), nullable=False),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_network_row_error_metrics/","title":"silver_network_row_error_metrics","text":"<p>Silver Network Data Row Error Metrics.</p>"},{"location":"reference/core/data_objects/silver/silver_network_row_error_metrics/#core.data_objects.silver.silver_network_row_error_metrics.SilverNetworkRowErrorMetrics","title":"<code>SilverNetworkRowErrorMetrics</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the Silver Network Topology Row Error Metrics data object</p> Source code in <code>multimno/core/data_objects/silver/silver_network_row_error_metrics.py</code> <pre><code>class SilverNetworkRowErrorMetrics(ParquetDataObject):\n    \"\"\"\n    Class that models the Silver Network Topology Row Error Metrics data object\n    \"\"\"\n\n    ID = \"SilverNetworkRowErrorMetricsDO\"\n\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.result_timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.variable, StringType(), nullable=False),\n            StructField(ColNames.value, IntegerType(), nullable=False),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_network_syntactic_quality_warnings_log_table/","title":"silver_network_syntactic_quality_warnings_log_table","text":"<p>Silver MNO Network Topology Quality Warnings Log Table Data Object</p>"},{"location":"reference/core/data_objects/silver/silver_network_syntactic_quality_warnings_log_table/#core.data_objects.silver.silver_network_syntactic_quality_warnings_log_table.SilverNetworkDataSyntacticQualityWarningsLogTable","title":"<code>SilverNetworkDataSyntacticQualityWarningsLogTable</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the log table keeping track of the quality warnings that may arise from the syntactic checks and cleaning of the MNO Network Topology Data.</p> Source code in <code>multimno/core/data_objects/silver/silver_network_syntactic_quality_warnings_log_table.py</code> <pre><code>class SilverNetworkDataSyntacticQualityWarningsLogTable(ParquetDataObject):\n    \"\"\"\n    Class that models the log table keeping track of the quality warnings that may arise from\n    the syntactic checks and cleaning of the MNO Network Topology Data.\n    \"\"\"\n\n    ID = \"SilverNetworkDataSyntacticQualityWarningsLogTable\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.title, StringType(), nullable=False),\n            StructField(ColNames.date, DateType(), nullable=False),  # date of study analysed\n            StructField(ColNames.timestamp, TimestampType(), nullable=False),  # moment when QW where generated\n            StructField(ColNames.measure_definition, StringType(), nullable=False),\n            StructField(ColNames.daily_value, FloatType(), nullable=False),\n            StructField(ColNames.condition, StringType(), nullable=False),\n            StructField(ColNames.lookback_period, StringType(), nullable=False),  # using same name as for events\n            StructField(ColNames.condition_value, FloatType(), nullable=False),\n            StructField(ColNames.warning_text, StringType(), nullable=False),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_network_syntactic_quality_warnings_plot_data/","title":"silver_network_syntactic_quality_warnings_plot_data","text":"<p>Silver MNO Network Topology Quality Warnings Data Object for the generation of plots</p>"},{"location":"reference/core/data_objects/silver/silver_network_syntactic_quality_warnings_plot_data/#core.data_objects.silver.silver_network_syntactic_quality_warnings_plot_data.SilverNetworkSyntacticQualityWarningsLinePlotData","title":"<code>SilverNetworkSyntacticQualityWarningsLinePlotData</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the data required to produce line plots reflecting the daily evolution of the number of rows before and after the syntactic checks, as well as the overall error rate.</p> Source code in <code>multimno/core/data_objects/silver/silver_network_syntactic_quality_warnings_plot_data.py</code> <pre><code>class SilverNetworkSyntacticQualityWarningsLinePlotData(ParquetDataObject):\n    \"\"\"\n    Class that models the data required to produce line plots reflecting the daily evolution of the number\n    of rows before and after the syntactic checks, as well as the overall error rate.\n    \"\"\"\n\n    ID = \"SilverNetworkSyntacticQualityWarningsLinePlotData\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.date, DateType(), nullable=False),\n            StructField(ColNames.daily_value, FloatType(), nullable=False),\n            StructField(ColNames.average, FloatType(), nullable=False),\n            StructField(ColNames.LCL, FloatType(), nullable=False),\n            StructField(ColNames.UCL, FloatType(), nullable=True),\n            # partition columns\n            StructField(ColNames.variable, StringType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.timestamp, TimestampType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [\n        ColNames.variable,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n        ColNames.timestamp,\n    ]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_network_syntactic_quality_warnings_plot_data/#core.data_objects.silver.silver_network_syntactic_quality_warnings_plot_data.SilverNetworkSyntacticQualityWarningsPiePlotData","title":"<code>SilverNetworkSyntacticQualityWarningsPiePlotData</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the data required to produce pie plots reflecting the percentage of each type of error for each field of the network topology data object.</p> Source code in <code>multimno/core/data_objects/silver/silver_network_syntactic_quality_warnings_plot_data.py</code> <pre><code>class SilverNetworkSyntacticQualityWarningsPiePlotData(ParquetDataObject):\n    \"\"\"\n    Class that models the data required to produce pie plots reflecting the percentage of each type of error\n    for each field of the network topology data object.\n    \"\"\"\n\n    ID = \"SilverNetworkSyntacticQualityWarningsPiePlotData\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.type_of_error, StringType(), nullable=False),\n            StructField(ColNames.value, IntegerType(), nullable=False),\n            # partition columns\n            StructField(ColNames.variable, StringType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.timestamp, TimestampType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [\n        ColNames.variable,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n        ColNames.timestamp,\n    ]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_present_population_data_object/","title":"silver_present_population_data_object","text":"<p>Silver present population estimatation per grid data object</p>"},{"location":"reference/core/data_objects/silver/silver_present_population_data_object/#core.data_objects.silver.silver_present_population_data_object.SilverPresentPopulationDataObject","title":"<code>SilverPresentPopulationDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Estimation of the population present at a given time at the grid tile level.</p> Source code in <code>multimno/core/data_objects/silver/silver_present_population_data_object.py</code> <pre><code>class SilverPresentPopulationDataObject(ParquetDataObject):\n    \"\"\"\n    Estimation of the population present at a given time at the grid tile level.\n    \"\"\"\n\n    ID = \"SilverPresentPopulationDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.grid_id, LongType(), nullable=False),\n            StructField(ColNames.population, FloatType(), nullable=False),\n            StructField(ColNames.timestamp, TimestampType(), nullable=False),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_present_population_zone_data_object/","title":"silver_present_population_zone_data_object","text":"<p>Silver present population estimatation per zone data object</p>"},{"location":"reference/core/data_objects/silver/silver_present_population_zone_data_object/#core.data_objects.silver.silver_present_population_zone_data_object.SilverPresentPopulationZoneDataObject","title":"<code>SilverPresentPopulationZoneDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Estimation of the population present at a given time at the level of some zoning system.</p> Source code in <code>multimno/core/data_objects/silver/silver_present_population_zone_data_object.py</code> <pre><code>class SilverPresentPopulationZoneDataObject(ParquetDataObject):\n    \"\"\"\n    Estimation of the population present at a given time at the level of some zoning system.\n    \"\"\"\n\n    ID = \"SilverPresentPopulationZoneDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.zone_id, StringType(), nullable=False),\n            StructField(ColNames.population, FloatType(), nullable=False),\n            StructField(ColNames.timestamp, TimestampType(), nullable=False),\n            # partition columns\n            StructField(ColNames.dataset_id, StringType(), nullable=False),\n            StructField(ColNames.level, ByteType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    VALUE_COLUMNS = [ColNames.population]\n\n    AGGREGATION_COLUMNS = [\n        ColNames.zone_id,\n        ColNames.timestamp,\n        ColNames.dataset_id,\n        ColNames.level,\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n    ]\n\n    PARTITION_COLUMNS = [ColNames.dataset_id, ColNames.level, ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_semantic_quality_metrics/","title":"silver_semantic_quality_metrics","text":""},{"location":"reference/core/data_objects/silver/silver_semantic_quality_metrics/#core.data_objects.silver.silver_semantic_quality_metrics.SilverEventSemanticQualityMetrics","title":"<code>SilverEventSemanticQualityMetrics</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> Source code in <code>multimno/core/data_objects/silver/silver_semantic_quality_metrics.py</code> <pre><code>class SilverEventSemanticQualityMetrics(ParquetDataObject):\n    \"\"\" \"\"\"\n\n    ID = \"SilverEventSemanticQualityMetrics\"\n\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.result_timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.variable, StringType(), nullable=False),\n            StructField(ColNames.type_of_error, IntegerType(), nullable=False),\n            StructField(ColNames.value, LongType(), nullable=False),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_semantic_quality_warnings_log_table/","title":"silver_semantic_quality_warnings_log_table","text":"<p>Silver MNO Network Topology Quality Warnings Log Table Data Object</p>"},{"location":"reference/core/data_objects/silver/silver_semantic_quality_warnings_log_table/#core.data_objects.silver.silver_semantic_quality_warnings_log_table.SilverEventSemanticQualityWarningsLogTable","title":"<code>SilverEventSemanticQualityWarningsLogTable</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the log table keeping track of the quality warnings that may arise from the semantic checks of the MNO Event data.</p> Source code in <code>multimno/core/data_objects/silver/silver_semantic_quality_warnings_log_table.py</code> <pre><code>class SilverEventSemanticQualityWarningsLogTable(ParquetDataObject):\n    \"\"\"\n    Class that models the log table keeping track of the quality warnings that may arise from\n    the semantic checks of the MNO Event data.\n    \"\"\"\n\n    ID = \"SilverEventSemanticQualityWarningsLogTable\"\n\n    SCHEMA = StructType(\n        [\n            StructField(\"date\", DateType(), nullable=False),\n            StructField(\"Error 1\", FloatType(), nullable=False),\n            StructField(\"Error 2\", FloatType(), nullable=False),\n            StructField(\"Error 3\", FloatType(), nullable=False),\n            StructField(\"Error 4\", FloatType(), nullable=False),\n            StructField(\"Error 5\", FloatType(), nullable=False),\n            StructField(\"Error 1 upper control limit\", FloatType(), nullable=True),\n            StructField(\"Error 2 upper control limit\", FloatType(), nullable=True),\n            StructField(\"Error 3 upper control limit\", FloatType(), nullable=True),\n            StructField(\"Error 4 upper control limit\", FloatType(), nullable=True),\n            StructField(\"Error 5 upper control limit\", FloatType(), nullable=True),\n            StructField(\"Error 1 display warning\", BooleanType(), nullable=False),\n            StructField(\"Error 2 display warning\", BooleanType(), nullable=False),\n            StructField(\"Error 3 display warning\", BooleanType(), nullable=False),\n            StructField(\"Error 4 display warning\", BooleanType(), nullable=False),\n            StructField(\"Error 5 display warning\", BooleanType(), nullable=False),\n            StructField(\"execution_id\", TimestampType(), nullable=False),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_semantic_quality_warnings_plot_data/","title":"silver_semantic_quality_warnings_plot_data","text":"<p>Silver MNO Network Topology Quality Warnings Log Table Data Object</p>"},{"location":"reference/core/data_objects/silver/silver_semantic_quality_warnings_plot_data/#core.data_objects.silver.silver_semantic_quality_warnings_plot_data.SilverEventSemanticQualityWarningsBarPlotData","title":"<code>SilverEventSemanticQualityWarningsBarPlotData</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the data required to produce line plots reflecting the daily evolution of the number of each type of error, as well as records without errors, as seen during the semantic checks of MNO Event data.</p> Source code in <code>multimno/core/data_objects/silver/silver_semantic_quality_warnings_plot_data.py</code> <pre><code>class SilverEventSemanticQualityWarningsBarPlotData(ParquetDataObject):\n    \"\"\"\n    Class that models the data required to produce line plots reflecting the daily evolution of the number\n    of each type of error, as well as records without errors, as seen during the semantic checks of MNO Event data.\n    \"\"\"\n\n    ID = \"SilverEventSemanticQualityWarningBarPlotData\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.date, DateType(), nullable=False),\n            StructField(ColNames.type_of_error, StringType(), nullable=False),\n            StructField(ColNames.value, FloatType(), nullable=False),\n            # partition columns\n            StructField(ColNames.variable, StringType(), nullable=False),\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.timestamp, TimestampType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.variable, ColNames.year, ColNames.month, ColNames.day, ColNames.timestamp]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_time_segments_data_object/","title":"silver_time_segments_data_object","text":""},{"location":"reference/core/data_objects/silver/silver_time_segments_data_object/#core.data_objects.silver.silver_time_segments_data_object.SilverTimeSegmentsDataObject","title":"<code>SilverTimeSegmentsDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the cleaned MNO Event data.</p> Source code in <code>multimno/core/data_objects/silver/silver_time_segments_data_object.py</code> <pre><code>class SilverTimeSegmentsDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models the cleaned MNO Event data.\n    \"\"\"\n\n    ID = \"SilverTimeSegmentsDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.time_segment_id, StringType(), nullable=False),\n            StructField(ColNames.start_timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.end_timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.mcc, ShortType(), nullable=False),\n            StructField(ColNames.mnc, StringType(), nullable=False),\n            StructField(ColNames.plmn, IntegerType(), nullable=True),\n            StructField(ColNames.cells, ArrayType(StringType()), nullable=False),\n            StructField(ColNames.state, StringType(), nullable=False),\n            StructField(ColNames.is_last, BooleanType(), nullable=True),\n            # partition columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.user_id_modulo, IntegerType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [\n        ColNames.year,\n        ColNames.month,\n        ColNames.day,\n        ColNames.user_id_modulo,\n    ]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_tourism_stays_data_object/","title":"silver_tourism_stays_data_object","text":""},{"location":"reference/core/data_objects/silver/silver_tourism_stays_data_object/#core.data_objects.silver.silver_tourism_stays_data_object.SilverTourismStaysDataObject","title":"<code>SilverTourismStaysDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Data Object for daily tourism stays.</p> Source code in <code>multimno/core/data_objects/silver/silver_tourism_stays_data_object.py</code> <pre><code>class SilverTourismStaysDataObject(ParquetDataObject):\n    \"\"\"\n    Data Object for daily tourism stays.\n    \"\"\"\n\n    ID = \"SilverTourismStaysDataObjectDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.time_segment_id, StringType(), nullable=False),\n            StructField(ColNames.start_timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.end_timestamp, TimestampType(), nullable=False),\n            StructField(ColNames.mcc, ShortType(), nullable=False),\n            StructField(ColNames.mnc, StringType(), nullable=False),\n            StructField(ColNames.zone_ids_list, ArrayType(StringType()), nullable=False),\n            StructField(ColNames.zone_weights_list, ArrayType(FloatType()), nullable=False),\n            StructField(ColNames.is_overnight, BooleanType(), nullable=False),\n            # partitioning columns\n            StructField(ColNames.year, ShortType(), nullable=False),\n            StructField(ColNames.month, ByteType(), nullable=False),\n            StructField(ColNames.day, ByteType(), nullable=False),\n            StructField(ColNames.user_id_modulo, IntegerType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.year, ColNames.month, ColNames.day, ColNames.user_id_modulo]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_usual_environment_labeling_quality_metrics_data_object/","title":"silver_usual_environment_labeling_quality_metrics_data_object","text":"<p>Silver Usual Environment Labeling Quality Metrics data object module</p>"},{"location":"reference/core/data_objects/silver/silver_usual_environment_labeling_quality_metrics_data_object/#core.data_objects.silver.silver_usual_environment_labeling_quality_metrics_data_object.SilverUsualEnvironmentLabelingQualityMetricsDataObject","title":"<code>SilverUsualEnvironmentLabelingQualityMetricsDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the Usual Environment Labeling Quality Metrics data object.</p> Source code in <code>multimno/core/data_objects/silver/silver_usual_environment_labeling_quality_metrics_data_object.py</code> <pre><code>class SilverUsualEnvironmentLabelingQualityMetricsDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models the Usual Environment Labeling Quality Metrics data object.\n    \"\"\"\n\n    ID = \"SilverUsualEnvironmentLabelingQualityMetricsDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.labeling_quality_metric, StringType(), nullable=False),\n            StructField(ColNames.labeling_quality_count, LongType(), nullable=False),\n            # partition columns\n            StructField(ColNames.start_date, DateType(), nullable=False),\n            StructField(ColNames.end_date, DateType(), nullable=False),\n            StructField(ColNames.season, StringType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.start_date, ColNames.end_date, ColNames.season]\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_usual_environment_labels_data_object/","title":"silver_usual_environment_labels_data_object","text":"<p>Silver Usual Environment Labels data object module</p>"},{"location":"reference/core/data_objects/silver/silver_usual_environment_labels_data_object/#core.data_objects.silver.silver_usual_environment_labels_data_object.SilverUsualEnvironmentLabelsDataObject","title":"<code>SilverUsualEnvironmentLabelsDataObject</code>","text":"<p>               Bases: <code>ParquetDataObject</code></p> <p>Class that models the Usual Environment Labels data object.</p> Source code in <code>multimno/core/data_objects/silver/silver_usual_environment_labels_data_object.py</code> <pre><code>class SilverUsualEnvironmentLabelsDataObject(ParquetDataObject):\n    \"\"\"\n    Class that models the Usual Environment Labels data object.\n    \"\"\"\n\n    ID = \"SilverUsualEnvironmentLabelsDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=False),\n            StructField(ColNames.grid_id, LongType(), nullable=False),\n            StructField(ColNames.label, StringType(), nullable=False),\n            StructField(ColNames.ue_label_rule, StringType(), nullable=False),\n            StructField(ColNames.location_label_rule, StringType(), nullable=False),\n            # partition columns\n            StructField(ColNames.start_date, DateType(), nullable=False),\n            StructField(ColNames.end_date, DateType(), nullable=False),\n            StructField(ColNames.season, StringType(), nullable=False),\n            StructField(ColNames.user_id_modulo, IntegerType(), nullable=False),\n        ]\n    )\n\n    PARTITION_COLUMNS = [ColNames.start_date, ColNames.end_date, ColNames.season, ColNames.user_id_modulo]\n</code></pre>"}]}