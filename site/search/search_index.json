{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"MultiMNO","text":"<p>This repository contains code that processes MNO Data to generate population and mobility insights.</p>"},{"location":"index.html#index","title":"Index:","text":"<ul> <li>Reference: Code documentation.</li> <li>Readme: Repository readme containing general information and usage of the code.</li> <li>Dev-Guide: Document with steps and recommendations to setup a development environment for this repository.</li> </ul>"},{"location":"dev_guide.html","title":"Developer Guide","text":"<p>The repository contains a devcontainer configuration compatible with VsCode. This configuration will create a docker container with all the necessary libraries and configurations to develop and execute the source code. </p> <ul> <li>Developer Guide</li> <li>Configurate docker container</li> <li>Start dev environment</li> <li>Hello world<ul> <li>Python execution</li> <li>Jupyter execution</li> </ul> </li> <li>Launching a single component</li> <li>Launching a pipeline</li> <li>Launching a spark history server</li> <li>Testing<ul> <li>See coverage in IDE (VsCode extension)</li> </ul> </li> <li>Code Linting</li> <li>Code Documentation<ul> <li>Update mkdocs index with readme</li> <li>Generate Static documentation</li> <li>Documentation server (Mkdocs)</li> </ul> </li> </ul>"},{"location":"dev_guide.html#configurate-docker-container","title":"Configurate docker container","text":"<p>Edit the .devcontainer/.env file:</p> <pre><code># ------------------- Docker Build parameters -------------------\nPYTHON_VERSION=3.11 # Python version.\nJDK_VERSION=17 # Java version.\nSPARK_VERSION=3.4.1 # Spark/Pyspark version.\nSEDONA_VERSION=1.5.0 # Sedona\nGEOTOOLS_WRAPPER=28.2 # Sedona dependency\n\n# ------------------- Docker run parameters -------------------\nCONTAINER_NAME=multimno_dev_container # Container name.\nDATA_DIR=../sample_data # Path of the host machine to the data to be used within the container.\nSPARK_LOGS_DIR=../sample_data/logs # Path of the host machine to where the spark logs will be stored.\nJL_PORT=8888 # Port of the host machine to deploy a jupyterlab.\nJL_CPU=4 # CPU cores of the container.\nJL_MEM=16g # RAM of the container.\n</code></pre>"},{"location":"dev_guide.html#start-dev-environment","title":"Start dev environment","text":"<p>In VsCode: F1 -&gt; Dev Containers: Rebuild and Reopen in container</p>"},{"location":"dev_guide.html#hello-world","title":"Hello world","text":""},{"location":"dev_guide.html#python-execution","title":"Python execution","text":"<p>Try the hello world app of section: Hello world</p>"},{"location":"dev_guide.html#jupyter-execution","title":"Jupyter execution","text":"<p>Open the hello world jupyter notebook stored in notebooks/hello_world.ipynb with VsCode and execute all cells. The Jupyter extension is installed automatically in the devcontainer.</p>"},{"location":"dev_guide.html#launching-a-single-component","title":"Launching a single component","text":"<p>In a terminal execute the command:  </p> <pre><code>spark-submit multimno/main.py &lt;component_id&gt; &lt;path_to_general_config&gt; &lt;path_to_component_config&gt; \n</code></pre> <p>Example:  </p> <pre><code>spark-submit multimno/main.py SyntheticEvents pipe_configs/configurations/general_config.ini pipe_configs/configurations/synthetic_events/synth_config.ini \n</code></pre>"},{"location":"dev_guide.html#launching-a-pipeline","title":"Launching a pipeline","text":"<p>In a terminal execute the command:  </p> <pre><code>python multimno/orchestrator.py &lt;pipeline_json_path&gt;\n</code></pre> <p>Example:  </p> <pre><code>python multimno/orchestrator.py pipe_configs/pipelines/pipeline.json \n</code></pre>"},{"location":"dev_guide.html#launching-a-spark-history-server","title":"Launching a spark history server","text":"<p>The history server will access SparkUI logs stored at the path ${SPARK_LOGS_DIR} defined in the .devcontainer/.env file.</p> <p>Starting the history server</p> <pre><code>start-history-server.sh \n</code></pre> <p>Accesing the history server * Go to the address http://localhost:18080</p>"},{"location":"dev_guide.html#testing","title":"Testing","text":""},{"location":"dev_guide.html#see-coverage-in-ide-vscode-extension","title":"See coverage in IDE (VsCode extension)","text":"<p>1) Generate the coverage report (xml|lcov)</p> <pre><code>pytest --cov-report=\"xml\" --cov=multimno tests/test_code/\n</code></pre> <p>2) Install the extension: Coverage Gutters 3) Right click and select Coverage Gutters: Watch</p> <p>Note: You can see the coverage percentage at the bottom bar</p>"},{"location":"dev_guide.html#code-linting","title":"Code Linting","text":"<p>The python code generated shall be formatted with autopep8. For formatting all source code execute the following command:</p> <pre><code>black -l 120 multimno tests/test_code/\n</code></pre>"},{"location":"dev_guide.html#code-documentation","title":"Code Documentation","text":""},{"location":"dev_guide.html#update-mkdocs-index-with-readme","title":"Update mkdocs index with readme","text":"<pre><code>cp README.md docs/readme.md\n</code></pre>"},{"location":"dev_guide.html#generate-static-documentation","title":"Generate Static documentation","text":"<pre><code>mkdocs build --no-directory-urls\n</code></pre> <p>The documentation will be generated under the site directory.</p>"},{"location":"dev_guide.html#documentation-server-mkdocs","title":"Documentation server (Mkdocs)","text":"<p>A code documentation can be deployed using mkdocs backend. </p> <p>1) Create documentation</p> <pre><code>./scripts/generate_docs.sh\n</code></pre> <p>2) Launch doc server</p> <pre><code>mkdocs serve\n</code></pre> <p>and navigate to the address: http://127.0.0.1:8000</p>"},{"location":"readme.html","title":"MultiMNO","text":"<p>This repository contains code that processes MNO Data to generate population and mobility insights.</p> <ul> <li>MultiMNO</li> <li>Setup<ul> <li>Docker installation</li> </ul> </li> <li>Local Execution<ul> <li>Docker image creation</li> <li>Docker container creation</li> <li>Hello world</li> <li>Try out the code</li> <li>Clean up</li> </ul> </li> <li>Production Deployment</li> </ul>"},{"location":"readme.html#setup","title":"Setup","text":"<p>The code stored in this repository is aimed to be executed in a PySpark compatible cluster. For an easy deployment in local environments, configuration for creating a docker container with all necessary dependencies is included in the <code>.devcontainer</code> folder. This allows users to execute the code in an isolated environment with all requirements and dependencies installed. </p>"},{"location":"readme.html#docker-installation","title":"Docker installation","text":"<p>Official guide: Click here</p>"},{"location":"readme.html#local-execution","title":"Local Execution","text":""},{"location":"readme.html#docker-image-creation","title":"Docker image creation","text":"<p>Execute the following command:</p> <pre><code>docker compose -f .devcontainer/docker-compose.yml --env-file=.devcontainer/.env build\n</code></pre>"},{"location":"readme.html#docker-container-creation","title":"Docker container creation","text":"<p>Create a container and start a shell session in it with the commands:</p> <pre><code>docker compose -f .devcontainer/docker-compose.yml --env-file=.devcontainer/.env up -d\ndocker exec -it multimno_dev_container bash\n</code></pre>"},{"location":"readme.html#hello-world","title":"Hello world","text":"<p>To test that the system has been correctly set try the hello world app with:</p> <pre><code>spark-submit multimno/hello_world.py\n</code></pre> <p>The hello world application will read a geoparquet file containing three geometries and will union all of them into a single geometry. The output will be a HTML file: sample_data/output/census.html where the single geometry (corresponding to the Basque Country) can be seen.</p>"},{"location":"readme.html#try-out-the-code","title":"Try out the code","text":"<p>Configuration for executing a demo pipeline is given in the file: pipe_configs/pipelines/pipeline.json This file contains the order of the execution of the pipeline components and references to its configuration files.</p> <pre><code>python multimno/orchestrator.py pipe_configs/pipelines/pipeline.json\n</code></pre> <p>This demo will create synthetic Event data and clean it under the path sample_data/lakehouse </p> <p>Synthetic event data will be created in: sample_data/lakehouse/bronze/mno_events </p> <p>Cleaned event data and the quality insights will be created in: sample_data/lakehouse/silver/mno_events </p> <p>A jupyter notebook is given for the results visualization. To use it, start a jupyterlab session with:</p> <pre><code>jl\n</code></pre> <p>Then go to http://localhost:${JL_PORT}/lab   * JL_PORT was defined in the .devcontainer/.env file. </p> <p>For example :http://localhost:8888/lab</p> <p>Then open the notebook: notebooks/demo_visualization.ipynb  and execute all cells.</p>"},{"location":"readme.html#clean-up","title":"Clean up","text":"<p>Exit the terminal with:</p> <p>Ctrl+D or writing <code>exit</code></p> <p>Delete the container created with:</p> <pre><code>docker compose -f .devcontainer/docker-compose.yml --env-file=.devcontainer/.env down\n</code></pre>"},{"location":"readme.html#production-deployment","title":"Production Deployment","text":"<p>TBD</p>"},{"location":"reference/SUMMARY.html","title":"SUMMARY","text":"<ul> <li>components<ul> <li>execution<ul> <li>event_cleaning<ul> <li>event_cleaning</li> </ul> </li> </ul> </li> <li>ingestion<ul> <li>synthetic<ul> <li>synthetic_events</li> </ul> </li> </ul> </li> <li>quality</li> </ul> </li> <li>core<ul> <li>component</li> <li>configuration</li> <li>constants<ul> <li>columns</li> <li>error_types</li> <li>transformations</li> </ul> </li> <li>data_objects<ul> <li>bronze<ul> <li>bronze_event_data_object</li> </ul> </li> <li>data_object</li> <li>landing</li> <li>silver<ul> <li>silver_event_data_object</li> <li>silver_event_data_syntactic_quality_metrics_by_column</li> <li>silver_event_data_syntactic_quality_metrics_frequency_distribution</li> </ul> </li> </ul> </li> <li>io_interface</li> <li>log</li> <li>settings</li> <li>spark_session</li> </ul> </li> <li>hello_world</li> <li>main</li> <li>orchestrator</li> </ul>"},{"location":"reference/hello_world.html","title":"hello_world","text":"<p>This module serves as an example of a hello world application for system testing purposes.</p> <p>Usage:</p> <pre><code>python multimno/hello_world.py\n</code></pre>"},{"location":"reference/hello_world.html#hello_world.build_local_session","title":"<code>build_local_session()</code>","text":"<p>Function that creates a Spark local session.</p> <p>Returns:</p> Name Type Description <code>SparkSession</code> <p>spark session</p> <code>SparkContext</code> <p>spark context</p> Source code in <code>multimno/hello_world.py</code> <pre><code>def build_local_session():\n    \"\"\"Function that creates a Spark local session.\n\n    Returns:\n        SparkSession: spark session\n        SparkContext: spark context\n    \"\"\"\n    builder = SedonaContext.builder().appName(\"Sedona Session\")\n    # Set sedona session\n    spark = SedonaContext.create(builder.getOrCreate())\n    sc = spark.sparkContext\n    sc.setSystemProperty(\"sedona.global.charset\", \"utf8\")\n    return spark, sc\n</code></pre>"},{"location":"reference/hello_world.html#hello_world.export_map","title":"<code>export_map(df, output_path)</code>","text":"<p>Function that exports a Sedona GeoDataframe to html.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe</p> required <code>output_path</code> <code>str</code> <p>path where the html map will be saved.</p> required Source code in <code>multimno/hello_world.py</code> <pre><code>def export_map(df, output_path: str):\n    \"\"\"Function that exports a Sedona GeoDataframe to html.\n\n    Args:\n        df (DataFrame): dataframe\n        output_path (str): path where the html map will be saved.\n    \"\"\"\n    fill_color = [255, 12, 250]\n    census_map = SedonaPyDeck.create_choropleth_map(df=df, fill_color=fill_color)\n    census_map.to_html(output_path)\n</code></pre>"},{"location":"reference/main.html","title":"main","text":"<p>Application entrypoint for launching a single component.</p> <p>Usage:</p> <pre><code>python multimno/main.py &lt;component_id&gt; &lt;general_config_path&gt; &lt;component_config_path&gt;\n</code></pre> <ul> <li>component_id: ID of the component to be executed.</li> <li>general_config_path: Path to a INI file with the general configuration of the execution.</li> <li>component_config_path: Path to a INI file with the specific configuration of the component.</li> </ul>"},{"location":"reference/orchestrator.html","title":"orchestrator","text":"<p>Module that orchestrates MultiMNO pipeline components. A spark-submit will be performed for each  component in the pipeline.</p> <p>Usage: </p> <pre><code>python multimno/orchestrator.py &lt;pipeline.json&gt;\n</code></pre> <ul> <li>pipeline.json: Path to a json file with the pipeline configuration.</li> </ul>"},{"location":"reference/components/index.html","title":"components","text":""},{"location":"reference/components/execution/index.html","title":"execution","text":""},{"location":"reference/components/execution/event_cleaning/index.html","title":"event_cleaning","text":""},{"location":"reference/components/execution/event_cleaning/event_cleaning.html","title":"event_cleaning","text":"<p>Module that cleans RAW MNO Event data.</p>"},{"location":"reference/components/execution/event_cleaning/event_cleaning.html#components.execution.event_cleaning.event_cleaning.EventCleaning","title":"<code>EventCleaning</code>","text":"<p>             Bases: <code>Component</code></p> <p>Class that cleans MNO Event data</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>class EventCleaning(Component):\n    \"\"\"\n    Class that cleans MNO Event data\n    \"\"\"\n\n    COMPONENT_ID = \"EventCleaning\"\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        super().__init__(general_config_path, component_config_path)\n\n        self.timestamp_format = self.config.get(EventCleaning.COMPONENT_ID, \"timestamp_format\")\n        self.input_timezone = self.config.get(EventCleaning.COMPONENT_ID, \"input_timezone\")\n\n        self.do_bounding_box_filtering = self.config.getboolean(\n            EventCleaning.COMPONENT_ID, \"do_bounding_box_filtering\", fallback=False\n        )\n        self.bounding_box = self.config.geteval(EventCleaning.COMPONENT_ID, \"bounding_box\")\n\n        self.spark_data_folder_date_format = self.config.get(\n            EventCleaning.COMPONENT_ID, \"spark_data_folder_date_format\"\n        )\n\n    def initalize_data_objects(self):\n        # Input\n        self.bronze_event_path = self.config.get(CONFIG_BRONZE_PATHS_KEY, \"event_data_bronze\")\n\n        self.data_period_start = self.config.get(EventCleaning.COMPONENT_ID, \"data_period_start\")\n        self.data_period_end = self.config.get(EventCleaning.COMPONENT_ID, \"data_period_end\")\n        self.data_folder_date_format = self.config.get(EventCleaning.COMPONENT_ID, \"data_folder_date_format\")\n        self.clear_destination_directory = self.config.get(EventCleaning.COMPONENT_ID, \"clear_destination_directory\")\n\n        # Create all possible dates between start and end\n        # It is suggested that data is already separated in date folders\n        # with names following self.data_folder_date_format (e.g. 20230101)\n        sdate = pd.to_datetime(self.data_period_start)\n        edate = pd.to_datetime(self.data_period_end)\n        self.to_process_dates = list(pd.date_range(sdate, edate, freq=\"d\"))\n\n        # Create all input data objects\n        self.input_event_data_objects = []\n        self.dates_to_process = []\n        for date in self.to_process_dates:\n            path = f\"{self.bronze_event_path}/year={date.year}/month={date.month}/day={date.day}\"\n            if check_if_data_path_exists(self.spark, path):\n                self.dates_to_process.append(date)\n                self.input_event_data_objects.append(BronzeEventDataObject(self.spark, path))\n            else:\n                self.logger.warning(f\"Expected path {path} to exist but it does not\")\n        # Output\n        self.output_data_objects = {}\n\n        silver_event_path = self.config.get(CONFIG_SILVER_PATHS_KEY, \"event_data_silver\")\n        silver_event_do = SilverEventDataObject(self.spark, silver_event_path)\n        self.output_data_objects[SilverEventDataObject.ID] = silver_event_do\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, silver_event_do.default_path)\n\n        event_syntactic_quality_metrics_by_column_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"event_syntactic_quality_metrics_by_column\"\n        )\n        event_syntactic_quality_metrics_by_column = SilverEventDataSyntacticQualityMetricsByColumn(\n            self.spark, event_syntactic_quality_metrics_by_column_path\n        )\n        self.output_data_objects[\n            SilverEventDataSyntacticQualityMetricsByColumn.ID\n        ] = event_syntactic_quality_metrics_by_column\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, event_syntactic_quality_metrics_by_column.default_path)\n\n        self.output_qa_by_column = event_syntactic_quality_metrics_by_column\n\n        event_syntactic_quality_metrics_frequency_distribution_path = self.config.get(\n            CONFIG_SILVER_PATHS_KEY, \"event_syntactic_quality_metrics_frequency_distribution\"\n        )\n        event_syntactic_quality_metrics_frequency_distribution = (\n            SilverEventDataSyntacticQualityMetricsFrequencyDistribution(\n                self.spark, event_syntactic_quality_metrics_frequency_distribution_path\n            )\n        )\n        if self.clear_destination_directory:\n            delete_file_or_folder(self.spark, event_syntactic_quality_metrics_frequency_distribution.default_path)\n\n        self.output_data_objects[\n            SilverEventDataSyntacticQualityMetricsFrequencyDistribution.ID\n        ] = event_syntactic_quality_metrics_frequency_distribution\n        # this instance of SilverEventDataSyntacticQualityMetricsFrequencyDistribution class\n        # will be used to write frequency distrobution of each preprocessing date (chunk)\n        # the path argument will be changed dynamically\n        self.output_qa_freq_distribution = event_syntactic_quality_metrics_frequency_distribution\n\n    def read(self):\n        self.current_input_do.read()\n\n    def write(self):\n        self.output_data_objects[SilverEventDataObject.ID].write()\n        self.save_syntactic_quality_metrics_frequency_distribution()\n\n    def execute(self):\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        for input_do, current_date in zip(self.input_event_data_objects, self.dates_to_process):\n            self.current_date = current_date\n            self.logger.info(f\"Reading from path {input_do.default_path}\")\n            self.current_input_do = input_do\n            self.read()\n            self.transform()  # Transforms the input_df\n            self.write()\n        self.save_syntactic_quality_metrics_by_column()\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def transform(self):\n        self.logger.info(f\"Transform method {self.COMPONENT_ID}\")\n        df_events = self.current_input_do.df\n        # cache before each filter function because we appply action count()\n        df_events = df_events.cache()\n\n        self.quality_metrics_distribution_before = df_events.groupBy(ColNames.cell_id, ColNames.user_id).agg(\n            psf.count(\"*\").alias(ColNames.initial_frequency)\n        )\n\n        df_events = self.filter_nulls_and_update_qa(\n            df_events,\n            [ColNames.user_id, ColNames.timestamp, ColNames.mcc],\n            self.output_qa_by_column.error_and_transformation_counts,\n        )\n\n        # MCC correct format verification: 3 digit value\n        df_events = self.filter_invalid_mcc_and_update_qa(\n            df_events, self.output_qa_by_column.error_and_transformation_counts\n        )\n\n        # already cached in previous function\n        df_events = self.filter_null_locations_and_update_qa(\n            df_events, self.output_qa_by_column.error_and_transformation_counts\n        )\n\n        df_events = df_events.cache()\n\n        # Remove rows with invalid cell_ids\n        df_events = self.filter_invalid_cell_id_and_update_qa(\n            df_events, self.output_qa_by_column.error_and_transformation_counts\n        )\n\n        df_events = df_events.cache()\n\n        df_events = self.convert_time_column_to_timestamp_and_update_qa(\n            df_events,\n            self.timestamp_format,\n            self.input_timezone,\n            self.output_qa_by_column.error_and_transformation_counts,\n        )\n\n        df_events = df_events.cache()\n        # TODO: discuss is this step even needed (did since it was in Method description)\n        df_events = self.data_period_filter_and_update_qa(\n            df_events,\n            self.data_period_start,\n            self.data_period_end,\n            self.output_qa_by_column.error_and_transformation_counts,\n        )\n\n        if self.do_bounding_box_filtering:\n            df_events = df_events.cache()\n            df_events = self.bounding_box_filtering_and_update_qa(\n                df_events, self.bounding_box, self.output_qa_by_column.error_and_transformation_counts\n            )\n\n        self.quality_metrics_distribution_after = df_events.groupBy(ColNames.cell_id, ColNames.user_id).agg(\n            psf.count(\"*\").alias(ColNames.final_frequency)\n        )\n\n        # TODO: discuss\n        # if we impose the rule on input data that data in a folder\n        # is of date specified in folder name - maybe better to use F.lit()\n        df_events = df_events.withColumns(\n            {\n                ColNames.year: psf.year(ColNames.timestamp).cast(\"smallint\"),\n                ColNames.month: psf.month(ColNames.timestamp).cast(\"tinyint\"),\n                ColNames.day: psf.dayofmonth(ColNames.timestamp).cast(\"tinyint\"),\n            }\n        )\n\n        df_events = df_events.sort([ColNames.user_id, ColNames.timestamp])\n        self.output_data_objects[SilverEventDataObject.ID].df = self.spark.createDataFrame(\n            df_events.rdd, SilverEventDataObject.SCHEMA\n        )\n        # after each chunk processing clear all Cache to free memory and disk\n        self.spark.catalog.clearCache()\n\n    def save_syntactic_quality_metrics_frequency_distribution(self):\n        \"\"\"\n        Join frequency distribution tables before and after,\n        from after table take only final_frequency and replace nulls with 0.\n        Create additional column date in DateType(),\n        match the schema of SilverEventDataSyntacticQualityMetricsByColumn class\n        Write chunk results in separate folders named by processing date\n        \"\"\"\n\n        self.output_qa_freq_distribution.df = self.quality_metrics_distribution_before.join(\n            self.quality_metrics_distribution_after, [ColNames.cell_id, ColNames.user_id], \"left\"\n        ).select(self.quality_metrics_distribution_before.columns + [ColNames.final_frequency])\n\n        self.output_qa_freq_distribution.df = self.output_qa_freq_distribution.df.fillna(0, ColNames.final_frequency)\n\n        self.output_qa_freq_distribution.df = self.output_qa_freq_distribution.df.withColumn(\n            ColNames.date, psf.to_date(psf.lit(self.current_date), self.spark_data_folder_date_format)\n        )\n\n        self.output_qa_freq_distribution.df = self.spark.createDataFrame(\n            self.output_qa_freq_distribution.df.rdd, self.output_qa_freq_distribution.SCHEMA\n        )\n\n        self.output_qa_freq_distribution.write()\n\n    def save_syntactic_quality_metrics_by_column(self):\n        \"\"\"\n        Convert output_qa_by_column.error_and_transformation_counts dictionary into spark df.\n        Add additional columns, match schema of SilverEventDataSyntacticQualityMetricsByColumn class.\n        Write results in default path of this class\n        \"\"\"\n        # self.output_qa_by_column( variable, type_of_error, type_of_transformation) : value\n        # 3 Nones to match the expected schema\n        df_tuples = [\n            (variable, type_of_error, type_of_transformation, value)\n            for (\n                variable,\n                type_of_error,\n                type_of_transformation,\n            ), value in self.output_qa_by_column.error_and_transformation_counts.items()\n        ]\n\n        temp_schema = StructType(\n            [\n                StructField(ColNames.variable, StringType(), nullable=True),\n                StructField(ColNames.type_of_error, ShortType(), nullable=True),\n                StructField(ColNames.type_of_transformation, ShortType(), nullable=True),\n                StructField(ColNames.value, IntegerType(), nullable=False),\n            ]\n        )\n\n        self.output_qa_by_column.df = self.spark.createDataFrame(df_tuples, temp_schema)\n\n        self.output_qa_by_column.df = self.output_qa_by_column.df.withColumns(\n            {\n                ColNames.result_timestamp: psf.lit(psf.current_timestamp()),\n                ColNames.data_period_start: psf.to_date(psf.lit(self.data_period_start)),\n                ColNames.data_period_end: psf.to_date(psf.lit(self.data_period_end)),\n            }\n        ).select(self.output_qa_by_column.SCHEMA.fieldNames())\n\n        self.output_qa_by_column.df = self.spark.createDataFrame(\n            self.output_qa_by_column.df.rdd, self.output_qa_by_column.SCHEMA\n        )\n\n        self.output_qa_by_column.write()\n\n    def filter_nulls_and_update_qa(\n        self,\n        df: pyspark.sql.dataframe.DataFrame,\n        filter_columns: list[str],\n        error_and_transformation_counts: dict[tuple:int],\n    ) -&gt; pyspark.sql.dataframe.DataFrame:\n        \"\"\"\n        Loop throuh filter columns (user_id, timestamp)\n        delete rows that have null in the corresponfing column.\n        Counts the number of filtered records for quality metrics,\n        for user_id also calculates the overall correct values,\n        since null check is the one and only filter for this column\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): df events with possible nulls values\n            filter_columns (list[str], optional): columns to check for nulls\n            error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n        Returns:\n            pyspark.sql.dataframe.DataFrame: df without null values in specified columns\n        \"\"\"\n\n        for filter_column in filter_columns:\n            filtered_df = df.na.drop(how=\"any\", subset=filter_column)\n            error_and_transformation_counts[(filter_column, ErrorTypes.missing_value, None)] += (\n                df.count() - filtered_df.count()\n            )\n            # because timestamp column is then also used in another filters, and no error count should be done in the last filter\n            if filter_column not in [ColNames.timestamp, ColNames.mcc]:\n                error_and_transformation_counts[(filter_column, ErrorTypes.no_error, None)] += filtered_df.count()\n\n            df = filtered_df.cache()\n\n        return df\n\n    def filter_invalid_mcc_and_update_qa(\n        self, df: pyspark.sql.dataframe.DataFrame, error_and_transformation_counts: dict[tuple:int]\n    ) -&gt; pyspark.sql.dataframe.DataFrame:\n        \"\"\"\n        Remove any rows where the value for mcc is not a 3 digit number (between 100 and 999)\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): dataframe of events\n            error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: Dataframe with erroneous values of mcc filtered out\n        \"\"\"\n\n        filtered_df = df.filter(psf.col(ColNames.mcc).between(100, 999))\n        error_and_transformation_counts[(ColNames.mcc, ErrorTypes.out_of_admissible_values, None)] += (\n            df.count() - filtered_df.count()\n        )\n        # because timestamp column is then also used in another filters,\n        # and no error count should be done in the last filter\n        error_and_transformation_counts[(ColNames.mcc, ErrorTypes.no_error, None)] += filtered_df.count()\n\n        df = filtered_df.cache()\n\n        return df\n\n    def filter_invalid_cell_id_and_update_qa(\n        self, df: pyspark.sql.dataframe.DataFrame, error_and_transformation_counts: dict[tuple:int]\n    ) -&gt; pyspark.sql.dataframe.DataFrame:\n        \"\"\"\n        Remove any rows where the value for cell_id is not a 15 digit number\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): dataframe of events\n            error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: Dataframe with erroneous values of cell_id filtered out\n        \"\"\"\n\n        filtered_df = df.filter(\n            (\n                (psf.length(psf.col(ColNames.cell_id)) == 15) &amp; (psf.col(ColNames.cell_id).cast(\"long\").isNotNull())\n                | psf.col(\"cell_id\").isNull()\n            )\n        )\n        error_and_transformation_counts[(ColNames.cell_id, ErrorTypes.out_of_admissible_values, None)] += (\n            df.count() - filtered_df.count()\n        )\n        # because timestamp column is then also used in another filters,\n        # and no error count should be done in the last filter\n        error_and_transformation_counts[(ColNames.cell_id, ErrorTypes.no_error, None)] += filtered_df.count()\n\n        df = filtered_df.cache()\n\n        return df\n\n    def filter_null_locations_and_update_qa(\n        self, df: pyspark.sql.dataframe.DataFrame, error_and_transformation_counts: dict[tuple:int]\n    ) -&gt; pyspark.sql.dataframe.DataFrame:\n        \"\"\"\n        Filter rows with inappropriate location: neither cell_id\n        nor latitude&amp;longitude are specified. Count the number of errors,\n        since this filter depends on several columns \"variable\" in quality metrics table will be None.\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): df with possible null location columns\n            error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: filtered df with correctly specified location\n        \"\"\"\n\n        filtered_df = df.filter(\n            (psf.col(ColNames.cell_id).isNotNull())\n            | (psf.col(ColNames.longitude).isNotNull() &amp; psf.col(ColNames.latitude).isNotNull())\n        )\n\n        error_and_transformation_counts[(None, ErrorTypes.no_location, None)] += df.count() - filtered_df.count()\n\n        return filtered_df\n\n    def convert_time_column_to_timestamp_and_update_qa(\n        self,\n        df: pyspark.sql.dataframe.DataFrame,\n        timestampt_format: str,\n        input_timezone: str,\n        error_and_transformation_counts: dict[tuple:int],\n    ) -&gt; pyspark.sql.dataframe.DataFrame:\n        \"\"\"\n        Based on config params timestampt format and input timezone\n        convert timestampt column from string to timestampt type, if filter rows with failed conversion.\n        Count number of succesful timestampt transformations and number of errors.\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): df with timestampt column\n            timestampt_format (str): expected string format to use in time conversion\n            input_timezone (str): timezone of the input data\n            error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: df with time column parsed to timestamp\n        \"\"\"\n\n        # TODO: Check timestamp validation\n        # Validating timestamp of format '2023-01-03T03:12:00+00:00' raises Exception\n        # Error: Fail to parse '2023-01-03T03:12:00+00:00' in the new parser\n        # Can we use a conditional(psf.when) to check if column can be casted?\n        filtered_df = df.withColumn(\n            ColNames.timestamp,\n            psf.to_utc_timestamp(psf.to_timestamp(ColNames.timestamp, timestampt_format), input_timezone),\n        ).filter(psf.col(ColNames.timestamp).isNotNull())\n\n        error_and_transformation_counts[\n            (ColNames.timestamp, None, Transformations.converted_timestamp)\n        ] += filtered_df.count()\n        error_and_transformation_counts[(ColNames.timestamp, ErrorTypes.not_right_syntactic_format, None)] += (\n            df.count() - filtered_df.count()\n        )\n\n        return filtered_df\n\n    def data_period_filter_and_update_qa(\n        self,\n        df: pyspark.sql.dataframe.DataFrame,\n        data_period_start: str,\n        data_period_end: str,\n        error_and_transformation_counts: dict[tuple:int],\n    ) -&gt; pyspark.sql.dataframe.DataFrame:\n        \"\"\"\n        Filter rows which timestampt is not in specified date range.\n        Count the number of error rows for quality metrics,\n        and the number of complitely correct timestampt (that pass all corresponding filters)\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): df with timestamp column\n            data_period_start (str): start of date period\n            data_period_end (str): end of date period\n            error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n        Returns:\n            pyspark.sql.dataframe.DataFrame: df with records in specified date period\n        \"\"\"\n        data_period_start = pd.to_datetime(data_period_start)\n        # timedelta is needed to include records happened in data_period_end\n        data_period_end = pd.to_datetime(data_period_end) + pd.Timedelta(days=1)\n        filtered_df = df.filter(psf.col(ColNames.timestamp).between(data_period_start, data_period_end))\n\n        error_and_transformation_counts[(ColNames.timestamp, ErrorTypes.out_of_admissible_values, None)] += (\n            df.count() - filtered_df.count()\n        )\n        # TODO: if we decide not to use data_period_filtering don't forget to put this in convert_time_column_to_timestamp\n        error_and_transformation_counts[(ColNames.timestamp, ErrorTypes.no_error, None)] += filtered_df.count()\n\n        return filtered_df\n\n    def bounding_box_filtering_and_update_qa(\n        self, df: pyspark.sql.dataframe.DataFrame, bounding_box: dict, error_and_transformation_counts: dict[tuple:int]\n    ) -&gt; pyspark.sql.dataframe.DataFrame:\n        \"\"\"\n        Filter rows which not null longitude &amp; latitude values are\n        within coordinates of bounding box. Count the number of errors,\n        since this filter depends on several columns \"variable\" in quality metrics table will be None\n\n        Args:\n            df (pyspark.sql.dataframe.DataFrame): df with longitude &amp; latitude columns\n            bounding_box (dict): coordinates of bounding box in df_events crs\n            error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n\n        Returns:\n            pyspark.sql.dataframe.DataFrame: filtered df with records within bbox\n        \"\"\"\n        # coordinates of bounding box should be of the same crs of mno data\n        lat_condition = (\n            psf.col(ColNames.latitude).between(bounding_box[\"min_lat\"], bounding_box[\"max_lat\"])\n            | psf.col(ColNames.latitude).isNull()\n        )\n        lon_condition = (\n            psf.col(ColNames.longitude).between(bounding_box[\"min_lon\"], bounding_box[\"max_lon\"])\n            | psf.col(ColNames.longitude).isNull()\n        )\n\n        filtered_df = df.filter(lat_condition &amp; lon_condition)\n        error_and_transformation_counts[(None, ErrorTypes.out_of_bounding_box, None)] += (\n            df.count() - filtered_df.count()\n        )\n\n        return filtered_df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning.html#components.execution.event_cleaning.event_cleaning.EventCleaning.bounding_box_filtering_and_update_qa","title":"<code>bounding_box_filtering_and_update_qa(df, bounding_box, error_and_transformation_counts)</code>","text":"<p>Filter rows which not null longitude &amp; latitude values are within coordinates of bounding box. Count the number of errors, since this filter depends on several columns \"variable\" in quality metrics table will be None</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>df with longitude &amp; latitude columns</p> required <code>bounding_box</code> <code>dict</code> <p>coordinates of bounding box in df_events crs</p> required <code>error_and_transformation_counts</code> <code>dict[tuple, int]</code> <p>dictionary to track qa</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: filtered df with records within bbox</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>def bounding_box_filtering_and_update_qa(\n    self, df: pyspark.sql.dataframe.DataFrame, bounding_box: dict, error_and_transformation_counts: dict[tuple:int]\n) -&gt; pyspark.sql.dataframe.DataFrame:\n    \"\"\"\n    Filter rows which not null longitude &amp; latitude values are\n    within coordinates of bounding box. Count the number of errors,\n    since this filter depends on several columns \"variable\" in quality metrics table will be None\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): df with longitude &amp; latitude columns\n        bounding_box (dict): coordinates of bounding box in df_events crs\n        error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: filtered df with records within bbox\n    \"\"\"\n    # coordinates of bounding box should be of the same crs of mno data\n    lat_condition = (\n        psf.col(ColNames.latitude).between(bounding_box[\"min_lat\"], bounding_box[\"max_lat\"])\n        | psf.col(ColNames.latitude).isNull()\n    )\n    lon_condition = (\n        psf.col(ColNames.longitude).between(bounding_box[\"min_lon\"], bounding_box[\"max_lon\"])\n        | psf.col(ColNames.longitude).isNull()\n    )\n\n    filtered_df = df.filter(lat_condition &amp; lon_condition)\n    error_and_transformation_counts[(None, ErrorTypes.out_of_bounding_box, None)] += (\n        df.count() - filtered_df.count()\n    )\n\n    return filtered_df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning.html#components.execution.event_cleaning.event_cleaning.EventCleaning.convert_time_column_to_timestamp_and_update_qa","title":"<code>convert_time_column_to_timestamp_and_update_qa(df, timestampt_format, input_timezone, error_and_transformation_counts)</code>","text":"<p>Based on config params timestampt format and input timezone convert timestampt column from string to timestampt type, if filter rows with failed conversion. Count number of succesful timestampt transformations and number of errors.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>df with timestampt column</p> required <code>timestampt_format</code> <code>str</code> <p>expected string format to use in time conversion</p> required <code>input_timezone</code> <code>str</code> <p>timezone of the input data</p> required <code>error_and_transformation_counts</code> <code>dict[tuple, int]</code> <p>dictionary to track qa</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: df with time column parsed to timestamp</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>def convert_time_column_to_timestamp_and_update_qa(\n    self,\n    df: pyspark.sql.dataframe.DataFrame,\n    timestampt_format: str,\n    input_timezone: str,\n    error_and_transformation_counts: dict[tuple:int],\n) -&gt; pyspark.sql.dataframe.DataFrame:\n    \"\"\"\n    Based on config params timestampt format and input timezone\n    convert timestampt column from string to timestampt type, if filter rows with failed conversion.\n    Count number of succesful timestampt transformations and number of errors.\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): df with timestampt column\n        timestampt_format (str): expected string format to use in time conversion\n        input_timezone (str): timezone of the input data\n        error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: df with time column parsed to timestamp\n    \"\"\"\n\n    # TODO: Check timestamp validation\n    # Validating timestamp of format '2023-01-03T03:12:00+00:00' raises Exception\n    # Error: Fail to parse '2023-01-03T03:12:00+00:00' in the new parser\n    # Can we use a conditional(psf.when) to check if column can be casted?\n    filtered_df = df.withColumn(\n        ColNames.timestamp,\n        psf.to_utc_timestamp(psf.to_timestamp(ColNames.timestamp, timestampt_format), input_timezone),\n    ).filter(psf.col(ColNames.timestamp).isNotNull())\n\n    error_and_transformation_counts[\n        (ColNames.timestamp, None, Transformations.converted_timestamp)\n    ] += filtered_df.count()\n    error_and_transformation_counts[(ColNames.timestamp, ErrorTypes.not_right_syntactic_format, None)] += (\n        df.count() - filtered_df.count()\n    )\n\n    return filtered_df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning.html#components.execution.event_cleaning.event_cleaning.EventCleaning.data_period_filter_and_update_qa","title":"<code>data_period_filter_and_update_qa(df, data_period_start, data_period_end, error_and_transformation_counts)</code>","text":"<p>Filter rows which timestampt is not in specified date range. Count the number of error rows for quality metrics, and the number of complitely correct timestampt (that pass all corresponding filters)</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>df with timestamp column</p> required <code>data_period_start</code> <code>str</code> <p>start of date period</p> required <code>data_period_end</code> <code>str</code> <p>end of date period</p> required <code>error_and_transformation_counts</code> <code>dict[tuple, int]</code> <p>dictionary to track qa</p> required <p>Returns:     pyspark.sql.dataframe.DataFrame: df with records in specified date period</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>def data_period_filter_and_update_qa(\n    self,\n    df: pyspark.sql.dataframe.DataFrame,\n    data_period_start: str,\n    data_period_end: str,\n    error_and_transformation_counts: dict[tuple:int],\n) -&gt; pyspark.sql.dataframe.DataFrame:\n    \"\"\"\n    Filter rows which timestampt is not in specified date range.\n    Count the number of error rows for quality metrics,\n    and the number of complitely correct timestampt (that pass all corresponding filters)\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): df with timestamp column\n        data_period_start (str): start of date period\n        data_period_end (str): end of date period\n        error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n    Returns:\n        pyspark.sql.dataframe.DataFrame: df with records in specified date period\n    \"\"\"\n    data_period_start = pd.to_datetime(data_period_start)\n    # timedelta is needed to include records happened in data_period_end\n    data_period_end = pd.to_datetime(data_period_end) + pd.Timedelta(days=1)\n    filtered_df = df.filter(psf.col(ColNames.timestamp).between(data_period_start, data_period_end))\n\n    error_and_transformation_counts[(ColNames.timestamp, ErrorTypes.out_of_admissible_values, None)] += (\n        df.count() - filtered_df.count()\n    )\n    # TODO: if we decide not to use data_period_filtering don't forget to put this in convert_time_column_to_timestamp\n    error_and_transformation_counts[(ColNames.timestamp, ErrorTypes.no_error, None)] += filtered_df.count()\n\n    return filtered_df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning.html#components.execution.event_cleaning.event_cleaning.EventCleaning.filter_invalid_cell_id_and_update_qa","title":"<code>filter_invalid_cell_id_and_update_qa(df, error_and_transformation_counts)</code>","text":"<p>Remove any rows where the value for cell_id is not a 15 digit number</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe of events</p> required <code>error_and_transformation_counts</code> <code>dict[tuple, int]</code> <p>dictionary to track qa</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: Dataframe with erroneous values of cell_id filtered out</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>def filter_invalid_cell_id_and_update_qa(\n    self, df: pyspark.sql.dataframe.DataFrame, error_and_transformation_counts: dict[tuple:int]\n) -&gt; pyspark.sql.dataframe.DataFrame:\n    \"\"\"\n    Remove any rows where the value for cell_id is not a 15 digit number\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): dataframe of events\n        error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: Dataframe with erroneous values of cell_id filtered out\n    \"\"\"\n\n    filtered_df = df.filter(\n        (\n            (psf.length(psf.col(ColNames.cell_id)) == 15) &amp; (psf.col(ColNames.cell_id).cast(\"long\").isNotNull())\n            | psf.col(\"cell_id\").isNull()\n        )\n    )\n    error_and_transformation_counts[(ColNames.cell_id, ErrorTypes.out_of_admissible_values, None)] += (\n        df.count() - filtered_df.count()\n    )\n    # because timestamp column is then also used in another filters,\n    # and no error count should be done in the last filter\n    error_and_transformation_counts[(ColNames.cell_id, ErrorTypes.no_error, None)] += filtered_df.count()\n\n    df = filtered_df.cache()\n\n    return df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning.html#components.execution.event_cleaning.event_cleaning.EventCleaning.filter_invalid_mcc_and_update_qa","title":"<code>filter_invalid_mcc_and_update_qa(df, error_and_transformation_counts)</code>","text":"<p>Remove any rows where the value for mcc is not a 3 digit number (between 100 and 999)</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe of events</p> required <code>error_and_transformation_counts</code> <code>dict[tuple, int]</code> <p>dictionary to track qa</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: Dataframe with erroneous values of mcc filtered out</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>def filter_invalid_mcc_and_update_qa(\n    self, df: pyspark.sql.dataframe.DataFrame, error_and_transformation_counts: dict[tuple:int]\n) -&gt; pyspark.sql.dataframe.DataFrame:\n    \"\"\"\n    Remove any rows where the value for mcc is not a 3 digit number (between 100 and 999)\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): dataframe of events\n        error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: Dataframe with erroneous values of mcc filtered out\n    \"\"\"\n\n    filtered_df = df.filter(psf.col(ColNames.mcc).between(100, 999))\n    error_and_transformation_counts[(ColNames.mcc, ErrorTypes.out_of_admissible_values, None)] += (\n        df.count() - filtered_df.count()\n    )\n    # because timestamp column is then also used in another filters,\n    # and no error count should be done in the last filter\n    error_and_transformation_counts[(ColNames.mcc, ErrorTypes.no_error, None)] += filtered_df.count()\n\n    df = filtered_df.cache()\n\n    return df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning.html#components.execution.event_cleaning.event_cleaning.EventCleaning.filter_null_locations_and_update_qa","title":"<code>filter_null_locations_and_update_qa(df, error_and_transformation_counts)</code>","text":"<p>Filter rows with inappropriate location: neither cell_id nor latitude&amp;longitude are specified. Count the number of errors, since this filter depends on several columns \"variable\" in quality metrics table will be None.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>df with possible null location columns</p> required <code>error_and_transformation_counts</code> <code>dict[tuple, int]</code> <p>dictionary to track qa</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.dataframe.DataFrame: filtered df with correctly specified location</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>def filter_null_locations_and_update_qa(\n    self, df: pyspark.sql.dataframe.DataFrame, error_and_transformation_counts: dict[tuple:int]\n) -&gt; pyspark.sql.dataframe.DataFrame:\n    \"\"\"\n    Filter rows with inappropriate location: neither cell_id\n    nor latitude&amp;longitude are specified. Count the number of errors,\n    since this filter depends on several columns \"variable\" in quality metrics table will be None.\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): df with possible null location columns\n        error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n\n    Returns:\n        pyspark.sql.dataframe.DataFrame: filtered df with correctly specified location\n    \"\"\"\n\n    filtered_df = df.filter(\n        (psf.col(ColNames.cell_id).isNotNull())\n        | (psf.col(ColNames.longitude).isNotNull() &amp; psf.col(ColNames.latitude).isNotNull())\n    )\n\n    error_and_transformation_counts[(None, ErrorTypes.no_location, None)] += df.count() - filtered_df.count()\n\n    return filtered_df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning.html#components.execution.event_cleaning.event_cleaning.EventCleaning.filter_nulls_and_update_qa","title":"<code>filter_nulls_and_update_qa(df, filter_columns, error_and_transformation_counts)</code>","text":"<p>Loop throuh filter columns (user_id, timestamp) delete rows that have null in the corresponfing column. Counts the number of filtered records for quality metrics, for user_id also calculates the overall correct values, since null check is the one and only filter for this column</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>df events with possible nulls values</p> required <code>filter_columns</code> <code>list[str]</code> <p>columns to check for nulls</p> required <code>error_and_transformation_counts</code> <code>dict[tuple, int]</code> <p>dictionary to track qa</p> required <p>Returns:     pyspark.sql.dataframe.DataFrame: df without null values in specified columns</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>def filter_nulls_and_update_qa(\n    self,\n    df: pyspark.sql.dataframe.DataFrame,\n    filter_columns: list[str],\n    error_and_transformation_counts: dict[tuple:int],\n) -&gt; pyspark.sql.dataframe.DataFrame:\n    \"\"\"\n    Loop throuh filter columns (user_id, timestamp)\n    delete rows that have null in the corresponfing column.\n    Counts the number of filtered records for quality metrics,\n    for user_id also calculates the overall correct values,\n    since null check is the one and only filter for this column\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame): df events with possible nulls values\n        filter_columns (list[str], optional): columns to check for nulls\n        error_and_transformation_counts (dict[tuple, int]): dictionary to track qa\n    Returns:\n        pyspark.sql.dataframe.DataFrame: df without null values in specified columns\n    \"\"\"\n\n    for filter_column in filter_columns:\n        filtered_df = df.na.drop(how=\"any\", subset=filter_column)\n        error_and_transformation_counts[(filter_column, ErrorTypes.missing_value, None)] += (\n            df.count() - filtered_df.count()\n        )\n        # because timestamp column is then also used in another filters, and no error count should be done in the last filter\n        if filter_column not in [ColNames.timestamp, ColNames.mcc]:\n            error_and_transformation_counts[(filter_column, ErrorTypes.no_error, None)] += filtered_df.count()\n\n        df = filtered_df.cache()\n\n    return df\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning.html#components.execution.event_cleaning.event_cleaning.EventCleaning.save_syntactic_quality_metrics_by_column","title":"<code>save_syntactic_quality_metrics_by_column()</code>","text":"<p>Convert output_qa_by_column.error_and_transformation_counts dictionary into spark df. Add additional columns, match schema of SilverEventDataSyntacticQualityMetricsByColumn class. Write results in default path of this class</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>def save_syntactic_quality_metrics_by_column(self):\n    \"\"\"\n    Convert output_qa_by_column.error_and_transformation_counts dictionary into spark df.\n    Add additional columns, match schema of SilverEventDataSyntacticQualityMetricsByColumn class.\n    Write results in default path of this class\n    \"\"\"\n    # self.output_qa_by_column( variable, type_of_error, type_of_transformation) : value\n    # 3 Nones to match the expected schema\n    df_tuples = [\n        (variable, type_of_error, type_of_transformation, value)\n        for (\n            variable,\n            type_of_error,\n            type_of_transformation,\n        ), value in self.output_qa_by_column.error_and_transformation_counts.items()\n    ]\n\n    temp_schema = StructType(\n        [\n            StructField(ColNames.variable, StringType(), nullable=True),\n            StructField(ColNames.type_of_error, ShortType(), nullable=True),\n            StructField(ColNames.type_of_transformation, ShortType(), nullable=True),\n            StructField(ColNames.value, IntegerType(), nullable=False),\n        ]\n    )\n\n    self.output_qa_by_column.df = self.spark.createDataFrame(df_tuples, temp_schema)\n\n    self.output_qa_by_column.df = self.output_qa_by_column.df.withColumns(\n        {\n            ColNames.result_timestamp: psf.lit(psf.current_timestamp()),\n            ColNames.data_period_start: psf.to_date(psf.lit(self.data_period_start)),\n            ColNames.data_period_end: psf.to_date(psf.lit(self.data_period_end)),\n        }\n    ).select(self.output_qa_by_column.SCHEMA.fieldNames())\n\n    self.output_qa_by_column.df = self.spark.createDataFrame(\n        self.output_qa_by_column.df.rdd, self.output_qa_by_column.SCHEMA\n    )\n\n    self.output_qa_by_column.write()\n</code></pre>"},{"location":"reference/components/execution/event_cleaning/event_cleaning.html#components.execution.event_cleaning.event_cleaning.EventCleaning.save_syntactic_quality_metrics_frequency_distribution","title":"<code>save_syntactic_quality_metrics_frequency_distribution()</code>","text":"<p>Join frequency distribution tables before and after, from after table take only final_frequency and replace nulls with 0. Create additional column date in DateType(), match the schema of SilverEventDataSyntacticQualityMetricsByColumn class Write chunk results in separate folders named by processing date</p> Source code in <code>multimno/components/execution/event_cleaning/event_cleaning.py</code> <pre><code>def save_syntactic_quality_metrics_frequency_distribution(self):\n    \"\"\"\n    Join frequency distribution tables before and after,\n    from after table take only final_frequency and replace nulls with 0.\n    Create additional column date in DateType(),\n    match the schema of SilverEventDataSyntacticQualityMetricsByColumn class\n    Write chunk results in separate folders named by processing date\n    \"\"\"\n\n    self.output_qa_freq_distribution.df = self.quality_metrics_distribution_before.join(\n        self.quality_metrics_distribution_after, [ColNames.cell_id, ColNames.user_id], \"left\"\n    ).select(self.quality_metrics_distribution_before.columns + [ColNames.final_frequency])\n\n    self.output_qa_freq_distribution.df = self.output_qa_freq_distribution.df.fillna(0, ColNames.final_frequency)\n\n    self.output_qa_freq_distribution.df = self.output_qa_freq_distribution.df.withColumn(\n        ColNames.date, psf.to_date(psf.lit(self.current_date), self.spark_data_folder_date_format)\n    )\n\n    self.output_qa_freq_distribution.df = self.spark.createDataFrame(\n        self.output_qa_freq_distribution.df.rdd, self.output_qa_freq_distribution.SCHEMA\n    )\n\n    self.output_qa_freq_distribution.write()\n</code></pre>"},{"location":"reference/components/ingestion/index.html","title":"ingestion","text":""},{"location":"reference/components/ingestion/synthetic/index.html","title":"synthetic","text":""},{"location":"reference/components/ingestion/synthetic/synthetic_events.html","title":"synthetic_events","text":""},{"location":"reference/components/ingestion/synthetic/synthetic_events.html#components.ingestion.synthetic.synthetic_events.LocationGeneratorType","title":"<code>LocationGeneratorType</code>","text":"<p>             Bases: <code>Enum</code></p> <p>Location Generator enumeration class.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>class LocationGeneratorType(Enum):\n    \"\"\"\n    Location Generator enumeration class.\n    \"\"\"\n\n    RANDOM_CELL_ID = \"random_cell_id\"\n    RANDOM_LAT_LON = \"random_lat_lon\"\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events.html#components.ingestion.synthetic.synthetic_events.SyntheticEvents","title":"<code>SyntheticEvents</code>","text":"<p>             Bases: <code>Component</code></p> <p>Class that generates the event synthetic data. It inherits from the Component abstract class.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>class SyntheticEvents(Component):\n    \"\"\"\n    Class that generates the event synthetic data. It inherits from the Component abstract class.\n    \"\"\"\n\n    COMPONENT_ID = \"SyntheticEvents\"\n\n    def __init__(self, general_config_path: str, component_config_path: str):\n        super().__init__(general_config_path=general_config_path, component_config_path=component_config_path)\n        self.seed = self.config.getint(self.COMPONENT_ID, \"seed\")\n        self.n_agents = self.config.getint(self.COMPONENT_ID, \"n_agents\")\n        self.n_events_per_agent = self.config.getint(self.COMPONENT_ID, \"n_events_per_agent\")\n        self.n_partitions = self.config.getint(self.COMPONENT_ID, \"n_partitions\")\n        self.timestamp_format = self.config.get(self.COMPONENT_ID, \"timestamp_format\")\n        self.starting_timestamp = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"starting_timestamp\"), self.timestamp_format\n        )\n        self.ending_timestamp = datetime.datetime.strptime(\n            self.config.get(self.COMPONENT_ID, \"ending_timestamp\"), self.timestamp_format\n        )\n\n        # Handle timestamp generation parameters.\n        # TODO support for other timestamp generation methods\n        timestamp_generator_type_str = self.config.get(self.COMPONENT_ID, \"timestamp_generator_type\")\n        try:\n            timestamp_generator_type = TimestampGeneratorType(timestamp_generator_type_str)\n        except:\n            raise ValueError(\n                f\"Unsupported timestamp_generator_type: {timestamp_generator_type}. Supported types are: {[e.value for e in TimestampGeneratorType]}\"\n            )\n        if timestamp_generator_type == TimestampGeneratorType.EQUAL_GAPS:\n            self.timestamp_generator_params = (\n                timestamp_generator_type.value,\n                self.starting_timestamp,\n                self.ending_timestamp,\n            )\n\n        # Handle location generation parameters.\n        location_generator_type_str = self.config.get(self.COMPONENT_ID, \"location_generator_type\")\n        try:\n            locationGenerator = LocationGeneratorType(location_generator_type_str)\n        except:\n            raise ValueError(\n                f\"Unsupported location_generator_type: {location_generator_type_str}. Supported types are: {[e.value for e in LocationGeneratorType]}\"\n            )\n        if locationGenerator == LocationGeneratorType.RANDOM_CELL_ID:\n            cell_id_min = self.config.getint(self.COMPONENT_ID, \"cell_id_min\")\n            cell_id_max = self.config.getint(self.COMPONENT_ID, \"cell_id_max\")\n            self.location_generator_params = (locationGenerator.value, cell_id_min, cell_id_max)\n        elif locationGenerator == LocationGeneratorType.RANDOM_LAT_LON:\n            latitude_min = float(self.config.get(self.COMPONENT_ID, \"latitude_min\"))\n            latitude_max = float(self.config.get(self.COMPONENT_ID, \"latitude_max\"))\n            longitude_min = float(self.config.get(self.COMPONENT_ID, \"longitude_min\"))\n            longitude_max = float(self.config.get(self.COMPONENT_ID, \"longitude_max\"))\n            self.location_generator_params = (\n                locationGenerator.value,\n                latitude_min,\n                latitude_max,\n                longitude_min,\n                longitude_max,\n            )\n\n        # Will we need better mcc generation later?\n        self.mcc = self.config.getint(self.COMPONENT_ID, \"mcc\")\n\n        # Error generation parameters.\n        self.do_error_generation = self.config.getboolean(self.COMPONENT_ID, \"do_error_generation\")\n        self.max_ratio_of_mandatory_columns = self.config.getfloat(\n            self.COMPONENT_ID, \"max_ratio_of_mandatory_columns_to_generate_as_null\"\n        )\n        self.null_row_prob = self.config.getfloat(self.COMPONENT_ID, \"null_row_probability\")\n        self.error_prob = self.config.getfloat(self.COMPONENT_ID, \"data_type_error_probability\")\n        self.out_of_bounds_prob = self.config.getfloat(self.COMPONENT_ID, \"out_of_bounds_probability\")\n        self.mandatory_columns = [i.name for i in BronzeEventDataObject.SCHEMA]\n        self.sort_output = self.config.getboolean(self.COMPONENT_ID, \"sort_output\")\n\n    def initalize_data_objects(self):\n        output_records_path = self.config.get(self.COMPONENT_ID, \"output_records_path\")\n\n        # TODO csv interface support needed ?\n        bronze_event = BronzeEventDataObject(\n            self.spark, output_records_path, partition_columns=[ColNames.year, ColNames.month, ColNames.day]\n        )  # ParquetInterface()\n\n        self.output_data_objects = {\"SyntheticEvents\": bronze_event}\n\n    def read(self):\n        pass  # No input datasets are used in this component\n\n    def transform(self):\n        spark = self.spark\n\n        # Initialize each agent, generate Spark dataframe\n        agents = self.generate_agents()\n        agents_df = spark.createDataFrame(agents)\n        # Generate events for each agent. Since the UDF generates a list, it has to be exploded to separate the rows.\n        records_df = agents_df.withColumn(\n            \"record_tuple\",\n            explode(\n                generate_agent_records(\n                    \"user_id\",\n                    \"n_events\",\n                    \"starting_event_id\",\n                    \"random_seed\",\n                    \"timestamp_generator_params\",\n                    \"location_generator_params\",\n                )\n            ),\n        ).select([\"*\", \"record_tuple.*\"])\n\n        # TODO add loc_error non-null value generation.\n        records_df = records_df.withColumn(ColNames.loc_error, lit(None).cast(FloatType()))\n\n        records_df = self.calc_hashed_user_id(records_df)\n        records_df = records_df.withColumn(ColNames.timestamp, col(ColNames.timestamp).cast(StringType()))\n        records_df = records_df.withColumn(ColNames.cell_id, col(ColNames.cell_id).cast(StringType()))\n        records_df = records_df.withColumn(ColNames.mcc, col(ColNames.mcc).cast(IntegerType()))\n\n        # TODO use DataObject schema for selecting the columns?\n        bronze_columns = [i.name for i in BronzeEventDataObject.SCHEMA]\n\n        # TODO Should certain location columns (depending on generator params) not be created?\n        # for unsupported_column in [\"longitude\", \"latitude\", \"loc_error\"]:\n        #     bronze_columns.remove(unsupported_column)\n\n        records_df = records_df.select(bronze_columns)\n\n        # Transform timestamp to expected format\n        records_df = records_df.withColumn(\n            \"timestamp\", date_format(to_timestamp(col(\"timestamp\")), format=\"yyyy-MM-dd'T'HH:mm:ss\")\n        )\n        records_df = records_df.withColumn(ColNames.year, F.year(col(ColNames.timestamp)))\n        records_df = records_df.withColumn(ColNames.month, F.month(col(ColNames.timestamp)))\n        records_df = records_df.withColumn(ColNames.day, F.dayofmonth(col(ColNames.timestamp)))\n\n        # If error generation is enabled, replace clean dataset with errorful dataset.\n        if self.do_error_generation:\n            records_df = self.generate_errors(records_df)\n\n        # Assign output data object dataframe\n        self.output_data_objects[\"SyntheticEvents\"].df = records_df\n\n    def write(self):\n        super().write()\n        # TODO Rename output directories to YYYY/MM/DD?\n\n    def execute(self):\n        # super().execute()\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.transform()\n        self.write()\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n\n    def generate_agents(self) -&gt; []:\n        \"\"\"\n        Generate agent rows according to parameters.\n        Each agent should include the information needed to generate the records for that user.\n        \"\"\"\n        # Initialize agents sequentially\n        # TODO event ids should be numbered per partition, not global?\n        agents = []\n        starting_event_id = 0\n        for user_id in range(self.n_agents):\n            partition_id = user_id % self.n_partitions\n            agents.append(\n                Row(\n                    user_id=user_id,\n                    partition_id=partition_id,\n                    starting_event_id=starting_event_id,\n                    mcc=self.mcc,\n                    n_events=self.n_events_per_agent,\n                    random_seed=self.seed,\n                    timestamp_generator_params=self.timestamp_generator_params,\n                    location_generator_params=self.location_generator_params,\n                )\n            )\n            starting_event_id += self.n_events_per_agent\n        return agents\n\n    def calc_hashed_user_id(self, df) -&gt; DataFrame:\n        \"\"\"\n        Calculates SHA2 hash of user id, takes the first 31 bits and converts them to a non-negative 32-bit integer.\n\n        Args:\n            df (pyspark.sql.DataFrame): Data of clean synthetic events with a user id column.\n\n        Returns:\n            pyspark.sql.DataFrame: Dataframe, where user_id column is transformered to a hashed value.\n\n        \"\"\"\n        df = df.withColumn(\"ms_id_binary\", col(ColNames.user_id).cast(BinaryType()))\n\n        df = df.withColumn(ColNames.user_id, sha2(col(\"ms_id_binary\"), numBits=256))\n\n        df = df.drop(\"ms_id_binary\")\n\n        return df\n\n    def generate_errors(self, synth_df_raw: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Transforms a dataframe with clean synthetic records, by calculating year, month and day columns,\n        creating an event_id column, and generating all types of erronous records.\n        Calls all error generation functions.\n\n        Args:\n            df (pyspark.sql.DataFrame): Data of raw and clean synthetic events.\n\n        Returns:\n            pyspark.sql.DataFrame: Dataframe, with erroneous records, according to probabilities defined in the configuration.\n        \"\"\"\n        # Create a copy of original MSID column for final sorting and joining\n        synth_df_raw = synth_df_raw.withColumn(\"user_id_copy\", F.col(ColNames.user_id))\n\n        synth_df_raw = synth_df_raw.withColumn(ColNames.year, F.year(F.col(ColNames.timestamp)))\n        synth_df_raw = synth_df_raw.withColumn(ColNames.month, F.month(F.col(ColNames.timestamp)))\n        synth_df_raw = synth_df_raw.withColumn(ColNames.day, F.dayofmonth(F.col(ColNames.timestamp)))\n\n        # Create event id column if missing\n        if ColNames.event_id not in synth_df_raw.columns:\n            windowSpec = Window.partitionBy(F.col(ColNames.user_id)).orderBy(F.col(ColNames.timestamp))\n            synth_df_raw = synth_df_raw.withColumn(ColNames.event_id, F.row_number().over(windowSpec))\n\n        # TODO optional column support\n        synth_df_raw = synth_df_raw.cache()\n        synth_df = synth_df_raw[self.mandatory_columns + [\"user_id_copy\", ColNames.event_id]]\n\n        synth_df_w_nulls = self.generate_nulls_in_mandatory_fields(synth_df)\n        synth_df_w_out_of_bounds_and_nulls = self.generate_out_of_bounds_dates(synth_df_w_nulls)\n        synth_df_w_out_of_bounds_nulls_errors = self.generate_erroneous_type_values(synth_df_w_out_of_bounds_and_nulls)\n\n        synth_df_w_out_of_bounds_nulls_errors = synth_df_w_out_of_bounds_nulls_errors.join(\n            synth_df_raw[[\"user_id_copy\", ColNames.year, ColNames.month, ColNames.day, ColNames.event_id]],\n            on=[\"user_id_copy\", ColNames.event_id],\n            how=\"left\",\n        )\n\n        # Sort\n        if self.sort_output:\n            synth_df_w_out_of_bounds_nulls_errors = synth_df_w_out_of_bounds_nulls_errors.orderBy(\n                [\"user_id_copy\", ColNames.event_id]\n            )\n\n        error_df = synth_df_w_out_of_bounds_nulls_errors.drop(F.col(\"user_id_copy\")).drop(F.col(ColNames.event_id))\n\n        return error_df\n\n    def generate_nulls_in_mandatory_fields(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Generates null values in mandatory field columns based on probabilities from config.\n\n        Args:\n            df (pyspark.sql.DataFrame): clean synthetic data\n\n        Returns:\n            pyspark.sql.DataFrame: synthetic records dataframe with nulls in some rows\n        \"\"\"\n\n        # Reading in two probability parameters from config:\n        # First one describes how many rows to create null values for\n        # Second one selects the maximum ratio of columns that are allowed to be nulls for rows, that are selected for error generation\n        # If 1.0, it means that all mandatory columns can be allowed to be null-s for rows that are selected as including nulls\n\n        if self.null_row_prob == 0:\n            # TODO logging\n            return df\n\n        # 1) sample rows 2) sample columns 3) do a final join/union\n        # Sampling a new dataframe\n        df = df.cache()\n        sampled_df = df.sample(self.null_row_prob, seed=self.seed)\n        df_with_nulls = sampled_df\n\n        # Selecting columns based on ratio param\n        for column in self.mandatory_columns:\n            df_with_nulls = df_with_nulls.withColumn(\n                column, F.when(F.rand() &lt; self.max_ratio_of_mandatory_columns, F.lit(None)).otherwise(F.col(column))\n            )\n\n        result_df = df.join(df_with_nulls, on=[\"user_id_copy\", ColNames.event_id], how=\"leftanti\")[\n            [df.columns]\n        ].unionAll(df_with_nulls)\n\n        return result_df\n\n    def generate_out_of_bounds_dates(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Transformers the timestamp column values to be out of bound of the selected period,\n        based on probabilities from configuration.\n\n        Args:\n            df (pyspark.sql.DataFrame): Dataframe of clean synthetic events.\n\n        Returns:\n            pyspark.sql.DataFrame: Data where some timestamp column values are out of bounds as per config.\n        \"\"\"\n\n        if self.out_of_bounds_prob == 0:\n            # TODO logging\n            return df\n\n        # approximate span in months is enough for error generation\n        events_span_in_months = (pd.Timestamp(self.ending_timestamp) - pd.Timestamp(self.starting_timestamp)).days / 30\n\n        # Current idea is to 1) sample rows 2) sample columns 3) do a final join/union 4) sort\n        df = df.cache()\n        df_not_null_dates = df.where(F.col(\"timestamp\").isNotNull())\n\n        # Current approach means that this should be run after nulls, but before wrong type generation\n\n        df_not_null_dates = df_not_null_dates.cache()\n        df_with_sample_column = df_not_null_dates.join(\n            df_not_null_dates[[ColNames.event_id, ColNames.user_id]]\n            .sample(self.out_of_bounds_prob, seed=self.seed)\n            .withColumn(\"out_of_bounds\", F.lit(True)),\n            on=[ColNames.user_id, ColNames.event_id],\n            how=\"left\",\n        ).withColumn(\n            \"out_of_bounds\", F.when(F.col(\"out_of_bounds\").isNull(), F.lit(False)).otherwise(F.col(\"out_of_bounds\"))\n        )\n\n        df_with_sample_column = df_with_sample_column.withColumn(\n            \"months_to_add\", (F.lit(1) + F.randn(self.seed)) * F.lit(events_span_in_months)\n        )\n\n        df_with_sample_column = df_with_sample_column.withColumn(\n            \"timestamp\",\n            F.when(F.col(\"out_of_bounds\"), F.add_months(F.col(\"timestamp\"), F.col(\"months_to_add\"))).otherwise(\n                F.col(\"timestamp\")\n            ),\n        ).drop(F.col(\"months_to_add\"))\n\n        columns_to_error_generation = [\"out_of_bounds\"]\n\n        if self.error_prob == 0:\n            columns_to_error_generation = []\n\n        result_df = (\n            df.where(F.col(\"timestamp\").isNull())\n            .withColumn(\"out_of_bounds\", F.lit(None))[df.columns + columns_to_error_generation]\n            .unionAll(df_with_sample_column[df.columns + columns_to_error_generation])\n        )\n\n        return result_df\n\n    def generate_erroneous_type_values(self, df: DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Generates errors for sampled rows. Errors are custom defined, for instance a random string, or corrupt timestamp.\n        Does not cast the columns to a different type.\n\n        Args:\n            df (pyspark.sql.DataFrame): dataframe that may have out of bound and null records.\n\n        Returns:\n            pyspark.sql.DataFrame: dataframe with erroneous rows, and possibly, with nulls and out of bound records.\n        \"\"\"\n\n        if self.error_prob == 0:\n            # TODO logging\n            return df\n\n        # Current idea is to 1) select not null rows 2) create a sampling column 3) perform type mutation\n        # This creates wrong type values on same selected rows for all mandatory columns\n        # Could be improved to select non-nulls for only that specific column, and do sampling for each column seperately\n        # Or to sample the probabilities for wrong types seperately?\n\n        # Errors shouldn't be generated, where there are out of bounds records, otherwise the probabilities don't realise in the final data\n        # as expected (for instance, the ratio of error records may be much smaller than the probability assigned,\n        # because some nulls are re-done as errors)\n\n        # In case the pipeline is such that out of bounds comes later, or out_of_bounds probability = 0\n        if \"out_of_bounds\" not in df.columns:\n            df = df.withColumn(\"out_of_bounds\", F.lit(False))\n\n        # Recast binary column\n        # Cast BinaryType column to StringType with error handling\n\n        df = df.withColumn(\n            ColNames.user_id, F.base64(F.col(ColNames.user_id)).cast(StringType())\n        )  # recasting here to enable union later\n\n        df = df.cache()\n        df_not_null = df.dropna().filter(F.col(\"out_of_bounds\") == F.lit(False))\n\n        df = df.drop(F.col(\"out_of_bounds\"))\n\n        df_not_null = df_not_null.cache()\n        df_with_sample_column = df_not_null.join(\n            df_not_null[[ColNames.event_id, ColNames.user_id]]\n            .sample(self.error_prob, seed=self.seed)\n            .withColumn(\"mutate_to_error\", F.lit(True)),\n            on=[ColNames.user_id, ColNames.event_id],\n            how=\"left\",\n        )\n\n        # Iterate over mandatory columns to mutate the type for a sampled row\n        # First cast sampled rows, then fill with values\n        # TODO refactor more compactly\n\n        for struct_schema in BronzeEventDataObject.SCHEMA:\n            if struct_schema.name not in self.mandatory_columns:\n                continue\n\n            column = struct_schema.name\n            col_dtype = struct_schema.dataType\n\n            if col_dtype in [BinaryType()]:\n                to_value = F.md5(F.col(column))  # numBits=224\n\n            if col_dtype in [FloatType(), IntegerType()]:\n                # changes mcc, lat, lon\n                to_value = F.col(column) + ((F.rand() + F.lit(180)) * 10000).cast(\"int\")\n\n            if column == ColNames.timestamp and col_dtype == StringType():\n                # Timezone difference manipulation may be performed here, if cleaning module were to support it.\n                # statically one timezone difference\n                # timezone_to = random.randint(0, 12)\n                to_value = F.concat(\n                    F.substring(F.col(column), 1, 10),\n                    F.lit(\"T\"),\n                    F.substring(F.col(column), 12, 9),\n                    # TODO: Temporary remove of timezone addition as cleaning\n                    # module does not support it\n                    # F.lit(f\"+0{timezone_to}:00\")\n                )\n\n            if column == ColNames.cell_id and col_dtype == StringType():\n                random_string = \"\".join(random.choice(string.ascii_uppercase + string.digits) for _ in range(6)) + \"_\"\n                to_value = F.concat(F.lit(random_string), (F.rand() * 100).cast(\"int\"))\n\n            df_with_sample_column = self.mutate_row(df_with_sample_column, column, to_value)\n\n        # TODO check for a more optimal join\n        result_df = df.join(df_with_sample_column, on=[\"user_id_copy\", ColNames.event_id], how=\"leftanti\")[\n            df.columns\n        ].unionAll(df_with_sample_column[df.columns])\n\n        return result_df\n\n    def mutate_row(self, df: DataFrame, column_name: str, to_value: F) -&gt; DataFrame:\n        \"\"\"\n        Mutates a row when mutate_to_error = True. changes the value accordingly.\n\n        Args:\n            df (pyspark.sql.DataFrame): dataframe, the rows of which to make erronous\n            column_name (str): column to change\n            to_value (any): pyspark sql function statement, for instance F.col(), or F.lit()\n\n        Returns:\n            pyspark.sql.DataFrame: dataframe with erronoeous rows\n        \"\"\"\n\n        df = df.withColumn(column_name, F.when(F.col(\"mutate_to_error\"), to_value).otherwise(F.col(column_name)))\n\n        return df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events.html#components.ingestion.synthetic.synthetic_events.SyntheticEvents.calc_hashed_user_id","title":"<code>calc_hashed_user_id(df)</code>","text":"<p>Calculates SHA2 hash of user id, takes the first 31 bits and converts them to a non-negative 32-bit integer.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Data of clean synthetic events with a user id column.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: Dataframe, where user_id column is transformered to a hashed value.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>def calc_hashed_user_id(self, df) -&gt; DataFrame:\n    \"\"\"\n    Calculates SHA2 hash of user id, takes the first 31 bits and converts them to a non-negative 32-bit integer.\n\n    Args:\n        df (pyspark.sql.DataFrame): Data of clean synthetic events with a user id column.\n\n    Returns:\n        pyspark.sql.DataFrame: Dataframe, where user_id column is transformered to a hashed value.\n\n    \"\"\"\n    df = df.withColumn(\"ms_id_binary\", col(ColNames.user_id).cast(BinaryType()))\n\n    df = df.withColumn(ColNames.user_id, sha2(col(\"ms_id_binary\"), numBits=256))\n\n    df = df.drop(\"ms_id_binary\")\n\n    return df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events.html#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_agents","title":"<code>generate_agents()</code>","text":"<p>Generate agent rows according to parameters. Each agent should include the information needed to generate the records for that user.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>def generate_agents(self) -&gt; []:\n    \"\"\"\n    Generate agent rows according to parameters.\n    Each agent should include the information needed to generate the records for that user.\n    \"\"\"\n    # Initialize agents sequentially\n    # TODO event ids should be numbered per partition, not global?\n    agents = []\n    starting_event_id = 0\n    for user_id in range(self.n_agents):\n        partition_id = user_id % self.n_partitions\n        agents.append(\n            Row(\n                user_id=user_id,\n                partition_id=partition_id,\n                starting_event_id=starting_event_id,\n                mcc=self.mcc,\n                n_events=self.n_events_per_agent,\n                random_seed=self.seed,\n                timestamp_generator_params=self.timestamp_generator_params,\n                location_generator_params=self.location_generator_params,\n            )\n        )\n        starting_event_id += self.n_events_per_agent\n    return agents\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events.html#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_erroneous_type_values","title":"<code>generate_erroneous_type_values(df)</code>","text":"<p>Generates errors for sampled rows. Errors are custom defined, for instance a random string, or corrupt timestamp. Does not cast the columns to a different type.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe that may have out of bound and null records.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: dataframe with erroneous rows, and possibly, with nulls and out of bound records.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>def generate_erroneous_type_values(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Generates errors for sampled rows. Errors are custom defined, for instance a random string, or corrupt timestamp.\n    Does not cast the columns to a different type.\n\n    Args:\n        df (pyspark.sql.DataFrame): dataframe that may have out of bound and null records.\n\n    Returns:\n        pyspark.sql.DataFrame: dataframe with erroneous rows, and possibly, with nulls and out of bound records.\n    \"\"\"\n\n    if self.error_prob == 0:\n        # TODO logging\n        return df\n\n    # Current idea is to 1) select not null rows 2) create a sampling column 3) perform type mutation\n    # This creates wrong type values on same selected rows for all mandatory columns\n    # Could be improved to select non-nulls for only that specific column, and do sampling for each column seperately\n    # Or to sample the probabilities for wrong types seperately?\n\n    # Errors shouldn't be generated, where there are out of bounds records, otherwise the probabilities don't realise in the final data\n    # as expected (for instance, the ratio of error records may be much smaller than the probability assigned,\n    # because some nulls are re-done as errors)\n\n    # In case the pipeline is such that out of bounds comes later, or out_of_bounds probability = 0\n    if \"out_of_bounds\" not in df.columns:\n        df = df.withColumn(\"out_of_bounds\", F.lit(False))\n\n    # Recast binary column\n    # Cast BinaryType column to StringType with error handling\n\n    df = df.withColumn(\n        ColNames.user_id, F.base64(F.col(ColNames.user_id)).cast(StringType())\n    )  # recasting here to enable union later\n\n    df = df.cache()\n    df_not_null = df.dropna().filter(F.col(\"out_of_bounds\") == F.lit(False))\n\n    df = df.drop(F.col(\"out_of_bounds\"))\n\n    df_not_null = df_not_null.cache()\n    df_with_sample_column = df_not_null.join(\n        df_not_null[[ColNames.event_id, ColNames.user_id]]\n        .sample(self.error_prob, seed=self.seed)\n        .withColumn(\"mutate_to_error\", F.lit(True)),\n        on=[ColNames.user_id, ColNames.event_id],\n        how=\"left\",\n    )\n\n    # Iterate over mandatory columns to mutate the type for a sampled row\n    # First cast sampled rows, then fill with values\n    # TODO refactor more compactly\n\n    for struct_schema in BronzeEventDataObject.SCHEMA:\n        if struct_schema.name not in self.mandatory_columns:\n            continue\n\n        column = struct_schema.name\n        col_dtype = struct_schema.dataType\n\n        if col_dtype in [BinaryType()]:\n            to_value = F.md5(F.col(column))  # numBits=224\n\n        if col_dtype in [FloatType(), IntegerType()]:\n            # changes mcc, lat, lon\n            to_value = F.col(column) + ((F.rand() + F.lit(180)) * 10000).cast(\"int\")\n\n        if column == ColNames.timestamp and col_dtype == StringType():\n            # Timezone difference manipulation may be performed here, if cleaning module were to support it.\n            # statically one timezone difference\n            # timezone_to = random.randint(0, 12)\n            to_value = F.concat(\n                F.substring(F.col(column), 1, 10),\n                F.lit(\"T\"),\n                F.substring(F.col(column), 12, 9),\n                # TODO: Temporary remove of timezone addition as cleaning\n                # module does not support it\n                # F.lit(f\"+0{timezone_to}:00\")\n            )\n\n        if column == ColNames.cell_id and col_dtype == StringType():\n            random_string = \"\".join(random.choice(string.ascii_uppercase + string.digits) for _ in range(6)) + \"_\"\n            to_value = F.concat(F.lit(random_string), (F.rand() * 100).cast(\"int\"))\n\n        df_with_sample_column = self.mutate_row(df_with_sample_column, column, to_value)\n\n    # TODO check for a more optimal join\n    result_df = df.join(df_with_sample_column, on=[\"user_id_copy\", ColNames.event_id], how=\"leftanti\")[\n        df.columns\n    ].unionAll(df_with_sample_column[df.columns])\n\n    return result_df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events.html#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_errors","title":"<code>generate_errors(synth_df_raw)</code>","text":"<p>Transforms a dataframe with clean synthetic records, by calculating year, month and day columns, creating an event_id column, and generating all types of erronous records. Calls all error generation functions.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Data of raw and clean synthetic events.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: Dataframe, with erroneous records, according to probabilities defined in the configuration.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>def generate_errors(self, synth_df_raw: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transforms a dataframe with clean synthetic records, by calculating year, month and day columns,\n    creating an event_id column, and generating all types of erronous records.\n    Calls all error generation functions.\n\n    Args:\n        df (pyspark.sql.DataFrame): Data of raw and clean synthetic events.\n\n    Returns:\n        pyspark.sql.DataFrame: Dataframe, with erroneous records, according to probabilities defined in the configuration.\n    \"\"\"\n    # Create a copy of original MSID column for final sorting and joining\n    synth_df_raw = synth_df_raw.withColumn(\"user_id_copy\", F.col(ColNames.user_id))\n\n    synth_df_raw = synth_df_raw.withColumn(ColNames.year, F.year(F.col(ColNames.timestamp)))\n    synth_df_raw = synth_df_raw.withColumn(ColNames.month, F.month(F.col(ColNames.timestamp)))\n    synth_df_raw = synth_df_raw.withColumn(ColNames.day, F.dayofmonth(F.col(ColNames.timestamp)))\n\n    # Create event id column if missing\n    if ColNames.event_id not in synth_df_raw.columns:\n        windowSpec = Window.partitionBy(F.col(ColNames.user_id)).orderBy(F.col(ColNames.timestamp))\n        synth_df_raw = synth_df_raw.withColumn(ColNames.event_id, F.row_number().over(windowSpec))\n\n    # TODO optional column support\n    synth_df_raw = synth_df_raw.cache()\n    synth_df = synth_df_raw[self.mandatory_columns + [\"user_id_copy\", ColNames.event_id]]\n\n    synth_df_w_nulls = self.generate_nulls_in_mandatory_fields(synth_df)\n    synth_df_w_out_of_bounds_and_nulls = self.generate_out_of_bounds_dates(synth_df_w_nulls)\n    synth_df_w_out_of_bounds_nulls_errors = self.generate_erroneous_type_values(synth_df_w_out_of_bounds_and_nulls)\n\n    synth_df_w_out_of_bounds_nulls_errors = synth_df_w_out_of_bounds_nulls_errors.join(\n        synth_df_raw[[\"user_id_copy\", ColNames.year, ColNames.month, ColNames.day, ColNames.event_id]],\n        on=[\"user_id_copy\", ColNames.event_id],\n        how=\"left\",\n    )\n\n    # Sort\n    if self.sort_output:\n        synth_df_w_out_of_bounds_nulls_errors = synth_df_w_out_of_bounds_nulls_errors.orderBy(\n            [\"user_id_copy\", ColNames.event_id]\n        )\n\n    error_df = synth_df_w_out_of_bounds_nulls_errors.drop(F.col(\"user_id_copy\")).drop(F.col(ColNames.event_id))\n\n    return error_df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events.html#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_nulls_in_mandatory_fields","title":"<code>generate_nulls_in_mandatory_fields(df)</code>","text":"<p>Generates null values in mandatory field columns based on probabilities from config.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>clean synthetic data</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: synthetic records dataframe with nulls in some rows</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>def generate_nulls_in_mandatory_fields(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Generates null values in mandatory field columns based on probabilities from config.\n\n    Args:\n        df (pyspark.sql.DataFrame): clean synthetic data\n\n    Returns:\n        pyspark.sql.DataFrame: synthetic records dataframe with nulls in some rows\n    \"\"\"\n\n    # Reading in two probability parameters from config:\n    # First one describes how many rows to create null values for\n    # Second one selects the maximum ratio of columns that are allowed to be nulls for rows, that are selected for error generation\n    # If 1.0, it means that all mandatory columns can be allowed to be null-s for rows that are selected as including nulls\n\n    if self.null_row_prob == 0:\n        # TODO logging\n        return df\n\n    # 1) sample rows 2) sample columns 3) do a final join/union\n    # Sampling a new dataframe\n    df = df.cache()\n    sampled_df = df.sample(self.null_row_prob, seed=self.seed)\n    df_with_nulls = sampled_df\n\n    # Selecting columns based on ratio param\n    for column in self.mandatory_columns:\n        df_with_nulls = df_with_nulls.withColumn(\n            column, F.when(F.rand() &lt; self.max_ratio_of_mandatory_columns, F.lit(None)).otherwise(F.col(column))\n        )\n\n    result_df = df.join(df_with_nulls, on=[\"user_id_copy\", ColNames.event_id], how=\"leftanti\")[\n        [df.columns]\n    ].unionAll(df_with_nulls)\n\n    return result_df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events.html#components.ingestion.synthetic.synthetic_events.SyntheticEvents.generate_out_of_bounds_dates","title":"<code>generate_out_of_bounds_dates(df)</code>","text":"<p>Transformers the timestamp column values to be out of bound of the selected period, based on probabilities from configuration.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe of clean synthetic events.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: Data where some timestamp column values are out of bounds as per config.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>def generate_out_of_bounds_dates(self, df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Transformers the timestamp column values to be out of bound of the selected period,\n    based on probabilities from configuration.\n\n    Args:\n        df (pyspark.sql.DataFrame): Dataframe of clean synthetic events.\n\n    Returns:\n        pyspark.sql.DataFrame: Data where some timestamp column values are out of bounds as per config.\n    \"\"\"\n\n    if self.out_of_bounds_prob == 0:\n        # TODO logging\n        return df\n\n    # approximate span in months is enough for error generation\n    events_span_in_months = (pd.Timestamp(self.ending_timestamp) - pd.Timestamp(self.starting_timestamp)).days / 30\n\n    # Current idea is to 1) sample rows 2) sample columns 3) do a final join/union 4) sort\n    df = df.cache()\n    df_not_null_dates = df.where(F.col(\"timestamp\").isNotNull())\n\n    # Current approach means that this should be run after nulls, but before wrong type generation\n\n    df_not_null_dates = df_not_null_dates.cache()\n    df_with_sample_column = df_not_null_dates.join(\n        df_not_null_dates[[ColNames.event_id, ColNames.user_id]]\n        .sample(self.out_of_bounds_prob, seed=self.seed)\n        .withColumn(\"out_of_bounds\", F.lit(True)),\n        on=[ColNames.user_id, ColNames.event_id],\n        how=\"left\",\n    ).withColumn(\n        \"out_of_bounds\", F.when(F.col(\"out_of_bounds\").isNull(), F.lit(False)).otherwise(F.col(\"out_of_bounds\"))\n    )\n\n    df_with_sample_column = df_with_sample_column.withColumn(\n        \"months_to_add\", (F.lit(1) + F.randn(self.seed)) * F.lit(events_span_in_months)\n    )\n\n    df_with_sample_column = df_with_sample_column.withColumn(\n        \"timestamp\",\n        F.when(F.col(\"out_of_bounds\"), F.add_months(F.col(\"timestamp\"), F.col(\"months_to_add\"))).otherwise(\n            F.col(\"timestamp\")\n        ),\n    ).drop(F.col(\"months_to_add\"))\n\n    columns_to_error_generation = [\"out_of_bounds\"]\n\n    if self.error_prob == 0:\n        columns_to_error_generation = []\n\n    result_df = (\n        df.where(F.col(\"timestamp\").isNull())\n        .withColumn(\"out_of_bounds\", F.lit(None))[df.columns + columns_to_error_generation]\n        .unionAll(df_with_sample_column[df.columns + columns_to_error_generation])\n    )\n\n    return result_df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events.html#components.ingestion.synthetic.synthetic_events.SyntheticEvents.mutate_row","title":"<code>mutate_row(df, column_name, to_value)</code>","text":"<p>Mutates a row when mutate_to_error = True. changes the value accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe, the rows of which to make erronous</p> required <code>column_name</code> <code>str</code> <p>column to change</p> required <code>to_value</code> <code>any</code> <p>pyspark sql function statement, for instance F.col(), or F.lit()</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: dataframe with erronoeous rows</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>def mutate_row(self, df: DataFrame, column_name: str, to_value: F) -&gt; DataFrame:\n    \"\"\"\n    Mutates a row when mutate_to_error = True. changes the value accordingly.\n\n    Args:\n        df (pyspark.sql.DataFrame): dataframe, the rows of which to make erronous\n        column_name (str): column to change\n        to_value (any): pyspark sql function statement, for instance F.col(), or F.lit()\n\n    Returns:\n        pyspark.sql.DataFrame: dataframe with erronoeous rows\n    \"\"\"\n\n    df = df.withColumn(column_name, F.when(F.col(\"mutate_to_error\"), to_value).otherwise(F.col(column_name)))\n\n    return df\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events.html#components.ingestion.synthetic.synthetic_events.TimestampGeneratorType","title":"<code>TimestampGeneratorType</code>","text":"<p>             Bases: <code>Enum</code></p> <p>Timestamp Generator enumeration class.</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>class TimestampGeneratorType(Enum):\n    \"\"\"\n    Timestamp Generator enumeration class.\n    \"\"\"\n\n    EQUAL_GAPS = \"equal_gaps\"\n</code></pre>"},{"location":"reference/components/ingestion/synthetic/synthetic_events.html#components.ingestion.synthetic.synthetic_events.generate_agent_records","title":"<code>generate_agent_records(user_id, n_events, starting_event_id, random_seed, timestamp_generator_params, location_generator_params)</code>","text":"<p>UDF to generate records from agent parameters. Generates an array of (event_id, timestamp, cell_id) tuples.</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>str</code> <p>description</p> required <code>n_events</code> <code>int</code> <p>number of events to generate.</p> required <code>starting_event_id</code> <code>int</code> <p>starting value for event_id column (used only internally, and not stored).</p> required <code>random_seed</code> <code>int</code> <p>random seed that is the same for all processes throughout the module.</p> required <code>timestamp_generator_params</code> <code>list</code> <p>list of parameters for timestamp generation, specified in config.</p> required <code>location_generator_params</code> <code>list</code> <p>list of parameters for loaction generation, specifiec in config.</p> required <p>Returns:</p> Name Type Description <code>zip</code> <p>list of tuples with all event id-s, timestamps, cell_ids, latitude and longitude values</p> Source code in <code>multimno/components/ingestion/synthetic/synthetic_events.py</code> <pre><code>@udf(returnType=agent_records_return_type)\ndef generate_agent_records(\n    user_id, n_events, starting_event_id, random_seed, timestamp_generator_params, location_generator_params\n):\n    \"\"\"\n    UDF to generate records from agent parameters.\n    Generates an array of (event_id, timestamp, cell_id) tuples.\n\n    Args:\n        user_id (str): _description_\n        n_events (int): number of events to generate.\n        starting_event_id (int): starting value for event_id column (used only internally, and not stored).\n        random_seed (int): random seed that is the same for all processes throughout the module.\n        timestamp_generator_params (list): list of parameters for timestamp generation, specified in config.\n        location_generator_params (list): list of parameters for loaction generation, specifiec in config.\n\n    Returns:\n        zip: list of tuples with all event id-s, timestamps, cell_ids, latitude and longitude values\n    \"\"\"\n    # Generate event id values.\n    event_ids = [i for i in range(starting_event_id, starting_event_id + n_events)]\n\n    # Generate timestamp values.\n    # TODO timestamp generator types\n    timestamp_generator_type = timestamp_generator_params[0]\n    if timestamp_generator_type == TimestampGeneratorType.EQUAL_GAPS.value:\n        starting_timestamp = timestamp_generator_params[1]\n        ending_timestamp = timestamp_generator_params[2]\n        gap_length_s = (ending_timestamp - starting_timestamp) / n_events\n        current_timestamp = starting_timestamp\n        timestamps = []\n        for i in range(n_events):\n            timestamps.append(current_timestamp)\n            current_timestamp += gap_length_s\n\n    # Generate location values.\n    # Location is identified either by cell id or by latitude and longitude.\n    location_generator_type = location_generator_params[0]\n    random.seed(random_seed + user_id)\n    if location_generator_type == LocationGeneratorType.RANDOM_CELL_ID.value:\n        cell_id_min = location_generator_params[1]\n        cell_id_max = location_generator_params[2]\n        cell_ids = [random.randint(cell_id_min, cell_id_max) for i in range(n_events)]\n        lats = [None for i in range(n_events)]\n        lons = [None for i in range(n_events)]\n    elif location_generator_type == LocationGeneratorType.RANDOM_LAT_LON.value:\n        lat_min = location_generator_params[1]\n        lat_max = location_generator_params[2]\n        lon_min = location_generator_params[3]\n        lon_max = location_generator_params[4]\n        cell_ids = [None for i in range(n_events)]\n        lats = [random.random() * (lat_max - lat_min) + lat_min for i in range(n_events)]\n        lons = [random.random() * (lon_max - lon_min) + lon_min for i in range(n_events)]\n\n    events = zip(event_ids, timestamps, cell_ids, lats, lons)\n    return events\n</code></pre>"},{"location":"reference/components/quality/index.html","title":"quality","text":""},{"location":"reference/core/index.html","title":"core","text":""},{"location":"reference/core/component.html","title":"component","text":"<p>Module that defines the abstract pipeline component class</p>"},{"location":"reference/core/component.html#core.component.Component","title":"<code>Component</code>","text":"<p>Class that models a pipeline component.</p> Source code in <code>multimno/core/component.py</code> <pre><code>class Component(metaclass=ABCMeta):\n    \"\"\"\n    Class that models a pipeline component.\n    \"\"\"\n\n    COMPONENT_ID: str = None\n\n    def __init__(self, general_config_path: str, component_config_path: str) -&gt; None:\n        self.input_data_objects: Dict[str, DataObject] = None\n        self.output_data_objects: Dict[str, DataObject] = None\n        self.config: ConfigParser = parse_configuration(general_config_path, component_config_path)\n        self.logger: Logger = generate_logger(self.config)\n        self.spark: SparkSession = generate_spark_session(self.config)\n        self.initalize_data_objects()\n\n    @abstractmethod\n    def initalize_data_objects(self):\n        \"\"\"\n        Method that initializes the data objects associated with the component.\n        \"\"\"\n\n    def read(self):\n        \"\"\"\n        Method that performs the read operation of the input data objects of the component.\n        \"\"\"\n        for data_object in self.input_data_objects.values():\n            data_object.read()\n\n    @abstractmethod\n    def transform(self):\n        \"\"\"\n        Method that performs the data transformations needed to set the dataframes of the output\n         data objects from the input data objects.\n        \"\"\"\n\n    def write(self):\n        \"\"\"\n        Method that performs the write operation of the output data objects.\n        \"\"\"\n        for data_object in self.output_data_objects.values():\n            data_object.write()\n\n    def execute(self):\n        \"\"\"\n        Method that performs the read, transform and write methods of the component.\n        \"\"\"\n        self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n        self.read()\n        self.transform()\n        self.write()\n        self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n</code></pre>"},{"location":"reference/core/component.html#core.component.Component.execute","title":"<code>execute()</code>","text":"<p>Method that performs the read, transform and write methods of the component.</p> Source code in <code>multimno/core/component.py</code> <pre><code>def execute(self):\n    \"\"\"\n    Method that performs the read, transform and write methods of the component.\n    \"\"\"\n    self.logger.info(f\"Starting {self.COMPONENT_ID}...\")\n    self.read()\n    self.transform()\n    self.write()\n    self.logger.info(f\"Finished {self.COMPONENT_ID}\")\n</code></pre>"},{"location":"reference/core/component.html#core.component.Component.initalize_data_objects","title":"<code>initalize_data_objects()</code>  <code>abstractmethod</code>","text":"<p>Method that initializes the data objects associated with the component.</p> Source code in <code>multimno/core/component.py</code> <pre><code>@abstractmethod\ndef initalize_data_objects(self):\n    \"\"\"\n    Method that initializes the data objects associated with the component.\n    \"\"\"\n</code></pre>"},{"location":"reference/core/component.html#core.component.Component.read","title":"<code>read()</code>","text":"<p>Method that performs the read operation of the input data objects of the component.</p> Source code in <code>multimno/core/component.py</code> <pre><code>def read(self):\n    \"\"\"\n    Method that performs the read operation of the input data objects of the component.\n    \"\"\"\n    for data_object in self.input_data_objects.values():\n        data_object.read()\n</code></pre>"},{"location":"reference/core/component.html#core.component.Component.transform","title":"<code>transform()</code>  <code>abstractmethod</code>","text":"<p>Method that performs the data transformations needed to set the dataframes of the output  data objects from the input data objects.</p> Source code in <code>multimno/core/component.py</code> <pre><code>@abstractmethod\ndef transform(self):\n    \"\"\"\n    Method that performs the data transformations needed to set the dataframes of the output\n     data objects from the input data objects.\n    \"\"\"\n</code></pre>"},{"location":"reference/core/component.html#core.component.Component.write","title":"<code>write()</code>","text":"<p>Method that performs the write operation of the output data objects.</p> Source code in <code>multimno/core/component.py</code> <pre><code>def write(self):\n    \"\"\"\n    Method that performs the write operation of the output data objects.\n    \"\"\"\n    for data_object in self.output_data_objects.values():\n        data_object.write()\n</code></pre>"},{"location":"reference/core/configuration.html","title":"configuration","text":"<p>Module that manages the application configuration.</p>"},{"location":"reference/core/configuration.html#core.configuration.parse_configuration","title":"<code>parse_configuration(general_config_path, component_config_path='')</code>","text":"<p>Function that parses a list of configurations in a single ConfigParser object. It expects the first element of the list to be the path to general configuration path. It will override values of the general configuration file with component configuration data.</p> <p>Parameters:</p> Name Type Description Default <code>config_paths</code> <code>list</code> <p>List of paths. First Element is the general configuration path.</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the general configuration path is doesn't exist</p> <p>Returns:</p> Name Type Description <code>ConfigParser</code> <p>ConfigParser object with some general configuration values overriden by</p> Source code in <code>multimno/core/configuration.py</code> <pre><code>def parse_configuration(general_config_path: str, component_config_path: str = \"\"):\n    \"\"\"Function that parses a list of configurations in a single ConfigParser object. It expects\n    the first element of the list to be the path to general configuration path. It will override\n    values of the general configuration file with component configuration data.\n\n    Args:\n        config_paths (list): List of paths. First Element is the general configuration path.\n\n    Raises:\n        FileNotFoundError: If the general configuration path is doesn't exist\n\n    Returns:\n        ConfigParser: ConfigParser object with some general configuration values overriden by\n    \"\"\"\n\n    # Check general configuration file\n    if not os.path.exists(general_config_path):\n        raise FileNotFoundError(f\"General Config file Not found: {general_config_path}\")\n\n    config_paths = [general_config_path, component_config_path]\n\n    converters = {\n        \"list\": lambda val: [i.strip() for i in val.strip().split(\"\\n\")],\n        \"eval\": eval,\n    }\n\n    parser: ConfigParser = ConfigParser(\n        converters=converters, interpolation=ExtendedInterpolation(), inline_comment_prefixes=\"#\"\n    )\n    parser.optionxform = str\n    parser.read(config_paths)\n\n    return parser\n</code></pre>"},{"location":"reference/core/io_interface.html","title":"io_interface","text":"<p>Module that implements classes for reading data from different data sources into a Spark DataFrames.</p>"},{"location":"reference/core/io_interface.html#core.io_interface.CsvInterface","title":"<code>CsvInterface</code>","text":"<p>             Bases: <code>PathInterface</code></p> <p>Class that implements the PathInterface abstract class for reading/writing data from a csv data source.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class CsvInterface(PathInterface):\n    \"\"\"Class that implements the PathInterface abstract class for reading/writing data from a csv data source.\"\"\"\n\n    FILE_FORMAT = \"csv\"\n\n    def read_from_interface(\n        self, spark: SparkSession, path: str, schema: StructType, header: bool = True, sep: str = \",\"\n    ):\n        \"\"\"Method that reads data from a csv type data source as a Spark DataFrame.\n\n        Args:\n            spark (SparkSession): Spark session.\n            path (str): Path to the data.\n            schema (StructType, optional): Schema of the data. Defaults to None.\n\n        Returns:\n            DataFrame: Spark dataframe.\n        \"\"\"\n        return spark.read.csv(path, schema=schema, header=header, sep=sep)\n\n    def write_from_interface(\n        self, df: DataFrame, path: str, partition_columns: list[str] = None, header: bool = True, sep: str = \",\"\n    ):\n        \"\"\"Method that writes data from a Spark DataFrame to a csv data source.\n\n        Args:\n            df (DataFrame): Spark DataFrame.\n            path (str): Path to the data.\n            partition_columns (list[str], optional): columns used for a partition write.\n        Raises:\n            NotImplementedError: csv files should not be written in this architecture.\n        \"\"\"\n        if partition_columns is None:\n            partition_columns = []\n        df.write.option(\"header\", header).option(\"sep\", sep).mode(\"overwrite\").format(\"csv\").save(path)\n</code></pre>"},{"location":"reference/core/io_interface.html#core.io_interface.CsvInterface.read_from_interface","title":"<code>read_from_interface(spark, path, schema, header=True, sep=',')</code>","text":"<p>Method that reads data from a csv type data source as a Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session.</p> required <code>path</code> <code>str</code> <p>Path to the data.</p> required <code>schema</code> <code>StructType</code> <p>Schema of the data. Defaults to None.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <p>Spark dataframe.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def read_from_interface(\n    self, spark: SparkSession, path: str, schema: StructType, header: bool = True, sep: str = \",\"\n):\n    \"\"\"Method that reads data from a csv type data source as a Spark DataFrame.\n\n    Args:\n        spark (SparkSession): Spark session.\n        path (str): Path to the data.\n        schema (StructType, optional): Schema of the data. Defaults to None.\n\n    Returns:\n        DataFrame: Spark dataframe.\n    \"\"\"\n    return spark.read.csv(path, schema=schema, header=header, sep=sep)\n</code></pre>"},{"location":"reference/core/io_interface.html#core.io_interface.CsvInterface.write_from_interface","title":"<code>write_from_interface(df, path, partition_columns=None, header=True, sep=',')</code>","text":"<p>Method that writes data from a Spark DataFrame to a csv data source.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Spark DataFrame.</p> required <code>path</code> <code>str</code> <p>Path to the data.</p> required <code>partition_columns</code> <code>list[str]</code> <p>columns used for a partition write.</p> <code>None</code> <p>Raises:     NotImplementedError: csv files should not be written in this architecture.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def write_from_interface(\n    self, df: DataFrame, path: str, partition_columns: list[str] = None, header: bool = True, sep: str = \",\"\n):\n    \"\"\"Method that writes data from a Spark DataFrame to a csv data source.\n\n    Args:\n        df (DataFrame): Spark DataFrame.\n        path (str): Path to the data.\n        partition_columns (list[str], optional): columns used for a partition write.\n    Raises:\n        NotImplementedError: csv files should not be written in this architecture.\n    \"\"\"\n    if partition_columns is None:\n        partition_columns = []\n    df.write.option(\"header\", header).option(\"sep\", sep).mode(\"overwrite\").format(\"csv\").save(path)\n</code></pre>"},{"location":"reference/core/io_interface.html#core.io_interface.GeoParquetInterface","title":"<code>GeoParquetInterface</code>","text":"<p>             Bases: <code>PathInterface</code></p> <p>Class that implements the PathInterface abstract class for reading/writing data from a geoparquet data source.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class GeoParquetInterface(PathInterface):\n    \"\"\"Class that implements the PathInterface abstract class for reading/writing data from a geoparquet data source.\"\"\"\n\n    FILE_FORMAT = \"geoparquet\"\n</code></pre>"},{"location":"reference/core/io_interface.html#core.io_interface.IOInterface","title":"<code>IOInterface</code>","text":"<p>Abstract interface that provides functionality for reading and writing data</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class IOInterface(metaclass=ABCMeta):\n    \"\"\"Abstract interface that provides functionality for reading and writing data\"\"\"\n\n    @classmethod\n    def __subclasshook__(cls, subclass: type) -&gt; bool:\n        if cls is IOInterface:\n            attrs: list[str] = []\n            callables: list[str] = [\"read_from_interface\", \"write_from_interface\"]\n            ret: bool = True\n            for attr in attrs:\n                ret = ret and (hasattr(subclass, attr) and isinstance(getattr(subclass, attr), property))\n            for call in callables:\n                ret = ret and (hasattr(subclass, call) and callable(getattr(subclass, call)))\n            return ret\n        else:\n            return NotImplemented\n\n    @abstractmethod\n    def read_from_interface(self, *args, **kwargs) -&gt; DataFrame:\n        pass\n\n    @abstractmethod\n    def write_from_interface(self, df: DataFrame, *args, **kwargs):\n        pass\n</code></pre>"},{"location":"reference/core/io_interface.html#core.io_interface.JsonInterface","title":"<code>JsonInterface</code>","text":"<p>             Bases: <code>PathInterface</code></p> <p>Class that implements the PathInterface abstract class for reading/writing data from a json data source.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class JsonInterface(PathInterface):\n    \"\"\"Class that implements the PathInterface abstract class for reading/writing data from a json data source.\"\"\"\n\n    FILE_FORMAT = \"json\"\n</code></pre>"},{"location":"reference/core/io_interface.html#core.io_interface.ParquetInterface","title":"<code>ParquetInterface</code>","text":"<p>             Bases: <code>PathInterface</code></p> <p>Class that implements the PathInterface abstract class for reading/writing data from a parquet data source.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class ParquetInterface(PathInterface):\n    \"\"\"Class that implements the PathInterface abstract class for reading/writing data from a parquet data source.\"\"\"\n\n    FILE_FORMAT = \"parquet\"\n</code></pre>"},{"location":"reference/core/io_interface.html#core.io_interface.PathInterface","title":"<code>PathInterface</code>","text":"<p>             Bases: <code>IOInterface</code></p> <p>Abstract interface for reading/writing data from a file type data source.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class PathInterface(IOInterface, metaclass=ABCMeta):\n    \"\"\"Abstract interface for reading/writing data from a file type data source.\"\"\"\n\n    FILE_FORMAT = \"\"\n\n    def read_from_interface(self, spark: SparkSession, path: str, schema: StructType = None):\n        \"\"\"Method that reads data from a file type data source as a Spark DataFrame.\n\n        Args:\n            spark (SparkSession): Spark session.\n            path (str): Path to the data.\n            schema (StructType, optional): Schema of the data. Defaults to None.\n\n        Returns:\n            DataFrame: Spark dataframe.\n        \"\"\"\n        return spark.read.schema(schema).format(self.FILE_FORMAT).load(path)  # Read schema  # File format  # Load path\n\n    def write_from_interface(self, df: DataFrame, path: str, partition_columns: list[str] = None):\n        \"\"\"Method that writes data from a Spark DataFrame to a file type data source.\n\n        Args:\n            df (DataFrame): Spark DataFrame.\n            path (str): Path to the data.\n            partition_columns (list[str], optional): columns used for a partition write.\n        \"\"\"\n        # Args check\n        if partition_columns is None:\n            partition_columns = []\n\n        df.write.format(\n            self.FILE_FORMAT,  # File format\n        ).partitionBy(partition_columns).mode(\n            \"overwrite\"\n        ).save(path)\n</code></pre>"},{"location":"reference/core/io_interface.html#core.io_interface.PathInterface.read_from_interface","title":"<code>read_from_interface(spark, path, schema=None)</code>","text":"<p>Method that reads data from a file type data source as a Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session.</p> required <code>path</code> <code>str</code> <p>Path to the data.</p> required <code>schema</code> <code>StructType</code> <p>Schema of the data. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <p>Spark dataframe.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def read_from_interface(self, spark: SparkSession, path: str, schema: StructType = None):\n    \"\"\"Method that reads data from a file type data source as a Spark DataFrame.\n\n    Args:\n        spark (SparkSession): Spark session.\n        path (str): Path to the data.\n        schema (StructType, optional): Schema of the data. Defaults to None.\n\n    Returns:\n        DataFrame: Spark dataframe.\n    \"\"\"\n    return spark.read.schema(schema).format(self.FILE_FORMAT).load(path)  # Read schema  # File format  # Load path\n</code></pre>"},{"location":"reference/core/io_interface.html#core.io_interface.PathInterface.write_from_interface","title":"<code>write_from_interface(df, path, partition_columns=None)</code>","text":"<p>Method that writes data from a Spark DataFrame to a file type data source.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Spark DataFrame.</p> required <code>path</code> <code>str</code> <p>Path to the data.</p> required <code>partition_columns</code> <code>list[str]</code> <p>columns used for a partition write.</p> <code>None</code> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def write_from_interface(self, df: DataFrame, path: str, partition_columns: list[str] = None):\n    \"\"\"Method that writes data from a Spark DataFrame to a file type data source.\n\n    Args:\n        df (DataFrame): Spark DataFrame.\n        path (str): Path to the data.\n        partition_columns (list[str], optional): columns used for a partition write.\n    \"\"\"\n    # Args check\n    if partition_columns is None:\n        partition_columns = []\n\n    df.write.format(\n        self.FILE_FORMAT,  # File format\n    ).partitionBy(partition_columns).mode(\n        \"overwrite\"\n    ).save(path)\n</code></pre>"},{"location":"reference/core/io_interface.html#core.io_interface.ShapefileInterface","title":"<code>ShapefileInterface</code>","text":"<p>             Bases: <code>PathInterface</code></p> <p>Class that implements the PathInterface abstract class for reading/writing data from a ShapeFile data source.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>class ShapefileInterface(PathInterface):\n    \"\"\"Class that implements the PathInterface abstract class for reading/writing data from a ShapeFile data source.\"\"\"\n\n    def read_from_interface(self, spark: SparkSession, path: str, schema: StructType = None):\n        \"\"\"Method that reads data from a ShapeFile type data source as a Spark DataFrame.\n\n        Args:\n            spark (SparkSession): Spark session.\n            path (str): Path to the data.\n            schema (StructType, optional): Schema of the data. Defaults to None.\n\n        Returns:\n            DataFrame: Spark dataframe.\n        \"\"\"\n        df = ShapefileReader.readToGeometryRDD(spark.sparkContext, path)\n        return Adapter.toDf(df, spark)\n\n    def write_from_interface(self, df: DataFrame, path: str, partition_columns: list = None):\n        \"\"\"Method that writes data from a Spark DataFrame to a ShapeFile data source.\n\n        Args:\n            df (DataFrame): Spark DataFrame.\n            path (str): Path to the data.\n            partition_columns (list[str], optional): columns used for a partition write.\n        Raises:\n            NotImplementedError: ShapeFile files should not be written in this architecture.\n        \"\"\"\n        raise NotImplementedError(\"Not implemented as Shapefiles shouldn't be written\")\n</code></pre>"},{"location":"reference/core/io_interface.html#core.io_interface.ShapefileInterface.read_from_interface","title":"<code>read_from_interface(spark, path, schema=None)</code>","text":"<p>Method that reads data from a ShapeFile type data source as a Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session.</p> required <code>path</code> <code>str</code> <p>Path to the data.</p> required <code>schema</code> <code>StructType</code> <p>Schema of the data. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <p>Spark dataframe.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def read_from_interface(self, spark: SparkSession, path: str, schema: StructType = None):\n    \"\"\"Method that reads data from a ShapeFile type data source as a Spark DataFrame.\n\n    Args:\n        spark (SparkSession): Spark session.\n        path (str): Path to the data.\n        schema (StructType, optional): Schema of the data. Defaults to None.\n\n    Returns:\n        DataFrame: Spark dataframe.\n    \"\"\"\n    df = ShapefileReader.readToGeometryRDD(spark.sparkContext, path)\n    return Adapter.toDf(df, spark)\n</code></pre>"},{"location":"reference/core/io_interface.html#core.io_interface.ShapefileInterface.write_from_interface","title":"<code>write_from_interface(df, path, partition_columns=None)</code>","text":"<p>Method that writes data from a Spark DataFrame to a ShapeFile data source.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Spark DataFrame.</p> required <code>path</code> <code>str</code> <p>Path to the data.</p> required <code>partition_columns</code> <code>list[str]</code> <p>columns used for a partition write.</p> <code>None</code> <p>Raises:     NotImplementedError: ShapeFile files should not be written in this architecture.</p> Source code in <code>multimno/core/io_interface.py</code> <pre><code>def write_from_interface(self, df: DataFrame, path: str, partition_columns: list = None):\n    \"\"\"Method that writes data from a Spark DataFrame to a ShapeFile data source.\n\n    Args:\n        df (DataFrame): Spark DataFrame.\n        path (str): Path to the data.\n        partition_columns (list[str], optional): columns used for a partition write.\n    Raises:\n        NotImplementedError: ShapeFile files should not be written in this architecture.\n    \"\"\"\n    raise NotImplementedError(\"Not implemented as Shapefiles shouldn't be written\")\n</code></pre>"},{"location":"reference/core/log.html","title":"log","text":"<p>Module that manages the logging functionality.</p>"},{"location":"reference/core/log.html#core.log.generate_logger","title":"<code>generate_logger(config)</code>","text":"<p>Function that initializes a logger.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ConfigParser</code> <p>Object with the final configuration.</p> required <p>Returns:</p> Name Type Description <code>Logger</code> <p>Python logging object.</p> Source code in <code>multimno/core/log.py</code> <pre><code>def generate_logger(config: ConfigParser):\n    \"\"\"Function that initializes a logger.\n\n    Args:\n        config (ConfigParser): Object with the final configuration.\n\n    Returns:\n        Logger: Python logging object.\n    \"\"\"\n    # Parse config\n    log_level = config.get(LOG_CONFIG_KEY, \"level\")\n    log_format = config.get(LOG_CONFIG_KEY, \"format\")\n    datefmt = config.get(LOG_CONFIG_KEY, \"datefmt\")\n\n    # Establish python log config templates\n    loggin_config = {\n        \"version\": 1,\n        \"disable_existing_loggers\": False,\n        \"formatters\": {\"verbose\": {\"format\": (log_format), \"datefmt\": datefmt}},\n        \"handlers\": None,\n        \"loggers\": {},\n    }\n\n    console_handler_config = {\n        \"level\": \"\",\n        \"class\": \"logging.StreamHandler\",\n        \"formatter\": \"verbose\",\n    }\n\n    logger_template_config = {\"level\": \"\", \"handlers\": [], \"propagate\": False}\n\n    # Create console handler\n    log_config = copy.deepcopy(loggin_config)\n    console_handler = copy.deepcopy(console_handler_config)\n    console_handler[\"level\"] = log_level\n    console_handler[\"stream\"] = stdout\n\n    # Create a console handler in Warning level\n    warning_level_console_handler = copy.deepcopy(console_handler_config)\n    warning_level_console_handler[\"level\"] = \"WARNING\"\n\n    # Add handlers to log config\n    log_config[\"handlers\"] = {\"console\": console_handler, \"root_console\": warning_level_console_handler}\n\n    # Create application logger and add it to the python log config\n    logger_template = copy.deepcopy(logger_template_config)\n    logger_template[\"level\"] = log_level\n    logger_template[\"handlers\"] = [\"console\"]\n    log_config[\"loggers\"][APP_NAME] = logger_template\n\n    # Create root logger that all logging using dependency will use\n    root_logger = copy.deepcopy(logger_template_config)\n    root_logger[\"level\"] = \"DEBUG\" if log_level == \"DEBUG\" else \"WARNING\"\n    root_logger[\"handlers\"] = [\"root_console\"]\n    log_config[\"loggers\"][\"\"] = root_logger  # Make all loggers inherit config\n\n    # Establish base logging\n    logging.config.dictConfig(log_config)\n\n    # Return application logger\n    return logging.getLogger(APP_NAME)\n</code></pre>"},{"location":"reference/core/settings.html","title":"settings","text":"<p>Settings module</p>"},{"location":"reference/core/spark_session.html","title":"spark_session","text":"<p>Module that manages the spark session.</p>"},{"location":"reference/core/spark_session.html#core.spark_session.check_if_data_path_exists","title":"<code>check_if_data_path_exists(spark, data_path)</code>","text":"<p>Checks whether data path exists, returns True if it does, False if not</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>active SparkSession</p> required <code>data_path</code> <code>str</code> <p>path to check</p> required <p>Returns:</p> Name Type Description <code>Bool</code> <p>Whether the passed path exists</p> Source code in <code>multimno/core/spark_session.py</code> <pre><code>def check_if_data_path_exists(spark: SparkSession, data_path: str):\n    \"\"\"\n    Checks whether data path exists, returns True if it does, False if not\n\n    Args:\n        spark (SparkSession): active SparkSession\n        data_path (str): path to check\n\n    Returns:\n        Bool: Whether the passed path exists\n    \"\"\"\n    conf = spark._jsc.hadoopConfiguration()\n    uri = spark._jvm.java.net.URI.create(data_path)\n    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(uri, conf)\n    return fs.exists(spark._jvm.org.apache.hadoop.fs.Path(data_path))\n</code></pre>"},{"location":"reference/core/spark_session.html#core.spark_session.check_or_create_data_path","title":"<code>check_or_create_data_path(spark, data_path)</code>","text":"<p>Create the provided path on a file system. If path already exists, do nothing.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>active SparkSession</p> required <code>data_path</code> <code>str</code> <p>path to check</p> required Source code in <code>multimno/core/spark_session.py</code> <pre><code>def check_or_create_data_path(spark: SparkSession, data_path: str):\n    \"\"\"\n    Create the provided path on a file system. If path already exists, do nothing.\n\n    Args:\n        spark (SparkSession): active SparkSession\n        data_path (str): path to check\n    \"\"\"\n    conf = spark._jsc.hadoopConfiguration()\n    uri = spark._jvm.java.net.URI.create(data_path)\n    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(uri, conf)\n    path = spark._jvm.org.apache.hadoop.fs.Path(data_path)\n    if not fs.exists(path):\n        fs.mkdirs(path)\n</code></pre>"},{"location":"reference/core/spark_session.html#core.spark_session.delete_file_or_folder","title":"<code>delete_file_or_folder(spark, data_path)</code>","text":"<p>Deletes file or folder with given path</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Currently active spark session</p> required <code>data_path</code> <code>str</code> <p>Path to remove</p> required Source code in <code>multimno/core/spark_session.py</code> <pre><code>def delete_file_or_folder(spark: SparkSession, data_path: str):\n    \"\"\"\n    Deletes file or folder with given path\n\n    Args:\n        spark (SparkSession): Currently active spark session\n        data_path (str): Path to remove\n    \"\"\"\n    conf = spark._jsc.hadoopConfiguration()\n    uri = spark._jvm.java.net.URI.create(data_path)\n    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(uri, conf)\n    path = spark._jvm.org.apache.hadoop.fs.Path(data_path)\n    fs.delete(path, True)\n</code></pre>"},{"location":"reference/core/spark_session.html#core.spark_session.generate_spark_session","title":"<code>generate_spark_session(config)</code>","text":"<p>Function that generates a Spark Sedona session.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ConfigParser</code> <p>Object with the final configuration.</p> required <p>Returns:</p> Name Type Description <code>SparkSession</code> <p>Session of spark.</p> Source code in <code>multimno/core/spark_session.py</code> <pre><code>def generate_spark_session(config: ConfigParser):\n    \"\"\"Function that generates a Spark Sedona session.\n\n    Args:\n        config (ConfigParser): Object with the final configuration.\n\n    Returns:\n        SparkSession: Session of spark.\n    \"\"\"\n    conf_dict = dict(config[SPARK_CONFIG_KEY])\n    master = conf_dict.pop(\"spark.master\")\n    session_name = conf_dict.pop(\"session_name\")\n\n    builder = SedonaContext.builder().appName(f\"{session_name}\").master(master)\n\n    # Configuration file spark configs\n    for k, v in conf_dict.items():\n        builder = builder.config(k, v)\n\n    # Set sedona session\n    spark = SedonaContext.create(builder.getOrCreate())\n    sc = spark.sparkContext\n    sc.setSystemProperty(\"sedona.global.charset\", \"utf8\")\n\n    # Set log\n    sc.setLogLevel(\"ERROR\")\n    log4j = sc._jvm.org.apache.log4j\n    log4j.LogManager.getRootLogger().setLevel(log4j.Level.ERROR)\n\n    return spark\n</code></pre>"},{"location":"reference/core/spark_session.html#core.spark_session.list_all_files_helper","title":"<code>list_all_files_helper(path, fs, conf)</code>","text":"<p>This function is used by list_all_files_recursively. This should not be called elsewhere Recursively traverses the file tree from given spot saving all files to a list and returns it.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>py4j.java_gateway.JavaObject: Object from parent function</p> required <code>hadoop</code> <code>JavaPackage</code> <p>Object from parent function</p> required <code>fs</code> <code>JavaClass</code> <p>Object from parent function</p> required <code>conf</code> <code>JavaObject</code> <p>Object from parent function</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list[str]</code> <p>List of all files this folder and subdirectories of this folder.</p> Source code in <code>multimno/core/spark_session.py</code> <pre><code>def list_all_files_helper(\n    path: py4j.java_gateway.JavaObject, fs: py4j.java_gateway.JavaClass, conf: py4j.java_gateway.JavaObject\n) -&gt; list[str]:\n    \"\"\"\n    This function is used by list_all_files_recursively. This should not be called elsewhere\n    Recursively traverses the file tree from given spot saving all files to a list and returns it.\n\n    Args:\n        path (str): py4j.java_gateway.JavaObject: Object from parent function\n        hadoop (py4j.java_gateway.JavaPackage): Object from parent function\n        fs (py4j.java_gateway.JavaClass): Object from parent function\n        conf (py4j.java_gateway.JavaObject): Object from parent function\n\n    Returns:\n        list: List of all files this folder and subdirectories of this folder.\n    \"\"\"\n    files_list = []\n\n    for f in fs.listStatus(path):\n        if f.isDirectory():\n            files_list.extend(list_all_files_helper(f.getPath(), fs, conf))\n        else:\n            files_list.append(str(f.getPath()))\n\n    return files_list\n</code></pre>"},{"location":"reference/core/spark_session.html#core.spark_session.list_all_files_recursively","title":"<code>list_all_files_recursively(spark, data_path)</code>","text":"<p>If path is a file, returns a singleton list with this path. If path is a folder, return a list of all files in this folder and any of its subfolders</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Currently active spark session</p> required <code>data_path</code> <code>str</code> <p>Path to list the files of</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of all files in that folder and its subfolders</p> Source code in <code>multimno/core/spark_session.py</code> <pre><code>def list_all_files_recursively(spark: SparkSession, data_path: str) -&gt; list[str]:\n    \"\"\"\n    If path is a file, returns a singleton list with this path.\n    If path is a folder, return a list of all files in this folder and any of its subfolders\n\n    Args:\n        spark (SparkSession): Currently active spark session\n        data_path (str): Path to list the files of\n\n    Returns:\n        list[str]: A list of all files in that folder and its subfolders\n    \"\"\"\n    conf = spark._jsc.hadoopConfiguration()\n    uri = spark._jvm.java.net.URI.create(data_path)\n    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(uri, conf)\n    path = spark._jvm.org.apache.hadoop.fs.Path(data_path)\n    return list_all_files_helper(path, fs, conf)\n</code></pre>"},{"location":"reference/core/spark_session.html#core.spark_session.list_parquet_partition_col_values","title":"<code>list_parquet_partition_col_values(spark, data_path)</code>","text":"<p>Lists all partition column values given a partition parquet folder</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Currently active spark session</p> required <code>data_path</code> <code>str</code> <p>Path of parquet</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>str, list[str]: Name of partition column, List of partition col values</p> Source code in <code>multimno/core/spark_session.py</code> <pre><code>def list_parquet_partition_col_values(spark: SparkSession, data_path: str) -&gt; list[str]:\n    \"\"\"\n    Lists all partition column values given a partition parquet folder\n\n    Args:\n        spark (SparkSession): Currently active spark session\n        data_path (str): Path of parquet\n\n    Returns:\n        str, list[str]: Name of partition column, List of partition col values\n    \"\"\"\n\n    hadoop = spark._jvm.org.apache.hadoop\n    fs = hadoop.fs.FileSystem\n    conf = hadoop.conf.Configuration()\n    path = hadoop.fs.Path(data_path)\n\n    partitions = []\n    for f in fs.get(conf).listStatus(path):\n        if f.isDirectory():\n            partitions.append(str(f.getPath().getName()))\n\n    if len(partitions) == 0:\n        return None, None\n\n    partition_col = partitions[0].split(\"=\")[0]\n\n    partitions = [p.split(\"=\")[1] for p in partitions]\n    return partition_col, sorted(partitions)\n</code></pre>"},{"location":"reference/core/constants/index.html","title":"constants","text":""},{"location":"reference/core/constants/columns.html","title":"columns","text":"<p>Reusable internal column names. Useful for referring to the the same column across multiple components.</p>"},{"location":"reference/core/constants/columns.html#core.constants.columns.ColNames","title":"<code>ColNames</code>","text":"<p>Class that enumerates all the column names.</p> Source code in <code>multimno/core/constants/columns.py</code> <pre><code>class ColNames:\n    \"\"\"\n    Class that enumerates all the column names.\n    \"\"\"\n\n    user_id = \"user_id\"\n    partition_id = \"partition_id\"\n    timestamp = \"timestamp\"\n    mcc = \"mcc\"\n    cell_id = \"cell_id\"\n    latitude = \"latitude\"\n    longitude = \"longitude\"\n    loc_error = \"loc_error\"\n    event_id = \"event_id\"\n\n    year = \"year\"\n    month = \"month\"\n    day = \"day\"\n\n    # for QA by column\n    variable = \"variable\"\n    type_of_error = \"type_of_error\"\n    type_of_transformation = \"type_of_transformation\"\n    value = \"value\"\n    result_timestamp = \"result_timestamp\"\n    data_period_start = \"data_period_start\"\n    data_period_end = \"data_period_end\"\n\n    initial_frequency = \"initial_frequency\"\n    final_frequency = \"final_frequency\"\n    date = \"date\"\n</code></pre>"},{"location":"reference/core/constants/error_types.html","title":"error_types","text":"<p>Transformations Error types module.</p>"},{"location":"reference/core/constants/error_types.html#core.constants.error_types.ErrorTypes","title":"<code>ErrorTypes</code>","text":"<p>Class that enumerates the multiple error types of data transformations.</p> Source code in <code>multimno/core/constants/error_types.py</code> <pre><code>class ErrorTypes:\n    \"\"\"\n    Class that enumerates the multiple error types of data transformations.\n    \"\"\"\n\n    missing_value = 1\n    not_right_syntactic_format = 2\n    out_of_admissible_values = 3\n    inconsistency_between_variables = 4\n    no_location = 5\n    out_of_bounding_box = 6\n    no_error = 9\n\n    # This shows the possible error types that can happen in syntactic event cleaning\n    # This is used for creating the quality metrics data object\n    event_syntactic_cleaning_possible_errors = [1, 2, 3, 4, 5, 6, 9]\n</code></pre>"},{"location":"reference/core/constants/transformations.html","title":"transformations","text":"<p>Data transformations types modukle</p>"},{"location":"reference/core/constants/transformations.html#core.constants.transformations.Transformations","title":"<code>Transformations</code>","text":"<p>Class that enumerates the multiple data transformations types.</p> Source code in <code>multimno/core/constants/transformations.py</code> <pre><code>class Transformations:\n    \"\"\"\n    Class that enumerates the multiple data transformations types.\n    \"\"\"\n\n    converted_timestamp = 1\n    other_conversion = 2\n    no_transformation = 9\n\n    event_syntactic_cleaning_possible_transformations = [1, 2, 9]\n</code></pre>"},{"location":"reference/core/data_objects/index.html","title":"data_objects","text":""},{"location":"reference/core/data_objects/data_object.html","title":"data_object","text":"<p>Module that defines the data object abstract classes</p>"},{"location":"reference/core/data_objects/data_object.html#core.data_objects.data_object.DataObject","title":"<code>DataObject</code>","text":"<p>Abstract class that models a DataObject. It defines its data schema including the attributes that compose it.</p> Source code in <code>multimno/core/data_objects/data_object.py</code> <pre><code>class DataObject(metaclass=ABCMeta):\n    \"\"\"\n    Abstract class that models a DataObject. It defines its data schema including the attributes that compose it.\n    \"\"\"\n\n    ID: str = None\n    SCHEMA: StructType = None\n\n    def __init__(self, spark: SparkSession) -&gt; None:\n        self.df: DataFrame = None\n        self.spark: SparkSession = spark\n        self.interface: IOInterface = None\n\n    def read(self, *args, **kwargs):\n        \"\"\"\n        Method that performs the read operation of the data object dataframe through an IOInterface.\n        \"\"\"\n        self.df = self.interface.read_from_interface(*args, **kwargs)\n\n    def write(self, *args, **kwargs):\n        \"\"\"\n        Method that performs the write operation of the data object dataframe through an IOInterface.\n        \"\"\"\n        self.interface.write_from_interface(self.df, *args, **kwargs)\n</code></pre>"},{"location":"reference/core/data_objects/data_object.html#core.data_objects.data_object.DataObject.read","title":"<code>read(*args, **kwargs)</code>","text":"<p>Method that performs the read operation of the data object dataframe through an IOInterface.</p> Source code in <code>multimno/core/data_objects/data_object.py</code> <pre><code>def read(self, *args, **kwargs):\n    \"\"\"\n    Method that performs the read operation of the data object dataframe through an IOInterface.\n    \"\"\"\n    self.df = self.interface.read_from_interface(*args, **kwargs)\n</code></pre>"},{"location":"reference/core/data_objects/data_object.html#core.data_objects.data_object.DataObject.write","title":"<code>write(*args, **kwargs)</code>","text":"<p>Method that performs the write operation of the data object dataframe through an IOInterface.</p> Source code in <code>multimno/core/data_objects/data_object.py</code> <pre><code>def write(self, *args, **kwargs):\n    \"\"\"\n    Method that performs the write operation of the data object dataframe through an IOInterface.\n    \"\"\"\n    self.interface.write_from_interface(self.df, *args, **kwargs)\n</code></pre>"},{"location":"reference/core/data_objects/data_object.html#core.data_objects.data_object.PathDataObject","title":"<code>PathDataObject</code>","text":"<p>             Bases: <code>DataObject</code></p> <p>Abstract Class that models DataObjects that will use a PathInterface for IO operations. It inherits the DataObject abstract class.</p> Source code in <code>multimno/core/data_objects/data_object.py</code> <pre><code>class PathDataObject(DataObject, metaclass=ABCMeta):\n    \"\"\"Abstract Class that models DataObjects that will use a PathInterface for IO operations.\n    It inherits the DataObject abstract class.\n    \"\"\"\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark)\n        self.interface: PathInterface = None\n        self.default_path: str = default_path\n\n    def read(self, *args, path: str = None, **kwargs):\n        if path is None:\n            path = self.default_path\n        self.df = self.interface.read_from_interface(self.spark, path, self.SCHEMA)\n\n    def write(self, *args, path: str = None, partition_columns: list[str] = None, **kwargs):\n        if path is None:\n            path = self.default_path\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/bronze/index.html","title":"bronze","text":""},{"location":"reference/core/data_objects/bronze/bronze_event_data_object.html","title":"bronze_event_data_object","text":"<p>Bronze MNO Event data module</p>"},{"location":"reference/core/data_objects/bronze/bronze_event_data_object.html#core.data_objects.bronze.bronze_event_data_object.BronzeEventDataObject","title":"<code>BronzeEventDataObject</code>","text":"<p>             Bases: <code>PathDataObject</code></p> <p>Class that models the RAW MNO Event data.</p> Source code in <code>multimno/core/data_objects/bronze/bronze_event_data_object.py</code> <pre><code>class BronzeEventDataObject(PathDataObject):\n    \"\"\"\n    Class that models the RAW MNO Event data.\n    \"\"\"\n\n    ID = \"BronzeEventDO\"\n    SCHEMA = StructType(\n        [\n            StructField(ColNames.user_id, BinaryType(), nullable=True),\n            StructField(ColNames.timestamp, StringType(), nullable=True),\n            StructField(ColNames.mcc, IntegerType(), nullable=True),\n            StructField(ColNames.cell_id, StringType(), nullable=True),\n            StructField(ColNames.latitude, FloatType(), nullable=True),\n            StructField(ColNames.longitude, FloatType(), nullable=True),\n            StructField(ColNames.loc_error, FloatType(), nullable=True),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str, partition_columns: list[str] = None) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface = ParquetInterface()\n        self.partition_columns = partition_columns\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.interface.write_from_interface(self.df, path, partition_columns)\n</code></pre>"},{"location":"reference/core/data_objects/landing/index.html","title":"landing","text":""},{"location":"reference/core/data_objects/silver/index.html","title":"silver","text":""},{"location":"reference/core/data_objects/silver/silver_event_data_object.html","title":"silver_event_data_object","text":"<p>Silver MNO Event data module</p>"},{"location":"reference/core/data_objects/silver/silver_event_data_object.html#core.data_objects.silver.silver_event_data_object.SilverEventDataObject","title":"<code>SilverEventDataObject</code>","text":"<p>             Bases: <code>PathDataObject</code></p> <p>Class that models the cleaned MNO Event data.</p> Source code in <code>multimno/core/data_objects/silver/silver_event_data_object.py</code> <pre><code>class SilverEventDataObject(PathDataObject):\n    \"\"\"\n    Class that models the cleaned MNO Event data.\n    \"\"\"\n\n    ID = \"SilverEventDO\"\n    SCHEMA = StructType(\n        [\n            StructField(\"user_id\", BinaryType(), nullable=False),\n            StructField(\"timestamp\", TimestampType(), nullable=False),\n            StructField(\"mcc\", IntegerType(), nullable=False),\n            StructField(\"cell_id\", StringType(), nullable=True),\n            StructField(\"latitude\", FloatType(), nullable=True),\n            StructField(\"longitude\", FloatType(), nullable=True),\n            StructField(\"loc_error\", FloatType(), nullable=True),\n            StructField(\"year\", ShortType(), nullable=False),\n            StructField(\"month\", ByteType(), nullable=False),\n            StructField(\"day\", ByteType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface: ParquetInterface = ParquetInterface()\n        self.partition_columns = [\"year\", \"month\", \"day\"]\n\n        # Clear path\n        self.first_write = True\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        # If it is the first writing of this data object, clear the input directory, otherwise add\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n        if path is None:\n            path = self.default_path\n\n        self.df.write.format(\n            self.interface.FILE_FORMAT,  # File format\n        ).partitionBy(self.partition_columns).mode(\n            \"append\"\n        ).save(path)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_metrics_by_column.html","title":"silver_event_data_syntactic_quality_metrics_by_column","text":"<p>Silver Event Data quality metrics.</p>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_metrics_by_column.html#core.data_objects.silver.silver_event_data_syntactic_quality_metrics_by_column.SilverEventDataSyntacticQualityMetricsByColumn","title":"<code>SilverEventDataSyntacticQualityMetricsByColumn</code>","text":"<p>             Bases: <code>PathDataObject</code></p> <p>Class that models the Silver Event Data quality metrics DataObject.</p> Source code in <code>multimno/core/data_objects/silver/silver_event_data_syntactic_quality_metrics_by_column.py</code> <pre><code>class SilverEventDataSyntacticQualityMetricsByColumn(PathDataObject):\n    \"\"\"\n    Class that models the Silver Event Data quality metrics DataObject.\n    \"\"\"\n\n    ID = \"SilverEventDataSyntacticQualityMetricsByColumn\"\n    SCHEMA = StructType(\n        [\n            StructField(\"result_timestamp\", TimestampType(), nullable=False),\n            StructField(\"data_period_start\", DateType(), nullable=False),\n            StructField(\"data_period_end\", DateType(), nullable=False),\n            StructField(\"variable\", StringType(), nullable=True),\n            StructField(\"type_of_error\", ShortType(), nullable=True),\n            StructField(\"type_of_transformation\", ShortType(), nullable=True),\n            StructField(\"value\", IntegerType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface: ParquetInterface = ParquetInterface()\n        self.partition_columns = None\n\n        # (variable, type_of_error, type_of_transformation) : value\n        self.error_and_transformation_counts = defaultdict(int)\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.df.write.format(\n            self.interface.FILE_FORMAT,  # File format\n        ).mode(\n            \"append\"\n        ).save(path)\n</code></pre>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_metrics_frequency_distribution.html","title":"silver_event_data_syntactic_quality_metrics_frequency_distribution","text":"<p>Silver Event Data frequency quality metrics.</p>"},{"location":"reference/core/data_objects/silver/silver_event_data_syntactic_quality_metrics_frequency_distribution.html#core.data_objects.silver.silver_event_data_syntactic_quality_metrics_frequency_distribution.SilverEventDataSyntacticQualityMetricsFrequencyDistribution","title":"<code>SilverEventDataSyntacticQualityMetricsFrequencyDistribution</code>","text":"<p>             Bases: <code>PathDataObject</code></p> <p>Class that models the Silver Event Data frequency quality metrics DataObject.</p> Source code in <code>multimno/core/data_objects/silver/silver_event_data_syntactic_quality_metrics_frequency_distribution.py</code> <pre><code>class SilverEventDataSyntacticQualityMetricsFrequencyDistribution(PathDataObject):\n    \"\"\"\n    Class that models the Silver Event Data frequency quality metrics DataObject.\n    \"\"\"\n\n    ID = \"SilverEventDataSyntacticQualityMetricsFrequencyDistribution\"\n    SCHEMA = StructType(\n        [\n            StructField(\"cell_id\", StringType(), nullable=True),\n            StructField(\"user_id\", BinaryType(), nullable=True),\n            StructField(\"initial_frequency\", IntegerType(), nullable=False),\n            StructField(\"final_frequency\", IntegerType(), nullable=False),\n            StructField(\"date\", DateType(), nullable=False),\n        ]\n    )\n\n    def __init__(self, spark: SparkSession, default_path: str) -&gt; None:\n        super().__init__(spark, default_path)\n        self.interface: ParquetInterface = ParquetInterface()\n        self.partition_columns = [\"date\"]\n\n    def write(self, path: str = None, partition_columns: list[str] = None):\n        if path is None:\n            path = self.default_path\n        if partition_columns is None:\n            partition_columns = self.partition_columns\n\n        self.df.write.format(\n            self.interface.FILE_FORMAT,  # File format\n        ).partitionBy(partition_columns).mode(\n            \"append\"\n        ).save(path)\n</code></pre>"}]}